[
    {
        "text_input": "[Pallas/Mosaic GPU] Expose the new `TCGEN05_COL` layout.\n\n`TCGEN05_COL` is to `TCGEN05` what `WGMMA_ROW` is to `WGMMA`.\n\nPiperOrigin-RevId: 767500979",
        "output": "```diff\nCommit: 986c41132710281bd3154299db62c30e25f5b6b8\nDate: 2025-06-05T09:03:50Z\nURL: https://github.com/jax-ml/jax/commit/986c41132710281bd3154299db62c30e25f5b6b8\nFiles changed: 5\nAdditions: +33, Deletions: -3\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex f37a003f4401..41c8a673b528 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -1452,6 +1452,7 @@ class Layout(enum.Enum):\n \n   TCGEN05 = enum.auto()\n   TCGEN05_ROW = enum.auto()\n+  TCGEN05_COL = enum.auto()\n \n   def __call__(self, *args, **kwargs) -> ParameterizedLayout:\n     return ParameterizedLayout(self, args, kwargs)\n@@ -1484,6 +1485,9 @@ def check_no_args():\n       case Layout.TCGEN05_ROW:\n         check_no_args()\n         return mgpu.TCGEN05_ROW_LAYOUT\n+      case Layout.TCGEN05_COL:\n+        check_no_args()\n+        return mgpu.TCGEN05_COL_LAYOUT\n \n @dataclasses.dataclass(frozen=True)\n class ParameterizedLayout:\ndiff --git a/jax/experimental/mosaic/gpu/__init__.py b/jax/experimental/mosaic/gpu/__init__.py\nindex 37bfa227fe61..f4b65c976ef5 100644\n--- a/jax/experimental/mosaic/gpu/__init__.py\n+++ b/jax/experimental/mosaic/gpu/__init__.py\n@@ -106,4 +106,5 @@\n from .tcgen05 import (\n   LAYOUT as TCGEN05_LAYOUT,  # noqa: F401\n   ROW_LAYOUT as TCGEN05_ROW_LAYOUT,  # noqa: F401\n+  COL_LAYOUT as TCGEN05_COL_LAYOUT,  # noqa: F401\n )\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex 3554ed95844c..40d95f65f210 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -461,6 +461,12 @@ def replace_tiled_dim(d: int | Replicated, size: int):\n         new_vector_dim,\n     )\n \n+  def reduce(self, axes: Sequence[int]) -> TiledLayout:\n+    reduced_layout = self\n+    for a in sorted(axes, reverse=True):\n+      reduced_layout = reduced_layout.remove_dimension(a)\n+    return reduced_layout\n+\n \n def _tiled_wgmma_layout(shape: tuple[int, ...]):\n   \"\"\"Returns the tiled layout relevant for WGMMA operations.\n@@ -1825,9 +1831,7 @@ def reduce(\n       out_reg = vector.extractelement(out_reg, position=c(0, index))\n       out_regs = np.asarray(out_reg, dtype=object)\n     else:\n-      reduced_layout = layout\n-      for a in sorted(axis, reverse=True):\n-        reduced_layout = reduced_layout.remove_dimension(a)\n+      reduced_layout = layout.reduce(axis)\n       out_regs = out_regs.reshape(reduced_layout.registers_shape(reduced_logical_shape))\n     return FragmentedArray(\n         _registers=out_regs, _layout=reduced_layout, _is_signed=self.is_signed\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex 91797fe65d1f..32f0453148f4 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -50,6 +50,13 @@\n     lane_dims=(-3, fa.Replicated(times=4)),\n     vector_dim=-1\n )\n+# COL_LAYOUT is to LAYOUT as WGMMA_COL_LAYOUT is to WGMMA_LAYOUT.\n+COL_LAYOUT = fa.TiledLayout(\n+    fa.Tiling(tiles=((8,), (8,), (8,), (2,))),\n+    warp_dim=fa.Replicated(times=4),\n+    lane_dims=(fa.Replicated(times=8), -2),\n+    vector_dim=-1\n+)\n # A layout resembling the logical organization of TMEM. The 128 rows in a tile\n # are assigned to 128 lanes in the warpgroup. Useful when the result needs to be\n # processed in registers and then stored back into TMEM. Should not be used if\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 31262dcae44d..a6828a31dd9f 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -2289,6 +2289,20 @@ def kernel(ctx, dst, _):\n     result = m * n if reduce_both else n\n     np.testing.assert_array_equal(kernel_fn(), jnp.full((m,), result, dtype))\n \n+  @parameterized.named_parameters(\n+      (\"wgmma_row\", fa.WGMMA_LAYOUT, fa.WGMMA_ROW_LAYOUT, 1),\n+      (\"wgmma_col\", fa.WGMMA_LAYOUT, fa.WGMMA_COL_LAYOUT, 0),\n+      (\"tcgen05_row\", tcgen05.LAYOUT, tcgen05.ROW_LAYOUT, 1),\n+      (\"tcgen05_col\", tcgen05.LAYOUT, tcgen05.COL_LAYOUT, 0),\n+  )\n+  def test_layout_reduction_definition(self, layout, expected_reduced_layout, axis):\n+    def squeeze_shape(shape):\n+      return tuple(s for s in shape if s != 1)\n+    reduced_layout = layout.reduce((axis,))\n+    tiled_shape = squeeze_shape(reduced_layout.tiled_tiling_shape)\n+    expected_tiled_shape = squeeze_shape(expected_reduced_layout.tiled_tiling_shape)\n+    self.assertEqual(tiled_shape, expected_tiled_shape)\n+\n   @parameterized.product(\n       op=(arith.addf, arith.maximumf),\n       m=(64, 128),\n\n```",
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29258 from mattjj:andy-fix\n\nPiperOrigin-RevId: 767380947",
        "output": "```diff\nCommit: 5f054067fa5b40afc2102f97a38087baf6ad0483\nDate: 2025-06-05T02:17:31Z\nURL: https://github.com/jax-ml/jax/commit/5f054067fa5b40afc2102f97a38087baf6ad0483\nFiles changed: 2\nAdditions: +45, Deletions: -4\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex b644bdefecc2..9c9a1dd8c1ba 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -4951,6 +4951,9 @@ def _to_edtype_abstract_eval(x, *, edtype):\n           not isinstance(x.dtype, dtypes.ExtendedDType))\n   # For backward compatibility, if the edtype rules have a `convert_to` method,\n   # use that rather than looking for an `allow_conversion: bool` attribute.\n+  if not isinstance(x, (ShapedArray, core.DShapedArray)):\n+    raise TypeError(\"can only convert to an extended dtype on an array type,\"\n+                    f\"but got {type(x)}\")\n   if convert_to := getattr(edtype._rules, 'convert_to', None):\n     allow_conversion = convert_to(x.dtype, edtype)\n   else:\n@@ -4960,6 +4963,7 @@ def _to_edtype_abstract_eval(x, *, edtype):\n         f\"Cannot convert_element_type from {dtype_to_string(x.dtype)} \"\n         f\"to {dtype_to_string(edtype)}\")\n   rep_aval = core.physical_element_aval(edtype)\n+  assert tuple(rep_aval.sharding.spec) == (None,) * rep_aval.ndim\n   if x.dtype != rep_aval.dtype:\n     raise ValueError(\n         \"can only convert to extended dtype from its representation dtype, \"\n@@ -4982,7 +4986,20 @@ def _to_edtype_abstract_eval(x, *, edtype):\n         f\" has a representation shape {rep_aval.shape} while the given \"\n         f\"representation array has shape {x.shape}, so the shape suffix \"\n         f\"does not match: given {shape_suffix} but required {rep_aval.shape}.\")\n-  return x.update(shape=shape_prefix, dtype=edtype)\n+  if isinstance(x, ShapedArray):\n+    spec_prefix, spec_suffix = x.sharding.spec[:n], x.sharding.spec[n:]\n+    if tuple(spec_suffix) != (None,) * len(spec_suffix):\n+      raise ValueError(\n+          \"can only convert to extended dtype from an array with trailing \"\n+          \"axes that are not explicitly sharded, but tried to convert from \"\n+          f\"{x.str_short(short_dtypes=True)} to an extended dtype with element \"\n+          f\"shape {rep_aval.shape}\")\n+    return x.update(shape=shape_prefix, dtype=edtype,\n+                    sharding=x.sharding.with_spec(spec_prefix))\n+  elif isinstance(x, core.DShapedArray):\n+    return x.update(shape=shape_prefix, dtype=edtype)\n+  else:\n+    assert False  # unreachable, see isinstance check above\n \n to_edtype_p = Primitive('to_edtype')\n to_edtype_p.def_impl(partial(dispatch.apply_primitive, to_edtype_p))\n@@ -4999,6 +5016,9 @@ def _to_edtype_abstract_eval(x, *, edtype):\n def _from_edtype_abstract_eval(x, *, dtype):\n   assert (isinstance(x.dtype, dtypes.ExtendedDType) and\n           not isinstance(dtype, dtypes.ExtendedDType))\n+  if not isinstance(x, (ShapedArray, core.DShapedArray)):\n+    raise TypeError(\"can only convert from an extended dtype on an array type,\"\n+                    f\"but got {type(x)}\")\n   if convert_from := getattr(x.dtype._rules, 'convert_from', None):\n     allow_conversion = convert_from(x.dtype, dtype)\n   else:\n@@ -5008,16 +5028,22 @@ def _from_edtype_abstract_eval(x, *, dtype):\n         f\"Cannot convert_element_type from {dtype_to_string(x.dtype)} \"\n         f\"to {dtype_to_string(dtype)}\")\n   rep_aval = core.physical_element_aval(x.dtype)\n+  assert tuple(rep_aval.sharding.spec) == (None,) * rep_aval.ndim\n   if rep_aval.dtype != dtype:\n     raise ValueError(\n         \"can only convert from extended dtype to its representation dtype, \"\n         f\"but tried to convert from {dtype_to_string(x.dtype)} to \"\n         f\"{dtype_to_string(dtype)} which doesn't match the representation type \"\n         f\"{dtype_to_string(rep_aval.dtype)}.\")\n-  if all(isinstance(d, int) for d in x.shape):\n-    return core.ShapedArray(shape=(*x.shape, *rep_aval.shape), dtype=dtype)\n+  if isinstance(x, ShapedArray):\n+    return x.update(shape=(*x.shape, *rep_aval.shape), dtype=dtype)\n+  elif isinstance(x, core.DShapedArray):\n+    if all(isinstance(d, int) for d in x.shape):\n+      return core.ShapedArray(shape=(*x.shape, *rep_aval.shape), dtype=dtype)\n+    else:\n+      raise NotImplementedError\n   else:\n-    raise NotImplementedError\n+    assert False  # unreachable, see isinstance check above\n \n from_edtype_p = Primitive('from_edtype')\n from_edtype_p.def_impl(partial(dispatch.apply_primitive, from_edtype_p))\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex b08d462b3485..8c9dcc05d406 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -46,6 +46,7 @@\n from jax._src.compilation_cache import is_persistent_cache_enabled\n from jax.experimental.custom_partitioning import (\n     custom_partitioning, SdyShardingRule, BATCHING)\n+from jax.experimental import primal_tangent_dtype\n from jax._src import array\n from jax._src.sharding import Sharding, common_devices_indices_map\n from jax._src import op_shardings\n@@ -7928,6 +7929,20 @@ def simple_func(w, x):\n \n     jax.lax.map(lambda _x: simple_func(w, _x), x, batch_size=2)  # doesn't crash\n \n+  @jtu.with_explicit_mesh((2,), ('x',))\n+  def test_extended_dtypes(self, mesh):\n+    dtype = primal_tangent_dtype(jnp.dtype('int8'), jnp.dtype('bfloat16'))\n+\n+    @jax.jit\n+    def f(x):\n+      x = jax.lax.convert_element_type(x, dtype)\n+      self.assertEqual(x.aval.sharding.spec, P('x'))\n+      x = jax.lax.convert_element_type(x, 'int8')\n+      self.assertEqual(x.aval.sharding.spec, P('x'))\n+\n+    x = jax.device_put(jnp.arange(8, dtype='int8'), P('x',))\n+    f(x)  # doesn't crash\n+\n \n @jtu.pytest_mark_if_available('multiaccelerator')\n class PJitErrorTest(jtu.JaxTestCase):\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "fix sharding-in-types + from_edtype\n\nCo-authored-by: Yash Katariya <yashkatariya@google.com>",
        "output": "```diff\nCommit: c14907890d18ca678e904a2a4acd6c513f72d43a\nDate: 2025-06-05T01:57:51Z\nURL: https://github.com/jax-ml/jax/commit/c14907890d18ca678e904a2a4acd6c513f72d43a\nFiles changed: 2\nAdditions: +45, Deletions: -4\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex b644bdefecc2..9c9a1dd8c1ba 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -4951,6 +4951,9 @@ def _to_edtype_abstract_eval(x, *, edtype):\n           not isinstance(x.dtype, dtypes.ExtendedDType))\n   # For backward compatibility, if the edtype rules have a `convert_to` method,\n   # use that rather than looking for an `allow_conversion: bool` attribute.\n+  if not isinstance(x, (ShapedArray, core.DShapedArray)):\n+    raise TypeError(\"can only convert to an extended dtype on an array type,\"\n+                    f\"but got {type(x)}\")\n   if convert_to := getattr(edtype._rules, 'convert_to', None):\n     allow_conversion = convert_to(x.dtype, edtype)\n   else:\n@@ -4960,6 +4963,7 @@ def _to_edtype_abstract_eval(x, *, edtype):\n         f\"Cannot convert_element_type from {dtype_to_string(x.dtype)} \"\n         f\"to {dtype_to_string(edtype)}\")\n   rep_aval = core.physical_element_aval(edtype)\n+  assert tuple(rep_aval.sharding.spec) == (None,) * rep_aval.ndim\n   if x.dtype != rep_aval.dtype:\n     raise ValueError(\n         \"can only convert to extended dtype from its representation dtype, \"\n@@ -4982,7 +4986,20 @@ def _to_edtype_abstract_eval(x, *, edtype):\n         f\" has a representation shape {rep_aval.shape} while the given \"\n         f\"representation array has shape {x.shape}, so the shape suffix \"\n         f\"does not match: given {shape_suffix} but required {rep_aval.shape}.\")\n-  return x.update(shape=shape_prefix, dtype=edtype)\n+  if isinstance(x, ShapedArray):\n+    spec_prefix, spec_suffix = x.sharding.spec[:n], x.sharding.spec[n:]\n+    if tuple(spec_suffix) != (None,) * len(spec_suffix):\n+      raise ValueError(\n+          \"can only convert to extended dtype from an array with trailing \"\n+          \"axes that are not explicitly sharded, but tried to convert from \"\n+          f\"{x.str_short(short_dtypes=True)} to an extended dtype with element \"\n+          f\"shape {rep_aval.shape}\")\n+    return x.update(shape=shape_prefix, dtype=edtype,\n+                    sharding=x.sharding.with_spec(spec_prefix))\n+  elif isinstance(x, core.DShapedArray):\n+    return x.update(shape=shape_prefix, dtype=edtype)\n+  else:\n+    assert False  # unreachable, see isinstance check above\n \n to_edtype_p = Primitive('to_edtype')\n to_edtype_p.def_impl(partial(dispatch.apply_primitive, to_edtype_p))\n@@ -4999,6 +5016,9 @@ def _to_edtype_abstract_eval(x, *, edtype):\n def _from_edtype_abstract_eval(x, *, dtype):\n   assert (isinstance(x.dtype, dtypes.ExtendedDType) and\n           not isinstance(dtype, dtypes.ExtendedDType))\n+  if not isinstance(x, (ShapedArray, core.DShapedArray)):\n+    raise TypeError(\"can only convert from an extended dtype on an array type,\"\n+                    f\"but got {type(x)}\")\n   if convert_from := getattr(x.dtype._rules, 'convert_from', None):\n     allow_conversion = convert_from(x.dtype, dtype)\n   else:\n@@ -5008,16 +5028,22 @@ def _from_edtype_abstract_eval(x, *, dtype):\n         f\"Cannot convert_element_type from {dtype_to_string(x.dtype)} \"\n         f\"to {dtype_to_string(dtype)}\")\n   rep_aval = core.physical_element_aval(x.dtype)\n+  assert tuple(rep_aval.sharding.spec) == (None,) * rep_aval.ndim\n   if rep_aval.dtype != dtype:\n     raise ValueError(\n         \"can only convert from extended dtype to its representation dtype, \"\n         f\"but tried to convert from {dtype_to_string(x.dtype)} to \"\n         f\"{dtype_to_string(dtype)} which doesn't match the representation type \"\n         f\"{dtype_to_string(rep_aval.dtype)}.\")\n-  if all(isinstance(d, int) for d in x.shape):\n-    return core.ShapedArray(shape=(*x.shape, *rep_aval.shape), dtype=dtype)\n+  if isinstance(x, ShapedArray):\n+    return x.update(shape=(*x.shape, *rep_aval.shape), dtype=dtype)\n+  elif isinstance(x, core.DShapedArray):\n+    if all(isinstance(d, int) for d in x.shape):\n+      return core.ShapedArray(shape=(*x.shape, *rep_aval.shape), dtype=dtype)\n+    else:\n+      raise NotImplementedError\n   else:\n-    raise NotImplementedError\n+    assert False  # unreachable, see isinstance check above\n \n from_edtype_p = Primitive('from_edtype')\n from_edtype_p.def_impl(partial(dispatch.apply_primitive, from_edtype_p))\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex b08d462b3485..8c9dcc05d406 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -46,6 +46,7 @@\n from jax._src.compilation_cache import is_persistent_cache_enabled\n from jax.experimental.custom_partitioning import (\n     custom_partitioning, SdyShardingRule, BATCHING)\n+from jax.experimental import primal_tangent_dtype\n from jax._src import array\n from jax._src.sharding import Sharding, common_devices_indices_map\n from jax._src import op_shardings\n@@ -7928,6 +7929,20 @@ def simple_func(w, x):\n \n     jax.lax.map(lambda _x: simple_func(w, _x), x, batch_size=2)  # doesn't crash\n \n+  @jtu.with_explicit_mesh((2,), ('x',))\n+  def test_extended_dtypes(self, mesh):\n+    dtype = primal_tangent_dtype(jnp.dtype('int8'), jnp.dtype('bfloat16'))\n+\n+    @jax.jit\n+    def f(x):\n+      x = jax.lax.convert_element_type(x, dtype)\n+      self.assertEqual(x.aval.sharding.spec, P('x'))\n+      x = jax.lax.convert_element_type(x, 'int8')\n+      self.assertEqual(x.aval.sharding.spec, P('x'))\n+\n+    x = jax.device_put(jnp.arange(8, dtype='int8'), P('x',))\n+    f(x)  # doesn't crash\n+\n \n @jtu.pytest_mark_if_available('multiaccelerator')\n class PJitErrorTest(jtu.JaxTestCase):\n\n```",
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "Merge pull request #29245 from jax-ml:quasi-dynamic-data\n\nPiperOrigin-RevId: 767365438",
        "output": "```diff\nCommit: 6ad1b1101889ac5ef03ccace1b6e6d0d73b66b1f\nDate: 2025-06-05T01:18:53Z\nURL: https://github.com/jax-ml/jax/commit/6ad1b1101889ac5ef03ccace1b6e6d0d73b66b1f\nFiles changed: 6\nAdditions: +277, Deletions: -181\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex 91ddd77e3d49..7260557ff4cc 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,8 +88,7 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info', '_is_high',\n-               '_initial_typechange_env', '_final_typechange_env']\n+               '_effects', '_debug_info', '_is_high']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -98,8 +97,6 @@ class Jaxpr:\n   _effects: Effects\n   _debug_info: DebugInfo\n   _is_high: bool\n-  _initial_typechange_env: dict[Var, Any]\n-  _final_typechange_env: dict[Var, Any]\n \n   @property\n   def constvars(self) -> list[Var]:\n@@ -129,14 +126,6 @@ def debug_info(self) -> DebugInfo:\n   def is_high(self) -> bool:\n     return self._is_high\n \n-  @property\n-  def initial_typechange_env(self) -> dict[Var, Any]:\n-    return self._initial_typechange_env\n-\n-  @property\n-  def final_typechange_env(self) -> dict[Var, Any]:\n-    return self._final_typechange_env\n-\n   def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                outvars: Sequence[Atom], eqns: Sequence[JaxprEqn],\n                effects: Effects = no_effects,\n@@ -145,8 +134,6 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n                is_high: bool = False,\n-               initial_typechange_env: dict | None = None,\n-               final_typechange_env: dict | None = None,\n                ):\n     \"\"\"\n     Args:\n@@ -172,8 +159,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n     self._is_high = is_high\n-    self._initial_typechange_env = initial_typechange_env or {}\n-    self._final_typechange_env = final_typechange_env or {}\n+    num_vars = len(constvars) + len(invars)\n \n   def __str__(self):\n     return str(self.pretty_print())\n@@ -201,10 +187,6 @@ def replace(self, **kwargs):\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n         is_high=kwargs.pop(\"is_high\", self.is_high),\n-        initial_typechange_env=kwargs.pop(\"initial_typechange_env\",\n-                                          self.initial_typechange_env),\n-        final_typechange_env=kwargs.pop(\"final_typechange_env\",\n-                                        self.final_typechange_env),\n     )\n     if kwargs:\n       raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n@@ -232,22 +214,6 @@ def subjaxprs(jaxpr: Jaxpr) -> Iterator[Jaxpr]:\n     yield from jaxprs_in_params(eqn.params)\n \n \n-@dataclass(frozen=True)\n-class TypeChange:\n-  aval: AbstractValue\n-  initial_type_state: Any\n-  final_type_state: Any\n-\n-  def to_tangent_aval(self):\n-    return TypeChange(self.aval.to_tangent_aval(),\n-                      self.initial_type_state.to_tangent_aval(),\n-                      self.final_type_state.to_tangent_aval())\n-\n-  def normalize(self):\n-    return TypeChange(self.aval.normalize(),\n-                      self.initial_type_state.normalize(),\n-                      self.final_type_state.normalize())\n-\n class ClosedJaxpr:\n   __slots__ = ['__weakref__', '_jaxpr', '_consts']\n \n@@ -268,10 +234,13 @@ def in_avals(self):\n     return [v.aval for v in self.jaxpr.invars]\n \n   @property\n-  def in_avals_aug(self):\n-    ienv = self.jaxpr.initial_typechange_env\n-    fenv = self.jaxpr.final_typechange_env\n-    return [TypeChange(v.aval, ienv[v], fenv[v]) if v.aval.mutable else v.aval\n+  def in_aval_qdds(self) -> list[AbstractValue | AvalQDD]:\n+    return [v.aval if v.initial_qdd is None else AvalQDD(v.aval, v.initial_qdd)\n+            for v in self.jaxpr.invars]\n+\n+  @property\n+  def final_aval_qdds(self) -> list[AbstractValue | AvalQDD]:\n+    return [v.aval if v.final_qdd is None else AvalQDD(v.aval, v.final_qdd)\n             for v in self.jaxpr.invars]\n \n   @property\n@@ -464,16 +433,22 @@ def new_jaxpr_eqn(invars, outvars, primitive, params, effects, source_info=None,\n _var_counter = it.count()\n \n class Var:\n-  __slots__ = [\"count\", \"suffix\", \"aval\"]\n+  __slots__ = [\"count\", \"suffix\", \"aval\", \"initial_qdd\", \"final_qdd\"]\n \n   count: int\n   suffix: str\n   aval: AbstractValue\n+  # these are only useful for jaxpr binders but rather than create a separate\n+  # type for those, breaking existing interpreters, we add fields here.\n+  initial_qdd : QuasiDynamicData | None\n+  final_qdd : QuasiDynamicData | None\n \n-  def __init__(self, suffix: str, aval: AbstractValue):\n+  def __init__(self, suffix: str, aval: AbstractValue, initial_qdd = None, final_qdd = None):\n     self.count = next(_var_counter)\n     self.suffix = suffix\n     self.aval = aval\n+    self.initial_qdd = initial_qdd\n+    self.final_qdd = final_qdd\n \n   def __repr__(self):\n     return f'Var(id={id(self)}){self.suffix}:{self.aval.str_short()}'\n@@ -483,7 +458,7 @@ def pretty_print(self, context: JaxprPpContext, *, print_dtype: bool = True):\n     return f\"{context.var_names[self]}{self.suffix}\"\n \n \n-def gensym(suffix: str = '') -> Callable[[AbstractValue], Var]:\n+def gensym(suffix: str = '') -> Callable:\n   \"\"\"Produce distinct variables, printed with the optional suffix.\"\"\"\n   return partial(Var, suffix)\n \n@@ -1114,6 +1089,8 @@ def process_custom_vjp_call(self, primitive, fun, fwd, bwd, tracers, **_):  # py\n     del primitive, fwd, bwd, _  # Unused.\n     return fun.call_wrapped(*tracers)\n \n+  def cur_qdd(self, x):\n+    return x.cur_qdd()\n \n class TraceTag:\n   # TODO: this works for surprisingly subtle reasons. Function transformations\n@@ -1505,7 +1482,7 @@ def definitely_equal(x, y):\n class AbstractValue:\n   __slots__: list[str] = []\n   is_high = False\n-  mutable = False\n+  has_qdd = False\n \n   def to_tangent_aval(self):\n     raise NotImplementedError(\"must override\")\n@@ -1533,6 +1510,12 @@ def normalize(self) -> AbstractValue:\n   def update(self, **kwargs):\n     raise NotImplementedError(\"must override\")\n \n+  def lo_ty(self):\n+    raise NotImplementedError(\"must override\")\n+\n+  def lo_ty_qdd(self, qdd):\n+    raise NotImplementedError(\"avals with qdd must override\")\n+\n   def str_short(self, short_dtypes=False, mesh_axis_types=False):\n     return str(self)\n \n@@ -1704,6 +1687,54 @@ def concrete_dim_or_error(val: Any, context=\"\"):\n   else:\n     return concrete_or_error(operator.index, val, context=context)\n \n+### Quasi-dynamic data\n+\n+# Quasi-dynamic data includes things like liveness bits and the content type of\n+# a type-changeable box. These change throughout the program but at a given\n+# point in the program they have a single statically known value.\n+\n+class MutableQuasiDynamicData:\n+  def __init__(self, val : QuasiDynamicData | None):\n+    self.init_val = val\n+    self.cur_val = val  # immutable payload\n+\n+  def update(self, val):\n+    self.cur_val = val\n+\n+class QuasiDynamicData:\n+  pass\n+\n+@dataclass(frozen=True)\n+class AvalQDD:\n+  aval: AbstractValue\n+  qdd: QuasiDynamicData | None # immutable\n+\n+  has_qdd = True\n+  def lo_ty(self):\n+    return self.aval.lo_ty_qdd(self.qdd)  # type: ignore\n+\n+  def read_loval(self, val):\n+    return self.aval.read_loval(self.qdd, val)  # type: ignore\n+\n+  def new_from_loval(self, *lovals):\n+    return self.aval.new_from_loval(self.qdd, *lovals)  # type: ignore\n+\n+  def to_tangent_aval(self):\n+    return AvalQDD(self.aval.to_tangent_aval(), self.qdd.to_tangent_qdd())\n+\n+@dataclass(frozen=True)\n+class AvalMutableQDD:\n+  aval: AbstractValue\n+  mutable_qdd: MutableQuasiDynamicData\n+\n+def cur_qdd(x):\n+  prev_trace = trace_ctx.trace\n+  trace_ctx.set_trace(eval_trace)\n+  try:\n+    return prev_trace.cur_qdd(x)\n+  finally:\n+    trace_ctx.set_trace(prev_trace)\n+\n ### Extended dtypes\n #\n # Extended dtypes are JAX-specific dtypes that allow us to represent logical\n@@ -2917,15 +2948,19 @@ def ctx_factory():\n     from jax.experimental.key_reuse._core import check_key_reuse_jaxpr  # pytype: disable=import-error\n     check_key_reuse_jaxpr(jaxpr)\n \n+# A place to track the quasi-dynamic data associated with a variable during typechecking\n+@dataclass(frozen=True)\n+class MutableTypecheckVal:\n+  aval : AbstractValue\n+  mutable_qdd : MutableQuasiDynamicData\n \n def _check_jaxpr(\n     ctx_factory: Callable[[], tuple[JaxprPpContext, JaxprPpSettings]],\n     jaxpr: Jaxpr\n   ) -> None:\n-  # Use set of variables to types to check that variables are in scope.\n-  env: set[Var] = set()\n+  env: dict[Var, Atom | MutableTypecheckVal] = {}\n \n-  def read(x: Atom) -> Atom:\n+  def read(x: Atom) -> Atom | MutableTypecheckVal:\n     # Check the type annotation is itself well-typed.\n     check_type(ctx_factory, env, x.aval)\n     if isinstance(x, Var):\n@@ -2933,7 +2968,7 @@ def read(x: Atom) -> Atom:\n       if x not in env:\n         ctx, _ = ctx_factory()\n         raise JaxprTypeError(f\"Variable '{pp_var(x, ctx)}' not defined\")\n-      return x\n+      return env[x]\n     elif isinstance(x, Literal):\n       # Check that the literal matches its type annotation.\n       if not typecheck(x.aval, x.val):\n@@ -2945,7 +2980,8 @@ def read(x: Atom) -> Atom:\n     else:\n       assert False, \"syntactically invalid jaxpr\"\n \n-  def write(v: Var, a: AbstractValue) -> None:\n+  def write(v: Var, a: AvalQDD) -> None:\n+    aval, qdd = a.aval, a.qdd\n     assert isinstance(v, Var), \"syntactically invalid jaxpr\"\n     # Check the type annotation of the binder is itself well-typed.\n     check_type(ctx_factory, env, v.aval)\n@@ -2954,19 +2990,23 @@ def write(v: Var, a: AbstractValue) -> None:\n       ctx, _ = ctx_factory()\n       raise JaxprTypeError(f\"Variable '{pp_var(v, ctx)}' already bound\")\n     # Check that the computed type is consistent with the binder annotation.\n-    if not typematch(v.aval, a):\n+    if not typematch(v.aval, aval):\n       ctx, _ = ctx_factory()\n       raise JaxprTypeError(\n           f\"Value for variable '{pp_var(v, ctx)}' inconsistently typed \"\n-          f\"as {pp_aval(a, ctx)} for let-binder of type {pp_aval(v.aval, ctx)}\")\n+          f\"as {pp_aval(aval, ctx)} for let-binder of type {pp_aval(v.aval, ctx)}\")\n+\n     # If the variable is not a DropVar, add it to the environment.\n     if not isinstance(v, DropVar):\n-      env.add(v)\n+      if qdd is None:\n+        env[v] = v\n+      else:\n+        env[v] = MutableTypecheckVal(aval, MutableQuasiDynamicData(qdd))\n \n   # Check type annotations on lambda binders.\n   for v in it.chain(jaxpr.constvars, jaxpr.invars):\n     check_type(ctx_factory, env, v.aval)\n-    write(v, v.aval)\n+    write(v, AvalQDD(v.aval, v.initial_qdd))\n \n   # Check each eqn.\n   sentinel = object()\n@@ -2976,7 +3016,8 @@ def write(v: Var, a: AbstractValue) -> None:\n     prim = eqn.primitive\n     try:\n       in_atoms = map(read, eqn.invars)\n-      in_avals = [x.aval for x in in_atoms]  # use in_atoms for dyn shapes\n+      in_avals = [AvalMutableQDD(x.aval, x.mutable_qdd) if isinstance(x, MutableTypecheckVal)\n+                  else x.aval for x in in_atoms]  # use in_atoms for dyn shapes\n \n       # Compute the type of the primitive application.\n       with eqn.ctx.manager:\n@@ -3026,6 +3067,7 @@ def write(v: Var, a: AbstractValue) -> None:\n \n       # Check out_type matches the let-binders' annotation (after substitution).\n       out_type = substitute_vars_in_output_ty(out_type, eqn.invars, eqn.outvars)\n+      out_type = [t if isinstance(t, AvalQDD) else AvalQDD(t, None) for t in out_type]\n       foreach(write, eqn.outvars, out_type)\n \n     except JaxprTypeError as e:\n@@ -3041,7 +3083,7 @@ def write(v: Var, a: AbstractValue) -> None:\n \n def check_type(\n     ctx_factory: Callable[[], tuple[JaxprPpContext, JaxprPpSettings]],\n-    env: set[Var],\n+    env: dict[Var, Atom | MutableTypecheckVal],\n     ty: AbstractValue,\n   ) -> None:\n   if isinstance(ty, DShapedArray):\n@@ -3111,7 +3153,7 @@ def _check_call(ctx_factory, prim, in_atoms, params):\n                          f\"{len(call_jaxpr.invars)} inputs\")\n \n   # Check `call_jaxpr` can be applied to in_atoms.\n-  env: dict[Var, Atom] = {}\n+  env: dict[Var, Atom | MutableTypecheckVal] = {}\n   def substitute(aval: AbstractValue):\n     if isinstance(aval, DShapedArray):\n       aval = aval.update(shape=tuple(env.get(d, d) for d in aval.shape))  # type: ignore\n@@ -3122,7 +3164,7 @@ def substitute(aval: AbstractValue):\n       raise JaxprTypeError(f\"Call primitive {prim} passes operand {x} of type \"\n                            f\"{x.aval} to jaxpr expecting type \"\n                            f\"{substitute(v.aval)}\")\n-    env[v] = x if type(x) is Var else x.val\n+    env[v] = x.val if type(x) is Literal else x\n \n   _check_jaxpr(ctx_factory, call_jaxpr)\n \ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex a77e93bb0696..69a123b12e23 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -501,6 +501,11 @@ def process_primitive(self, primitive, tracers, params):\n     else:\n       return maybe_jvp_tracer(self, primal_out, tangent_out)\n \n+  def cur_qdd(self, x):\n+    p, _ = self.to_primal_tangent_pair(x)\n+    with core.set_current_trace(self.parent_trace):\n+      return core.cur_qdd(p)\n+\n   def process_call(self, call_primitive, f, tracers, params):\n     assert call_primitive.multiple_results\n     primals, tangents = unzip2(map(self.to_primal_tangent_pair, tracers))\n@@ -629,6 +634,9 @@ def __init__(self, trace, primal, tangent):\n   def aval(self):\n     return get_aval(self.primal)\n \n+  def cur_qdd(self):\n+    return core.cur_qdd(self.primal)\n+\n   def full_lower(self):\n     if type(self.tangent) is Zero:\n       return core.full_lower(self.primal)\n@@ -1170,8 +1178,8 @@ def _jvp_jaxpr(jaxpr: core.ClosedJaxpr,\n   f_jvp, out_nonzeros = f_jvp_traceable(\n       jvp(f, instantiate=instantiate, transform_stack=False), nonzeros)\n   tangent_avals = [aval.to_tangent_aval()\n-                   for aval, nz in zip(jaxpr.in_avals_aug, nonzeros) if nz]\n-  avals_in = list(it.chain(jaxpr.in_avals_aug, tangent_avals))\n+                   for aval, nz in zip(jaxpr.in_aval_qdds, nonzeros) if nz]\n+  avals_in = list(it.chain(jaxpr.in_aval_qdds, tangent_avals))\n   jaxpr_out, avals_out, literals_out, () = pe.trace_to_jaxpr_dynamic(\n       f_jvp, avals_in)\n   return core.ClosedJaxpr(jaxpr_out, literals_out), out_nonzeros()\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 1bcd3f00321c..a4f4fcd12429 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -222,6 +222,14 @@ def instantiate_const_abstracted(self, tracer) -> JaxprTracer:\n       aval = get_aval(const).update_weak_type(np.isscalar(const))\n       return JaxprTracer(self, PartialVal.unknown(aval), ConstVar(const))\n \n+  def cur_qdd(self, x):\n+    const = self.to_jaxpr_tracer(x).pval.get_known()\n+    if const is None:\n+      assert False # TODO: track tangent QDDs\n+    else:\n+      with core.set_current_trace(self.parent_trace):\n+        return core.cur_qdd(const)\n+\n   def process_primitive(self, primitive, tracers, params):\n     with core.set_current_trace(self.parent_trace):\n       if primitive in custom_partial_eval_rules:\n@@ -1012,7 +1020,7 @@ def fun(*known_vals_in):\n     known_vals_out = [pval.get_known() for pval in out_pvals if pval.is_known()]\n     return [*known_vals_out, *residuals]\n \n-  known_avals = [a for a, uk in zip(jaxpr.in_avals_aug, in_unknowns) if not uk]\n+  known_avals = [a for a, uk in zip(jaxpr.in_aval_qdds, in_unknowns) if not uk]\n   jaxpr_known, _, consts_known, () = trace_to_jaxpr_dynamic(\n       lu.wrap_init(fun, debug_info=f.debug_info), known_avals)\n   (out_unknowns, jaxpr_unknown, res_avals), = cell  # pytype: disable=bad-unpacking\n@@ -1201,14 +1209,11 @@ def has_effects(effects) -> bool:\n   known_outvars = [*outs_known, *residuals]\n   known_effects = make_jaxpr_effects(jaxpr.constvars, ins_known_and_ref_res,\n                                      known_outvars, known_eqns)\n-  known_mut, staged_mut, ins_known_ = {}, {}, set(ins_known)  # type: ignore\n-  for v, t in jaxpr.final_typechange_env.items():\n-    [staged_mut, known_mut][v in ins_known_][v] = t\n \n   # TODO(mattjj,necula): debug info should be updated here\n   jaxpr_known = jaxpr.replace(\n       invars=ins_known_and_ref_res, outvars=known_outvars,\n-      eqns=known_eqns, effects=known_effects, final_typechange_env=known_mut)\n+      eqns=known_eqns, effects=known_effects)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_known)\n \n   _, ins_staged = partition_list(in_inst, jaxpr.invars)\n@@ -1219,7 +1224,7 @@ def has_effects(effects) -> bool:\n   # TODO(mattjj,necula): debug info should be updated here\n   jaxpr_staged = jaxpr.replace(\n       invars=staged_invars, outvars=outs_staged, eqns=staged_eqns,\n-      effects=staged_effects, final_typechange_env=staged_mut)\n+      effects=staged_effects)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_staged)\n \n   return (jaxpr_known, jaxpr_staged, out_unknowns, out_inst, len(residuals),\n@@ -1634,15 +1639,30 @@ def _move_outvars_to_back(jaxpr, to_move):\n \n \n class DynamicJaxprTracer(core.Tracer):\n-  __slots__ = ['aval', '_debug_info']\n+  __slots__ = ['aval', 'mutable_qdd', '_debug_info']\n \n   def __init__(self, trace: DynamicJaxprTrace,\n-               aval: core.AbstractValue,\n+               aval: core.AbstractValue | core.AvalQDD,\n                line_info: source_info_util.SourceInfo | None = None):\n+    if isinstance(aval, core.AvalQDD):\n+      assert aval.qdd is not None\n+      aval, qdd = aval.aval, aval.qdd\n+    else:\n+      assert not aval.has_qdd\n+      qdd = None\n     self._trace = trace\n     self._line_info = line_info\n     self._debug_info = self._trace.frame.debug_info  # for UnexpectedTracerError\n     self.aval = aval  # type: ignore[misc]\n+    self.mutable_qdd = core.MutableQuasiDynamicData(qdd)\n+\n+  @property\n+  def aval_mutable_qdd(self):\n+    aval = self.aval\n+    if aval.has_qdd:\n+      return core.AvalMutableQDD(aval, self.mutable_qdd)\n+    else:\n+      return aval\n \n   def full_lower(self):\n     var = self._trace.frame.tracer_to_var.get(id(self))\n@@ -1651,10 +1671,6 @@ def full_lower(self):\n     if val is None: return self\n     return core.full_lower(val)\n \n-  def type_state(self):\n-    var = self._trace.frame.tracer_to_var.get(id(self))\n-    return self._trace.frame.current_typechange_env[var]\n-\n   def _contents(self):\n     return ()\n \n@@ -1750,8 +1766,8 @@ class JaxprStackFrame:\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n   is_high: bool\n-  initial_typechange_env: dict\n-  current_typechange_env: dict\n+  mutable_qdds: list[tuple[Var, core.MutableQuasiDynamicData]]\n+\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1767,8 +1783,7 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_vars = []\n     self.debug_info = debug_info\n     self.is_high = False\n-    self.initial_typechange_env = {}\n-    self.current_typechange_env = {}\n+    self.mutable_qdds = []\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1794,11 +1809,13 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n-    final_typechange_env = {v: s for v, s in self.current_typechange_env.items()\n-                            if v in self.initial_typechange_env}\n+\n+    # TODO(dougalm): handle qdd for consts\n+    for v, qdd in self.mutable_qdds:\n+      v.final_qdd = qdd.cur_val\n+\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info, self.is_high, self.initial_typechange_env,\n-                  final_typechange_env)\n+                  debug_info, self.is_high)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1827,7 +1844,10 @@ def newvar(self, aval):\n                    for d in aval.shape]\n       new_shape = [d.val if isinstance(d, Literal) else d for d in new_shape]\n       aval = aval.update(shape=tuple(new_shape))\n-    return self.gensym(aval)\n+    if isinstance(aval, core.AvalQDD):\n+       return self.gensym(aval.aval, initial_qdd=aval.qdd)\n+    else:\n+       return self.gensym(aval)\n \n   def find_progenitors(self, tracer):\n     var = self.tracer_to_var.get(id(tracer))\n@@ -1883,12 +1903,13 @@ def vars(atom: Atom) -> list[Var]:\n \n \n class DynamicJaxprTrace(core.Trace):\n-  __slots__ = (\"frame\", \"tag\")\n+  __slots__ = (\"frame\", \"tag\", \"parent_trace\")\n \n-  def __init__(self, debug_info: core.DebugInfo, lower=False):\n+  def __init__(self, debug_info: core.DebugInfo, parent_trace=None, lower=False):\n     super().__init__()\n     self.requires_low = lower\n     self.frame = JaxprStackFrame(debug_info)\n+    self.parent_trace = parent_trace\n \n   def invalidate(self):\n     # avoid cyclic refs\n@@ -1915,6 +1936,7 @@ def new_arg(self, aval, source_info: SourceInfo):\n     self.frame.tracers.append(tracer)\n     self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n     self.frame.invars.append(var)\n+    self.frame.mutable_qdds.append((var, tracer.mutable_qdd))\n     return tracer\n \n   def new_const(self, c, source_info: SourceInfo):\n@@ -1922,6 +1944,9 @@ def new_const(self, c, source_info: SourceInfo):\n     tracer = self.frame.constid_to_tracer.get(id(c))\n     if tracer is None:\n       aval = get_aval(c)\n+      if aval.has_qdd:\n+        with core.set_current_trace(self.parent_trace):\n+          aval = core.AvalQDD(aval, core.cur_qdd(c))\n       if hasattr(aval, \"weak_type\"):\n         aval = aval.update_weak_type(dtypes.is_weakly_typed(c))\n       aval = self._lift_tracers_in_aval(aval, source_info)\n@@ -1938,9 +1963,9 @@ def _new_const(self, aval, c, source_info: SourceInfo) -> DynamicJaxprTracer:\n     else:\n       self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n       self.frame.constid_to_tracer[id(c)] = tracer\n+      if isinstance(aval, core.AvalQDD):\n+        self.frame.mutable_qdds.append((var, tracer.mutable_qdd))\n       self.frame.constvar_to_val[var] = c\n-      if aval.mutable:\n-        self.frame.initial_typechange_env[var] = c.type_state()\n     return tracer\n \n   def get_const(self, tracer) -> Any:\n@@ -1971,7 +1996,12 @@ def makevar(self, tracer):\n     var = self.frame.tracer_to_var[id(tracer)] = self.frame.newvar(tracer.aval)\n     return var\n \n+  def cur_qdd(self, x):\n+    source_info = source_info_util.current()\n+    return self.to_jaxpr_tracer(x, source_info=source_info).mutable_qdd.cur_val\n+\n   def process_primitive(self, primitive, tracers, params):\n+    self.frame.is_high |= primitive.is_high(**params)\n     if config.eager_constant_folding.value and not any(isinstance(x, Tracer) for x in tracers):\n       return primitive.bind_with_trace(core.eval_trace, tracers, params)\n     source_info = source_info_util.current()\n@@ -1982,8 +2012,8 @@ def process_primitive(self, primitive, tracers, params):\n     return self.default_process_primitive(primitive, jaxpr_tracers, params)\n \n   def default_process_primitive(self, primitive, tracers, params):\n-    avals = [t.aval for t in tracers]\n-    out_avals, effs = primitive.abstract_eval(*avals, **params)\n+    aval_qdds = [t.aval_mutable_qdd for t in tracers]\n+    out_avals, effs = primitive.abstract_eval(*aval_qdds, **params)\n     if isinstance(out_avals, (tuple, list)) != primitive.multiple_results:\n       raise ValueError(f\"{primitive}.abstract_eval() method should return \"\n                        f\"a tuple or a list iff {primitive}.multiple_results.\")\n@@ -2254,17 +2284,13 @@ def trace_to_jaxpr_dynamic(\n ) -> tuple[Jaxpr, list[AbstractValue], list[Any],\n            list[tuple[PyTreeDef, PyTreeDef, tuple[Any, str, AttrKind]]]]:\n   keep_inputs = [True] * len(in_avals) if keep_inputs is None else keep_inputs\n-  trace = DynamicJaxprTrace(fun.debug_info, lower=lower)\n-  in_avals_ = [a.aval if isinstance(a, core.TypeChange) else a for a in in_avals]\n+  parent_trace = core.trace_ctx.trace\n+  trace = DynamicJaxprTrace(fun.debug_info, parent_trace=parent_trace, lower=lower)\n   with core.ensure_no_leaks(trace), source_info_util.reset_name_stack():\n     source_info = source_info_util.current()\n     in_tracers = _input_type_to_tracers(\n-        partial(trace.new_arg, source_info=source_info), in_avals_)\n+        partial(trace.new_arg, source_info=source_info), in_avals)\n     in_tracers = [t for t, keep in zip(in_tracers, keep_inputs) if keep]\n-    trace.frame.initial_typechange_env = initial_typechange_env = {\n-        v: a.initial_type_state for v, a in zip(trace.frame.invars, in_avals)\n-        if isinstance(a, core.TypeChange)}\n-    trace.frame.current_typechange_env = dict(initial_typechange_env)\n \n     try:\n       with core.set_current_trace(trace):\n@@ -2329,7 +2355,8 @@ def trace_to_jaxpr_dynamic2(\n   ) -> tuple[Jaxpr, OutputType, list[Any]]:\n   assert fun.in_type is not None, \"fun must be annotated with lu.annotate()\"\n \n-  trace = DynamicJaxprTrace(fun.debug_info)\n+  parent_trace = core.trace_ctx.trace\n+  trace = DynamicJaxprTrace(fun.debug_info, parent_trace=parent_trace)\n   with core.ensure_no_leaks(trace), source_info_util.reset_name_stack():\n     source_info = source_info_util.current()\n     in_avals, keep_inputs = unzip2(fun.in_type)\n@@ -2744,33 +2771,28 @@ def _linearize_of_pmap_hack(f: lu.WrappedFun, jaxpr, consts) -> tuple[Jaxpr, lis\n \n @weakref_lru_cache\n def lower_jaxpr(hi_jaxpr):\n-  initial_env = hi_jaxpr.jaxpr.initial_typechange_env\n-  lo_avals = [lo_ty for v in hi_jaxpr.jaxpr.invars\n-              for lo_ty in (v.aval.lo_ty_(initial_env[v]) if v.aval.mutable\n-                            else v.aval.lo_ty())]\n+  lo_avals = [lo_ty for aval in hi_jaxpr.in_aval_qdds for lo_ty in aval.lo_ty()]\n   f = lu.wrap_init(partial(lower_traceable, hi_jaxpr),\n                    debug_info=hi_jaxpr.jaxpr.debug_info)\n   lo_jaxpr, _, lo_consts, () = trace_to_jaxpr_dynamic(f, lo_avals, lower=True)\n   return core.ClosedJaxpr(lo_jaxpr, lo_consts)\n \n def lower_traceable(jaxpr, *lo_args):\n-  env = jaxpr.jaxpr.initial_typechange_env\n   lo_args_ = iter(lo_args)\n-  hi_args = [v.aval.raise_val(*it.islice(lo_args_, len(v.aval.lo_ty())))\n-             if not v.aval.mutable else\n-             v.aval.new_from_loval(env[v], *it.islice(lo_args_, len(v.aval.lo_ty_(env[v]))))\n-             for v in jaxpr.jaxpr.invars]\n+  hi_args = [aval.raise_val(*it.islice(lo_args_, len(aval.lo_ty())))\n+             if not aval.has_qdd else\n+             aval.new_from_loval(*it.islice(lo_args_, len(aval.lo_ty())))\n+             for aval in jaxpr.in_aval_qdds]\n   assert (problem := next(lo_args_, None)) is None\n   hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n-  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.final_typechange_env.items()\n-              for lo_val in v.aval.read_loval(ty, hi_args[in_idx[v]])]\n+  mut_outs = [lo_val for aval, hi_arg in zip(jaxpr.final_aval_qdds, hi_args) if aval.has_qdd\n+              for lo_val in aval.read_loval(hi_arg)]\n   lo_outs = [lo_val for v, hi_val in zip(jaxpr.jaxpr.outvars, hi_outs)\n              for lo_val in v.aval.lower_val(hi_val)]\n   return mut_outs + lo_outs\n \n def convert_const_himutables(jaxpr):\n-  move = [core.typeof(c).mutable for c in jaxpr.consts]\n+  move = [core.typeof(c).has_qdd for c in jaxpr.consts]\n   constvals, in_mutables = partition_list(move, jaxpr.consts)\n   constvars, boxvars = partition_list(move, jaxpr.jaxpr.constvars)\n   invars = *boxvars, *jaxpr.jaxpr.invars\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex ad09292731cf..3fefab78f7f7 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -1512,17 +1512,6 @@ def arrange_jaxpr_args_for_wrapped(args):\n   assert len(refs_out_matching_in_avals) == len(in_avals)\n   return refs_out_matching_in_avals, [*carry_out, *ys]\n \n-def _scan_staging(trace, *args, **params):\n-  outs = trace.default_process_primitive(scan_p, args, params)\n-  jaxpr = params['jaxpr']\n-  trace.frame.is_high = jaxpr.jaxpr.is_high\n-  invars = [trace.frame.tracer_to_var[id(t)] for t in args]\n-  var_map = dict(zip(jaxpr.jaxpr.invars, invars))\n-  final_env = {var_map[v]: ty for v, ty in\n-               jaxpr.jaxpr.final_typechange_env.items()}\n-  trace.frame.current_typechange_env.update(final_env)\n-  return outs\n-\n scan_p = core.Primitive(\"scan\")\n scan_p.multiple_results = True\n scan_p.skip_canonicalization = True\n@@ -1541,37 +1530,32 @@ def _scan_staging(trace, *args, **params):\n pe.padding_rules[scan_p] = _scan_padding_rule\n pe.dce_rules[scan_p] = _scan_dce_rule\n state_discharge.register_partial_discharge_rule(scan_p)(_scan_state_partial_discharge_rule)\n-pe.custom_staging_rules[scan_p] = _scan_staging\n \n def _is_high(jaxpr, **_) -> bool:\n   return jaxpr.jaxpr.is_high\n scan_p.is_high = _is_high  # type: ignore\n \n def _to_lojax(*hi_args, jaxpr, num_carry, num_consts, linear, **params):\n-  ienv, fenv = jaxpr.jaxpr.initial_typechange_env, jaxpr.jaxpr.final_typechange_env\n \n   # move box binders and hi_args from consts slots to carry slots\n-  to_move = [t.mutable for t in jaxpr.in_avals[:num_consts]]\n+  to_move = [t.has_qdd for t in jaxpr.in_aval_qdds[:num_consts]]\n   jaxpr = pe.move_invars_right(jaxpr, to_move)\n   hi_args = _move_right(hi_args, to_move)\n   num_consts -= sum(to_move)\n   num_carry += sum(to_move)\n \n   # expand num_consts, num_carry, linear according to lo types\n-  const_invars, carry_invars, _ = split_list(jaxpr.jaxpr.invars, [num_consts, num_carry])\n-  num_consts = sum(len(v.aval.lo_ty() if not v.aval.mutable\n-                       else v.aval.lo_ty_(ienv[v])) for v in const_invars)\n-  num_carry = sum(len(v.aval.lo_ty() if not v.aval.mutable\n-                      else v.aval.lo_ty_(ienv[v])) for v in carry_invars)\n-  linear = [l for v, l_ in zip(jaxpr.jaxpr.invars, linear)\n-            for l in (l_,) * len(v.aval.lo_ty() if not v.aval.mutable\n-                                 else v.aval.lo_ty_(ienv[v]))]\n-  lo_muts_out = sum(len(m.leaf_avals) for m in fenv.values())  # TODO hardcoded\n-\n-  # collect lo inputs values\n-  lo_args = [lo_val for v, x in zip(jaxpr.jaxpr.invars, hi_args)\n-             for lo_val in (v.aval.read_loval(ienv[v], x) if v.aval.mutable\n-                            else v.aval.lower_val(x))]\n+  const_in_avals, carry_in_avals, _ = split_list(jaxpr.in_aval_qdds, [num_consts, num_carry])\n+  num_consts = sum(len(aval.lo_ty()) for aval in const_in_avals)\n+  num_carry = sum(len(aval.lo_ty()) for aval in carry_in_avals)\n+  linear = [l for aval, l_ in zip(jaxpr.in_aval_qdds, linear)\n+            for l in (l_,) * len(aval.lo_ty())]\n+  lo_muts_out = sum(len(aval.lo_ty()) for aval in jaxpr.final_aval_qdds if aval.has_qdd)\n+\n+  # collect lo input values\n+  lo_args = [lo_val for aval, x in zip(jaxpr.in_aval_qdds, hi_args)\n+             for lo_val in (aval.read_loval(x) if aval.has_qdd\n+                            else aval.lower_val(x))]\n \n   # lower the jaxpr and bind it using lo input values\n   lo_jaxpr = pe.lower_jaxpr(jaxpr)\n@@ -1582,9 +1566,13 @@ def _to_lojax(*hi_args, jaxpr, num_carry, num_consts, linear, **params):\n   # collect and apply mutations\n   out_mut_ = iter(out_mut)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n-    lo_vals = it.islice(out_mut_, len(var.aval.lo_ty_(ty)))\n-    var.aval.update_from_loval(ty, hi_args[in_idx[var]], *lo_vals)\n+\n+  for v in jaxpr.jaxpr.invars:\n+    if v.final_qdd is not None:\n+      qdd = v.final_qdd\n+      lo_vals = it.islice(out_mut_, len(v.aval.lo_ty_qdd(qdd)))\n+      v.aval.update_from_loval(qdd, hi_args[in_idx[v]], *lo_vals)\n+\n   assert next(out_mut_, None) is None\n \n   # collect output values into hi types\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 52e876400ed9..deb1d92f853b 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -592,7 +592,7 @@ def _infer_params_impl(\n     in_type = in_avals = tuple(core.shaped_abstractify(x) for x in explicit_args)  # type: ignore\n   else:\n     in_type = in_avals  # type: ignore\n-    in_type = tuple(core.TypeChange(a, x.type_state(), None) if a.mutable  # type: ignore\n+    in_type = tuple(core.AvalQDD(a, core.cur_qdd(x)) if a.has_qdd  # type: ignore\n                     else a for a, x in zip(in_type, explicit_args))\n   assert in_avals is not None\n \n@@ -1418,7 +1418,7 @@ def _create_pjit_jaxpr(\n     from jax.experimental.key_reuse._core import check_key_reuse_jaxpr  # pytype: disable=import-error\n     check_key_reuse_jaxpr(jaxpr)\n \n-  if any(isinstance(c, core.Tracer) or core.typeof(c).mutable for c in consts):\n+  if any(isinstance(c, core.Tracer) or core.typeof(c).has_qdd for c in consts):\n     closed_jaxpr = pe.close_jaxpr(pe.convert_constvars_jaxpr(jaxpr))\n     final_consts = consts\n   else:\n@@ -1564,25 +1564,22 @@ def _is_high(jaxpr, **_) -> bool:\n pjit_p.is_high = _is_high  # type: ignore\n \n def _to_lojax(*hi_args, jaxpr, **params):\n-  ienv, fenv = jaxpr.jaxpr.initial_typechange_env, jaxpr.jaxpr.final_typechange_env\n-\n   # convert closed-over boxes to explicit args\n   jaxpr, closed_over_himutables = pe.convert_const_himutables(jaxpr)\n   hi_args = [*closed_over_himutables, *hi_args]\n   params = _converted_mutables_add_params(len(closed_over_himutables), **params)\n \n+\n   # expand pjit params that must match number of lo inputs/outputs\n-  lo_nums_in = [len(v.aval.lo_ty() if not v.aval.mutable\n-                    else v.aval.lo_ty_(ienv[v]))\n-                for v in jaxpr.jaxpr.invars]\n+  lo_nums_in = [len(aval.lo_ty()) for aval in jaxpr.in_aval_qdds]\n   lo_nums_out = [len(t.lo_ty()) for t in jaxpr.out_avals]\n-  lo_muts_out = sum(len(m.leaf_avals) for m in fenv.values())  # TODO hardcoded\n+  lo_muts_out = sum(len(aval.lo_ty()) for aval in jaxpr.final_aval_qdds if aval.has_qdd)\n   params = _lojax_expand_params(lo_nums_in, lo_nums_out, lo_muts_out, **params)\n \n   # collect lo input values\n-  lo_args = [lo_val for v, x in zip(jaxpr.jaxpr.invars, hi_args)\n-             for lo_val in (v.aval.read_loval(ienv[v], x) if v.aval.mutable\n-                            else v.aval.lower_val(x))]\n+  lo_args = [lo_val for aval, x in zip(jaxpr.in_aval_qdds, hi_args)\n+             for lo_val in (aval.read_loval(x) if aval.has_qdd\n+                            else aval.lower_val(x))]\n \n   # lower the jaxpr and bind it using lo input values\n   lo_jaxpr = pe.lower_jaxpr(jaxpr)\n@@ -1592,9 +1589,11 @@ def _to_lojax(*hi_args, jaxpr, **params):\n   # collect and apply mutations\n   out_mut_ = iter(out_mut)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n-    lo_vals = it.islice(out_mut_, len(var.aval.lo_ty_(ty)))\n-    var.aval.update_from_loval(ty, hi_args[in_idx[var]], *lo_vals)\n+  for v in jaxpr.jaxpr.invars:\n+    if v.final_qdd is not None:\n+      qdd = v.final_qdd\n+      lo_vals = it.islice(out_mut_, len(v.aval.lo_ty_qdd(qdd)))\n+      v.aval.update_from_loval(qdd, hi_args[in_idx[v]], *lo_vals)\n   assert next(out_mut_, None) is None\n \n   # collect output values into hi types\n@@ -1614,6 +1613,7 @@ def _converted_mutables_add_params(\n   return dict(params, donated_invars=donated_invars, in_shardings=in_shardings,\n               in_layouts=in_layouts)\n \n+\n def _lojax_expand_params(\n     nums_in, nums_out, muts_out, *, donated_invars, in_shardings, in_layouts,\n     out_shardings, out_layouts, **params):\n@@ -2016,13 +2016,6 @@ def pjit_staging_rule(trace, *args, **params):\n   else:\n     out_tracers = trace.default_process_primitive(pjit_p, args, params)\n \n-  trace.frame.is_high = jaxpr.jaxpr.is_high\n-  invars = [trace.frame.tracer_to_var[id(t)] for t in it.chain(args, consts)]\n-  var_map = dict(zip(jaxpr.jaxpr.invars, invars))\n-  final_env = {var_map[v]: ty for v, ty in\n-               jaxpr.jaxpr.final_typechange_env.items()}\n-  trace.frame.current_typechange_env.update(final_env)\n-\n   return out_tracers\n pe.custom_staging_rules[pjit_p] = pjit_staging_rule\n \ndiff --git a/tests/hijax_test.py b/tests/hijax_test.py\nindex 8b7d045c6c5a..b272e0aa8986 100644\n--- a/tests/hijax_test.py\n+++ b/tests/hijax_test.py\n@@ -213,7 +213,7 @@ def new_box():\n   return new_box_p.bind(treedef=treedef)\n \n def box_get(box):\n-  tys = box.type_state()\n+  tys = core.cur_qdd(box)\n   leaf_vals = box_get_p.bind(box, avals=tys.leaf_avals)\n   return jax.tree.unflatten(tys.treedef, leaf_vals)\n \n@@ -222,11 +222,11 @@ def box_set(box, val):\n   box_set_p.bind(box, *leaves, treedef=treedef)\n \n @dataclass(frozen=True)\n-class BoxTypeState:\n+class BoxTypeState(core.QuasiDynamicData):\n   leaf_avals: tuple[core.AbstractValue, ...]\n   treedef: PyTreeDef\n \n-  def to_tangent_aval(self):\n+  def to_tangent_qdd(self):\n     return BoxTypeState(tuple(a.to_tangent_aval() for a in self.leaf_avals),\n                         self.treedef)\n \n@@ -235,11 +235,12 @@ def normalize(self):\n                         self.treedef)\n \n class BoxTy(core.AbstractValue):\n-  mutable = True\n+  has_qdd = True\n \n   # forwarded to value\n   get = core.aval_method(box_get)\n   set = core.aval_method(box_set)\n+  type_state = core.aval_method(core.cur_qdd)\n \n   # aval interface: hashability and str_short\n   def __hash__(self): return hash(BoxTy)\n@@ -249,7 +250,7 @@ def str_short(self, short_dtypes=False):\n     return 'BoxTy'\n \n   # mutable interface\n-  def lo_ty_(self, box_state):\n+  def lo_ty_qdd(self, box_state):\n     return [lo_ty for t in box_state.leaf_avals for lo_ty in t.lo_ty()]\n \n   def new_from_loval(self, box_state: BoxTypeState, *lo_vals):\n@@ -285,6 +286,9 @@ def get(self):\n   def set(self, val):\n     box_set(self, val)\n \n+  def cur_qdd(self):\n+    return self.type_state()\n+\n   @property\n   def ty(self):\n     return BoxTy()\n@@ -299,15 +303,10 @@ def type_state(self):\n class NewBox(HiPrimitive):\n   def is_high(self, *, treedef) -> bool: return True\n \n-  def staging(self, trace, *, treedef):\n-    tracer = super().staging(trace, treedef=treedef)\n-    var = trace.frame.tracer_to_var[id(tracer)]\n-    leaves, treedef = jax.tree.flatten(None)\n-    trace.frame.current_typechange_env[var] = BoxTypeState(leaves, treedef)\n-    return tracer\n-\n   def abstract_eval(self, *, treedef):\n-    return BoxTy(), set()\n+    leaves, treedef = jax.tree.flatten(None)\n+    qdd = BoxTypeState(leaves, treedef)\n+    return core.AvalQDD(BoxTy(), qdd), set()\n \n   def to_lojax(_, *, treedef):\n     return Box(None)\n@@ -325,14 +324,8 @@ class BoxSet(HiPrimitive):\n \n   def is_high(self, *, treedef) -> bool: return True\n \n-  def staging(self, trace, box_tracer, *leaves, treedef):\n-    super().staging(trace, box_tracer, *leaves, treedef=treedef)\n-    var = trace.getvar(box_tracer)\n-    avals = tuple(t.aval for t in leaves)\n-    trace.frame.current_typechange_env[var] = BoxTypeState(avals, treedef)\n-    return []\n-\n   def abstract_eval(self, box_ty, *leaf_avals, treedef):\n+    box_ty.mutable_qdd.update(BoxTypeState(leaf_avals, treedef))\n     return [], set()  # TODO better typechecking...\n \n   def to_lojax(_, box, *leaves, treedef):\n@@ -375,6 +368,36 @@ def transpose(_, *args):\n \n class BoxTest(jtu.JaxTestCase):\n \n+  @parameterized.parameters([False, True])\n+  def test_qdd(self, jit):\n+\n+    val1 = 1.0\n+    val2 = jnp.arange(3)\n+\n+    box1 = Box(val1)\n+\n+    def f(box2):\n+      assert core.cur_qdd(box2).leaf_avals == (core.typeof(val1),)\n+      box2.set(val2)\n+      assert core.cur_qdd(box2).leaf_avals == (core.typeof(val2),)\n+\n+      box3 = new_box()\n+      box3.set(val2)\n+      assert core.cur_qdd(box3).leaf_avals == (core.typeof(val2),)\n+      box3.set(val1)\n+      assert core.cur_qdd(box3).leaf_avals == (core.typeof(val1),)\n+\n+      assert core.cur_qdd(box1).leaf_avals == (core.typeof(val1),)\n+      box1.set(val2)\n+      assert core.cur_qdd(box1).leaf_avals == (core.typeof(val2),)\n+\n+      return\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    f(Box(val1))\n+\n   def test_jit_arg(self):\n     @jax.jit\n     def f(box, x):\n@@ -470,6 +493,24 @@ def k(x):\n     ans = h(2.0)\n     self.assertAllClose(ans, 4.0)\n \n+  def test_jit_closure_nested3(self):\n+    box = new_box()\n+\n+    @jax.jit\n+    def h(x):\n+      box.set(x)\n+\n+      @jax.jit\n+      def k(x):\n+        box.set(box.get() + x)\n+\n+      k(1.0)\n+      k(1.0)\n+      return box.get()\n+\n+    ans = h(2.0)\n+    self.assertAllClose(ans, 4.0)\n+\n   @parameterized.parameters([False, True])\n   def test_jvp_closure_stop_gradient(self, jit):\n     box = Box(1.0)\n@@ -528,6 +569,7 @@ def f(x):\n       f = jax.jit(f)\n \n     jax.grad(f)(1.0)\n+\n     self.assertAllClose(box.get(), 2.0)\n \n   # TODO(mattjj,dougalm): make this work...\n@@ -587,6 +629,7 @@ def body(_, __):\n       double_it_10 = jax.jit(double_it_10)\n \n     double_it_10()\n+\n     self.assertAllClose(box.get(), 1024., check_dtypes=False)\n \n   # TODO error-checking tests from attrs_test.py\n@@ -674,7 +717,7 @@ class MyArray:\n \n     @dataclass(frozen=True)\n     class MyTy(core.AbstractValue):\n-      mutable = False\n+      has_qdd = False\n \n       def to_tangent_aval(self):\n         return MyTy()\n@@ -711,7 +754,7 @@ def f(box):\n \n \n class ListTy(core.AbstractValue):\n-  mutable = True\n+  has_qdd = True\n \n   # forwarded to value\n   get = core.aval_method(box_get)\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Rollback] Roll-forward with fix and test: prototype of cross-host device transfers for TFRT TPU.\n\nReverts 8d8cc2bca67fc75718b73337c9ce19d6b77065e9\n\nPiperOrigin-RevId: 767348122",
        "output": "```diff\nCommit: 9f7e802980fdaed415f0513e1f0ab331cb646b91\nDate: 2025-06-05T00:21:18Z\nURL: https://github.com/jax-ml/jax/commit/9f7e802980fdaed415f0513e1f0ab331cb646b91\nFiles changed: 2\nAdditions: +50, Deletions: -13\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex b5e588cbc10e..b9ef8f49f801 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -356,16 +356,6 @@ def _different_device_order_reshard(x, target_sharding, copy: CopySemantics):\n     return api.jit(_identity_fn, out_shardings=target_sharding,\n                    donate_argnums=donate_argnums)(x)\n \n-  if inp_sharding.device_set != target_sharding.device_set:\n-    inp_ids = [d.id for d in inp_sharding._device_assignment]\n-    inp_plat = inp_sharding._device_assignment[0].platform.upper()\n-    target_ids = [d.id for d in target_sharding._device_assignment]\n-    target_plat = target_sharding._device_assignment[0].platform.upper()\n-    raise ValueError(\"Input and target sharding should have the same set of \"\n-                     f\"devices. Got input's device set ids: {inp_ids} on \"\n-                     f\"platform {inp_plat} and target sharding's device set \"\n-                     f\"ids: {target_ids} on platform {target_plat}\")\n-\n   if inp_sharding.is_fully_replicated:\n     permute_order = None\n   else:\n@@ -389,6 +379,25 @@ def _reorder_shards(x, new_s, copy_semantics: CopySemantics):\n   return xc.reorder_shards(x, new_s, xc_copy_semantics)  # type: ignore\n \n \n+@util.cache()\n+def _is_supported_cross_host_transfer(ndim, src_sharding, dst_sharding):\n+  \"\"\"Returns True if src->dst is a supported cross-host transfer.\"\"\"\n+  backend = xla_bridge.get_backend()\n+  # There is experimental support for cross-host device transfers on TFRT TPU\n+  # backends only.\n+  if (xla_bridge.process_count() == 1 or backend.platform != \"tpu\" or\n+      \"TFRT TPU\" not in backend.platform_version):\n+    return False\n+  if (src_sharding._to_xla_hlo_sharding(ndim) !=\n+      dst_sharding._to_xla_hlo_sharding(ndim)):\n+    return False\n+  # This check excludes the case where the source and destination shardings\n+  # have the same process index sets but there are shards that require\n+  # cross-host transfers. This case is supportable but expensive to check for.\n+  return (src_sharding._internal_device_list.process_indices !=\n+          dst_sharding._internal_device_list.process_indices)\n+\n+\n @dataclasses.dataclass(frozen=True)\n class _DeferredShardArg:\n   \"\"\"Deferred call to `pxla.shard_args`.\n@@ -419,7 +428,8 @@ def _device_put_sharding_impl(x, aval, device, copy):\n       return x\n \n     if (not s.is_fully_addressable and\n-        isinstance(x, array.ArrayImpl) and not x.is_fully_addressable):\n+        isinstance(x, array.ArrayImpl) and not x.is_fully_addressable and\n+        s.device_set == x.sharding.device_set):\n       assert isinstance(s, Sharding)\n       return _different_device_order_reshard(x, s, copy)\n \n@@ -430,7 +440,32 @@ def _device_put_sharding_impl(x, aval, device, copy):\n       assert isinstance(s, Sharding)\n       return _different_device_order_reshard(x, s, copy)\n \n+    # There is experimental support for cross-host device transfers on TFRT TPU.\n+    if (isinstance(x, array.ArrayImpl) and x._committed\n+        and _is_supported_cross_host_transfer(x.ndim, x.sharding, s)):\n+      return xc.batched_copy_array_to_devices_with_sharding(\n+          [x], [s._internal_device_list], [s],  # pytype: disable=attribute-error\n+          pxla.to_xc_copy_semantics([copy]))[0]\n+\n     if not s.is_fully_addressable:\n+      # If both the source and target shardings are not fully addressable and\n+      # one of the above conditions has not been met, then assume that the user\n+      # is attempting a different device order reshard.\n+      if (isinstance(x, array.ArrayImpl) and not x.is_fully_addressable\n+          and s.device_set != x.sharding.device_set):\n+        inp_ids = [d.id for d in x.sharding._device_assignment]\n+        inp_plat = x.sharding._device_assignment[0].platform.upper()\n+        target_ids = [d.id for d in s._device_assignment]\n+        target_plat = s._device_assignment[0].platform.upper()\n+        raise ValueError(\n+            \"For a cross-host reshard in multi-controller JAX, input and target\"\n+            \" sharding should have the same set of devices. Got input's device\"\n+            f\" set ids: {inp_ids} on platform {inp_plat} and target sharding's\"\n+            f\" device set ids: {target_ids} on platform {target_plat}.\\n\\n\"\n+            \"There is experimental support for cross-host transfers with \"\n+            \"different device sets, when input/output shardings have the same \"\n+            \"indices and layouts, in the TFRT TPU runtime only.\")\n+\n       if ((isinstance(x, array.ArrayImpl) and not x._committed) or\n           type(x) in array_types or type(x) in dtypes.python_scalar_dtypes):\n         # If all hosts participate in the sharding, assert that the input is the\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 572c2225af74..52e876400ed9 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -414,8 +414,10 @@ def _parse_jit_arguments(fun: Callable, *, in_shardings: Any,\n   if backend is not None or device is not None:\n     warnings.warn(\n         'backend and device argument on jit is deprecated. You can use'\n-        ' `jax.device_put(..., jax.local_devices(\"cpu\")[0])` on the inputs to'\n-        ' the jitted function to get the same behavior.', DeprecationWarning)\n+        ' `jax.device_put(..., jax.local_devices(backend=\"cpu\")[0])` on the'\n+        ' inputs to the jitted function to get the same behavior.',\n+        DeprecationWarning,\n+    )\n     if device is not None and backend is not None:\n       raise ValueError(\"can't specify both a device and a backend for jit, \"\n                        f\"got {device=} and {backend=}\")\n\n```",
        "from_id": [
            "emilyfertig",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas Fuser] Add basic reshape push rule\n\nPiperOrigin-RevId: 767341856",
        "output": "```diff\nCommit: 1ccc387528d69bcdbd8152a87771376566744375\nDate: 2025-06-05T00:02:17Z\nURL: https://github.com/jax-ml/jax/commit/1ccc387528d69bcdbd8152a87771376566744375\nFiles changed: 2\nAdditions: +197, Deletions: -32\ndiff --git a/jax/_src/pallas/fuser/block_spec.py b/jax/_src/pallas/fuser/block_spec.py\nindex 4f9d1c344429..56f75699735c 100644\n--- a/jax/_src/pallas/fuser/block_spec.py\n+++ b/jax/_src/pallas/fuser/block_spec.py\n@@ -814,13 +814,16 @@ def register_default_eval_rule(prim: core.Primitive):\n   def default_rule(ctx, *args, **params):\n     assert all(bs is pallas_core.no_block_spec for bs in ctx.out_block_specs)\n     return prim.bind(*args, **params)\n+\n   register_eval_rule(prim)(default_rule)\n \n+\n def register_binop_rule(prim: core.Primitive):\n   register_pull_block_spec_rule(prim)(functools.partial(_binop_pull_rule, prim))\n   register_usage_rule(prim)(functools.partial(_binop_usage_rule, prim))\n   register_eval_rule(prim)(functools.partial(_binop_eval_rule, prim))\n \n+\n register_default_eval_rule(state_primitives.get_p)\n \n register_binop_rule(lax.mul_p)\n@@ -1074,6 +1077,7 @@ def new_index_map(*args):\n       len(ctx.avals_in) - 1\n   )\n \n+\n @register_pull_block_spec_rule(state_primitives.swap_p)\n def _swap_pull_rule(\n     ctx: PullRuleContext,\n@@ -1084,14 +1088,9 @@ def _swap_pull_rule(\n   # The output and val block spec are the same.\n   return [block_spec, block_spec]\n \n+\n @register_eval_rule(state_primitives.swap_p)\n-def _swap_eval_rule(\n-    ctx: KernelEvalContext,\n-    ref,\n-    val,\n-    *idx,\n-    tree\n-):\n+def _swap_eval_rule(ctx: KernelEvalContext, ref, val, *idx, tree):\n   indexers = tree_util.tree_unflatten(tree, idx)\n   ref_aval, _ = ctx.avals_in[:2]\n   indexers_avals = tree_util.tree_unflatten(tree, ctx.avals_in[2:])\n@@ -1123,17 +1122,15 @@ def _slice(i, b):\n     return i if b is None else indexing.ds(i * b, b)\n \n   indexer = tuple(\n-      _slice(i, b) for i, b in zip(block_idx, block_spec.block_shape,\n-                                   strict=True)\n+      _slice(i, b)\n+      for i, b in zip(block_idx, block_spec.block_shape, strict=True)\n   )\n   return ref.swap(val, idx=indexer)\n \n+\n @register_pull_block_spec_rule(state_primitives.get_p)\n def _get_pull_rule(\n-    ctx: PullRuleContext,\n-    block_spec: pallas_core.BlockSpec,\n-    *,\n-    tree\n+    ctx: PullRuleContext, block_spec: pallas_core.BlockSpec, *, tree\n ):\n   ref_aval = ctx.avals_in[0]\n   assert hasattr(ref_aval, 'shape')\n@@ -1166,6 +1163,7 @@ def _get_pull_rule(\n     bd = next(block_shape_iter)\n     block_shape.append(_block_size(bd))\n   assert next(block_shape_iter, None) is None\n+\n   def new_index_map(*args):\n     idx = block_spec.index_map(*args)\n     idx_iter = iter(idx)\n@@ -1177,16 +1175,13 @@ def new_index_map(*args):\n     )\n     assert next(idx_iter, None) is None\n     return indices\n+\n   block_spec = pallas_core.BlockSpec(block_shape, new_index_map)\n   return [block_spec] + [pallas_core.no_block_spec] * (len(ctx.avals_in) - 1)\n \n+\n @register_eval_rule(state_primitives.get_p)\n-def _get_eval_rule(\n-    ctx: KernelEvalContext,\n-    ref,\n-    *idx,\n-    tree\n-):\n+def _get_eval_rule(ctx: KernelEvalContext, ref, *idx, tree):\n   indexers = tree_util.tree_unflatten(tree, idx)\n   ref_aval = ctx.avals_in[0]\n   indexers_avals = tree_util.tree_unflatten(tree, ctx.avals_in[1:])\n@@ -1240,6 +1235,7 @@ def _slice(i, b):\n   assert next(block_idx_iter, None) is None\n   return ref.get(idx=tuple(block_indexer))\n \n+\n @register_eval_rule(lax.concatenate_p)\n def _concatenate_eval_rule(ctx: KernelEvalContext, *args, dimension):\n   # We now handle the case where each of the concatenated array dimensions\n@@ -1525,17 +1521,16 @@ def _random_bits_pull_rule(\n   )\n   return [key_block_spec]\n \n+\n @register_eval_rule(prng.random_wrap_p)\n def _random_wrap_eval_rule(eval_ctx: KernelEvalContext, arr, *, impl):\n   del eval_ctx\n   return jax.random.wrap_key_data(arr, impl=impl)\n \n+\n @register_pull_block_spec_rule(prng.random_wrap_p)\n def _random_wrap_pull_rule(\n-    ctx: PullRuleContext,\n-    block_spec: pallas_core.BlockSpec,\n-    *,\n-    impl\n+    ctx: PullRuleContext, block_spec: pallas_core.BlockSpec, *, impl\n ):\n   del ctx, block_spec, impl\n   return [pallas_core.BlockSpec(block_shape=None)]\n@@ -1578,13 +1573,32 @@ def _pattern_match_sublanes_to_lanes_reshape(\n     aval_out: core.ShapedArray,\n ) -> bool:\n   # Pattern matches a reshape of the form (..., n/l, l) -> (..., n * l)\n-  # where l is a multiple of 128 n/l is a multiple of packing.\n+  # where l is a multiple of 128.\n \n   *leading_in, second_to_last_dim, last_dim = aval_in.shape\n   *leading_out, last_dim_out = aval_out.shape\n   if leading_in != leading_out:\n     return False\n-  assert last_dim_out == second_to_last_dim * last_dim\n+  if second_to_last_dim * last_dim != last_dim_out:\n+    return False\n+  if last_dim % 128 != 0:\n+    return False\n+  return True\n+\n+\n+def _pattern_match_lanes_to_sublanes_reshape(\n+    aval_in: core.ShapedArray,\n+    aval_out: core.ShapedArray,\n+) -> bool:\n+  # Pattern matches a reshape of the form (..., n * l) -> (..., n, l)\n+  # where l is a multiple of 128.\n+\n+  *leading_out, last_dim_in = aval_in.shape\n+  *leading_in, second_to_last_dim_out, last_dim = aval_out.shape\n+  if leading_in != leading_out:\n+    return False\n+  if second_to_last_dim_out * last_dim != last_dim_in:\n+    return False\n   if last_dim % 128 != 0:\n     return False\n   return True\n@@ -1606,6 +1620,8 @@ def _reshape_pull_rule(\n   assert isinstance(aval_in, core.ShapedArray)\n   aval_out = ctx.avals_out[0]\n   assert isinstance(aval_out, core.ShapedArray)\n+\n+  # Handle the case where we reshape from (..., n/l, l) -> (..., n * l)\n   if _pattern_match_sublanes_to_lanes_reshape(aval_in, aval_out):\n     block_shape = tuple(block_spec.block_shape)\n     if not isinstance(block_shape[-1], (int, pallas_core.Blocked)):\n@@ -1625,6 +1641,44 @@ def new_index_map(*args):\n       return *idx, 0\n \n     return [pallas_core.BlockSpec(new_block_shape, new_index_map)]\n+\n+  # Handle the case where we reshape from (..., n * l) -> (..., n, l)\n+  if _pattern_match_lanes_to_sublanes_reshape(aval_in, aval_out):\n+    block_shape = tuple(block_spec.block_shape)\n+    if not isinstance(block_shape[-1], (int, pallas_core.Blocked)):\n+      raise NotImplementedError(\n+          f'reshape must use Blocked block size on lanes: {block_shape}'\n+      )\n+    if not isinstance(block_shape[-2], (int, pallas_core.Blocked)):\n+      raise NotImplementedError(\n+          f'reshape must use Blocked block size on sublanes: {block_shape}'\n+      )\n+    last_dim = aval_out.shape[-1]\n+    block_sublane_dim, block_lane_dim = (\n+        _block_size(block_shape[-2]),\n+        _block_size(block_shape[-1]),\n+    )\n+    total_block_size = block_sublane_dim * block_lane_dim\n+    if total_block_size % 128 != 0:\n+      raise NotImplementedError(\n+          'reshape with non-128 aligned block size on lanes not supported yet'\n+      )\n+    if block_lane_dim != last_dim:\n+      raise NotImplementedError(\n+          'reshape with non-matching block size on lanes not supported yet:'\n+          f' {block_shape}'\n+      )\n+    new_block_shape = block_shape[:-2] + (total_block_size,)\n+    def new_index_map(*args):  # pylint: disable=function-redefined\n+      *idx, second_to_last, last = block_spec.index_map(*args)\n+      # last should always be 0\n+      if not isinstance(last, int) and last != 0:\n+        raise NotImplementedError(\n+            'Must select entire block on last dimension for reshape'\n+        )\n+      return *idx, second_to_last\n+    return [pallas_core.BlockSpec(new_block_shape, new_index_map)]\n+\n   raise NotImplementedError(f'reshape not supported yet: {aval_in}, {aval_out}')\n \n \n@@ -1639,13 +1693,8 @@ def _reshape_eval_rule(\n   out_shape = tuple(s for s in out_shape_nones if s is not None)\n   # Because we have restricted the pull block spec rule, we can just apply a\n   # basic reshape here.\n-  orig_dtype = x.dtype\n-  if jnp.issubdtype(orig_dtype, jnp.integer):\n-    x = x.astype(jnp.int32)\n-  elif jnp.issubdtype(orig_dtype, jnp.floating):\n-    x = x.astype(jnp.float32)\n   x = x.reshape(out_shape)\n-  return x.astype(orig_dtype)\n+  return x\n \n \n # Higher order primitives\n@@ -1667,8 +1716,10 @@ def _jit_eval_rule(ctx: KernelEvalContext, *args, jaxpr, **kwargs):\n     raise NotImplementedError('pjit with consts not supported yet')\n   out_tree = tree_util.tree_structure(tuple(jaxpr.outvars))\n   in_tree = tree_util.tree_structure((tuple(jaxpr.invars), {}))\n+\n   def read_usage_env(_: core.Var):\n     return {Usage.REGULAR}\n+\n   _, env, _ = _pull_block_spec(\n       jaxpr,\n       ctx.out_block_specs,\n@@ -1697,8 +1748,10 @@ def _jit_pull_block_spec_rule(\n   jaxpr, consts = jaxpr.jaxpr, jaxpr.consts\n   if consts:\n     raise NotImplementedError('pjit with consts not supported yet')\n+\n   def read_usage_env(_: core.Var):\n     return {Usage.REGULAR}\n+\n   in_block_specs, _, _ = _pull_block_spec(\n       jaxpr,\n       out_block_specs,\n@@ -1728,8 +1781,10 @@ def _custom_jvp_call_eval_rule(\n     raise NotImplementedError('custom_jvp_call with consts not supported yet')\n   out_tree = tree_util.tree_structure(tuple(jaxpr.outvars))\n   in_tree = tree_util.tree_structure((tuple(jaxpr.invars), {}))\n+\n   def read_usage_env(_: core.Var):\n     return {Usage.REGULAR}\n+\n   _, env, _ = _pull_block_spec(\n       jaxpr,\n       ctx.out_block_specs,\n@@ -1758,8 +1813,10 @@ def _custom_jvp_call_pull_block_spec_rule(\n   jaxpr, consts = call_jaxpr.jaxpr, call_jaxpr.consts\n   if consts:\n     raise NotImplementedError('custom_jvp_call with consts not supported yet')\n+\n   def read_usage_env(_: core.Var):\n     return {Usage.REGULAR}\n+\n   in_block_specs, _, _ = _pull_block_spec(\n       jaxpr,\n       out_block_specs,\n@@ -2009,3 +2066,45 @@ def register_eltwise_rule(prim: core.Primitive):\n register_eltwise_rule(lax.rsqrt_p)\n register_eltwise_rule(lax.log_p)\n register_eltwise_rule(lax.integer_pow_p)\n+\n+@register_push_block_spec_rule(lax.reshape_p)\n+def _reshape_push_rule(\n+    ctx: PullRuleContext,\n+    block_spec: pallas_core.BlockSpec,\n+    *,\n+    dimensions: tuple[int, ...] | None,\n+    new_sizes: tuple[int, ...],\n+    sharding: jax.sharding.Sharding,\n+):\n+  del sharding, new_sizes\n+  if dimensions is not None:\n+    raise NotImplementedError('reshape with None dimensions not supported yet')\n+  aval_in = ctx.avals_in[0]\n+  assert isinstance(aval_in, core.ShapedArray)\n+  aval_out = ctx.avals_out[0]\n+  assert isinstance(aval_out, core.ShapedArray)\n+  if _pattern_match_lanes_to_sublanes_reshape(aval_in, aval_out):\n+    block_shape = tuple(block_spec.block_shape)\n+    if not isinstance(block_shape[-1], (int, pallas_core.Blocked)):\n+      raise NotImplementedError(\n+          f'reshape must use Blocked block size on lanes: {block_shape}'\n+      )\n+    last_dim = aval_out.shape[-1]\n+    last_block_dim = _block_size(block_shape[-1])\n+    if last_block_dim % 128 != 0:\n+      raise NotImplementedError(\n+          'reshape with non-128 aligned block size on lanes not supported yet'\n+      )\n+    if last_block_dim % last_dim != 0:\n+      raise NotImplementedError(\n+          'reshape with non-divisible block size on lanes not supported yet'\n+      )\n+    num_last_dim_blocks = last_block_dim // last_dim\n+    new_block_shape = block_shape[:1] + (num_last_dim_blocks, last_dim)\n+\n+    def new_index_map(*args):\n+      *idx, last = block_spec.index_map(*args)\n+      return *idx, last, 0\n+\n+    return pallas_core.BlockSpec(new_block_shape, new_index_map)\n+  raise NotImplementedError(f'reshape not supported yet: {aval_in}, {aval_out}')\ndiff --git a/tests/pallas/fuser_block_spec_test.py b/tests/pallas/fuser_block_spec_test.py\nindex b348ba971c38..db242fb1e400 100644\n--- a/tests/pallas/fuser_block_spec_test.py\n+++ b/tests/pallas/fuser_block_spec_test.py\n@@ -771,7 +771,7 @@ def f():\n         x_block,\n     )\n \n-  def test_basic_reshape(self):\n+  def test_basic_reshape_sublanes_to_lanes(self):\n \n     def f(x):\n       return x.reshape((512, 2048))\n@@ -800,6 +800,44 @@ def f(x):\n     y = kernel_fn((0, 1, 2), scalar_prefetch_values, (), x)\n     np.testing.assert_array_equal(y, x.reshape((256, 1024)))\n \n+  def test_basic_reshape_lanes_to_sublanes(self):\n+\n+    def f(x):\n+      return x.reshape((512, 32, 128))\n+\n+    in_type = jax.ShapeDtypeStruct((512, 4096), jnp.float32)\n+    f2, new_values, scalar_prefetch_values = block_spec_lib.get_fusion_values(\n+        f, in_type\n+    )\n+    self.assertEmpty(new_values)\n+    self.assertEmpty(scalar_prefetch_values)\n+\n+    block_spec = pl.BlockSpec((256, 8, 128), lambda i, j, k: (i, k, 0))\n+    kernel_fn, (value_block_specs, x_block_spec), _ = (\n+        block_spec_lib.pull_block_spec(\n+            f2,\n+            block_spec,\n+            grid=(2, 3, 4),\n+            scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+        )(new_values, in_type)\n+    )\n+    self.assertEmpty(value_block_specs)\n+    self.assertEqual(x_block_spec.index_map(0, 1, 2), (0, 2))\n+    self.assertEqual(x_block_spec.index_map(3, 2, 1), (3, 1))\n+\n+    x = jnp.arange((256 * 1024), dtype=jnp.float32).reshape((256, 1024))\n+    y = kernel_fn((0, 1, 2), scalar_prefetch_values, (), x)\n+    np.testing.assert_array_equal(y, x.reshape((256, 8, 128)))\n+\n+    block_spec = pl.BlockSpec((256, 4, 256), lambda i, j, k: (i, j, k))\n+    with self.assertRaises(NotImplementedError):\n+      _ = block_spec_lib.pull_block_spec(\n+          f2,\n+          block_spec,\n+          grid=(2, 3, 4),\n+          scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+      )(new_values, in_type)\n+\n   def test_basic_swap(self):\n     value = jnp.arange((512 * 1024), dtype=jnp.int32).reshape((512, 1024)) * 2\n     x = jnp.zeros((256, 512), dtype=jnp.int32)\n@@ -934,12 +972,14 @@ def f(key):\n     self.assertEmpty(value_block_specs)\n     self.assertEqual(key_block_spec.memory_space, pl.MemorySpace.KEY)\n     self.assertIsNone(key_block_spec.block_shape)\n+\n     @jax.jit\n     def gen(idx):\n       k = key\n       for i in idx:\n         k = jax.random.fold_in(k, i)\n       return jax.random.uniform(k, (128, 256), dtype=jnp.float32)\n+\n     for i in range(4):\n       for j in range(2):\n         out = kernel_fn((i, j), scalar_prefetch_values, (), key)\n@@ -1086,6 +1126,32 @@ def f(x):\n     out_block_spec = block_spec_lib.push_block_spec(f, block_spec)(x_type)\n     self.assertEqual(out_block_spec.block_shape, block_spec.block_shape)\n \n+  def test_push_reshape_lanes_to_sublanes(self):\n+    def f(x):\n+      return x.reshape((512, 32, 128))\n+\n+    x_type = jax.ShapeDtypeStruct((512, 4096), jnp.float32)\n+    block_spec = pl.BlockSpec(\n+        (256, 1024), lambda i, j, k: (i, k)\n+    )\n+    out_block_spec = block_spec_lib.push_block_spec(f, block_spec)(x_type)\n+    self.assertEqual(out_block_spec.block_shape, (256, 8, 128))\n+    self.assertTupleEqual(out_block_spec.index_map(0, 1, 2), (0, 2, 0))\n+    self.assertEqual(out_block_spec.index_map(3, 2, 1), (3, 1, 0))\n+\n+    def f(x):\n+      return x.reshape((512, 16, 256))\n+\n+    x_type = jax.ShapeDtypeStruct((512, 4096), jnp.float32)\n+    block_spec = pl.BlockSpec(\n+        (256, 1024), lambda i, j, k: (i, k)\n+    )\n+    out_block_spec = block_spec_lib.push_block_spec(f, block_spec)(x_type)\n+    self.assertEqual(out_block_spec.block_shape, (256, 4, 256))\n+    self.assertTupleEqual(out_block_spec.index_map(0, 1, 2), (0, 2, 0))\n+    self.assertEqual(out_block_spec.index_map(3, 2, 1), (3, 1, 0))\n+\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "sharadmv",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add a general system for keeping track of quasi-dynamic data (QDD).",
        "output": "```diff\nCommit: 2acbbcc255c543867319d548803a97a5cc7cbf6c\nDate: 2025-06-04T20:32:50Z\nURL: https://github.com/jax-ml/jax/commit/2acbbcc255c543867319d548803a97a5cc7cbf6c\nFiles changed: 6\nAdditions: +277, Deletions: -181\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex 91ddd77e3d49..7260557ff4cc 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,8 +88,7 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info', '_is_high',\n-               '_initial_typechange_env', '_final_typechange_env']\n+               '_effects', '_debug_info', '_is_high']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -98,8 +97,6 @@ class Jaxpr:\n   _effects: Effects\n   _debug_info: DebugInfo\n   _is_high: bool\n-  _initial_typechange_env: dict[Var, Any]\n-  _final_typechange_env: dict[Var, Any]\n \n   @property\n   def constvars(self) -> list[Var]:\n@@ -129,14 +126,6 @@ def debug_info(self) -> DebugInfo:\n   def is_high(self) -> bool:\n     return self._is_high\n \n-  @property\n-  def initial_typechange_env(self) -> dict[Var, Any]:\n-    return self._initial_typechange_env\n-\n-  @property\n-  def final_typechange_env(self) -> dict[Var, Any]:\n-    return self._final_typechange_env\n-\n   def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                outvars: Sequence[Atom], eqns: Sequence[JaxprEqn],\n                effects: Effects = no_effects,\n@@ -145,8 +134,6 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n                is_high: bool = False,\n-               initial_typechange_env: dict | None = None,\n-               final_typechange_env: dict | None = None,\n                ):\n     \"\"\"\n     Args:\n@@ -172,8 +159,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n     self._is_high = is_high\n-    self._initial_typechange_env = initial_typechange_env or {}\n-    self._final_typechange_env = final_typechange_env or {}\n+    num_vars = len(constvars) + len(invars)\n \n   def __str__(self):\n     return str(self.pretty_print())\n@@ -201,10 +187,6 @@ def replace(self, **kwargs):\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n         is_high=kwargs.pop(\"is_high\", self.is_high),\n-        initial_typechange_env=kwargs.pop(\"initial_typechange_env\",\n-                                          self.initial_typechange_env),\n-        final_typechange_env=kwargs.pop(\"final_typechange_env\",\n-                                        self.final_typechange_env),\n     )\n     if kwargs:\n       raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n@@ -232,22 +214,6 @@ def subjaxprs(jaxpr: Jaxpr) -> Iterator[Jaxpr]:\n     yield from jaxprs_in_params(eqn.params)\n \n \n-@dataclass(frozen=True)\n-class TypeChange:\n-  aval: AbstractValue\n-  initial_type_state: Any\n-  final_type_state: Any\n-\n-  def to_tangent_aval(self):\n-    return TypeChange(self.aval.to_tangent_aval(),\n-                      self.initial_type_state.to_tangent_aval(),\n-                      self.final_type_state.to_tangent_aval())\n-\n-  def normalize(self):\n-    return TypeChange(self.aval.normalize(),\n-                      self.initial_type_state.normalize(),\n-                      self.final_type_state.normalize())\n-\n class ClosedJaxpr:\n   __slots__ = ['__weakref__', '_jaxpr', '_consts']\n \n@@ -268,10 +234,13 @@ def in_avals(self):\n     return [v.aval for v in self.jaxpr.invars]\n \n   @property\n-  def in_avals_aug(self):\n-    ienv = self.jaxpr.initial_typechange_env\n-    fenv = self.jaxpr.final_typechange_env\n-    return [TypeChange(v.aval, ienv[v], fenv[v]) if v.aval.mutable else v.aval\n+  def in_aval_qdds(self) -> list[AbstractValue | AvalQDD]:\n+    return [v.aval if v.initial_qdd is None else AvalQDD(v.aval, v.initial_qdd)\n+            for v in self.jaxpr.invars]\n+\n+  @property\n+  def final_aval_qdds(self) -> list[AbstractValue | AvalQDD]:\n+    return [v.aval if v.final_qdd is None else AvalQDD(v.aval, v.final_qdd)\n             for v in self.jaxpr.invars]\n \n   @property\n@@ -464,16 +433,22 @@ def new_jaxpr_eqn(invars, outvars, primitive, params, effects, source_info=None,\n _var_counter = it.count()\n \n class Var:\n-  __slots__ = [\"count\", \"suffix\", \"aval\"]\n+  __slots__ = [\"count\", \"suffix\", \"aval\", \"initial_qdd\", \"final_qdd\"]\n \n   count: int\n   suffix: str\n   aval: AbstractValue\n+  # these are only useful for jaxpr binders but rather than create a separate\n+  # type for those, breaking existing interpreters, we add fields here.\n+  initial_qdd : QuasiDynamicData | None\n+  final_qdd : QuasiDynamicData | None\n \n-  def __init__(self, suffix: str, aval: AbstractValue):\n+  def __init__(self, suffix: str, aval: AbstractValue, initial_qdd = None, final_qdd = None):\n     self.count = next(_var_counter)\n     self.suffix = suffix\n     self.aval = aval\n+    self.initial_qdd = initial_qdd\n+    self.final_qdd = final_qdd\n \n   def __repr__(self):\n     return f'Var(id={id(self)}){self.suffix}:{self.aval.str_short()}'\n@@ -483,7 +458,7 @@ def pretty_print(self, context: JaxprPpContext, *, print_dtype: bool = True):\n     return f\"{context.var_names[self]}{self.suffix}\"\n \n \n-def gensym(suffix: str = '') -> Callable[[AbstractValue], Var]:\n+def gensym(suffix: str = '') -> Callable:\n   \"\"\"Produce distinct variables, printed with the optional suffix.\"\"\"\n   return partial(Var, suffix)\n \n@@ -1114,6 +1089,8 @@ def process_custom_vjp_call(self, primitive, fun, fwd, bwd, tracers, **_):  # py\n     del primitive, fwd, bwd, _  # Unused.\n     return fun.call_wrapped(*tracers)\n \n+  def cur_qdd(self, x):\n+    return x.cur_qdd()\n \n class TraceTag:\n   # TODO: this works for surprisingly subtle reasons. Function transformations\n@@ -1505,7 +1482,7 @@ def definitely_equal(x, y):\n class AbstractValue:\n   __slots__: list[str] = []\n   is_high = False\n-  mutable = False\n+  has_qdd = False\n \n   def to_tangent_aval(self):\n     raise NotImplementedError(\"must override\")\n@@ -1533,6 +1510,12 @@ def normalize(self) -> AbstractValue:\n   def update(self, **kwargs):\n     raise NotImplementedError(\"must override\")\n \n+  def lo_ty(self):\n+    raise NotImplementedError(\"must override\")\n+\n+  def lo_ty_qdd(self, qdd):\n+    raise NotImplementedError(\"avals with qdd must override\")\n+\n   def str_short(self, short_dtypes=False, mesh_axis_types=False):\n     return str(self)\n \n@@ -1704,6 +1687,54 @@ def concrete_dim_or_error(val: Any, context=\"\"):\n   else:\n     return concrete_or_error(operator.index, val, context=context)\n \n+### Quasi-dynamic data\n+\n+# Quasi-dynamic data includes things like liveness bits and the content type of\n+# a type-changeable box. These change throughout the program but at a given\n+# point in the program they have a single statically known value.\n+\n+class MutableQuasiDynamicData:\n+  def __init__(self, val : QuasiDynamicData | None):\n+    self.init_val = val\n+    self.cur_val = val  # immutable payload\n+\n+  def update(self, val):\n+    self.cur_val = val\n+\n+class QuasiDynamicData:\n+  pass\n+\n+@dataclass(frozen=True)\n+class AvalQDD:\n+  aval: AbstractValue\n+  qdd: QuasiDynamicData | None # immutable\n+\n+  has_qdd = True\n+  def lo_ty(self):\n+    return self.aval.lo_ty_qdd(self.qdd)  # type: ignore\n+\n+  def read_loval(self, val):\n+    return self.aval.read_loval(self.qdd, val)  # type: ignore\n+\n+  def new_from_loval(self, *lovals):\n+    return self.aval.new_from_loval(self.qdd, *lovals)  # type: ignore\n+\n+  def to_tangent_aval(self):\n+    return AvalQDD(self.aval.to_tangent_aval(), self.qdd.to_tangent_qdd())\n+\n+@dataclass(frozen=True)\n+class AvalMutableQDD:\n+  aval: AbstractValue\n+  mutable_qdd: MutableQuasiDynamicData\n+\n+def cur_qdd(x):\n+  prev_trace = trace_ctx.trace\n+  trace_ctx.set_trace(eval_trace)\n+  try:\n+    return prev_trace.cur_qdd(x)\n+  finally:\n+    trace_ctx.set_trace(prev_trace)\n+\n ### Extended dtypes\n #\n # Extended dtypes are JAX-specific dtypes that allow us to represent logical\n@@ -2917,15 +2948,19 @@ def ctx_factory():\n     from jax.experimental.key_reuse._core import check_key_reuse_jaxpr  # pytype: disable=import-error\n     check_key_reuse_jaxpr(jaxpr)\n \n+# A place to track the quasi-dynamic data associated with a variable during typechecking\n+@dataclass(frozen=True)\n+class MutableTypecheckVal:\n+  aval : AbstractValue\n+  mutable_qdd : MutableQuasiDynamicData\n \n def _check_jaxpr(\n     ctx_factory: Callable[[], tuple[JaxprPpContext, JaxprPpSettings]],\n     jaxpr: Jaxpr\n   ) -> None:\n-  # Use set of variables to types to check that variables are in scope.\n-  env: set[Var] = set()\n+  env: dict[Var, Atom | MutableTypecheckVal] = {}\n \n-  def read(x: Atom) -> Atom:\n+  def read(x: Atom) -> Atom | MutableTypecheckVal:\n     # Check the type annotation is itself well-typed.\n     check_type(ctx_factory, env, x.aval)\n     if isinstance(x, Var):\n@@ -2933,7 +2968,7 @@ def read(x: Atom) -> Atom:\n       if x not in env:\n         ctx, _ = ctx_factory()\n         raise JaxprTypeError(f\"Variable '{pp_var(x, ctx)}' not defined\")\n-      return x\n+      return env[x]\n     elif isinstance(x, Literal):\n       # Check that the literal matches its type annotation.\n       if not typecheck(x.aval, x.val):\n@@ -2945,7 +2980,8 @@ def read(x: Atom) -> Atom:\n     else:\n       assert False, \"syntactically invalid jaxpr\"\n \n-  def write(v: Var, a: AbstractValue) -> None:\n+  def write(v: Var, a: AvalQDD) -> None:\n+    aval, qdd = a.aval, a.qdd\n     assert isinstance(v, Var), \"syntactically invalid jaxpr\"\n     # Check the type annotation of the binder is itself well-typed.\n     check_type(ctx_factory, env, v.aval)\n@@ -2954,19 +2990,23 @@ def write(v: Var, a: AbstractValue) -> None:\n       ctx, _ = ctx_factory()\n       raise JaxprTypeError(f\"Variable '{pp_var(v, ctx)}' already bound\")\n     # Check that the computed type is consistent with the binder annotation.\n-    if not typematch(v.aval, a):\n+    if not typematch(v.aval, aval):\n       ctx, _ = ctx_factory()\n       raise JaxprTypeError(\n           f\"Value for variable '{pp_var(v, ctx)}' inconsistently typed \"\n-          f\"as {pp_aval(a, ctx)} for let-binder of type {pp_aval(v.aval, ctx)}\")\n+          f\"as {pp_aval(aval, ctx)} for let-binder of type {pp_aval(v.aval, ctx)}\")\n+\n     # If the variable is not a DropVar, add it to the environment.\n     if not isinstance(v, DropVar):\n-      env.add(v)\n+      if qdd is None:\n+        env[v] = v\n+      else:\n+        env[v] = MutableTypecheckVal(aval, MutableQuasiDynamicData(qdd))\n \n   # Check type annotations on lambda binders.\n   for v in it.chain(jaxpr.constvars, jaxpr.invars):\n     check_type(ctx_factory, env, v.aval)\n-    write(v, v.aval)\n+    write(v, AvalQDD(v.aval, v.initial_qdd))\n \n   # Check each eqn.\n   sentinel = object()\n@@ -2976,7 +3016,8 @@ def write(v: Var, a: AbstractValue) -> None:\n     prim = eqn.primitive\n     try:\n       in_atoms = map(read, eqn.invars)\n-      in_avals = [x.aval for x in in_atoms]  # use in_atoms for dyn shapes\n+      in_avals = [AvalMutableQDD(x.aval, x.mutable_qdd) if isinstance(x, MutableTypecheckVal)\n+                  else x.aval for x in in_atoms]  # use in_atoms for dyn shapes\n \n       # Compute the type of the primitive application.\n       with eqn.ctx.manager:\n@@ -3026,6 +3067,7 @@ def write(v: Var, a: AbstractValue) -> None:\n \n       # Check out_type matches the let-binders' annotation (after substitution).\n       out_type = substitute_vars_in_output_ty(out_type, eqn.invars, eqn.outvars)\n+      out_type = [t if isinstance(t, AvalQDD) else AvalQDD(t, None) for t in out_type]\n       foreach(write, eqn.outvars, out_type)\n \n     except JaxprTypeError as e:\n@@ -3041,7 +3083,7 @@ def write(v: Var, a: AbstractValue) -> None:\n \n def check_type(\n     ctx_factory: Callable[[], tuple[JaxprPpContext, JaxprPpSettings]],\n-    env: set[Var],\n+    env: dict[Var, Atom | MutableTypecheckVal],\n     ty: AbstractValue,\n   ) -> None:\n   if isinstance(ty, DShapedArray):\n@@ -3111,7 +3153,7 @@ def _check_call(ctx_factory, prim, in_atoms, params):\n                          f\"{len(call_jaxpr.invars)} inputs\")\n \n   # Check `call_jaxpr` can be applied to in_atoms.\n-  env: dict[Var, Atom] = {}\n+  env: dict[Var, Atom | MutableTypecheckVal] = {}\n   def substitute(aval: AbstractValue):\n     if isinstance(aval, DShapedArray):\n       aval = aval.update(shape=tuple(env.get(d, d) for d in aval.shape))  # type: ignore\n@@ -3122,7 +3164,7 @@ def substitute(aval: AbstractValue):\n       raise JaxprTypeError(f\"Call primitive {prim} passes operand {x} of type \"\n                            f\"{x.aval} to jaxpr expecting type \"\n                            f\"{substitute(v.aval)}\")\n-    env[v] = x if type(x) is Var else x.val\n+    env[v] = x.val if type(x) is Literal else x\n \n   _check_jaxpr(ctx_factory, call_jaxpr)\n \ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex a77e93bb0696..69a123b12e23 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -501,6 +501,11 @@ def process_primitive(self, primitive, tracers, params):\n     else:\n       return maybe_jvp_tracer(self, primal_out, tangent_out)\n \n+  def cur_qdd(self, x):\n+    p, _ = self.to_primal_tangent_pair(x)\n+    with core.set_current_trace(self.parent_trace):\n+      return core.cur_qdd(p)\n+\n   def process_call(self, call_primitive, f, tracers, params):\n     assert call_primitive.multiple_results\n     primals, tangents = unzip2(map(self.to_primal_tangent_pair, tracers))\n@@ -629,6 +634,9 @@ def __init__(self, trace, primal, tangent):\n   def aval(self):\n     return get_aval(self.primal)\n \n+  def cur_qdd(self):\n+    return core.cur_qdd(self.primal)\n+\n   def full_lower(self):\n     if type(self.tangent) is Zero:\n       return core.full_lower(self.primal)\n@@ -1170,8 +1178,8 @@ def _jvp_jaxpr(jaxpr: core.ClosedJaxpr,\n   f_jvp, out_nonzeros = f_jvp_traceable(\n       jvp(f, instantiate=instantiate, transform_stack=False), nonzeros)\n   tangent_avals = [aval.to_tangent_aval()\n-                   for aval, nz in zip(jaxpr.in_avals_aug, nonzeros) if nz]\n-  avals_in = list(it.chain(jaxpr.in_avals_aug, tangent_avals))\n+                   for aval, nz in zip(jaxpr.in_aval_qdds, nonzeros) if nz]\n+  avals_in = list(it.chain(jaxpr.in_aval_qdds, tangent_avals))\n   jaxpr_out, avals_out, literals_out, () = pe.trace_to_jaxpr_dynamic(\n       f_jvp, avals_in)\n   return core.ClosedJaxpr(jaxpr_out, literals_out), out_nonzeros()\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 1bcd3f00321c..a4f4fcd12429 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -222,6 +222,14 @@ def instantiate_const_abstracted(self, tracer) -> JaxprTracer:\n       aval = get_aval(const).update_weak_type(np.isscalar(const))\n       return JaxprTracer(self, PartialVal.unknown(aval), ConstVar(const))\n \n+  def cur_qdd(self, x):\n+    const = self.to_jaxpr_tracer(x).pval.get_known()\n+    if const is None:\n+      assert False # TODO: track tangent QDDs\n+    else:\n+      with core.set_current_trace(self.parent_trace):\n+        return core.cur_qdd(const)\n+\n   def process_primitive(self, primitive, tracers, params):\n     with core.set_current_trace(self.parent_trace):\n       if primitive in custom_partial_eval_rules:\n@@ -1012,7 +1020,7 @@ def fun(*known_vals_in):\n     known_vals_out = [pval.get_known() for pval in out_pvals if pval.is_known()]\n     return [*known_vals_out, *residuals]\n \n-  known_avals = [a for a, uk in zip(jaxpr.in_avals_aug, in_unknowns) if not uk]\n+  known_avals = [a for a, uk in zip(jaxpr.in_aval_qdds, in_unknowns) if not uk]\n   jaxpr_known, _, consts_known, () = trace_to_jaxpr_dynamic(\n       lu.wrap_init(fun, debug_info=f.debug_info), known_avals)\n   (out_unknowns, jaxpr_unknown, res_avals), = cell  # pytype: disable=bad-unpacking\n@@ -1201,14 +1209,11 @@ def has_effects(effects) -> bool:\n   known_outvars = [*outs_known, *residuals]\n   known_effects = make_jaxpr_effects(jaxpr.constvars, ins_known_and_ref_res,\n                                      known_outvars, known_eqns)\n-  known_mut, staged_mut, ins_known_ = {}, {}, set(ins_known)  # type: ignore\n-  for v, t in jaxpr.final_typechange_env.items():\n-    [staged_mut, known_mut][v in ins_known_][v] = t\n \n   # TODO(mattjj,necula): debug info should be updated here\n   jaxpr_known = jaxpr.replace(\n       invars=ins_known_and_ref_res, outvars=known_outvars,\n-      eqns=known_eqns, effects=known_effects, final_typechange_env=known_mut)\n+      eqns=known_eqns, effects=known_effects)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_known)\n \n   _, ins_staged = partition_list(in_inst, jaxpr.invars)\n@@ -1219,7 +1224,7 @@ def has_effects(effects) -> bool:\n   # TODO(mattjj,necula): debug info should be updated here\n   jaxpr_staged = jaxpr.replace(\n       invars=staged_invars, outvars=outs_staged, eqns=staged_eqns,\n-      effects=staged_effects, final_typechange_env=staged_mut)\n+      effects=staged_effects)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_staged)\n \n   return (jaxpr_known, jaxpr_staged, out_unknowns, out_inst, len(residuals),\n@@ -1634,15 +1639,30 @@ def _move_outvars_to_back(jaxpr, to_move):\n \n \n class DynamicJaxprTracer(core.Tracer):\n-  __slots__ = ['aval', '_debug_info']\n+  __slots__ = ['aval', 'mutable_qdd', '_debug_info']\n \n   def __init__(self, trace: DynamicJaxprTrace,\n-               aval: core.AbstractValue,\n+               aval: core.AbstractValue | core.AvalQDD,\n                line_info: source_info_util.SourceInfo | None = None):\n+    if isinstance(aval, core.AvalQDD):\n+      assert aval.qdd is not None\n+      aval, qdd = aval.aval, aval.qdd\n+    else:\n+      assert not aval.has_qdd\n+      qdd = None\n     self._trace = trace\n     self._line_info = line_info\n     self._debug_info = self._trace.frame.debug_info  # for UnexpectedTracerError\n     self.aval = aval  # type: ignore[misc]\n+    self.mutable_qdd = core.MutableQuasiDynamicData(qdd)\n+\n+  @property\n+  def aval_mutable_qdd(self):\n+    aval = self.aval\n+    if aval.has_qdd:\n+      return core.AvalMutableQDD(aval, self.mutable_qdd)\n+    else:\n+      return aval\n \n   def full_lower(self):\n     var = self._trace.frame.tracer_to_var.get(id(self))\n@@ -1651,10 +1671,6 @@ def full_lower(self):\n     if val is None: return self\n     return core.full_lower(val)\n \n-  def type_state(self):\n-    var = self._trace.frame.tracer_to_var.get(id(self))\n-    return self._trace.frame.current_typechange_env[var]\n-\n   def _contents(self):\n     return ()\n \n@@ -1750,8 +1766,8 @@ class JaxprStackFrame:\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n   is_high: bool\n-  initial_typechange_env: dict\n-  current_typechange_env: dict\n+  mutable_qdds: list[tuple[Var, core.MutableQuasiDynamicData]]\n+\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1767,8 +1783,7 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_vars = []\n     self.debug_info = debug_info\n     self.is_high = False\n-    self.initial_typechange_env = {}\n-    self.current_typechange_env = {}\n+    self.mutable_qdds = []\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1794,11 +1809,13 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n-    final_typechange_env = {v: s for v, s in self.current_typechange_env.items()\n-                            if v in self.initial_typechange_env}\n+\n+    # TODO(dougalm): handle qdd for consts\n+    for v, qdd in self.mutable_qdds:\n+      v.final_qdd = qdd.cur_val\n+\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info, self.is_high, self.initial_typechange_env,\n-                  final_typechange_env)\n+                  debug_info, self.is_high)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1827,7 +1844,10 @@ def newvar(self, aval):\n                    for d in aval.shape]\n       new_shape = [d.val if isinstance(d, Literal) else d for d in new_shape]\n       aval = aval.update(shape=tuple(new_shape))\n-    return self.gensym(aval)\n+    if isinstance(aval, core.AvalQDD):\n+       return self.gensym(aval.aval, initial_qdd=aval.qdd)\n+    else:\n+       return self.gensym(aval)\n \n   def find_progenitors(self, tracer):\n     var = self.tracer_to_var.get(id(tracer))\n@@ -1883,12 +1903,13 @@ def vars(atom: Atom) -> list[Var]:\n \n \n class DynamicJaxprTrace(core.Trace):\n-  __slots__ = (\"frame\", \"tag\")\n+  __slots__ = (\"frame\", \"tag\", \"parent_trace\")\n \n-  def __init__(self, debug_info: core.DebugInfo, lower=False):\n+  def __init__(self, debug_info: core.DebugInfo, parent_trace=None, lower=False):\n     super().__init__()\n     self.requires_low = lower\n     self.frame = JaxprStackFrame(debug_info)\n+    self.parent_trace = parent_trace\n \n   def invalidate(self):\n     # avoid cyclic refs\n@@ -1915,6 +1936,7 @@ def new_arg(self, aval, source_info: SourceInfo):\n     self.frame.tracers.append(tracer)\n     self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n     self.frame.invars.append(var)\n+    self.frame.mutable_qdds.append((var, tracer.mutable_qdd))\n     return tracer\n \n   def new_const(self, c, source_info: SourceInfo):\n@@ -1922,6 +1944,9 @@ def new_const(self, c, source_info: SourceInfo):\n     tracer = self.frame.constid_to_tracer.get(id(c))\n     if tracer is None:\n       aval = get_aval(c)\n+      if aval.has_qdd:\n+        with core.set_current_trace(self.parent_trace):\n+          aval = core.AvalQDD(aval, core.cur_qdd(c))\n       if hasattr(aval, \"weak_type\"):\n         aval = aval.update_weak_type(dtypes.is_weakly_typed(c))\n       aval = self._lift_tracers_in_aval(aval, source_info)\n@@ -1938,9 +1963,9 @@ def _new_const(self, aval, c, source_info: SourceInfo) -> DynamicJaxprTracer:\n     else:\n       self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n       self.frame.constid_to_tracer[id(c)] = tracer\n+      if isinstance(aval, core.AvalQDD):\n+        self.frame.mutable_qdds.append((var, tracer.mutable_qdd))\n       self.frame.constvar_to_val[var] = c\n-      if aval.mutable:\n-        self.frame.initial_typechange_env[var] = c.type_state()\n     return tracer\n \n   def get_const(self, tracer) -> Any:\n@@ -1971,7 +1996,12 @@ def makevar(self, tracer):\n     var = self.frame.tracer_to_var[id(tracer)] = self.frame.newvar(tracer.aval)\n     return var\n \n+  def cur_qdd(self, x):\n+    source_info = source_info_util.current()\n+    return self.to_jaxpr_tracer(x, source_info=source_info).mutable_qdd.cur_val\n+\n   def process_primitive(self, primitive, tracers, params):\n+    self.frame.is_high |= primitive.is_high(**params)\n     if config.eager_constant_folding.value and not any(isinstance(x, Tracer) for x in tracers):\n       return primitive.bind_with_trace(core.eval_trace, tracers, params)\n     source_info = source_info_util.current()\n@@ -1982,8 +2012,8 @@ def process_primitive(self, primitive, tracers, params):\n     return self.default_process_primitive(primitive, jaxpr_tracers, params)\n \n   def default_process_primitive(self, primitive, tracers, params):\n-    avals = [t.aval for t in tracers]\n-    out_avals, effs = primitive.abstract_eval(*avals, **params)\n+    aval_qdds = [t.aval_mutable_qdd for t in tracers]\n+    out_avals, effs = primitive.abstract_eval(*aval_qdds, **params)\n     if isinstance(out_avals, (tuple, list)) != primitive.multiple_results:\n       raise ValueError(f\"{primitive}.abstract_eval() method should return \"\n                        f\"a tuple or a list iff {primitive}.multiple_results.\")\n@@ -2254,17 +2284,13 @@ def trace_to_jaxpr_dynamic(\n ) -> tuple[Jaxpr, list[AbstractValue], list[Any],\n            list[tuple[PyTreeDef, PyTreeDef, tuple[Any, str, AttrKind]]]]:\n   keep_inputs = [True] * len(in_avals) if keep_inputs is None else keep_inputs\n-  trace = DynamicJaxprTrace(fun.debug_info, lower=lower)\n-  in_avals_ = [a.aval if isinstance(a, core.TypeChange) else a for a in in_avals]\n+  parent_trace = core.trace_ctx.trace\n+  trace = DynamicJaxprTrace(fun.debug_info, parent_trace=parent_trace, lower=lower)\n   with core.ensure_no_leaks(trace), source_info_util.reset_name_stack():\n     source_info = source_info_util.current()\n     in_tracers = _input_type_to_tracers(\n-        partial(trace.new_arg, source_info=source_info), in_avals_)\n+        partial(trace.new_arg, source_info=source_info), in_avals)\n     in_tracers = [t for t, keep in zip(in_tracers, keep_inputs) if keep]\n-    trace.frame.initial_typechange_env = initial_typechange_env = {\n-        v: a.initial_type_state for v, a in zip(trace.frame.invars, in_avals)\n-        if isinstance(a, core.TypeChange)}\n-    trace.frame.current_typechange_env = dict(initial_typechange_env)\n \n     try:\n       with core.set_current_trace(trace):\n@@ -2329,7 +2355,8 @@ def trace_to_jaxpr_dynamic2(\n   ) -> tuple[Jaxpr, OutputType, list[Any]]:\n   assert fun.in_type is not None, \"fun must be annotated with lu.annotate()\"\n \n-  trace = DynamicJaxprTrace(fun.debug_info)\n+  parent_trace = core.trace_ctx.trace\n+  trace = DynamicJaxprTrace(fun.debug_info, parent_trace=parent_trace)\n   with core.ensure_no_leaks(trace), source_info_util.reset_name_stack():\n     source_info = source_info_util.current()\n     in_avals, keep_inputs = unzip2(fun.in_type)\n@@ -2744,33 +2771,28 @@ def _linearize_of_pmap_hack(f: lu.WrappedFun, jaxpr, consts) -> tuple[Jaxpr, lis\n \n @weakref_lru_cache\n def lower_jaxpr(hi_jaxpr):\n-  initial_env = hi_jaxpr.jaxpr.initial_typechange_env\n-  lo_avals = [lo_ty for v in hi_jaxpr.jaxpr.invars\n-              for lo_ty in (v.aval.lo_ty_(initial_env[v]) if v.aval.mutable\n-                            else v.aval.lo_ty())]\n+  lo_avals = [lo_ty for aval in hi_jaxpr.in_aval_qdds for lo_ty in aval.lo_ty()]\n   f = lu.wrap_init(partial(lower_traceable, hi_jaxpr),\n                    debug_info=hi_jaxpr.jaxpr.debug_info)\n   lo_jaxpr, _, lo_consts, () = trace_to_jaxpr_dynamic(f, lo_avals, lower=True)\n   return core.ClosedJaxpr(lo_jaxpr, lo_consts)\n \n def lower_traceable(jaxpr, *lo_args):\n-  env = jaxpr.jaxpr.initial_typechange_env\n   lo_args_ = iter(lo_args)\n-  hi_args = [v.aval.raise_val(*it.islice(lo_args_, len(v.aval.lo_ty())))\n-             if not v.aval.mutable else\n-             v.aval.new_from_loval(env[v], *it.islice(lo_args_, len(v.aval.lo_ty_(env[v]))))\n-             for v in jaxpr.jaxpr.invars]\n+  hi_args = [aval.raise_val(*it.islice(lo_args_, len(aval.lo_ty())))\n+             if not aval.has_qdd else\n+             aval.new_from_loval(*it.islice(lo_args_, len(aval.lo_ty())))\n+             for aval in jaxpr.in_aval_qdds]\n   assert (problem := next(lo_args_, None)) is None\n   hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n-  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.final_typechange_env.items()\n-              for lo_val in v.aval.read_loval(ty, hi_args[in_idx[v]])]\n+  mut_outs = [lo_val for aval, hi_arg in zip(jaxpr.final_aval_qdds, hi_args) if aval.has_qdd\n+              for lo_val in aval.read_loval(hi_arg)]\n   lo_outs = [lo_val for v, hi_val in zip(jaxpr.jaxpr.outvars, hi_outs)\n              for lo_val in v.aval.lower_val(hi_val)]\n   return mut_outs + lo_outs\n \n def convert_const_himutables(jaxpr):\n-  move = [core.typeof(c).mutable for c in jaxpr.consts]\n+  move = [core.typeof(c).has_qdd for c in jaxpr.consts]\n   constvals, in_mutables = partition_list(move, jaxpr.consts)\n   constvars, boxvars = partition_list(move, jaxpr.jaxpr.constvars)\n   invars = *boxvars, *jaxpr.jaxpr.invars\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex ad09292731cf..3fefab78f7f7 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -1512,17 +1512,6 @@ def arrange_jaxpr_args_for_wrapped(args):\n   assert len(refs_out_matching_in_avals) == len(in_avals)\n   return refs_out_matching_in_avals, [*carry_out, *ys]\n \n-def _scan_staging(trace, *args, **params):\n-  outs = trace.default_process_primitive(scan_p, args, params)\n-  jaxpr = params['jaxpr']\n-  trace.frame.is_high = jaxpr.jaxpr.is_high\n-  invars = [trace.frame.tracer_to_var[id(t)] for t in args]\n-  var_map = dict(zip(jaxpr.jaxpr.invars, invars))\n-  final_env = {var_map[v]: ty for v, ty in\n-               jaxpr.jaxpr.final_typechange_env.items()}\n-  trace.frame.current_typechange_env.update(final_env)\n-  return outs\n-\n scan_p = core.Primitive(\"scan\")\n scan_p.multiple_results = True\n scan_p.skip_canonicalization = True\n@@ -1541,37 +1530,32 @@ def _scan_staging(trace, *args, **params):\n pe.padding_rules[scan_p] = _scan_padding_rule\n pe.dce_rules[scan_p] = _scan_dce_rule\n state_discharge.register_partial_discharge_rule(scan_p)(_scan_state_partial_discharge_rule)\n-pe.custom_staging_rules[scan_p] = _scan_staging\n \n def _is_high(jaxpr, **_) -> bool:\n   return jaxpr.jaxpr.is_high\n scan_p.is_high = _is_high  # type: ignore\n \n def _to_lojax(*hi_args, jaxpr, num_carry, num_consts, linear, **params):\n-  ienv, fenv = jaxpr.jaxpr.initial_typechange_env, jaxpr.jaxpr.final_typechange_env\n \n   # move box binders and hi_args from consts slots to carry slots\n-  to_move = [t.mutable for t in jaxpr.in_avals[:num_consts]]\n+  to_move = [t.has_qdd for t in jaxpr.in_aval_qdds[:num_consts]]\n   jaxpr = pe.move_invars_right(jaxpr, to_move)\n   hi_args = _move_right(hi_args, to_move)\n   num_consts -= sum(to_move)\n   num_carry += sum(to_move)\n \n   # expand num_consts, num_carry, linear according to lo types\n-  const_invars, carry_invars, _ = split_list(jaxpr.jaxpr.invars, [num_consts, num_carry])\n-  num_consts = sum(len(v.aval.lo_ty() if not v.aval.mutable\n-                       else v.aval.lo_ty_(ienv[v])) for v in const_invars)\n-  num_carry = sum(len(v.aval.lo_ty() if not v.aval.mutable\n-                      else v.aval.lo_ty_(ienv[v])) for v in carry_invars)\n-  linear = [l for v, l_ in zip(jaxpr.jaxpr.invars, linear)\n-            for l in (l_,) * len(v.aval.lo_ty() if not v.aval.mutable\n-                                 else v.aval.lo_ty_(ienv[v]))]\n-  lo_muts_out = sum(len(m.leaf_avals) for m in fenv.values())  # TODO hardcoded\n-\n-  # collect lo inputs values\n-  lo_args = [lo_val for v, x in zip(jaxpr.jaxpr.invars, hi_args)\n-             for lo_val in (v.aval.read_loval(ienv[v], x) if v.aval.mutable\n-                            else v.aval.lower_val(x))]\n+  const_in_avals, carry_in_avals, _ = split_list(jaxpr.in_aval_qdds, [num_consts, num_carry])\n+  num_consts = sum(len(aval.lo_ty()) for aval in const_in_avals)\n+  num_carry = sum(len(aval.lo_ty()) for aval in carry_in_avals)\n+  linear = [l for aval, l_ in zip(jaxpr.in_aval_qdds, linear)\n+            for l in (l_,) * len(aval.lo_ty())]\n+  lo_muts_out = sum(len(aval.lo_ty()) for aval in jaxpr.final_aval_qdds if aval.has_qdd)\n+\n+  # collect lo input values\n+  lo_args = [lo_val for aval, x in zip(jaxpr.in_aval_qdds, hi_args)\n+             for lo_val in (aval.read_loval(x) if aval.has_qdd\n+                            else aval.lower_val(x))]\n \n   # lower the jaxpr and bind it using lo input values\n   lo_jaxpr = pe.lower_jaxpr(jaxpr)\n@@ -1582,9 +1566,13 @@ def _to_lojax(*hi_args, jaxpr, num_carry, num_consts, linear, **params):\n   # collect and apply mutations\n   out_mut_ = iter(out_mut)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n-    lo_vals = it.islice(out_mut_, len(var.aval.lo_ty_(ty)))\n-    var.aval.update_from_loval(ty, hi_args[in_idx[var]], *lo_vals)\n+\n+  for v in jaxpr.jaxpr.invars:\n+    if v.final_qdd is not None:\n+      qdd = v.final_qdd\n+      lo_vals = it.islice(out_mut_, len(v.aval.lo_ty_qdd(qdd)))\n+      v.aval.update_from_loval(qdd, hi_args[in_idx[v]], *lo_vals)\n+\n   assert next(out_mut_, None) is None\n \n   # collect output values into hi types\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 572c2225af74..985d1a46a053 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -590,7 +590,7 @@ def _infer_params_impl(\n     in_type = in_avals = tuple(core.shaped_abstractify(x) for x in explicit_args)  # type: ignore\n   else:\n     in_type = in_avals  # type: ignore\n-    in_type = tuple(core.TypeChange(a, x.type_state(), None) if a.mutable  # type: ignore\n+    in_type = tuple(core.AvalQDD(a, core.cur_qdd(x)) if a.has_qdd  # type: ignore\n                     else a for a, x in zip(in_type, explicit_args))\n   assert in_avals is not None\n \n@@ -1416,7 +1416,7 @@ def _create_pjit_jaxpr(\n     from jax.experimental.key_reuse._core import check_key_reuse_jaxpr  # pytype: disable=import-error\n     check_key_reuse_jaxpr(jaxpr)\n \n-  if any(isinstance(c, core.Tracer) or core.typeof(c).mutable for c in consts):\n+  if any(isinstance(c, core.Tracer) or core.typeof(c).has_qdd for c in consts):\n     closed_jaxpr = pe.close_jaxpr(pe.convert_constvars_jaxpr(jaxpr))\n     final_consts = consts\n   else:\n@@ -1562,25 +1562,22 @@ def _is_high(jaxpr, **_) -> bool:\n pjit_p.is_high = _is_high  # type: ignore\n \n def _to_lojax(*hi_args, jaxpr, **params):\n-  ienv, fenv = jaxpr.jaxpr.initial_typechange_env, jaxpr.jaxpr.final_typechange_env\n-\n   # convert closed-over boxes to explicit args\n   jaxpr, closed_over_himutables = pe.convert_const_himutables(jaxpr)\n   hi_args = [*closed_over_himutables, *hi_args]\n   params = _converted_mutables_add_params(len(closed_over_himutables), **params)\n \n+\n   # expand pjit params that must match number of lo inputs/outputs\n-  lo_nums_in = [len(v.aval.lo_ty() if not v.aval.mutable\n-                    else v.aval.lo_ty_(ienv[v]))\n-                for v in jaxpr.jaxpr.invars]\n+  lo_nums_in = [len(aval.lo_ty()) for aval in jaxpr.in_aval_qdds]\n   lo_nums_out = [len(t.lo_ty()) for t in jaxpr.out_avals]\n-  lo_muts_out = sum(len(m.leaf_avals) for m in fenv.values())  # TODO hardcoded\n+  lo_muts_out = sum(len(aval.lo_ty()) for aval in jaxpr.final_aval_qdds if aval.has_qdd)\n   params = _lojax_expand_params(lo_nums_in, lo_nums_out, lo_muts_out, **params)\n \n   # collect lo input values\n-  lo_args = [lo_val for v, x in zip(jaxpr.jaxpr.invars, hi_args)\n-             for lo_val in (v.aval.read_loval(ienv[v], x) if v.aval.mutable\n-                            else v.aval.lower_val(x))]\n+  lo_args = [lo_val for aval, x in zip(jaxpr.in_aval_qdds, hi_args)\n+             for lo_val in (aval.read_loval(x) if aval.has_qdd\n+                            else aval.lower_val(x))]\n \n   # lower the jaxpr and bind it using lo input values\n   lo_jaxpr = pe.lower_jaxpr(jaxpr)\n@@ -1590,9 +1587,11 @@ def _to_lojax(*hi_args, jaxpr, **params):\n   # collect and apply mutations\n   out_mut_ = iter(out_mut)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n-    lo_vals = it.islice(out_mut_, len(var.aval.lo_ty_(ty)))\n-    var.aval.update_from_loval(ty, hi_args[in_idx[var]], *lo_vals)\n+  for v in jaxpr.jaxpr.invars:\n+    if v.final_qdd is not None:\n+      qdd = v.final_qdd\n+      lo_vals = it.islice(out_mut_, len(v.aval.lo_ty_qdd(qdd)))\n+      v.aval.update_from_loval(qdd, hi_args[in_idx[v]], *lo_vals)\n   assert next(out_mut_, None) is None\n \n   # collect output values into hi types\n@@ -1612,6 +1611,7 @@ def _converted_mutables_add_params(\n   return dict(params, donated_invars=donated_invars, in_shardings=in_shardings,\n               in_layouts=in_layouts)\n \n+\n def _lojax_expand_params(\n     nums_in, nums_out, muts_out, *, donated_invars, in_shardings, in_layouts,\n     out_shardings, out_layouts, **params):\n@@ -2014,13 +2014,6 @@ def pjit_staging_rule(trace, *args, **params):\n   else:\n     out_tracers = trace.default_process_primitive(pjit_p, args, params)\n \n-  trace.frame.is_high = jaxpr.jaxpr.is_high\n-  invars = [trace.frame.tracer_to_var[id(t)] for t in it.chain(args, consts)]\n-  var_map = dict(zip(jaxpr.jaxpr.invars, invars))\n-  final_env = {var_map[v]: ty for v, ty in\n-               jaxpr.jaxpr.final_typechange_env.items()}\n-  trace.frame.current_typechange_env.update(final_env)\n-\n   return out_tracers\n pe.custom_staging_rules[pjit_p] = pjit_staging_rule\n \ndiff --git a/tests/hijax_test.py b/tests/hijax_test.py\nindex 8b7d045c6c5a..b272e0aa8986 100644\n--- a/tests/hijax_test.py\n+++ b/tests/hijax_test.py\n@@ -213,7 +213,7 @@ def new_box():\n   return new_box_p.bind(treedef=treedef)\n \n def box_get(box):\n-  tys = box.type_state()\n+  tys = core.cur_qdd(box)\n   leaf_vals = box_get_p.bind(box, avals=tys.leaf_avals)\n   return jax.tree.unflatten(tys.treedef, leaf_vals)\n \n@@ -222,11 +222,11 @@ def box_set(box, val):\n   box_set_p.bind(box, *leaves, treedef=treedef)\n \n @dataclass(frozen=True)\n-class BoxTypeState:\n+class BoxTypeState(core.QuasiDynamicData):\n   leaf_avals: tuple[core.AbstractValue, ...]\n   treedef: PyTreeDef\n \n-  def to_tangent_aval(self):\n+  def to_tangent_qdd(self):\n     return BoxTypeState(tuple(a.to_tangent_aval() for a in self.leaf_avals),\n                         self.treedef)\n \n@@ -235,11 +235,12 @@ def normalize(self):\n                         self.treedef)\n \n class BoxTy(core.AbstractValue):\n-  mutable = True\n+  has_qdd = True\n \n   # forwarded to value\n   get = core.aval_method(box_get)\n   set = core.aval_method(box_set)\n+  type_state = core.aval_method(core.cur_qdd)\n \n   # aval interface: hashability and str_short\n   def __hash__(self): return hash(BoxTy)\n@@ -249,7 +250,7 @@ def str_short(self, short_dtypes=False):\n     return 'BoxTy'\n \n   # mutable interface\n-  def lo_ty_(self, box_state):\n+  def lo_ty_qdd(self, box_state):\n     return [lo_ty for t in box_state.leaf_avals for lo_ty in t.lo_ty()]\n \n   def new_from_loval(self, box_state: BoxTypeState, *lo_vals):\n@@ -285,6 +286,9 @@ def get(self):\n   def set(self, val):\n     box_set(self, val)\n \n+  def cur_qdd(self):\n+    return self.type_state()\n+\n   @property\n   def ty(self):\n     return BoxTy()\n@@ -299,15 +303,10 @@ def type_state(self):\n class NewBox(HiPrimitive):\n   def is_high(self, *, treedef) -> bool: return True\n \n-  def staging(self, trace, *, treedef):\n-    tracer = super().staging(trace, treedef=treedef)\n-    var = trace.frame.tracer_to_var[id(tracer)]\n-    leaves, treedef = jax.tree.flatten(None)\n-    trace.frame.current_typechange_env[var] = BoxTypeState(leaves, treedef)\n-    return tracer\n-\n   def abstract_eval(self, *, treedef):\n-    return BoxTy(), set()\n+    leaves, treedef = jax.tree.flatten(None)\n+    qdd = BoxTypeState(leaves, treedef)\n+    return core.AvalQDD(BoxTy(), qdd), set()\n \n   def to_lojax(_, *, treedef):\n     return Box(None)\n@@ -325,14 +324,8 @@ class BoxSet(HiPrimitive):\n \n   def is_high(self, *, treedef) -> bool: return True\n \n-  def staging(self, trace, box_tracer, *leaves, treedef):\n-    super().staging(trace, box_tracer, *leaves, treedef=treedef)\n-    var = trace.getvar(box_tracer)\n-    avals = tuple(t.aval for t in leaves)\n-    trace.frame.current_typechange_env[var] = BoxTypeState(avals, treedef)\n-    return []\n-\n   def abstract_eval(self, box_ty, *leaf_avals, treedef):\n+    box_ty.mutable_qdd.update(BoxTypeState(leaf_avals, treedef))\n     return [], set()  # TODO better typechecking...\n \n   def to_lojax(_, box, *leaves, treedef):\n@@ -375,6 +368,36 @@ def transpose(_, *args):\n \n class BoxTest(jtu.JaxTestCase):\n \n+  @parameterized.parameters([False, True])\n+  def test_qdd(self, jit):\n+\n+    val1 = 1.0\n+    val2 = jnp.arange(3)\n+\n+    box1 = Box(val1)\n+\n+    def f(box2):\n+      assert core.cur_qdd(box2).leaf_avals == (core.typeof(val1),)\n+      box2.set(val2)\n+      assert core.cur_qdd(box2).leaf_avals == (core.typeof(val2),)\n+\n+      box3 = new_box()\n+      box3.set(val2)\n+      assert core.cur_qdd(box3).leaf_avals == (core.typeof(val2),)\n+      box3.set(val1)\n+      assert core.cur_qdd(box3).leaf_avals == (core.typeof(val1),)\n+\n+      assert core.cur_qdd(box1).leaf_avals == (core.typeof(val1),)\n+      box1.set(val2)\n+      assert core.cur_qdd(box1).leaf_avals == (core.typeof(val2),)\n+\n+      return\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    f(Box(val1))\n+\n   def test_jit_arg(self):\n     @jax.jit\n     def f(box, x):\n@@ -470,6 +493,24 @@ def k(x):\n     ans = h(2.0)\n     self.assertAllClose(ans, 4.0)\n \n+  def test_jit_closure_nested3(self):\n+    box = new_box()\n+\n+    @jax.jit\n+    def h(x):\n+      box.set(x)\n+\n+      @jax.jit\n+      def k(x):\n+        box.set(box.get() + x)\n+\n+      k(1.0)\n+      k(1.0)\n+      return box.get()\n+\n+    ans = h(2.0)\n+    self.assertAllClose(ans, 4.0)\n+\n   @parameterized.parameters([False, True])\n   def test_jvp_closure_stop_gradient(self, jit):\n     box = Box(1.0)\n@@ -528,6 +569,7 @@ def f(x):\n       f = jax.jit(f)\n \n     jax.grad(f)(1.0)\n+\n     self.assertAllClose(box.get(), 2.0)\n \n   # TODO(mattjj,dougalm): make this work...\n@@ -587,6 +629,7 @@ def body(_, __):\n       double_it_10 = jax.jit(double_it_10)\n \n     double_it_10()\n+\n     self.assertAllClose(box.get(), 1024., check_dtypes=False)\n \n   # TODO error-checking tests from attrs_test.py\n@@ -674,7 +717,7 @@ class MyArray:\n \n     @dataclass(frozen=True)\n     class MyTy(core.AbstractValue):\n-      mutable = False\n+      has_qdd = False\n \n       def to_tangent_aval(self):\n         return MyTy()\n@@ -711,7 +754,7 @@ def f(box):\n \n \n class ListTy(core.AbstractValue):\n-  mutable = True\n+  has_qdd = True\n \n   # forwarded to value\n   get = core.aval_method(box_get)\n\n```",
        "from_id": [
            "dougalm"
        ]
    },
    {
        "text_input": "Merge pull request #29146 from cclauss:codespell\n\nPiperOrigin-RevId: 767199650",
        "output": "```diff\nCommit: 032afca925404261df9953633a83a8e1840a754c\nDate: 2025-06-04T17:31:56Z\nURL: https://github.com/jax-ml/jax/commit/032afca925404261df9953633a83a8e1840a754c\nFiles changed: 119\nAdditions: +209, Deletions: -209\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex afd15a357b48..c43cc7fb9b4e 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -30,7 +30,7 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n \n * Changes\n   * Additional checking for the versions of CUDA package dependencies was\n-    reenabled, having been accidentally disabled in a previous release.\n+    re-enabled, having been accidentally disabled in a previous release.\n   * JAX nightly packages are now published to artifact registry. To install\n     these packages, see the [JAX installation guide](https://docs.jax.dev/en/latest/installation.html#jax-nightly-installation).\n   * `jax.sharding.PartitionSpec` no longer inherits from a tuple.\n@@ -232,7 +232,7 @@ to signify this.\n       developers at this point. So it is difficult for us to fix this kind of\n       problem even if we wanted to.\n \n-    We are open to readding support for Mac x86 if the community is willing\n+    We are open to re-adding support for Mac x86 if the community is willing\n     to help support that platform: in particular, we would need the JAX test\n     suite to pass cleanly on Mac x86 before we could ship releases again.\n \n@@ -457,7 +457,7 @@ This is a patch release of jax 0.4.36. Only \"jax\" was released at this version.\n   * `jax_pmap_no_rank_reduction` flag is set to `True` by default.\n     * array[0] on a pmap result now introduces a reshape (use array[0:1]\n       instead).\n-    * The per-shard shape (accessable via jax_array.addressable_shards or\n+    * The per-shard shape (accessible via jax_array.addressable_shards or\n       jax_array.addressable_data(0)) now has a leading (1, ...). Update code\n       that directly accesses shards accordingly. The rank of the per-shard-shape\n       now matches that of the global shape which is the same behavior as jit.\n@@ -1513,7 +1513,7 @@ See the 0.4.33 release notes for more details.\n     dict of string stat names with int values, e.g. `\"bytes_in_use\"`, or None if\n     the platform doesn't support memory statistics. The exact stats returned may\n     vary across platforms. Currently only implemented on Cloud TPU.\n-  * Readded support for the Python buffer protocol (`memoryview`) on CPU\n+  * Re-added support for the Python buffer protocol (`memoryview`) on CPU\n     devices.\n \n ## jax 0.4.10 (May 11, 2023)\ndiff --git a/benchmarks/mosaic/matmul_bench.py b/benchmarks/mosaic/matmul_bench.py\nindex 32c147916407..fd3fcd6da315 100644\n--- a/benchmarks/mosaic/matmul_bench.py\n+++ b/benchmarks/mosaic/matmul_bench.py\n@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"Microbenchmarks for mosaic gpu matrix mutliplication.\"\"\"\n+\"\"\"Microbenchmarks for mosaic gpu matrix multiplication.\"\"\"\n \n import functools\n import sys\ndiff --git a/build/build.py b/build/build.py\nindex d059251552eb..40e02a100d98 100755\n--- a/build/build.py\n+++ b/build/build.py\n@@ -612,7 +612,7 @@ async def main():\n       wheel_build_command_base.append(\"--config=rocm\")\n       wheel_build_command_base.append(f\"--action_env=CLANG_COMPILER_PATH=\\\"{clang_path}\\\"\")\n     if args.rocm_path:\n-      logging.debug(\"ROCm tookit path: %s\", args.rocm_path)\n+      logging.debug(\"ROCm toolkit path: %s\", args.rocm_path)\n       wheel_build_command_base.append(f\"--action_env=ROCM_PATH=\\\"{args.rocm_path}\\\"\")\n     if args.rocm_amdgpu_targets:\n       logging.debug(\"ROCm AMD GPU targets: %s\", args.rocm_amdgpu_targets)\ndiff --git a/build/rocm/setup.rocm.sh b/build/rocm/setup.rocm.sh\nindex 3893d817e3a8..faa79d2ce1fd 100755\n--- a/build/rocm/setup.rocm.sh\n+++ b/build/rocm/setup.rocm.sh\n@@ -13,7 +13,7 @@ ROCM_BUILD_NAME=ubuntu\n ROCM_BUILD_NUM=main\n \n # Adjust the ROCM repo location\n-# Intial release don't have the trialing '.0'\n+# Initial release don't have the trialing '.0'\n # For example ROCM 5.7.0 is at https://repo.radeon.com/rocm/apt/5.7/\n if [ ${ROCM_VERSION##*[^0-9]} -eq '0' ]; then\n         ROCM_VERS=${ROCM_VERSION%.*}\ndiff --git a/build/rocm/tools/build_wheels.py b/build/rocm/tools/build_wheels.py\nindex a7ebdf86f916..3b3d697addc9 100644\n--- a/build/rocm/tools/build_wheels.py\n+++ b/build/rocm/tools/build_wheels.py\n@@ -227,7 +227,7 @@ def fix_wheel(path, jax_path):\n         env[\"PATH\"] = \"%s:%s\" % (py_bin, env[\"PATH\"])\n \n         # NOTE(mrodden): auditwheel 6.0 added lddtree module, but 6.3.0 changed\n-        # the fuction to ldd and also changed its behavior\n+        # the function to ldd and also changed its behavior\n         # constrain range to 6.0 to 6.2.x\n         cmd = [\"pip\", \"install\", \"auditwheel>=6,<6.3\"]\n         subprocess.run(cmd, check=True, env=env)\n@@ -325,7 +325,7 @@ def main():\n     shutil.rmtree(os.path.join(args.jax_path, \"jax.egg-info\"))\n     shutil.rmtree(os.path.join(args.jax_path, \"jax\", \"__pycache__\"))\n \n-    # Make the wheels deleteable by the runner\n+    # Make the wheels deletable by the runner\n     whl_house = os.path.join(args.jax_path, \"wheelhouse\")\n     logging.info(\"Changing permissions for %s\" % whl_house)\n     mode = 0o664\ndiff --git a/ci/envs/README.md b/ci/envs/README.md\nindex cf7a0c12fc9f..2a81d0f3240d 100644\n--- a/ci/envs/README.md\n+++ b/ci/envs/README.md\n@@ -9,7 +9,7 @@ Name                                        | Default Value\n ------------------------------------------- | ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -----\n `JAXCI_JAX_GIT_DIR`                         | Present working directory: `$(pwd)`      | Path to the JAX's Git directory.                                                                                                                                                                                                                                                                                                                             | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_JAX_GIT_DIR&type=code)\n `JAXCI_HERMETIC_PYTHON_VERSION`             | System default                           | Controls the version of hermetic Python to use. This affects the Bazel commands only such as when building artifacts or when running the Bazel test scripts.                                                                                                                                                                                                                                                                            | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_HERMETIC_PYTHON_VERSION&type=code)\n-`JAXCI_XLA_GIT_DIR`                         | Unset                                    | When using a local copy of XLA, this points to the root of the XLA git repoistory.                                                                                                                                                                                                                                                                           | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_XLA_GIT_DIR&type=code)\n+`JAXCI_XLA_GIT_DIR`                         | Unset                                    | When using a local copy of XLA, this points to the root of the XLA git repository.                                                                                                                                                                                                                                                                           | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_XLA_GIT_DIR&type=code)\n `JAXCI_CLONE_MAIN_XLA`                      | 0                                        | If set to 1, the XLA repository is cloned at HEAD and its path is set in `JAXCI_XLA_GIT_DIR`                                                                                                                                                                                                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_CLONE_MAIN_XLA&type=code)\n `JAXCI_XLA_COMMIT`                          | Unset                                    | Allows overriding the XLA commit that is used when using a local copy of XLA.                                                                                                                                                                                                                                                                                | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_XLA_COMMIT&type=code)\n `JAXCI_OUTPUT_DIR`                          | `$(pwd)/dist`                            | Controls the location where the artifacts are written to. The directory will be automatically created if it does not exist.                                                                                                                                                                                                                                  | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_OUTPUT_DIR&type=code)\n@@ -37,5 +37,5 @@ Name                                        | Default Value\n Name                    | Default Value                                                                                                | Behavior                                                                                             | Usage\n ----------------------- | ------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- | -----\n `JAXCI_DOCKER_WORK_DIR` | \"/jax\"                                                                                                       | The path on the container where the JAX Git repository is mounted to.                                | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_WORK_DIR&type=code)\n-`JAXCI_DOCKER_ARGS`     | Empty String                                                                                                 | Space seprated string of additional arguments that will be passed when starting the Docker container | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_ARGS&type=code)\n+`JAXCI_DOCKER_ARGS`     | Empty String                                                                                                 | Space separated string of additional arguments that will be passed when starting the Docker container | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_ARGS&type=code)\n `JAXCI_DOCKER_IMAGE`    | Depends on the system (see [ci/envs/docker.env](https://github.com/jax-ml/jax/blob/main/ci/envs/docker.env)) | Docker image to pull                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_IMAGE&type=code)\ndiff --git a/ci/envs/docker.env b/ci/envs/docker.env\nindex d556cb82d74d..cef2cda27bf4 100644\n--- a/ci/envs/docker.env\n+++ b/ci/envs/docker.env\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# This file contains all the docker specifc envs that are needed by the\n+# This file contains all the docker specific envs that are needed by the\n # ci/utilities/run_docker_container.sh script.\n \n os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/ci/utilities/install_wheels_locally.sh b/ci/utilities/install_wheels_locally.sh\nindex b1472d765c08..d66e1fea967b 100644\n--- a/ci/utilities/install_wheels_locally.sh\n+++ b/ci/utilities/install_wheels_locally.sh\n@@ -22,7 +22,7 @@ WHEELS=( $(/usr/bin/find \"$JAXCI_OUTPUT_DIR/\" -type f \\(  -name \"*jax*py3*\" -o -\n \n for i in \"${!WHEELS[@]}\"; do\n   if [[ \"${WHEELS[$i]}\" == *jax*py3*none*any.whl ]]; then\n-    # Apppend an extra to the end of the JAX wheel path to install those\n+    # Append an extra to the end of the JAX wheel path to install those\n     # packages as well from PyPI. E.g. jax[tpu] will install the libtpu package\n     # from PyPI. See ci/envs/README.md for more details.\n     if [[ -n \"$JAXCI_JAX_PYPI_EXTRAS\" ]]; then\ndiff --git a/ci/utilities/setup_build_environment.sh b/ci/utilities/setup_build_environment.sh\nindex 114acf2479ff..246665cd2f9f 100644\n--- a/ci/utilities/setup_build_environment.sh\n+++ b/ci/utilities/setup_build_environment.sh\n@@ -16,7 +16,7 @@\n # Set up the build environment for JAX CI jobs. This script depends on the\n # \"JAXCI_\" environment variables set or sourced in the build script.\n \n-# Pre-emptively mark the JAX git directory as safe. This is necessary for JAX CI\n+# Preemptively mark the JAX git directory as safe. This is necessary for JAX CI\n # jobs running on Linux runners in GitHub Actions. Without this, git complains\n # that the directory has dubious ownership and refuses to run any commands.\n # Avoid running on Windows runners as git runs into issues with not being able\ndiff --git a/docs/api_compatibility.md b/docs/api_compatibility.md\nindex dda86e2e5d31..985b2145c5c4 100644\n--- a/docs/api_compatibility.md\n+++ b/docs/api_compatibility.md\n@@ -96,7 +96,7 @@ guarantees of the main JAX package. If you have code that uses `jax.extend`,\n we would strongly recommend CI tests against JAX's nightly releases, so as to\n catch potential changes before they are released.\n \n-For details on `jax.extend`, see the [`jax.extend` module docuementation](https://docs.jax.dev/en/latest/jax.extend.html), or the design document, {ref}`jax-extend-jep`.\n+For details on `jax.extend`, see the [`jax.extend` module documentation](https://docs.jax.dev/en/latest/jax.extend.html), or the design document, {ref}`jax-extend-jep`.\n \n ## Numerics and randomness\n \ndiff --git a/docs/autodidax2_part1.ipynb b/docs/autodidax2_part1.ipynb\nindex 0a5a89c8ed98..7a58f54b16c8 100644\n--- a/docs/autodidax2_part1.ipynb\n+++ b/docs/autodidax2_part1.ipynb\n@@ -674,7 +674,7 @@\n     \"something is constant with respect to differentiation? It's tempting to say\\n\",\n     \"\\\"it's a constant if and only if it's not a dual number\\\". But actually dual\\n\",\n     \"numbers created by a *different* JVPInterpreter also need to be considered\\n\",\n-    \"constants with resepect to the JVPInterpreter we're currently handling. That's\\n\",\n+    \"constants with respect to the JVPInterpreter we're currently handling. That's\\n\",\n     \"why we need the `x.interpreter is self` check in `JVPInterpreter.lift`. This\\n\",\n     \"comes up in higher order differentiation when there are multiple JVPInterprers\\n\",\n     \"in scope. The sort of bug where you accidentally interpret a dual number from\\n\",\n@@ -1046,7 +1046,7 @@\n     \"That's it for part one of this tutorial. We've done two primitives, three\\n\",\n     \"interpreters and the tracing mechanism that weaves them together. In the next\\n\",\n     \"part we'll add types other than floats, error handling, compilation,\\n\",\n-    \"reverse-mode AD and higher-order primtives. Note that the second part is\\n\",\n+    \"reverse-mode AD and higher-order primitives. Note that the second part is\\n\",\n     \"structured differently. Rather than trying to have a top-to-bottom order that\\n\",\n     \"obeys both code dependencies (e.g. data structures need to be defined before\\n\",\n     \"they're used) and pedagogical dependencies (concepts need to be introduced\\n\",\ndiff --git a/docs/autodidax2_part1.md b/docs/autodidax2_part1.md\nindex 70dd0e4b696b..a4af594fb253 100644\n--- a/docs/autodidax2_part1.md\n+++ b/docs/autodidax2_part1.md\n@@ -348,7 +348,7 @@ There are some subtleties worth discussing. First, how do you tell if\n something is constant with respect to differentiation? It's tempting to say\n \"it's a constant if and only if it's not a dual number\". But actually dual\n numbers created by a *different* JVPInterpreter also need to be considered\n-constants with resepect to the JVPInterpreter we're currently handling. That's\n+constants with respect to the JVPInterpreter we're currently handling. That's\n why we need the `x.interpreter is self` check in `JVPInterpreter.lift`. This\n comes up in higher order differentiation when there are multiple JVPInterprers\n in scope. The sort of bug where you accidentally interpret a dual number from\n@@ -539,7 +539,7 @@ print(jvp(lambda x: eval_jaxpr(build_jaxpr(foo, 1), (x,)), 2.0, 1.0))\n That's it for part one of this tutorial. We've done two primitives, three\n interpreters and the tracing mechanism that weaves them together. In the next\n part we'll add types other than floats, error handling, compilation,\n-reverse-mode AD and higher-order primtives. Note that the second part is\n+reverse-mode AD and higher-order primitives. Note that the second part is\n structured differently. Rather than trying to have a top-to-bottom order that\n obeys both code dependencies (e.g. data structures need to be defined before\n they're used) and pedagogical dependencies (concepts need to be introduced\ndiff --git a/docs/autodidax2_part1.py b/docs/autodidax2_part1.py\nindex bfe59df359d3..44bf843c91b3 100644\n--- a/docs/autodidax2_part1.py\n+++ b/docs/autodidax2_part1.py\n@@ -307,7 +307,7 @@ def nth_order_derivative(n, f, x):\n # something is constant with respect to differentiation? It's tempting to say\n # \"it's a constant if and only if it's not a dual number\". But actually dual\n # numbers created by a *different* JVPInterpreter also need to be considered\n-# constants with resepect to the JVPInterpreter we're currently handling. That's\n+# constants with respect to the JVPInterpreter we're currently handling. That's\n # why we need the `x.interpreter is self` check in `JVPInterpreter.lift`. This\n # comes up in higher order differentiation when there are multiple JVPInterprers\n # in scope. The sort of bug where you accidentally interpret a dual number from\n@@ -483,7 +483,7 @@ def eval_atom(x): return env[x] if isinstance(x, Var) else x\n # That's it for part one of this tutorial. We've done two primitives, three\n # interpreters and the tracing mechanism that weaves them together. In the next\n # part we'll add types other than floats, error handling, compilation,\n-# reverse-mode AD and higher-order primtives. Note that the second part is\n+# reverse-mode AD and higher-order primitives. Note that the second part is\n # structured differently. Rather than trying to have a top-to-bottom order that\n # obeys both code dependencies (e.g. data structures need to be defined before\n # they're used) and pedagogical dependencies (concepts need to be introduced\ndiff --git a/docs/developer.md b/docs/developer.md\nindex cfb3f16cf649..1b50a9b65bc0 100644\n--- a/docs/developer.md\n+++ b/docs/developer.md\n@@ -374,7 +374,7 @@ in terms of files, not installations):\n    --repo_env=HERMETIC_PYTHON_URL=\"https://remote/url/to/my_python.tgz\"\n    --repo_env=HERMETIC_PYTHON_SHA256=<file's_sha256_sum>\n \n-   # We assume that top-level folder in the tarbal is called \"python\", if it is\n+   # We assume that top-level folder in the tarball is called \"python\", if it is\n    # something different just pass additional HERMETIC_PYTHON_PREFIX parameter\n    --repo_env=HERMETIC_PYTHON_URL=\"https://remote/url/to/my_python.tgz\"\n    --repo_env=HERMETIC_PYTHON_SHA256=<file's_sha256_sum>\ndiff --git a/docs/export/shape_poly.md b/docs/export/shape_poly.md\nindex 6b63a536ab48..68da231c4a68 100644\n--- a/docs/export/shape_poly.md\n+++ b/docs/export/shape_poly.md\n@@ -441,7 +441,7 @@ to {func}`jax.export.symbolic_shape` share a scope and\n can be mixed up in arithmetic operations. The result would\n also share the same scope.\n \n-You can re-use scopes:\n+You can reuse scopes:\n \n ```python\n >>> a, = export.symbolic_shape(\"a,\", constraints=(\"a >= 8\",))\ndiff --git a/docs/gpu_performance_tips.md b/docs/gpu_performance_tips.md\nindex bade464d22a1..c9034a515501 100644\n--- a/docs/gpu_performance_tips.md\n+++ b/docs/gpu_performance_tips.md\n@@ -93,7 +93,7 @@ export JAX_PGLE_AGGREGATION_PERCENTILE=85\n \n # Right now the auto PGLE profile collection doesn't work with command buffer.\n # If the command buffer is enabled, Auto PGLE will disable it during profile\n-# colletion and enable it back after the recompilation. If you need to have a\n+# collection and enable it back after the recompilation. If you need to have a\n # consistent command buffer logic with and with PGLE profile you can disable it\n # manually:\n export XLA_FLAGS=\"${XLA_FLAGS} --xla_gpu_enable_command_buffer=''\"\n@@ -371,7 +371,7 @@ def while_body(carry, i):\n       (NUM_DEVICES, 1, CONTRACTING_DIM_SIZE, NON_CONTRACTING_DIM_SIZE),\n   )\n \n-  # Colelctive permute on the \"back edge\" passes data to the first device.\n+  # Collective permute on the \"back edge\" passes data to the first device.\n   bwd_edge_data = cycle_back(bwd_edge_data)\n \n   # Update output buffer. We do this after reading from it to avoid the data\ndiff --git a/docs/index.rst b/docs/index.rst\nindex 07739c01c2fb..93fc6c284685 100644\n--- a/docs/index.rst\n+++ b/docs/index.rst\n@@ -108,7 +108,7 @@ numerical computing tools; the following is just a small sample of what is out t\n \n     .. grid-item:: :material-regular:`bar_chart;2em` **Probabilistic modeling**\n \n-       - `TensorFlow Probabilty`_\n+       - `TensorFlow Probability`_\n        - Distrax_\n \n     .. grid-item:: :material-outlined:`animation;2em` **Physics & simulation**\n@@ -199,4 +199,4 @@ maintains an up-to-date list.\n .. _Orbax: https://orbax.readthedocs.io/\n .. _PyMC: https://www.pymc.io/\n .. _TensorFlow Datasets: https://www.tensorflow.org/datasets\n-.. _TensorFlow Probabilty: https://www.tensorflow.org/probability\n+.. _TensorFlow Probability: https://www.tensorflow.org/probability\ndiff --git a/docs/notebooks/explicit-sharding.ipynb b/docs/notebooks/explicit-sharding.ipynb\nindex e1bee4b99fb5..37010b4ab3d3 100644\n--- a/docs/notebooks/explicit-sharding.ipynb\n+++ b/docs/notebooks/explicit-sharding.ipynb\n@@ -414,7 +414,7 @@\n     \"wherever types need to match. For example, the two sides of a `lax.cond` need to\\n\",\n     \"have results with matching shardings. And the carry of `lax.scan` needs to have the\\n\",\n     \"same sharding at the input and the output of the scan body. And when you\\n\",\n-    \"contruct a jaxpr without concrete arguments using `make_jaxpr` you need to\\n\",\n+    \"construct a jaxpr without concrete arguments using `make_jaxpr` you need to\\n\",\n     \"provide shardings too. Certain JAX transformations perform type-level\\n\",\n     \"operations. Automatic differentation constructs a tangent type for each primal\\n\",\n     \"type in the original computation (e.g. `TangentOf(float) == float`,\\n\",\ndiff --git a/docs/notebooks/explicit-sharding.md b/docs/notebooks/explicit-sharding.md\nindex 1402bca2415f..8989d426ffbc 100644\n--- a/docs/notebooks/explicit-sharding.md\n+++ b/docs/notebooks/explicit-sharding.md\n@@ -251,7 +251,7 @@ sharding is part of that type. This means that shardings need to match\n wherever types need to match. For example, the two sides of a `lax.cond` need to\n have results with matching shardings. And the carry of `lax.scan` needs to have the\n same sharding at the input and the output of the scan body. And when you\n-contruct a jaxpr without concrete arguments using `make_jaxpr` you need to\n+construct a jaxpr without concrete arguments using `make_jaxpr` you need to\n provide shardings too. Certain JAX transformations perform type-level\n operations. Automatic differentation constructs a tangent type for each primal\n type in the original computation (e.g. `TangentOf(float) == float`,\ndiff --git a/docs/notebooks/host-offloading.ipynb b/docs/notebooks/host-offloading.ipynb\nindex 9c806c2d56e7..f56cb90ff77e 100644\n--- a/docs/notebooks/host-offloading.ipynb\n+++ b/docs/notebooks/host-offloading.ipynb\n@@ -240,7 +240,7 @@\n    ],\n    \"source\": [\n     \"f = jax.jit(lambda x: x, out_shardings=s_dev)\\n\",\n-    \"out_host = f(arr_host)      # Input arrays in hte device memory while output arrays in the host memory\\n\",\n+    \"out_host = f(arr_host)      # Input arrays in the device memory while output arrays in the host memory\\n\",\n     \"print(\\\"Result value of D2H: \\\\n\\\", out_host)\"\n    ]\n   },\ndiff --git a/docs/notebooks/host-offloading.md b/docs/notebooks/host-offloading.md\nindex 7e113d40a4b3..cffe8b4340fe 100644\n--- a/docs/notebooks/host-offloading.md\n+++ b/docs/notebooks/host-offloading.md\n@@ -154,7 +154,7 @@ id: FjZzkxI8ky4r\n outputId: 2a1b6e7a-1c29-4347-c020-7b47c27a5cc3\n ---\n f = jax.jit(lambda x: x, out_shardings=s_dev)\n-out_host = f(arr_host)      # Input arrays in hte device memory while output arrays in the host memory\n+out_host = f(arr_host)      # Input arrays in the device memory while output arrays in the host memory\n print(\"Result value of D2H: \\n\", out_host)\n ```\n \ndiff --git a/docs/pallas/design/async_note.md b/docs/pallas/design/async_note.md\nindex 0fda9fe0a4e2..b255a91d3ec8 100644\n--- a/docs/pallas/design/async_note.md\n+++ b/docs/pallas/design/async_note.md\n@@ -464,7 +464,7 @@ def f(x):\n   return fori_loop(0, 8, body, x)\n ```\n \n-If you run the alias analysis, youll find that all of the buffers have been colored the same\\! Intuitively, this is problematic because if we are doing a loop of `ppermute`s, we cant write into the same buffer we are sending into. We generally need an extra (i.e. a double) buffer to receive, and then usually we will switch the send/recv buffers on the next iteration. What XLA will do in practice is that it will observe the buffer re-use and defensively insert a copy.\n+If you run the alias analysis, youll find that all of the buffers have been colored the same\\! Intuitively, this is problematic because if we are doing a loop of `ppermute`s, we cant write into the same buffer we are sending into. We generally need an extra (i.e. a double) buffer to receive, and then usually we will switch the send/recv buffers on the next iteration. What XLA will do in practice is that it will observe the buffer reuse and defensively insert a copy.\n \n ```py\n def f(x):\ndiff --git a/docs/pallas/design/design.md b/docs/pallas/design/design.md\nindex 17c7a6dbdc0f..53a5eb209510 100644\n--- a/docs/pallas/design/design.md\n+++ b/docs/pallas/design/design.md\n@@ -71,7 +71,7 @@ A JAX-based kernel language offers several advantages:\n * JAX as a tracing-based frontend for numerical computing is both\n   mature and well-used.\n   By embedding the kernel programming language in JAX itself,\n-  we can re-use JAXs tracing infrastructure and provide a\n+  we can reuse JAXs tracing infrastructure and provide a\n   NumPy-like frontend thats already familiar to users.\n * JAX transformations are key to its success, allowing users to\n   express simple programs but transform them to achieve complex\n@@ -551,7 +551,7 @@ along that dimension.\n `grad` of `pallas_call` enables automatic differentiation of kernels.\n `jax.grad` breaks down into applications of three distinct transforms:\n `jvp`, `partial_eval` and `transpose`.\n-In principle, we can re-use most of JAXs infrastructure when\n+In principle, we can reuse most of JAXs infrastructure when\n implementing these rules for `pallas_call` (since it behaves much like\n existing JAX higher order primitives).\n \ndiff --git a/docs/pallas/gpu/reference.md b/docs/pallas/gpu/reference.md\nindex 7b4a1e6e9c7d..1a4f39dff5f2 100644\n--- a/docs/pallas/gpu/reference.md\n+++ b/docs/pallas/gpu/reference.md\n@@ -30,7 +30,7 @@ the next instruction.\n <center><img alt=\"A diagram of one NVIDIA SM\" src=\"../../_static/pallas/gpu/nvidia_sm.svg\" style=\"width:60%; min-width: 400px;\"></center>\n \n Going further, recent CUDA versions also outline the concept of a _warpgroup_, which are\n-4 consecutive warps. Knowing how the hardware looks like, we can see where this is comming\n+4 consecutive warps. Knowing how the hardware looks like, we can see where this is coming\n from: 4 consecutive warps occupy the 4 quarters of an SM and let us issue instructions\n that utilize the whole SM.\n \n@@ -49,7 +49,7 @@ warps always run in lockstep (modulo the jitter from hardware scheduling) and ne\n different paths through control flow (with the small exception of `core_map` that we will\n discuss later). One notable addition here is that we still allow you to co-schedule multiple\n of those Pallas-level threads on the same SM so that they can cooperate and communicate\n-through shared memory (we relize that by putting them in the same CUDA block).\n+through shared memory (we realize that by putting them in the same CUDA block).\n \n ```{note}\n From now on, whenever we say \"thread\", we refer to the Pallas thread, not a CUDA thread/lane.\n@@ -329,7 +329,7 @@ transforms specified upon their allocation. For all currently supported generati\n the TensorCore requires the data to be laid out into row-major 2D tiles of shape\n `(8, swizzle_elems)`, where `swizzle_elems` is derived by dividing the swizzle by the\n element type bytewidth.  The currently supported swizzles are: 128, 64, and 32. Larger\n-swizzles are preferrable as they improve the performance of GMEM-to-SMEM copies.\n+swizzles are preferable as they improve the performance of GMEM-to-SMEM copies.\n \n ```python\n def mma_transforms(shape_dtype: jax.ShapeDtypeStruct):\ndiff --git a/docs/pallas/pipelining.md b/docs/pallas/pipelining.md\nindex a79876a0ca97..0ff9eaf5a24b 100644\n--- a/docs/pallas/pipelining.md\n+++ b/docs/pallas/pipelining.md\n@@ -34,7 +34,7 @@ import numpy as np\n <!-- #region id=\"shnVghWUSvpx\" -->\n ## Memory Hierarchies\n \n-The first step in understanding pipelining conceptually involves understanding the different forms of memory available and the tradeoffs between them. Most hardware architectures (including CPUs, GPUs, and TPUs) utilize a wide variety of memory spaces that tradeoff capicity vs latency/bandwidth. For the purpose of Pallas, we are typically interested in registers, SRAM, DRAM, and potentially network communication:\n+The first step in understanding pipelining conceptually involves understanding the different forms of memory available and the tradeoffs between them. Most hardware architectures (including CPUs, GPUs, and TPUs) utilize a wide variety of memory spaces that tradeoff capacity vs latency/bandwidth. For the purpose of Pallas, we are typically interested in registers, SRAM, DRAM, and potentially network communication:\n - **Registers** are the the memory physically closest to the processor, and typically values must be loaded directly into registers before doing any compute on them.\n - **SRAM** (also known as Shared Memory/L1 and L2 cache on GPUs, or VMEM on TPUs) also lives fairly close to the processor, but has larger capacity than registers.\n SRAM on modern ML accelerators typically range in the 10-100MB range (TPU v5p contains 96MB of VMEM, and H100 GPUs contain ~30MB of L1 cache and 50MB of L2).\ndiff --git a/docs/pallas/tpu/distributed.ipynb b/docs/pallas/tpu/distributed.ipynb\nindex ae82b7a80ac6..3ac1206bd14a 100644\n--- a/docs/pallas/tpu/distributed.ipynb\n+++ b/docs/pallas/tpu/distributed.ipynb\n@@ -71,7 +71,7 @@\n     \"\\n\",\n     \"![tpu_topologies](https://cloud.google.com/static/tpu/docs/images/v4-topologies.png)\\n\",\n     \"\\n\",\n-    \"Flattened as a graph, the torus can be visualized as follows. Each edge (orange or black) is a bidirectional connection between two devices. You will commonly hear about rings in conjunction with discussion about device toplogies  a key feature of a torus is that when taking a slice along an axis of the pod, such as the nodes `[(0,1), (1, 1), (2, 1), (3, 1)]` or `[(0, 1), (1, 1)]`, we have a ring of devices. This is a feature we can use to simplify communication patterns within the pod.\\n\",\n+    \"Flattened as a graph, the torus can be visualized as follows. Each edge (orange or black) is a bidirectional connection between two devices. You will commonly hear about rings in conjunction with discussion about device topologies  a key feature of a torus is that when taking a slice along an axis of the pod, such as the nodes `[(0,1), (1, 1), (2, 1), (3, 1)]` or `[(0, 1), (1, 1)]`, we have a ring of devices. This is a feature we can use to simplify communication patterns within the pod.\\n\",\n     \"\\n\",\n     \"![tpu_torus](https://cloud.google.com/static/tpu/docs/images/untwisted-tori.png)\"\n    ]\n@@ -477,7 +477,7 @@\n     \"id\": \"KgU7HI2pS4om\"\n    },\n    \"source\": [\n-    \"A detail worth mentioning here is the use of multiple receive semaphores. Because we only block on the receiving device, it is still possible for a sender to have sent multiple DMAs in flight before the receiver has finished processing the first one (see the next section and reduce-sum example which discusses race conditions in more detail). In this situation we may hit a situation where the same semaphore is being used for multiple DMAs occurring simultaneously. To avoid this, we allocate `num_devices-1` semaphores so there is no risk of re-use. While this race condition is unlikely to happen on such a small kernel, on larger kernels there is more chance for devices to fall out of sync and potentially cause a silent failure.\"\n+    \"A detail worth mentioning here is the use of multiple receive semaphores. Because we only block on the receiving device, it is still possible for a sender to have sent multiple DMAs in flight before the receiver has finished processing the first one (see the next section and reduce-sum example which discusses race conditions in more detail). In this situation we may hit a situation where the same semaphore is being used for multiple DMAs occurring simultaneously. To avoid this, we allocate `num_devices-1` semaphores so there is no risk of reuse. While this race condition is unlikely to happen on such a small kernel, on larger kernels there is more chance for devices to fall out of sync and potentially cause a silent failure.\"\n    ]\n   },\n   {\n@@ -529,7 +529,7 @@\n     \"\\n\",\n     \"In order to use regular semaphores, they can be allocated in the same way as a DMA semaphore, but by specifying `pltpu.SemaphoreType.REGULAR` rather than `pltpu.SemaphoreType.DMA`.\\n\",\n     \"\\n\",\n-    \"Semaphores must be zero at the end of a Pallas program to complete succesfully. There are two error cases where this may happen:\\n\",\n+    \"Semaphores must be zero at the end of a Pallas program to complete successfully. There are two error cases where this may happen:\\n\",\n     \" - If a semaphore is over-signaled, the program will end with non-zero (>0) semaphores. In this case, the program will crash upon completion. This is useful for debugging as non-zero semaphores typically means there is a bug somewhere inside of the program.\\n\",\n     \" - If a semaphore is over-waited, the program will hang on the blocking `semaphore_wait` call while it waits for the semaphore to be incremented. In this case the device or program will need to be restarted.\\n\",\n     \"\\n\",\n@@ -644,7 +644,7 @@\n     \"\\n\",\n     \"The main body assumes that a value has already been copied into our local working slot, either from the previous iteration or from the prologue. A complicating factor is that our destination buffers live in HBM, but we need to load values to VMEM before we perform arithmetic. Therefore, we simultaneously copy the working slot value into our VMEM (`receive_scratch`) and pass the value on to our right neighbor's receiving slot. Once the value has been copied into our VMEM, we can accumulate it into our result (contained in `o_ref`).\\n\",\n     \"\\n\",\n-    \"A subtle race condition can occur if one device runs one loop ahead of it's right neighbor. In this case, it could copy into the receiver's `working_slot` at the same time the receiver is reading from it. In order to avoid this, each device will block on a `REGULAR` semaphore before copying into the right neighbor's `dst_ref` until it has signaled that it is done reading from its `working_slot`. This race condition is rarely triggered for a small kernel such as this example, but can it can be explicitly triggered if for example using a `pltpu.delay` instruction to artifically hang a device.\\n\",\n+    \"A subtle race condition can occur if one device runs one loop ahead of it's right neighbor. In this case, it could copy into the receiver's `working_slot` at the same time the receiver is reading from it. In order to avoid this, each device will block on a `REGULAR` semaphore before copying into the right neighbor's `dst_ref` until it has signaled that it is done reading from its `working_slot`. This race condition is rarely triggered for a small kernel such as this example, but can it can be explicitly triggered if for example using a `pltpu.delay` instruction to artificially hang a device.\\n\",\n     \"\\n\",\n     \"Note that this is not an optimal or fully general kernel, as the block sizes must entirely fit in VMEM and we could better interleave communication and accumulation. We will discuss these optimizations in later sections.\"\n    ]\n@@ -691,7 +691,7 @@\n     \"  \\\"\\\"\\\"Performs a barrier with neighbors on the global barrier semaphore.\\n\",\n     \"\\n\",\n     \"  Optionally performs a second barrier, which prevents a potential race\\n\",\n-    \"  when re-using the same collective_id across kernel invocations.\\n\",\n+    \"  when reusing the same collective_id across kernel invocations.\\n\",\n     \"  \\\"\\\"\\\"\\n\",\n     \"  barrier_sem = pltpu.get_barrier_semaphore()\\n\",\n     \"  for neighbor in [left_neighbor, right_neighbor]:\\n\",\n@@ -1701,7 +1701,7 @@\n     \"\\n\",\n     \"### Next Steps\\n\",\n     \"\\n\",\n-    \"Excellent follow-up excercises for the reader could include implementing a distributed matrix multiplication, implementing `lax.all_to_all`, and relaxing synchronization to allow for additional run-ahead.\"\n+    \"Excellent follow-up exercises for the reader could include implementing a distributed matrix multiplication, implementing `lax.all_to_all`, and relaxing synchronization to allow for additional run-ahead.\"\n    ]\n   }\n  ],\ndiff --git a/docs/pallas/tpu/distributed.md b/docs/pallas/tpu/distributed.md\nindex b16116549972..19b336005c28 100644\n--- a/docs/pallas/tpu/distributed.md\n+++ b/docs/pallas/tpu/distributed.md\n@@ -61,7 +61,7 @@ TPUs pods are typically arranged in an ND torus topology. The following graphic\n \n ![tpu_topologies](https://cloud.google.com/static/tpu/docs/images/v4-topologies.png)\n \n-Flattened as a graph, the torus can be visualized as follows. Each edge (orange or black) is a bidirectional connection between two devices. You will commonly hear about rings in conjunction with discussion about device toplogies  a key feature of a torus is that when taking a slice along an axis of the pod, such as the nodes `[(0,1), (1, 1), (2, 1), (3, 1)]` or `[(0, 1), (1, 1)]`, we have a ring of devices. This is a feature we can use to simplify communication patterns within the pod.\n+Flattened as a graph, the torus can be visualized as follows. Each edge (orange or black) is a bidirectional connection between two devices. You will commonly hear about rings in conjunction with discussion about device topologies  a key feature of a torus is that when taking a slice along an axis of the pod, such as the nodes `[(0,1), (1, 1), (2, 1), (3, 1)]` or `[(0, 1), (1, 1)]`, we have a ring of devices. This is a feature we can use to simplify communication patterns within the pod.\n \n ![tpu_torus](https://cloud.google.com/static/tpu/docs/images/untwisted-tori.png)\n \n@@ -409,7 +409,7 @@ print('Difference |Pallas - lax.all_gather| = ',\n \n +++ {\"id\": \"KgU7HI2pS4om\"}\n \n-A detail worth mentioning here is the use of multiple receive semaphores. Because we only block on the receiving device, it is still possible for a sender to have sent multiple DMAs in flight before the receiver has finished processing the first one (see the next section and reduce-sum example which discusses race conditions in more detail). In this situation we may hit a situation where the same semaphore is being used for multiple DMAs occurring simultaneously. To avoid this, we allocate `num_devices-1` semaphores so there is no risk of re-use. While this race condition is unlikely to happen on such a small kernel, on larger kernels there is more chance for devices to fall out of sync and potentially cause a silent failure.\n+A detail worth mentioning here is the use of multiple receive semaphores. Because we only block on the receiving device, it is still possible for a sender to have sent multiple DMAs in flight before the receiver has finished processing the first one (see the next section and reduce-sum example which discusses race conditions in more detail). In this situation we may hit a situation where the same semaphore is being used for multiple DMAs occurring simultaneously. To avoid this, we allocate `num_devices-1` semaphores so there is no risk of reuse. While this race condition is unlikely to happen on such a small kernel, on larger kernels there is more chance for devices to fall out of sync and potentially cause a silent failure.\n \n +++ {\"id\": \"EDCmAaHVtY7x\"}\n \n@@ -451,7 +451,7 @@ def semaphore_read(\n \n In order to use regular semaphores, they can be allocated in the same way as a DMA semaphore, but by specifying `pltpu.SemaphoreType.REGULAR` rather than `pltpu.SemaphoreType.DMA`.\n \n-Semaphores must be zero at the end of a Pallas program to complete succesfully. There are two error cases where this may happen:\n+Semaphores must be zero at the end of a Pallas program to complete successfully. There are two error cases where this may happen:\n  - If a semaphore is over-signaled, the program will end with non-zero (>0) semaphores. In this case, the program will crash upon completion. This is useful for debugging as non-zero semaphores typically means there is a bug somewhere inside of the program.\n  - If a semaphore is over-waited, the program will hang on the blocking `semaphore_wait` call while it waits for the semaphore to be incremented. In this case the device or program will need to be restarted.\n \n@@ -556,7 +556,7 @@ The prologue (executed when `outer_step==0`) first initiates a barrier with both\n \n The main body assumes that a value has already been copied into our local working slot, either from the previous iteration or from the prologue. A complicating factor is that our destination buffers live in HBM, but we need to load values to VMEM before we perform arithmetic. Therefore, we simultaneously copy the working slot value into our VMEM (`receive_scratch`) and pass the value on to our right neighbor's receiving slot. Once the value has been copied into our VMEM, we can accumulate it into our result (contained in `o_ref`).\n \n-A subtle race condition can occur if one device runs one loop ahead of it's right neighbor. In this case, it could copy into the receiver's `working_slot` at the same time the receiver is reading from it. In order to avoid this, each device will block on a `REGULAR` semaphore before copying into the right neighbor's `dst_ref` until it has signaled that it is done reading from its `working_slot`. This race condition is rarely triggered for a small kernel such as this example, but can it can be explicitly triggered if for example using a `pltpu.delay` instruction to artifically hang a device.\n+A subtle race condition can occur if one device runs one loop ahead of it's right neighbor. In this case, it could copy into the receiver's `working_slot` at the same time the receiver is reading from it. In order to avoid this, each device will block on a `REGULAR` semaphore before copying into the right neighbor's `dst_ref` until it has signaled that it is done reading from its `working_slot`. This race condition is rarely triggered for a small kernel such as this example, but can it can be explicitly triggered if for example using a `pltpu.delay` instruction to artificially hang a device.\n \n Note that this is not an optimal or fully general kernel, as the block sizes must entirely fit in VMEM and we could better interleave communication and accumulation. We will discuss these optimizations in later sections.\n \n@@ -585,7 +585,7 @@ def local_barrier(left_neighbor, right_neighbor, double_barrier=True):\n   \"\"\"Performs a barrier with neighbors on the global barrier semaphore.\n \n   Optionally performs a second barrier, which prevents a potential race\n-  when re-using the same collective_id across kernel invocations.\n+  when reusing the same collective_id across kernel invocations.\n   \"\"\"\n   barrier_sem = pltpu.get_barrier_semaphore()\n   for neighbor in [left_neighbor, right_neighbor]:\n@@ -1514,4 +1514,4 @@ In this tutorial we covered several kernel examples which replicate the function\n \n ### Next Steps\n \n-Excellent follow-up excercises for the reader could include implementing a distributed matrix multiplication, implementing `lax.all_to_all`, and relaxing synchronization to allow for additional run-ahead.\n+Excellent follow-up exercises for the reader could include implementing a distributed matrix multiplication, implementing `lax.all_to_all`, and relaxing synchronization to allow for additional run-ahead.\ndiff --git a/docs/pallas/tpu/sparse.ipynb b/docs/pallas/tpu/sparse.ipynb\nindex ac3a0dad2404..31cfa8eeb328 100644\n--- a/docs/pallas/tpu/sparse.ipynb\n+++ b/docs/pallas/tpu/sparse.ipynb\n@@ -491,7 +491,7 @@\n    \"source\": [\n     \"def sparsify_mask(mask: jax.Array,\\n\",\n     \"                  block_shape: tuple[int, int]):\\n\",\n-    \"  \\\"\\\"\\\"Preprocesses a mask into a sparse reprentation.\\n\",\n+    \"  \\\"\\\"\\\"Preprocesses a mask into a sparse representation.\\n\",\n     \"\\n\",\n     \"  Args:\\n\",\n     \"    mask: A boolean array of shape [M, N]\\n\",\ndiff --git a/docs/pallas/tpu/sparse.md b/docs/pallas/tpu/sparse.md\nindex 113f31d8bab2..35613acdb2c9 100644\n--- a/docs/pallas/tpu/sparse.md\n+++ b/docs/pallas/tpu/sparse.md\n@@ -391,7 +391,7 @@ As we will be working with a sparse mask, we will begin by implementing a functi\n \n def sparsify_mask(mask: jax.Array,\n                   block_shape: tuple[int, int]):\n-  \"\"\"Preprocesses a mask into a sparse reprentation.\n+  \"\"\"Preprocesses a mask into a sparse representation.\n \n   Args:\n     mask: A boolean array of shape [M, N]\ndiff --git a/docs/persistent_compilation_cache.md b/docs/persistent_compilation_cache.md\nindex e241e76e3c5f..d795a054bc87 100644\n--- a/docs/persistent_compilation_cache.md\n+++ b/docs/persistent_compilation_cache.md\n@@ -260,7 +260,7 @@ If we were to merely compile this function without shard_map, the cache key for\n layernorm_matmul_without_shard_map = jax.jit(F, in_shardings=(...), out_sharding=(...))(x1, x2, gamma, beta)\n ```\n \n-However, if we were to wrap the layernorm primitive in shard_map and define a function G that performs the same computation, the cache key for `layernorm_matmul_with_shard_map` will be the same everytime despite `LayerNorm` being implementing `custom_partitioning`:\n+However, if we were to wrap the layernorm primitive in shard_map and define a function G that performs the same computation, the cache key for `layernorm_matmul_with_shard_map` will be the same every time despite `LayerNorm` being implementing `custom_partitioning`:\n \n ```python\n import jax\ndiff --git a/docs/random-numbers.md b/docs/random-numbers.md\nindex 134b690839e0..5562dc3f43d5 100644\n--- a/docs/random-numbers.md\n+++ b/docs/random-numbers.md\n@@ -150,7 +150,7 @@ print(random.normal(key))\n print(random.normal(key))\n ```\n \n-Re-using the same key, even with different {mod}`~jax.random` APIs, can result in correlated outputs, which is generally undesirable. \n+Reusing the same key, even with different {mod}`~jax.random` APIs, can result in correlated outputs, which is generally undesirable. \n \n **The rule of thumb is: never reuse keys (unless you want identical outputs). Reusing the same state will cause __sadness__ and __monotony__, depriving the end user of __lifegiving chaos__.**\n \ndiff --git a/examples/ffi/src/jax_ffi_example/rms_norm.py b/examples/ffi/src/jax_ffi_example/rms_norm.py\nindex 5ba97f48ebad..996eb9e5d935 100644\n--- a/examples/ffi/src/jax_ffi_example/rms_norm.py\n+++ b/examples/ffi/src/jax_ffi_example/rms_norm.py\n@@ -16,7 +16,7 @@\n This example is exactly the same as the one in the `FFI tutorial\n <https://docs.jax.dev/en/latest/ffi.html>`, so more details can be found\n on that page. But, the high level summary is that we implement our custom\n-extension in ``rms_norm.cc``, then call it usin ``jax.ffi.ffi_call`` in\n+extension in ``rms_norm.cc``, then call it using ``jax.ffi.ffi_call`` in\n this module. The behavior under autodiff is implemented using\n ``jax.custom_vjp``.\n \"\"\"\ndiff --git a/jax/_src/api_util.py b/jax/_src/api_util.py\nindex 2e7ba551c624..5261764d0bf8 100644\n--- a/jax/_src/api_util.py\n+++ b/jax/_src/api_util.py\n@@ -606,7 +606,7 @@ def debug_info(\n   \"\"\"Constructd core.DebugInfo for a function given example args and kwargs.\n \n   `args` and `kwargs` are example positional and keyword arguments, users with\n-  `inspect.Signature` to get the names of argments. The arguments that are\n+  `inspect.Signature` to get the names of arguments. The arguments that are\n   considered static for tracing purposes should be included, and designated\n   using `static_argnums` and `static_argnames`.\n \ndiff --git a/jax/_src/cache_key.py b/jax/_src/cache_key.py\nindex 6fe3d8819d3c..906e686727ef 100644\n--- a/jax/_src/cache_key.py\n+++ b/jax/_src/cache_key.py\n@@ -56,7 +56,7 @@ def get_flag_prefixes() -> list[str]:\n def custom_hook() -> str:\n   \"\"\"Custom hook for any addition to the cache key.\n \n-  The custom hook will be called everytime get() is called and can be\n+  The custom hook will be called every time get() is called and can be\n   defined to return a string that will be hashed into the cache key.\n   \"\"\"\n   return \"\"\ndiff --git a/jax/_src/clusters/cluster.py b/jax/_src/clusters/cluster.py\nindex 69ef77a6421d..1c0a6fca9df6 100644\n--- a/jax/_src/clusters/cluster.py\n+++ b/jax/_src/clusters/cluster.py\n@@ -23,7 +23,7 @@\n class ClusterEnv:\n   \"\"\"Interface for defining a cluster environment.\n \n-  To enable auto bootrapping (aka :func:`jax.distributed.initialize()`),\n+  To enable auto bootstrapping (aka :func:`jax.distributed.initialize()`),\n   cluster environments need to derive from :class:`ClusterEnv` and implement\n   :func:`is_env_present`, :func:`get_coordinator_address`,\n   :func:`get_process_count`, and :func:`get_process_id`.\ndiff --git a/jax/_src/clusters/k8s_cluster.py b/jax/_src/clusters/k8s_cluster.py\nindex af1b7c020eed..fb312038bf2c 100644\n--- a/jax/_src/clusters/k8s_cluster.py\n+++ b/jax/_src/clusters/k8s_cluster.py\n@@ -78,7 +78,7 @@ def is_env_present(cls) -> bool:\n             textwrap.fill(\n               \"Kubernetes environment detected, but the `kubernetes` package \"\n               \"is not installed to enable automatic bootstrapping in this \"\n-              \"environment. To enable automatic boostrapping, please install \"\n+              \"environment. To enable automatic bootstrapping, please install \"\n               \"jax with the [k8s] extra. For example:\"),\n             \"    pip install jax[k8s]\",\n             \"    pip install jax[k8s,<MORE-EXTRAS...>]\",\ndiff --git a/jax/_src/cudnn/fused_attention_stablehlo.py b/jax/_src/cudnn/fused_attention_stablehlo.py\nindex e15d9ddbce09..c7ae830e92fd 100644\n--- a/jax/_src/cudnn/fused_attention_stablehlo.py\n+++ b/jax/_src/cudnn/fused_attention_stablehlo.py\n@@ -357,7 +357,7 @@ def check_is_flash_attention(\n         H_max = 256 if cudnn_version >= 90500 and is_on_hopper else 128\n         if not (H <= H_max and H % 8 == 0):\n           raise NotImplementedError(\n-              f\"The head dim must be <= {H_max} and a mutiple of 8, \"\n+              f\"The head dim must be <= {H_max} and a multiple of 8, \"\n               f\"but got {H}.\"\n           )\n \n@@ -1849,7 +1849,7 @@ def dot_product_attention(\n         # should be broadcast to same shape\n         bias = bias + mask\n \n-    # check if input shape and data type is compatiable\n+    # check if input shape and data type is compatible\n     check_layout(query, key, value, bias, q_seqlen, kv_seqlen, q_offsets, kv_offsets, layout)\n     has_bias = bias is not None\n     has_dbias = has_bias and \\\ndiff --git a/jax/_src/custom_batching.py b/jax/_src/custom_batching.py\nindex 338074837ea5..83c9ffb5ee36 100644\n--- a/jax/_src/custom_batching.py\n+++ b/jax/_src/custom_batching.py\n@@ -103,7 +103,7 @@ class custom_vmap:\n     >>> jax.grad(f)(jnp.zeros(()), jnp.ones(()))\n     Array(1., dtype=float32)\n \n-  Note that the :py:class:`jax.custom_vjp` must be on the ouside, wrapping the\n+  Note that the :py:class:`jax.custom_vjp` must be on the outside, wrapping the\n   ``custom_vmap``-decorated function.\n   \"\"\"\n \ndiff --git a/jax/_src/custom_dce.py b/jax/_src/custom_dce.py\nindex d336c969a3c4..25fe604085fd 100644\n--- a/jax/_src/custom_dce.py\n+++ b/jax/_src/custom_dce.py\n@@ -251,9 +251,9 @@ def flatten_dce_rule(\n   # For error checking purposes, we need to reformat the pytree structure\n   # of the output of the DCE rule to match the original output. The catch is\n   # that the DCE rule can return a None to indicated an unused subtree, so we\n-  # need to rebuild those subtrees with a sentinal value at the leaves. This\n+  # need to rebuild those subtrees with a sentinel value at the leaves. This\n   # logic is very similar to what is used in custom_dervatives._flatten_bwd.\n-  sentinal = object()\n+  sentinel = object()\n   dummy = tree_util.tree_unflatten(out_tree, [object()] * out_tree.num_leaves)\n   keypaths, _ = util.unzip2(tree_util.tree_flatten_with_path(dummy)[0])\n   out_flat = []\n@@ -261,7 +261,7 @@ def flatten_dce_rule(\n   def append(x, d):\n     num_leaves = len(tree_util.tree_flatten(d)[0])\n     if x is None and d is not None:\n-      out_flat.extend([sentinal] * num_leaves)\n+      out_flat.extend([sentinel] * num_leaves)\n     elif x is not None:\n       out_flat.extend([x] * num_leaves)\n     return x\n@@ -281,7 +281,7 @@ def append(x, d):\n   for kp, used, aval, val in zip(keypaths, used_outs, out_avals, out_flat):\n     if not used:\n       continue\n-    if val is sentinal:\n+    if val is sentinel:\n       raise ValueError(\n           f\"Custom DCE rule {rule_name} for function {fun_name} must produce \"\n           \"values for all of the required outputs (as specified by the \"\ndiff --git a/jax/_src/custom_partitioning_sharding_rule.py b/jax/_src/custom_partitioning_sharding_rule.py\nindex d17399beda5b..bc27f34b3bfb 100644\n--- a/jax/_src/custom_partitioning_sharding_rule.py\n+++ b/jax/_src/custom_partitioning_sharding_rule.py\n@@ -138,12 +138,12 @@ def __init__(self, operand_mappings: tuple[ArrayMapping, ...],\n     # Check that factors that are used for a whole dimension aren't in\n     # factor_sizes and factors that are never used for a whole dimension are\n     # in factor_sizes.\n-    for factor, inferrable in factors_inferrable.items():\n-      if factor not in factor_sizes and not inferrable:\n+    for factor, inferable in factors_inferrable.items():\n+      if factor not in factor_sizes and not inferable:\n         raise ValueError(\n           f\"Factor {factor} is only used in compound factors; must specify\"\n           \" its size\")\n-      if factor in factor_sizes and inferrable:\n+      if factor in factor_sizes and inferable:\n         raise ValueError(\n           f\"Factor {factor} represents a whole dimension; do not specify its\"\n           \" size\")\ndiff --git a/jax/_src/dlpack.py b/jax/_src/dlpack.py\nindex 40a69d1e0390..1f19ac0f45c0 100644\n--- a/jax/_src/dlpack.py\n+++ b/jax/_src/dlpack.py\n@@ -130,7 +130,7 @@ def to_dlpack(x: Array, stream: int | Any | None = None,\n       ) from None\n \n   # As new versions are adopted over time, we can maintain some legacy paths\n-  # for compatability mediated through the max_version parameter.\n+  # for compatibility mediated through the max_version parameter.\n   # TODO(micky774): Deprecate default usage of DLPackManagedTensor when XLA\n   # supports DLManagedTensorVersioned (DLPack version 1.0) and repurpose the\n   # current _to_dlpack as a legacy path for (0,5) <= max_version < (1,0).\ndiff --git a/jax/_src/errors.py b/jax/_src/errors.py\nindex 20b82f629f6f..a548714869ab 100644\n--- a/jax/_src/errors.py\n+++ b/jax/_src/errors.py\n@@ -503,7 +503,7 @@ class TracerBoolConversionError(ConcretizationTypeError):\n \n     In this case, the error occurs because Python's built-in ``min`` function is not\n     compatible with JAX transforms. This can be fixed by replacing it with\n-    ``jnp.minumum``:\n+    ``jnp.minimum``:\n \n       >>> @jit\n       ... def func(x):\ndiff --git a/jax/_src/export/shape_poly.py b/jax/_src/export/shape_poly.py\nindex 31371cf345a1..bb8a159ee54b 100644\n--- a/jax/_src/export/shape_poly.py\n+++ b/jax/_src/export/shape_poly.py\n@@ -978,7 +978,7 @@ def cmp_sequence(s1, s2, elem_cmp) -> int:\n \n \n class SymbolicScope:\n-  \"\"\"Indentifies a scope for symbolic expressions.\n+  \"\"\"Identifies a scope for symbolic expressions.\n \n   All symbolic expressions that interact (e.g., appear in the argument shapes\n   for one JAX function invocation, or are involved in arithmetic operations)\ndiff --git a/jax/_src/ffi.py b/jax/_src/ffi.py\nindex 3bfe8130ccda..db943d675b80 100644\n--- a/jax/_src/ffi.py\n+++ b/jax/_src/ffi.py\n@@ -56,7 +56,7 @@ def register_ffi_target(\n     name: the name of the target.\n     fn: a ``PyCapsule`` object containing the function pointer, or a ``dict``\n       where the keys are FFI stage names (e.g. `\"execute\"`) and the values are\n-      ``PyCapsule`` objects continaing a pointer to the handler for that stage.\n+      ``PyCapsule`` objects containing a pointer to the handler for that stage.\n     platform: the target platform.\n     api_version: the XLA custom call API version to use. Supported versions are:\n       1 (default) for the typed FFI or 0 for the earlier \"custom call\" API.\n@@ -369,7 +369,7 @@ def ffi_call(\n \n   Like :func:`~jax.pure_callback`, the behavior of ``ffi_call`` under\n   :func:`~jax.vmap` depends on the value of ``vmap_method``. See the\n-  :func:`~jax.pure_callback` documenation for more details about the allowed\n+  :func:`~jax.pure_callback` documentation for more details about the allowed\n   values and examples of their behavior.\n \n   The current default behavior is to use ``vmap_method=\"sequential\"`` when\ndiff --git a/jax/_src/internal_test_util/export_back_compat_test_util.py b/jax/_src/internal_test_util/export_back_compat_test_util.py\nindex b86b24e2b4fc..7b4af36e5dc4 100644\n--- a/jax/_src/internal_test_util/export_back_compat_test_util.py\n+++ b/jax/_src/internal_test_util/export_back_compat_test_util.py\n@@ -321,7 +321,7 @@ def ndarray_to_aval(a: np.ndarray) -> core.ShapedArray:\n     in_avals_tree = tree_util.tree_map(ndarray_to_aval, args_specs)\n     # TODO: we ought to ensure that out_avals are polymorphic if need be. We\n     # could either save the in/out_avals (but we need to first implement that\n-    # support in export), or we can just re-use them from the current\n+    # support in export), or we can just reuse them from the current\n     # exported.\n     out_avals_tree = tree_util.tree_map(ndarray_to_aval, data.expected_outputs)\n     # in_tree must be for (args, kwargs)\ndiff --git a/jax/_src/interpreters/mlir.py b/jax/_src/interpreters/mlir.py\nindex 6bad0ee2a018..0864ec8646c9 100644\n--- a/jax/_src/interpreters/mlir.py\n+++ b/jax/_src/interpreters/mlir.py\n@@ -706,7 +706,7 @@ class ModuleContext:\n   # Cached primitive lowerings.\n   cached_primitive_lowerings: dict[Any, func_dialect.FuncOp]\n \n-  # Cached traceback infromation.\n+  # Cached traceback information.\n   traceback_caches: TracebackCaches\n \n   lowering_parameters: LoweringParameters\ndiff --git a/jax/_src/jaxpr_util.py b/jax/_src/jaxpr_util.py\nindex a6c93c8c120c..cb9eef0b9ea2 100644\n--- a/jax/_src/jaxpr_util.py\n+++ b/jax/_src/jaxpr_util.py\n@@ -233,7 +233,7 @@ def jaxpr_and_binder_in_params(params, index: int) -> Iterator[tuple[core.Jaxpr,\n \n def eqns_using_var(jaxpr: core.Jaxpr, invar: core.Var) -> Iterator[core.JaxprEqn]:\n   \"\"\"Find the leaf equations using a variable\"\"\"\n-  # The complexity of this call is becauase the invar might originate from a nested jaxpr\n+  # The complexity of this call is because the invar might originate from a nested jaxpr\n   for eqn, invar_index in eqns_using_var_with_invar_index(jaxpr, invar):\n     if (child_jaxprs_and_vars := tuple(jaxpr_and_binder_in_params(eqn.params, invar_index))):\n       for (jaxpr, invar) in child_jaxprs_and_vars:\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 985c5ba52294..ad09292731cf 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -198,7 +198,7 @@ def scan(f, init, xs, length=None):\n       a single iteration of a loop. If an integer is provided, it determines how\n       many unrolled loop iterations to run within a single rolled iteration of\n       the loop. If a boolean is provided, it will determine if the loop is\n-      competely unrolled (i.e. `unroll=True`) or left completely rolled (i.e.\n+      completely unrolled (i.e. `unroll=True`) or left completely rolled (i.e.\n       `unroll=False`).\n     _split_transpose: experimental optional bool specifying whether to further\n       split the transpose into a scan (computing activation gradients), and a\n@@ -2427,7 +2427,7 @@ def fori_loop(lower, upper, body_fun, init_val):\n     unroll: An optional integer or boolean that determines how much to unroll\n       the loop. If an integer is provided, it determines how many unrolled\n       loop iterations to run within a single rolled iteration of the loop. If a\n-      boolean is provided, it will determine if the loop is competely unrolled\n+      boolean is provided, it will determine if the loop is completely unrolled\n       (i.e. `unroll=True`) or left completely unrolled (i.e. `unroll=False`).\n       This argument is only applicable if the loop bounds are statically known.\n \ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex 389960a7c94d..b644bdefecc2 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -935,7 +935,7 @@ def integer_pow(x: ArrayLike, y: int) -> Array:\n     An array of the same shape and dtype as ``x`` containing the elementwise power.\n \n   See also:\n-    :func:`jax.lax.pow`: Elementwise pwoer where ``y`` is an array.\n+    :func:`jax.lax.pow`: Elementwise power where ``y`` is an array.\n \n   .. _stablehlo.multiply: https://openxla.org/stablehlo/spec#multiply\n   \"\"\"\n@@ -2102,7 +2102,7 @@ class DotAlgorithm(NamedTuple):\n \n   The `StableHLO spec <https://openxla.org/stablehlo/spec#dot_general>`_ for\n   the dot operation doesn't require that the precision types be the same as the\n-  storage types for the inputs or outputs, but some plaforms may require that\n+  storage types for the inputs or outputs, but some platforms may require that\n   these types match. Furthermore, the return type of\n   :func:`~jax.lax.dot_general` is always defined by the ``accumulation_type``\n   parameter of the input algorithm, if specified.\n@@ -7923,7 +7923,7 @@ def _sort_abstract_eval(*args, **kwargs):\n \n \n def _canonicalize_float_for_sort(x):\n-  # In the sort comparator, we are going to use a comparision operator where -0\n+  # In the sort comparator, we are going to use a comparison operator where -0\n   # would be before 0, and -NaN and NaN appear at the beginning and end of the\n   # ordering. In this scheme, -0 would be before 0, and -NaN and NaN appear at\n   # the beginning and end of the ordering. This causes issues for stable\n@@ -8164,7 +8164,7 @@ def _create_token_lowering(ctx, *operands):\n def after_all(*operands):\n   \"\"\"Merges one or more XLA token values. Experimental.\n \n-  Wraps the XLA AfterAll operator.\"\"\"\n+  Wraps the XLA after all operator.\"\"\"\n   operands = core.standard_insert_pvary(*operands)\n   return after_all_p.bind(*operands)\n \ndiff --git a/jax/_src/lax/linalg.py b/jax/_src/lax/linalg.py\nindex 2fda4a90369d..3ee7cc2a6807 100644\n--- a/jax/_src/lax/linalg.py\n+++ b/jax/_src/lax/linalg.py\n@@ -2148,7 +2148,7 @@ def _svd_gpu_sub_lowering(ctx, operand, *, full_matrices, compute_uv,\n   # default QR algorithm, but users can (in principle) override this behavior\n   # by passing `use_jacobi=True`.\n   #\n-  # TODO(danfm): Since this was originally implemented, hipSolver appers to\n+  # TODO(danfm): Since this was originally implemented, hipSolver appears to\n   # have added support for the Jacobi algorithm, so we should investigate\n   # removing this condition.\n   if algorithm is None or algorithm == SvdAlgorithm.DEFAULT:\n@@ -2339,7 +2339,7 @@ def a_inverse(rhs):\n                             transpose_a=transpose_a, conjugate_a=conjugate_a,\n                             unit_diagonal=unit_diagonal)\n \n-  # triangular_solve is about the same cost as matrix multplication (~n^2 FLOPs\n+  # triangular_solve is about the same cost as matrix multiplication (~n^2 FLOPs\n   # for matrix/vector inputs). Order these operations in whichever order is\n   # cheaper.\n   if left_side:\n@@ -2776,8 +2776,8 @@ def _column_major_matrix_layout(dim: int) -> tuple[int, ...]:\n \n def _sdy_rule_for_aval(letters, num_batch_dims, aval):\n   d = len(aval.shape) - num_batch_dims\n-  preffix = \"... \" if num_batch_dims and d >= 0 else \"\"\n-  return preffix + \" \".join(next(letters) for _ in range(d))\n+  prefix = \"... \" if num_batch_dims and d >= 0 else \"\"\n+  return prefix + \" \".join(next(letters) for _ in range(d))\n \n def _build_sdy_sharding_rule(num_batch_dims, avals_in, avals_out):\n   letters = iter(string.ascii_letters)\ndiff --git a/jax/_src/nn/functions.py b/jax/_src/nn/functions.py\nindex 3f7647758003..f01c4fa52804 100644\n--- a/jax/_src/nn/functions.py\n+++ b/jax/_src/nn/functions.py\n@@ -1077,7 +1077,7 @@ def dot_product_attention(\n       token's local window. If set, this specifies the (left_window_size,\n       right_window_size) for each token. E.g., if local_window_size == (3, 2)\n       and the sequence is [0, 1, 2, 3, 4, 5, c, 7, 8, 9], token `c` can attend\n-      to [3, 4, 5, c, 7, 8]. If a single int is given, it will be intepreted as\n+      to [3, 4, 5, c, 7, 8]. If a single int is given, it will be interpreted as\n       a symmetric window (window_size, window_size).\n     implementation: A string to control which implementation backend to use.\n       Supported strings are `xla`, `cudnn` (cuDNN flash attention). It defaults\ndiff --git a/jax/_src/numpy/fft.py b/jax/_src/numpy/fft.py\nindex 21da91ce613f..970847532e46 100644\n--- a/jax/_src/numpy/fft.py\n+++ b/jax/_src/numpy/fft.py\n@@ -712,7 +712,7 @@ def hfft(a: ArrayLike, n: int | None = None,\n       are supported. Default is \"backward\".\n \n   Returns:\n-    A real-valued array containing the one-dimensional discret Fourier transform\n+    A real-valued array containing the one-dimensional discrete Fourier transform\n     of ``a`` by exploiting its inherent Hermitian-symmetry, having a dimension of\n     ``n`` along ``axis``.\n \ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex f323bc64718b..a35bcbb23213 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -272,7 +272,7 @@ def load(file: IO[bytes] | str | os.PathLike[Any], *args: Any, **kwargs: Any) ->\n def fmin(x1: ArrayLike, x2: ArrayLike) -> Array:\n   \"\"\"Return element-wise minimum of the input arrays.\n \n-  JAX implemtentation of :func:`numpy.fmin`.\n+  JAX implementation of :func:`numpy.fmin`.\n \n   Args:\n     x1: input array or scalar.\n@@ -2251,7 +2251,7 @@ def resize(a: ArrayLike, new_shape: Shape) -> Array:\n \n   Returns:\n     A resized array with specified shape. The elements of ``a`` are repeated in\n-    the resized array, if the resized array is larger than the original aray.\n+    the resized array, if the resized array is larger than the original array.\n \n   See also:\n     - :func:`jax.numpy.reshape`: Returns a reshaped copy of an array.\n@@ -5575,7 +5575,7 @@ def astype(x: ArrayLike, dtype: DTypeLike | None,\n            device: xc.Device | Sharding | None = None) -> Array:\n   \"\"\"Convert an array to a specified dtype.\n \n-  JAX imlementation of :func:`numpy.astype`.\n+  JAX implementation of :func:`numpy.astype`.\n \n   This is implemented via :func:`jax.lax.convert_element_type`, which may\n   have slightly different behavior than :func:`numpy.astype` in some cases.\n@@ -5957,7 +5957,7 @@ def from_dlpack(x: Any, /, *, device: xc.Device | Sharding | None = None,\n       if needed for a device transfer.\n \n   Returns:\n-    A JAX array of the imput buffer.\n+    A JAX array of the input buffer.\n \n   Note:\n     While JAX arrays are always immutable, dlpack buffers cannot be marked as\n@@ -8419,7 +8419,7 @@ def vander(\n            [3, 1],\n            [4, 1]], dtype=int32)\n \n-    Generates the Vandermonde matrix in increaing order of powers, when\n+    Generates the Vandermonde matrix in increasing order of powers, when\n     ``increasing=True``.\n \n     >>> jnp.vander(x, increasing=True)\ndiff --git a/jax/_src/numpy/linalg.py b/jax/_src/numpy/linalg.py\nindex f2deddd52f05..2351b0ccb075 100644\n--- a/jax/_src/numpy/linalg.py\n+++ b/jax/_src/numpy/linalg.py\n@@ -1617,7 +1617,7 @@ def matrix_transpose(x: ArrayLike, /) -> Array:\n   x_arr = ensure_arraylike('jnp.linalg.matrix_transpose', x)\n   ndim = x_arr.ndim\n   if ndim < 2:\n-    raise ValueError(f\"matrix_transpose requres at least 2 dimensions; got {ndim=}\")\n+    raise ValueError(f\"matrix_transpose requires at least 2 dimensions; got {ndim=}\")\n   return lax.transpose(x_arr, (*range(ndim - 2), ndim - 1, ndim - 2))\n \n \ndiff --git a/jax/_src/numpy/ufunc_api.py b/jax/_src/numpy/ufunc_api.py\nindex 243ab9aa0878..c85621d6cdba 100644\n--- a/jax/_src/numpy/ufunc_api.py\n+++ b/jax/_src/numpy/ufunc_api.py\n@@ -92,7 +92,7 @@ class ufunc:\n            [ 5,  6,  7,  8,  9],\n            [ 6,  7,  8,  9, 10]], dtype=int32)\n \n-    The :meth:`ufunc.reduce` method perfoms a reduction over the array.\n+    The :meth:`ufunc.reduce` method performs a reduction over the array.\n     For example, :meth:`jnp.add.reduce` is equivalent to ``jnp.sum``:\n \n     >>> jnp.add.reduce(x)\n@@ -112,7 +112,7 @@ class ufunc:\n     Array([101,   2,   3,   4,   5], dtype=int32)\n \n     And the :meth:`ufunc.reduceat` method performs a number of ``reduce``\n-    operations bewteen specified indices of an array; for ``jnp.add`` the\n+    operations between specified indices of an array; for ``jnp.add`` the\n     operation is similar to :func:`jax.ops.segment_sum`:\n \n     >>> jnp.add.reduceat(x, jnp.array([0, 2]))\n@@ -574,7 +574,7 @@ def outer(self, A: ArrayLike, B: ArrayLike, /) -> Array:\n        [ 10  20  30  40  50  60  70  80  90 100]]\n \n       For input arrays with ``N`` and ``M`` dimensions respectively, the output\n-      will have dimesion ``N + M``:\n+      will have dimension ``N + M``:\n \n       >>> x = jnp.ones((1, 3, 5))\n       >>> y = jnp.ones((2, 4))\ndiff --git a/jax/_src/numpy/ufuncs.py b/jax/_src/numpy/ufuncs.py\nindex 486d3f15e17c..b0ff3cb9747a 100644\n--- a/jax/_src/numpy/ufuncs.py\n+++ b/jax/_src/numpy/ufuncs.py\n@@ -297,7 +297,7 @@ def sign(x: ArrayLike, /) -> Array:\n       -1, & x < 0\n     \\end{cases}\n \n-  For complex valued input, ``jnp.sign`` returns a unit vector repesenting the\n+  For complex valued input, ``jnp.sign`` returns a unit vector representing the\n   phase. For generalized case, the sign of ``x`` is given by:\n \n   .. math::\n@@ -347,8 +347,8 @@ def floor(x: ArrayLike, /) -> Array:\n     the nearest integer that is less than or equal to the value itself.\n \n   See also:\n-    - :func:`jax.numpy.fix`: Rounds the input to the nearest interger towards zero.\n-    - :func:`jax.numpy.trunc`: Rounds the input to the nearest interger towards\n+    - :func:`jax.numpy.fix`: Rounds the input to the nearest integer towards zero.\n+    - :func:`jax.numpy.trunc`: Rounds the input to the nearest integer towards\n       zero.\n     - :func:`jax.numpy.ceil`: Rounds the input up to the nearest integer.\n \n@@ -386,8 +386,8 @@ def ceil(x: ArrayLike, /) -> Array:\n     the nearest integer that is greater than or equal to the value itself.\n \n   See also:\n-    - :func:`jax.numpy.fix`: Rounds the input to the nearest interger towards zero.\n-    - :func:`jax.numpy.trunc`: Rounds the input to the nearest interger towards\n+    - :func:`jax.numpy.fix`: Rounds the input to the nearest integer towards zero.\n+    - :func:`jax.numpy.trunc`: Rounds the input to the nearest integer towards\n       zero.\n     - :func:`jax.numpy.floor`: Rounds the input down to the nearest integer.\n \n@@ -1621,7 +1621,7 @@ def arctan2(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n \n     The results match the input ``theta``, except at the endpoints where :math:`+\\pi`\n     and :math:`-\\pi` represent indistinguishable points on the unit circle. By convention,\n-    :func:`arctan2` alwasy returns values between :math:`-\\pi` and :math:`+\\pi` inclusive.\n+    :func:`arctan2` always returns values between :math:`-\\pi` and :math:`+\\pi` inclusive.\n   \"\"\"\n   return lax.atan2(*promote_args_inexact(\"arctan2\", x1, x2))\n \n@@ -1710,7 +1710,7 @@ def maximum(x: ArrayLike, y: ArrayLike, /) -> Array:\n       arrays.\n     - :func:`jax.numpy.fmax`: Returns element-wise maximum of the input arrays,\n       ignoring NaNs.\n-    - :func:`jax.numpy.amax`: Retruns the maximum of array elements along a given\n+    - :func:`jax.numpy.amax`: Returns the maximum of array elements along a given\n       axis.\n     - :func:`jax.numpy.nanmax`: Returns the maximum of the array elements along\n       a given axis, ignoring NaNs.\n@@ -1774,7 +1774,7 @@ def float_power(x: ArrayLike, y: ArrayLike, /) -> Array:\n     >>> jnp.float_power(x, y)\n     Array([ 9. ,  1. , -0.2], dtype=float32)\n \n-    Inputs with broacast compatibility:\n+    Inputs with broadcast compatibility:\n \n     >>> x1 = jnp.array([[2, -4, 1],\n     ...                 [-1, 2, 3]])\ndiff --git a/jax/_src/pallas/fuser/jaxpr_fusion.py b/jax/_src/pallas/fuser/jaxpr_fusion.py\nindex d1e375e33ef1..8e12b5db483d 100644\n--- a/jax/_src/pallas/fuser/jaxpr_fusion.py\n+++ b/jax/_src/pallas/fuser/jaxpr_fusion.py\n@@ -176,7 +176,7 @@ def _construct_output_fusions(\n       unflat_fusible_outvars\n   )\n \n-  # 3. Calculate dependencies and check disjointness\n+  # 3. Calculate dependencies and check disjointedness\n   downstream_outputs_used_masks = []  # List of bool tuples, one per group\n   already_used_final_outputs = set()  # Indices of final outputs already claimed\n   for outvars_group in partial_flat:\ndiff --git a/jax/_src/pallas/hlo_interpreter.py b/jax/_src/pallas/hlo_interpreter.py\nindex 755df2cd8ceb..fac798fe9dc1 100644\n--- a/jax/_src/pallas/hlo_interpreter.py\n+++ b/jax/_src/pallas/hlo_interpreter.py\n@@ -189,7 +189,7 @@ def eval_jaxpr_recursive(\n     consts: Consts that ``jaxpr`` closes over.\n     *args: Input arguments to the ``jaxpr``.\n     recurse_hop_rule: A Jaxpr interpreter to call on sub-jaxprs of\n-      higher-order primtives.\n+      higher-order primitives.\n     propagate_source_info: Whether to propagate source info.\n   \"\"\"\n   def read(v: jax_core.Atom) -> Any:\n@@ -419,7 +419,7 @@ def pallas_call_hlo_interpret(\n     num_iterations = 1\n \n   # The scan carry: (i, loop_idx, *consts, *ins, *outs, *scratch)\n-  # i:int32 is the interation index\n+  # i:int32 is the iteration index\n   # loop_idx: tuple[int32] are the program ids for each grid axis\n   def cond(carry):\n     i, *_ = carry\ndiff --git a/jax/_src/pallas/mosaic/interpret.py b/jax/_src/pallas/mosaic/interpret.py\nindex e278168d999a..7a6c18d43bb7 100644\n--- a/jax/_src/pallas/mosaic/interpret.py\n+++ b/jax/_src/pallas/mosaic/interpret.py\n@@ -82,7 +82,7 @@ class InterpretParams:\n       Default: False.\n     skip_floating_point_ops: If True, operations that produce only floating\n       point values will not be interpreted; instead, their results will be\n-      replaced with arrays all of `jnp.inf`. Additionaly any floating point\n+      replaced with arrays all of `jnp.inf`. Additionally any floating point\n       operands to any operation will be replaced with (arrays of) `jnp.inf`.\n       Default: False.\n     uninitialized_memory: If \"nan\", allocated buffers are initialized to contain\n@@ -937,7 +937,7 @@ def get(\n         raise ValueError(\n             'Out-of-bounds read of'\n             f' ({device_id} {local_core_id} {memory_space} {buffer_id}):'\n-            f' reading [{read_range}] but bufer has shape {buffer.shape} .'\n+            f' reading [{read_range}] but buffer has shape {buffer.shape} .'\n         )\n \n   if shared_memory.interpret_params.detect_races:\n@@ -1817,7 +1817,7 @@ def _get_randomized_grid_coordinates(\n   For a dimension with 'parallel' semantics at position `d` in the grid, the\n   returned tuple contains a random permutation of the sequence `[0,...,\n   grid[d] - 1]` at index `d`. For each dimension with 'arbitrary' semantics,\n-  the resulting tuple contains an empty array. (Inserting an empty arry for an\n+  the resulting tuple contains an empty array. (Inserting an empty array for an\n   'arbitrary' dimension at position `d` in the grid, instead of the sequence\n   `[0,..., grid[d] - 1]`, allows `grid[d]` to be a dynamic value, i.e. a value\n   not known at Jax trace time.)\n@@ -2059,7 +2059,7 @@ def interpret_pallas_call(\n   output_block_shapes = block_shapes[num_inputs : num_inputs + num_outputs]\n   for i, bm in enumerate(grid_mapping.block_mappings_output):\n     if i in oi_alias_map:\n-      # Re-use the HBM buffer for the aliased pallas_call input.\n+      # Reuse the HBM buffer for the aliased pallas_call input.\n       output_buffer_ids.append(input_buffer_ids[oi_alias_map[i]])\n       output_buffer_shapes.append(input_args[oi_alias_map[i]].shape)\n       output_vals.append(input_args[oi_alias_map[i]])\n@@ -2230,7 +2230,7 @@ def _body(\n       Args:\n         carry: (iteration_idx, loop_idx, grid_point, prev_start_indices,\n                 cur_start_indices).\n-          - iteration_idx: the interation index.\n+          - iteration_idx: the iteration index.\n           - loop_idx: internal indices for looping over the grid.\n           - grid_point: the current positions along all axes of the grid.\n           - prev_start_indices: a rank-1 array that contains the start indices\ndiff --git a/jax/_src/pallas/mosaic/primitives.py b/jax/_src/pallas/mosaic/primitives.py\nindex c9cdcbf56f85..af50773eec20 100644\n--- a/jax/_src/pallas/mosaic/primitives.py\n+++ b/jax/_src/pallas/mosaic/primitives.py\n@@ -663,7 +663,7 @@ def get_barrier_semaphore():\n   to share a collective_id. However, if in doubt, prefer not sharing\n   collective_ids, as doing so incorrectly can lead to silent data corruption or\n   crashes.\n-  Note that re-using the same collective_id doesn't guarantee that the same\n+  Note that reusing the same collective_id doesn't guarantee that the same\n   semaphore is provided by XLA.\n   \"\"\"\n   return get_barrier_semaphore_p.bind()\ndiff --git a/jax/_src/pallas/mosaic/random.py b/jax/_src/pallas/mosaic/random.py\nindex 6a2c557fd55d..8d29f857afb2 100644\n--- a/jax/_src/pallas/mosaic/random.py\n+++ b/jax/_src/pallas/mosaic/random.py\n@@ -177,7 +177,7 @@ def sample_block(sampler_fn: SampleFnType,\n \n   `tile_size` should be chosen such that it is a divisor to all block sizes\n   one needs to be invariant to. The larger the `tile_size`, the more\n-  efficient the sampling process wil be and therefore the best choice is\n+  efficient the sampling process will be and therefore the best choice is\n   typically the greatest common divisor between all possible block sizes.\n \n   Args:\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 7fb933f5623d..3b28ebdd5d20 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -368,7 +368,7 @@ class RefUnion(GPUMemoryRef):\n   \"\"\"A sequence of trees of refs that are allowed to reuse the same memory.\n \n   One should not make assumptions as to how each ref will map to the underlying\n-  memory region, since arbitrary padding may be applied inbetween different\n+  memory region, since arbitrary padding may be applied in between different\n   refs.\n \n   As such, ref unions are only safe to use when the groups of refs that we\n@@ -459,7 +459,7 @@ def untransform_transpose(\n       self, perm: tuple[int, ...]\n   ) -> tuple[tuple[int, ...], state_types.Transform]:\n     # The transpose in question is applied to the utiled ref so we\n-    # need to translate it by duplicating and offseting the last part.\n+    # need to translate it by duplicating and offsetting the last part.\n     off = len(perm)\n     new_suffix = [i + off for i in perm[-len(self.tiling) :]]\n     if set(new_suffix) != set(range(off, off + len(self.tiling))):\n@@ -871,7 +871,7 @@ class Barrier:\n       barriers can be accessed by indexing into the barrier Ref.\n     for_tensor_core: Whether this barrier is used for synchronizing with\n       the tensor core. This should be set to True when waiting on Blackwell\n-      (TC Gen 5) asynchoronous matmul instructions.\n+      (TC Gen 5) asynchronous matmul instructions.\n   \"\"\"\n   num_arrivals: int = 1\n   num_barriers: int = 1\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 5695da4cc8b1..87bb85cfcd70 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -2377,7 +2377,7 @@ def _run_scoped_lowering_rule(\n     if any(should_discharge):\n       # We convert consts to args, because we only have ir.Values and\n       # not JAX values during lowering. discharge_state() produces JAX\n-      # valiues for the aguments but expects them to be provided for the\n+      # valiues for the arguments but expects them to be provided for the\n       # consts. We also don't want to wrap the values in refs.\n       no_const_jaxpr = pe.convert_constvars_jaxpr(jaxpr)\n       should_discharge = [False] * len(consts) + should_discharge\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 616f7e501cd8..f37a003f4401 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -651,7 +651,7 @@ def _extract_barrier_indexer(transforms) -> indexing.NDIndexer | None:\n     case []:\n       return None\n     case _:\n-      raise ValueError(\"Barrier does not support arbirary transforms\")\n+      raise ValueError(\"Barrier does not support arbitrary transforms\")\n \n \n barrier_arrive_p = jax_core.Primitive(\"barrier_arrive\")\n@@ -835,7 +835,7 @@ def _commit_group_lowering(ctx: lowering.LoweringRuleContext):\n \n \n def commit_smem_to_gmem_group() -> None:\n-  \"\"\"Commits all issued but uncommited SMEM->GMEM copies to a group.\"\"\"\n+  \"\"\"Commits all issued but uncommitted SMEM->GMEM copies to a group.\"\"\"\n   commit_group_p.bind()\n \n \ndiff --git a/jax/_src/pallas/pallas_call.py b/jax/_src/pallas/pallas_call.py\nindex b14259556faf..52360b997743 100644\n--- a/jax/_src/pallas/pallas_call.py\n+++ b/jax/_src/pallas/pallas_call.py\n@@ -304,7 +304,7 @@ def _broadcast_input_output_aliases(\n \n   When we have input/output aliasing, since the output will be mapped, we need\n   to make sure to broadcast the input across that dimension if it is not\n-  mapped. If the input is mapped, but on a different axis, we tranpose the input\n+  mapped. If the input is mapped, but on a different axis, we transpose the input\n   to match the output.\n   \"\"\"\n \n@@ -370,7 +370,7 @@ def _batch_with_explicit_loop(\n       axis_size=axis_size,\n   )\n \n-  # The output arrays are completelly overwritten, so we can just initialize\n+  # The output arrays are completely overwritten, so we can just initialize\n   # empty arrays.\n   initial_state = [\n       jnp.empty(tuple_insert(bm.array_shape_dtype.shape, 0, axis_size),\n@@ -801,7 +801,7 @@ def index_rewrite_kernel(*indexer_args):\n         ragged_axis_dim = per_input_ragged_axis_dim[arg_pos]\n \n         # the problem here seems to be that we are rnning this for all inputs, per input, because they each have an indexer - which means\n-        # that the indexer for output isnt getting written - before, it always was\n+        # that the indexer for output isn't getting written - before, it always was\n \n         lengths_ref = indexer_args[-1]\n         rest_indexer_args = indexer_args[:-1]\n@@ -896,7 +896,7 @@ def index_rewrite_kernel(*indexer_args):\n         raise NotImplementedError(\"consts not supported in pallas_call\")\n \n     # We need to rewrite the input_output_aliases here, the initial call\n-    # to broadcast is done, and we have inseted a new input (lengths), so\n+    # to broadcast is done, and we have inserted a new input (lengths), so\n     # there's an off-by-one here now.\n     new_input_output_aliases = []\n     for k, v in input_output_aliases:\n@@ -987,7 +987,7 @@ def pallas_call_checkify_oob_grid(error: checkify.Error,\n       for bm in grid_mapping.block_mappings\n   ]\n   # The scan carry: (i, loop_idx, *consts, *ins, *outs, *scratch)\n-  # i:int32 is the interation index\n+  # i:int32 is the iteration index\n   # loop_idx: tuple[int32] are the program ids for each grid axis\n   def cond(carry):\n     i, *_ = carry\n@@ -1144,7 +1144,7 @@ def _ensure_2d_error_shape(arg):\n   # for the new error inputs and outputs.\n   error_block_specs = [pallas_core.BlockSpec(None, None)] * len(shaped_err_avals)\n   error_paths, _ = unzip2(tree_util.tree_flatten_with_path(error_block_specs)[0])\n-  error_origins = tuple(f\"errrors[{tree_util.keystr(p)}\" for p in error_paths)\n+  error_origins = tuple(f\"errors[{tree_util.keystr(p)}\" for p in error_paths)\n   error_block_mappings = map(\n         partial(\n             pallas_core._convert_block_spec_to_block_mapping,\n@@ -1762,7 +1762,7 @@ def in_path_to_input_origin(\n \n \n # We import the TPU backend at the top level because it defines flags. Note that\n-# we can only do that at the bottom of this file, beacuse it also depends on\n+# we can only do that at the bottom of this file, because it also depends on\n # this module already being initialized.\n \n try:\ndiff --git a/jax/_src/pallas/primitives.py b/jax/_src/pallas/primitives.py\nindex 5038ac6e5171..95ae15e5bf4e 100644\n--- a/jax/_src/pallas/primitives.py\n+++ b/jax/_src/pallas/primitives.py\n@@ -490,7 +490,7 @@ def _load_discharge_rule(in_avals, out_avals, *args_flat, args_tree, **_):\n     scalar_dims = [not isinstance(s, Slice) and not s.shape for s in indices]\n     slice_starts = [s.start if isinstance(s, Slice) else s for s in indices]\n     slice_sizes = tuple(s.size if isinstance(s, Slice) else 1 for s in indices)\n-    # fixes an inconstency with lax.dynamic_slice where if the slice goes out\n+    # fixes an inconsistency with lax.dynamic_slice where if the slice goes out\n     # of bounds, it will instead move the start_index backwards so the slice\n     # will fit in memory.\n     ref = _pad_values_to_avoid_dynamic_slice_oob_shift(ref, slice_sizes)\ndiff --git a/jax/_src/profiler.py b/jax/_src/profiler.py\nindex 424e2b81035f..efdd7bd1e2a1 100644\n--- a/jax/_src/profiler.py\n+++ b/jax/_src/profiler.py\n@@ -402,7 +402,7 @@ def save_device_memory_profile(filename, backend: str | None = None) -> None:\n \n \n # Allows to run model with profiler given amount of times. After required amount\n-# of retries achived client can collect FDO data.\n+# of retries achieved client can collect FDO data.\n class PGLEProfiler:\n \n   def __init__(self, retries: int, percentile: int):\ndiff --git a/jax/_src/scipy/linalg.py b/jax/_src/scipy/linalg.py\nindex 55961607b252..7b0bb06f044c 100644\n--- a/jax/_src/scipy/linalg.py\n+++ b/jax/_src/scipy/linalg.py\n@@ -2189,7 +2189,7 @@ def pascal(n: int, kind: str | None = None) -> Array:\n \n   JAX implementation of :func:`scipy.linalg.pascal`.\n \n-  The elements of the Pascal matrix approximate the binomial coefficents. This\n+  The elements of the Pascal matrix approximate the binomial coefficients. This\n   implementation is not exact as JAX does not support exact factorials.\n \n   Args:\ndiff --git a/jax/_src/scipy/signal.py b/jax/_src/scipy/signal.py\nindex f8c2563027f5..d4ca7c2c6147 100644\n--- a/jax/_src/scipy/signal.py\n+++ b/jax/_src/scipy/signal.py\n@@ -148,7 +148,7 @@ def _fftconvolve_unbatched(in1: Array, in2: Array, mode: str) -> Array:\n   return lax.dynamic_slice(conv, start_indices, out_shape)\n \n \n-# Note: we do not re-use the code from jax.numpy.convolve here, because the handling\n+# Note: we do not reuse the code from jax.numpy.convolve here, because the handling\n # of padding differs slightly between the two implementations (particularly for\n # mode='same').\n def _convolve_nd(in1: Array, in2: Array, mode: str, *, precision: PrecisionLike) -> Array:\n@@ -1030,16 +1030,16 @@ def _overlap_and_add(x: Array, step_size: int) -> Array:\n   x = x.reshape((flat_batchsize, nframes, nstep_per_segment, step_size))\n \n   # For obtaining shifted signals, this routine reinterprets flattened array\n-  # with a shrinked axis.  With appropriate truncation/ padding, this operation\n+  # with a shrunken axis.  With appropriate truncation/ padding, this operation\n   # pushes the last padded elements of the previous row to the head of the\n   # current row.\n   # See implementation of `overlap_and_add` in Tensorflow for details.\n   x = x.transpose((0, 2, 1, 3))  # x: (B, S, N, T)\n   x = jnp.pad(x, ((0, 0), (0, 0), (0, nframes), (0, 0)))  # x: (B, S, N*2, T)\n-  shrinked = x.shape[2] - 1\n+  shrunken = x.shape[2] - 1\n   x = x.reshape((flat_batchsize, -1))\n-  x = x[:, :(nstep_per_segment * shrinked * step_size)]\n-  x = x.reshape((flat_batchsize, nstep_per_segment, shrinked * step_size))\n+  x = x[:, :(nstep_per_segment * shrunken * step_size)]\n+  x = x.reshape((flat_batchsize, nstep_per_segment, shrunken * step_size))\n \n   # Finally, sum shifted segments, and truncate results to the output_size.\n   x = x.sum(axis=1)[:, :output_size]\ndiff --git a/jax/_src/scipy/stats/_core.py b/jax/_src/scipy/stats/_core.py\nindex 65c457f79cc8..ae93dd793844 100644\n--- a/jax/_src/scipy/stats/_core.py\n+++ b/jax/_src/scipy/stats/_core.py\n@@ -285,7 +285,7 @@ def sem(a: ArrayLike, axis: int | None = 0, ddof: int = 1, nan_policy: str = \"pr\n     Array([1.73,  nan, 1.53,  nan,  nan,  nan], dtype=float32)\n \n     If ``nan_policy='omit```, ``sem`` omits the ``nan`` values and computes the error\n-    for the remainging values along the specified axis.\n+    for the remaining values along the specified axis.\n \n     >>> with jnp.printoptions(precision=2, suppress=True):\n     ...   jax.scipy.stats.sem(x2, nan_policy='omit')\ndiff --git a/jax/_src/state/types.py b/jax/_src/state/types.py\nindex e3a86e241bf2..7ca1d8e48f9e 100644\n--- a/jax/_src/state/types.py\n+++ b/jax/_src/state/types.py\n@@ -255,7 +255,7 @@ def shape(self) -> tuple[int | Array, ...]:\n     if not unprocessed:\n       return shape\n     # If there are any unprocessed transforms left, we apply them to the shape\n-    # we've found previuously.\n+    # we've found previously.\n     for t in self.transforms[-unprocessed:]:\n       shape = t.transform_shape(shape)\n     assert shape is not None\ndiff --git a/jax/_src/xla_bridge.py b/jax/_src/xla_bridge.py\nindex 22e71d7d9ce6..1b95806c37c6 100644\n--- a/jax/_src/xla_bridge.py\n+++ b/jax/_src/xla_bridge.py\n@@ -381,7 +381,7 @@ def discover_pjrt_plugins() -> None:\n   \"\"\"Discovers plugins in the namespace package `jax_plugins` and import them.\n \n   There are two methods used to discover plugin modules. They are intended\n-  to be used together by implementors in order to cover all packaging and\n+  to be used together by implementers in order to cover all packaging and\n   development cases:\n \n   1. Define a globally unique module under the `jax_plugins` namespace\n@@ -964,7 +964,7 @@ def backend_xla_version(platform=None) -> int | None:\n   \"\"\"Returns the XLA version of the backend.\n \n   Returns None if the backend does not use PJRT C API or does not have\n-  xla_version in the plugin attributes. This methon can be used to skip features\n+  xla_version in the plugin attributes. This method can be used to skip features\n   that are not available before certain xla_version if the backend is a\n   plugin and uses xla_version.\n   \"\"\"\n@@ -975,7 +975,7 @@ def backend_stablehlo_version(platform=None) -> Sequence[int] | None:\n   \"\"\"Returns the StableHLO version of the backend.\n \n   Returns None if the backend does not use PJRT C API or does not have\n-  stablehlo_current_version in the plugin attributes. This methon can be used to\n+  stablehlo_current_version in the plugin attributes. This method can be used to\n   skip features that are not available before certain stablehlo_current_version\n   if the backend is a plugin and uses stablehlo_current_version.\n   \"\"\"\ndiff --git a/jax/experimental/colocated_python/serialization.py b/jax/experimental/colocated_python/serialization.py\nindex a8a62d78359f..1f1b96487fab 100644\n--- a/jax/experimental/colocated_python/serialization.py\n+++ b/jax/experimental/colocated_python/serialization.py\n@@ -201,7 +201,7 @@ def _serialize_specs(\n   if not hasattr(np.dtypes, \"StringDType\"):\n     raise TypeError(\n         \"Serializing Colocated Python requires StringDType. Please use\"\n-        \" numpy to 2.0.0 or later, or explicityly provide an output spec\"\n+        \" numpy to 2.0.0 or later, or explicitly provide an output spec\"\n         \" function.\"\n     )\n \ndiff --git a/jax/experimental/jax2tf/README.md b/jax/experimental/jax2tf/README.md\nindex 06cc5c86a109..ac9829d69006 100644\n--- a/jax/experimental/jax2tf/README.md\n+++ b/jax/experimental/jax2tf/README.md\n@@ -840,7 +840,7 @@ to `export.symbolic_shape` share a scope and\n can be mixed up in arithmetic operations. The result would\n also share the same scope.\n \n-You can re-use scopes:\n+You can reuse scopes:\n \n ```python\n a, = export.symbolic_shape(\"a,\", constraints=(\"a >= 8\",))\ndiff --git a/jax/experimental/jax2tf/impl_no_xla.py b/jax/experimental/jax2tf/impl_no_xla.py\nindex 644c3324b4e2..70a6dccf8915 100644\n--- a/jax/experimental/jax2tf/impl_no_xla.py\n+++ b/jax/experimental/jax2tf/impl_no_xla.py\n@@ -659,7 +659,7 @@ def tf_pool(inputs, pooling_type):\n       raise NotImplementedError(\n           f\"TODO: use tf.nn.pool with dynamic shapes{window_dimensions=} \"\n           f\" {window_strides=} {dilations=}\")\n-    # tf.nn.pool() currently does not suport tf.int32 and so we cast back and\n+    # tf.nn.pool() currently does not support tf.int32 and so we cast back and\n     # forth in order to be able to convert.\n     if (inputs.dtype in [tf.int16, tf.int32]) and computation_name == \"add\":\n       original_dtype = inputs.dtype\ndiff --git a/jax/experimental/key_reuse/_core.py b/jax/experimental/key_reuse/_core.py\nindex 6f604f1195a0..a2ffc6582fff 100644\n--- a/jax/experimental/key_reuse/_core.py\n+++ b/jax/experimental/key_reuse/_core.py\n@@ -212,7 +212,7 @@ def key_reuse_signature_from_eqn(eqn: core.JaxprEqn) -> KeyReuseSignature:\n       return sig.signature(eqn)\n     else:\n       raise TypeError(\n-        f\"Unrecognized key reuse sigature of type {type(sig)}: {sig}\")\n+        f\"Unrecognized key reuse signature of type {type(sig)}: {sig}\")\n   else:\n     return unknown_signature(eqn)\n \n@@ -231,7 +231,7 @@ def key_reuse_signature_from_primitive(prim, *args, **params):\n     return jaxpr_type_signature(jaxpr)\n   else:\n     raise TypeError(\n-      f\"Unrecognized key reuse sigature of type {type(sig)}: {sig}\")\n+      f\"Unrecognized key reuse signature of type {type(sig)}: {sig}\")\n \n \n consume_p = core.Primitive(\"consume\")\ndiff --git a/jax/experimental/mosaic/gpu/dialect_lowering.py b/jax/experimental/mosaic/gpu/dialect_lowering.py\nindex 20138bbe6fd4..e9293d9ffe08 100644\n--- a/jax/experimental/mosaic/gpu/dialect_lowering.py\n+++ b/jax/experimental/mosaic/gpu/dialect_lowering.py\n@@ -957,7 +957,7 @@ def _mgpu_wgmma_op_lowering_rule(\n     raise ValueError(\"Layout mismatch\")\n   wgmma_layout = fa_layouts[0]\n \n-  # TODO(dasenov): Move the value -> accumulator conversion outisde of wgmma.\n+  # TODO(dasenov): Move the value -> accumulator conversion outside of wgmma.\n   # The associated fence could be a little expensive and is not needed if the\n   # result a wgmma feeds into another wgmma (even in another loop step).\n   acc_in = _fragmented_array_from_ir(wgmma_op.accumulator, wgmma_layout)\ndiff --git a/jax/experimental/mosaic/gpu/examples/flash_attention.py b/jax/experimental/mosaic/gpu/examples/flash_attention.py\nindex 78ef1faddc59..280efd513187 100644\n--- a/jax/experimental/mosaic/gpu/examples/flash_attention.py\n+++ b/jax/experimental/mosaic/gpu/examples/flash_attention.py\n@@ -243,8 +243,8 @@ def kv_loop(kv_step, carry):\n \n         perform_schedule_barrier()\n \n-        # This is quite suprising, but it seems like warp shuffles cannot\n-        # run simutaneously with the WGMMA. For that reason we include it as\n+        # This is quite surprising, but it seems like warp shuffles cannot\n+        # run simultaneously with the WGMMA. For that reason we include it as\n         # part of the TensorCore critical section and not the ALU section.\n         with ctx.named_region(\"Softmax reduction\"):\n           l_i += p.reduce(arith.addf, axis=1)\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex 925aa1575e2d..3554ed95844c 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -669,7 +669,7 @@ def linear_thread_idxs(self):\n #          ...\n #\n # You can see that we have taken 2x2 submatrices from the above layout and\n-# transposed them. The assigment of lanes to elements is such that in both\n+# transposed them. The assignment of lanes to elements is such that in both\n # layouts the same two lanes map to a single 2x2 submatrix, making the transpose\n # very cheap (one shuffle and permute suffices to change between those layouts).\n WGMMA_TRANSPOSED_LAYOUT = TiledLayout(\n@@ -1743,7 +1743,7 @@ def reduce(\n         out_reg = vector.splat(\n             ir.VectorType.get((1,), out_reg.type.element_type), scalar_out_reg\n         )\n-      # Reduce accross warp lanes, if necessary (using warp shuffles).\n+      # Reduce across warp lanes, if necessary (using warp shuffles).\n       if any(reduced_dims[d] for d in layout.partitioned_lane_dims):\n         if utils.bitwidth(out_reg.type) > 32:\n           raise NotImplementedError  # Need to implement wide shfl_bfly.\n@@ -1762,7 +1762,7 @@ def reduce(\n               lane_stride *= 2\n               reduction_size //= 2\n         assert lane_stride == WARP_SIZE, lane_stride\n-      # Reduce accross warps in the warpgroup, if necessary.\n+      # Reduce across warps in the warpgroup, if necessary.\n       if (\n           not isinstance(layout.warp_dim, Replicated)\n           and reduced_dims[layout.warp_dim]\ndiff --git a/jax/experimental/mosaic/gpu/launch_context.py b/jax/experimental/mosaic/gpu/launch_context.py\nindex 2a5bb96f4708..852ac90c0d73 100644\n--- a/jax/experimental/mosaic/gpu/launch_context.py\n+++ b/jax/experimental/mosaic/gpu/launch_context.py\n@@ -779,7 +779,7 @@ def partition_dim(dim: int, idx: ir.Value, num_chunks: int):\n         tuple(slice_shape), swizzle, reduction_op,\n     )\n \n-    # We constuct TMA descriptors in column-major order.\n+    # We construct TMA descriptors in column-major order.\n     rev_dyn_base_indices = [\n         arith.index_cast(i32, idx) for idx in reversed(dyn_base_indices)\n     ]\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex d72994f45a87..91797fe65d1f 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -465,7 +465,7 @@ def _tmem_access_helper(shape, num):\n   num_regs *= num\n   if num_regs > 255:\n     raise ValueError(\n-        f\"TMEM transation too big : {shape=} and {num=} involve\"\n+        f\"TMEM translation too big : {shape=} and {num=} involve\"\n         f\" {num_regs} registers per-thread, which exceeds the limit of 255\"\n     )\n   regs_vector = \",\".join(f\"${i}\" for i in range(num_regs))\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex ac002aa8bffe..b5dbfb62c88f 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -275,7 +275,7 @@ class ThreadSubset(enum.IntEnum):\n   BLOCK = enum.auto()\n \n \n-# True withon `once()` contexts.\n+# True within `once()` contexts.\n _ONCE_PER: ThreadSubset | None = None\n \n \n@@ -468,7 +468,7 @@ def fold_until(shape, off , target)  -> tuple[int, int]:\n         # TODO(cperivol): Implement dependent fold-unfolds for subsections\n         # of the shape eg (..., 4,5,5, ...) -> (..., 10,10, ...) could be\n         # supported without touching any other dimensions.\n-        raise NotImplementedError(f\"Can't reshape {sh0} to {sh1} bu composing independent folds/unfolds.\")\n+        raise NotImplementedError(f\"Can't reshape {sh0} to {sh1} by composing independent folds/unfolds.\")\n \n     raise AssertionError(f\"Unreachable: number of elements don't match in each shape ({sh0} ans {sh1})\")\n \ndiff --git a/jax/experimental/mosaic/gpu/wgmma.py b/jax/experimental/mosaic/gpu/wgmma.py\nindex 2fe826e173e5..9b4fc7678538 100644\n--- a/jax/experimental/mosaic/gpu/wgmma.py\n+++ b/jax/experimental/mosaic/gpu/wgmma.py\n@@ -113,7 +113,7 @@ def wgmma_m64(\n ):\n   out_ty = ir.VectorType(acc.flat[0].type).element_type\n   if not _supported_wgmma_types(out_ty, element_type):\n-    raise ValueError(f\"Usupported wgmma types {(out_ty, element_type)=}\")\n+    raise ValueError(f\"Unsupported wgmma types {(out_ty, element_type)=}\")\n   if n % 8:\n     raise ValueError\n \ndiff --git a/jax/experimental/multihost_utils.py b/jax/experimental/multihost_utils.py\nindex 3a83ff16d612..07a0f747443c 100644\n--- a/jax/experimental/multihost_utils.py\n+++ b/jax/experimental/multihost_utils.py\n@@ -203,7 +203,7 @@ def should_save(step_id: int) -> bool:\n     after some hosts are preempted.\n \n   Raises:\n-    RuntimeError: if preemption sync manager has not been inititialized.\n+    RuntimeError: if preemption sync manager has not been initialized.\n   \"\"\"\n   if distributed.global_state.client is None:\n     return False\n@@ -328,7 +328,7 @@ def host_local_array_to_global_array(\n   >>>\n   >>> host_local_output = multihost_utils.global_array_to_host_local_array(global_out, mesh, out_pspecs) # doctest: +SKIP\n \n-  Please note ths function requires global mesh to be a continuous mesh, meaning\n+  Please note this function requires global mesh to be a continuous mesh, meaning\n   that  devices that belong to each host should form a subcube in this mesh.\n   To move local data to global array with non-continuous mesh use\n   jax.make_array_from_callback or jax.make_array_from_single_device_arrays\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex 650668daf67a..d9d62afe93c5 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -524,7 +524,7 @@ def _compute_sT(acc_ref):\n \n       def _compute(refs):\n         # Combining two WGMMA calls in one block to avoid the unnecessary\n-        # sychronization from two `wgmma.wait_group` calls.\n+        # synchronization from two `wgmma.wait_group` calls.\n         dv_acc_ref, dpT_acc_ref = refs\n         plgpu.wgmma(dv_acc_ref, pT.astype(dtype), do_smem)  # dV\n         plgpu.wgmma(dpT_acc_ref, v_smem, plgpu.transpose_ref(do_smem, (1, 0)))  # dpT\ndiff --git a/jax/experimental/pallas/ops/tpu/paged_attention/util.py b/jax/experimental/pallas/ops/tpu/paged_attention/util.py\nindex 6d6ceca3733f..92aa3a7a1b2c 100644\n--- a/jax/experimental/pallas/ops/tpu/paged_attention/util.py\n+++ b/jax/experimental/pallas/ops/tpu/paged_attention/util.py\n@@ -64,7 +64,7 @@ def grouped_query_attention_reference(\n   if debug:\n     jax.debug.print(\"qk: {qk}\", qk=qk)\n \n-  # Enfore causal mask (adding dimensions when necessary)\n+  # Enforce causal mask (adding dimensions when necessary)\n   mask = jnp.arange(max_seq_len)[None] < seq_lens[:, None]\n   qk += jnp.where(mask, 0.0, MASK_VALUE)[:, None, None, :]\n   if debug:\ndiff --git a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\nindex e7bc599b2b2b..3f12448f2a9c 100644\n--- a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n+++ b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n@@ -638,7 +638,7 @@ def prefetch_next_kv_blk():\n             v = v.astype(q_ref.dtype)\n           kv_head_idx = kv_head_chunk_idx + step_idx\n           q_head_idx = kv_head_idx * num_q_heads_per_kv_head\n-          # TODO(jevinjiang): extra handlig for packed type that can start at\n+          # TODO(jevinjiang): extra handling for packed type that can start at\n           # unaligned position!\n           q = fold_on_2nd_minor(\n               q_ref[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :]\ndiff --git a/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask.py b/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask.py\nindex 3f7a0d863188..354fdb24f9df 100644\n--- a/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask.py\n+++ b/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask.py\n@@ -352,7 +352,7 @@ class ChunkedCausalMask(_ComputableMask):\n   \"\"\"Lazy chunked causal mask.\n \n   Attention is causal within each chunk (0, K), (K, 2K), (2K, 3K), ... tokens\n-  attend to each other but not accross chunks.\n+  attend to each other but not across chunks.\n   Llama4 models use interleaved chunk attention along with global attention.\n \n \n@@ -412,7 +412,7 @@ class LocalMask(_ComputableMask):\n   \"\"\"Lazy local mask, prevents model from attending to tokens outside window.\n \n   Attributes:\n-    window_size: Size of the two sides of the local window (None identifes no\n+    window_size: Size of the two sides of the local window (None identifies no\n       limit for the given side).\n     offset: Offset of q start wrt kv. A positive offset shifts the bottom\n       triangle upward, a negative one shifts it downward. A negative offset\ndiff --git a/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask_info.py b/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask_info.py\nindex 9c79fbbf7e09..37ef92c2d33d 100644\n--- a/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask_info.py\n+++ b/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask_info.py\n@@ -418,7 +418,7 @@ def _process_dynamic_mask(\n   # tensors of this shape:\n   mask_info_slice_shape = (heads_per_shard, q_blocks_per_shard, kv_blocks_count)\n \n-  # Collect mask_info shards along the head dimension, concatentate (or\n+  # Collect mask_info shards along the head dimension, concatenate (or\n   # broadcast) them after the loop.\n   data_next_per_head_list, mask_next_per_head_list = [], []\n   for head_shard in range(head_shards):\n@@ -633,7 +633,7 @@ def assign_unique_ids(objects):\n   ]\n \n   # TODO(amagni): checking the validity of the masks is slow for large masks.\n-  # Disable it for now, reevalute in the future.\n+  # Disable it for now, reevaluate in the future.\n \n   partial_mask_block_ids: dict[_HashableNDArray, int] = collections.defaultdict(\n       lambda: len(partial_mask_block_ids)\n@@ -747,7 +747,7 @@ def set_block_mask(mask_id: int, q_index: int, kv_index: int, value: int):\n   q_sequence_axis = 1\n   head_axis = 0\n \n-  # Collect mask_info shards along the head dimension, concatentate (or\n+  # Collect mask_info shards along the head dimension, concatenate (or\n   # broadcast) them after the loop.\n   data_next_per_head_list, mask_next_per_head_list = [], []\n   for head_shard in range(shards_to_process):\ndiff --git a/jax/experimental/shard_map.py b/jax/experimental/shard_map.py\nindex 8f9548bdce1b..027cbd36feea 100644\n--- a/jax/experimental/shard_map.py\n+++ b/jax/experimental/shard_map.py\n@@ -53,7 +53,7 @@ def shard_map(\n     out_specs: a pytree with :class:`~jax.sharding.PartitionSpec` instances as leaves,\n       with a tree structure that is a tree prefix of the output of ``f``. Each\n       ``PartitionSpec`` represents how the corresponding output shards should be\n-      concatenated. In each ``PartitionSpec``, metioning a ``mesh`` axis name at\n+      concatenated. In each ``PartitionSpec``, mentioning a ``mesh`` axis name at\n       a position expresses concatenation of that mesh axis's shards along the\n       corresponding positional axis. Not mentioning a ``mesh`` axis name\n       expresses a promise that the output values are equal along that mesh axis,\ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 02bcbcf16dbc..f916cf385a66 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -182,7 +182,7 @@ def _version_check(name: str,\n                  scale_for_comparison=100)\n   # TODO(phawkins): for some reason this check fails with a cusolver internal\n   # error when fetching the version. This may be a path error from our stubs.\n-  # Figure out what's happening here and reenable.\n+  # Figure out what's happening here and re-enable.\n   # _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n   #                cuda_versions.cusolver_build_version,\n   #                # Ignore patch versions.\ndiff --git a/jaxlib/config.cc b/jaxlib/config.cc\nindex 3d701c516990..625a5aa5a319 100644\n--- a/jaxlib/config.cc\n+++ b/jaxlib/config.cc\n@@ -34,7 +34,7 @@ namespace jax {\n \n namespace nb = nanobind;\n \n-// Singleton object used to represet \"value not set\" in thread-local configs.\n+// Singleton object used to represent \"value not set\" in thread-local configs.\n nb::object UnsetObject() {\n   return nb::steal(PyObject_CallObject(\n       reinterpret_cast<PyObject*>(&PyBaseObject_Type), nullptr));\n@@ -71,7 +71,7 @@ class ThreadLocalConfigState {\n   // These values are accessed in one of two ways:\n   // * The owning thread reads or writes them, while holding the GIL, or, under\n   //   free-threading, while the owning thread is in ATTACHED gc state.\n-  // * Other threads may read or clear values while performing a garbarge\n+  // * Other threads may read or clear values while performing a garbage\n   //   collection.\n   // No locking is needed because a GC thread cannot run concurrently with other\n   // Python threads; even under free-threading Python uses a stop-the-world GC.\n@@ -117,7 +117,7 @@ class GlobalConfigState {\n  private:\n   friend class Config;\n \n-  // The set of thread-local states. This is used during garbarge collection to\n+  // The set of thread-local states. This is used during garbage collection to\n   // visit thread-local values.\n   absl::Mutex mu_;\n   absl::flat_hash_set<ThreadLocalConfigState*> thread_local_states_\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex 678d92bc434a..00e26756ded9 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -173,7 +173,7 @@ def if_building_jaxlib(\n     })\n \n def _cpu_test_deps():\n-    \"\"\"Returns the test depencies needed for a CPU-only JAX test.\"\"\"\n+    \"\"\"Returns the test dependencies needed for a CPU-only JAX test.\"\"\"\n     return select({\n         \"//jax:config_build_jaxlib_true\": [],\n         \"//jax:config_build_jaxlib_false\": [\"@pypi//jaxlib\"],\ndiff --git a/jaxlib/mosaic/dialect/gpu/mosaic_gpu.cc b/jaxlib/mosaic/dialect/gpu/mosaic_gpu.cc\nindex 9d6085397493..2f3bfb808981 100644\n--- a/jaxlib/mosaic/dialect/gpu/mosaic_gpu.cc\n+++ b/jaxlib/mosaic/dialect/gpu/mosaic_gpu.cc\n@@ -441,7 +441,7 @@ llvm::LogicalResult BroadcastInDimOp::verify() {\n     }\n     if (i > 0 && dims[i] <= dims[i - 1]) {\n       return error(\n-          \"The values in the `broadcast_dimensions` attribute must be stricly \"\n+          \"The values in the `broadcast_dimensions` attribute must be strictly \"\n           \"increasing.\");\n     }\n   }\ndiff --git a/jaxlib/mosaic/dialect/gpu/mosaic_gpu.td b/jaxlib/mosaic/dialect/gpu/mosaic_gpu.td\nindex 1465f76aa7bf..217bf1a3593b 100644\n--- a/jaxlib/mosaic/dialect/gpu/mosaic_gpu.td\n+++ b/jaxlib/mosaic/dialect/gpu/mosaic_gpu.td\n@@ -405,7 +405,7 @@ def MosaicGPU_SliceSMEMOp : Op<MosaicGPU_Dialect, \"slice_smem\", []> {\n }\n \n def MosaicGPU_WGMMAOp : Op<MosaicGPU_Dialect, \"wgmma\", [InferTypeOpInterface]> {\n-  let summary = \"Multiply two matrices asyncronously using warpgroup level matrix multiply operations.\";\n+  let summary = \"Multiply two matrices asynchronously using warpgroup level matrix multiply operations.\";\n   let description = [{\n     Schedules WGMMA operations that perform the following matrix multiply and\n     accumulate:\n@@ -434,7 +434,7 @@ def MosaicGPU_WGMMAOp : Op<MosaicGPU_Dialect, \"wgmma\", [InferTypeOpInterface]> {\n     registers need to be synchronized with a memory fence.\n \n     Usually `a` is read from shared memory if it is used directly in the WGMMA\n-    operation. If `a` needs to be transfromed before it is used in the WGMMA\n+    operation. If `a` needs to be transformed before it is used in the WGMMA\n     operation, it may be more convenient to read it directly form registers.\n     This avoids the need to store the data and wait for a fence.\n   }];\ndiff --git a/jaxlib/mosaic/dialect/tpu/layout.h b/jaxlib/mosaic/dialect/tpu/layout.h\nindex 8261d09697e3..dceee9cf41a8 100644\n--- a/jaxlib/mosaic/dialect/tpu/layout.h\n+++ b/jaxlib/mosaic/dialect/tpu/layout.h\n@@ -168,7 +168,7 @@ class RectangularVregBounds : public VRegDataBounds {\n // ---\n //\n // The tiling attribute makes it possible to subdivide a single vector register\n-// into multiple subtiles that traverse the last dimension of a value. For\n+// into multiple sub-tiles that traverse the last dimension of a value. For\n // example, consider vregs of shape (4, 5) on (2, 10) array:\n //\n //   a b c d e f g h i j\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 1669d1bf1586..390ce9d3db32 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -3292,7 +3292,7 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n       // a bunch of loads!\n     } else {\n       return op.emitOpError(\n-          \"Not implemented: dismatch in memref tiling and vector tiling in \"\n+          \"Not implemented: mismatch in memref tiling and vector tiling in \"\n           \"load\");\n     }\n   }\n@@ -4772,7 +4772,7 @@ LogicalResult vector_store_impl(RewriteContext &ctx, Op store_op,\n       // us a bunch of stores!\n     } else {\n       return op.emitOpError(\n-          \"Not implemented: dismatch in memref tiling and vector tiling in \"\n+          \"Not implemented: mismatch in memref tiling and vector tiling in \"\n           \"store\");\n     }\n   }\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex 17575183bd81..7d279c5cb307 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -469,7 +469,7 @@ class VectorLayoutInferer {\n     TPU_CHECK_OP(else_yield->getOperandTypes() == op->getResultTypes(),\n                  \"scf if results and else branch yield operands do not match\");\n     auto else_yield_in_layouts = getLayoutFromOperands(else_yield);\n-    // Find a compatible layout from then and else branches for each reuslt. For\n+    // Find a compatible layout from then and else branches for each result. For\n     // example, if we yield offset (*, *) in then branch and offset (*, 0) in\n     // else branch, the result offset should be (*, 0).\n     SmallVector<Layout, 4> out_layouts;\n@@ -649,7 +649,7 @@ class VectorLayoutInferer {\n     auto yield_in_layouts = getLayoutFromOperands(yield_op);\n \n     // Find a compatible layout from condition body and loop body for each\n-    // reuslt. For example, if we yield offset (*, *) in condition body and\n+    // result. For example, if we yield offset (*, *) in condition body and\n     // offset (*, 0) in loop body, the result offset should be (*, 0).\n     SmallVector<Layout, 4> out_layouts;\n     out_layouts.reserve(op->getNumResults());\ndiff --git a/jaxlib/pjit.cc b/jaxlib/pjit.cc\nindex 804352161597..1e9d53547b1d 100644\n--- a/jaxlib/pjit.cc\n+++ b/jaxlib/pjit.cc\n@@ -653,7 +653,7 @@ absl::StatusOr<nb::object> PjitFunction::Call(nb::handle callable,\n     // development.\n     //\n     // TODO(chky): Consider support uncommitted PyArray in cpp when the python\n-    // side stablizes.\n+    // side stabilizes.\n     if (!py_array.committed() &&\n         jax::Sharding::SafeNumDevices(py_array.sharding()) > 1) {\n       VLOG(2) << \"PyArray argument is not committed and number of global \"\ndiff --git a/jaxlib/py_client.h b/jaxlib/py_client.h\nindex 520fbf8b1e59..772dba864684 100644\n--- a/jaxlib/py_client.h\n+++ b/jaxlib/py_client.h\n@@ -92,7 +92,7 @@ class PyClient {\n     return pjrt_client->shared_ptr_pjrt_client();\n   }\n \n-  // Legacy alises.\n+  // Legacy aliases.\n   std::shared_ptr<PjRtClient> shared_pjrt_client() {\n     return shared_ptr_pjrt_client();\n   }\ndiff --git a/jaxlib/xla_compiler.cc b/jaxlib/xla_compiler.cc\nindex 1b9c8c43b126..57de57b26aee 100644\n--- a/jaxlib/xla_compiler.cc\n+++ b/jaxlib/xla_compiler.cc\n@@ -196,7 +196,7 @@ absl::StatusOr<Shape> MakeShapeWithDenseLayout(\n // `subgroup_types`: indicates the subgroups of the last `subgroup_types.size()`\n //   dimensions in `dims`.\n //\n-// In practice, `reshape_dims` often maps to the axises of user defined device\n+// In practice, `reshape_dims` often maps to the axes of user defined device\n // mesh, and `transpose_perm` often maps to the user specification of how a\n // tensor is partitioned based on the axes defined in the mesh, e.g. for a mesh\n // of size 4x2x2 as AxBxC:\ndiff --git a/tests/checkify_test.py b/tests/checkify_test.py\nindex e7ae4d0468fd..050ac5314da3 100644\n--- a/tests/checkify_test.py\n+++ b/tests/checkify_test.py\n@@ -1228,7 +1228,7 @@ def while_body(s):\n \n     with self.assertRaisesRegex(ValueError, \"checkify-of-vmap-of-while\"):\n       checked_f(jnp.asarray([1., 2., 3.]), jnp.asarray([5., 2., 4.]))\n-    # TODO(lenamartens): reenable assertions below.\n+    # TODO(lenamartens): re-enable assertions below.\n     # self.assertIsNotNone(err.get())\n     # self.assertStartsWith(err.get(), \"division by zero\")\n \n@@ -1257,7 +1257,7 @@ def fun(x):\n \n     with self.assertRaisesRegex(ValueError, \"checkify-of-vmap-of-while\"):\n       checked_f(jnp.arange(5))\n-    # TODO(lenamartens): reenable assertions below.\n+    # TODO(lenamartens): re-enable assertions below.\n     # self.assertIsNone(err.get())\n \n   def test_assert_cond_no_data_dependence(self):\ndiff --git a/tests/debug_info_test.py b/tests/debug_info_test.py\nindex b5e875c03676..5d0f747a85ad 100644\n--- a/tests/debug_info_test.py\n+++ b/tests/debug_info_test.py\n@@ -131,7 +131,7 @@ def _check_tracers_and_jaxprs(self, traceable: Any,\n     mode. The debug infos in the nested Jaxprs are first converted to\n     strings using `_debug_info_to_string` and then\n     compared against `expected_jaxpr_debug_infos`. During this conversion,\n-    we strip occurences of this test file name and a line number\n+    we strip occurrences of this test file name and a line number\n     (e.g., .*/debug_info_test.py:56)\n     An element of `expected_jaxpr_debug_infos` can be a string, in which case\n     it is compared by equality, or a `re.Pattern` (the result of `re.compile`)\ndiff --git a/tests/error_check_test.py b/tests/error_check_test.py\nindex a7eeb4dbf86b..e20017a39a9b 100644\n--- a/tests/error_check_test.py\n+++ b/tests/error_check_test.py\n@@ -35,7 +35,7 @@\n \n \n # TODO: AOT tests fails with the tracer leak checker.\n-# Reenable once https://github.com/jax-ml/jax/issues/27315 is fixed.\n+# Re-enable once https://github.com/jax-ml/jax/issues/27315 is fixed.\n # @jtu.with_config(jax_check_tracer_leaks=True)\n class ErrorCheckTests(jtu.JaxTestCase):\n \ndiff --git a/tests/export_test.py b/tests/export_test.py\nindex 0dfebdcec054..2be5313b0b3a 100644\n--- a/tests/export_test.py\n+++ b/tests/export_test.py\n@@ -1548,7 +1548,7 @@ def test_multi_platform(self):\n     self.assertIn(\"jax.uses_shape_polymorphism = true\",\n                   module_str)\n \n-    # Call with argument placed on different plaforms\n+    # Call with argument placed on different platforms\n     for platform in self.platforms:\n       x_device = jax.device_put(x, jax.devices(platform)[0])\n       res_exp = exp.call(x_device)\n@@ -1573,7 +1573,7 @@ def test_multi_platform_nested(self):\n     count_sine = len(re.findall(\"stablehlo.sine\", exp2_module_str))\n     self.assertEqual(1, count_sine)\n \n-    # Call with argument placed on different plaforms\n+    # Call with argument placed on different platforms\n     for platform in self.platforms:\n       if platform == \"tpu\": continue\n       x_device = jax.device_put(x, jax.devices(platform)[0])\n@@ -1716,7 +1716,7 @@ def f_jax(b):  # b: f32[16 // DEVICES, 4]\n     res_native = f_jax(a)\n     exp = get_exported(f_jax, platforms=(\"cpu\", \"tpu\", \"cuda\", \"rocm\"))(a)\n \n-    # Call with argument placed on different plaforms\n+    # Call with argument placed on different platforms\n     for platform in self.platforms:\n       run_devices = jax.devices(platform)[0:len(export_devices)]\n       if len(run_devices) != len(export_devices):\ndiff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py\nindex cdfeeba6275b..67c19179bb8b 100644\n--- a/tests/generated_fun_test.py\n+++ b/tests/generated_fun_test.py\n@@ -218,7 +218,7 @@ def check_all_close(xs, ys, tol=1e-3):\n \n def check_close(x, y, tol=1e-3):\n   assert jnp.shape(x) == jnp.shape(y)\n-  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs\n+  # TODO(dougalm): re-enable once we've tackled the less pedantic bugs\n   # assert x.dtype == y.dtype\n   assert jnp.allclose(x, y, rtol=tol, atol=tol), \\\n      f\"Value mismatch:\\n{x}\\n  vs\\n{y}\\n\"\ndiff --git a/tests/gpu_memory_flags_test.py b/tests/gpu_memory_flags_test.py\nindex 87f60dd86f20..bada2bebc74e 100644\n--- a/tests/gpu_memory_flags_test.py\n+++ b/tests/gpu_memory_flags_test.py\n@@ -29,7 +29,7 @@ class GpuMemoryAllocationTest(absltest.TestCase):\n   @jtu.skip_under_pytest(\"Test must run in an isolated process\")\n   @unittest.skipIf(\n       \"XLA_PYTHON_CLIENT_ALLOCATOR\" in os.environ,\n-      \"Test does not work if the python client allocator has been overriden\",\n+      \"Test does not work if the python client allocator has been overridden\",\n   )\n   def test_gpu_memory_allocation(self):\n     falsey_values = (\"0\", \"False\", \"false\")\ndiff --git a/tests/lax_metal_test.py b/tests/lax_metal_test.py\nindex e44ff9ebc930..ecbf908d2f09 100644\n--- a/tests/lax_metal_test.py\n+++ b/tests/lax_metal_test.py\n@@ -3867,7 +3867,7 @@ def testItem(self, shape, dtype, num_args, use_tuple):\n       self._CheckAgainstNumpy(np_op, jnp_op, args_maker)\n \n   @jtu.sample_product(\n-    # Final dimension must be a multiple of 16 to ensure compatibilty of all dtype pairs.\n+    # Final dimension must be a multiple of 16 to ensure compatibility of all dtype pairs.\n     shape=[(0,), (32,), (2, 16)],\n     a_dtype=all_dtypes,\n     dtype=(*all_dtypes, None) if config.enable_x64.value else all_dtypes,\ndiff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py\nindex e98ac6986cae..09a081761d0e 100644\n--- a/tests/lax_numpy_test.py\n+++ b/tests/lax_numpy_test.py\n@@ -6214,7 +6214,7 @@ def test_isdtype(self, dtype, kind):\n     ],\n     dtype=float_dtypes + int_dtypes,\n   )\n-  @jtu.skip_on_devices(\"tpu\")  # TODO(jakevdp): fix and reenable this test.\n+  @jtu.skip_on_devices(\"tpu\")  # TODO(jakevdp): fix and re-enable this test.\n   @jax.numpy_rank_promotion('allow')  # This test explicitly exercises implicit rank promotion.\n   def test_trapezoid(self, yshape, xshape, dtype, dx, axis):\n     rng = jtu.rand_default(self.rng())\ndiff --git a/tests/lax_numpy_ufuncs_test.py b/tests/lax_numpy_ufuncs_test.py\nindex 905d7eed1acd..f2155afb841d 100644\n--- a/tests/lax_numpy_ufuncs_test.py\n+++ b/tests/lax_numpy_ufuncs_test.py\n@@ -543,7 +543,7 @@ def test_binary_ufunc_reduceat(self, name, shape, axis, idx_shape, dtype):\n     if (jnp_fun.nin, jnp_fun.nout) != (2, 1):\n       self.skipTest(f\"accumulate requires (nin, nout)=(2, 1); got {(jnp_fun.nin, jnp_fun.nout)=}\")\n     if name in ['add', 'multiply'] and dtype == bool:\n-      # TODO(jakevdp): figure out how to fix thest cases.\n+      # TODO(jakevdp): figure out how to fix test cases.\n       self.skipTest(f\"known failure for {name}.reduceat with {dtype=}\")\n \n     rng = jtu.rand_default(self.rng())\ndiff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py\nindex e0d3528dfa41..610bf5fabefd 100644\n--- a/tests/lax_scipy_test.py\n+++ b/tests/lax_scipy_test.py\n@@ -646,7 +646,7 @@ def test_spence(self, shape, dtype):\n     ],\n     dtype=float_dtypes + int_dtypes,\n   )\n-  @jtu.skip_on_devices(\"tpu\")  # TODO(jakevdp): fix and reenable this test.\n+  @jtu.skip_on_devices(\"tpu\")  # TODO(jakevdp): fix and re-enable this test.\n   @jax.numpy_rank_promotion('allow')  # This test explicitly exercises implicit rank promotion.\n   def testIntegrateTrapezoid(self, yshape, xshape, dtype, dx, axis):\n     rng = jtu.rand_default(self.rng())\ndiff --git a/tests/lax_test.py b/tests/lax_test.py\nindex a11c989fc9c5..4d30cd70ea70 100644\n--- a/tests/lax_test.py\n+++ b/tests/lax_test.py\n@@ -4395,7 +4395,7 @@ def _testOnComplexPlaneWorker(self, name, dtype, kind):\n     #\n     # In addition, the 1/3 middle parts of regions q1, q2, q3, q4,\n     # neg, pos are tested separately as these don't contain extremely\n-    # small or extremelly large values and functions on these regions\n+    # small or extremely large values and functions on these regions\n     # ought not to possess any incorrectness issues.\n \n     s0, s1 = size_re, size_im\ndiff --git a/tests/mosaic/gpu_dialect_test.py b/tests/mosaic/gpu_dialect_test.py\nindex af8b296fe536..5fc6bc8c703c 100644\n--- a/tests/mosaic/gpu_dialect_test.py\n+++ b/tests/mosaic/gpu_dialect_test.py\n@@ -711,7 +711,7 @@ def test_broadcast_in_dim_dim_transpose(self):\n \n     with self.assertRaisesRegex(\n         ir.MLIRError,\n-        r\"`broadcast_dimensions` attribute must be stricly increasing\",\n+        r\"`broadcast_dimensions` attribute must be strictly increasing\",\n     ):\n       self.module.operation.verify()\n \ndiff --git a/tests/pallas/mgpu_collective_matmul_test.py b/tests/pallas/mgpu_collective_matmul_test.py\nindex 3760c7ccddb7..b1b7e0ffd118 100644\n--- a/tests/pallas/mgpu_collective_matmul_test.py\n+++ b/tests/pallas/mgpu_collective_matmul_test.py\n@@ -90,7 +90,7 @@ def test_all_gather_lhs_matmul(\n     if n_shard != block_n:\n       self.skipTest(\"n_shard must be equal to block_n for now.\")\n     if n_shard % block_n:\n-      self.skipTest(\"n_shard must be divisble by block_n for now.\")\n+      self.skipTest(\"n_shard must be divisible by block_n for now.\")\n     if m_shard % block_m:\n       self.skipTest(\"m_shard must be divisible by block_m for now.\")\n \ndiff --git a/tests/pallas/tpu_fusible_matmul_test.py b/tests/pallas/tpu_fusible_matmul_test.py\nindex 4bde9b95483b..ae56d3db2f3a 100644\n--- a/tests/pallas/tpu_fusible_matmul_test.py\n+++ b/tests/pallas/tpu_fusible_matmul_test.py\n@@ -416,7 +416,7 @@ def matmul_slice_ref(x, y, b, i, j, k):\n \n   @parameterized.parameters('float32', 'bfloat16')\n   def test_matmul_input_concat_output(self, dtype):\n-    self.skipTest('select_n doesnt support more than 3 elements')\n+    self.skipTest('select_n does not support more than 3 elements')\n     # TODO(sharadmv): fix this test\n     k0, k1, k2, k3 = jax.random.split(jax.random.key(0), 4)\n     x = jax.random.normal(k0, (128, 128), dtype)\ndiff --git a/tests/pgle_test.py b/tests/pgle_test.py\nindex 7087bcad58bf..e03e5127d023 100644\n--- a/tests/pgle_test.py\n+++ b/tests/pgle_test.py\n@@ -157,7 +157,7 @@ def f(x):\n \n       with config.pgle_profiling_runs(2), config.enable_pgle(True):\n         # Run 1: Module should be compiled without FDO. Two modules are expected\n-        # One is the funtion f, the other one is multi slice module\n+        # One is the function f, the other one is multi slice module\n         with jtu.count_pjit_cpp_cache_miss() as cache_miss_count:\n           self.assertArraysEqual(f(x), expected)\n         self.assertEqual(cache_miss_count(), 2)\ndiff --git a/tests/scaled_matmul_stablehlo_test.py b/tests/scaled_matmul_stablehlo_test.py\nindex d2483966c984..fb5a7560d947 100644\n--- a/tests/scaled_matmul_stablehlo_test.py\n+++ b/tests/scaled_matmul_stablehlo_test.py\n@@ -280,9 +280,9 @@ def setUp(self):\n       self.skipTest(str(e))\n       return\n     if _dtypes.float8_e8m0fnu is None:\n-      self.skipTest(\"Requries >= ml_dtypes 0.5.0 to support float8_e8m0fnu\")\n+      self.skipTest(\"Requires >= ml_dtypes 0.5.0 to support float8_e8m0fnu\")\n     if _dtypes.float4_e2m1fn is None:\n-      self.skipTest(\"Requries >= ml_dtypes 0.5.0 to support float4_e2m1fn\")\n+      self.skipTest(\"Requires >= ml_dtypes 0.5.0 to support float4_e2m1fn\")\n     if cudnn_version < 90700:\n       self.skipTest(\"Requires >= cuDNN 9.7.0\")\n     if not jtu.is_cuda_compute_capability_at_least(\"10.0\"):\n@@ -473,7 +473,7 @@ def setUp(self):\n       self.skipTest(str(e))\n       return\n     if _dtypes.float8_e8m0fnu is None:\n-      self.skipTest(\"Requries >= ml_dtypes 0.5.0 to support float8_e8m0fnu\")\n+      self.skipTest(\"Requires >= ml_dtypes 0.5.0 to support float8_e8m0fnu\")\n     if cudnn_version < 90700:\n       self.skipTest(\"Requires >= cuDNN 9.7.0\")\n     if not jtu.is_cuda_compute_capability_at_least(\"10.0\"):\ndiff --git a/tests/shape_poly_test.py b/tests/shape_poly_test.py\nindex 6d1ffe744ed9..68aaf4e29553 100644\n--- a/tests/shape_poly_test.py\n+++ b/tests/shape_poly_test.py\n@@ -961,7 +961,7 @@ def test_constraints_ge_complex_gen(self,\n     self.assertEqual(bounds, _bounds(exp))\n \n   def test_constraints_ge_override(self):\n-    # Some constaints override other\n+    # Some constraints override other\n     a, b = shape_poly.symbolic_shape(\"a, b\",\n                                      constraints=(\"a >= 5\", \"b <= 16\",\n                                                   \"a >= 10\", \"b <= 10\"))\n@@ -979,7 +979,7 @@ def test_constraint_eq_0(self):\n     self.assertIs(d, 5)\n \n   def test_constraints_eq_1(self):\n-    # Some constaints override other\n+    # Some constraints override other\n     a, b, c = shape_poly.symbolic_shape(\"a, b, c\",\n                                         constraints=(\"max(a, b) == c\",))\n     self.assertEqual(_bounds(core.max_dim(a, b) - c + 3), (3, 3))\ndiff --git a/third_party/repo.bzl b/third_party/repo.bzl\nindex 17e0bbb03542..185c5a4294dc 100644\n--- a/third_party/repo.bzl\n+++ b/third_party/repo.bzl\n@@ -129,7 +129,7 @@ def tf_http_archive(name, sha256, urls, **kwargs):\n         \"storage.googleapis.com\",\n     )]):\n         fail(\"The first entry of tf_http_archive(urls) must be a mirror \" +\n-             \"URL, preferrably mirror.tensorflow.org. Even if you don't have \" +\n+             \"URL, preferably mirror.tensorflow.org. Even if you don't have \" +\n              \"permission to mirror the file, please put the correctly \" +\n              \"formatted mirror URL there anyway, because someone will come \" +\n              \"along shortly thereafter and mirror the file.\")\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add a test for TMA multicasts in pallas. This also effectively tests `lax.axis_index` and `lax.axis_size` on clusters axes.\n\nPiperOrigin-RevId: 767185068",
        "output": "```diff\nCommit: 8c348652d687429a2f2de30cca82bf905f57e622\nDate: 2025-06-04T16:53:21Z\nURL: https://github.com/jax-ml/jax/commit/8c348652d687429a2f2de30cca82bf905f57e622\nFiles changed: 1\nAdditions: +87, Deletions: -0\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 6cedbc6ae14c..1170b2ac5cdb 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -15,6 +15,7 @@\n import contextlib\n import dataclasses\n import functools\n+import itertools\n import math\n import operator\n import os\n@@ -1787,6 +1788,92 @@ def body(x_ref, y_ref, barrier):\n     else:\n       self.fail(\"Should have raised an exception\")\n \n+  @parameterized.named_parameters(\n+    (\n+        f\"_{''.join(map(str, collective_dims))}={collective_size}{'_' + ''.join(map(str, noncollective_dims)) if noncollective_dims else ''}\",\n+        collective_dims,\n+        noncollective_dims,\n+        collective_size,\n+    )\n+    for collective_dims in itertools.chain.from_iterable(\n+        itertools.combinations(\"xyz\", n) for n in range(1, 4)\n+    )\n+    for noncollective_dims in itertools.chain.from_iterable(\n+        itertools.combinations(\"xyz\", n) for n in range(3)\n+    )\n+    for collective_size in (1, 2, 4)\n+    if all(d not in noncollective_dims for d in collective_dims)\n+  )\n+  def test_tma_load_multicast(self, collective_dims, noncollective_dims, collective_dim_size):\n+    \"\"\"\n+      1. Broadcast a GMEM slice to SMEM across collective CTAs.\n+      2. Send a SMEM slice from each collective CTA to reconstruct the GMEM slice.\n+        It's not strictly necessary to use every collective CTA, but we use them\n+        to test that the cluster axes are used correctly.\n+    \"\"\"\n+\n+    dtype = jnp.float16\n+    cluster = [1, 1, 1]\n+    for d in collective_dims:\n+      cluster[\"xyz\".index(d)] = collective_dim_size\n+    for d in noncollective_dims:\n+      cluster[\"xyz\".index(d)] = 2\n+    if math.prod(cluster) > 16:\n+      self.skipTest(\"Cluster is too big.\")\n+\n+    collective_size = math.prod(cluster[\"xyz\".index(d)] for d in collective_dims)\n+    noncollective_size = math.prod(cluster) // collective_size\n+\n+    swizzle = 128\n+    swizzle_elems = swizzle // jnp.dtype(dtype).itemsize\n+    transforms = (\n+        plgpu.TilingTransform((8, swizzle_elems)),\n+        plgpu.SwizzleTransform(swizzle),\n+    )\n+    shape = (noncollective_size, collective_size * 8, swizzle_elems)\n+\n+    def body(x_gmem, out_gmem, smem, tma_barrier):\n+      # Compute the index in a subset of the cluster.\n+      def cluster_id(axes):\n+        idx, stride = 0, 1\n+        for d in sorted(axes):\n+          idx += lax.axis_index(d) * stride\n+          stride *= lax.axis_size(d)\n+        return idx\n+\n+      noncollective_idx = cluster_id(noncollective_dims)\n+      collective_idx = cluster_id(collective_dims)\n+\n+      plgpu.copy_gmem_to_smem(\n+            x_gmem.at[noncollective_idx],\n+            smem,\n+            tma_barrier,\n+            collective_axes=collective_dims)\n+      plgpu.barrier_wait(tma_barrier)\n+\n+      plgpu.commit_smem()\n+      collective_slice = pl.ds(8 * collective_idx, 8)\n+      plgpu.copy_smem_to_gmem(\n+          smem.at[collective_slice],\n+          out_gmem.at[noncollective_idx, collective_slice, :],\n+      )\n+      plgpu.wait_smem_to_gmem(0)\n+\n+    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n+    kernel = plgpu.kernel(\n+      body,\n+      grid=cluster,\n+      grid_names=(\"grid_x\", \"grid_y\", \"grid_z\"),\n+      cluster=cluster,\n+      cluster_names=(\"x\", \"y\", \"z\"),\n+      out_shape=jax.ShapeDtypeStruct(shape, dtype),\n+      scratch_shapes=(\n+        plgpu.SMEM(shape[1:], dtype, transforms=transforms),\n+        plgpu.Barrier(),\n+      )\n+    )\n+    np.testing.assert_array_equal(kernel(x), x)\n+\n \n class PallasCallWarpPrimitiveSemanticsTest(PallasTest):\n   def setUp(self):\n\n```",
        "from_id": [
            "Rifur13",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Make `unreduced` argument in `PartitionSpec` a `set | frozenset` instead of a `tuple`. This is because the order of axes in `unreduced` doesn't matter.\n\nWhile lowering, `unreduced` is sorted wrt the mesh axis names so in McJAX all hosts lower to the same thing.\n\nPiperOrigin-RevId: 767178835",
        "output": "```diff\nCommit: b3db37426a967936c7ec95f66ac6ff018ae23bdf\nDate: 2025-06-04T16:34:35Z\nURL: https://github.com/jax-ml/jax/commit/b3db37426a967936c7ec95f66ac6ff018ae23bdf\nFiles changed: 6\nAdditions: +60, Deletions: -79\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex d8481afe872a..91ddd77e3d49 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -1924,8 +1924,8 @@ def modify_spec_for_auto_manual(spec, mesh) -> P:\n       temp_s = s[0] if isinstance(s, tuple) else s\n       new_spec.append(s if mesh._name_to_type[temp_s] == AxisType.Explicit\n                       else None)\n-  new_unreduced = tuple(u for u in spec.unreduced\n-                        if mesh._name_to_type[u] == AxisType.Explicit)\n+  new_unreduced = {u for u in spec.unreduced\n+                   if mesh._name_to_type[u] == AxisType.Explicit}\n   return P(*new_spec, unreduced=new_unreduced)\n \n def _maybe_modify_sharding(sharding, ndim):\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex 43dffc7bef9c..389960a7c94d 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -5228,7 +5228,7 @@ def _dot_general_sharding_rule(lhs, rhs, *, dimension_numbers, precision,\n             ' out_sharding provided to dot_general mentions unreduced_axes.'\n             f' Got {out_sharding=}, {lhs_contracting_spec=},'\n             f' {rhs_contracting_spec=}')\n-      if out_sharding.spec.unreduced != lhs_contracting_spec:\n+      if out_sharding.spec.unreduced != frozenset(lhs_contracting_spec):\n         raise core.ShardingTypeError(\n             \"out_sharding's unreduced axes should be equal to the contracting\"\n             f' specs. Got unreduced axes={out_sharding.spec.unreduced} and'\ndiff --git a/jax/_src/named_sharding.py b/jax/_src/named_sharding.py\nindex faf0b2a9f2b2..13124d4a36aa 100644\n--- a/jax/_src/named_sharding.py\n+++ b/jax/_src/named_sharding.py\n@@ -302,7 +302,7 @@ class SdyArray:\n   dim_shardings: Sequence[SdyDim]\n   logical_device_ids: tuple[int, ...] | None = None\n   replicated_axes: tuple[str, ...] = ()\n-  unreduced_axes: tuple[str, ...] = ()\n+  unreduced_axes: frozenset[str] = frozenset()\n \n   def build(self) -> sdy.TensorShardingAttr:\n     if self.mesh_shape is None:\n@@ -503,24 +503,11 @@ def _check_mesh_resource_axis(mesh, pspec):\n         f' axis_types: {mesh._axis_types_dict}')\n \n def _check_mesh_unreduced(mesh, pspec):\n-  counts = {}\n-  duplicate = False\n   for u in pspec.unreduced:\n     if u not in mesh.axis_names:\n       raise ValueError(\n           f'Unreduced axes {u} is not found in {mesh.axis_names=}. '\n           f'Got {pspec=}')\n-    count = counts.get(u, 0)\n-    if count > 0:\n-      duplicate = True\n-    counts[u] = count + 1\n-  if duplicate:\n-    multiple_uses = [r for r, c in counts.items() if c > 1]\n-    raise ValueError(\n-        f'Unreduced axes in {pspec} has duplicate entries which is not allowed.'\n-        f' Got {mesh_lib.show_axes(multiple_uses)}')\n-\n-  for u in pspec.unreduced:\n     if mesh._name_to_type[u] in (AxisType.Auto, AxisType.Manual):\n       raise ValueError(\n           'Unreduced axes can only refer to mesh axes that is of type'\ndiff --git a/jax/_src/partition_spec.py b/jax/_src/partition_spec.py\nindex 2c833e6544e4..629af80ed38b 100644\n--- a/jax/_src/partition_spec.py\n+++ b/jax/_src/partition_spec.py\n@@ -13,6 +13,7 @@\n # limitations under the License.\n \n from __future__ import annotations\n+from collections.abc import Set\n from typing import Any\n \n class UnconstrainedSingleton:\n@@ -44,11 +45,10 @@ def _canonicalize_partition(partition):\n   return partition\n \n def _check(partitions, unreduced):\n-  us = set(unreduced)\n   for p in partitions:\n     p = p if isinstance(p, tuple) else (p,)\n     for r in p:\n-      if r in us:\n+      if r in unreduced:\n         raise ValueError(\n             \"partitions cannot overlap with unreduced axes passed to\"\n             f\" PartitionSpec. Got partitions: {partitions} and unreduced axes:\"\n@@ -58,7 +58,7 @@ def _check(partitions, unreduced):\n         \"unreduced cannot contain None. All elements in unreduced should refer\"\n         \" to the mesh axes.\")\n \n-def unpicke_pspec(partitions, unreduced):\n+def unpickle_pspec(partitions, unreduced):\n   return PartitionSpec(*partitions, unreduced=unreduced)\n \n AxisName = Any\n@@ -72,34 +72,32 @@ class PartitionSpec:\n   This class exists so JAX's pytree utilities can distinguish a partition\n   specifications from tuples that should be treated as pytrees.\n   \"\"\"\n-  __slots__ = (\"_partitions\", \"_unreduced\")\n+  __slots__ = (\"_partitions\", \"unreduced\")\n   __match_args__ = (\"_partitions\",)\n \n   # A sentinel value representing a dim is unconstrained.\n   UNCONSTRAINED = _UNCONSTRAINED_PARTITION\n \n   def __init__(self, *partitions,\n-               unreduced: tuple[AxisName, ...] | AxisName | None = None):\n+               unreduced: Set[AxisName] | None = None):\n     self._partitions = tuple(_canonicalize_partition(p) for p in partitions)\n-    self._unreduced = (\n-        () if unreduced is None else tuple(unreduced)\n-        if isinstance(unreduced, (list, tuple)) else (unreduced,))\n-    _check(self._partitions, self._unreduced)\n-\n-  @property\n-  def unreduced(self):\n-    return self._unreduced\n+    if unreduced is not None and not isinstance(unreduced, (set, frozenset)):\n+      raise TypeError(\n+          \"`unreduced` argument of PartitionSpec should be `None` or of type\"\n+          f\" `frozenset` or `set`. Got type {type(unreduced)}\")\n+    self.unreduced = frozenset() if unreduced is None else frozenset(unreduced)\n+    _check(self._partitions, self.unreduced)\n \n   def __repr__(self):\n     pr = repr(self._partitions)[1:-1]\n-    if not self._unreduced:\n+    if not self.unreduced:\n       return f\"PartitionSpec({pr})\"\n-    ur_str = f\"unreduced={self._unreduced!r}\"\n+    ur_str = f\"unreduced={set(self.unreduced)!r}\"\n     pr = '' if not pr else f\"{pr} \" if pr.endswith(',') else f\"{pr}, \"\n     return (f\"PartitionSpec({pr}{ur_str})\")\n \n   def __reduce__(self):\n-    return (unpicke_pspec, (self._partitions, self._unreduced))\n+    return (unpickle_pspec, (self._partitions, self.unreduced))\n \n   def __getitem__(self, i):\n     return self._partitions[i]\n@@ -113,9 +111,9 @@ def __len__(self):\n   def __eq__(self, other):\n     if isinstance(other, PartitionSpec):\n       return (self._partitions == other._partitions and\n-              self._unreduced == other._unreduced)\n+              self.unreduced == other.unreduced)\n     elif isinstance(other, tuple):\n-      if self._unreduced:\n+      if self.unreduced:\n         raise TypeError(\n             f\"other {other} cannot be of instance `tuple` when self {self} has\"\n             \" unreduced in `__eq__` of PartitionSpec.\")\n@@ -125,27 +123,27 @@ def __eq__(self, other):\n       return False\n \n   def __hash__(self):\n-    return hash((self._partitions, self._unreduced))\n+    return hash((self._partitions, self.unreduced))\n \n   def __add__(self, other):\n-    if not isinstance(other, (tuple, PartitionSpec)):\n-      raise NotImplementedError\n     if isinstance(other, PartitionSpec):\n       return PartitionSpec(\n           *self, *other,\n-          unreduced=(*self._unreduced, *other._unreduced))\n-    else:\n-      if self._unreduced:\n+          unreduced={*self.unreduced, *other.unreduced})\n+    elif isinstance(other, tuple):\n+      if self.unreduced:\n         raise TypeError(\n             f\"other {other} cannot be of instance `tuple` when self {self} has\"\n             \" unreduced in `__add__` of PartitionSpec.\")\n       return PartitionSpec(*self, *other)\n+    else:\n+      raise NotImplementedError\n \n   def __radd__(self, other):\n     if not isinstance(other, tuple):\n       raise NotImplementedError\n     # other will always be a tuple.\n-    if self._unreduced:\n+    if self.unreduced:\n       raise TypeError(\n           f\"other {other} cannot be of instance `tuple` when self {self} has\"\n           \" unreduced in `__radd__` of PartitionSpec.\")\n@@ -158,7 +156,7 @@ def count(self, value):\n     return self._partitions.count(_canonicalize_partition(value))\n \n   def with_partitions(self, new_partitions):\n-    return PartitionSpec(*new_partitions, unreduced=self._unreduced)\n+    return PartitionSpec(*new_partitions, unreduced=self.unreduced)\n \n   def with_unreduced(self, new_unreduced):\n     return PartitionSpec(*self._partitions, unreduced=new_unreduced)\ndiff --git a/tests/array_test.py b/tests/array_test.py\nindex 44734e64a995..4403755a1dff 100644\n--- a/tests/array_test.py\n+++ b/tests/array_test.py\n@@ -1412,67 +1412,63 @@ def test_memory_kind_with_abstract_mesh(self):\n       NamedSharding(abstract_mesh, P(), memory_kind='weird_device')\n \n   def test_pspec_unreduced(self):\n-    pspec1 = P('a', 'b', None, unreduced=('c',))\n+    pspec1 = P('a', 'b', None, unreduced={'c'})\n     self.assertEqual(repr(pspec1),\n-                     \"PartitionSpec('a', 'b', None, unreduced=('c',))\")\n+                     \"PartitionSpec('a', 'b', None, unreduced={'c'})\")\n \n-    pspec2 = P('a', 'b', None, unreduced=('c',))\n+    pspec2 = P('a', 'b', None, unreduced={'c'})\n     self.assertEqual(pspec1, pspec2)\n \n-    pspec3 = P('a', 'b', None, unreduced=('d',))\n+    pspec3 = P('a', 'b', None, unreduced={'d'})\n     self.assertNotEqual(pspec1, pspec3)\n \n-    out = P('x', unreduced=('z',)) + P('a', unreduced='b')\n-    self.assertEqual(out, P('x', 'a', unreduced=('z', 'b')))\n+    out = P('x', unreduced={'z'}) + P('a', unreduced={'b'})\n+    self.assertEqual(out, P('x', 'a', unreduced={'z', 'b'}))\n \n-    pspec4 = P('x', unreduced='y')\n+    pspec4 = P('x', unreduced={'y'})\n     self.assertEqual(repr(pspec4),\n-                     \"PartitionSpec('x', unreduced=('y',))\")\n+                     \"PartitionSpec('x', unreduced={'y'})\")\n \n-    pspec5 = P(None, None, unreduced='x')\n+    pspec5 = P(None, None, unreduced={'x'})\n     self.assertEqual(repr(pspec5),\n-                     \"PartitionSpec(None, None, unreduced=('x',))\")\n+                     \"PartitionSpec(None, None, unreduced={'x'})\")\n \n-    pspec6 = P(None, unreduced='x')\n-    self.assertEqual(repr(pspec6), \"PartitionSpec(None, unreduced=('x',))\")\n+    pspec6 = P(None, unreduced={'x'})\n+    self.assertEqual(repr(pspec6), \"PartitionSpec(None, unreduced={'x'})\")\n \n-    pspec7 = P(unreduced='x')\n-    self.assertEqual(repr(pspec7), \"PartitionSpec(unreduced=('x',))\")\n+    pspec7 = P(unreduced={'x'})\n+    self.assertEqual(repr(pspec7), \"PartitionSpec(unreduced={'x'})\")\n \n     with self.assertRaisesRegex(\n         TypeError, 'unreduced in `__add__` of PartitionSpec'):\n-      P('x', unreduced=('z',)) + (None,) * 2\n+      P('x', unreduced={'z'}) + (None,) * 2\n \n     with self.assertRaisesRegex(\n         TypeError, \"unreduced in `__radd__` of PartitionSpec\"):\n-      (None,) * 2 + P('x', unreduced='y')\n+      (None,) * 2 + P('x', unreduced={'y'})\n \n     with self.assertRaisesRegex(\n         ValueError, \"partitions cannot overlap with unreduced\"):\n-      P('x', 'y', unreduced='x')\n+      P('x', 'y', unreduced={'x'})\n \n     with self.assertRaisesRegex(\n         ValueError, \"partitions cannot overlap with unreduced\"):\n-      P('x', None, 'y', unreduced=('z', 'y'))\n+      P('x', None, 'y', unreduced={'z', 'y'})\n \n   def test_named_sharding_unreduced_error(self):\n     mesh = jtu.create_mesh((1, 1, 1), ('x', 'y', 'z'))\n \n     with self.assertRaisesRegex(\n         ValueError, \"Unreduced axes.*not found in mesh.*\"):\n-      NamedSharding(mesh, P('x', unreduced='a'))\n-\n-    with self.assertRaisesRegex(\n-        ValueError, \"Unreduced.*has duplicate entries\"):\n-      NamedSharding(mesh, P('x', unreduced=('y', 'y')))\n+      NamedSharding(mesh, P('x', unreduced={'a'}))\n \n     with self.assertRaisesRegex(\n         ValueError, \"Unreduced axes can only refer to mesh axes.*Explicit\"):\n-      NamedSharding(mesh, P('x', unreduced=('y', 'z')))\n+      NamedSharding(mesh, P('x', unreduced={'y', 'z'}))\n \n     with self.assertRaisesRegex(\n         ValueError, \"unreduced cannot contain None.*\"):\n-      NamedSharding(mesh, P('x', unreduced=('y', None)))\n+      NamedSharding(mesh, P('x', unreduced={'y', None}))\n \n   def test_hlo_sharding_get_axis_sizes(self):\n     if jaxlib_extension_version < 343:\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 04df138e68e2..b08d462b3485 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7781,14 +7781,14 @@ def test_unreduced_basic(self, mesh):\n \n     @jax.jit\n     def f(x, y, a, b):\n-      m1 = jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='y'))\n-      self.assertEqual(m1.aval.sharding.spec, P('x', None, unreduced='y'))\n+      m1 = jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced={'y'}))\n+      self.assertEqual(m1.aval.sharding.spec, P('x', None, unreduced={'y'}))\n \n-      m2 = jnp.einsum('xy,yz->xz', a, b, out_sharding=P('x', unreduced='y'))\n-      self.assertEqual(m2.aval.sharding.spec, P('x', None, unreduced='y'))\n+      m2 = jnp.einsum('xy,yz->xz', a, b, out_sharding=P('x', unreduced={'y'}))\n+      self.assertEqual(m2.aval.sharding.spec, P('x', None, unreduced={'y'}))\n \n       s = m1 + m2  # unreduced\n-      self.assertEqual(s.aval.sharding.spec, P('x', None, unreduced='y'))\n+      self.assertEqual(s.aval.sharding.spec, P('x', None, unreduced={'y'}))\n \n       out = reshard(s, P('x'))  # reduce\n       self.assertEqual(out.aval.sharding.spec, P('x', None))\n@@ -7808,7 +7808,7 @@ def test_dot_general_unreduced_error(self, mesh):\n \n     @jax.jit\n     def f(x, y):\n-      return jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='z'))\n+      return jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced={'z'}))\n     with self.assertRaisesRegex(\n         core.ShardingTypeError,\n         \"unreduced axes should be equal to the contracting specs\"):\n@@ -7819,7 +7819,7 @@ def f(x, y):\n     y = jax.device_put(np_inp.T, P(None, None))\n     @jax.jit\n     def g(x, y):\n-      return jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='y'))\n+      return jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced={'y'}))\n     with self.assertRaisesRegex(\n         core.ShardingTypeError,\n         \"lhs and rhs contracting dims should be sharded identically\"):\n@@ -7831,7 +7831,7 @@ def g(x, y):\n \n     @jax.jit\n     def h(x, y):\n-      return jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='y'))\n+      return jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced={'y'}))\n     with self.assertRaisesRegex(\n         core.ShardingTypeError,\n         \"unreduced axes should be equal to the contracting specs\"):\n@@ -7847,8 +7847,8 @@ def test_add_unreduced_error(self, mesh):\n \n     @jax.jit\n     def f(x, y, a, b):\n-      m1 = jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='y'))\n-      m2 = jnp.einsum('xy,yz->xz', a, b, out_sharding=P('x', unreduced='z'))\n+      m1 = jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced={'y'}))\n+      m2 = jnp.einsum('xy,yz->xz', a, b, out_sharding=P('x', unreduced={'z'}))\n       return m1 + m2\n \n     with self.assertRaisesRegex(\n@@ -7858,7 +7858,7 @@ def f(x, y, a, b):\n \n     @jax.jit\n     def g(x, y):\n-      m1 = jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='y'))\n+      m1 = jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced={'y'}))\n       m2 = jnp.einsum('xy,yz->xz', a, b, out_sharding=P('x'))\n       return m1 + m2\n \n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add not-implemented sharding rule in `third_party/py/jax/_src/cudnn/fused_attention_stablehlo.py`.\n\nPiperOrigin-RevId: 767166345",
        "output": "```diff\nCommit: e19e18d4d7a873661c48f774f4e282505264fed7\nDate: 2025-06-04T15:59:48Z\nURL: https://github.com/jax-ml/jax/commit/e19e18d4d7a873661c48f774f4e282505264fed7\nFiles changed: 1\nAdditions: +7, Deletions: -2\ndiff --git a/jax/_src/cudnn/fused_attention_stablehlo.py b/jax/_src/cudnn/fused_attention_stablehlo.py\nindex 46df84e08e0f..e15d9ddbce09 100644\n--- a/jax/_src/cudnn/fused_attention_stablehlo.py\n+++ b/jax/_src/cudnn/fused_attention_stablehlo.py\n@@ -1078,16 +1078,21 @@ def sharded_impl(*args):\n     _dot_product_attention_bwd_p_wrapper\n ] = _dot_product_attention_bwd_batcher\n \n+def not_implemented_sharding_rule(*args, **kwargs):\n+  return NotImplementedError(\"Sharding rule not implemented.\")\n+\n _dot_product_attention_fwd_lower.def_partition(\n   infer_sharding_from_operands=_dot_product_attention_fwd_infer_sharding_from_operands,\n-  partition=_dot_product_attention_fwd_partition)\n+  partition=_dot_product_attention_fwd_partition,\n+  sharding_rule=not_implemented_sharding_rule)\n \n mlir.register_lowering(_dot_product_attention_fwd_p_wrapper,\n                         mlir.lower_fun(_dot_product_attention_fwd_lower, multiple_results=True))\n \n _dot_product_attention_bwd_lower.def_partition(\n   infer_sharding_from_operands=_dot_product_attention_bwd_infer_sharding_from_operands,\n-  partition=_dot_product_attention_bwd_partition)\n+  partition=_dot_product_attention_bwd_partition,\n+  sharding_rule=not_implemented_sharding_rule)\n \n mlir.register_lowering(_dot_product_attention_bwd_p_wrapper,\n                         mlir.lower_fun(_dot_product_attention_bwd_lower, multiple_results=True))\n\n```",
        "from_id": [
            "ZixuanJiang",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reverts 6cd196a5db22b8db0ed4000e4cf67ad748bf52f3\n\nPiperOrigin-RevId: 767149635",
        "output": "```diff\nCommit: 8d8cc2bca67fc75718b73337c9ce19d6b77065e9\nDate: 2025-06-04T15:09:55Z\nURL: https://github.com/jax-ml/jax/commit/8d8cc2bca67fc75718b73337c9ce19d6b77065e9\nFiles changed: 1\nAdditions: +11, Deletions: -46\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex b9ef8f49f801..b5e588cbc10e 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -356,6 +356,16 @@ def _different_device_order_reshard(x, target_sharding, copy: CopySemantics):\n     return api.jit(_identity_fn, out_shardings=target_sharding,\n                    donate_argnums=donate_argnums)(x)\n \n+  if inp_sharding.device_set != target_sharding.device_set:\n+    inp_ids = [d.id for d in inp_sharding._device_assignment]\n+    inp_plat = inp_sharding._device_assignment[0].platform.upper()\n+    target_ids = [d.id for d in target_sharding._device_assignment]\n+    target_plat = target_sharding._device_assignment[0].platform.upper()\n+    raise ValueError(\"Input and target sharding should have the same set of \"\n+                     f\"devices. Got input's device set ids: {inp_ids} on \"\n+                     f\"platform {inp_plat} and target sharding's device set \"\n+                     f\"ids: {target_ids} on platform {target_plat}\")\n+\n   if inp_sharding.is_fully_replicated:\n     permute_order = None\n   else:\n@@ -379,25 +389,6 @@ def _reorder_shards(x, new_s, copy_semantics: CopySemantics):\n   return xc.reorder_shards(x, new_s, xc_copy_semantics)  # type: ignore\n \n \n-@util.cache()\n-def _is_supported_cross_host_transfer(ndim, src_sharding, dst_sharding):\n-  \"\"\"Returns True if src->dst is a supported cross-host transfer.\"\"\"\n-  backend = xla_bridge.get_backend()\n-  # There is experimental support for cross-host device transfers on TFRT TPU\n-  # backends only.\n-  if (xla_bridge.process_count() == 1 or backend.platform != \"tpu\" or\n-      \"TFRT TPU\" not in backend.platform_version):\n-    return False\n-  if (src_sharding._to_xla_hlo_sharding(ndim) !=\n-      dst_sharding._to_xla_hlo_sharding(ndim)):\n-    return False\n-  # This check excludes the case where the source and destination shardings\n-  # have the same process index sets but there are shards that require\n-  # cross-host transfers. This case is supportable but expensive to check for.\n-  return (src_sharding._internal_device_list.process_indices !=\n-          dst_sharding._internal_device_list.process_indices)\n-\n-\n @dataclasses.dataclass(frozen=True)\n class _DeferredShardArg:\n   \"\"\"Deferred call to `pxla.shard_args`.\n@@ -428,8 +419,7 @@ def _device_put_sharding_impl(x, aval, device, copy):\n       return x\n \n     if (not s.is_fully_addressable and\n-        isinstance(x, array.ArrayImpl) and not x.is_fully_addressable and\n-        s.device_set == x.sharding.device_set):\n+        isinstance(x, array.ArrayImpl) and not x.is_fully_addressable):\n       assert isinstance(s, Sharding)\n       return _different_device_order_reshard(x, s, copy)\n \n@@ -440,32 +430,7 @@ def _device_put_sharding_impl(x, aval, device, copy):\n       assert isinstance(s, Sharding)\n       return _different_device_order_reshard(x, s, copy)\n \n-    # There is experimental support for cross-host device transfers on TFRT TPU.\n-    if (isinstance(x, array.ArrayImpl) and x._committed\n-        and _is_supported_cross_host_transfer(x.ndim, x.sharding, s)):\n-      return xc.batched_copy_array_to_devices_with_sharding(\n-          [x], [s._internal_device_list], [s],  # pytype: disable=attribute-error\n-          pxla.to_xc_copy_semantics([copy]))[0]\n-\n     if not s.is_fully_addressable:\n-      # If both the source and target shardings are not fully addressable and\n-      # one of the above conditions has not been met, then assume that the user\n-      # is attempting a different device order reshard.\n-      if (isinstance(x, array.ArrayImpl) and not x.is_fully_addressable\n-          and s.device_set != x.sharding.device_set):\n-        inp_ids = [d.id for d in x.sharding._device_assignment]\n-        inp_plat = x.sharding._device_assignment[0].platform.upper()\n-        target_ids = [d.id for d in s._device_assignment]\n-        target_plat = s._device_assignment[0].platform.upper()\n-        raise ValueError(\n-            \"For a cross-host reshard in multi-controller JAX, input and target\"\n-            \" sharding should have the same set of devices. Got input's device\"\n-            f\" set ids: {inp_ids} on platform {inp_plat} and target sharding's\"\n-            f\" device set ids: {target_ids} on platform {target_plat}.\\n\\n\"\n-            \"There is experimental support for cross-host transfers with \"\n-            \"different device sets, when input/output shardings have the same \"\n-            \"indices and layouts, in the TFRT TPU runtime only.\")\n-\n       if ((isinstance(x, array.ArrayImpl) and not x._committed) or\n           type(x) in array_types or type(x) in dtypes.python_scalar_dtypes):\n         # If all hosts participate in the sharding, assert that the input is the\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix typos discovered by codespell",
        "output": "```diff\nCommit: 2226be4569d6572139bac32ac0e0ed1410af5d0e\nDate: 2025-06-04T14:55:08Z\nURL: https://github.com/jax-ml/jax/commit/2226be4569d6572139bac32ac0e0ed1410af5d0e\nFiles changed: 119\nAdditions: +209, Deletions: -209\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex afd15a357b48..c43cc7fb9b4e 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -30,7 +30,7 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n \n * Changes\n   * Additional checking for the versions of CUDA package dependencies was\n-    reenabled, having been accidentally disabled in a previous release.\n+    re-enabled, having been accidentally disabled in a previous release.\n   * JAX nightly packages are now published to artifact registry. To install\n     these packages, see the [JAX installation guide](https://docs.jax.dev/en/latest/installation.html#jax-nightly-installation).\n   * `jax.sharding.PartitionSpec` no longer inherits from a tuple.\n@@ -232,7 +232,7 @@ to signify this.\n       developers at this point. So it is difficult for us to fix this kind of\n       problem even if we wanted to.\n \n-    We are open to readding support for Mac x86 if the community is willing\n+    We are open to re-adding support for Mac x86 if the community is willing\n     to help support that platform: in particular, we would need the JAX test\n     suite to pass cleanly on Mac x86 before we could ship releases again.\n \n@@ -457,7 +457,7 @@ This is a patch release of jax 0.4.36. Only \"jax\" was released at this version.\n   * `jax_pmap_no_rank_reduction` flag is set to `True` by default.\n     * array[0] on a pmap result now introduces a reshape (use array[0:1]\n       instead).\n-    * The per-shard shape (accessable via jax_array.addressable_shards or\n+    * The per-shard shape (accessible via jax_array.addressable_shards or\n       jax_array.addressable_data(0)) now has a leading (1, ...). Update code\n       that directly accesses shards accordingly. The rank of the per-shard-shape\n       now matches that of the global shape which is the same behavior as jit.\n@@ -1513,7 +1513,7 @@ See the 0.4.33 release notes for more details.\n     dict of string stat names with int values, e.g. `\"bytes_in_use\"`, or None if\n     the platform doesn't support memory statistics. The exact stats returned may\n     vary across platforms. Currently only implemented on Cloud TPU.\n-  * Readded support for the Python buffer protocol (`memoryview`) on CPU\n+  * Re-added support for the Python buffer protocol (`memoryview`) on CPU\n     devices.\n \n ## jax 0.4.10 (May 11, 2023)\ndiff --git a/benchmarks/mosaic/matmul_bench.py b/benchmarks/mosaic/matmul_bench.py\nindex 32c147916407..fd3fcd6da315 100644\n--- a/benchmarks/mosaic/matmul_bench.py\n+++ b/benchmarks/mosaic/matmul_bench.py\n@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"Microbenchmarks for mosaic gpu matrix mutliplication.\"\"\"\n+\"\"\"Microbenchmarks for mosaic gpu matrix multiplication.\"\"\"\n \n import functools\n import sys\ndiff --git a/build/build.py b/build/build.py\nindex d059251552eb..40e02a100d98 100755\n--- a/build/build.py\n+++ b/build/build.py\n@@ -612,7 +612,7 @@ async def main():\n       wheel_build_command_base.append(\"--config=rocm\")\n       wheel_build_command_base.append(f\"--action_env=CLANG_COMPILER_PATH=\\\"{clang_path}\\\"\")\n     if args.rocm_path:\n-      logging.debug(\"ROCm tookit path: %s\", args.rocm_path)\n+      logging.debug(\"ROCm toolkit path: %s\", args.rocm_path)\n       wheel_build_command_base.append(f\"--action_env=ROCM_PATH=\\\"{args.rocm_path}\\\"\")\n     if args.rocm_amdgpu_targets:\n       logging.debug(\"ROCm AMD GPU targets: %s\", args.rocm_amdgpu_targets)\ndiff --git a/build/rocm/setup.rocm.sh b/build/rocm/setup.rocm.sh\nindex 3893d817e3a8..faa79d2ce1fd 100755\n--- a/build/rocm/setup.rocm.sh\n+++ b/build/rocm/setup.rocm.sh\n@@ -13,7 +13,7 @@ ROCM_BUILD_NAME=ubuntu\n ROCM_BUILD_NUM=main\n \n # Adjust the ROCM repo location\n-# Intial release don't have the trialing '.0'\n+# Initial release don't have the trialing '.0'\n # For example ROCM 5.7.0 is at https://repo.radeon.com/rocm/apt/5.7/\n if [ ${ROCM_VERSION##*[^0-9]} -eq '0' ]; then\n         ROCM_VERS=${ROCM_VERSION%.*}\ndiff --git a/build/rocm/tools/build_wheels.py b/build/rocm/tools/build_wheels.py\nindex a7ebdf86f916..3b3d697addc9 100644\n--- a/build/rocm/tools/build_wheels.py\n+++ b/build/rocm/tools/build_wheels.py\n@@ -227,7 +227,7 @@ def fix_wheel(path, jax_path):\n         env[\"PATH\"] = \"%s:%s\" % (py_bin, env[\"PATH\"])\n \n         # NOTE(mrodden): auditwheel 6.0 added lddtree module, but 6.3.0 changed\n-        # the fuction to ldd and also changed its behavior\n+        # the function to ldd and also changed its behavior\n         # constrain range to 6.0 to 6.2.x\n         cmd = [\"pip\", \"install\", \"auditwheel>=6,<6.3\"]\n         subprocess.run(cmd, check=True, env=env)\n@@ -325,7 +325,7 @@ def main():\n     shutil.rmtree(os.path.join(args.jax_path, \"jax.egg-info\"))\n     shutil.rmtree(os.path.join(args.jax_path, \"jax\", \"__pycache__\"))\n \n-    # Make the wheels deleteable by the runner\n+    # Make the wheels deletable by the runner\n     whl_house = os.path.join(args.jax_path, \"wheelhouse\")\n     logging.info(\"Changing permissions for %s\" % whl_house)\n     mode = 0o664\ndiff --git a/ci/envs/README.md b/ci/envs/README.md\nindex cf7a0c12fc9f..2a81d0f3240d 100644\n--- a/ci/envs/README.md\n+++ b/ci/envs/README.md\n@@ -9,7 +9,7 @@ Name                                        | Default Value\n ------------------------------------------- | ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -----\n `JAXCI_JAX_GIT_DIR`                         | Present working directory: `$(pwd)`      | Path to the JAX's Git directory.                                                                                                                                                                                                                                                                                                                             | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_JAX_GIT_DIR&type=code)\n `JAXCI_HERMETIC_PYTHON_VERSION`             | System default                           | Controls the version of hermetic Python to use. This affects the Bazel commands only such as when building artifacts or when running the Bazel test scripts.                                                                                                                                                                                                                                                                            | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_HERMETIC_PYTHON_VERSION&type=code)\n-`JAXCI_XLA_GIT_DIR`                         | Unset                                    | When using a local copy of XLA, this points to the root of the XLA git repoistory.                                                                                                                                                                                                                                                                           | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_XLA_GIT_DIR&type=code)\n+`JAXCI_XLA_GIT_DIR`                         | Unset                                    | When using a local copy of XLA, this points to the root of the XLA git repository.                                                                                                                                                                                                                                                                           | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_XLA_GIT_DIR&type=code)\n `JAXCI_CLONE_MAIN_XLA`                      | 0                                        | If set to 1, the XLA repository is cloned at HEAD and its path is set in `JAXCI_XLA_GIT_DIR`                                                                                                                                                                                                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_CLONE_MAIN_XLA&type=code)\n `JAXCI_XLA_COMMIT`                          | Unset                                    | Allows overriding the XLA commit that is used when using a local copy of XLA.                                                                                                                                                                                                                                                                                | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_XLA_COMMIT&type=code)\n `JAXCI_OUTPUT_DIR`                          | `$(pwd)/dist`                            | Controls the location where the artifacts are written to. The directory will be automatically created if it does not exist.                                                                                                                                                                                                                                  | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_OUTPUT_DIR&type=code)\n@@ -37,5 +37,5 @@ Name                                        | Default Value\n Name                    | Default Value                                                                                                | Behavior                                                                                             | Usage\n ----------------------- | ------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- | -----\n `JAXCI_DOCKER_WORK_DIR` | \"/jax\"                                                                                                       | The path on the container where the JAX Git repository is mounted to.                                | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_WORK_DIR&type=code)\n-`JAXCI_DOCKER_ARGS`     | Empty String                                                                                                 | Space seprated string of additional arguments that will be passed when starting the Docker container | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_ARGS&type=code)\n+`JAXCI_DOCKER_ARGS`     | Empty String                                                                                                 | Space separated string of additional arguments that will be passed when starting the Docker container | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_ARGS&type=code)\n `JAXCI_DOCKER_IMAGE`    | Depends on the system (see [ci/envs/docker.env](https://github.com/jax-ml/jax/blob/main/ci/envs/docker.env)) | Docker image to pull                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_DOCKER_IMAGE&type=code)\ndiff --git a/ci/envs/docker.env b/ci/envs/docker.env\nindex d556cb82d74d..cef2cda27bf4 100644\n--- a/ci/envs/docker.env\n+++ b/ci/envs/docker.env\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# This file contains all the docker specifc envs that are needed by the\n+# This file contains all the docker specific envs that are needed by the\n # ci/utilities/run_docker_container.sh script.\n \n os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/ci/utilities/install_wheels_locally.sh b/ci/utilities/install_wheels_locally.sh\nindex b1472d765c08..d66e1fea967b 100644\n--- a/ci/utilities/install_wheels_locally.sh\n+++ b/ci/utilities/install_wheels_locally.sh\n@@ -22,7 +22,7 @@ WHEELS=( $(/usr/bin/find \"$JAXCI_OUTPUT_DIR/\" -type f \\(  -name \"*jax*py3*\" -o -\n \n for i in \"${!WHEELS[@]}\"; do\n   if [[ \"${WHEELS[$i]}\" == *jax*py3*none*any.whl ]]; then\n-    # Apppend an extra to the end of the JAX wheel path to install those\n+    # Append an extra to the end of the JAX wheel path to install those\n     # packages as well from PyPI. E.g. jax[tpu] will install the libtpu package\n     # from PyPI. See ci/envs/README.md for more details.\n     if [[ -n \"$JAXCI_JAX_PYPI_EXTRAS\" ]]; then\ndiff --git a/ci/utilities/setup_build_environment.sh b/ci/utilities/setup_build_environment.sh\nindex 114acf2479ff..246665cd2f9f 100644\n--- a/ci/utilities/setup_build_environment.sh\n+++ b/ci/utilities/setup_build_environment.sh\n@@ -16,7 +16,7 @@\n # Set up the build environment for JAX CI jobs. This script depends on the\n # \"JAXCI_\" environment variables set or sourced in the build script.\n \n-# Pre-emptively mark the JAX git directory as safe. This is necessary for JAX CI\n+# Preemptively mark the JAX git directory as safe. This is necessary for JAX CI\n # jobs running on Linux runners in GitHub Actions. Without this, git complains\n # that the directory has dubious ownership and refuses to run any commands.\n # Avoid running on Windows runners as git runs into issues with not being able\ndiff --git a/docs/api_compatibility.md b/docs/api_compatibility.md\nindex dda86e2e5d31..985b2145c5c4 100644\n--- a/docs/api_compatibility.md\n+++ b/docs/api_compatibility.md\n@@ -96,7 +96,7 @@ guarantees of the main JAX package. If you have code that uses `jax.extend`,\n we would strongly recommend CI tests against JAX's nightly releases, so as to\n catch potential changes before they are released.\n \n-For details on `jax.extend`, see the [`jax.extend` module docuementation](https://docs.jax.dev/en/latest/jax.extend.html), or the design document, {ref}`jax-extend-jep`.\n+For details on `jax.extend`, see the [`jax.extend` module documentation](https://docs.jax.dev/en/latest/jax.extend.html), or the design document, {ref}`jax-extend-jep`.\n \n ## Numerics and randomness\n \ndiff --git a/docs/autodidax2_part1.ipynb b/docs/autodidax2_part1.ipynb\nindex 0a5a89c8ed98..7a58f54b16c8 100644\n--- a/docs/autodidax2_part1.ipynb\n+++ b/docs/autodidax2_part1.ipynb\n@@ -674,7 +674,7 @@\n     \"something is constant with respect to differentiation? It's tempting to say\\n\",\n     \"\\\"it's a constant if and only if it's not a dual number\\\". But actually dual\\n\",\n     \"numbers created by a *different* JVPInterpreter also need to be considered\\n\",\n-    \"constants with resepect to the JVPInterpreter we're currently handling. That's\\n\",\n+    \"constants with respect to the JVPInterpreter we're currently handling. That's\\n\",\n     \"why we need the `x.interpreter is self` check in `JVPInterpreter.lift`. This\\n\",\n     \"comes up in higher order differentiation when there are multiple JVPInterprers\\n\",\n     \"in scope. The sort of bug where you accidentally interpret a dual number from\\n\",\n@@ -1046,7 +1046,7 @@\n     \"That's it for part one of this tutorial. We've done two primitives, three\\n\",\n     \"interpreters and the tracing mechanism that weaves them together. In the next\\n\",\n     \"part we'll add types other than floats, error handling, compilation,\\n\",\n-    \"reverse-mode AD and higher-order primtives. Note that the second part is\\n\",\n+    \"reverse-mode AD and higher-order primitives. Note that the second part is\\n\",\n     \"structured differently. Rather than trying to have a top-to-bottom order that\\n\",\n     \"obeys both code dependencies (e.g. data structures need to be defined before\\n\",\n     \"they're used) and pedagogical dependencies (concepts need to be introduced\\n\",\ndiff --git a/docs/autodidax2_part1.md b/docs/autodidax2_part1.md\nindex 70dd0e4b696b..a4af594fb253 100644\n--- a/docs/autodidax2_part1.md\n+++ b/docs/autodidax2_part1.md\n@@ -348,7 +348,7 @@ There are some subtleties worth discussing. First, how do you tell if\n something is constant with respect to differentiation? It's tempting to say\n \"it's a constant if and only if it's not a dual number\". But actually dual\n numbers created by a *different* JVPInterpreter also need to be considered\n-constants with resepect to the JVPInterpreter we're currently handling. That's\n+constants with respect to the JVPInterpreter we're currently handling. That's\n why we need the `x.interpreter is self` check in `JVPInterpreter.lift`. This\n comes up in higher order differentiation when there are multiple JVPInterprers\n in scope. The sort of bug where you accidentally interpret a dual number from\n@@ -539,7 +539,7 @@ print(jvp(lambda x: eval_jaxpr(build_jaxpr(foo, 1), (x,)), 2.0, 1.0))\n That's it for part one of this tutorial. We've done two primitives, three\n interpreters and the tracing mechanism that weaves them together. In the next\n part we'll add types other than floats, error handling, compilation,\n-reverse-mode AD and higher-order primtives. Note that the second part is\n+reverse-mode AD and higher-order primitives. Note that the second part is\n structured differently. Rather than trying to have a top-to-bottom order that\n obeys both code dependencies (e.g. data structures need to be defined before\n they're used) and pedagogical dependencies (concepts need to be introduced\ndiff --git a/docs/autodidax2_part1.py b/docs/autodidax2_part1.py\nindex bfe59df359d3..44bf843c91b3 100644\n--- a/docs/autodidax2_part1.py\n+++ b/docs/autodidax2_part1.py\n@@ -307,7 +307,7 @@ def nth_order_derivative(n, f, x):\n # something is constant with respect to differentiation? It's tempting to say\n # \"it's a constant if and only if it's not a dual number\". But actually dual\n # numbers created by a *different* JVPInterpreter also need to be considered\n-# constants with resepect to the JVPInterpreter we're currently handling. That's\n+# constants with respect to the JVPInterpreter we're currently handling. That's\n # why we need the `x.interpreter is self` check in `JVPInterpreter.lift`. This\n # comes up in higher order differentiation when there are multiple JVPInterprers\n # in scope. The sort of bug where you accidentally interpret a dual number from\n@@ -483,7 +483,7 @@ def eval_atom(x): return env[x] if isinstance(x, Var) else x\n # That's it for part one of this tutorial. We've done two primitives, three\n # interpreters and the tracing mechanism that weaves them together. In the next\n # part we'll add types other than floats, error handling, compilation,\n-# reverse-mode AD and higher-order primtives. Note that the second part is\n+# reverse-mode AD and higher-order primitives. Note that the second part is\n # structured differently. Rather than trying to have a top-to-bottom order that\n # obeys both code dependencies (e.g. data structures need to be defined before\n # they're used) and pedagogical dependencies (concepts need to be introduced\ndiff --git a/docs/developer.md b/docs/developer.md\nindex cfb3f16cf649..1b50a9b65bc0 100644\n--- a/docs/developer.md\n+++ b/docs/developer.md\n@@ -374,7 +374,7 @@ in terms of files, not installations):\n    --repo_env=HERMETIC_PYTHON_URL=\"https://remote/url/to/my_python.tgz\"\n    --repo_env=HERMETIC_PYTHON_SHA256=<file's_sha256_sum>\n \n-   # We assume that top-level folder in the tarbal is called \"python\", if it is\n+   # We assume that top-level folder in the tarball is called \"python\", if it is\n    # something different just pass additional HERMETIC_PYTHON_PREFIX parameter\n    --repo_env=HERMETIC_PYTHON_URL=\"https://remote/url/to/my_python.tgz\"\n    --repo_env=HERMETIC_PYTHON_SHA256=<file's_sha256_sum>\ndiff --git a/docs/export/shape_poly.md b/docs/export/shape_poly.md\nindex 6b63a536ab48..68da231c4a68 100644\n--- a/docs/export/shape_poly.md\n+++ b/docs/export/shape_poly.md\n@@ -441,7 +441,7 @@ to {func}`jax.export.symbolic_shape` share a scope and\n can be mixed up in arithmetic operations. The result would\n also share the same scope.\n \n-You can re-use scopes:\n+You can reuse scopes:\n \n ```python\n >>> a, = export.symbolic_shape(\"a,\", constraints=(\"a >= 8\",))\ndiff --git a/docs/gpu_performance_tips.md b/docs/gpu_performance_tips.md\nindex bade464d22a1..c9034a515501 100644\n--- a/docs/gpu_performance_tips.md\n+++ b/docs/gpu_performance_tips.md\n@@ -93,7 +93,7 @@ export JAX_PGLE_AGGREGATION_PERCENTILE=85\n \n # Right now the auto PGLE profile collection doesn't work with command buffer.\n # If the command buffer is enabled, Auto PGLE will disable it during profile\n-# colletion and enable it back after the recompilation. If you need to have a\n+# collection and enable it back after the recompilation. If you need to have a\n # consistent command buffer logic with and with PGLE profile you can disable it\n # manually:\n export XLA_FLAGS=\"${XLA_FLAGS} --xla_gpu_enable_command_buffer=''\"\n@@ -371,7 +371,7 @@ def while_body(carry, i):\n       (NUM_DEVICES, 1, CONTRACTING_DIM_SIZE, NON_CONTRACTING_DIM_SIZE),\n   )\n \n-  # Colelctive permute on the \"back edge\" passes data to the first device.\n+  # Collective permute on the \"back edge\" passes data to the first device.\n   bwd_edge_data = cycle_back(bwd_edge_data)\n \n   # Update output buffer. We do this after reading from it to avoid the data\ndiff --git a/docs/index.rst b/docs/index.rst\nindex 07739c01c2fb..93fc6c284685 100644\n--- a/docs/index.rst\n+++ b/docs/index.rst\n@@ -108,7 +108,7 @@ numerical computing tools; the following is just a small sample of what is out t\n \n     .. grid-item:: :material-regular:`bar_chart;2em` **Probabilistic modeling**\n \n-       - `TensorFlow Probabilty`_\n+       - `TensorFlow Probability`_\n        - Distrax_\n \n     .. grid-item:: :material-outlined:`animation;2em` **Physics & simulation**\n@@ -199,4 +199,4 @@ maintains an up-to-date list.\n .. _Orbax: https://orbax.readthedocs.io/\n .. _PyMC: https://www.pymc.io/\n .. _TensorFlow Datasets: https://www.tensorflow.org/datasets\n-.. _TensorFlow Probabilty: https://www.tensorflow.org/probability\n+.. _TensorFlow Probability: https://www.tensorflow.org/probability\ndiff --git a/docs/notebooks/explicit-sharding.ipynb b/docs/notebooks/explicit-sharding.ipynb\nindex e1bee4b99fb5..37010b4ab3d3 100644\n--- a/docs/notebooks/explicit-sharding.ipynb\n+++ b/docs/notebooks/explicit-sharding.ipynb\n@@ -414,7 +414,7 @@\n     \"wherever types need to match. For example, the two sides of a `lax.cond` need to\\n\",\n     \"have results with matching shardings. And the carry of `lax.scan` needs to have the\\n\",\n     \"same sharding at the input and the output of the scan body. And when you\\n\",\n-    \"contruct a jaxpr without concrete arguments using `make_jaxpr` you need to\\n\",\n+    \"construct a jaxpr without concrete arguments using `make_jaxpr` you need to\\n\",\n     \"provide shardings too. Certain JAX transformations perform type-level\\n\",\n     \"operations. Automatic differentation constructs a tangent type for each primal\\n\",\n     \"type in the original computation (e.g. `TangentOf(float) == float`,\\n\",\ndiff --git a/docs/notebooks/explicit-sharding.md b/docs/notebooks/explicit-sharding.md\nindex 1402bca2415f..8989d426ffbc 100644\n--- a/docs/notebooks/explicit-sharding.md\n+++ b/docs/notebooks/explicit-sharding.md\n@@ -251,7 +251,7 @@ sharding is part of that type. This means that shardings need to match\n wherever types need to match. For example, the two sides of a `lax.cond` need to\n have results with matching shardings. And the carry of `lax.scan` needs to have the\n same sharding at the input and the output of the scan body. And when you\n-contruct a jaxpr without concrete arguments using `make_jaxpr` you need to\n+construct a jaxpr without concrete arguments using `make_jaxpr` you need to\n provide shardings too. Certain JAX transformations perform type-level\n operations. Automatic differentation constructs a tangent type for each primal\n type in the original computation (e.g. `TangentOf(float) == float`,\ndiff --git a/docs/notebooks/host-offloading.ipynb b/docs/notebooks/host-offloading.ipynb\nindex 9c806c2d56e7..f56cb90ff77e 100644\n--- a/docs/notebooks/host-offloading.ipynb\n+++ b/docs/notebooks/host-offloading.ipynb\n@@ -240,7 +240,7 @@\n    ],\n    \"source\": [\n     \"f = jax.jit(lambda x: x, out_shardings=s_dev)\\n\",\n-    \"out_host = f(arr_host)      # Input arrays in hte device memory while output arrays in the host memory\\n\",\n+    \"out_host = f(arr_host)      # Input arrays in the device memory while output arrays in the host memory\\n\",\n     \"print(\\\"Result value of D2H: \\\\n\\\", out_host)\"\n    ]\n   },\ndiff --git a/docs/notebooks/host-offloading.md b/docs/notebooks/host-offloading.md\nindex 7e113d40a4b3..cffe8b4340fe 100644\n--- a/docs/notebooks/host-offloading.md\n+++ b/docs/notebooks/host-offloading.md\n@@ -154,7 +154,7 @@ id: FjZzkxI8ky4r\n outputId: 2a1b6e7a-1c29-4347-c020-7b47c27a5cc3\n ---\n f = jax.jit(lambda x: x, out_shardings=s_dev)\n-out_host = f(arr_host)      # Input arrays in hte device memory while output arrays in the host memory\n+out_host = f(arr_host)      # Input arrays in the device memory while output arrays in the host memory\n print(\"Result value of D2H: \\n\", out_host)\n ```\n \ndiff --git a/docs/pallas/design/async_note.md b/docs/pallas/design/async_note.md\nindex 0fda9fe0a4e2..b255a91d3ec8 100644\n--- a/docs/pallas/design/async_note.md\n+++ b/docs/pallas/design/async_note.md\n@@ -464,7 +464,7 @@ def f(x):\n   return fori_loop(0, 8, body, x)\n ```\n \n-If you run the alias analysis, youll find that all of the buffers have been colored the same\\! Intuitively, this is problematic because if we are doing a loop of `ppermute`s, we cant write into the same buffer we are sending into. We generally need an extra (i.e. a double) buffer to receive, and then usually we will switch the send/recv buffers on the next iteration. What XLA will do in practice is that it will observe the buffer re-use and defensively insert a copy.\n+If you run the alias analysis, youll find that all of the buffers have been colored the same\\! Intuitively, this is problematic because if we are doing a loop of `ppermute`s, we cant write into the same buffer we are sending into. We generally need an extra (i.e. a double) buffer to receive, and then usually we will switch the send/recv buffers on the next iteration. What XLA will do in practice is that it will observe the buffer reuse and defensively insert a copy.\n \n ```py\n def f(x):\ndiff --git a/docs/pallas/design/design.md b/docs/pallas/design/design.md\nindex 17c7a6dbdc0f..53a5eb209510 100644\n--- a/docs/pallas/design/design.md\n+++ b/docs/pallas/design/design.md\n@@ -71,7 +71,7 @@ A JAX-based kernel language offers several advantages:\n * JAX as a tracing-based frontend for numerical computing is both\n   mature and well-used.\n   By embedding the kernel programming language in JAX itself,\n-  we can re-use JAXs tracing infrastructure and provide a\n+  we can reuse JAXs tracing infrastructure and provide a\n   NumPy-like frontend thats already familiar to users.\n * JAX transformations are key to its success, allowing users to\n   express simple programs but transform them to achieve complex\n@@ -551,7 +551,7 @@ along that dimension.\n `grad` of `pallas_call` enables automatic differentiation of kernels.\n `jax.grad` breaks down into applications of three distinct transforms:\n `jvp`, `partial_eval` and `transpose`.\n-In principle, we can re-use most of JAXs infrastructure when\n+In principle, we can reuse most of JAXs infrastructure when\n implementing these rules for `pallas_call` (since it behaves much like\n existing JAX higher order primitives).\n \ndiff --git a/docs/pallas/gpu/reference.md b/docs/pallas/gpu/reference.md\nindex 7b4a1e6e9c7d..1a4f39dff5f2 100644\n--- a/docs/pallas/gpu/reference.md\n+++ b/docs/pallas/gpu/reference.md\n@@ -30,7 +30,7 @@ the next instruction.\n <center><img alt=\"A diagram of one NVIDIA SM\" src=\"../../_static/pallas/gpu/nvidia_sm.svg\" style=\"width:60%; min-width: 400px;\"></center>\n \n Going further, recent CUDA versions also outline the concept of a _warpgroup_, which are\n-4 consecutive warps. Knowing how the hardware looks like, we can see where this is comming\n+4 consecutive warps. Knowing how the hardware looks like, we can see where this is coming\n from: 4 consecutive warps occupy the 4 quarters of an SM and let us issue instructions\n that utilize the whole SM.\n \n@@ -49,7 +49,7 @@ warps always run in lockstep (modulo the jitter from hardware scheduling) and ne\n different paths through control flow (with the small exception of `core_map` that we will\n discuss later). One notable addition here is that we still allow you to co-schedule multiple\n of those Pallas-level threads on the same SM so that they can cooperate and communicate\n-through shared memory (we relize that by putting them in the same CUDA block).\n+through shared memory (we realize that by putting them in the same CUDA block).\n \n ```{note}\n From now on, whenever we say \"thread\", we refer to the Pallas thread, not a CUDA thread/lane.\n@@ -329,7 +329,7 @@ transforms specified upon their allocation. For all currently supported generati\n the TensorCore requires the data to be laid out into row-major 2D tiles of shape\n `(8, swizzle_elems)`, where `swizzle_elems` is derived by dividing the swizzle by the\n element type bytewidth.  The currently supported swizzles are: 128, 64, and 32. Larger\n-swizzles are preferrable as they improve the performance of GMEM-to-SMEM copies.\n+swizzles are preferable as they improve the performance of GMEM-to-SMEM copies.\n \n ```python\n def mma_transforms(shape_dtype: jax.ShapeDtypeStruct):\ndiff --git a/docs/pallas/pipelining.md b/docs/pallas/pipelining.md\nindex a79876a0ca97..0ff9eaf5a24b 100644\n--- a/docs/pallas/pipelining.md\n+++ b/docs/pallas/pipelining.md\n@@ -34,7 +34,7 @@ import numpy as np\n <!-- #region id=\"shnVghWUSvpx\" -->\n ## Memory Hierarchies\n \n-The first step in understanding pipelining conceptually involves understanding the different forms of memory available and the tradeoffs between them. Most hardware architectures (including CPUs, GPUs, and TPUs) utilize a wide variety of memory spaces that tradeoff capicity vs latency/bandwidth. For the purpose of Pallas, we are typically interested in registers, SRAM, DRAM, and potentially network communication:\n+The first step in understanding pipelining conceptually involves understanding the different forms of memory available and the tradeoffs between them. Most hardware architectures (including CPUs, GPUs, and TPUs) utilize a wide variety of memory spaces that tradeoff capacity vs latency/bandwidth. For the purpose of Pallas, we are typically interested in registers, SRAM, DRAM, and potentially network communication:\n - **Registers** are the the memory physically closest to the processor, and typically values must be loaded directly into registers before doing any compute on them.\n - **SRAM** (also known as Shared Memory/L1 and L2 cache on GPUs, or VMEM on TPUs) also lives fairly close to the processor, but has larger capacity than registers.\n SRAM on modern ML accelerators typically range in the 10-100MB range (TPU v5p contains 96MB of VMEM, and H100 GPUs contain ~30MB of L1 cache and 50MB of L2).\ndiff --git a/docs/pallas/tpu/distributed.ipynb b/docs/pallas/tpu/distributed.ipynb\nindex ae82b7a80ac6..3ac1206bd14a 100644\n--- a/docs/pallas/tpu/distributed.ipynb\n+++ b/docs/pallas/tpu/distributed.ipynb\n@@ -71,7 +71,7 @@\n     \"\\n\",\n     \"![tpu_topologies](https://cloud.google.com/static/tpu/docs/images/v4-topologies.png)\\n\",\n     \"\\n\",\n-    \"Flattened as a graph, the torus can be visualized as follows. Each edge (orange or black) is a bidirectional connection between two devices. You will commonly hear about rings in conjunction with discussion about device toplogies  a key feature of a torus is that when taking a slice along an axis of the pod, such as the nodes `[(0,1), (1, 1), (2, 1), (3, 1)]` or `[(0, 1), (1, 1)]`, we have a ring of devices. This is a feature we can use to simplify communication patterns within the pod.\\n\",\n+    \"Flattened as a graph, the torus can be visualized as follows. Each edge (orange or black) is a bidirectional connection between two devices. You will commonly hear about rings in conjunction with discussion about device topologies  a key feature of a torus is that when taking a slice along an axis of the pod, such as the nodes `[(0,1), (1, 1), (2, 1), (3, 1)]` or `[(0, 1), (1, 1)]`, we have a ring of devices. This is a feature we can use to simplify communication patterns within the pod.\\n\",\n     \"\\n\",\n     \"![tpu_torus](https://cloud.google.com/static/tpu/docs/images/untwisted-tori.png)\"\n    ]\n@@ -477,7 +477,7 @@\n     \"id\": \"KgU7HI2pS4om\"\n    },\n    \"source\": [\n-    \"A detail worth mentioning here is the use of multiple receive semaphores. Because we only block on the receiving device, it is still possible for a sender to have sent multiple DMAs in flight before the receiver has finished processing the first one (see the next section and reduce-sum example which discusses race conditions in more detail). In this situation we may hit a situation where the same semaphore is being used for multiple DMAs occurring simultaneously. To avoid this, we allocate `num_devices-1` semaphores so there is no risk of re-use. While this race condition is unlikely to happen on such a small kernel, on larger kernels there is more chance for devices to fall out of sync and potentially cause a silent failure.\"\n+    \"A detail worth mentioning here is the use of multiple receive semaphores. Because we only block on the receiving device, it is still possible for a sender to have sent multiple DMAs in flight before the receiver has finished processing the first one (see the next section and reduce-sum example which discusses race conditions in more detail). In this situation we may hit a situation where the same semaphore is being used for multiple DMAs occurring simultaneously. To avoid this, we allocate `num_devices-1` semaphores so there is no risk of reuse. While this race condition is unlikely to happen on such a small kernel, on larger kernels there is more chance for devices to fall out of sync and potentially cause a silent failure.\"\n    ]\n   },\n   {\n@@ -529,7 +529,7 @@\n     \"\\n\",\n     \"In order to use regular semaphores, they can be allocated in the same way as a DMA semaphore, but by specifying `pltpu.SemaphoreType.REGULAR` rather than `pltpu.SemaphoreType.DMA`.\\n\",\n     \"\\n\",\n-    \"Semaphores must be zero at the end of a Pallas program to complete succesfully. There are two error cases where this may happen:\\n\",\n+    \"Semaphores must be zero at the end of a Pallas program to complete successfully. There are two error cases where this may happen:\\n\",\n     \" - If a semaphore is over-signaled, the program will end with non-zero (>0) semaphores. In this case, the program will crash upon completion. This is useful for debugging as non-zero semaphores typically means there is a bug somewhere inside of the program.\\n\",\n     \" - If a semaphore is over-waited, the program will hang on the blocking `semaphore_wait` call while it waits for the semaphore to be incremented. In this case the device or program will need to be restarted.\\n\",\n     \"\\n\",\n@@ -644,7 +644,7 @@\n     \"\\n\",\n     \"The main body assumes that a value has already been copied into our local working slot, either from the previous iteration or from the prologue. A complicating factor is that our destination buffers live in HBM, but we need to load values to VMEM before we perform arithmetic. Therefore, we simultaneously copy the working slot value into our VMEM (`receive_scratch`) and pass the value on to our right neighbor's receiving slot. Once the value has been copied into our VMEM, we can accumulate it into our result (contained in `o_ref`).\\n\",\n     \"\\n\",\n-    \"A subtle race condition can occur if one device runs one loop ahead of it's right neighbor. In this case, it could copy into the receiver's `working_slot` at the same time the receiver is reading from it. In order to avoid this, each device will block on a `REGULAR` semaphore before copying into the right neighbor's `dst_ref` until it has signaled that it is done reading from its `working_slot`. This race condition is rarely triggered for a small kernel such as this example, but can it can be explicitly triggered if for example using a `pltpu.delay` instruction to artifically hang a device.\\n\",\n+    \"A subtle race condition can occur if one device runs one loop ahead of it's right neighbor. In this case, it could copy into the receiver's `working_slot` at the same time the receiver is reading from it. In order to avoid this, each device will block on a `REGULAR` semaphore before copying into the right neighbor's `dst_ref` until it has signaled that it is done reading from its `working_slot`. This race condition is rarely triggered for a small kernel such as this example, but can it can be explicitly triggered if for example using a `pltpu.delay` instruction to artificially hang a device.\\n\",\n     \"\\n\",\n     \"Note that this is not an optimal or fully general kernel, as the block sizes must entirely fit in VMEM and we could better interleave communication and accumulation. We will discuss these optimizations in later sections.\"\n    ]\n@@ -691,7 +691,7 @@\n     \"  \\\"\\\"\\\"Performs a barrier with neighbors on the global barrier semaphore.\\n\",\n     \"\\n\",\n     \"  Optionally performs a second barrier, which prevents a potential race\\n\",\n-    \"  when re-using the same collective_id across kernel invocations.\\n\",\n+    \"  when reusing the same collective_id across kernel invocations.\\n\",\n     \"  \\\"\\\"\\\"\\n\",\n     \"  barrier_sem = pltpu.get_barrier_semaphore()\\n\",\n     \"  for neighbor in [left_neighbor, right_neighbor]:\\n\",\n@@ -1701,7 +1701,7 @@\n     \"\\n\",\n     \"### Next Steps\\n\",\n     \"\\n\",\n-    \"Excellent follow-up excercises for the reader could include implementing a distributed matrix multiplication, implementing `lax.all_to_all`, and relaxing synchronization to allow for additional run-ahead.\"\n+    \"Excellent follow-up exercises for the reader could include implementing a distributed matrix multiplication, implementing `lax.all_to_all`, and relaxing synchronization to allow for additional run-ahead.\"\n    ]\n   }\n  ],\ndiff --git a/docs/pallas/tpu/distributed.md b/docs/pallas/tpu/distributed.md\nindex b16116549972..19b336005c28 100644\n--- a/docs/pallas/tpu/distributed.md\n+++ b/docs/pallas/tpu/distributed.md\n@@ -61,7 +61,7 @@ TPUs pods are typically arranged in an ND torus topology. The following graphic\n \n ![tpu_topologies](https://cloud.google.com/static/tpu/docs/images/v4-topologies.png)\n \n-Flattened as a graph, the torus can be visualized as follows. Each edge (orange or black) is a bidirectional connection between two devices. You will commonly hear about rings in conjunction with discussion about device toplogies  a key feature of a torus is that when taking a slice along an axis of the pod, such as the nodes `[(0,1), (1, 1), (2, 1), (3, 1)]` or `[(0, 1), (1, 1)]`, we have a ring of devices. This is a feature we can use to simplify communication patterns within the pod.\n+Flattened as a graph, the torus can be visualized as follows. Each edge (orange or black) is a bidirectional connection between two devices. You will commonly hear about rings in conjunction with discussion about device topologies  a key feature of a torus is that when taking a slice along an axis of the pod, such as the nodes `[(0,1), (1, 1), (2, 1), (3, 1)]` or `[(0, 1), (1, 1)]`, we have a ring of devices. This is a feature we can use to simplify communication patterns within the pod.\n \n ![tpu_torus](https://cloud.google.com/static/tpu/docs/images/untwisted-tori.png)\n \n@@ -409,7 +409,7 @@ print('Difference |Pallas - lax.all_gather| = ',\n \n +++ {\"id\": \"KgU7HI2pS4om\"}\n \n-A detail worth mentioning here is the use of multiple receive semaphores. Because we only block on the receiving device, it is still possible for a sender to have sent multiple DMAs in flight before the receiver has finished processing the first one (see the next section and reduce-sum example which discusses race conditions in more detail). In this situation we may hit a situation where the same semaphore is being used for multiple DMAs occurring simultaneously. To avoid this, we allocate `num_devices-1` semaphores so there is no risk of re-use. While this race condition is unlikely to happen on such a small kernel, on larger kernels there is more chance for devices to fall out of sync and potentially cause a silent failure.\n+A detail worth mentioning here is the use of multiple receive semaphores. Because we only block on the receiving device, it is still possible for a sender to have sent multiple DMAs in flight before the receiver has finished processing the first one (see the next section and reduce-sum example which discusses race conditions in more detail). In this situation we may hit a situation where the same semaphore is being used for multiple DMAs occurring simultaneously. To avoid this, we allocate `num_devices-1` semaphores so there is no risk of reuse. While this race condition is unlikely to happen on such a small kernel, on larger kernels there is more chance for devices to fall out of sync and potentially cause a silent failure.\n \n +++ {\"id\": \"EDCmAaHVtY7x\"}\n \n@@ -451,7 +451,7 @@ def semaphore_read(\n \n In order to use regular semaphores, they can be allocated in the same way as a DMA semaphore, but by specifying `pltpu.SemaphoreType.REGULAR` rather than `pltpu.SemaphoreType.DMA`.\n \n-Semaphores must be zero at the end of a Pallas program to complete succesfully. There are two error cases where this may happen:\n+Semaphores must be zero at the end of a Pallas program to complete successfully. There are two error cases where this may happen:\n  - If a semaphore is over-signaled, the program will end with non-zero (>0) semaphores. In this case, the program will crash upon completion. This is useful for debugging as non-zero semaphores typically means there is a bug somewhere inside of the program.\n  - If a semaphore is over-waited, the program will hang on the blocking `semaphore_wait` call while it waits for the semaphore to be incremented. In this case the device or program will need to be restarted.\n \n@@ -556,7 +556,7 @@ The prologue (executed when `outer_step==0`) first initiates a barrier with both\n \n The main body assumes that a value has already been copied into our local working slot, either from the previous iteration or from the prologue. A complicating factor is that our destination buffers live in HBM, but we need to load values to VMEM before we perform arithmetic. Therefore, we simultaneously copy the working slot value into our VMEM (`receive_scratch`) and pass the value on to our right neighbor's receiving slot. Once the value has been copied into our VMEM, we can accumulate it into our result (contained in `o_ref`).\n \n-A subtle race condition can occur if one device runs one loop ahead of it's right neighbor. In this case, it could copy into the receiver's `working_slot` at the same time the receiver is reading from it. In order to avoid this, each device will block on a `REGULAR` semaphore before copying into the right neighbor's `dst_ref` until it has signaled that it is done reading from its `working_slot`. This race condition is rarely triggered for a small kernel such as this example, but can it can be explicitly triggered if for example using a `pltpu.delay` instruction to artifically hang a device.\n+A subtle race condition can occur if one device runs one loop ahead of it's right neighbor. In this case, it could copy into the receiver's `working_slot` at the same time the receiver is reading from it. In order to avoid this, each device will block on a `REGULAR` semaphore before copying into the right neighbor's `dst_ref` until it has signaled that it is done reading from its `working_slot`. This race condition is rarely triggered for a small kernel such as this example, but can it can be explicitly triggered if for example using a `pltpu.delay` instruction to artificially hang a device.\n \n Note that this is not an optimal or fully general kernel, as the block sizes must entirely fit in VMEM and we could better interleave communication and accumulation. We will discuss these optimizations in later sections.\n \n@@ -585,7 +585,7 @@ def local_barrier(left_neighbor, right_neighbor, double_barrier=True):\n   \"\"\"Performs a barrier with neighbors on the global barrier semaphore.\n \n   Optionally performs a second barrier, which prevents a potential race\n-  when re-using the same collective_id across kernel invocations.\n+  when reusing the same collective_id across kernel invocations.\n   \"\"\"\n   barrier_sem = pltpu.get_barrier_semaphore()\n   for neighbor in [left_neighbor, right_neighbor]:\n@@ -1514,4 +1514,4 @@ In this tutorial we covered several kernel examples which replicate the function\n \n ### Next Steps\n \n-Excellent follow-up excercises for the reader could include implementing a distributed matrix multiplication, implementing `lax.all_to_all`, and relaxing synchronization to allow for additional run-ahead.\n+Excellent follow-up exercises for the reader could include implementing a distributed matrix multiplication, implementing `lax.all_to_all`, and relaxing synchronization to allow for additional run-ahead.\ndiff --git a/docs/pallas/tpu/sparse.ipynb b/docs/pallas/tpu/sparse.ipynb\nindex ac3a0dad2404..31cfa8eeb328 100644\n--- a/docs/pallas/tpu/sparse.ipynb\n+++ b/docs/pallas/tpu/sparse.ipynb\n@@ -491,7 +491,7 @@\n    \"source\": [\n     \"def sparsify_mask(mask: jax.Array,\\n\",\n     \"                  block_shape: tuple[int, int]):\\n\",\n-    \"  \\\"\\\"\\\"Preprocesses a mask into a sparse reprentation.\\n\",\n+    \"  \\\"\\\"\\\"Preprocesses a mask into a sparse representation.\\n\",\n     \"\\n\",\n     \"  Args:\\n\",\n     \"    mask: A boolean array of shape [M, N]\\n\",\ndiff --git a/docs/pallas/tpu/sparse.md b/docs/pallas/tpu/sparse.md\nindex 113f31d8bab2..35613acdb2c9 100644\n--- a/docs/pallas/tpu/sparse.md\n+++ b/docs/pallas/tpu/sparse.md\n@@ -391,7 +391,7 @@ As we will be working with a sparse mask, we will begin by implementing a functi\n \n def sparsify_mask(mask: jax.Array,\n                   block_shape: tuple[int, int]):\n-  \"\"\"Preprocesses a mask into a sparse reprentation.\n+  \"\"\"Preprocesses a mask into a sparse representation.\n \n   Args:\n     mask: A boolean array of shape [M, N]\ndiff --git a/docs/persistent_compilation_cache.md b/docs/persistent_compilation_cache.md\nindex e241e76e3c5f..d795a054bc87 100644\n--- a/docs/persistent_compilation_cache.md\n+++ b/docs/persistent_compilation_cache.md\n@@ -260,7 +260,7 @@ If we were to merely compile this function without shard_map, the cache key for\n layernorm_matmul_without_shard_map = jax.jit(F, in_shardings=(...), out_sharding=(...))(x1, x2, gamma, beta)\n ```\n \n-However, if we were to wrap the layernorm primitive in shard_map and define a function G that performs the same computation, the cache key for `layernorm_matmul_with_shard_map` will be the same everytime despite `LayerNorm` being implementing `custom_partitioning`:\n+However, if we were to wrap the layernorm primitive in shard_map and define a function G that performs the same computation, the cache key for `layernorm_matmul_with_shard_map` will be the same every time despite `LayerNorm` being implementing `custom_partitioning`:\n \n ```python\n import jax\ndiff --git a/docs/random-numbers.md b/docs/random-numbers.md\nindex 134b690839e0..5562dc3f43d5 100644\n--- a/docs/random-numbers.md\n+++ b/docs/random-numbers.md\n@@ -150,7 +150,7 @@ print(random.normal(key))\n print(random.normal(key))\n ```\n \n-Re-using the same key, even with different {mod}`~jax.random` APIs, can result in correlated outputs, which is generally undesirable. \n+Reusing the same key, even with different {mod}`~jax.random` APIs, can result in correlated outputs, which is generally undesirable. \n \n **The rule of thumb is: never reuse keys (unless you want identical outputs). Reusing the same state will cause __sadness__ and __monotony__, depriving the end user of __lifegiving chaos__.**\n \ndiff --git a/examples/ffi/src/jax_ffi_example/rms_norm.py b/examples/ffi/src/jax_ffi_example/rms_norm.py\nindex 5ba97f48ebad..996eb9e5d935 100644\n--- a/examples/ffi/src/jax_ffi_example/rms_norm.py\n+++ b/examples/ffi/src/jax_ffi_example/rms_norm.py\n@@ -16,7 +16,7 @@\n This example is exactly the same as the one in the `FFI tutorial\n <https://docs.jax.dev/en/latest/ffi.html>`, so more details can be found\n on that page. But, the high level summary is that we implement our custom\n-extension in ``rms_norm.cc``, then call it usin ``jax.ffi.ffi_call`` in\n+extension in ``rms_norm.cc``, then call it using ``jax.ffi.ffi_call`` in\n this module. The behavior under autodiff is implemented using\n ``jax.custom_vjp``.\n \"\"\"\ndiff --git a/jax/_src/api_util.py b/jax/_src/api_util.py\nindex 2e7ba551c624..5261764d0bf8 100644\n--- a/jax/_src/api_util.py\n+++ b/jax/_src/api_util.py\n@@ -606,7 +606,7 @@ def debug_info(\n   \"\"\"Constructd core.DebugInfo for a function given example args and kwargs.\n \n   `args` and `kwargs` are example positional and keyword arguments, users with\n-  `inspect.Signature` to get the names of argments. The arguments that are\n+  `inspect.Signature` to get the names of arguments. The arguments that are\n   considered static for tracing purposes should be included, and designated\n   using `static_argnums` and `static_argnames`.\n \ndiff --git a/jax/_src/cache_key.py b/jax/_src/cache_key.py\nindex 6fe3d8819d3c..906e686727ef 100644\n--- a/jax/_src/cache_key.py\n+++ b/jax/_src/cache_key.py\n@@ -56,7 +56,7 @@ def get_flag_prefixes() -> list[str]:\n def custom_hook() -> str:\n   \"\"\"Custom hook for any addition to the cache key.\n \n-  The custom hook will be called everytime get() is called and can be\n+  The custom hook will be called every time get() is called and can be\n   defined to return a string that will be hashed into the cache key.\n   \"\"\"\n   return \"\"\ndiff --git a/jax/_src/clusters/cluster.py b/jax/_src/clusters/cluster.py\nindex 69ef77a6421d..1c0a6fca9df6 100644\n--- a/jax/_src/clusters/cluster.py\n+++ b/jax/_src/clusters/cluster.py\n@@ -23,7 +23,7 @@\n class ClusterEnv:\n   \"\"\"Interface for defining a cluster environment.\n \n-  To enable auto bootrapping (aka :func:`jax.distributed.initialize()`),\n+  To enable auto bootstrapping (aka :func:`jax.distributed.initialize()`),\n   cluster environments need to derive from :class:`ClusterEnv` and implement\n   :func:`is_env_present`, :func:`get_coordinator_address`,\n   :func:`get_process_count`, and :func:`get_process_id`.\ndiff --git a/jax/_src/clusters/k8s_cluster.py b/jax/_src/clusters/k8s_cluster.py\nindex af1b7c020eed..fb312038bf2c 100644\n--- a/jax/_src/clusters/k8s_cluster.py\n+++ b/jax/_src/clusters/k8s_cluster.py\n@@ -78,7 +78,7 @@ def is_env_present(cls) -> bool:\n             textwrap.fill(\n               \"Kubernetes environment detected, but the `kubernetes` package \"\n               \"is not installed to enable automatic bootstrapping in this \"\n-              \"environment. To enable automatic boostrapping, please install \"\n+              \"environment. To enable automatic bootstrapping, please install \"\n               \"jax with the [k8s] extra. For example:\"),\n             \"    pip install jax[k8s]\",\n             \"    pip install jax[k8s,<MORE-EXTRAS...>]\",\ndiff --git a/jax/_src/cudnn/fused_attention_stablehlo.py b/jax/_src/cudnn/fused_attention_stablehlo.py\nindex 46df84e08e0f..84ca9226e82c 100644\n--- a/jax/_src/cudnn/fused_attention_stablehlo.py\n+++ b/jax/_src/cudnn/fused_attention_stablehlo.py\n@@ -357,7 +357,7 @@ def check_is_flash_attention(\n         H_max = 256 if cudnn_version >= 90500 and is_on_hopper else 128\n         if not (H <= H_max and H % 8 == 0):\n           raise NotImplementedError(\n-              f\"The head dim must be <= {H_max} and a mutiple of 8, \"\n+              f\"The head dim must be <= {H_max} and a multiple of 8, \"\n               f\"but got {H}.\"\n           )\n \n@@ -1844,7 +1844,7 @@ def dot_product_attention(\n         # should be broadcast to same shape\n         bias = bias + mask\n \n-    # check if input shape and data type is compatiable\n+    # check if input shape and data type is compatible\n     check_layout(query, key, value, bias, q_seqlen, kv_seqlen, q_offsets, kv_offsets, layout)\n     has_bias = bias is not None\n     has_dbias = has_bias and \\\ndiff --git a/jax/_src/custom_batching.py b/jax/_src/custom_batching.py\nindex 338074837ea5..83c9ffb5ee36 100644\n--- a/jax/_src/custom_batching.py\n+++ b/jax/_src/custom_batching.py\n@@ -103,7 +103,7 @@ class custom_vmap:\n     >>> jax.grad(f)(jnp.zeros(()), jnp.ones(()))\n     Array(1., dtype=float32)\n \n-  Note that the :py:class:`jax.custom_vjp` must be on the ouside, wrapping the\n+  Note that the :py:class:`jax.custom_vjp` must be on the outside, wrapping the\n   ``custom_vmap``-decorated function.\n   \"\"\"\n \ndiff --git a/jax/_src/custom_dce.py b/jax/_src/custom_dce.py\nindex d336c969a3c4..25fe604085fd 100644\n--- a/jax/_src/custom_dce.py\n+++ b/jax/_src/custom_dce.py\n@@ -251,9 +251,9 @@ def flatten_dce_rule(\n   # For error checking purposes, we need to reformat the pytree structure\n   # of the output of the DCE rule to match the original output. The catch is\n   # that the DCE rule can return a None to indicated an unused subtree, so we\n-  # need to rebuild those subtrees with a sentinal value at the leaves. This\n+  # need to rebuild those subtrees with a sentinel value at the leaves. This\n   # logic is very similar to what is used in custom_dervatives._flatten_bwd.\n-  sentinal = object()\n+  sentinel = object()\n   dummy = tree_util.tree_unflatten(out_tree, [object()] * out_tree.num_leaves)\n   keypaths, _ = util.unzip2(tree_util.tree_flatten_with_path(dummy)[0])\n   out_flat = []\n@@ -261,7 +261,7 @@ def flatten_dce_rule(\n   def append(x, d):\n     num_leaves = len(tree_util.tree_flatten(d)[0])\n     if x is None and d is not None:\n-      out_flat.extend([sentinal] * num_leaves)\n+      out_flat.extend([sentinel] * num_leaves)\n     elif x is not None:\n       out_flat.extend([x] * num_leaves)\n     return x\n@@ -281,7 +281,7 @@ def append(x, d):\n   for kp, used, aval, val in zip(keypaths, used_outs, out_avals, out_flat):\n     if not used:\n       continue\n-    if val is sentinal:\n+    if val is sentinel:\n       raise ValueError(\n           f\"Custom DCE rule {rule_name} for function {fun_name} must produce \"\n           \"values for all of the required outputs (as specified by the \"\ndiff --git a/jax/_src/custom_partitioning_sharding_rule.py b/jax/_src/custom_partitioning_sharding_rule.py\nindex d17399beda5b..bc27f34b3bfb 100644\n--- a/jax/_src/custom_partitioning_sharding_rule.py\n+++ b/jax/_src/custom_partitioning_sharding_rule.py\n@@ -138,12 +138,12 @@ def __init__(self, operand_mappings: tuple[ArrayMapping, ...],\n     # Check that factors that are used for a whole dimension aren't in\n     # factor_sizes and factors that are never used for a whole dimension are\n     # in factor_sizes.\n-    for factor, inferrable in factors_inferrable.items():\n-      if factor not in factor_sizes and not inferrable:\n+    for factor, inferable in factors_inferrable.items():\n+      if factor not in factor_sizes and not inferable:\n         raise ValueError(\n           f\"Factor {factor} is only used in compound factors; must specify\"\n           \" its size\")\n-      if factor in factor_sizes and inferrable:\n+      if factor in factor_sizes and inferable:\n         raise ValueError(\n           f\"Factor {factor} represents a whole dimension; do not specify its\"\n           \" size\")\ndiff --git a/jax/_src/dlpack.py b/jax/_src/dlpack.py\nindex 40a69d1e0390..1f19ac0f45c0 100644\n--- a/jax/_src/dlpack.py\n+++ b/jax/_src/dlpack.py\n@@ -130,7 +130,7 @@ def to_dlpack(x: Array, stream: int | Any | None = None,\n       ) from None\n \n   # As new versions are adopted over time, we can maintain some legacy paths\n-  # for compatability mediated through the max_version parameter.\n+  # for compatibility mediated through the max_version parameter.\n   # TODO(micky774): Deprecate default usage of DLPackManagedTensor when XLA\n   # supports DLManagedTensorVersioned (DLPack version 1.0) and repurpose the\n   # current _to_dlpack as a legacy path for (0,5) <= max_version < (1,0).\ndiff --git a/jax/_src/errors.py b/jax/_src/errors.py\nindex 20b82f629f6f..a548714869ab 100644\n--- a/jax/_src/errors.py\n+++ b/jax/_src/errors.py\n@@ -503,7 +503,7 @@ class TracerBoolConversionError(ConcretizationTypeError):\n \n     In this case, the error occurs because Python's built-in ``min`` function is not\n     compatible with JAX transforms. This can be fixed by replacing it with\n-    ``jnp.minumum``:\n+    ``jnp.minimum``:\n \n       >>> @jit\n       ... def func(x):\ndiff --git a/jax/_src/export/shape_poly.py b/jax/_src/export/shape_poly.py\nindex 31371cf345a1..bb8a159ee54b 100644\n--- a/jax/_src/export/shape_poly.py\n+++ b/jax/_src/export/shape_poly.py\n@@ -978,7 +978,7 @@ def cmp_sequence(s1, s2, elem_cmp) -> int:\n \n \n class SymbolicScope:\n-  \"\"\"Indentifies a scope for symbolic expressions.\n+  \"\"\"Identifies a scope for symbolic expressions.\n \n   All symbolic expressions that interact (e.g., appear in the argument shapes\n   for one JAX function invocation, or are involved in arithmetic operations)\ndiff --git a/jax/_src/ffi.py b/jax/_src/ffi.py\nindex 3bfe8130ccda..db943d675b80 100644\n--- a/jax/_src/ffi.py\n+++ b/jax/_src/ffi.py\n@@ -56,7 +56,7 @@ def register_ffi_target(\n     name: the name of the target.\n     fn: a ``PyCapsule`` object containing the function pointer, or a ``dict``\n       where the keys are FFI stage names (e.g. `\"execute\"`) and the values are\n-      ``PyCapsule`` objects continaing a pointer to the handler for that stage.\n+      ``PyCapsule`` objects containing a pointer to the handler for that stage.\n     platform: the target platform.\n     api_version: the XLA custom call API version to use. Supported versions are:\n       1 (default) for the typed FFI or 0 for the earlier \"custom call\" API.\n@@ -369,7 +369,7 @@ def ffi_call(\n \n   Like :func:`~jax.pure_callback`, the behavior of ``ffi_call`` under\n   :func:`~jax.vmap` depends on the value of ``vmap_method``. See the\n-  :func:`~jax.pure_callback` documenation for more details about the allowed\n+  :func:`~jax.pure_callback` documentation for more details about the allowed\n   values and examples of their behavior.\n \n   The current default behavior is to use ``vmap_method=\"sequential\"`` when\ndiff --git a/jax/_src/internal_test_util/export_back_compat_test_util.py b/jax/_src/internal_test_util/export_back_compat_test_util.py\nindex b86b24e2b4fc..7b4af36e5dc4 100644\n--- a/jax/_src/internal_test_util/export_back_compat_test_util.py\n+++ b/jax/_src/internal_test_util/export_back_compat_test_util.py\n@@ -321,7 +321,7 @@ def ndarray_to_aval(a: np.ndarray) -> core.ShapedArray:\n     in_avals_tree = tree_util.tree_map(ndarray_to_aval, args_specs)\n     # TODO: we ought to ensure that out_avals are polymorphic if need be. We\n     # could either save the in/out_avals (but we need to first implement that\n-    # support in export), or we can just re-use them from the current\n+    # support in export), or we can just reuse them from the current\n     # exported.\n     out_avals_tree = tree_util.tree_map(ndarray_to_aval, data.expected_outputs)\n     # in_tree must be for (args, kwargs)\ndiff --git a/jax/_src/interpreters/mlir.py b/jax/_src/interpreters/mlir.py\nindex 6bad0ee2a018..0864ec8646c9 100644\n--- a/jax/_src/interpreters/mlir.py\n+++ b/jax/_src/interpreters/mlir.py\n@@ -706,7 +706,7 @@ class ModuleContext:\n   # Cached primitive lowerings.\n   cached_primitive_lowerings: dict[Any, func_dialect.FuncOp]\n \n-  # Cached traceback infromation.\n+  # Cached traceback information.\n   traceback_caches: TracebackCaches\n \n   lowering_parameters: LoweringParameters\ndiff --git a/jax/_src/jaxpr_util.py b/jax/_src/jaxpr_util.py\nindex a6c93c8c120c..cb9eef0b9ea2 100644\n--- a/jax/_src/jaxpr_util.py\n+++ b/jax/_src/jaxpr_util.py\n@@ -233,7 +233,7 @@ def jaxpr_and_binder_in_params(params, index: int) -> Iterator[tuple[core.Jaxpr,\n \n def eqns_using_var(jaxpr: core.Jaxpr, invar: core.Var) -> Iterator[core.JaxprEqn]:\n   \"\"\"Find the leaf equations using a variable\"\"\"\n-  # The complexity of this call is becauase the invar might originate from a nested jaxpr\n+  # The complexity of this call is because the invar might originate from a nested jaxpr\n   for eqn, invar_index in eqns_using_var_with_invar_index(jaxpr, invar):\n     if (child_jaxprs_and_vars := tuple(jaxpr_and_binder_in_params(eqn.params, invar_index))):\n       for (jaxpr, invar) in child_jaxprs_and_vars:\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 985c5ba52294..ad09292731cf 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -198,7 +198,7 @@ def scan(f, init, xs, length=None):\n       a single iteration of a loop. If an integer is provided, it determines how\n       many unrolled loop iterations to run within a single rolled iteration of\n       the loop. If a boolean is provided, it will determine if the loop is\n-      competely unrolled (i.e. `unroll=True`) or left completely rolled (i.e.\n+      completely unrolled (i.e. `unroll=True`) or left completely rolled (i.e.\n       `unroll=False`).\n     _split_transpose: experimental optional bool specifying whether to further\n       split the transpose into a scan (computing activation gradients), and a\n@@ -2427,7 +2427,7 @@ def fori_loop(lower, upper, body_fun, init_val):\n     unroll: An optional integer or boolean that determines how much to unroll\n       the loop. If an integer is provided, it determines how many unrolled\n       loop iterations to run within a single rolled iteration of the loop. If a\n-      boolean is provided, it will determine if the loop is competely unrolled\n+      boolean is provided, it will determine if the loop is completely unrolled\n       (i.e. `unroll=True`) or left completely unrolled (i.e. `unroll=False`).\n       This argument is only applicable if the loop bounds are statically known.\n \ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex 43dffc7bef9c..922515926f2f 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -935,7 +935,7 @@ def integer_pow(x: ArrayLike, y: int) -> Array:\n     An array of the same shape and dtype as ``x`` containing the elementwise power.\n \n   See also:\n-    :func:`jax.lax.pow`: Elementwise pwoer where ``y`` is an array.\n+    :func:`jax.lax.pow`: Elementwise power where ``y`` is an array.\n \n   .. _stablehlo.multiply: https://openxla.org/stablehlo/spec#multiply\n   \"\"\"\n@@ -2102,7 +2102,7 @@ class DotAlgorithm(NamedTuple):\n \n   The `StableHLO spec <https://openxla.org/stablehlo/spec#dot_general>`_ for\n   the dot operation doesn't require that the precision types be the same as the\n-  storage types for the inputs or outputs, but some plaforms may require that\n+  storage types for the inputs or outputs, but some platforms may require that\n   these types match. Furthermore, the return type of\n   :func:`~jax.lax.dot_general` is always defined by the ``accumulation_type``\n   parameter of the input algorithm, if specified.\n@@ -7923,7 +7923,7 @@ def _sort_abstract_eval(*args, **kwargs):\n \n \n def _canonicalize_float_for_sort(x):\n-  # In the sort comparator, we are going to use a comparision operator where -0\n+  # In the sort comparator, we are going to use a comparison operator where -0\n   # would be before 0, and -NaN and NaN appear at the beginning and end of the\n   # ordering. In this scheme, -0 would be before 0, and -NaN and NaN appear at\n   # the beginning and end of the ordering. This causes issues for stable\n@@ -8164,7 +8164,7 @@ def _create_token_lowering(ctx, *operands):\n def after_all(*operands):\n   \"\"\"Merges one or more XLA token values. Experimental.\n \n-  Wraps the XLA AfterAll operator.\"\"\"\n+  Wraps the XLA after all operator.\"\"\"\n   operands = core.standard_insert_pvary(*operands)\n   return after_all_p.bind(*operands)\n \ndiff --git a/jax/_src/lax/linalg.py b/jax/_src/lax/linalg.py\nindex 2fda4a90369d..3ee7cc2a6807 100644\n--- a/jax/_src/lax/linalg.py\n+++ b/jax/_src/lax/linalg.py\n@@ -2148,7 +2148,7 @@ def _svd_gpu_sub_lowering(ctx, operand, *, full_matrices, compute_uv,\n   # default QR algorithm, but users can (in principle) override this behavior\n   # by passing `use_jacobi=True`.\n   #\n-  # TODO(danfm): Since this was originally implemented, hipSolver appers to\n+  # TODO(danfm): Since this was originally implemented, hipSolver appears to\n   # have added support for the Jacobi algorithm, so we should investigate\n   # removing this condition.\n   if algorithm is None or algorithm == SvdAlgorithm.DEFAULT:\n@@ -2339,7 +2339,7 @@ def a_inverse(rhs):\n                             transpose_a=transpose_a, conjugate_a=conjugate_a,\n                             unit_diagonal=unit_diagonal)\n \n-  # triangular_solve is about the same cost as matrix multplication (~n^2 FLOPs\n+  # triangular_solve is about the same cost as matrix multiplication (~n^2 FLOPs\n   # for matrix/vector inputs). Order these operations in whichever order is\n   # cheaper.\n   if left_side:\n@@ -2776,8 +2776,8 @@ def _column_major_matrix_layout(dim: int) -> tuple[int, ...]:\n \n def _sdy_rule_for_aval(letters, num_batch_dims, aval):\n   d = len(aval.shape) - num_batch_dims\n-  preffix = \"... \" if num_batch_dims and d >= 0 else \"\"\n-  return preffix + \" \".join(next(letters) for _ in range(d))\n+  prefix = \"... \" if num_batch_dims and d >= 0 else \"\"\n+  return prefix + \" \".join(next(letters) for _ in range(d))\n \n def _build_sdy_sharding_rule(num_batch_dims, avals_in, avals_out):\n   letters = iter(string.ascii_letters)\ndiff --git a/jax/_src/nn/functions.py b/jax/_src/nn/functions.py\nindex 3f7647758003..f01c4fa52804 100644\n--- a/jax/_src/nn/functions.py\n+++ b/jax/_src/nn/functions.py\n@@ -1077,7 +1077,7 @@ def dot_product_attention(\n       token's local window. If set, this specifies the (left_window_size,\n       right_window_size) for each token. E.g., if local_window_size == (3, 2)\n       and the sequence is [0, 1, 2, 3, 4, 5, c, 7, 8, 9], token `c` can attend\n-      to [3, 4, 5, c, 7, 8]. If a single int is given, it will be intepreted as\n+      to [3, 4, 5, c, 7, 8]. If a single int is given, it will be interpreted as\n       a symmetric window (window_size, window_size).\n     implementation: A string to control which implementation backend to use.\n       Supported strings are `xla`, `cudnn` (cuDNN flash attention). It defaults\ndiff --git a/jax/_src/numpy/fft.py b/jax/_src/numpy/fft.py\nindex 21da91ce613f..970847532e46 100644\n--- a/jax/_src/numpy/fft.py\n+++ b/jax/_src/numpy/fft.py\n@@ -712,7 +712,7 @@ def hfft(a: ArrayLike, n: int | None = None,\n       are supported. Default is \"backward\".\n \n   Returns:\n-    A real-valued array containing the one-dimensional discret Fourier transform\n+    A real-valued array containing the one-dimensional discrete Fourier transform\n     of ``a`` by exploiting its inherent Hermitian-symmetry, having a dimension of\n     ``n`` along ``axis``.\n \ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex f323bc64718b..a35bcbb23213 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -272,7 +272,7 @@ def load(file: IO[bytes] | str | os.PathLike[Any], *args: Any, **kwargs: Any) ->\n def fmin(x1: ArrayLike, x2: ArrayLike) -> Array:\n   \"\"\"Return element-wise minimum of the input arrays.\n \n-  JAX implemtentation of :func:`numpy.fmin`.\n+  JAX implementation of :func:`numpy.fmin`.\n \n   Args:\n     x1: input array or scalar.\n@@ -2251,7 +2251,7 @@ def resize(a: ArrayLike, new_shape: Shape) -> Array:\n \n   Returns:\n     A resized array with specified shape. The elements of ``a`` are repeated in\n-    the resized array, if the resized array is larger than the original aray.\n+    the resized array, if the resized array is larger than the original array.\n \n   See also:\n     - :func:`jax.numpy.reshape`: Returns a reshaped copy of an array.\n@@ -5575,7 +5575,7 @@ def astype(x: ArrayLike, dtype: DTypeLike | None,\n            device: xc.Device | Sharding | None = None) -> Array:\n   \"\"\"Convert an array to a specified dtype.\n \n-  JAX imlementation of :func:`numpy.astype`.\n+  JAX implementation of :func:`numpy.astype`.\n \n   This is implemented via :func:`jax.lax.convert_element_type`, which may\n   have slightly different behavior than :func:`numpy.astype` in some cases.\n@@ -5957,7 +5957,7 @@ def from_dlpack(x: Any, /, *, device: xc.Device | Sharding | None = None,\n       if needed for a device transfer.\n \n   Returns:\n-    A JAX array of the imput buffer.\n+    A JAX array of the input buffer.\n \n   Note:\n     While JAX arrays are always immutable, dlpack buffers cannot be marked as\n@@ -8419,7 +8419,7 @@ def vander(\n            [3, 1],\n            [4, 1]], dtype=int32)\n \n-    Generates the Vandermonde matrix in increaing order of powers, when\n+    Generates the Vandermonde matrix in increasing order of powers, when\n     ``increasing=True``.\n \n     >>> jnp.vander(x, increasing=True)\ndiff --git a/jax/_src/numpy/linalg.py b/jax/_src/numpy/linalg.py\nindex f2deddd52f05..2351b0ccb075 100644\n--- a/jax/_src/numpy/linalg.py\n+++ b/jax/_src/numpy/linalg.py\n@@ -1617,7 +1617,7 @@ def matrix_transpose(x: ArrayLike, /) -> Array:\n   x_arr = ensure_arraylike('jnp.linalg.matrix_transpose', x)\n   ndim = x_arr.ndim\n   if ndim < 2:\n-    raise ValueError(f\"matrix_transpose requres at least 2 dimensions; got {ndim=}\")\n+    raise ValueError(f\"matrix_transpose requires at least 2 dimensions; got {ndim=}\")\n   return lax.transpose(x_arr, (*range(ndim - 2), ndim - 1, ndim - 2))\n \n \ndiff --git a/jax/_src/numpy/ufunc_api.py b/jax/_src/numpy/ufunc_api.py\nindex 243ab9aa0878..c85621d6cdba 100644\n--- a/jax/_src/numpy/ufunc_api.py\n+++ b/jax/_src/numpy/ufunc_api.py\n@@ -92,7 +92,7 @@ class ufunc:\n            [ 5,  6,  7,  8,  9],\n            [ 6,  7,  8,  9, 10]], dtype=int32)\n \n-    The :meth:`ufunc.reduce` method perfoms a reduction over the array.\n+    The :meth:`ufunc.reduce` method performs a reduction over the array.\n     For example, :meth:`jnp.add.reduce` is equivalent to ``jnp.sum``:\n \n     >>> jnp.add.reduce(x)\n@@ -112,7 +112,7 @@ class ufunc:\n     Array([101,   2,   3,   4,   5], dtype=int32)\n \n     And the :meth:`ufunc.reduceat` method performs a number of ``reduce``\n-    operations bewteen specified indices of an array; for ``jnp.add`` the\n+    operations between specified indices of an array; for ``jnp.add`` the\n     operation is similar to :func:`jax.ops.segment_sum`:\n \n     >>> jnp.add.reduceat(x, jnp.array([0, 2]))\n@@ -574,7 +574,7 @@ def outer(self, A: ArrayLike, B: ArrayLike, /) -> Array:\n        [ 10  20  30  40  50  60  70  80  90 100]]\n \n       For input arrays with ``N`` and ``M`` dimensions respectively, the output\n-      will have dimesion ``N + M``:\n+      will have dimension ``N + M``:\n \n       >>> x = jnp.ones((1, 3, 5))\n       >>> y = jnp.ones((2, 4))\ndiff --git a/jax/_src/numpy/ufuncs.py b/jax/_src/numpy/ufuncs.py\nindex 486d3f15e17c..b0ff3cb9747a 100644\n--- a/jax/_src/numpy/ufuncs.py\n+++ b/jax/_src/numpy/ufuncs.py\n@@ -297,7 +297,7 @@ def sign(x: ArrayLike, /) -> Array:\n       -1, & x < 0\n     \\end{cases}\n \n-  For complex valued input, ``jnp.sign`` returns a unit vector repesenting the\n+  For complex valued input, ``jnp.sign`` returns a unit vector representing the\n   phase. For generalized case, the sign of ``x`` is given by:\n \n   .. math::\n@@ -347,8 +347,8 @@ def floor(x: ArrayLike, /) -> Array:\n     the nearest integer that is less than or equal to the value itself.\n \n   See also:\n-    - :func:`jax.numpy.fix`: Rounds the input to the nearest interger towards zero.\n-    - :func:`jax.numpy.trunc`: Rounds the input to the nearest interger towards\n+    - :func:`jax.numpy.fix`: Rounds the input to the nearest integer towards zero.\n+    - :func:`jax.numpy.trunc`: Rounds the input to the nearest integer towards\n       zero.\n     - :func:`jax.numpy.ceil`: Rounds the input up to the nearest integer.\n \n@@ -386,8 +386,8 @@ def ceil(x: ArrayLike, /) -> Array:\n     the nearest integer that is greater than or equal to the value itself.\n \n   See also:\n-    - :func:`jax.numpy.fix`: Rounds the input to the nearest interger towards zero.\n-    - :func:`jax.numpy.trunc`: Rounds the input to the nearest interger towards\n+    - :func:`jax.numpy.fix`: Rounds the input to the nearest integer towards zero.\n+    - :func:`jax.numpy.trunc`: Rounds the input to the nearest integer towards\n       zero.\n     - :func:`jax.numpy.floor`: Rounds the input down to the nearest integer.\n \n@@ -1621,7 +1621,7 @@ def arctan2(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n \n     The results match the input ``theta``, except at the endpoints where :math:`+\\pi`\n     and :math:`-\\pi` represent indistinguishable points on the unit circle. By convention,\n-    :func:`arctan2` alwasy returns values between :math:`-\\pi` and :math:`+\\pi` inclusive.\n+    :func:`arctan2` always returns values between :math:`-\\pi` and :math:`+\\pi` inclusive.\n   \"\"\"\n   return lax.atan2(*promote_args_inexact(\"arctan2\", x1, x2))\n \n@@ -1710,7 +1710,7 @@ def maximum(x: ArrayLike, y: ArrayLike, /) -> Array:\n       arrays.\n     - :func:`jax.numpy.fmax`: Returns element-wise maximum of the input arrays,\n       ignoring NaNs.\n-    - :func:`jax.numpy.amax`: Retruns the maximum of array elements along a given\n+    - :func:`jax.numpy.amax`: Returns the maximum of array elements along a given\n       axis.\n     - :func:`jax.numpy.nanmax`: Returns the maximum of the array elements along\n       a given axis, ignoring NaNs.\n@@ -1774,7 +1774,7 @@ def float_power(x: ArrayLike, y: ArrayLike, /) -> Array:\n     >>> jnp.float_power(x, y)\n     Array([ 9. ,  1. , -0.2], dtype=float32)\n \n-    Inputs with broacast compatibility:\n+    Inputs with broadcast compatibility:\n \n     >>> x1 = jnp.array([[2, -4, 1],\n     ...                 [-1, 2, 3]])\ndiff --git a/jax/_src/pallas/fuser/jaxpr_fusion.py b/jax/_src/pallas/fuser/jaxpr_fusion.py\nindex d1e375e33ef1..8e12b5db483d 100644\n--- a/jax/_src/pallas/fuser/jaxpr_fusion.py\n+++ b/jax/_src/pallas/fuser/jaxpr_fusion.py\n@@ -176,7 +176,7 @@ def _construct_output_fusions(\n       unflat_fusible_outvars\n   )\n \n-  # 3. Calculate dependencies and check disjointness\n+  # 3. Calculate dependencies and check disjointedness\n   downstream_outputs_used_masks = []  # List of bool tuples, one per group\n   already_used_final_outputs = set()  # Indices of final outputs already claimed\n   for outvars_group in partial_flat:\ndiff --git a/jax/_src/pallas/hlo_interpreter.py b/jax/_src/pallas/hlo_interpreter.py\nindex 755df2cd8ceb..fac798fe9dc1 100644\n--- a/jax/_src/pallas/hlo_interpreter.py\n+++ b/jax/_src/pallas/hlo_interpreter.py\n@@ -189,7 +189,7 @@ def eval_jaxpr_recursive(\n     consts: Consts that ``jaxpr`` closes over.\n     *args: Input arguments to the ``jaxpr``.\n     recurse_hop_rule: A Jaxpr interpreter to call on sub-jaxprs of\n-      higher-order primtives.\n+      higher-order primitives.\n     propagate_source_info: Whether to propagate source info.\n   \"\"\"\n   def read(v: jax_core.Atom) -> Any:\n@@ -419,7 +419,7 @@ def pallas_call_hlo_interpret(\n     num_iterations = 1\n \n   # The scan carry: (i, loop_idx, *consts, *ins, *outs, *scratch)\n-  # i:int32 is the interation index\n+  # i:int32 is the iteration index\n   # loop_idx: tuple[int32] are the program ids for each grid axis\n   def cond(carry):\n     i, *_ = carry\ndiff --git a/jax/_src/pallas/mosaic/interpret.py b/jax/_src/pallas/mosaic/interpret.py\nindex e278168d999a..7a6c18d43bb7 100644\n--- a/jax/_src/pallas/mosaic/interpret.py\n+++ b/jax/_src/pallas/mosaic/interpret.py\n@@ -82,7 +82,7 @@ class InterpretParams:\n       Default: False.\n     skip_floating_point_ops: If True, operations that produce only floating\n       point values will not be interpreted; instead, their results will be\n-      replaced with arrays all of `jnp.inf`. Additionaly any floating point\n+      replaced with arrays all of `jnp.inf`. Additionally any floating point\n       operands to any operation will be replaced with (arrays of) `jnp.inf`.\n       Default: False.\n     uninitialized_memory: If \"nan\", allocated buffers are initialized to contain\n@@ -937,7 +937,7 @@ def get(\n         raise ValueError(\n             'Out-of-bounds read of'\n             f' ({device_id} {local_core_id} {memory_space} {buffer_id}):'\n-            f' reading [{read_range}] but bufer has shape {buffer.shape} .'\n+            f' reading [{read_range}] but buffer has shape {buffer.shape} .'\n         )\n \n   if shared_memory.interpret_params.detect_races:\n@@ -1817,7 +1817,7 @@ def _get_randomized_grid_coordinates(\n   For a dimension with 'parallel' semantics at position `d` in the grid, the\n   returned tuple contains a random permutation of the sequence `[0,...,\n   grid[d] - 1]` at index `d`. For each dimension with 'arbitrary' semantics,\n-  the resulting tuple contains an empty array. (Inserting an empty arry for an\n+  the resulting tuple contains an empty array. (Inserting an empty array for an\n   'arbitrary' dimension at position `d` in the grid, instead of the sequence\n   `[0,..., grid[d] - 1]`, allows `grid[d]` to be a dynamic value, i.e. a value\n   not known at Jax trace time.)\n@@ -2059,7 +2059,7 @@ def interpret_pallas_call(\n   output_block_shapes = block_shapes[num_inputs : num_inputs + num_outputs]\n   for i, bm in enumerate(grid_mapping.block_mappings_output):\n     if i in oi_alias_map:\n-      # Re-use the HBM buffer for the aliased pallas_call input.\n+      # Reuse the HBM buffer for the aliased pallas_call input.\n       output_buffer_ids.append(input_buffer_ids[oi_alias_map[i]])\n       output_buffer_shapes.append(input_args[oi_alias_map[i]].shape)\n       output_vals.append(input_args[oi_alias_map[i]])\n@@ -2230,7 +2230,7 @@ def _body(\n       Args:\n         carry: (iteration_idx, loop_idx, grid_point, prev_start_indices,\n                 cur_start_indices).\n-          - iteration_idx: the interation index.\n+          - iteration_idx: the iteration index.\n           - loop_idx: internal indices for looping over the grid.\n           - grid_point: the current positions along all axes of the grid.\n           - prev_start_indices: a rank-1 array that contains the start indices\ndiff --git a/jax/_src/pallas/mosaic/primitives.py b/jax/_src/pallas/mosaic/primitives.py\nindex c9cdcbf56f85..af50773eec20 100644\n--- a/jax/_src/pallas/mosaic/primitives.py\n+++ b/jax/_src/pallas/mosaic/primitives.py\n@@ -663,7 +663,7 @@ def get_barrier_semaphore():\n   to share a collective_id. However, if in doubt, prefer not sharing\n   collective_ids, as doing so incorrectly can lead to silent data corruption or\n   crashes.\n-  Note that re-using the same collective_id doesn't guarantee that the same\n+  Note that reusing the same collective_id doesn't guarantee that the same\n   semaphore is provided by XLA.\n   \"\"\"\n   return get_barrier_semaphore_p.bind()\ndiff --git a/jax/_src/pallas/mosaic/random.py b/jax/_src/pallas/mosaic/random.py\nindex 6a2c557fd55d..8d29f857afb2 100644\n--- a/jax/_src/pallas/mosaic/random.py\n+++ b/jax/_src/pallas/mosaic/random.py\n@@ -177,7 +177,7 @@ def sample_block(sampler_fn: SampleFnType,\n \n   `tile_size` should be chosen such that it is a divisor to all block sizes\n   one needs to be invariant to. The larger the `tile_size`, the more\n-  efficient the sampling process wil be and therefore the best choice is\n+  efficient the sampling process will be and therefore the best choice is\n   typically the greatest common divisor between all possible block sizes.\n \n   Args:\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 7fb933f5623d..3b28ebdd5d20 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -368,7 +368,7 @@ class RefUnion(GPUMemoryRef):\n   \"\"\"A sequence of trees of refs that are allowed to reuse the same memory.\n \n   One should not make assumptions as to how each ref will map to the underlying\n-  memory region, since arbitrary padding may be applied inbetween different\n+  memory region, since arbitrary padding may be applied in between different\n   refs.\n \n   As such, ref unions are only safe to use when the groups of refs that we\n@@ -459,7 +459,7 @@ def untransform_transpose(\n       self, perm: tuple[int, ...]\n   ) -> tuple[tuple[int, ...], state_types.Transform]:\n     # The transpose in question is applied to the utiled ref so we\n-    # need to translate it by duplicating and offseting the last part.\n+    # need to translate it by duplicating and offsetting the last part.\n     off = len(perm)\n     new_suffix = [i + off for i in perm[-len(self.tiling) :]]\n     if set(new_suffix) != set(range(off, off + len(self.tiling))):\n@@ -871,7 +871,7 @@ class Barrier:\n       barriers can be accessed by indexing into the barrier Ref.\n     for_tensor_core: Whether this barrier is used for synchronizing with\n       the tensor core. This should be set to True when waiting on Blackwell\n-      (TC Gen 5) asynchoronous matmul instructions.\n+      (TC Gen 5) asynchronous matmul instructions.\n   \"\"\"\n   num_arrivals: int = 1\n   num_barriers: int = 1\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 5695da4cc8b1..87bb85cfcd70 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -2377,7 +2377,7 @@ def _run_scoped_lowering_rule(\n     if any(should_discharge):\n       # We convert consts to args, because we only have ir.Values and\n       # not JAX values during lowering. discharge_state() produces JAX\n-      # valiues for the aguments but expects them to be provided for the\n+      # valiues for the arguments but expects them to be provided for the\n       # consts. We also don't want to wrap the values in refs.\n       no_const_jaxpr = pe.convert_constvars_jaxpr(jaxpr)\n       should_discharge = [False] * len(consts) + should_discharge\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 616f7e501cd8..f37a003f4401 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -651,7 +651,7 @@ def _extract_barrier_indexer(transforms) -> indexing.NDIndexer | None:\n     case []:\n       return None\n     case _:\n-      raise ValueError(\"Barrier does not support arbirary transforms\")\n+      raise ValueError(\"Barrier does not support arbitrary transforms\")\n \n \n barrier_arrive_p = jax_core.Primitive(\"barrier_arrive\")\n@@ -835,7 +835,7 @@ def _commit_group_lowering(ctx: lowering.LoweringRuleContext):\n \n \n def commit_smem_to_gmem_group() -> None:\n-  \"\"\"Commits all issued but uncommited SMEM->GMEM copies to a group.\"\"\"\n+  \"\"\"Commits all issued but uncommitted SMEM->GMEM copies to a group.\"\"\"\n   commit_group_p.bind()\n \n \ndiff --git a/jax/_src/pallas/pallas_call.py b/jax/_src/pallas/pallas_call.py\nindex b14259556faf..52360b997743 100644\n--- a/jax/_src/pallas/pallas_call.py\n+++ b/jax/_src/pallas/pallas_call.py\n@@ -304,7 +304,7 @@ def _broadcast_input_output_aliases(\n \n   When we have input/output aliasing, since the output will be mapped, we need\n   to make sure to broadcast the input across that dimension if it is not\n-  mapped. If the input is mapped, but on a different axis, we tranpose the input\n+  mapped. If the input is mapped, but on a different axis, we transpose the input\n   to match the output.\n   \"\"\"\n \n@@ -370,7 +370,7 @@ def _batch_with_explicit_loop(\n       axis_size=axis_size,\n   )\n \n-  # The output arrays are completelly overwritten, so we can just initialize\n+  # The output arrays are completely overwritten, so we can just initialize\n   # empty arrays.\n   initial_state = [\n       jnp.empty(tuple_insert(bm.array_shape_dtype.shape, 0, axis_size),\n@@ -801,7 +801,7 @@ def index_rewrite_kernel(*indexer_args):\n         ragged_axis_dim = per_input_ragged_axis_dim[arg_pos]\n \n         # the problem here seems to be that we are rnning this for all inputs, per input, because they each have an indexer - which means\n-        # that the indexer for output isnt getting written - before, it always was\n+        # that the indexer for output isn't getting written - before, it always was\n \n         lengths_ref = indexer_args[-1]\n         rest_indexer_args = indexer_args[:-1]\n@@ -896,7 +896,7 @@ def index_rewrite_kernel(*indexer_args):\n         raise NotImplementedError(\"consts not supported in pallas_call\")\n \n     # We need to rewrite the input_output_aliases here, the initial call\n-    # to broadcast is done, and we have inseted a new input (lengths), so\n+    # to broadcast is done, and we have inserted a new input (lengths), so\n     # there's an off-by-one here now.\n     new_input_output_aliases = []\n     for k, v in input_output_aliases:\n@@ -987,7 +987,7 @@ def pallas_call_checkify_oob_grid(error: checkify.Error,\n       for bm in grid_mapping.block_mappings\n   ]\n   # The scan carry: (i, loop_idx, *consts, *ins, *outs, *scratch)\n-  # i:int32 is the interation index\n+  # i:int32 is the iteration index\n   # loop_idx: tuple[int32] are the program ids for each grid axis\n   def cond(carry):\n     i, *_ = carry\n@@ -1144,7 +1144,7 @@ def _ensure_2d_error_shape(arg):\n   # for the new error inputs and outputs.\n   error_block_specs = [pallas_core.BlockSpec(None, None)] * len(shaped_err_avals)\n   error_paths, _ = unzip2(tree_util.tree_flatten_with_path(error_block_specs)[0])\n-  error_origins = tuple(f\"errrors[{tree_util.keystr(p)}\" for p in error_paths)\n+  error_origins = tuple(f\"errors[{tree_util.keystr(p)}\" for p in error_paths)\n   error_block_mappings = map(\n         partial(\n             pallas_core._convert_block_spec_to_block_mapping,\n@@ -1762,7 +1762,7 @@ def in_path_to_input_origin(\n \n \n # We import the TPU backend at the top level because it defines flags. Note that\n-# we can only do that at the bottom of this file, beacuse it also depends on\n+# we can only do that at the bottom of this file, because it also depends on\n # this module already being initialized.\n \n try:\ndiff --git a/jax/_src/pallas/primitives.py b/jax/_src/pallas/primitives.py\nindex 5038ac6e5171..95ae15e5bf4e 100644\n--- a/jax/_src/pallas/primitives.py\n+++ b/jax/_src/pallas/primitives.py\n@@ -490,7 +490,7 @@ def _load_discharge_rule(in_avals, out_avals, *args_flat, args_tree, **_):\n     scalar_dims = [not isinstance(s, Slice) and not s.shape for s in indices]\n     slice_starts = [s.start if isinstance(s, Slice) else s for s in indices]\n     slice_sizes = tuple(s.size if isinstance(s, Slice) else 1 for s in indices)\n-    # fixes an inconstency with lax.dynamic_slice where if the slice goes out\n+    # fixes an inconsistency with lax.dynamic_slice where if the slice goes out\n     # of bounds, it will instead move the start_index backwards so the slice\n     # will fit in memory.\n     ref = _pad_values_to_avoid_dynamic_slice_oob_shift(ref, slice_sizes)\ndiff --git a/jax/_src/profiler.py b/jax/_src/profiler.py\nindex 424e2b81035f..efdd7bd1e2a1 100644\n--- a/jax/_src/profiler.py\n+++ b/jax/_src/profiler.py\n@@ -402,7 +402,7 @@ def save_device_memory_profile(filename, backend: str | None = None) -> None:\n \n \n # Allows to run model with profiler given amount of times. After required amount\n-# of retries achived client can collect FDO data.\n+# of retries achieved client can collect FDO data.\n class PGLEProfiler:\n \n   def __init__(self, retries: int, percentile: int):\ndiff --git a/jax/_src/scipy/linalg.py b/jax/_src/scipy/linalg.py\nindex 55961607b252..7b0bb06f044c 100644\n--- a/jax/_src/scipy/linalg.py\n+++ b/jax/_src/scipy/linalg.py\n@@ -2189,7 +2189,7 @@ def pascal(n: int, kind: str | None = None) -> Array:\n \n   JAX implementation of :func:`scipy.linalg.pascal`.\n \n-  The elements of the Pascal matrix approximate the binomial coefficents. This\n+  The elements of the Pascal matrix approximate the binomial coefficients. This\n   implementation is not exact as JAX does not support exact factorials.\n \n   Args:\ndiff --git a/jax/_src/scipy/signal.py b/jax/_src/scipy/signal.py\nindex f8c2563027f5..d4ca7c2c6147 100644\n--- a/jax/_src/scipy/signal.py\n+++ b/jax/_src/scipy/signal.py\n@@ -148,7 +148,7 @@ def _fftconvolve_unbatched(in1: Array, in2: Array, mode: str) -> Array:\n   return lax.dynamic_slice(conv, start_indices, out_shape)\n \n \n-# Note: we do not re-use the code from jax.numpy.convolve here, because the handling\n+# Note: we do not reuse the code from jax.numpy.convolve here, because the handling\n # of padding differs slightly between the two implementations (particularly for\n # mode='same').\n def _convolve_nd(in1: Array, in2: Array, mode: str, *, precision: PrecisionLike) -> Array:\n@@ -1030,16 +1030,16 @@ def _overlap_and_add(x: Array, step_size: int) -> Array:\n   x = x.reshape((flat_batchsize, nframes, nstep_per_segment, step_size))\n \n   # For obtaining shifted signals, this routine reinterprets flattened array\n-  # with a shrinked axis.  With appropriate truncation/ padding, this operation\n+  # with a shrunken axis.  With appropriate truncation/ padding, this operation\n   # pushes the last padded elements of the previous row to the head of the\n   # current row.\n   # See implementation of `overlap_and_add` in Tensorflow for details.\n   x = x.transpose((0, 2, 1, 3))  # x: (B, S, N, T)\n   x = jnp.pad(x, ((0, 0), (0, 0), (0, nframes), (0, 0)))  # x: (B, S, N*2, T)\n-  shrinked = x.shape[2] - 1\n+  shrunken = x.shape[2] - 1\n   x = x.reshape((flat_batchsize, -1))\n-  x = x[:, :(nstep_per_segment * shrinked * step_size)]\n-  x = x.reshape((flat_batchsize, nstep_per_segment, shrinked * step_size))\n+  x = x[:, :(nstep_per_segment * shrunken * step_size)]\n+  x = x.reshape((flat_batchsize, nstep_per_segment, shrunken * step_size))\n \n   # Finally, sum shifted segments, and truncate results to the output_size.\n   x = x.sum(axis=1)[:, :output_size]\ndiff --git a/jax/_src/scipy/stats/_core.py b/jax/_src/scipy/stats/_core.py\nindex 65c457f79cc8..ae93dd793844 100644\n--- a/jax/_src/scipy/stats/_core.py\n+++ b/jax/_src/scipy/stats/_core.py\n@@ -285,7 +285,7 @@ def sem(a: ArrayLike, axis: int | None = 0, ddof: int = 1, nan_policy: str = \"pr\n     Array([1.73,  nan, 1.53,  nan,  nan,  nan], dtype=float32)\n \n     If ``nan_policy='omit```, ``sem`` omits the ``nan`` values and computes the error\n-    for the remainging values along the specified axis.\n+    for the remaining values along the specified axis.\n \n     >>> with jnp.printoptions(precision=2, suppress=True):\n     ...   jax.scipy.stats.sem(x2, nan_policy='omit')\ndiff --git a/jax/_src/state/types.py b/jax/_src/state/types.py\nindex e3a86e241bf2..7ca1d8e48f9e 100644\n--- a/jax/_src/state/types.py\n+++ b/jax/_src/state/types.py\n@@ -255,7 +255,7 @@ def shape(self) -> tuple[int | Array, ...]:\n     if not unprocessed:\n       return shape\n     # If there are any unprocessed transforms left, we apply them to the shape\n-    # we've found previuously.\n+    # we've found previously.\n     for t in self.transforms[-unprocessed:]:\n       shape = t.transform_shape(shape)\n     assert shape is not None\ndiff --git a/jax/_src/xla_bridge.py b/jax/_src/xla_bridge.py\nindex 22e71d7d9ce6..1b95806c37c6 100644\n--- a/jax/_src/xla_bridge.py\n+++ b/jax/_src/xla_bridge.py\n@@ -381,7 +381,7 @@ def discover_pjrt_plugins() -> None:\n   \"\"\"Discovers plugins in the namespace package `jax_plugins` and import them.\n \n   There are two methods used to discover plugin modules. They are intended\n-  to be used together by implementors in order to cover all packaging and\n+  to be used together by implementers in order to cover all packaging and\n   development cases:\n \n   1. Define a globally unique module under the `jax_plugins` namespace\n@@ -964,7 +964,7 @@ def backend_xla_version(platform=None) -> int | None:\n   \"\"\"Returns the XLA version of the backend.\n \n   Returns None if the backend does not use PJRT C API or does not have\n-  xla_version in the plugin attributes. This methon can be used to skip features\n+  xla_version in the plugin attributes. This method can be used to skip features\n   that are not available before certain xla_version if the backend is a\n   plugin and uses xla_version.\n   \"\"\"\n@@ -975,7 +975,7 @@ def backend_stablehlo_version(platform=None) -> Sequence[int] | None:\n   \"\"\"Returns the StableHLO version of the backend.\n \n   Returns None if the backend does not use PJRT C API or does not have\n-  stablehlo_current_version in the plugin attributes. This methon can be used to\n+  stablehlo_current_version in the plugin attributes. This method can be used to\n   skip features that are not available before certain stablehlo_current_version\n   if the backend is a plugin and uses stablehlo_current_version.\n   \"\"\"\ndiff --git a/jax/experimental/colocated_python/serialization.py b/jax/experimental/colocated_python/serialization.py\nindex a8a62d78359f..1f1b96487fab 100644\n--- a/jax/experimental/colocated_python/serialization.py\n+++ b/jax/experimental/colocated_python/serialization.py\n@@ -201,7 +201,7 @@ def _serialize_specs(\n   if not hasattr(np.dtypes, \"StringDType\"):\n     raise TypeError(\n         \"Serializing Colocated Python requires StringDType. Please use\"\n-        \" numpy to 2.0.0 or later, or explicityly provide an output spec\"\n+        \" numpy to 2.0.0 or later, or explicitly provide an output spec\"\n         \" function.\"\n     )\n \ndiff --git a/jax/experimental/jax2tf/README.md b/jax/experimental/jax2tf/README.md\nindex 06cc5c86a109..ac9829d69006 100644\n--- a/jax/experimental/jax2tf/README.md\n+++ b/jax/experimental/jax2tf/README.md\n@@ -840,7 +840,7 @@ to `export.symbolic_shape` share a scope and\n can be mixed up in arithmetic operations. The result would\n also share the same scope.\n \n-You can re-use scopes:\n+You can reuse scopes:\n \n ```python\n a, = export.symbolic_shape(\"a,\", constraints=(\"a >= 8\",))\ndiff --git a/jax/experimental/jax2tf/impl_no_xla.py b/jax/experimental/jax2tf/impl_no_xla.py\nindex 644c3324b4e2..70a6dccf8915 100644\n--- a/jax/experimental/jax2tf/impl_no_xla.py\n+++ b/jax/experimental/jax2tf/impl_no_xla.py\n@@ -659,7 +659,7 @@ def tf_pool(inputs, pooling_type):\n       raise NotImplementedError(\n           f\"TODO: use tf.nn.pool with dynamic shapes{window_dimensions=} \"\n           f\" {window_strides=} {dilations=}\")\n-    # tf.nn.pool() currently does not suport tf.int32 and so we cast back and\n+    # tf.nn.pool() currently does not support tf.int32 and so we cast back and\n     # forth in order to be able to convert.\n     if (inputs.dtype in [tf.int16, tf.int32]) and computation_name == \"add\":\n       original_dtype = inputs.dtype\ndiff --git a/jax/experimental/key_reuse/_core.py b/jax/experimental/key_reuse/_core.py\nindex 6f604f1195a0..a2ffc6582fff 100644\n--- a/jax/experimental/key_reuse/_core.py\n+++ b/jax/experimental/key_reuse/_core.py\n@@ -212,7 +212,7 @@ def key_reuse_signature_from_eqn(eqn: core.JaxprEqn) -> KeyReuseSignature:\n       return sig.signature(eqn)\n     else:\n       raise TypeError(\n-        f\"Unrecognized key reuse sigature of type {type(sig)}: {sig}\")\n+        f\"Unrecognized key reuse signature of type {type(sig)}: {sig}\")\n   else:\n     return unknown_signature(eqn)\n \n@@ -231,7 +231,7 @@ def key_reuse_signature_from_primitive(prim, *args, **params):\n     return jaxpr_type_signature(jaxpr)\n   else:\n     raise TypeError(\n-      f\"Unrecognized key reuse sigature of type {type(sig)}: {sig}\")\n+      f\"Unrecognized key reuse signature of type {type(sig)}: {sig}\")\n \n \n consume_p = core.Primitive(\"consume\")\ndiff --git a/jax/experimental/mosaic/gpu/dialect_lowering.py b/jax/experimental/mosaic/gpu/dialect_lowering.py\nindex 20138bbe6fd4..e9293d9ffe08 100644\n--- a/jax/experimental/mosaic/gpu/dialect_lowering.py\n+++ b/jax/experimental/mosaic/gpu/dialect_lowering.py\n@@ -957,7 +957,7 @@ def _mgpu_wgmma_op_lowering_rule(\n     raise ValueError(\"Layout mismatch\")\n   wgmma_layout = fa_layouts[0]\n \n-  # TODO(dasenov): Move the value -> accumulator conversion outisde of wgmma.\n+  # TODO(dasenov): Move the value -> accumulator conversion outside of wgmma.\n   # The associated fence could be a little expensive and is not needed if the\n   # result a wgmma feeds into another wgmma (even in another loop step).\n   acc_in = _fragmented_array_from_ir(wgmma_op.accumulator, wgmma_layout)\ndiff --git a/jax/experimental/mosaic/gpu/examples/flash_attention.py b/jax/experimental/mosaic/gpu/examples/flash_attention.py\nindex 78ef1faddc59..280efd513187 100644\n--- a/jax/experimental/mosaic/gpu/examples/flash_attention.py\n+++ b/jax/experimental/mosaic/gpu/examples/flash_attention.py\n@@ -243,8 +243,8 @@ def kv_loop(kv_step, carry):\n \n         perform_schedule_barrier()\n \n-        # This is quite suprising, but it seems like warp shuffles cannot\n-        # run simutaneously with the WGMMA. For that reason we include it as\n+        # This is quite surprising, but it seems like warp shuffles cannot\n+        # run simultaneously with the WGMMA. For that reason we include it as\n         # part of the TensorCore critical section and not the ALU section.\n         with ctx.named_region(\"Softmax reduction\"):\n           l_i += p.reduce(arith.addf, axis=1)\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex 925aa1575e2d..3554ed95844c 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -669,7 +669,7 @@ def linear_thread_idxs(self):\n #          ...\n #\n # You can see that we have taken 2x2 submatrices from the above layout and\n-# transposed them. The assigment of lanes to elements is such that in both\n+# transposed them. The assignment of lanes to elements is such that in both\n # layouts the same two lanes map to a single 2x2 submatrix, making the transpose\n # very cheap (one shuffle and permute suffices to change between those layouts).\n WGMMA_TRANSPOSED_LAYOUT = TiledLayout(\n@@ -1743,7 +1743,7 @@ def reduce(\n         out_reg = vector.splat(\n             ir.VectorType.get((1,), out_reg.type.element_type), scalar_out_reg\n         )\n-      # Reduce accross warp lanes, if necessary (using warp shuffles).\n+      # Reduce across warp lanes, if necessary (using warp shuffles).\n       if any(reduced_dims[d] for d in layout.partitioned_lane_dims):\n         if utils.bitwidth(out_reg.type) > 32:\n           raise NotImplementedError  # Need to implement wide shfl_bfly.\n@@ -1762,7 +1762,7 @@ def reduce(\n               lane_stride *= 2\n               reduction_size //= 2\n         assert lane_stride == WARP_SIZE, lane_stride\n-      # Reduce accross warps in the warpgroup, if necessary.\n+      # Reduce across warps in the warpgroup, if necessary.\n       if (\n           not isinstance(layout.warp_dim, Replicated)\n           and reduced_dims[layout.warp_dim]\ndiff --git a/jax/experimental/mosaic/gpu/launch_context.py b/jax/experimental/mosaic/gpu/launch_context.py\nindex 2a5bb96f4708..852ac90c0d73 100644\n--- a/jax/experimental/mosaic/gpu/launch_context.py\n+++ b/jax/experimental/mosaic/gpu/launch_context.py\n@@ -779,7 +779,7 @@ def partition_dim(dim: int, idx: ir.Value, num_chunks: int):\n         tuple(slice_shape), swizzle, reduction_op,\n     )\n \n-    # We constuct TMA descriptors in column-major order.\n+    # We construct TMA descriptors in column-major order.\n     rev_dyn_base_indices = [\n         arith.index_cast(i32, idx) for idx in reversed(dyn_base_indices)\n     ]\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex d72994f45a87..91797fe65d1f 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -465,7 +465,7 @@ def _tmem_access_helper(shape, num):\n   num_regs *= num\n   if num_regs > 255:\n     raise ValueError(\n-        f\"TMEM transation too big : {shape=} and {num=} involve\"\n+        f\"TMEM translation too big : {shape=} and {num=} involve\"\n         f\" {num_regs} registers per-thread, which exceeds the limit of 255\"\n     )\n   regs_vector = \",\".join(f\"${i}\" for i in range(num_regs))\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex ac002aa8bffe..b5dbfb62c88f 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -275,7 +275,7 @@ class ThreadSubset(enum.IntEnum):\n   BLOCK = enum.auto()\n \n \n-# True withon `once()` contexts.\n+# True within `once()` contexts.\n _ONCE_PER: ThreadSubset | None = None\n \n \n@@ -468,7 +468,7 @@ def fold_until(shape, off , target)  -> tuple[int, int]:\n         # TODO(cperivol): Implement dependent fold-unfolds for subsections\n         # of the shape eg (..., 4,5,5, ...) -> (..., 10,10, ...) could be\n         # supported without touching any other dimensions.\n-        raise NotImplementedError(f\"Can't reshape {sh0} to {sh1} bu composing independent folds/unfolds.\")\n+        raise NotImplementedError(f\"Can't reshape {sh0} to {sh1} by composing independent folds/unfolds.\")\n \n     raise AssertionError(f\"Unreachable: number of elements don't match in each shape ({sh0} ans {sh1})\")\n \ndiff --git a/jax/experimental/mosaic/gpu/wgmma.py b/jax/experimental/mosaic/gpu/wgmma.py\nindex 2fe826e173e5..9b4fc7678538 100644\n--- a/jax/experimental/mosaic/gpu/wgmma.py\n+++ b/jax/experimental/mosaic/gpu/wgmma.py\n@@ -113,7 +113,7 @@ def wgmma_m64(\n ):\n   out_ty = ir.VectorType(acc.flat[0].type).element_type\n   if not _supported_wgmma_types(out_ty, element_type):\n-    raise ValueError(f\"Usupported wgmma types {(out_ty, element_type)=}\")\n+    raise ValueError(f\"Unsupported wgmma types {(out_ty, element_type)=}\")\n   if n % 8:\n     raise ValueError\n \ndiff --git a/jax/experimental/multihost_utils.py b/jax/experimental/multihost_utils.py\nindex 3a83ff16d612..07a0f747443c 100644\n--- a/jax/experimental/multihost_utils.py\n+++ b/jax/experimental/multihost_utils.py\n@@ -203,7 +203,7 @@ def should_save(step_id: int) -> bool:\n     after some hosts are preempted.\n \n   Raises:\n-    RuntimeError: if preemption sync manager has not been inititialized.\n+    RuntimeError: if preemption sync manager has not been initialized.\n   \"\"\"\n   if distributed.global_state.client is None:\n     return False\n@@ -328,7 +328,7 @@ def host_local_array_to_global_array(\n   >>>\n   >>> host_local_output = multihost_utils.global_array_to_host_local_array(global_out, mesh, out_pspecs) # doctest: +SKIP\n \n-  Please note ths function requires global mesh to be a continuous mesh, meaning\n+  Please note this function requires global mesh to be a continuous mesh, meaning\n   that  devices that belong to each host should form a subcube in this mesh.\n   To move local data to global array with non-continuous mesh use\n   jax.make_array_from_callback or jax.make_array_from_single_device_arrays\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex 650668daf67a..d9d62afe93c5 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -524,7 +524,7 @@ def _compute_sT(acc_ref):\n \n       def _compute(refs):\n         # Combining two WGMMA calls in one block to avoid the unnecessary\n-        # sychronization from two `wgmma.wait_group` calls.\n+        # synchronization from two `wgmma.wait_group` calls.\n         dv_acc_ref, dpT_acc_ref = refs\n         plgpu.wgmma(dv_acc_ref, pT.astype(dtype), do_smem)  # dV\n         plgpu.wgmma(dpT_acc_ref, v_smem, plgpu.transpose_ref(do_smem, (1, 0)))  # dpT\ndiff --git a/jax/experimental/pallas/ops/tpu/paged_attention/util.py b/jax/experimental/pallas/ops/tpu/paged_attention/util.py\nindex 6d6ceca3733f..92aa3a7a1b2c 100644\n--- a/jax/experimental/pallas/ops/tpu/paged_attention/util.py\n+++ b/jax/experimental/pallas/ops/tpu/paged_attention/util.py\n@@ -64,7 +64,7 @@ def grouped_query_attention_reference(\n   if debug:\n     jax.debug.print(\"qk: {qk}\", qk=qk)\n \n-  # Enfore causal mask (adding dimensions when necessary)\n+  # Enforce causal mask (adding dimensions when necessary)\n   mask = jnp.arange(max_seq_len)[None] < seq_lens[:, None]\n   qk += jnp.where(mask, 0.0, MASK_VALUE)[:, None, None, :]\n   if debug:\ndiff --git a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\nindex e7bc599b2b2b..3f12448f2a9c 100644\n--- a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n+++ b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n@@ -638,7 +638,7 @@ def prefetch_next_kv_blk():\n             v = v.astype(q_ref.dtype)\n           kv_head_idx = kv_head_chunk_idx + step_idx\n           q_head_idx = kv_head_idx * num_q_heads_per_kv_head\n-          # TODO(jevinjiang): extra handlig for packed type that can start at\n+          # TODO(jevinjiang): extra handling for packed type that can start at\n           # unaligned position!\n           q = fold_on_2nd_minor(\n               q_ref[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :]\ndiff --git a/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask.py b/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask.py\nindex 3f7a0d863188..354fdb24f9df 100644\n--- a/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask.py\n+++ b/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask.py\n@@ -352,7 +352,7 @@ class ChunkedCausalMask(_ComputableMask):\n   \"\"\"Lazy chunked causal mask.\n \n   Attention is causal within each chunk (0, K), (K, 2K), (2K, 3K), ... tokens\n-  attend to each other but not accross chunks.\n+  attend to each other but not across chunks.\n   Llama4 models use interleaved chunk attention along with global attention.\n \n \n@@ -412,7 +412,7 @@ class LocalMask(_ComputableMask):\n   \"\"\"Lazy local mask, prevents model from attending to tokens outside window.\n \n   Attributes:\n-    window_size: Size of the two sides of the local window (None identifes no\n+    window_size: Size of the two sides of the local window (None identifies no\n       limit for the given side).\n     offset: Offset of q start wrt kv. A positive offset shifts the bottom\n       triangle upward, a negative one shifts it downward. A negative offset\ndiff --git a/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask_info.py b/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask_info.py\nindex 9c79fbbf7e09..37ef92c2d33d 100644\n--- a/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask_info.py\n+++ b/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_mask_info.py\n@@ -418,7 +418,7 @@ def _process_dynamic_mask(\n   # tensors of this shape:\n   mask_info_slice_shape = (heads_per_shard, q_blocks_per_shard, kv_blocks_count)\n \n-  # Collect mask_info shards along the head dimension, concatentate (or\n+  # Collect mask_info shards along the head dimension, concatenate (or\n   # broadcast) them after the loop.\n   data_next_per_head_list, mask_next_per_head_list = [], []\n   for head_shard in range(head_shards):\n@@ -633,7 +633,7 @@ def assign_unique_ids(objects):\n   ]\n \n   # TODO(amagni): checking the validity of the masks is slow for large masks.\n-  # Disable it for now, reevalute in the future.\n+  # Disable it for now, reevaluate in the future.\n \n   partial_mask_block_ids: dict[_HashableNDArray, int] = collections.defaultdict(\n       lambda: len(partial_mask_block_ids)\n@@ -747,7 +747,7 @@ def set_block_mask(mask_id: int, q_index: int, kv_index: int, value: int):\n   q_sequence_axis = 1\n   head_axis = 0\n \n-  # Collect mask_info shards along the head dimension, concatentate (or\n+  # Collect mask_info shards along the head dimension, concatenate (or\n   # broadcast) them after the loop.\n   data_next_per_head_list, mask_next_per_head_list = [], []\n   for head_shard in range(shards_to_process):\ndiff --git a/jax/experimental/shard_map.py b/jax/experimental/shard_map.py\nindex 8f9548bdce1b..027cbd36feea 100644\n--- a/jax/experimental/shard_map.py\n+++ b/jax/experimental/shard_map.py\n@@ -53,7 +53,7 @@ def shard_map(\n     out_specs: a pytree with :class:`~jax.sharding.PartitionSpec` instances as leaves,\n       with a tree structure that is a tree prefix of the output of ``f``. Each\n       ``PartitionSpec`` represents how the corresponding output shards should be\n-      concatenated. In each ``PartitionSpec``, metioning a ``mesh`` axis name at\n+      concatenated. In each ``PartitionSpec``, mentioning a ``mesh`` axis name at\n       a position expresses concatenation of that mesh axis's shards along the\n       corresponding positional axis. Not mentioning a ``mesh`` axis name\n       expresses a promise that the output values are equal along that mesh axis,\ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 02bcbcf16dbc..f916cf385a66 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -182,7 +182,7 @@ def _version_check(name: str,\n                  scale_for_comparison=100)\n   # TODO(phawkins): for some reason this check fails with a cusolver internal\n   # error when fetching the version. This may be a path error from our stubs.\n-  # Figure out what's happening here and reenable.\n+  # Figure out what's happening here and re-enable.\n   # _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n   #                cuda_versions.cusolver_build_version,\n   #                # Ignore patch versions.\ndiff --git a/jaxlib/config.cc b/jaxlib/config.cc\nindex 3d701c516990..625a5aa5a319 100644\n--- a/jaxlib/config.cc\n+++ b/jaxlib/config.cc\n@@ -34,7 +34,7 @@ namespace jax {\n \n namespace nb = nanobind;\n \n-// Singleton object used to represet \"value not set\" in thread-local configs.\n+// Singleton object used to represent \"value not set\" in thread-local configs.\n nb::object UnsetObject() {\n   return nb::steal(PyObject_CallObject(\n       reinterpret_cast<PyObject*>(&PyBaseObject_Type), nullptr));\n@@ -71,7 +71,7 @@ class ThreadLocalConfigState {\n   // These values are accessed in one of two ways:\n   // * The owning thread reads or writes them, while holding the GIL, or, under\n   //   free-threading, while the owning thread is in ATTACHED gc state.\n-  // * Other threads may read or clear values while performing a garbarge\n+  // * Other threads may read or clear values while performing a garbage\n   //   collection.\n   // No locking is needed because a GC thread cannot run concurrently with other\n   // Python threads; even under free-threading Python uses a stop-the-world GC.\n@@ -117,7 +117,7 @@ class GlobalConfigState {\n  private:\n   friend class Config;\n \n-  // The set of thread-local states. This is used during garbarge collection to\n+  // The set of thread-local states. This is used during garbage collection to\n   // visit thread-local values.\n   absl::Mutex mu_;\n   absl::flat_hash_set<ThreadLocalConfigState*> thread_local_states_\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex 678d92bc434a..00e26756ded9 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -173,7 +173,7 @@ def if_building_jaxlib(\n     })\n \n def _cpu_test_deps():\n-    \"\"\"Returns the test depencies needed for a CPU-only JAX test.\"\"\"\n+    \"\"\"Returns the test dependencies needed for a CPU-only JAX test.\"\"\"\n     return select({\n         \"//jax:config_build_jaxlib_true\": [],\n         \"//jax:config_build_jaxlib_false\": [\"@pypi//jaxlib\"],\ndiff --git a/jaxlib/mosaic/dialect/gpu/mosaic_gpu.cc b/jaxlib/mosaic/dialect/gpu/mosaic_gpu.cc\nindex 9d6085397493..2f3bfb808981 100644\n--- a/jaxlib/mosaic/dialect/gpu/mosaic_gpu.cc\n+++ b/jaxlib/mosaic/dialect/gpu/mosaic_gpu.cc\n@@ -441,7 +441,7 @@ llvm::LogicalResult BroadcastInDimOp::verify() {\n     }\n     if (i > 0 && dims[i] <= dims[i - 1]) {\n       return error(\n-          \"The values in the `broadcast_dimensions` attribute must be stricly \"\n+          \"The values in the `broadcast_dimensions` attribute must be strictly \"\n           \"increasing.\");\n     }\n   }\ndiff --git a/jaxlib/mosaic/dialect/gpu/mosaic_gpu.td b/jaxlib/mosaic/dialect/gpu/mosaic_gpu.td\nindex 1465f76aa7bf..217bf1a3593b 100644\n--- a/jaxlib/mosaic/dialect/gpu/mosaic_gpu.td\n+++ b/jaxlib/mosaic/dialect/gpu/mosaic_gpu.td\n@@ -405,7 +405,7 @@ def MosaicGPU_SliceSMEMOp : Op<MosaicGPU_Dialect, \"slice_smem\", []> {\n }\n \n def MosaicGPU_WGMMAOp : Op<MosaicGPU_Dialect, \"wgmma\", [InferTypeOpInterface]> {\n-  let summary = \"Multiply two matrices asyncronously using warpgroup level matrix multiply operations.\";\n+  let summary = \"Multiply two matrices asynchronously using warpgroup level matrix multiply operations.\";\n   let description = [{\n     Schedules WGMMA operations that perform the following matrix multiply and\n     accumulate:\n@@ -434,7 +434,7 @@ def MosaicGPU_WGMMAOp : Op<MosaicGPU_Dialect, \"wgmma\", [InferTypeOpInterface]> {\n     registers need to be synchronized with a memory fence.\n \n     Usually `a` is read from shared memory if it is used directly in the WGMMA\n-    operation. If `a` needs to be transfromed before it is used in the WGMMA\n+    operation. If `a` needs to be transformed before it is used in the WGMMA\n     operation, it may be more convenient to read it directly form registers.\n     This avoids the need to store the data and wait for a fence.\n   }];\ndiff --git a/jaxlib/mosaic/dialect/tpu/layout.h b/jaxlib/mosaic/dialect/tpu/layout.h\nindex 8261d09697e3..dceee9cf41a8 100644\n--- a/jaxlib/mosaic/dialect/tpu/layout.h\n+++ b/jaxlib/mosaic/dialect/tpu/layout.h\n@@ -168,7 +168,7 @@ class RectangularVregBounds : public VRegDataBounds {\n // ---\n //\n // The tiling attribute makes it possible to subdivide a single vector register\n-// into multiple subtiles that traverse the last dimension of a value. For\n+// into multiple sub-tiles that traverse the last dimension of a value. For\n // example, consider vregs of shape (4, 5) on (2, 10) array:\n //\n //   a b c d e f g h i j\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 1669d1bf1586..390ce9d3db32 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -3292,7 +3292,7 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n       // a bunch of loads!\n     } else {\n       return op.emitOpError(\n-          \"Not implemented: dismatch in memref tiling and vector tiling in \"\n+          \"Not implemented: mismatch in memref tiling and vector tiling in \"\n           \"load\");\n     }\n   }\n@@ -4772,7 +4772,7 @@ LogicalResult vector_store_impl(RewriteContext &ctx, Op store_op,\n       // us a bunch of stores!\n     } else {\n       return op.emitOpError(\n-          \"Not implemented: dismatch in memref tiling and vector tiling in \"\n+          \"Not implemented: mismatch in memref tiling and vector tiling in \"\n           \"store\");\n     }\n   }\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex 17575183bd81..7d279c5cb307 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -469,7 +469,7 @@ class VectorLayoutInferer {\n     TPU_CHECK_OP(else_yield->getOperandTypes() == op->getResultTypes(),\n                  \"scf if results and else branch yield operands do not match\");\n     auto else_yield_in_layouts = getLayoutFromOperands(else_yield);\n-    // Find a compatible layout from then and else branches for each reuslt. For\n+    // Find a compatible layout from then and else branches for each result. For\n     // example, if we yield offset (*, *) in then branch and offset (*, 0) in\n     // else branch, the result offset should be (*, 0).\n     SmallVector<Layout, 4> out_layouts;\n@@ -649,7 +649,7 @@ class VectorLayoutInferer {\n     auto yield_in_layouts = getLayoutFromOperands(yield_op);\n \n     // Find a compatible layout from condition body and loop body for each\n-    // reuslt. For example, if we yield offset (*, *) in condition body and\n+    // result. For example, if we yield offset (*, *) in condition body and\n     // offset (*, 0) in loop body, the result offset should be (*, 0).\n     SmallVector<Layout, 4> out_layouts;\n     out_layouts.reserve(op->getNumResults());\ndiff --git a/jaxlib/pjit.cc b/jaxlib/pjit.cc\nindex 804352161597..1e9d53547b1d 100644\n--- a/jaxlib/pjit.cc\n+++ b/jaxlib/pjit.cc\n@@ -653,7 +653,7 @@ absl::StatusOr<nb::object> PjitFunction::Call(nb::handle callable,\n     // development.\n     //\n     // TODO(chky): Consider support uncommitted PyArray in cpp when the python\n-    // side stablizes.\n+    // side stabilizes.\n     if (!py_array.committed() &&\n         jax::Sharding::SafeNumDevices(py_array.sharding()) > 1) {\n       VLOG(2) << \"PyArray argument is not committed and number of global \"\ndiff --git a/jaxlib/py_client.h b/jaxlib/py_client.h\nindex 520fbf8b1e59..772dba864684 100644\n--- a/jaxlib/py_client.h\n+++ b/jaxlib/py_client.h\n@@ -92,7 +92,7 @@ class PyClient {\n     return pjrt_client->shared_ptr_pjrt_client();\n   }\n \n-  // Legacy alises.\n+  // Legacy aliases.\n   std::shared_ptr<PjRtClient> shared_pjrt_client() {\n     return shared_ptr_pjrt_client();\n   }\ndiff --git a/jaxlib/xla_compiler.cc b/jaxlib/xla_compiler.cc\nindex 1b9c8c43b126..57de57b26aee 100644\n--- a/jaxlib/xla_compiler.cc\n+++ b/jaxlib/xla_compiler.cc\n@@ -196,7 +196,7 @@ absl::StatusOr<Shape> MakeShapeWithDenseLayout(\n // `subgroup_types`: indicates the subgroups of the last `subgroup_types.size()`\n //   dimensions in `dims`.\n //\n-// In practice, `reshape_dims` often maps to the axises of user defined device\n+// In practice, `reshape_dims` often maps to the axes of user defined device\n // mesh, and `transpose_perm` often maps to the user specification of how a\n // tensor is partitioned based on the axes defined in the mesh, e.g. for a mesh\n // of size 4x2x2 as AxBxC:\ndiff --git a/tests/checkify_test.py b/tests/checkify_test.py\nindex e7ae4d0468fd..050ac5314da3 100644\n--- a/tests/checkify_test.py\n+++ b/tests/checkify_test.py\n@@ -1228,7 +1228,7 @@ def while_body(s):\n \n     with self.assertRaisesRegex(ValueError, \"checkify-of-vmap-of-while\"):\n       checked_f(jnp.asarray([1., 2., 3.]), jnp.asarray([5., 2., 4.]))\n-    # TODO(lenamartens): reenable assertions below.\n+    # TODO(lenamartens): re-enable assertions below.\n     # self.assertIsNotNone(err.get())\n     # self.assertStartsWith(err.get(), \"division by zero\")\n \n@@ -1257,7 +1257,7 @@ def fun(x):\n \n     with self.assertRaisesRegex(ValueError, \"checkify-of-vmap-of-while\"):\n       checked_f(jnp.arange(5))\n-    # TODO(lenamartens): reenable assertions below.\n+    # TODO(lenamartens): re-enable assertions below.\n     # self.assertIsNone(err.get())\n \n   def test_assert_cond_no_data_dependence(self):\ndiff --git a/tests/debug_info_test.py b/tests/debug_info_test.py\nindex b5e875c03676..5d0f747a85ad 100644\n--- a/tests/debug_info_test.py\n+++ b/tests/debug_info_test.py\n@@ -131,7 +131,7 @@ def _check_tracers_and_jaxprs(self, traceable: Any,\n     mode. The debug infos in the nested Jaxprs are first converted to\n     strings using `_debug_info_to_string` and then\n     compared against `expected_jaxpr_debug_infos`. During this conversion,\n-    we strip occurences of this test file name and a line number\n+    we strip occurrences of this test file name and a line number\n     (e.g., .*/debug_info_test.py:56)\n     An element of `expected_jaxpr_debug_infos` can be a string, in which case\n     it is compared by equality, or a `re.Pattern` (the result of `re.compile`)\ndiff --git a/tests/error_check_test.py b/tests/error_check_test.py\nindex a7eeb4dbf86b..e20017a39a9b 100644\n--- a/tests/error_check_test.py\n+++ b/tests/error_check_test.py\n@@ -35,7 +35,7 @@\n \n \n # TODO: AOT tests fails with the tracer leak checker.\n-# Reenable once https://github.com/jax-ml/jax/issues/27315 is fixed.\n+# Re-enable once https://github.com/jax-ml/jax/issues/27315 is fixed.\n # @jtu.with_config(jax_check_tracer_leaks=True)\n class ErrorCheckTests(jtu.JaxTestCase):\n \ndiff --git a/tests/export_test.py b/tests/export_test.py\nindex 0dfebdcec054..2be5313b0b3a 100644\n--- a/tests/export_test.py\n+++ b/tests/export_test.py\n@@ -1548,7 +1548,7 @@ def test_multi_platform(self):\n     self.assertIn(\"jax.uses_shape_polymorphism = true\",\n                   module_str)\n \n-    # Call with argument placed on different plaforms\n+    # Call with argument placed on different platforms\n     for platform in self.platforms:\n       x_device = jax.device_put(x, jax.devices(platform)[0])\n       res_exp = exp.call(x_device)\n@@ -1573,7 +1573,7 @@ def test_multi_platform_nested(self):\n     count_sine = len(re.findall(\"stablehlo.sine\", exp2_module_str))\n     self.assertEqual(1, count_sine)\n \n-    # Call with argument placed on different plaforms\n+    # Call with argument placed on different platforms\n     for platform in self.platforms:\n       if platform == \"tpu\": continue\n       x_device = jax.device_put(x, jax.devices(platform)[0])\n@@ -1716,7 +1716,7 @@ def f_jax(b):  # b: f32[16 // DEVICES, 4]\n     res_native = f_jax(a)\n     exp = get_exported(f_jax, platforms=(\"cpu\", \"tpu\", \"cuda\", \"rocm\"))(a)\n \n-    # Call with argument placed on different plaforms\n+    # Call with argument placed on different platforms\n     for platform in self.platforms:\n       run_devices = jax.devices(platform)[0:len(export_devices)]\n       if len(run_devices) != len(export_devices):\ndiff --git a/tests/generated_fun_test.py b/tests/generated_fun_test.py\nindex cdfeeba6275b..67c19179bb8b 100644\n--- a/tests/generated_fun_test.py\n+++ b/tests/generated_fun_test.py\n@@ -218,7 +218,7 @@ def check_all_close(xs, ys, tol=1e-3):\n \n def check_close(x, y, tol=1e-3):\n   assert jnp.shape(x) == jnp.shape(y)\n-  # TODO(dougalm): re-enable once we've tackled the less pendantic bugs\n+  # TODO(dougalm): re-enable once we've tackled the less pedantic bugs\n   # assert x.dtype == y.dtype\n   assert jnp.allclose(x, y, rtol=tol, atol=tol), \\\n      f\"Value mismatch:\\n{x}\\n  vs\\n{y}\\n\"\ndiff --git a/tests/gpu_memory_flags_test.py b/tests/gpu_memory_flags_test.py\nindex 87f60dd86f20..bada2bebc74e 100644\n--- a/tests/gpu_memory_flags_test.py\n+++ b/tests/gpu_memory_flags_test.py\n@@ -29,7 +29,7 @@ class GpuMemoryAllocationTest(absltest.TestCase):\n   @jtu.skip_under_pytest(\"Test must run in an isolated process\")\n   @unittest.skipIf(\n       \"XLA_PYTHON_CLIENT_ALLOCATOR\" in os.environ,\n-      \"Test does not work if the python client allocator has been overriden\",\n+      \"Test does not work if the python client allocator has been overridden\",\n   )\n   def test_gpu_memory_allocation(self):\n     falsey_values = (\"0\", \"False\", \"false\")\ndiff --git a/tests/lax_metal_test.py b/tests/lax_metal_test.py\nindex e44ff9ebc930..ecbf908d2f09 100644\n--- a/tests/lax_metal_test.py\n+++ b/tests/lax_metal_test.py\n@@ -3867,7 +3867,7 @@ def testItem(self, shape, dtype, num_args, use_tuple):\n       self._CheckAgainstNumpy(np_op, jnp_op, args_maker)\n \n   @jtu.sample_product(\n-    # Final dimension must be a multiple of 16 to ensure compatibilty of all dtype pairs.\n+    # Final dimension must be a multiple of 16 to ensure compatibility of all dtype pairs.\n     shape=[(0,), (32,), (2, 16)],\n     a_dtype=all_dtypes,\n     dtype=(*all_dtypes, None) if config.enable_x64.value else all_dtypes,\ndiff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py\nindex e98ac6986cae..09a081761d0e 100644\n--- a/tests/lax_numpy_test.py\n+++ b/tests/lax_numpy_test.py\n@@ -6214,7 +6214,7 @@ def test_isdtype(self, dtype, kind):\n     ],\n     dtype=float_dtypes + int_dtypes,\n   )\n-  @jtu.skip_on_devices(\"tpu\")  # TODO(jakevdp): fix and reenable this test.\n+  @jtu.skip_on_devices(\"tpu\")  # TODO(jakevdp): fix and re-enable this test.\n   @jax.numpy_rank_promotion('allow')  # This test explicitly exercises implicit rank promotion.\n   def test_trapezoid(self, yshape, xshape, dtype, dx, axis):\n     rng = jtu.rand_default(self.rng())\ndiff --git a/tests/lax_numpy_ufuncs_test.py b/tests/lax_numpy_ufuncs_test.py\nindex 905d7eed1acd..f2155afb841d 100644\n--- a/tests/lax_numpy_ufuncs_test.py\n+++ b/tests/lax_numpy_ufuncs_test.py\n@@ -543,7 +543,7 @@ def test_binary_ufunc_reduceat(self, name, shape, axis, idx_shape, dtype):\n     if (jnp_fun.nin, jnp_fun.nout) != (2, 1):\n       self.skipTest(f\"accumulate requires (nin, nout)=(2, 1); got {(jnp_fun.nin, jnp_fun.nout)=}\")\n     if name in ['add', 'multiply'] and dtype == bool:\n-      # TODO(jakevdp): figure out how to fix thest cases.\n+      # TODO(jakevdp): figure out how to fix test cases.\n       self.skipTest(f\"known failure for {name}.reduceat with {dtype=}\")\n \n     rng = jtu.rand_default(self.rng())\ndiff --git a/tests/lax_scipy_test.py b/tests/lax_scipy_test.py\nindex e0d3528dfa41..610bf5fabefd 100644\n--- a/tests/lax_scipy_test.py\n+++ b/tests/lax_scipy_test.py\n@@ -646,7 +646,7 @@ def test_spence(self, shape, dtype):\n     ],\n     dtype=float_dtypes + int_dtypes,\n   )\n-  @jtu.skip_on_devices(\"tpu\")  # TODO(jakevdp): fix and reenable this test.\n+  @jtu.skip_on_devices(\"tpu\")  # TODO(jakevdp): fix and re-enable this test.\n   @jax.numpy_rank_promotion('allow')  # This test explicitly exercises implicit rank promotion.\n   def testIntegrateTrapezoid(self, yshape, xshape, dtype, dx, axis):\n     rng = jtu.rand_default(self.rng())\ndiff --git a/tests/lax_test.py b/tests/lax_test.py\nindex a11c989fc9c5..4d30cd70ea70 100644\n--- a/tests/lax_test.py\n+++ b/tests/lax_test.py\n@@ -4395,7 +4395,7 @@ def _testOnComplexPlaneWorker(self, name, dtype, kind):\n     #\n     # In addition, the 1/3 middle parts of regions q1, q2, q3, q4,\n     # neg, pos are tested separately as these don't contain extremely\n-    # small or extremelly large values and functions on these regions\n+    # small or extremely large values and functions on these regions\n     # ought not to possess any incorrectness issues.\n \n     s0, s1 = size_re, size_im\ndiff --git a/tests/mosaic/gpu_dialect_test.py b/tests/mosaic/gpu_dialect_test.py\nindex af8b296fe536..5fc6bc8c703c 100644\n--- a/tests/mosaic/gpu_dialect_test.py\n+++ b/tests/mosaic/gpu_dialect_test.py\n@@ -711,7 +711,7 @@ def test_broadcast_in_dim_dim_transpose(self):\n \n     with self.assertRaisesRegex(\n         ir.MLIRError,\n-        r\"`broadcast_dimensions` attribute must be stricly increasing\",\n+        r\"`broadcast_dimensions` attribute must be strictly increasing\",\n     ):\n       self.module.operation.verify()\n \ndiff --git a/tests/pallas/mgpu_collective_matmul_test.py b/tests/pallas/mgpu_collective_matmul_test.py\nindex 3760c7ccddb7..b1b7e0ffd118 100644\n--- a/tests/pallas/mgpu_collective_matmul_test.py\n+++ b/tests/pallas/mgpu_collective_matmul_test.py\n@@ -90,7 +90,7 @@ def test_all_gather_lhs_matmul(\n     if n_shard != block_n:\n       self.skipTest(\"n_shard must be equal to block_n for now.\")\n     if n_shard % block_n:\n-      self.skipTest(\"n_shard must be divisble by block_n for now.\")\n+      self.skipTest(\"n_shard must be divisible by block_n for now.\")\n     if m_shard % block_m:\n       self.skipTest(\"m_shard must be divisible by block_m for now.\")\n \ndiff --git a/tests/pallas/tpu_fusible_matmul_test.py b/tests/pallas/tpu_fusible_matmul_test.py\nindex 4bde9b95483b..ae56d3db2f3a 100644\n--- a/tests/pallas/tpu_fusible_matmul_test.py\n+++ b/tests/pallas/tpu_fusible_matmul_test.py\n@@ -416,7 +416,7 @@ def matmul_slice_ref(x, y, b, i, j, k):\n \n   @parameterized.parameters('float32', 'bfloat16')\n   def test_matmul_input_concat_output(self, dtype):\n-    self.skipTest('select_n doesnt support more than 3 elements')\n+    self.skipTest('select_n does not support more than 3 elements')\n     # TODO(sharadmv): fix this test\n     k0, k1, k2, k3 = jax.random.split(jax.random.key(0), 4)\n     x = jax.random.normal(k0, (128, 128), dtype)\ndiff --git a/tests/pgle_test.py b/tests/pgle_test.py\nindex 7087bcad58bf..e03e5127d023 100644\n--- a/tests/pgle_test.py\n+++ b/tests/pgle_test.py\n@@ -157,7 +157,7 @@ def f(x):\n \n       with config.pgle_profiling_runs(2), config.enable_pgle(True):\n         # Run 1: Module should be compiled without FDO. Two modules are expected\n-        # One is the funtion f, the other one is multi slice module\n+        # One is the function f, the other one is multi slice module\n         with jtu.count_pjit_cpp_cache_miss() as cache_miss_count:\n           self.assertArraysEqual(f(x), expected)\n         self.assertEqual(cache_miss_count(), 2)\ndiff --git a/tests/scaled_matmul_stablehlo_test.py b/tests/scaled_matmul_stablehlo_test.py\nindex d2483966c984..fb5a7560d947 100644\n--- a/tests/scaled_matmul_stablehlo_test.py\n+++ b/tests/scaled_matmul_stablehlo_test.py\n@@ -280,9 +280,9 @@ def setUp(self):\n       self.skipTest(str(e))\n       return\n     if _dtypes.float8_e8m0fnu is None:\n-      self.skipTest(\"Requries >= ml_dtypes 0.5.0 to support float8_e8m0fnu\")\n+      self.skipTest(\"Requires >= ml_dtypes 0.5.0 to support float8_e8m0fnu\")\n     if _dtypes.float4_e2m1fn is None:\n-      self.skipTest(\"Requries >= ml_dtypes 0.5.0 to support float4_e2m1fn\")\n+      self.skipTest(\"Requires >= ml_dtypes 0.5.0 to support float4_e2m1fn\")\n     if cudnn_version < 90700:\n       self.skipTest(\"Requires >= cuDNN 9.7.0\")\n     if not jtu.is_cuda_compute_capability_at_least(\"10.0\"):\n@@ -473,7 +473,7 @@ def setUp(self):\n       self.skipTest(str(e))\n       return\n     if _dtypes.float8_e8m0fnu is None:\n-      self.skipTest(\"Requries >= ml_dtypes 0.5.0 to support float8_e8m0fnu\")\n+      self.skipTest(\"Requires >= ml_dtypes 0.5.0 to support float8_e8m0fnu\")\n     if cudnn_version < 90700:\n       self.skipTest(\"Requires >= cuDNN 9.7.0\")\n     if not jtu.is_cuda_compute_capability_at_least(\"10.0\"):\ndiff --git a/tests/shape_poly_test.py b/tests/shape_poly_test.py\nindex 6d1ffe744ed9..68aaf4e29553 100644\n--- a/tests/shape_poly_test.py\n+++ b/tests/shape_poly_test.py\n@@ -961,7 +961,7 @@ def test_constraints_ge_complex_gen(self,\n     self.assertEqual(bounds, _bounds(exp))\n \n   def test_constraints_ge_override(self):\n-    # Some constaints override other\n+    # Some constraints override other\n     a, b = shape_poly.symbolic_shape(\"a, b\",\n                                      constraints=(\"a >= 5\", \"b <= 16\",\n                                                   \"a >= 10\", \"b <= 10\"))\n@@ -979,7 +979,7 @@ def test_constraint_eq_0(self):\n     self.assertIs(d, 5)\n \n   def test_constraints_eq_1(self):\n-    # Some constaints override other\n+    # Some constraints override other\n     a, b, c = shape_poly.symbolic_shape(\"a, b, c\",\n                                         constraints=(\"max(a, b) == c\",))\n     self.assertEqual(_bounds(core.max_dim(a, b) - c + 3), (3, 3))\ndiff --git a/third_party/repo.bzl b/third_party/repo.bzl\nindex 17e0bbb03542..185c5a4294dc 100644\n--- a/third_party/repo.bzl\n+++ b/third_party/repo.bzl\n@@ -129,7 +129,7 @@ def tf_http_archive(name, sha256, urls, **kwargs):\n         \"storage.googleapis.com\",\n     )]):\n         fail(\"The first entry of tf_http_archive(urls) must be a mirror \" +\n-             \"URL, preferrably mirror.tensorflow.org. Even if you don't have \" +\n+             \"URL, preferably mirror.tensorflow.org. Even if you don't have \" +\n              \"permission to mirror the file, please put the correctly \" +\n              \"formatted mirror URL there anyway, because someone will come \" +\n              \"along shortly thereafter and mirror the file.\")\n\n```",
        "from_id": [
            "cclauss"
        ]
    },
    {
        "text_input": "[jax::compiler] Bind `compiler.backend_compile` to `xla::PyClient::Compile`.\n\nCurrently, we just forward any calls to `compiler.backend_compile_and_load`, which returns an `xla::PyLoadedExecutable` whereas we'd like `compiler.backend_compile` to return an unloaded `xla::PyExecutable`.\n\nPiperOrigin-RevId: 767142396",
        "output": "```diff\nCommit: bf635d87b69d9e27df34289cea2571ad4a67deda\nDate: 2025-06-04T14:50:08Z\nURL: https://github.com/jax-ml/jax/commit/bf635d87b69d9e27df34289cea2571ad4a67deda\nFiles changed: 1\nAdditions: +31, Deletions: -8\ndiff --git a/jax/_src/compiler.py b/jax/_src/compiler.py\nindex 4f805034e99c..f501454b73be 100644\n--- a/jax/_src/compiler.py\n+++ b/jax/_src/compiler.py\n@@ -292,11 +292,35 @@ def backend_compile(\n     module: ir.Module,\n     executable_devices: xc.DeviceList,\n     options: xc.CompileOptions,\n-    host_callbacks: Sequence[Any],\n-) -> xc.LoadedExecutable:\n-  return backend_compile_and_load(\n-      backend, module, executable_devices, options, host_callbacks\n-  )\n+) -> xc.Executable:\n+  sym_name = module.operation.attributes['sym_name']\n+  module_name = ir.StringAttr(sym_name).value\n+  # Convert ir.Module to a string representation, unless the backend\n+  # explicitly flags the ability to handle a module directly (avoiding the\n+  # overhead of back and forth conversions).\n+  # TODO(slebedev): Change the backend.compile() to accept ir.Module.\n+  built_c: Any\n+  if getattr(backend, \"needs_str_ir\", True):\n+    built_c = mlir.module_to_bytecode(module)\n+  else:\n+    built_c = module\n+\n+  if (options.executable_build_options.fdo_profile is not None\n+      and len(options.executable_build_options.fdo_profile)):\n+    logger.debug(\n+        \"Compiling module %s with FDO profile of length %d\",\n+        module_name,\n+        len(options.executable_build_options.fdo_profile),\n+    )\n+\n+  try:\n+    return backend.compile(built_c, executable_devices, options)\n+  except xc.XlaRuntimeError as e:\n+    for error_handler in _XLA_RUNTIME_ERROR_HANDLERS:\n+      handler_result = error_handler(e)\n+      if handler_result is not None:\n+        raise handler_result from e\n+    raise e\n \n \n @profiler.annotate_function\n@@ -330,8 +354,7 @@ def backend_compile_and_load(\n   try:\n     # we use a separate function call to ensure that XLA compilation appears\n     # separately in Python profiling results\n-    # TODO(dsuo): Simplify this logic once backend_compile actually returns an\n-    # unloaded executable.\n+    # TODO(dsuo): Simplify this logic once we delete _jax.CompileOnlyPyClient.\n     if jaxlib_extension_version < 345 or (\n         jaxlib_extension_version >= 345\n         and isinstance(backend, _jax.CompileOnlyPyClient)\n@@ -341,7 +364,7 @@ def backend_compile_and_load(\n             built_c,\n             executable_devices=executable_devices,  # type: ignore\n             compile_options=options,\n-            host_callbacks=host_callbacks,\n+            host_callbacks=host_callbacks,  # type: ignore\n         )\n       # Some backends don't have `host_callbacks` option yet\n       # TODO(sharadmv): remove this fallback when all backends allow `compile`\n\n```",
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[jaxlib] Bind 'compile' to `xla::PyClient::Compile` rather than `xla::PyClient::CompileAndLoad`.\n\n- Remove redundant `xla::PyClient` `compile` bindings.\n- Remove host_callback arguments to `compile`.\n\nPiperOrigin-RevId: 767135320",
        "output": "```diff\nCommit: b9658ed8af4f239661448172ba3e0afe38849595\nDate: 2025-06-04T14:27:57Z\nURL: https://github.com/jax-ml/jax/commit/b9658ed8af4f239661448172ba3e0afe38849595\nFiles changed: 5\nAdditions: +15, Deletions: -90\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 67dc9ffc6001..26125d7edcbc 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -511,8 +511,7 @@ class Client:\n       computation: str | bytes,\n       executable_devices: DeviceList | Sequence[Device],\n       compile_options: CompileOptions = ...,\n-      host_callbacks: Sequence[Any] = ...,\n-  ) -> LoadedExecutable: ...\n+  ) -> Executable: ...\n   def compile_and_load(\n       self,\n       computation: str | bytes,\n@@ -560,8 +559,7 @@ class CompileOnlyPyClient(Client):\n       computation: str | bytes,\n       executable_devices: DeviceList | Sequence[Device],\n       compile_options: CompileOptions = ...,\n-      host_callbacks: Sequence[Any] = ...,\n-  ) -> LoadedExecutable: ...\n+  ) -> Executable: ...\n \n \n class CpuCollectives: ...\ndiff --git a/jaxlib/py_client.cc b/jaxlib/py_client.cc\nindex b663c00da35c..0ddadbc3038a 100644\n--- a/jaxlib/py_client.cc\n+++ b/jaxlib/py_client.cc\n@@ -818,93 +818,25 @@ PyType_Slot PyClient::slots_[] = {\n       .def(\n           \"compile\",\n           [](nb_class_ptr<PyClient> client, nb::bytes mlir_module,\n-             jax::PyDeviceList& py_executable_devices, CompileOptions options,\n-             std::vector<nb::capsule> host_callbacks) {\n-            ifrt::DeviceListRef executable_devices =\n-                ValueOrThrow(py_executable_devices.ifrt_device_list());\n-            return ValueOrThrow(PyClient::CompileAndLoad(\n-                std::move(client),\n-                std::string(mlir_module.c_str(), mlir_module.size()),\n-                std::move(executable_devices), std::move(options),\n-                std::move(host_callbacks)));\n-          },\n-          nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n-          nb::arg(\"compile_options\") = CompileOptions(),\n-          nb::arg(\"host_callbacks\") = std::vector<nb::capsule>())\n-      .def(\n-          \"compile\",\n-          [](nb_class_ptr<PyClient> client, nb::bytes mlir_module,\n-             jax::PyDeviceList& py_executable_devices, CompileOptions options,\n-             std::vector<nb::callable> host_callbacks) {\n-            ifrt::DeviceListRef executable_devices =\n-                ValueOrThrow(py_executable_devices.ifrt_device_list());\n-            return ValueOrThrow(PyClient::CompileAndLoad(\n-                std::move(client),\n-                std::string(mlir_module.c_str(), mlir_module.size()),\n-                std::move(executable_devices), std::move(options),\n-                std::move(host_callbacks)));\n-          },\n-          nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n-          nb::arg(\"compile_options\") = CompileOptions(),\n-          nb::arg(\"host_callbacks\") = std::vector<nb::callable>())\n-      .def(\n-          \"compile\",\n-          [](nb_class_ptr<PyClient> client, std::string mlir_module,\n-             jax::PyDeviceList& py_executable_devices, CompileOptions options,\n-             std::vector<nb::capsule> host_callbacks) {\n-            ifrt::DeviceListRef executable_devices =\n-                ValueOrThrow(py_executable_devices.ifrt_device_list());\n-            return ValueOrThrow(PyClient::CompileAndLoad(\n-                std::move(client), std::move(mlir_module),\n-                std::move(executable_devices), std::move(options),\n-                std::move(host_callbacks)));\n-          },\n-          nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n-          nb::arg(\"compile_options\") = CompileOptions(),\n-          nb::arg(\"host_callbacks\") = std::vector<nb::capsule>())\n-      .def(\n-          \"compile\",\n-          [](nb_class_ptr<PyClient> client, std::string mlir_module,\n-             jax::PyDeviceList& py_executable_devices, CompileOptions options,\n-             std::vector<nb::callable> host_callbacks) {\n+             jax::PyDeviceList& py_executable_devices, CompileOptions options) {\n             ifrt::DeviceListRef executable_devices =\n                 ValueOrThrow(py_executable_devices.ifrt_device_list());\n-            return ValueOrThrow(PyClient::CompileAndLoad(\n-                std::move(client), std::move(mlir_module),\n-                std::move(executable_devices), std::move(options),\n-                std::move(host_callbacks)));\n-          },\n-          nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n-          nb::arg(\"compile_options\") = CompileOptions(),\n-          nb::arg(\"host_callbacks\") = std::vector<nb::callable>())\n-      // The following two overloads are for users of deprecated APIs who call\n-      // `backend.compile` but do not have visibility to `DeviceList`.\n-      .def(\n-          \"compile\",\n-          [](nb_class_ptr<PyClient> client, nb::bytes mlir_module,\n-             nb::sequence& py_executable_devices, CompileOptions options) {\n-            ifrt::DeviceListRef executable_devices =\n-                ValueOrThrow(jax::PyDeviceList(nb::tuple(py_executable_devices))\n-                                 .ifrt_device_list());\n-            return ValueOrThrow(PyClient::CompileAndLoad(\n+            return ValueOrThrow(PyClient::Compile(\n                 std::move(client),\n                 std::string(mlir_module.c_str(), mlir_module.size()),\n-                std::move(executable_devices), std::move(options),\n-                std::vector<nb::capsule>()));\n+                std::move(executable_devices), std::move(options)));\n           },\n           nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n           nb::arg(\"compile_options\") = CompileOptions())\n       .def(\n           \"compile\",\n           [](nb_class_ptr<PyClient> client, std::string mlir_module,\n-             nb::sequence& py_executable_devices, CompileOptions options) {\n+             jax::PyDeviceList& py_executable_devices, CompileOptions options) {\n             ifrt::DeviceListRef executable_devices =\n-                ValueOrThrow(jax::PyDeviceList(nb::tuple(py_executable_devices))\n-                                 .ifrt_device_list());\n-            return ValueOrThrow(PyClient::CompileAndLoad(\n+                ValueOrThrow(py_executable_devices.ifrt_device_list());\n+            return ValueOrThrow(PyClient::Compile(\n                 std::move(client), std::move(mlir_module),\n-                std::move(executable_devices), std::move(options),\n-                std::vector<nb::capsule>()));\n+                std::move(executable_devices), std::move(options)));\n           },\n           nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n           nb::arg(\"compile_options\") = CompileOptions())\ndiff --git a/jaxlib/py_compile_only_client.cc b/jaxlib/py_compile_only_client.cc\nindex bcae15cd6438..49f68ec1e24f 100644\n--- a/jaxlib/py_compile_only_client.cc\n+++ b/jaxlib/py_compile_only_client.cc\n@@ -73,12 +73,7 @@ class CompileOnlyPyClient : public PyClient {\n \n   absl::StatusOr<nb_class_ptr<PyExecutable>> CompileUnloaded(\n       absl::string_view mlir_module, ifrt::DeviceListRef executable_devices,\n-      CompileOptions options, std::vector<nb::capsule> host_callbacks) {\n-    if (!host_callbacks.empty()) {\n-      return Unimplemented(\n-          \"Compiling with host_callbacks not available with compile-only \"\n-          \"client.\");\n-    }\n+      CompileOptions options) {\n     ifrt::ExecutableRef ifrt_executable;\n     {\n       nb::gil_scoped_release gil_release;\n@@ -125,8 +120,7 @@ void RegisterCompileOnlyClient(nb::module_& m) {\n                 ValueOrThrow(py_executable_devices.ifrt_device_list());\n             return ValueOrThrow(self.CompileUnloaded(\n                 absl::string_view(mlir_module.c_str(), mlir_module.size()),\n-                std::move(executable_devices), std::move(options),\n-                std::move(host_callbacks)));\n+                std::move(executable_devices), std::move(options)));\n           },\n           nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n           nb::arg(\"compile_options\") = CompileOptions(),\n@@ -134,8 +128,7 @@ void RegisterCompileOnlyClient(nb::module_& m) {\n       .def(\"compile\",\n            ValueOrThrowWrapper(&CompileOnlyPyClient::CompileUnloaded),\n            nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n-           nb::arg(\"compile_options\") = CompileOptions(),\n-           nb::arg(\"host_callbacks\") = std::vector<nb::capsule>());\n+           nb::arg(\"compile_options\") = CompileOptions());\n }\n \n }  // namespace xla\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 85de5f947e49..a098bd4baf3e 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 347\n+_version = 348\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\n@@ -308,6 +308,7 @@ def computation_count():\n Array = _xla.Array\n ArrayImpl = _xla.ArrayImpl\n LoadedExecutable = _xla.LoadedExecutable\n+Executable = _xla.Executable\n DeviceList = _xla.DeviceList\n OpSharding = _xla.OpSharding\n HloSharding = _xla.HloSharding\ndiff --git a/jaxlib/xla_client.pyi b/jaxlib/xla_client.pyi\nindex 80599e86676b..72e85500d1fe 100644\n--- a/jaxlib/xla_client.pyi\n+++ b/jaxlib/xla_client.pyi\n@@ -37,6 +37,7 @@ from jaxlib._jax import HostBufferSemantics as HostBufferSemantics\n from jaxlib._jax import ifrt_programs as ifrt_programs\n from jaxlib._jax import Layout as Layout\n from jaxlib._jax import LoadedExecutable as LoadedExecutable\n+from jaxlib._jax import Executable as Executable\n from jaxlib._jax import Memory as Memory\n from jaxlib._jax import NamedSharding as NamedSharding\n from jaxlib._jax import OpSharding as OpSharding\n\n```",
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Raise `NotImplementedError` instead of `ValueError` when using Shardy without sharding rule.\n\nPiperOrigin-RevId: 767131346",
        "output": "```diff\nCommit: 6e75a04eafe67b3982a7bfa63045a491bede1a42\nDate: 2025-06-04T14:13:32Z\nURL: https://github.com/jax-ml/jax/commit/6e75a04eafe67b3982a7bfa63045a491bede1a42\nFiles changed: 2\nAdditions: +7, Deletions: -5\ndiff --git a/jax/_src/custom_partitioning.py b/jax/_src/custom_partitioning.py\nindex feb1e0c39cc6..0d0411f5177a 100644\n--- a/jax/_src/custom_partitioning.py\n+++ b/jax/_src/custom_partitioning.py\n@@ -503,9 +503,11 @@ def __call__(self, *args, **kwargs):\n       if (self.sharding_rule is None and\n           (self.propagate_user_sharding is not None or\n             self.infer_sharding_from_operands is not None)):\n-        raise ValueError(\"Shardy is used, but sharding propagation callbacks \"\n-                         \"instead of sharding_rule are provided. Need to \"\n-                         \"provide sharding_rule to migrate to Shardy.\")\n+        raise NotImplementedError(\n+            \"Shardy is used, but sharding propagation callbacks instead of \"\n+            \"sharding_rule are provided. Need to provide sharding_rule to \"\n+            \"migrate to Shardy.\"\n+        )\n       sharding_rule = self.sharding_rule\n     else:\n       propagate_user_sharding = self.propagate_user_sharding\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex fa68a441c4c0..04df138e68e2 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -8812,8 +8812,8 @@ def f(x):\n     mesh = jtu.create_mesh((4, 2), ('x', 'y'))\n     x = jax.device_put(np.arange(32 * 16).reshape(32, 16),\n                        NamedSharding(mesh, P(None, 'x')))\n-    with self.assertRaisesRegex(ValueError, \"provide sharding_rule to migrate \"\n-                                \"to Shardy\"):\n+    with self.assertRaisesRegex(\n+        NotImplementedError, 'provide sharding_rule to migrate to Shardy'):\n       jax.jit(f)(x)\n \n   def test_reshard_empty_mesh_error(self):\n\n```",
        "from_id": [
            "ZixuanJiang",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Use the `mosaic_gpu.sliceSMEM` MLIR op when using WG semantics.\n\nPiperOrigin-RevId: 767125470",
        "output": "```diff\nCommit: b21861712f6853a5ee1bf8eeaae1336104a4eee8\nDate: 2025-06-04T13:55:22Z\nURL: https://github.com/jax-ml/jax/commit/b21861712f6853a5ee1bf8eeaae1336104a4eee8\nFiles changed: 1\nAdditions: +28, Deletions: -10\ndiff --git a/jax/experimental/mosaic/gpu/core.py b/jax/experimental/mosaic/gpu/core.py\nindex 79a0cd56328b..435e6976a5b1 100644\n--- a/jax/experimental/mosaic/gpu/core.py\n+++ b/jax/experimental/mosaic/gpu/core.py\n@@ -316,6 +316,19 @@ def dealloc(self):\n     )\n \n \n+def _slice_smem(\n+    result: ir.Type,\n+    smem_base: ir.Value,\n+    offset: ir.Value,  # This should be an ir.IndexType.\n+    lowering_semantics: LoweringSemantics,\n+) -> ir.Value:\n+  if lowering_semantics == LoweringSemantics.Warpgroup:\n+    offset = arith.index_cast(ir.IntegerType.get_signless(32), offset)\n+    return dialect.slice_smem(result, offset)\n+  else:\n+    return memref.view(result, smem_base, offset, [])\n+\n+\n def _construct_smem_reftree(\n     cluster_shape: tuple[int, int, int],\n     dynamic_smem: ir.Value,\n@@ -392,9 +405,11 @@ def ref(member_thunks=member_thunks):\n             cluster_shape,\n         )\n       case TMEM(shape, dtype, layout=layout, collective=collective, packing=packing):\n-        addr_ref = memref.view(\n+        addr_ref = _slice_smem(\n             ir.MemRefType.get([], i32, memory_space=smem),\n-            dynamic_smem, c(dynamic_smem_offset, index), [],\n+            dynamic_smem,\n+            c(dynamic_smem_offset, index),\n+            lowering_semantics,\n         )\n         if layout is None:\n           layout = tcgen05._infer_tmem_layout(\n@@ -410,9 +425,11 @@ def ref(addr_ref=addr_ref, shape=shape, dtype=dtype, layout=layout):\n         dynamic_smem_offset += 4  # i32 takes up 4 bytes\n       case _:\n         mlir_dtype = utils.dtype_to_ir_type(ref_ty.dtype)\n-        tile_smem = memref.view(\n+        tile_smem = _slice_smem(\n             ir.MemRefType.get(ref_ty.shape, mlir_dtype, memory_space=smem),\n-            dynamic_smem, c(dynamic_smem_offset, index), [],\n+            dynamic_smem,\n+            c(dynamic_smem_offset, index),\n+            lowering_semantics,\n         )\n         dynamic_smem_offset += _count_buffer_bytes(ref_ty)\n         ref = tile_smem\n@@ -510,18 +527,19 @@ def _launch(\n   smem = ir.Attribute.parse(\"#gpu.address_space<workgroup>\")\n   with ir.InsertionPoint(launch_op.body.blocks[0]):\n     dynamic_smem = gpu.dynamic_shared_memory(\n-        ir.MemRefType.get(\n-            (ir.ShapedType.get_dynamic_size(),), i8, memory_space=smem\n-        )\n+        ir.MemRefType.get((utils.DYNAMIC,), i8, memory_space=smem)\n     )\n \n     if profiler_spec:\n-      prof_smem = memref.view(\n+      prof_smem = _slice_smem(\n           ir.MemRefType.get(\n               (profiler_spec.smem_i32_elements(block=block),),\n-              i32, memory_space=smem,\n+              i32,\n+              memory_space=smem,\n           ),\n-          dynamic_smem, c(profiler_start, index), [],\n+          dynamic_smem,\n+          c(profiler_start, index),\n+          lowering_semantics,\n       )\n       prof = profiler.OnDeviceProfiler(\n           profiler_spec, prof_smem, maybe_prof_buffer\n\n```",
        "from_id": [
            "dimitar-asenov",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "#sdy Have JAX export compat tests also run on Shardy.\n\nThe lowering b/w Shardy and GSPMD is slightly different with the custom calls, so I needed to choose different test data based on whether or not Shardy was enabled.\n\nPiperOrigin-RevId: 767074094",
        "output": "```diff\nCommit: d2d6211ff095c86eaaa02aba2dc82ef00f5e16a4\nDate: 2025-06-04T10:52:50Z\nURL: https://github.com/jax-ml/jax/commit/d2d6211ff095c86eaaa02aba2dc82ef00f5e16a4\nFiles changed: 4\nAdditions: +121, Deletions: -11\ndiff --git a/jax/_src/internal_test_util/export_back_compat_test_data/annotate_data_placement.py b/jax/_src/internal_test_util/export_back_compat_test_data/annotate_data_placement.py\nindex bf70df2cdb3a..d3aab292c9fe 100644\n--- a/jax/_src/internal_test_util/export_back_compat_test_data/annotate_data_placement.py\n+++ b/jax/_src/internal_test_util/export_back_compat_test_data/annotate_data_placement.py\n@@ -15,11 +15,12 @@\n # ruff: noqa\n \n import datetime\n-from numpy import array, float32, int32\n+from numpy import array, float32\n \n+data_2025_04_07_tpu = {}\n \n # Pasted from the test output (see export_back_compat_test_util.py module docstring)\n-data_2025_04_07_tpu = dict(\n+data_2025_04_07_tpu['gspmd'] = dict(\n     testdata_version=1,\n     platform='tpu',\n     custom_call_targets=['annotate_device_placement'],\n@@ -46,7 +47,38 @@\n )  # End paste\n \n # Pasted from the test output (see export_back_compat_test_util.py module docstring)\n-data_2025_04_07_cuda = dict(\n+data_2025_04_07_tpu['shardy'] = dict(\n+    testdata_version=1,\n+    platform='tpu',\n+    custom_call_targets=['annotate_device_placement', 'xla.sdy.FuncResultSharding'],\n+    serialized_date=datetime.date(2025, 5, 28),\n+    inputs=(array([0.], dtype=float32), array([0.], dtype=float32)),\n+    expected_outputs=(array([0.], dtype=float32),),\n+    mlir_module_text=r\"\"\"\n+#loc1 = loc(\"x\")\n+#loc2 = loc(\"y\")\n+module @jit_func attributes {jax.uses_shape_polymorphism = false, mhlo.frontend_attributes = {xla.sdy.meshes = \"{mesh = #sdy.mesh<[\\22a\\22=1]>}\"}, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n+  func.func public @main(%arg0: tensor<1xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding<@mesh, [{\\22a\\22}]>\"}, mhlo.memory_kind = \"device\", mhlo.sharding = \"{devices=[1]<=[1]}\"} loc(\"x\"), %arg1: tensor<1xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding<@mesh, [{\\22a\\22}]>\"}, mhlo.memory_kind = \"pinned_host\", mhlo.sharding = \"{devices=[1]<=[1]}\"} loc(\"y\")) -> (tensor<1xf32> {jax.result_info = \"result\", mhlo.memory_kind = \"pinned_host\", mhlo.sharding = \"{devices=[1]<=[1]}\"}) {\n+    %0 = stablehlo.add %arg0, %arg1 : tensor<1xf32> loc(#loc4)\n+    %1 = stablehlo.custom_call @annotate_device_placement(%0) {has_side_effect = true, mhlo.frontend_attributes = {_xla_buffer_placement = \"pinned_host\"}} : (tensor<1xf32>) -> tensor<1xf32> loc(#loc)\n+    %2 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%1) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding_per_value<[<@mesh, [{\\22a\\22}]>]>\"}} : (tensor<1xf32>) -> tensor<1xf32> loc(#loc)\n+    return %2 : tensor<1xf32> loc(#loc)\n+  } loc(#loc)\n+} loc(#loc)\n+#loc = loc(unknown)\n+#loc3 = loc(\"third_party/py/jax/tests/export_back_compat_test.py\":801:13)\n+#loc4 = loc(\"jit(func)/jit(main)/add\"(#loc3))\n+\"\"\",\n+    mlir_module_serialized=b'ML\\xefR\\rStableHLO_v1.10.3\\x00\\x01\\x1b\\x05\\x01\\x05\\x0b\\x01\\x03\\x0b\\x03\\t\\x0f\\x13\\x17\\x1b\\x03\\x85g\\x0b\\x01-\\x07\\x0b\\x0f+\\x0b\\x0f\\x13\\x0b\\x0b\\x0b\\x0b\\x0b\\x0f\\x0b\\x0f\\x0b\\x0f\\x0b\\x17\\x0b\\x13\\x13\\x03;\\x0b\\x0b\\x0b\\x0b\\x0b\\x0b\\x13\\x0b\\x0b\\x0b\\x0b\\x13#\\x0b\\x0b#\\x0b\\x0f#\\x0b\\x0b\\x0b\\x0b\\x13\\x0b\\x0b\\x13\\x0b\\x0b\\x01\\x05\\x0b\\x0f\\x03\\x07\\x13\\x1b\\x07\\x02\\x9a\\x02\\x1f\\x05\\x0f\\x11\\x03\\x05\\x03\\t\\t\\x0b\\x03\\r\\x13\\x05\\x15\\x05\\x05\\x11\\x11\\x01\\x00\\x03\\x03\\x0f\\x11\\x05\\x13\\x05\\x15\\x05\\x17\\x05\\x19\\x05\\x1b\\x1d\\x1b\\x01\\x05\\x1d\\x1d\\x1f\\x01\\x05\\x1f\\x1d#%\\x05!\\x17\\'\\x86\\x0c\\x1b\\x05#\\x03\\x03\\x03[\\x03\\x03\\x03a\\x03\\x01\\x1d%\\x1d\\'\\x1d)\\x1d+\\x1d\\x0f\\r\\x03;G\\x1d-\\x0b\\x03\\x1d/\\x05\\x03\\x03\\x05EK\\r\\x0779/I13\\x1d1\\x1d3\\r\\x0779/513#\\x07\\x03\\x03Q\\r\\x07SU/513\\x1d5\\x1d7\\x1d9\\x1d;\\r\\x03]5\\x1d=\\x1d?\\r\\x03;c\\x1dA\\x1dC\\x01\\t\\x01\\x02\\x02)\\x03\\x05\\t\\x11\\x05\\x05\\x05\\x03\\x05\\t\\x04w\\x05\\x01Q\\x01\\x07\\x01\\x07\\x04e\\x03\\x01\\x05\\x05P\\x01\\x03\\x07\\x04Q\\x03\\x0b\\x13\\x05\\x0b\\x19\\x0b\\x1d\\x00\\x07\\x06!\\x03\\x05\\x05\\x01\\x03\\x03G\\x01)\\x05\\x03\\x05\\x03\\x05\\x03G\\x01+\\x07\\x03\\x05\\x03\\x07\\t\\x04\\x01\\x03\\t\\x06\\x03\\x01\\x05\\x01\\x006\\tE7Y5-\\x0f\\x0b\\x0f!\\x0f=\\x03#\\x19\\'\\x1d#i1\\x05\\x05\\x13%)9\\x1f93\\x15\\x0f\\x11\\x1f\\x0f\\x0b\\x11builtin\\x00vhlo\\x00module\\x00custom_call_v1\\x00func_v1\\x00add_v1\\x00return_v1\\x00mhlo.frontend_attributes\\x00jax.uses_shape_polymorphism\\x00xla.sdy.meshes\\x00{mesh = #sdy.mesh<[\"a\"=1]>}\\x00mhlo.num_partitions\\x00mhlo.num_replicas\\x00jit_func\\x00x\\x00y\\x00jit(func)/jit(main)/add\\x00third_party/py/jax/tests/export_back_compat_test.py\\x00mhlo.memory_kind\\x00mhlo.sharding\\x00{devices=[1]<=[1]}\\x00pinned_host\\x00xla.sdy.sharding\\x00\\x00#sdy.sharding<@mesh, [{\"a\"}]>\\x00device\\x00jax.result_info\\x00result\\x00main\\x00public\\x00_xla_buffer_placement\\x00annotate_device_placement\\x00#sdy.sharding_per_value<[<@mesh, [{\"a\"}]>]>\\x00xla.sdy.FuncResultSharding\\x00\\x089\\t\\x05/\\x01\\x0bCMOWY\\x11=?_-A---\\x11=?e-A---',\n+    xla_call_module_version=9,\n+    nr_devices=1,\n+)  # End paste\n+\n+\n+data_2025_04_07_cuda = {}\n+\n+# Pasted from the test output (see export_back_compat_test_util.py module docstring)\n+data_2025_04_07_cuda['gspmd'] = dict(\n     testdata_version=1,\n     platform='cuda',\n     custom_call_targets=['annotate_device_placement'],\n@@ -71,3 +103,32 @@\n     xla_call_module_version=9,\n     nr_devices=1,\n )  # End paste\n+\n+\n+# Pasted from the test output (see export_back_compat_test_util.py module docstring)\n+data_2025_04_07_cuda['shardy'] = dict(\n+    testdata_version=1,\n+    platform='cuda',\n+    custom_call_targets=['annotate_device_placement', 'xla.sdy.FuncResultSharding'],\n+    serialized_date=datetime.date(2025, 5, 28),\n+    inputs=(array([0.], dtype=float32), array([0.], dtype=float32)),\n+    expected_outputs=(array([0.], dtype=float32),),\n+    mlir_module_text=r\"\"\"\n+#loc1 = loc(\"x\")\n+#loc2 = loc(\"y\")\n+module @jit_func attributes {jax.uses_shape_polymorphism = false, mhlo.frontend_attributes = {xla.sdy.meshes = \"{mesh = #sdy.mesh<[\\22a\\22=1]>}\"}, mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n+  func.func public @main(%arg0: tensor<1xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding<@mesh, [{\\22a\\22}]>\"}, mhlo.memory_kind = \"device\", mhlo.sharding = \"{devices=[1]<=[1]}\"} loc(\"x\"), %arg1: tensor<1xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding<@mesh, [{\\22a\\22}]>\"}, mhlo.memory_kind = \"pinned_host\", mhlo.sharding = \"{devices=[1]<=[1]}\"} loc(\"y\")) -> (tensor<1xf32> {jax.result_info = \"result\", mhlo.memory_kind = \"pinned_host\", mhlo.sharding = \"{devices=[1]<=[1]}\"}) {\n+    %0 = stablehlo.add %arg0, %arg1 : tensor<1xf32> loc(#loc4)\n+    %1 = stablehlo.custom_call @annotate_device_placement(%0) {has_side_effect = true, mhlo.frontend_attributes = {_xla_buffer_placement = \"pinned_host\"}} : (tensor<1xf32>) -> tensor<1xf32> loc(#loc)\n+    %2 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%1) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding_per_value<[<@mesh, [{\\22a\\22}]>]>\"}} : (tensor<1xf32>) -> tensor<1xf32> loc(#loc)\n+    return %2 : tensor<1xf32> loc(#loc)\n+  } loc(#loc)\n+} loc(#loc)\n+#loc = loc(unknown)\n+#loc3 = loc(\"third_party/py/jax/tests/export_back_compat_test.py\":806:13)\n+#loc4 = loc(\"jit(func)/jit(main)/add\"(#loc3))\n+\"\"\",\n+    mlir_module_serialized=b'ML\\xefR\\rStableHLO_v1.10.3\\x00\\x01\\x1b\\x05\\x01\\x05\\x0b\\x01\\x03\\x0b\\x03\\t\\x0f\\x13\\x17\\x1b\\x03\\x85g\\x0b\\x01-\\x07\\x0b\\x0f+\\x0b\\x0f\\x13\\x0b\\x0b\\x0b\\x0b\\x0b\\x0f\\x0b\\x0f\\x0b\\x0f\\x0b\\x17\\x0b\\x13\\x13\\x03;\\x0b\\x0b\\x0b\\x0b\\x0b\\x0b\\x13\\x0b\\x0b\\x0b\\x0b\\x13#\\x0b\\x0b#\\x0b\\x0f#\\x0b\\x0b\\x0b\\x0b\\x13\\x0b\\x0b\\x13\\x0b\\x0b\\x01\\x05\\x0b\\x0f\\x03\\x07\\x13\\x1b\\x07\\x02\\x9a\\x02\\x1f\\x05\\x0f\\x11\\x03\\x05\\x03\\t\\t\\x0b\\x03\\r\\x13\\x05\\x15\\x05\\x05\\x11\\x11\\x01\\x00\\x03\\x03\\x0f\\x11\\x05\\x13\\x05\\x15\\x05\\x17\\x05\\x19\\x05\\x1b\\x1d\\x1b\\x01\\x05\\x1d\\x1d\\x1f\\x01\\x05\\x1f\\x1d#%\\x05!\\x17\\'\\x9a\\x0c\\x1b\\x05#\\x03\\x03\\x03[\\x03\\x03\\x03a\\x03\\x01\\x1d%\\x1d\\'\\x1d)\\x1d+\\x1d\\x0f\\r\\x03;G\\x1d-\\x0b\\x03\\x1d/\\x05\\x03\\x03\\x05EK\\r\\x0779/I13\\x1d1\\x1d3\\r\\x0779/513#\\x07\\x03\\x03Q\\r\\x07SU/513\\x1d5\\x1d7\\x1d9\\x1d;\\r\\x03]5\\x1d=\\x1d?\\r\\x03;c\\x1dA\\x1dC\\x01\\t\\x01\\x02\\x02)\\x03\\x05\\t\\x11\\x05\\x05\\x05\\x03\\x05\\t\\x04w\\x05\\x01Q\\x01\\x07\\x01\\x07\\x04e\\x03\\x01\\x05\\x05P\\x01\\x03\\x07\\x04Q\\x03\\x0b\\x13\\x05\\x0b\\x19\\x0b\\x1d\\x00\\x07\\x06!\\x03\\x05\\x05\\x01\\x03\\x03G\\x01)\\x05\\x03\\x05\\x03\\x05\\x03G\\x01+\\x07\\x03\\x05\\x03\\x07\\t\\x04\\x01\\x03\\t\\x06\\x03\\x01\\x05\\x01\\x006\\tE7Y5-\\x0f\\x0b\\x0f!\\x0f=\\x03#\\x19\\'\\x1d#i1\\x05\\x05\\x13%)9\\x1f93\\x15\\x0f\\x11\\x1f\\x0f\\x0b\\x11builtin\\x00vhlo\\x00module\\x00custom_call_v1\\x00func_v1\\x00add_v1\\x00return_v1\\x00mhlo.frontend_attributes\\x00jax.uses_shape_polymorphism\\x00xla.sdy.meshes\\x00{mesh = #sdy.mesh<[\"a\"=1]>}\\x00mhlo.num_partitions\\x00mhlo.num_replicas\\x00jit_func\\x00x\\x00y\\x00jit(func)/jit(main)/add\\x00third_party/py/jax/tests/export_back_compat_test.py\\x00mhlo.memory_kind\\x00mhlo.sharding\\x00{devices=[1]<=[1]}\\x00pinned_host\\x00xla.sdy.sharding\\x00\\x00#sdy.sharding<@mesh, [{\"a\"}]>\\x00device\\x00jax.result_info\\x00result\\x00main\\x00public\\x00_xla_buffer_placement\\x00annotate_device_placement\\x00#sdy.sharding_per_value<[<@mesh, [{\"a\"}]>]>\\x00xla.sdy.FuncResultSharding\\x00\\x089\\t\\x05/\\x01\\x0bCMOWY\\x11=?_-A---\\x11=?e-A---',\n+    xla_call_module_version=9,\n+    nr_devices=1,\n+)  # End paste\ndiff --git a/jax/_src/internal_test_util/export_back_compat_test_data/tpu_Sharding.py b/jax/_src/internal_test_util/export_back_compat_test_data/tpu_Sharding.py\nindex f2d8be3b958a..1caac00a4680 100644\n--- a/jax/_src/internal_test_util/export_back_compat_test_data/tpu_Sharding.py\n+++ b/jax/_src/internal_test_util/export_back_compat_test_data/tpu_Sharding.py\n@@ -15,8 +15,10 @@\n import datetime\n from numpy import array, float32\n \n+data_2023_03_16 = {}\n+\n # Pasted from the test output (see module docstring)\n-data_2023_03_16 = dict(\n+data_2023_03_16['gspmd'] = dict(\n     testdata_version=1,\n     platform='tpu',\n     custom_call_targets=['SPMDFullToShardShape', 'SPMDShardToFullShape', 'Sharding'],\n@@ -47,3 +49,40 @@\n     xla_call_module_version=4,\n     nr_devices=2,\n )  # End paste\n+\n+\n+# Pasted from the test output (see export_back_compat_test_util.py module docstring)\n+data_2023_03_16['shardy'] = dict(\n+    testdata_version=1,\n+    platform='tpu',\n+    custom_call_targets=['xla.sdy.FuncResultSharding', 'xla.sdy.GlobalToLocalShape', 'xla.sdy.LocalToGlobalShape'],\n+    serialized_date=datetime.date(2025, 5, 28),\n+    inputs=(array([[0., 1., 2., 3.],\n+       [4., 5., 6., 7.]], dtype=float32),),\n+    expected_outputs=(array([[4., 5., 6., 7.],\n+       [0., 1., 2., 3.]], dtype=float32),),\n+    mlir_module_text=r\"\"\"\n+#loc1 = loc(\"x\")\n+#loc2 = loc(\"third_party/py/jax/tests/export_back_compat_test.py\":783:6)\n+#loc4 = loc(\"jit(func)/jit(main)/shard_map\"(#loc2))\n+module @jit_func attributes {jax.uses_shape_polymorphism = false, mhlo.frontend_attributes = {xla.sdy.meshes = \"{mesh = #sdy.mesh<[\\22a\\22=2]>}\"}, mhlo.num_partitions = 2 : i32, mhlo.num_replicas = 1 : i32} {\n+  func.func public @main(%arg0: tensor<2x4xf32> {mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding<@mesh, [{\\22a\\22}, {}]>\"}, mhlo.sharding = \"{devices=[2,1]<=[2]}\"} loc(\"x\")) -> (tensor<2x4xf32> {jax.result_info = \"result\", mhlo.sharding = \"{devices=[2,1]<=[2]}\"}) {\n+    %0 = stablehlo.custom_call @xla.sdy.GlobalToLocalShape(%arg0) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.in_shardings = \"#sdy.sharding_per_value<[<@mesh, [{\\22a\\22}, {}]>]>\", xla.sdy.manual_axes = \"#sdy<manual_axes{\\22a\\22}>\"}} : (tensor<2x4xf32>) -> tensor<1x4xf32> loc(#loc4)\n+    %1 = call @xla.sdy.manual_computation_body(%0) : (tensor<1x4xf32>) -> tensor<1x4xf32> loc(#loc4)\n+    %2 = stablehlo.custom_call @xla.sdy.LocalToGlobalShape(%1) {mhlo.frontend_attributes = {xla.sdy.manual_axes = \"#sdy<manual_axes{\\22a\\22}>\", xla.sdy.out_shardings = \"#sdy.sharding_per_value<[<@mesh, [{\\22a\\22}, {}]>]>\"}} : (tensor<1x4xf32>) -> tensor<2x4xf32> loc(#loc4)\n+    %3 = stablehlo.custom_call @xla.sdy.FuncResultSharding(%2) {has_side_effect = true, mhlo.frontend_attributes = {xla.sdy.sharding = \"#sdy.sharding_per_value<[<@mesh, [{\\22a\\22}, {}]>]>\"}} : (tensor<2x4xf32>) -> tensor<2x4xf32> loc(#loc4)\n+    return %3 : tensor<2x4xf32> loc(#loc)\n+  } loc(#loc)\n+  func.func @xla.sdy.manual_computation_body(%arg0: tensor<1x4xf32> loc(\"jit(func)/jit(main)/shard_map\"(#loc2))) -> tensor<1x4xf32> {\n+    %0 = \"stablehlo.collective_permute\"(%arg0) <{channel_handle = #stablehlo.channel_handle<handle = 1, type = 0>, source_target_pairs = dense<[[0, 1], [1, 0]]> : tensor<2x2xi64>}> : (tensor<1x4xf32>) -> tensor<1x4xf32> loc(#loc5)\n+    return %0 : tensor<1x4xf32> loc(#loc4)\n+  } loc(#loc4)\n+} loc(#loc)\n+#loc = loc(unknown)\n+#loc3 = loc(\"third_party/py/jax/tests/export_back_compat_test.py\":779:13)\n+#loc5 = loc(\"jit(func)/jit(main)/ppermute\"(#loc3))\n+\"\"\",\n+    mlir_module_serialized=b'ML\\xefR\\rStableHLO_v1.10.3\\x00\\x01\\x1d\\x05\\x01\\x05\\r\\x01\\x03\\x0b\\x03\\x0b\\x0f\\x13\\x17\\x1b\\x1f\\x03\\x9fy\\x13\\x013\\x0f\\x0b\\x07\\x0b+\\x0b\\x0f\\x13\\x0b\\x0b\\x0b\\x0f\\x0b\\x0f\\x0b\\x0b\\x17\\x0f\\x0b\\x17\\x0f\\x0b\\x13\\x13\\x13\\x03G\\x0b\\x0b\\x0b\\x0b\\x0b\\x0b\\x0b\\x0b\\x0b\\x0b\\x0b\\x0f\\x1b\\x0b\\x13\\x0b\\x0b\\x0f\\x1b\\x0b\\x0b\\x0b\\x0b\\x0b\\x0f\\x8f\\x1b\\x0b\\x0b\\x1b\\x0b\\x0b\\x0b\\x13\\x0b\\x01\\x05\\x0f\\x0b\\x03\\x0f\\x17\\x17\\x07\\x07\\x17\\x17\\x17\\x02\\xae\\x03\\x1d\\x1f!\\x05\\x11\\x1f\\x05\\x13\\x03\\t\\x0b\\r\\x03\\x0f\\x15\\x17\\x19\\x1b\\x05\\x15\\x11\\x03\\x00\\x03\\x03\\x11\\x13\\x05\\x17\\x05\\x19\\x05\\x1b\\x11\\x01\\t\\x05\\x1d\\x11\\x01\\x05\\x05\\x1f\\x05!\\x17\\x07>\\x0c\\r\\x1d%\\'\\x05#\\x17\\x07.\\x0c\\x1b\\x1d+\\x05\\x05%\\x03\\x03\\x03g\\x03\\x03\\x03m\\x03\\x03\\x03u\\x03\\x01\\x1d\\'\\x1d)\\x0b\\x03\\x1d+\\x1d-\\x1d/\\x1d1\\x1d3\\x1d5\\x05\\x03\\x03\\x03K\\r\\x05MO=?\\x1d\\x11\\r\\x03;Q\\x1d7#\\r\\x03\\x03W\\r\\x05Y[=?\\x1d9\\x1d;\\x1d=\\x1d?#\\x0f\\x13\\x0b\\x05\\x1f\\x11A\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\r\\x05i7CE\\x1dA\\x1dC\\r\\x05CEo7\\x1dE\\x1dG\\x05\\x01\\r\\x03;7\\x1dI\\x01\\x02\\x02\\x01\\t)\\x05\\x05\\x11\\t)\\x05\\t\\x11\\t\\t\\x1d\\x11\\x03\\x07\\x03\\x07\\x11\\x03\\x05\\x03\\x05)\\x05\\t\\t\\x0b\\x04\\xbb\\x05\\x01Q\\x05\\t\\x01\\x07\\x04\\xa9\\x03\\x01\\t\\x05P\\x05\\x03\\x07\\x04_\\x03\\x0b\\x17\\x03\\x0f)\\x00\\x03G\\x01-\\x05\\x03\\x05\\x03\\x01\\x0bF\\x01\\x07\\x03\\x05\\x03\\x03\\x03G\\x01/\\t\\x03\\x07\\x03\\x05\\x03G\\x011\\x0b\\x03\\x07\\x03\\x07\\x07\\x04\\x05\\x03\\t\\x05P\\x01\\r\\x07\\x04)\\x03\\x05\\x0b\\x03\\x0b\\x01\\x00\\tF#\\x0f\\x03\\x05\\x03\\x01\\x07\\x04\\x01\\x03\\x03\\x06\\x03\\x01\\x05\\x01\\x00\\xaa\\x0bK77-7+\\x0f\\x0b\\x0f!E/)A+\\x1d#a\\x03\\x05;=\\x13%)9\\x1f9i3\\x11-\\x15\\x11\\x1f\\x0f\\x0b\\x11builtin\\x00vhlo\\x00module\\x00custom_call_v1\\x00func_v1\\x00return_v1\\x00collective_permute_v1\\x00call_v1\\x00mhlo.frontend_attributes\\x00third_party/py/jax/tests/export_back_compat_test.py\\x00jax.uses_shape_polymorphism\\x00xla.sdy.meshes\\x00{mesh = #sdy.mesh<[\"a\"=2]>}\\x00mhlo.num_partitions\\x00mhlo.num_replicas\\x00jit_func\\x00jit(func)/jit(main)/shard_map\\x00jit(func)/jit(main)/ppermute\\x00x\\x00\\x00#sdy.sharding_per_value<[<@mesh, [{\"a\"}, {}]>]>\\x00xla.sdy.sharding\\x00mhlo.sharding\\x00{devices=[2,1]<=[2]}\\x00xla.sdy.manual_computation_body\\x00xla.sdy.manual_axes\\x00#sdy<manual_axes{\"a\"}>\\x00#sdy.sharding<@mesh, [{\"a\"}, {}]>\\x00jax.result_info\\x00result\\x00main\\x00public\\x00xla.sdy.in_shardings\\x00xla.sdy.GlobalToLocalShape\\x00xla.sdy.out_shardings\\x00xla.sdy.LocalToGlobalShape\\x00xla.sdy.FuncResultSharding\\x00\\x08a\\x11\\x05;\\x01\\x0bISU]_\\x1195k3G333\\x03A\\x1195q3s333\\x1195w3G333\\x0b3a3A5\\x05ce',\n+    xla_call_module_version=9,\n+    nr_devices=2,\n+)  # End paste\ndiff --git a/tests/BUILD b/tests/BUILD\nindex faa3367aecba..a38f5e91192e 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -2039,10 +2039,9 @@ jax_multiplatform_test(\n jax_multiplatform_test(\n     name = \"export_back_compat_test\",\n     srcs = [\"export_back_compat_test.py\"],\n-    # TODO(b/415285434): enable once we have backwards compatibility support with GSPMD checkpoints.\n-    # enable_configs = [\n-    #     \"tpu_v3_x4_shardy\",\n-    # ],\n+    enable_configs = [\n+        \"tpu_v3_x4_shardy\",\n+    ],\n     tags = [],\n     deps = [\n         \"//jax:internal_export_back_compat_test_data\",\ndiff --git a/tests/export_back_compat_test.py b/tests/export_back_compat_test.py\nindex be87b4e3e5b3..0f443ae47929 100644\n--- a/tests/export_back_compat_test.py\n+++ b/tests/export_back_compat_test.py\n@@ -778,7 +778,12 @@ def func(x):  # b: f32[2, 4]\n       perm = [(j, (j + 1) % axis_size) for j in range(axis_size)]\n       return lax.ppermute(x, 'a', perm=perm)\n \n-    data = self.load_testdata(tpu_Sharding.data_2023_03_16)\n+    data = tpu_Sharding.data_2023_03_16\n+    if jax.config.jax_use_shardy_partitioner:\n+      data = data[\"shardy\"]\n+    else:\n+      data = data[\"gspmd\"]\n+    data = self.load_testdata(data)\n     with mesh:\n       self.run_one_test(func, data)\n \n@@ -801,9 +806,15 @@ def func(x, y):\n       return x + y\n \n     if platform == \"tpu\":\n-      data = self.load_testdata(annotate_data_placement.data_2025_04_07_tpu)\n+      data = annotate_data_placement.data_2025_04_07_tpu\n+    else:\n+      data = annotate_data_placement.data_2025_04_07_cuda\n+\n+    if jax.config.jax_use_shardy_partitioner:\n+      data = data[\"shardy\"]\n     else:\n-      data = self.load_testdata(annotate_data_placement.data_2025_04_07_cuda)\n+      data = data[\"gspmd\"]\n+    data = self.load_testdata(data)\n \n     self.run_one_test(func, data)\n \n\n```",
        "from_id": [
            "bartchr808",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas/Mosaic GPU] Expose the new `TCGEN05_ROW` layout.\n\n`TCGEN05_ROW` is to `TCGEN05` what `WGMMA_ROW` is to `WGMMA`.\n\nPiperOrigin-RevId: 767068597",
        "output": "```diff\nCommit: d34f1ddbaf7a706fe3abda7e527af5f13cf341e8\nDate: 2025-06-04T10:33:51Z\nURL: https://github.com/jax-ml/jax/commit/d34f1ddbaf7a706fe3abda7e527af5f13cf341e8\nFiles changed: 3\nAdditions: +12, Deletions: -0\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 2bd191b859ba..616f7e501cd8 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -1451,6 +1451,7 @@ class Layout(enum.Enum):\n   WG_STRIDED = enum.auto()\n \n   TCGEN05 = enum.auto()\n+  TCGEN05_ROW = enum.auto()\n \n   def __call__(self, *args, **kwargs) -> ParameterizedLayout:\n     return ParameterizedLayout(self, args, kwargs)\n@@ -1480,6 +1481,9 @@ def check_no_args():\n       case Layout.TCGEN05:\n         check_no_args()\n         return mgpu.TCGEN05_LAYOUT\n+      case Layout.TCGEN05_ROW:\n+        check_no_args()\n+        return mgpu.TCGEN05_ROW_LAYOUT\n \n @dataclasses.dataclass(frozen=True)\n class ParameterizedLayout:\ndiff --git a/jax/experimental/mosaic/gpu/__init__.py b/jax/experimental/mosaic/gpu/__init__.py\nindex cd207c2b2519..37bfa227fe61 100644\n--- a/jax/experimental/mosaic/gpu/__init__.py\n+++ b/jax/experimental/mosaic/gpu/__init__.py\n@@ -105,4 +105,5 @@\n \n from .tcgen05 import (\n   LAYOUT as TCGEN05_LAYOUT,  # noqa: F401\n+  ROW_LAYOUT as TCGEN05_ROW_LAYOUT,  # noqa: F401\n )\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex 0438400f6310..d72994f45a87 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -43,6 +43,13 @@\n     lane_dims=(-4, -3),\n     vector_dim=-1,\n )\n+# ROW_LAYOUT is to LAYOUT as WGMMA_ROW_LAYOUT is to WGMMA_LAYOUT.\n+ROW_LAYOUT = fa.TiledLayout(\n+    fa.Tiling(tiles=((128,), (32,), (8,), (1,), (1,))),\n+    warp_dim=-5,\n+    lane_dims=(-3, fa.Replicated(times=4)),\n+    vector_dim=-1\n+)\n # A layout resembling the logical organization of TMEM. The 128 rows in a tile\n # are assigned to 128 lanes in the warpgroup. Useful when the result needs to be\n # processed in registers and then stored back into TMEM. Should not be used if\n\n```",
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "#sdy Fallback to GSPMD in JAX export if the loaded module was lowered for GSPMD.\n\nThe final module that will be created by JAX export will contain a bit of Shardy and GSPMD ops. What we then do during compilation is detect whether there is a mix of these ops. If there is, we override the build option and instead use GSPMD for propagation (we have well tested code to export Shardy->GSPMD, but not vice versa).\n\nPiperOrigin-RevId: 767064075",
        "output": "```diff\nCommit: b7833e94c1940ed475dae1f5e83e2a984cda5cea\nDate: 2025-06-04T10:15:45Z\nURL: https://github.com/jax-ml/jax/commit/b7833e94c1940ed475dae1f5e83e2a984cda5cea\nFiles changed: 5\nAdditions: +136, Deletions: -14\ndiff --git a/jax/_src/export/_export.py b/jax/_src/export/_export.py\nindex b390574c0a79..5e3c4cf0f209 100644\n--- a/jax/_src/export/_export.py\n+++ b/jax/_src/export/_export.py\n@@ -1431,9 +1431,16 @@ def _call_exported_lowering(ctx: mlir.LoweringRuleContext, *args,\n   submodule_bc = mlir.module_to_bytecode(submodule)\n   shardy_enabled = _jax.sdy.lowered_with_shardy(submodule_bc)\n   if shardy_enabled:\n+    if not config.use_shardy_partitioner.value:\n+      raise ValueError(\n+          \"The function was exported with shardy enabled but you are calling \"\n+          \"it with Shardy disabled. Please enable Shardy using \"\n+          \"`--jax_use_shardy_partitioner=True`.\")\n     submodule = ir.Module.parse(\n         _jax.sdy.sdy_round_trip_import_shardings(submodule_bc)\n     )\n+  elif config.use_shardy_partitioner.value:\n+    shardy_enabled = True\n \n   with submodule.context:\n     pipeline = passmanager.PassManager.parse(\n@@ -1444,7 +1451,7 @@ def _call_exported_lowering(ctx: mlir.LoweringRuleContext, *args,\n   if shardy_enabled:\n     sdy_mesh_axes = _jax.sdy.get_mesh(mlir.module_to_bytecode(submodule))\n     mesh = (mesh_lib.AbstractMesh(*list(zip(*sdy_mesh_axes))[::-1])\n-            if sdy_mesh_axes else mesh_lib.empty_abstract_mesh)\n+            if sdy_mesh_axes else None)\n \n   axis_context = ctx.module_context.axis_context\n   if isinstance(axis_context, sharding_impls.ShardingContext):\n@@ -1473,15 +1480,19 @@ def _call_exported_lowering(ctx: mlir.LoweringRuleContext, *args,\n       )\n \n   # Apply in_shardings\n-  if shardy_enabled:\n+  if mesh:\n+    # A mesh only exists if Shardy is enabled.\n     args = tuple(\n         wrap_with_sharding(\n             ctx, x, x_aval,\n-            _hlo_sharding_to_named_sharding(x_sharding, mesh))  # type: ignore[arg-type]\n+            _hlo_sharding_to_named_sharding(x_sharding, mesh), use_shardy=True)  # type: ignore[arg-type]\n         for x, x_aval, x_sharding in zip(args, ctx.avals_in, exported.in_shardings_hlo))\n   else:\n+    # Since there is no mesh - either due to shardy being disabled or the loaded\n+    # function being lowered for GSPMD (so no shardy mesh) - need to create a\n+    # GSPMD sharding from the HLO sharding (can't use shardy lowering).\n     args = tuple(\n-        wrap_with_sharding(ctx, x, x_aval, x_sharding)\n+        wrap_with_sharding(ctx, x, x_aval, x_sharding, use_shardy=False)\n         for x, x_aval, x_sharding in zip(args, ctx.avals_in, exported.in_shardings_hlo))\n \n   symtab = ir.SymbolTable(submodule.operation)\n@@ -1570,14 +1581,19 @@ def convert_shape(x: ir.Value, x_aval: core.AbstractValue, new_aval: core.Abstra\n       for out, out_aval, refined_out_aval in zip(call.results[len(ordered_effects):],\n                                                  exported.out_avals, ctx.avals_out))\n   # Apply out_shardings\n-  if shardy_enabled:\n+  if mesh:\n+    # A mesh only exists if Shardy is enabled.\n     results = tuple(\n         wrap_with_sharding(\n-            ctx, x, x_aval, _hlo_sharding_to_named_sharding(x_sharding, mesh))  # type: ignore[arg-type]\n+            ctx, x, x_aval, _hlo_sharding_to_named_sharding(x_sharding, mesh),\n+            use_shardy=True)  # type: ignore[arg-type]\n         for x, x_aval, x_sharding in zip(results, ctx.avals_out, exported.out_shardings_hlo))\n   else:\n+    # Since there is no mesh - either due to shardy being disabled or the loaded\n+    # function being lowered for GSPMD (so no shardy mesh) - need to create a\n+    # GSPMD sharding from the HLO sharding (can't use shardy lowering).\n     results = tuple(\n-        wrap_with_sharding(ctx, x, x_aval, x_sharding)\n+        wrap_with_sharding(ctx, x, x_aval, x_sharding, use_shardy=False)\n         for x, x_aval, x_sharding in zip(results, ctx.avals_out, exported.out_shardings_hlo))\n   return results\n \n@@ -1588,12 +1604,14 @@ def wrap_with_sharding(\n     ctx: mlir.LoweringRuleContext,\n     x: ir.Value,\n     x_aval: core.AbstractValue,\n-    x_sharding: sharding_impls.NamedSharding | HloSharding | None,\n+    x_sharding: sharding_impls.NamedSharding | sharding_impls.GSPMDSharding | HloSharding | None,\n+    use_shardy: bool,\n ) -> ir.Value:\n   if x_sharding is None:\n     return x\n-  if config.use_shardy_partitioner.value:\n+  if use_shardy:\n     x_sharding = x_sharding._to_sdy_sharding(x_aval.ndim)  # type: ignore\n   else:\n     x_sharding = x_sharding.to_proto()  # type: ignore\n-  return mlir.wrap_with_sharding_op(ctx, x, x_aval, x_sharding)  # type: ignore[arg-type]\n+  return mlir.wrap_with_sharding_op(ctx, x, x_aval, x_sharding,  # type: ignore[arg-type]\n+                                    allow_shardy_lowering=use_shardy)\ndiff --git a/jax/_src/interpreters/mlir.py b/jax/_src/interpreters/mlir.py\nindex 0256057b8b09..6bad0ee2a018 100644\n--- a/jax/_src/interpreters/mlir.py\n+++ b/jax/_src/interpreters/mlir.py\n@@ -2731,7 +2731,7 @@ def lower_with_sharding_in_types(ctx, op, aval, sharding_proto=None):\n \n \n def set_sharding(op, sharding: xc.OpSharding | SdyArray | SdyArrayList):\n-  if config.use_shardy_partitioner.value:\n+  if isinstance(sharding, (SdyArray, SdyArrayList)):\n     op.attributes[\"sdy.sharding\"] = get_sharding_attr(sharding)\n   else:\n     op.attributes[\"mhlo.sharding\"] = get_sharding_attr(sharding)\n@@ -2740,7 +2740,7 @@ def set_sharding(op, sharding: xc.OpSharding | SdyArray | SdyArrayList):\n def get_sharding_attr(\n     sharding: xc.OpSharding | SdyArray | SdyArrayList\n ) -> ir.Attribute:\n-  if config.use_shardy_partitioner.value:\n+  if isinstance(sharding, (SdyArray, SdyArrayList)):\n     return sharding.build()  # type: ignore\n   else:\n     # If there are very large numbers of devices, use the proto representation.\ndiff --git a/jaxlib/BUILD b/jaxlib/BUILD\nindex 834103063aae..6fd606966b53 100644\n--- a/jaxlib/BUILD\n+++ b/jaxlib/BUILD\n@@ -870,12 +870,14 @@ cc_library(\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:cord\",\n         \"@com_google_absl//absl/strings:str_format\",\n-        \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/types:span\",\n         \"@llvm-project//llvm:Support\",\n+        \"@llvm-project//mlir:FuncDialect\",\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:Pass\",\n         \"@nanobind\",\n+        \"@shardy//shardy/dialect/sdy/ir:dialect\",\n+        \"@stablehlo//:stablehlo_ops\",\n         \"@tsl//tsl/platform:fingerprint\",\n         \"@tsl//tsl/platform:ml_dtypes\",\n         \"@tsl//tsl/profiler/lib:traceme\",\n@@ -915,6 +917,7 @@ cc_library(\n         \"@xla//xla/python/pjrt_ifrt:pjrt_dtype\",\n         \"@xla//xla/python/pjrt_ifrt:xla_ifrt\",\n         \"@xla//xla/service:platform_util\",\n+        \"@xla//xla/service/spmd/shardy:constants\",\n         \"@xla//xla/tsl/concurrency:ref_count\",\n         \"@xla//xla/tsl/framework:allocator\",\n         \"@xla//xla/tsl/platform:env\",\ndiff --git a/jaxlib/py_client.cc b/jaxlib/py_client.cc\nindex 8e78f024e1ae..b663c00da35c 100644\n--- a/jaxlib/py_client.cc\n+++ b/jaxlib/py_client.cc\n@@ -35,9 +35,11 @@ limitations under the License.\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n #include \"llvm/Support/Casting.h\"\n+#include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/IR/BuiltinOps.h\"\n #include \"mlir/IR/MLIRContext.h\"\n #include \"mlir/IR/OwningOpRef.h\"\n+#include \"mlir/IR/Visitors.h\"\n #include \"mlir/Pass/PassManager.h\"\n #include \"nanobind/nanobind.h\"\n #include \"nanobind/stl/optional.h\"  // IWYU pragma: keep\n@@ -48,6 +50,7 @@ limitations under the License.\n #include \"nanobind/stl/unique_ptr.h\"  // IWYU pragma: keep\n #include \"nanobind/stl/variant.h\"  // IWYU pragma: keep\n #include \"nanobind/stl/vector.h\"  // IWYU pragma: keep\n+#include \"shardy/dialect/sdy/ir/dialect.h\"\n #include \"jaxlib/guard_lib.h\"\n #include \"jaxlib/nb_class_ptr.h\"\n #include \"jaxlib/py_array.h\"\n@@ -60,6 +63,7 @@ limitations under the License.\n #include \"jaxlib/python_ref_manager.h\"\n #include \"jaxlib/sharding.h\"\n #include \"jaxlib/traceback.h\"\n+#include \"stablehlo/dialect/StablehloOps.h\"\n #include \"xla/literal.h\"\n #include \"xla/pjrt/exceptions.h\"\n #include \"xla/pjrt/mlir_to_hlo.h\"\n@@ -89,6 +93,7 @@ limitations under the License.\n #include \"xla/python/types.h\"\n #include \"xla/python/version.h\"\n #include \"xla/service/platform_util.h\"  // IWYU pragma: keep\n+#include \"xla/service/spmd/shardy/constants.h\"\n #include \"xla/shape.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n@@ -399,6 +404,47 @@ MakeIfrtDeserializeExecutableOptions(std::optional<CompileOptions> options,\n       std::move(ifrt_loaded_host_callbacks));\n }\n \n+// Returns true if the module has at least one GSPMD attribute or op, like an\n+// `mhlo.sharding` attribute or `Sharding` custom call.\n+// TODO(b/420837831): delete this once we don't fall back to GSPMD.\n+bool HasGspmdAttrsOrOps(mlir::ModuleOp module) {\n+  for (auto func : module.getOps<mlir::func::FuncOp>()) {\n+    for (int64_t arg_index = 0; arg_index < func.getNumArguments();\n+         ++arg_index) {\n+      if (func.getArgAttr(arg_index, sdy::kXlaShardingAttr)) {\n+        return true;\n+      }\n+    }\n+    for (int64_t result_index = 0; result_index < func.getNumResults();\n+         ++result_index) {\n+      if (func.getResultAttr(result_index, sdy::kXlaShardingAttr)) {\n+        return true;\n+      }\n+    }\n+  }\n+  // Check the module for a `Sharding` custom call.\n+  bool has_gspmd = false;\n+  module->walk([&has_gspmd](mlir::stablehlo::CustomCallOp custom_call) {\n+    if (custom_call.getCallTargetName() ==\n+        sdy::kShardingCustomCallTargetName &&\n+       custom_call->hasAttr(sdy::kXlaShardingAttr)) {\n+      has_gspmd = true;\n+      return mlir::WalkResult::interrupt();\n+    }\n+    return mlir::WalkResult::advance();\n+  });\n+  return has_gspmd;\n+}\n+\n+// Check if the module has any sort of Shardy mesh:\n+// - `mesh`\n+// - `maximal_mesh_{X}`\n+// - `empty_mesh`\n+// TODO(b/420837831): delete this once we don't fall back to GSPMD.\n+bool HasShardyMesh(mlir::ModuleOp module) {\n+  return !module.getOps<mlir::sdy::MeshOp>().empty();\n+}\n+\n }  // namespace\n \n /* static */ absl::StatusOr<nb_class_ptr<PyLoadedExecutable>>\n@@ -483,6 +529,19 @@ PyClient::CompileAndLoad(nb_class_ptr<PyClient> client, std::string mlir_module,\n   mlir::MLIRContext context;\n   TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n                       ParseMlirModuleString(mlir_module, context));\n+  // TODO(b/420837831): Remove this once we don't need to fall back to GSPMD.\n+  if (options.executable_build_options.use_shardy_partitioner() &&\n+      HasGspmdAttrsOrOps(module.get())) {\n+    LOG(WARNING)\n+        << \"Module has GSPMD attrs or ops, but Shardy is enabled. Disabling \"\n+           \"Shardy and falling back to using GSPMD propagation.\";\n+    options.executable_build_options.set_use_shardy_partitioner(false);\n+    if (HasShardyMesh(module.get())) {\n+      // Shardy is not enabled, but the module has shardy ops. Likely due to\n+      // export loading a GSPMD checkpoint. Fall back to GSPMD.\n+      TF_RETURN_IF_ERROR(ExportShardyForGSPMD(*module));\n+    }\n+  }\n   return CompileAndLoadIfrtProgram(\n       client, std::make_unique<xla::ifrt::HloProgram>(module.get()),\n       MakeIfrtCompileOptions(std::move(options), std::move(executable_devices),\ndiff --git a/tests/export_test.py b/tests/export_test.py\nindex 598a6634e1e3..0dfebdcec054 100644\n--- a/tests/export_test.py\n+++ b/tests/export_test.py\n@@ -2008,7 +2008,7 @@ def f(x, y):\n     r = jax.jit(exp.call, out_shardings=NamedSharding(old_mesh_0, P(\"old_b\")))(a, b)\n     self.assertAllClose(a + b, r)\n \n-  def test_lower_wth_different_meshes_axis_names(self):\n+  def test_lower_with_different_meshes_axis_names(self):\n     mesh1 = jtu.create_mesh((4, 2), (\"a\", \"b\"))\n     mesh2 = jtu.create_mesh((4, 2), (\"x\", \"y\"))\n     @jax.jit\n@@ -2033,6 +2033,48 @@ def f(tree):\n     else:\n        get_exported(f)(args)\n \n+  @jtu.parameterized_filterable(\n+    kwargs=[\n+        {\"use_shardy_on_save\": True, \"error_msg\": \"Please enable Shardy\"},\n+        {\"use_shardy_on_save\": False, \"error_msg\": \"\"},\n+    ])\n+  def test_lower_load_with_different_partitioners(self, use_shardy_on_save,\n+                                                  error_msg):\n+    old_shardy = config.use_shardy_partitioner.value\n+    try:\n+      jax.config.update(\"jax_use_shardy_partitioner\", use_shardy_on_save)\n+      mesh = jtu.create_mesh((8,), (\"a\",))\n+      @jax.jit\n+      def f(x, y):\n+        z = x + y\n+        return jax.lax.with_sharding_constraint(\n+            z, NamedSharding(mesh, P(\"a\")))\n+\n+      args = (\n+          jax.ShapeDtypeStruct(\n+              (32, 32), dtype=np.float32,\n+              sharding=NamedSharding(mesh, P(None, \"a\"))),\n+          jax.ShapeDtypeStruct(\n+              (32, 32), dtype=np.float32,\n+              sharding=NamedSharding(mesh, P(\"a\"))))\n+\n+      exp = get_exported(f)(*args)\n+\n+      jax.config.update(\"jax_use_shardy_partitioner\", not use_shardy_on_save)\n+\n+      a = jnp.arange(32 * 32, dtype=np.float32).reshape((32, 32))\n+      a = jax.device_put(a, NamedSharding(mesh, P(None, \"a\")))\n+      b = jnp.arange(32 * 32, dtype=np.float32).reshape((32, 32))\n+      b = jax.device_put(b, NamedSharding(mesh, P(\"a\")))\n+\n+      if use_shardy_on_save:\n+        with self.assertRaisesRegex(ValueError, error_msg):\n+          jax.jit(exp.call, out_shardings=NamedSharding(mesh, P(\"a\")))(a, b)\n+      else:\n+        jax.jit(exp.call, out_shardings=NamedSharding(mesh, P(\"a\")))(a, b)\n+    finally:\n+      jax.config.update(\"jax_use_shardy_partitioner\", old_shardy)\n+\n \n \n if __name__ == \"__main__\":\n\n```",
        "from_id": [
            "bartchr808",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Make experimental pytree_serialization visible in OSS jax build\n\nPiperOrigin-RevId: 766962750",
        "output": "```diff\nCommit: 7d93eee9e79e72e79d45c721f08b72a53e902a07\nDate: 2025-06-04T04:44:39Z\nURL: https://github.com/jax-ml/jax/commit/7d93eee9e79e72e79d45c721f08b72a53e902a07\nFiles changed: 2\nAdditions: +9, Deletions: -6\ndiff --git a/BUILD.bazel b/BUILD.bazel\nindex 44885124797f..d51b9f8c9cef 100644\n--- a/BUILD.bazel\n+++ b/BUILD.bazel\n@@ -52,6 +52,7 @@ wheel_sources(\n         \"//jax/_src/pallas/fuser\",\n         \"//jax/_src/pallas/mosaic_gpu\",\n         \"//jax/experimental/array_serialization:serialization\",\n+        \"//jax/experimental/array_serialization:pytree_serialization\",\n         \"//jax/experimental/jax2tf\",\n         \"//jax/experimental/mosaic/gpu/examples:flash_attention\",\n         \"//jax/experimental/mosaic/gpu/examples:matmul\",\ndiff --git a/jax/experimental/array_serialization/BUILD b/jax/experimental/array_serialization/BUILD\nindex ebd78decf6a3..559d8eb16269 100644\n--- a/jax/experimental/array_serialization/BUILD\n+++ b/jax/experimental/array_serialization/BUILD\n@@ -52,9 +52,10 @@ pytype_library(\n         \"//jax\",\n         \"//jax/experimental/array_serialization:pytree_serialization_utils\",\n         \"//jax/experimental/array_serialization:tensorstore_impl\",\n-        \"//third_party/py/absl/logging\",\n-        \"//third_party/py/numpy\",\n-    ],\n+    ] + py_deps([\n+        \"absl/logging\",\n+        \"numpy\",\n+    ]),\n )\n \n pytype_library(\n@@ -62,9 +63,10 @@ pytype_library(\n     srcs = [\"pytree_serialization_utils.py\"],\n     deps = [\n         \"//jax\",\n-        \"//third_party/py/absl/logging\",\n-        \"//third_party/py/numpy\",\n-    ],\n+    ] + py_deps([\n+        \"absl/logging\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n\n```",
        "from_id": [
            "rdyro",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Only infer sharding from input in full_like (in eager mode) if the input's sharding is concrete i.e. does not contain an AbstractMesh\n\nPiperOrigin-RevId: 766962130",
        "output": "```diff\nCommit: 002078b3184add3c06fd80aca8799a09eeb70c12\nDate: 2025-06-04T04:42:09Z\nURL: https://github.com/jax-ml/jax/commit/002078b3184add3c06fd80aca8799a09eeb70c12\nFiles changed: 2\nAdditions: +10, Deletions: -1\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex e03951eb4730..43dffc7bef9c 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -3564,12 +3564,13 @@ def full_like(x: ArrayLike | DuckTypedArray,\n         # This bypasses the check.\n         and not isinstance(x, core.Tracer)\n         and hasattr(x, 'sharding')\n+        and x.sharding is not None\n+        and x.sharding._is_concrete\n         and getattr(x, '_committed', True)\n         and not weak_type\n         and fill_shape == np.shape(x)  # type: ignore[arg-type]\n     )\n     if use_x_sharding:\n-      # TODO(yashkatariya): Use shard_alike in tracing_mode once it is supported.\n       sharding = x.sharding  # type: ignore\n   val = full(fill_shape, _convert_element_type(fill_value, dtype, weak_type),\n              sharding=sharding)\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 2fe2c4696cbe..fa68a441c4c0 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -5912,6 +5912,14 @@ def f(x, y):\n     self.assertArraysEqual(out, (np_inp @ np_inp.T) * 2)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('x', None)))\n \n+  def test_full_like_eager_non_concrete_sharding(self):\n+    s = NamedSharding(mesh_lib.AbstractMesh((2,), ('x',)), P('x'))\n+    arr = jax.ShapeDtypeStruct((8, 2), np.float32, sharding=s)\n+    out = jax.lax.full_like(arr, 0)\n+    # The sharding is single device because the sharding of input `arr`` to\n+    # full_like is not concrete.\n+    self.assertEqual(out.sharding, SingleDeviceSharding(jax.devices()[0]))\n+\n   @jtu.with_explicit_mesh((2, 2), ('x', 'y'))\n   def test_slice(self, mesh):\n     np_inp = np.arange(16.).reshape(4, 4)\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix pgle test breakage\n\nPiperOrigin-RevId: 766895212",
        "output": "```diff\nCommit: 31fde29e415a8915a8cf23c97c331e9be44e2e08\nDate: 2025-06-04T00:53:22Z\nURL: https://github.com/jax-ml/jax/commit/31fde29e415a8915a8cf23c97c331e9be44e2e08\nFiles changed: 1\nAdditions: +4, Deletions: -4\ndiff --git a/tests/pgle_test.py b/tests/pgle_test.py\nindex e136c3ab8a5a..7087bcad58bf 100644\n--- a/tests/pgle_test.py\n+++ b/tests/pgle_test.py\n@@ -158,12 +158,12 @@ def f(x):\n       with config.pgle_profiling_runs(2), config.enable_pgle(True):\n         # Run 1: Module should be compiled without FDO. Two modules are expected\n         # One is the funtion f, the other one is multi slice module\n-        with jtu.count_jit_compilation_cache_miss() as cache_miss_count:\n+        with jtu.count_pjit_cpp_cache_miss() as cache_miss_count:\n           self.assertArraysEqual(f(x), expected)\n         self.assertEqual(cache_miss_count(), 2)\n \n         # Run 2: Second PGLE run. Profile should be empty.\n-        with jtu.count_jit_compilation_cache_miss() as cache_miss_count:\n+        with jtu.count_pjit_cpp_cache_miss() as cache_miss_count:\n           self.assertArraysEqual(f(x), expected)\n         self.assertEqual(cache_miss_count(), 2)\n         fdo_profiles_before_pgle = self.get_fdo_profiles(dump_dir)\n@@ -175,7 +175,7 @@ def f(x):\n             os.path.getsize(os.path.join(dump_dir, fdo_profiles_before_pgle[0])), 0)\n \n         # Run 3: The module should be recompiled with FDO profiles\n-        with jtu.count_jit_compilation_cache_miss() as cache_miss_count:\n+        with jtu.count_pjit_cpp_cache_miss() as cache_miss_count:\n           self.assertArraysEqual(f(x), expected)\n         self.assertEqual(cache_miss_count(), 2)\n         fdo_profiles_after_pgle = self.get_fdo_profiles(dump_dir)\n@@ -190,7 +190,7 @@ def f(x):\n             )\n \n         # Run 4: Fast-path should be used after PGLE is done\n-        with jtu.count_jit_compilation_cache_miss() as cache_miss_count:\n+        with jtu.count_pjit_cpp_cache_miss() as cache_miss_count:\n           self.assertArraysEqual(f(x), expected)\n         self.assertLess(cache_miss_count(), 2)\n \n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update more uses of `backend.compile` to `backend.compile_and_load`.\n\nPiperOrigin-RevId: 766874092",
        "output": "```diff\nCommit: ab84dde2301ca57edea443d64cadf003bd126626\nDate: 2025-06-03T23:48:44Z\nURL: https://github.com/jax-ml/jax/commit/ab84dde2301ca57edea443d64cadf003bd126626\nFiles changed: 3\nAdditions: +3, Deletions: -3\ndiff --git a/docs/autodidax.ipynb b/docs/autodidax.ipynb\nindex 16d4da37b3f2..f57ce09e0bf6 100644\n--- a/docs/autodidax.ipynb\n+++ b/docs/autodidax.ipynb\n@@ -2020,7 +2020,7 @@\n     \"  output = io.StringIO()\\n\",\n     \"  c.module.operation.print(file=output)\\n\",\n     \"  backend = xb.get_backend(None)\\n\",\n-    \"  compiled = backend.compile(output.getvalue(), backend.devices()[:1])\\n\",\n+    \"  compiled = backend.compile_and_load(output.getvalue(), backend.devices()[:1])\\n\",\n     \"  return partial(execute_compiled, compiled, [v.aval for v in jaxpr.outs])\\n\",\n     \"\\n\",\n     \"def _mlir_dtype(dtype: np.dtype) -> ir.Type:\\n\",\ndiff --git a/docs/autodidax.md b/docs/autodidax.md\nindex 870ee20f0f9a..5bf0e8f78e12 100644\n--- a/docs/autodidax.md\n+++ b/docs/autodidax.md\n@@ -1590,7 +1590,7 @@ def xla_callable(hashable_jaxpr: IDHashable,\n   output = io.StringIO()\n   c.module.operation.print(file=output)\n   backend = xb.get_backend(None)\n-  compiled = backend.compile(output.getvalue(), backend.devices()[:1])\n+  compiled = backend.compile_and_load(output.getvalue(), backend.devices()[:1])\n   return partial(execute_compiled, compiled, [v.aval for v in jaxpr.outs])\n \n def _mlir_dtype(dtype: np.dtype) -> ir.Type:\ndiff --git a/docs/autodidax.py b/docs/autodidax.py\nindex b0dbf9f73d9f..695fc9993df5 100644\n--- a/docs/autodidax.py\n+++ b/docs/autodidax.py\n@@ -1582,7 +1582,7 @@ def main(*params):\n   output = io.StringIO()\n   c.module.operation.print(file=output)\n   backend = xb.get_backend(None)\n-  compiled = backend.compile(output.getvalue(), backend.devices()[:1])\n+  compiled = backend.compile_and_load(output.getvalue(), backend.devices()[:1])\n   return partial(execute_compiled, compiled, [v.aval for v in jaxpr.outs])\n \n def _mlir_dtype(dtype: np.dtype) -> ir.Type:\n\n```",
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas] Add forward-compatible i1 broadcast.\n\nMissed this in https://github.com/jax-ml/jax/commit/6c18aa8a468e35b8c11b101dceaa43d05b497177\n\nPiperOrigin-RevId: 766873399",
        "output": "```diff\nCommit: 3c1df032564c7346f0e26ee858b8fc4b9f7eb8a7\nDate: 2025-06-03T23:46:54Z\nURL: https://github.com/jax-ml/jax/commit/3c1df032564c7346f0e26ee858b8fc4b9f7eb8a7\nFiles changed: 1\nAdditions: +16, Deletions: -0\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex b1a2c186e2c9..73becf5e03c6 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -1928,6 +1928,22 @@ def _broadcast_in_dim_lowering_rule(\n   if aval_in.shape == shape:\n     return val\n \n+  if jnp.issubdtype(aval_in.dtype, jnp.bool_) and (\n+      ctx.forward_compatible or is_cloud_tpu_older_than(2025, 6, 3)\n+  ):\n+    # Direct broadcasts for bools are not supported in Mosaic due to booleans\n+    # living in mask registers and broadcast operating on vregs. Broadcast as an\n+    # integer instead and cast back to a bool.\n+    def _proxy_fun(val, *, shape, broadcast_dimensions):\n+      int_val = jnp.where(val, 1, 0)\n+      bcast_val = jax.lax.broadcast_in_dim(int_val, shape, broadcast_dimensions)\n+      return bcast_val == 1\n+\n+    proxy_lowering = lower_fun(_proxy_fun, multiple_results=False)\n+    return proxy_lowering(\n+        ctx, val, shape=shape, broadcast_dimensions=broadcast_dimensions\n+    )\n+\n   if broadcast_dimensions:\n     out_shape_list = [1] * len(shape)\n     for i, s in zip(broadcast_dimensions, aval_in.shape):\n\n```",
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] In TPU interpret mode, run kernels in parallel over Megacore cores.\n\nInternally, TPU interpret mode uses a new io_callback which spawns multiple threads to simulate multiple Megacore cores.\n\nAlso updates some comments / code / variable names to better distinguish between internal indices used in interpret mode vs. indices into the Pallas grid.\n\nPiperOrigin-RevId: 766851983",
        "output": "```diff\nCommit: b6a1575a45e2ba6f6840a527e0d84652cdc13989\nDate: 2025-06-03T22:47:55Z\nURL: https://github.com/jax-ml/jax/commit/b6a1575a45e2ba6f6840a527e0d84652cdc13989\nFiles changed: 3\nAdditions: +394, Deletions: -283\ndiff --git a/jax/_src/pallas/mosaic/interpret.py b/jax/_src/pallas/mosaic/interpret.py\nindex 3be718aa0aa6..e278168d999a 100644\n--- a/jax/_src/pallas/mosaic/interpret.py\n+++ b/jax/_src/pallas/mosaic/interpret.py\n@@ -35,7 +35,6 @@\n from jax._src.pallas import core as pallas_core\n from jax._src.pallas import primitives\n from jax._src import pjit\n-from jax._src.pallas.mosaic import core as tpu_core\n from jax._src.state import discharge as state_discharge\n from jax._src.state import indexing\n from jax._src.state import primitives as state_primitives\n@@ -532,7 +531,6 @@ class SharedMemory:\n   clean_up_barrier: threading.Barrier\n \n   # (memory_space, buffer_id, device_id, local_core_id) -> NumPy array\n-  # TODO(jburnim): Handle Megacore.\n   mem: dict[tuple[str, int, int, int], np.ndarray] = dataclasses.field(\n       default_factory=dict)\n \n@@ -691,16 +689,17 @@ def _allocate_buffer(\n     local_core_ids = (local_core_id_int,)\n   del local_core_id\n \n-  local_core_id_to_buffer_id = {}\n+  local_core_id_to_buffer_id : dict[int, int] = {}\n   with shared_memory.lock:\n     for lci in local_core_ids:\n       buffer_id = shared_memory.next_buffer_id[(device_id, lci)]\n       shared_memory.next_buffer_id[(device_id, lci)] = buffer_id + 1\n+      # If allocating in HBM, only actually allocate a buffer for core 0.\n       if lci == 0 or memory_space_str != 'any':\n-        # If allocating in HBM, only actually allocate a buffer for local core\n-        # id 0.\n-        # TODO(jburnim): Add options for initializing memory (e.g., with NaNs,\n-        # with zeros, or with the buffer ID).\n+        # If we are allocating more than one buffer, we must make additional\n+        # copies of `val` so that each buffer is a distinct ndarray.\n+        if len(local_core_id_to_buffer_id) > 0:\n+          val = val.copy()\n         shared_memory.mem[(memory_space_str, buffer_id, device_id, lci)] = val\n \n       local_core_id_to_buffer_id[lci] = buffer_id\n@@ -1040,7 +1039,6 @@ def swap(\n   buffer_id = int(buffer_id)\n   try:\n     transforms = jax.tree.map(int, transforms)\n-    # jax.debug.print(f'swap: {transforms}')\n   except:\n     raise ValueError('Advanced indexers are not supported on TPU')\n   val = np.array(val)\n@@ -1387,7 +1385,6 @@ def write(var, value):\n   # TODO(jburnim): Clean up and finish this evaluation loop.  For example:\n   #  - Replace the big if-statement with a dictionary of rules.\n   #  - Handle other higher-order primitives?\n-  #  - Megacore.\n   _interpret = functools.partial(\n       _interpret_jaxpr,\n       mesh=mesh,\n@@ -1458,9 +1455,9 @@ def write(var, value):\n \n       elif ((prim is lax.axis_index_p)\n             and (mesh is not None) and (eqn.params['axis_name'] in mesh.shape)):\n-        # For now, there can only be one core.\n-        # TODO(jburnim): Support two Megacore cores.\n-        out = jnp.int32(0)\n+        # We are interpreting a core_map, and this lax.axis_index call is\n+        # querying our index along the core axis, so return our core ID.\n+        out = local_core_id\n \n       elif prim is lax.cond_p:\n         def _make_branch(jaxpr):\n@@ -1708,7 +1705,8 @@ def f(*args, jaxpr):\n   return jax._src.util.safe_map(read, jaxpr.outvars)\n \n def _compute_start_indices(\n-    block_mapping, loop_idx, *args, mesh, compiler_params, interpret_params):\n+    block_mapping, loop_idx, local_core_id,\n+    *args, mesh, compiler_params, interpret_params):\n   jaxpr = block_mapping.index_map_jaxpr\n   block_indices = _interpret_jaxpr(\n       jaxpr.jaxpr,\n@@ -1716,7 +1714,7 @@ def _compute_start_indices(\n       *loop_idx,\n       *args,\n       mesh=mesh,\n-      local_core_id=0,\n+      local_core_id=local_core_id,\n       compiler_params=compiler_params,\n       interpret_params=interpret_params,\n   )\n@@ -1748,12 +1746,19 @@ def _get_next_indices(grid, indices):\n     next_indices.append(jnp.where(carry, 0, i))\n   return tuple(reversed(next_indices))\n \n+def _get_indices(grid, loop_index):\n+  indices = []\n+  for dim_size in reversed(grid):\n+    i = loop_index % dim_size\n+    loop_index = loop_index // dim_size\n+    indices.append(i)\n+  return tuple(reversed(indices))\n \n-def _get_mosaic_params(compiler_params: dict[str, pallas_core.CompilerParams]) -> tpu_core.CompilerParams:\n+def _get_mosaic_params(compiler_params: dict[str, pallas_core.CompilerParams]) -> mosaic_core.CompilerParams:\n   try:\n-    return cast(tpu_core.CompilerParams, compiler_params['mosaic_tpu'])\n+    return cast(mosaic_core.CompilerParams, compiler_params['mosaic_tpu'])\n   except KeyError:\n-    return tpu_core.CompilerParams()\n+    return mosaic_core.CompilerParams()\n \n \n def _get_parallel_dim_semantics(\n@@ -1777,7 +1782,8 @@ def _get_parallel_dim_semantics(\n   mosaic_params = _get_mosaic_params(compiler_params)\n   if mosaic_params.dimension_semantics is None:\n     return (False,) * num_dimensions_in_grid\n-  result = tuple(ds == 'parallel' for ds in mosaic_params.dimension_semantics)\n+  result = tuple(ds in ('parallel', mosaic_core.PARALLEL)\n+                 for ds in mosaic_params.dimension_semantics)\n   for ds0, ds1 in zip(result[:-1], result[1:]):\n     if ds1 and not ds0:\n       raise ValueError(\n@@ -1916,6 +1922,52 @@ def _pad_to_block_dimension(value, block_shape, interpret_params):\n def get_interpret_effects():\n   return {callback._OrderedIOEffect}\n \n+def _thread_map(f, num_threads):\n+  if num_threads == 1:\n+    f(jnp.int32(0))\n+    return\n+\n+  def _f(core_index):\n+    f(core_index)\n+    return ()\n+  jaxpr = jax.make_jaxpr(_f)(jnp.int32(0))\n+\n+  _call_threadmap_callback(jaxpr.jaxpr, num_threads, *jaxpr.consts)\n+\n+def _run_jaxpr(jaxpr, consts, *args):\n+  def _run(jaxpr, consts, *args):\n+    jax_core.eval_jaxpr(jaxpr, consts, *args)\n+  traced = jax.jit(_run, static_argnums=(0,)).trace(jaxpr, consts, *args)\n+  traced.lower().compile()(consts, *args)\n+  return\n+\n+def _thread_map_callback(jaxpr, num_threads, consts):\n+  num_threads = int(num_threads)\n+  threads = []\n+  for i in range(num_threads):\n+    threads.append(\n+        threading.Thread(target=_run_jaxpr, args=(jaxpr, consts, jnp.int32(i))))\n+  for i in range(num_threads):\n+    threads[i].start()\n+  for i in range(num_threads):\n+    threads[i].join()\n+\n+def _call_threadmap_callback(jaxpr, num_threads, *consts):\n+  # NOTE: At runtime, _thread_map_callback will lower and compile the\n+  # given jaxpr.  (JAX's caches should ensure the jaxpr is only lowered and\n+  # compiled once.)\n+  #\n+  # TODO(jburnim): Would it be worth trying to lower/compile the jaxpr at\n+  # lowering/compilation time?  E.g., by using a custom primitive here, could\n+  # we lower/compile jaxpr at lowering time, and then pass the compiled\n+  # function to the callback?\n+  return callback.io_callback(\n+      functools.partial(_thread_map_callback, jaxpr),\n+      (),\n+      num_threads,\n+      consts,\n+      ordered=True)\n+\n def interpret_pallas_call(\n     *args,\n     jaxpr: jax_core.Jaxpr,\n@@ -1930,6 +1982,14 @@ def interpret_pallas_call(\n ):\n   del debug, cost_estimate, out_avals\n \n+  if isinstance(mesh, mosaic_core.TensorCoreMesh):\n+    # As a convenience for users, if we are interpreting a pl.core_map over a\n+    # TensorCoreMesh, we automatically set the number of cores per device so\n+    # that users don't have to specify it in the InterpretParams.\n+    assert len(mesh.shape) == 1\n+    interpret_params = dataclasses.replace(\n+        interpret_params, num_cores_per_device=mesh.devices.shape[0])\n+\n   # args contains: *dynamic_grid_sizes, *index, *inputs.  (No consts?)\n   dynamic_grid_args, scalars, input_args = split_list(\n       args,\n@@ -2101,9 +2161,14 @@ def interpret_pallas_call(\n     # Base case is always one iteration when grid is ()\n     num_iterations = 1\n \n-  randomized_grid_coordinates = _get_randomized_grid_coordinates(\n-      grid, compiler_params, interpret_params.random_seed  # type: ignore[arg-type]\n-  )\n+  if isinstance(mesh, mosaic_core.TensorCoreMesh):\n+    # We are interpreting a pl.core_map over a TensorCoreMesh, so we use a\n+    # fixed division of the grid between cores, instead of a random division.\n+    randomized_grid_coordinates = (jnp.array((), dtype=jnp.int32),) * len(grid)\n+  else:\n+    randomized_grid_coordinates = _get_randomized_grid_coordinates(\n+        grid, compiler_params, interpret_params.random_seed  # type: ignore[arg-type]\n+    )\n \n   parallel_dim_semantics = _get_parallel_dim_semantics(\n       compiler_params, len(grid)\n@@ -2122,272 +2187,271 @@ def interpret_pallas_call(\n       num_points_in_parallel_subgrid_per_core\n       * num_iterations_per_point_in_parallel_subgrid\n   )\n-\n-  def _get_local_grid_env(loop_idx):\n+  def _get_local_grid_env(grid_point):\n     if grid_mapping.local_grid_env is not None:\n-      return grid_mapping.local_grid_env(loop_idx, grid)\n+      return grid_mapping.local_grid_env(grid_point, grid)\n     else:\n       return tuple(\n           pallas_core.GridAxis(idx, b)\n-          for dim, (idx, b) in enumerate(zip(loop_idx, grid))\n+          for dim, (idx, b) in enumerate(zip(grid_point, grid))\n           if dim not in grid_mapping.vmapped_dims\n       )\n \n-  def body(\n-      carry: tuple[\n-          jnp.int32,\n-          tuple[jnp.int32, ...],\n-          jnp.ndarray,\n-          jnp.int32,\n-          jnp.int32,\n-          list[jnp.ndarray],\n-          list[jnp.ndarray],\n-      ],\n-  ) -> tuple[\n-      jnp.int32,\n-      tuple[jnp.int32, ...],\n-      jnp.ndarray,\n-      jnp.int32,\n-      jnp.int32,\n-      list[jnp.ndarray],\n-      list[jnp.ndarray],\n-  ]:\n-    \"\"\"Performs a single iteration of `jaxpr` in the device grid.\n-\n-    Execution of `jaxpr` is preceded by reading kernel input buffers and\n-    followed by writing kernel output buffers.\n-\n-    Args:\n-      carry: (iteration_idx, loop_idx, grid_point, prev_local_core_id,\n-              cur_local_core_id, prev_start_indices, cur_start_indices).\n-        - iteration_idx is the interation index.\n-        - loop_idx are the program ids for each grid axis.\n-        - grid_point is the grid point for the current loop iteration.\n-        - prev_local_core_id is the (device-local) core id from the previous\n-          loop iteration.\n-        - cur_local_core_id is the (device-local) core id for the current loop\n-          iteration.\n-        - prev_start_indices is a rank-1 array that contains the start indices\n-          for the slices of inputs and outputs processed in the previous loop\n-          iteration.\n-        - cur_start_indices is a rank-1 array that contains the start indices\n-          for the slices of inputs and outputs processed in the current loop\n-          iteration.\n-\n-        Note that by carrying the previous *and* current start indices between\n-        loop iterations, it suffices to compute only one list of start indices,\n-        i.e. `next_start_indices` (see below), per iteration.\n-\n-    Returns:\n-      The carry for the next iteration.\n-    \"\"\"\n-    (\n-        iteration_idx,\n-        loop_idx,\n-        grid_point,\n-        prev_local_core_id,\n-        cur_local_core_id,\n-        prev_start_indices,\n-        cur_start_indices,\n-    ) = carry\n-    if interpret_params.grid_point_recorder is not None:\n-      callback.io_callback(\n-          interpret_params.grid_point_recorder,\n-          (),\n+  def _execute_grid_for_core(core_index):\n+    # NOTE: We assume here that all parallel dimensions appear before all\n+    # arbitrary dimensions in the grid.  (We will have raised an error earlier\n+    # if this is not the case.)\n+    #\n+    # TODO(jburnim): Are we overusing nested local functions here?\n+    initial_iteration_idx = core_index * num_iterations_per_core\n+    loop_bound = jnp.minimum(\n+        (core_index + 1) * num_iterations_per_core, num_iterations)\n+\n+    def _body(\n+        carry: tuple[\n+            jnp.int32,\n+            tuple[jnp.int32, ...],\n+            jnp.ndarray,\n+            list[jnp.ndarray],\n+            list[jnp.ndarray],\n+        ],\n+    ) -> tuple[\n+        jnp.int32,\n+        tuple[jnp.int32, ...],\n+        jnp.ndarray,\n+        list[jnp.ndarray],\n+        list[jnp.ndarray],\n+    ]:\n+      \"\"\"Performs one execution of the kernel body.\n+\n+      Execution of `jaxpr` is preceded by reading kernel input buffers and\n+      followed by writing kernel output buffers.\n+\n+      Args:\n+        carry: (iteration_idx, loop_idx, grid_point, prev_start_indices,\n+                cur_start_indices).\n+          - iteration_idx: the interation index.\n+          - loop_idx: internal indices for looping over the grid.\n+          - grid_point: the current positions along all axes of the grid.\n+          - prev_start_indices: a rank-1 array that contains the start indices\n+            for the slices of inputs and outputs processed in the previous loop\n+            iteration.\n+          - cur_start_indices: a rank-1 array that contains the start indices\n+            for the slices of inputs and outputs processed in the current loop\n+            iteration.\n+\n+          Note that by carrying the previous *and* current start indices between\n+          loop iterations, it suffices to compute only one list of start indices,\n+          i.e. `next_start_indices` (see below), per iteration.\n+\n+      Returns:\n+        The carry for the next iteration.\n+      \"\"\"\n+      (\n+          iteration_idx,\n+          loop_idx,\n           grid_point,\n-          cur_local_core_id,\n-      )\n-\n-    next_local_core_id = (iteration_idx + 1) // num_iterations_per_core\n-\n-    with pallas_core.grid_env(_get_local_grid_env(loop_idx)):\n-      next_loop_idx = _get_next_indices(grid, loop_idx)\n-      next_grid_point = _get_grid_point(\n-          next_loop_idx, randomized_grid_coordinates\n-      )\n-      next_start_indices = [\n-          _compute_start_indices(\n-              bm,\n-              next_grid_point,\n-              *scalar_buffer_ids,\n-              mesh=mesh,\n-              compiler_params=compiler_params,\n-              interpret_params=interpret_params,\n-          )\n-          for bm in grid_mapping.block_mappings\n-      ]\n-\n-      # Copy slices of the input to the kernel buffers.\n-      def _store_slice_to_kernel_input(index, input_var):\n-        # Copy from the HBM buffer for the pallas_call input to the kernel\n-        # input buffer.\n-        # TODO(jburnim): Just use input_args[j] when the input is not aliased?\n-        transform = indexing.NDIndexer(\n-            indices=tuple(\n-                indexing.ds(st, sz) if not iid else st\n-                for st, sz, iid in zip(\n-                    cur_start_indices[index],\n-                    block_shapes[index],\n-                    is_squeeze_dim[index],\n-                )\n-            ),\n-            shape=input_args[index].shape,\n-            int_indexer_shape=(),\n-        )\n-        sliced_val = callback.io_callback(\n-            # TODO(jburnim): Pass source_info from the pallas_call, in case this\n-            # read is involved in a data race.\n-            get,\n-            jax.ShapeDtypeStruct(input_var.aval.shape, input_var.aval.dtype),\n-            device_id,\n-            cur_local_core_id,\n-            TPU_MEMORY_SPACE_IDXS[mosaic_core.MemorySpace.ANY],\n-            input_buffer_ids[index],\n-            (transform,),\n-            ordered=True,\n-        )\n+          prev_start_indices,\n+          cur_start_indices,\n+      ) = carry\n+      if interpret_params.grid_point_recorder is not None:\n         callback.io_callback(\n-            # TODO(jburnim): Pass source_info from the pallas_call, in case this\n-            # store is involved in a data race.\n-            store,\n-            (),\n-            device_id,\n-            cur_local_core_id,\n-            TPU_MEMORY_SPACE_IDXS[input_var.aval.memory_space],\n-            input_ids[index],\n+            interpret_params.grid_point_recorder,\n             (),\n-            sliced_val,\n-            ordered=True,\n+            grid_point,\n+            core_index,\n         )\n \n-      for j, var in enumerate(input_vars):\n-        if _is_any(var.aval.memory_space):\n-          continue\n-        assert len(cur_start_indices[j].shape) == 1\n-        assert len(prev_start_indices[j].shape) == 1\n-        jax.lax.cond(\n-            (iteration_idx == 0)\n-            | (cur_local_core_id != prev_local_core_id)\n-            | jax.lax.reduce_or(\n-                cur_start_indices[j] != prev_start_indices[j], axes=(0,)\n-            ),\n-            functools.partial(_store_slice_to_kernel_input, j, var),\n-            lambda: None,\n+      with pallas_core.grid_env(_get_local_grid_env(grid_point)):\n+        next_loop_idx = _get_next_indices(grid, loop_idx)\n+        next_grid_point = _get_grid_point(\n+            next_loop_idx, randomized_grid_coordinates\n         )\n+        next_start_indices = [\n+            _compute_start_indices(\n+                bm,\n+                next_grid_point,\n+                core_index,\n+                *scalar_buffer_ids,\n+                mesh=mesh,\n+                compiler_params=compiler_params,\n+                interpret_params=interpret_params,\n+            )\n+            for bm in grid_mapping.block_mappings\n+        ]\n+\n+        # Copy slices of the input to the kernel buffers.\n+        def _store_slice_to_kernel_input(index, input_var):\n+          # Copy from the HBM buffer for the pallas_call input to the kernel\n+          # input buffer.\n+          # TODO(jburnim): Just use input_args[j] when the input is not aliased?\n+          transform = indexing.NDIndexer(\n+              indices=tuple(\n+                  indexing.ds(st, sz) if not iid else st\n+                  for st, sz, iid in zip(\n+                      cur_start_indices[index],\n+                      block_shapes[index],\n+                      is_squeeze_dim[index],\n+                  )\n+              ),\n+              shape=input_args[index].shape,\n+              int_indexer_shape=(),\n+          )\n+          sliced_val = callback.io_callback(\n+              # TODO(jburnim): Pass source_info from the pallas_call, in case this\n+              # read is involved in a data race.\n+              get,\n+              jax.ShapeDtypeStruct(input_var.aval.shape, input_var.aval.dtype),\n+              device_id,\n+              core_index,\n+              TPU_MEMORY_SPACE_IDXS[mosaic_core.MemorySpace.ANY],\n+              input_buffer_ids[index],\n+              (transform,),\n+              ordered=True,\n+          )\n+          callback.io_callback(\n+              # TODO(jburnim): Pass source_info from the pallas_call, in case this\n+              # store is involved in a data race.\n+              store,\n+              (),\n+              device_id,\n+              core_index,\n+              TPU_MEMORY_SPACE_IDXS[input_var.aval.memory_space],\n+              input_ids[index],\n+              (),\n+              sliced_val,\n+              ordered=True,\n+          )\n \n-      # Invoke the kernel.\n-      _interpret_jaxpr(\n-          jaxpr,\n-          *kernel_buffer_ids,\n-          mesh=mesh,\n-          local_core_id=cur_local_core_id,\n-          compiler_params=compiler_params,\n-          interpret_params=interpret_params,\n-      )\n+        for j, var in enumerate(input_vars):\n+          if _is_any(var.aval.memory_space):\n+            continue\n+          assert len(cur_start_indices[j].shape) == 1\n+          assert len(prev_start_indices[j].shape) == 1\n+          jax.lax.cond(\n+              (iteration_idx == initial_iteration_idx)\n+              | jax.lax.reduce_or(\n+                  cur_start_indices[j] != prev_start_indices[j], axes=(0,)\n+              ),\n+              functools.partial(_store_slice_to_kernel_input, j, var),\n+              lambda: None,\n+          )\n \n-      # Copy from the kernel buffers to slices of the output in HBM.\n-      def _store_to_output_buffer(index, output_var):\n-        kernel_output_val = callback.io_callback(\n-            # TODO(jburnim): Pass source_info from the pallas_call, in case this\n-            # get is involved in a data race.\n-            get,\n-            output_var.aval,\n-            device_id,\n-            cur_local_core_id,\n-            TPU_MEMORY_SPACE_IDXS[output_var.aval.memory_space],\n-            kernel_output_ids[j],\n-            (),\n-            ordered=True,\n-        )\n-        transform = indexing.NDIndexer(\n-            indices=tuple(\n-                indexing.ds(st, sz) if not iid else st\n-                for st, sz, iid in zip(\n-                    cur_start_indices[num_inputs + index],\n-                    block_shapes[num_inputs + index],\n-                    is_squeeze_dim[num_inputs + index],\n-                )\n-            ),\n-            shape=output_vals[index].shape,\n-            int_indexer_shape=(index),\n-        )\n-        callback.io_callback(\n-            # TODO(jburnim): Pass source_info from the pallas_call, in case this\n-            # store is involved in a data race.\n-            store,\n-            (),\n-            device_id,\n-            cur_local_core_id,\n-            TPU_MEMORY_SPACE_IDXS[mosaic_core.MemorySpace.ANY],\n-            output_buffer_ids[index],\n-            (transform,),\n-            kernel_output_val,\n-            ordered=True,\n+        # Invoke the kernel.\n+        _interpret_jaxpr(\n+            jaxpr,\n+            *kernel_buffer_ids,\n+            mesh=mesh,\n+            local_core_id=core_index,\n+            compiler_params=compiler_params,\n+            interpret_params=interpret_params,\n         )\n \n-      for j, var in enumerate(output_vars):\n-        if _is_any(var.aval.memory_space):\n-          continue\n-        assert len(cur_start_indices[num_inputs + j].shape) == 1\n-        assert len(next_start_indices[num_inputs + j].shape) == 1\n-        jax.lax.cond(\n-            (iteration_idx + 1 == num_iterations)\n-            | (cur_local_core_id != next_local_core_id)\n-            | jax.lax.reduce_or(\n-                cur_start_indices[num_inputs + j]\n-                != next_start_indices[num_inputs + j],\n-                axes=(0,),\n-            ),\n-            functools.partial(_store_to_output_buffer, j, var),\n-            lambda: None,\n+        # Copy from the kernel buffers to slices of the output in HBM.\n+        def _store_to_output_buffer(index, output_var):\n+          kernel_output_val = callback.io_callback(\n+              # TODO(jburnim): Pass source_info from the pallas_call, in case this\n+              # get is involved in a data race.\n+              get,\n+              output_var.aval,\n+              device_id,\n+              core_index,\n+              TPU_MEMORY_SPACE_IDXS[output_var.aval.memory_space],\n+              kernel_output_ids[j],\n+              (),\n+              ordered=True,\n+          )\n+          transform = indexing.NDIndexer(\n+              indices=tuple(\n+                  indexing.ds(st, sz) if not iid else st\n+                  for st, sz, iid in zip(\n+                      cur_start_indices[num_inputs + index],\n+                      block_shapes[num_inputs + index],\n+                      is_squeeze_dim[num_inputs + index],\n+                  )\n+              ),\n+              shape=output_vals[index].shape,\n+              int_indexer_shape=(index),\n+          )\n+          callback.io_callback(\n+              # TODO(jburnim): Pass source_info from the pallas_call, in case this\n+              # store is involved in a data race.\n+              store,\n+              (),\n+              device_id,\n+              core_index,\n+              TPU_MEMORY_SPACE_IDXS[mosaic_core.MemorySpace.ANY],\n+              output_buffer_ids[index],\n+              (transform,),\n+              kernel_output_val,\n+              ordered=True,\n+          )\n+\n+        for j, var in enumerate(output_vars):\n+          if _is_any(var.aval.memory_space):\n+            continue\n+          assert len(cur_start_indices[num_inputs + j].shape) == 1\n+          assert len(next_start_indices[num_inputs + j].shape) == 1\n+          jax.lax.cond(\n+              (iteration_idx + 1 == loop_bound)\n+              | jax.lax.reduce_or(\n+                  cur_start_indices[num_inputs + j]\n+                  != next_start_indices[num_inputs + j],\n+                  axes=(0,),\n+              ),\n+              functools.partial(_store_to_output_buffer, j, var),\n+              lambda: None,\n+          )\n+\n+        return (\n+            iteration_idx + 1,\n+            next_loop_idx,\n+            next_grid_point,\n+            cur_start_indices,\n+            next_start_indices,\n         )\n \n-      return (\n-          iteration_idx + 1,\n-          next_loop_idx,\n-          next_grid_point,\n-          cur_local_core_id,\n-          next_local_core_id,\n-          cur_start_indices,\n-          next_start_indices,\n-      )\n+    initial_loop_idx = _get_indices(grid, initial_iteration_idx)\n+    initial_grid_point = _get_grid_point(\n+      initial_loop_idx, randomized_grid_coordinates)\n+    with pallas_core.grid_env(_get_local_grid_env(initial_grid_point)):\n+      initial_start_indices = [\n+          _compute_start_indices(\n+              bm,\n+              initial_grid_point,\n+              core_index,\n+              *scalar_buffer_ids,\n+              mesh=mesh,\n+              compiler_params=compiler_params,\n+              interpret_params=interpret_params,\n+          )\n+          for bm in grid_mapping.block_mappings\n+      ]\n \n-  initial_loop_idx = (jnp.int32(0),) * len(grid)\n-  initial_grid_point = _get_grid_point(\n-      initial_loop_idx, randomized_grid_coordinates\n-  )\n-  with pallas_core.grid_env(_get_local_grid_env(initial_loop_idx)):\n-    initial_start_indices = [\n-        _compute_start_indices(\n-            bm,\n+    _ = lax.while_loop(\n+        lambda carry: carry[0] < loop_bound,\n+        _body,\n+        (\n+            initial_iteration_idx,\n+            initial_loop_idx,\n             initial_grid_point,\n-            *scalar_buffer_ids,\n-            mesh=mesh,\n-            compiler_params=compiler_params,\n-            interpret_params=interpret_params,\n-        )\n-        for bm in grid_mapping.block_mappings\n-    ]\n-  # TODO(jburnim): Handle parallel grid dimensions + megacore.\n+            initial_start_indices,  # Previous start indices are ignored on the first iteration.\n+            initial_start_indices,\n+        ),\n+    )\n+\n+  # TODO(jburnim): Should we only create happens-before here from core 0 to\n+  # the other cores?\n   callback.io_callback(\n       _update_clocks_for_device_barrier, (), device_id, ordered=True\n   )\n-  _ = lax.while_loop(\n-      lambda carry: carry[0] < num_iterations,\n-      body,\n-      (\n-          jnp.int32(0),\n-          initial_loop_idx,\n-          initial_grid_point,\n-          jnp.int32(0),  # Previous core id is ignored on the first iteration.\n-          jnp.int32(0),  # Current core id is set to 0 for the first iteration.\n-          initial_start_indices,  # Previous start indices are ignored on the first iteration.\n-          initial_start_indices,\n-      ),\n-  )\n+\n+  _thread_map(_execute_grid_for_core, interpret_params.num_cores_per_device)\n+\n+  # TODO(jburnim): Should we only create happens-before here from the other\n+  # # cores to core 0?\n   callback.io_callback(\n-      _update_clocks_for_device_barrier, (), device_id, ordered=True\n-  )\n+      _update_clocks_for_device_barrier, (), device_id, ordered=True)\n \n   # Read the output from the allocated output buffers.\n   ret = [\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 5580cb2abd73..eb0eb05ac6a7 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -558,6 +558,7 @@ jax_multiplatform_test(\n     disable_configs = [\"cpu_shardy\"],\n     enable_backends = [\"cpu\"],\n     deps = [\n+        \"//jax:experimental\",\n         \"//jax:pallas\",\n         \"//jax:pallas_tpu\",\n     ] + py_deps([\ndiff --git a/tests/pallas/tpu_pallas_interpret_test.py b/tests/pallas/tpu_pallas_interpret_test.py\nindex 5bfca2270aa1..5fafb3007993 100644\n--- a/tests/pallas/tpu_pallas_interpret_test.py\n+++ b/tests/pallas/tpu_pallas_interpret_test.py\n@@ -21,6 +21,7 @@\n from collections.abc import Callable\n import dataclasses\n import functools\n+import threading\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -91,7 +92,7 @@ def _recorder(grid_point, core_id):\n \n   @property\n   def grid_points(self) -> list[ProcessedGridPoint]:\n-    return self._grid_points\n+    return sorted(self._grid_points, key=lambda x: x.core_id)\n \n \n # TODO(jburnim): Figure out how to safely run different instance of TPU\n@@ -471,40 +472,52 @@ def kernel_call_dynamic_parallel_dimension():\n     with self.assertRaises(jax.errors.ConcretizationTypeError):\n       kernel_call_dynamic_parallel_dimension()\n \n-  def test_core_map_over_one_core(self):\n-    mesh = pltpu.create_tensorcore_mesh(\"x\", num_cores=1)\n+  @parameterized.parameters(1, 2, 4)\n+  def test_core_map(self, num_cores):\n+    mesh = pltpu.create_tensorcore_mesh('x', num_cores=num_cores)\n+    interpret = pltpu.InterpretParams()\n \n     @jax.jit\n     def f(x):\n       y = jnp.zeros_like(x)\n       def inner(refs):\n         x_ref, y_ref = refs\n-        @pl.core_map(mesh, interpret=pltpu.InterpretParams())\n+        @pl.core_map(mesh, interpret=interpret)\n         def _():\n-          num_cores = jax.lax.psum(1, \"x\")\n+          num_cores = jax.lax.axis_size('x')\n           slc_size = 16 // num_cores\n-          def alloc(x_vmem_ref, y_vmem_ref, sem):\n-            core_index = jax.lax.axis_index(\"x\")\n+          def alloc(x_vmem_ref, y_vmem_ref, dma_sem, sem):\n+            # Barrier so we deadlock unless the core_map is actually parallel.\n+            for i in range(num_cores):\n+              pl.semaphore_signal(sem, 1, core_index=i)\n+            pl.semaphore_wait(sem, num_cores)\n+\n+            core_index = jax.lax.axis_index('x')\n             slc = pl.ds(core_index * slc_size, slc_size)\n             pltpu.async_copy(\n                 x_ref.at[slc],\n                 x_vmem_ref,\n-                sem,\n+                dma_sem,\n             ).wait()\n-            y = x_vmem_ref[...] + 1 + jax.lax.axis_index(\"x\")\n+            y = x_vmem_ref[...] + jax.lax.axis_index('x') + 1\n             y_vmem_ref[...] = y\n-            pltpu.async_copy(y_vmem_ref, y_ref.at[slc], sem).wait()\n+            pltpu.async_copy(y_vmem_ref, y_ref.at[slc], dma_sem).wait()\n           pl.run_scoped(\n               alloc,\n               pltpu.VMEM((slc_size, 128), x_ref.dtype),\n               pltpu.VMEM((slc_size, 128), y_ref.dtype),\n               pltpu.SemaphoreType.DMA,\n+              pltpu.SemaphoreType.REGULAR,\n           )\n       _, y = pl.run_state(inner)((x, y))\n       return y\n     x = jnp.arange(16 * 128, dtype=jnp.int32).reshape((16, 128))\n+    expected_out = (\n+        x.reshape((num_cores, -1, 128)) + 1\n+        + jnp.arange(num_cores, dtype=jnp.int32)[..., None, None]\n+    ).reshape(x.shape)\n     y = f(x)\n-    np.testing.assert_array_equal(y, x + 1)\n+    np.testing.assert_array_equal(y, expected_out)\n \n   def test_two_cores_along_parallel_dimension_with_race(self):\n     def kernel(x_ref, o_ref, vmem_ref):\n@@ -566,32 +579,43 @@ def kernel(x_ref, o_ref, vmem_ref):\n     np.testing.assert_allclose(y, 2.0 * x)\n \n   def test_parallel_dimension_and_multiple_cores(self):\n-    def kernel(s_ref, o_ref):\n+    def kernel(s_ref, in_ref, o_ref):\n+      # NOTE: diff should be 0.\n+      diff = in_ref[...] - jnp.float32(4 * pl.program_id(0) + pl.program_id(1))\n+\n       s = s_ref[0]\n       s_ref[0] = s + 1\n-      o_ref[:] = jax.lax.full_like(o_ref, s)\n+      o_ref[:] = jax.lax.full_like(o_ref, s) + diff\n \n     def kernel_call(s, num_cores_per_device, grid_point_recorder):\n+      block_input = jnp.repeat(\n+          jnp.repeat(\n+              jnp.arange(16, dtype=jnp.float32).reshape((4, 4)), 128, axis=1),\n+          8, axis=0)\n       return pl.pallas_call(\n           kernel,\n           out_shape=jax.ShapeDtypeStruct((32, 512), jnp.float32),\n           grid=(4, 4),\n-          in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n+          in_specs=[\n+              pl.BlockSpec(memory_space=pltpu.SMEM),\n+              pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n+          ],\n           out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n           interpret=pltpu.InterpretParams(\n               random_seed=12345,\n               num_cores_per_device=num_cores_per_device,\n               grid_point_recorder=grid_point_recorder,\n+              detect_races=True,\n           ),\n           compiler_params=pltpu.CompilerParams(\n               dimension_semantics=('parallel', 'arbitrary')\n           ),\n-      )(s)\n+      )(s, block_input)\n \n     with self.subTest('num_cores_per_device=1'):\n       with GridPointRecorderContext() as grid_point_recorder:\n         result = jax.jit(kernel_call, static_argnums=(1, 2))(\n-            jnp.zeros((1,), jnp.int32), 1, grid_point_recorder.get_recorder()\n+            jnp.zeros((1,), jnp.float32), 1, grid_point_recorder.get_recorder()\n         )\n         np.testing.assert_allclose(\n             result[::8, ::128],\n@@ -630,7 +654,7 @@ def kernel_call(s, num_cores_per_device, grid_point_recorder):\n     with self.subTest('num_cores_per_device=2'):\n       with GridPointRecorderContext() as grid_point_recorder:\n         result = jax.jit(kernel_call, static_argnums=(1, 2))(\n-            jnp.zeros((1,), jnp.int32), 2, grid_point_recorder.get_recorder()\n+            jnp.zeros((1,), jnp.float32), 2, grid_point_recorder.get_recorder()\n         )\n         np.testing.assert_allclose(\n             result[::8, ::128],\n@@ -669,7 +693,7 @@ def kernel_call(s, num_cores_per_device, grid_point_recorder):\n     with self.subTest('num_cores_per_device=3'):\n       with GridPointRecorderContext() as grid_point_recorder:\n         result = jax.jit(kernel_call, static_argnums=(1, 2))(\n-            jnp.zeros((1,), jnp.int32), 3, grid_point_recorder.get_recorder()\n+            jnp.zeros((1,), jnp.float32), 3, grid_point_recorder.get_recorder()\n         )\n         np.testing.assert_allclose(\n             result[::8, ::128],\n@@ -708,7 +732,7 @@ def kernel_call(s, num_cores_per_device, grid_point_recorder):\n     with self.subTest('num_cores_per_device=4'):\n       with GridPointRecorderContext() as grid_point_recorder:\n         result = jax.jit(kernel_call, static_argnums=(1, 2))(\n-            jnp.zeros((1,), jnp.int32), 4, grid_point_recorder.get_recorder()\n+            jnp.zeros((1,), jnp.float32), 4, grid_point_recorder.get_recorder()\n         )\n         np.testing.assert_allclose(\n             result[::8, ::128],\n@@ -747,7 +771,7 @@ def kernel_call(s, num_cores_per_device, grid_point_recorder):\n     with self.subTest('num_cores_per_device=5'):\n       with GridPointRecorderContext() as grid_point_recorder:\n         result = jax.jit(kernel_call, static_argnums=(1, 2))(\n-            jnp.zeros((1,), jnp.int32), 5, grid_point_recorder.get_recorder()\n+            jnp.zeros((1,), jnp.float32), 5, grid_point_recorder.get_recorder()\n         )\n         np.testing.assert_allclose(\n             result[::8, ::128],\n@@ -786,7 +810,7 @@ def kernel_call(s, num_cores_per_device, grid_point_recorder):\n     with self.subTest('num_cores_per_device=6'):\n       with GridPointRecorderContext() as grid_point_recorder:\n         result = jax.jit(kernel_call, static_argnums=(1, 2))(\n-            jnp.zeros((1,), jnp.int32), 6, grid_point_recorder.get_recorder()\n+            jnp.zeros((1,), jnp.float32), 6, grid_point_recorder.get_recorder()\n         )\n         np.testing.assert_allclose(\n             result[::8, ::128],\n@@ -822,5 +846,27 @@ def kernel_call(s, num_cores_per_device, grid_point_recorder):\n             ],\n         )\n \n+  def test_thread_map(self):\n+    barrier = threading.Barrier(8)\n+    lock = threading.Lock()\n+    concurrent_calls = [0]\n+    max_concurrent_calls = [0]\n+\n+    def _barrier():\n+      with lock:\n+        concurrent_calls[0] += 1\n+        max_concurrent_calls[0] = max(\n+            max_concurrent_calls[0], concurrent_calls[0])\n+      barrier.wait()\n+      with lock:\n+        concurrent_calls[0] -= 1\n+\n+    def f(core_index):\n+      del core_index\n+      jax.experimental.io_callback(_barrier, (), ordered=True)\n+\n+    mosaic_interpret._thread_map(f, 8)\n+    self.assertEqual(max_concurrent_calls[0], 8)\n+\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "jburnim",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Resurrect _pjit_lower's cache because it's important for python dispatch performance.\n\nPiperOrigin-RevId: 766811714",
        "output": "```diff\nCommit: 1216dacabdb938251cfa03f786aac56116727a36\nDate: 2025-06-03T21:02:19Z\nURL: https://github.com/jax-ml/jax/commit/1216dacabdb938251cfa03f786aac56116727a36\nFiles changed: 3\nAdditions: +13, Deletions: -6\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 229dee979d06..1f7cc19206a9 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -3164,6 +3164,7 @@ def clear_backends():\n   dispatch.xla_primitive_callable.cache_clear()\n   util.clear_all_caches()\n   pjit._infer_params_cached.cache_clear()\n+  pjit._pjit_lower_cached.cache_clear()\n   pjit._create_pjit_jaxpr.cache_clear()  # pytype: disable=attribute-error\n   pjit._cpp_pjit_cache_fun_only.clear()\n   pjit._cpp_pjit_cache_explicit_attributes.clear()\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex f2446e9a4939..572c2225af74 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -1923,7 +1923,13 @@ def call_impl_cache_miss(*args_, **kwargs_):\n pjit_p.def_impl(_pjit_call_impl)\n \n \n-def _pjit_lower(\n+def _pjit_lower(*args, **kwargs):\n+  util.test_event(\"pjit_lower\")\n+  return _pjit_lower_cached(*args, **kwargs)\n+\n+# This cache is important for python dispatch performance.\n+@weakref_lru_cache\n+def _pjit_lower_cached(\n     jaxpr: core.ClosedJaxpr,\n     in_shardings,\n     out_shardings,\n@@ -1939,7 +1945,6 @@ def _pjit_lower(\n     lowering_platforms: tuple[str, ...] | None,\n     lowering_parameters: mlir.LoweringParameters,\n     pgle_profiler: profiler.PGLEProfiler | None):\n-  util.test_event(\"pjit_lower\")\n   return pxla.lower_sharding_computation(\n       jaxpr, 'jit', name, in_shardings, out_shardings,\n       in_layouts, out_layouts, tuple(donated_invars),\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex b8f81438c104..2fe2c4696cbe 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -54,7 +54,8 @@\n     AUTO, UNSPECIFIED, NamedSharding, GSPMDSharding, PositionalSharding,\n     SingleDeviceSharding, parse_flatten_op_sharding)\n from jax._src.pjit import (pjit, mesh_cast, auto_axes, explicit_axes,\n-                           use_auto_axes, use_explicit_axes, reshard)\n+                           use_auto_axes, use_explicit_axes, reshard,\n+                           _pjit_lower_cached)\n from jax._src.layout import Format, DeviceLocalLayout as DLL\n from jax._src.named_sharding import DuplicateSpecError\n from jax._src import mesh as mesh_lib\n@@ -2306,13 +2307,13 @@ def add(x, y):\n       return x + y\n \n     out = add(a, b)\n-    cache_info1 = pxla._cached_lowering_to_hlo.cache_info()\n+    cache_info1 = _pjit_lower_cached.cache_info()\n     self.assertIsInstance(out, array.ArrayImpl)\n     self.assertArraysEqual(out, a + b)\n     self.assertFalse(out._committed)\n \n     out2 = add(out, out)\n-    cache_info2 = pxla._cached_lowering_to_hlo.cache_info()\n+    cache_info2 = _pjit_lower_cached.cache_info()\n     self.assertIsInstance(out2, array.ArrayImpl)\n     self.assertArraysEqual(out2, 2 * (a + b))\n     self.assertFalse(out2._committed)\n@@ -2322,7 +2323,7 @@ def add(x, y):\n \n     c = jax.device_put(a, jax.devices()[0])\n     out3 = add(c, c)\n-    cache_info3 = pxla._cached_lowering_to_hlo.cache_info()\n+    cache_info3 = _pjit_lower_cached.cache_info()\n     self.assertArraysEqual(out3, 2 * c)\n     self.assertTrue(out3._committed)\n \n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix sharding-in-types + lax.map usage when batch_size usage has a remainder left. Fixes https://github.com/jax-ml/jax/issues/29195\n\nPiperOrigin-RevId: 766801968",
        "output": "```diff\nCommit: e20b3a4e34f211a3a7d2905b730cc282dbe2c170\nDate: 2025-06-03T20:39:44Z\nURL: https://github.com/jax-ml/jax/commit/e20b3a4e34f211a3a7d2905b730cc282dbe2c170\nFiles changed: 3\nAdditions: +45, Deletions: -14\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex 2452d12dad5e..d8481afe872a 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -1533,7 +1533,7 @@ def normalize(self) -> AbstractValue:\n   def update(self, **kwargs):\n     raise NotImplementedError(\"must override\")\n \n-  def str_short(self, short_dtypes=False):\n+  def str_short(self, short_dtypes=False, mesh_axis_types=False):\n     return str(self)\n \n # For type signatures involving dynamic shapes, we use lists of abstract values\n@@ -1790,7 +1790,7 @@ def __str__(self):\n   _oct     = concretization_function_error(oct)\n   _index   = concretization_function_error(operator.index)\n \n-  def str_short(self, short_dtypes=False) -> str:\n+  def str_short(self, short_dtypes=False, mesh_axis_types=False) -> str:\n     return dtypes.short_dtype_name(self.dtype) if short_dtypes else self.dtype.name\n \n   def update_weak_type(self, weak_type):\n@@ -2191,7 +2191,7 @@ def __init__(self, shape, dtype, weak_type=False):\n                   0 if any(type(d) is int and d == 0 for d in self.shape)\n                   else math.prod(self.shape))\n \n-  def str_short(self, short_dtypes=False) -> str:\n+  def str_short(self, short_dtypes=False, mesh_axis_types=False) -> str:\n     del short_dtypes  # ignored\n     shape = f'{\",\".join(str(d) for d in self.shape)}' if self.shape else ''\n     dtype = dtypes.short_dtype_name(self.dtype)\n@@ -2358,7 +2358,7 @@ def _freeze_impl(ref):\n   return ref[()]\n \n class AbstractToken(AbstractValue):\n-  def str_short(self, short_dtypes=False): return 'Tok'\n+  def str_short(self, short_dtypes=False, mesh_axis_types=False): return 'Tok'\n   def to_tangent_aval(self): return self\n abstract_token: AbstractToken = AbstractToken()\n \ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 146f27e5d2e7..985c5ba52294 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -53,6 +53,8 @@\n     _initial_style_jaxpr_attrs, _make_closed_jaxpr_attrs, _prune_zeros,\n     _typecheck_param)\n from jax._src.lax.other import logaddexp\n+from jax._src.pjit import auto_axes, PartitionSpec as P\n+from jax._src.mesh import get_abstract_mesh\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n from jax._src.state import discharge as state_discharge\n@@ -2507,24 +2509,41 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n ### map and miscellaneous rules\n \n+def _scan_leaf(leaf, batch_elems, num_batches, batch_size):\n+  def f(l):\n+    return l[:batch_elems].reshape(num_batches, batch_size, *leaf.shape[1:])\n+\n+  aval = core.typeof(leaf)\n+  if aval.sharding.spec[0] is not None:\n+    raise ValueError(\n+        '0th dimension of leaf passed to `jax.lax.map` should be replicated.'\n+        f' Got {aval.str_short(True, True)}')\n+  if get_abstract_mesh()._are_all_axes_explicit:\n+    out_s = aval.sharding.with_spec(P(None, None, *aval.sharding.spec[1:]))\n+    return auto_axes(f, out_sharding=out_s)(leaf)\n+  return f(leaf)\n+\n+def _remainder_leaf(leaf, batch_elems):\n+  def f(l):\n+    return l[batch_elems:]\n+  if get_abstract_mesh()._are_all_axes_explicit:\n+    return auto_axes(f, out_sharding=core.typeof(leaf).sharding)(leaf)\n+  return f(leaf)\n+\n def _batch_and_remainder(x, batch_size: int):\n   leaves, treedef = tree_flatten(x)\n   if not leaves:\n     return x, None\n   num_batches, remainder = divmod(leaves[0].shape[0], batch_size)\n-  total_batch_elems = num_batches * batch_size\n+  batch_elems = num_batches * batch_size\n   if remainder:\n-    scan_leaves, remainder_leaves = [], []\n-    for leaf in leaves:\n-      scan_leaves.append(leaf[:total_batch_elems].reshape(\n-          num_batches, batch_size, *leaf.shape[1:]))\n-      remainder_leaves.append(leaf[total_batch_elems:])\n+    scan_leaves, remainder_leaves = unzip2(\n+        [(_scan_leaf(leaf, batch_elems, num_batches, batch_size),\n+          _remainder_leaf(leaf, batch_elems)) for leaf in leaves])\n     return treedef.unflatten(scan_leaves), treedef.unflatten(remainder_leaves)\n   else:\n-    scan_leaves = [\n-        leaf[:total_batch_elems].reshape(num_batches, batch_size, *leaf.shape[1:])\n-        for leaf in leaves\n-    ]\n+    scan_leaves = tuple(_scan_leaf(leaf, batch_elems, num_batches, batch_size)\n+                        for leaf in leaves)\n     return treedef.unflatten(scan_leaves), None\n \n @api_boundary\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex c4d36ab78d10..b8f81438c104 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7907,6 +7907,18 @@ def simple_func(w, x):\n \n     jax.lax.map(lambda _x: simple_func(w, _x), x, batch_size=2)  # doesn't crash\n \n+  @config.numpy_rank_promotion('allow')\n+  @jtu.with_explicit_mesh((2,), ('x',))\n+  def test_lax_map_remainder(self, mesh):\n+    def simple_func(w, x):\n+      return jnp.sum(w * x, axis=-1)\n+\n+    w = jax.device_put(np.arange(4, dtype=np.float32), P())\n+    x = jax.device_put(np.ones((5, 2, 4), dtype=np.float32),\n+                       P(None, 'x', None))\n+\n+    jax.lax.map(lambda _x: simple_func(w, _x), x, batch_size=2)  # doesn't crash\n+\n \n @jtu.pytest_mark_if_available('multiaccelerator')\n class PJitErrorTest(jtu.JaxTestCase):\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reduce block sizes in attention to prevent running out of shared memory on L4.\n\nNew values (though small) show very good performance on ampere.\n\nPiperOrigin-RevId: 766797437",
        "output": "```diff\nCommit: b7adddff1793e5434c1e7c2189d34ec728d49736\nDate: 2025-06-03T20:28:12Z\nURL: https://github.com/jax-ml/jax/commit/b7adddff1793e5434c1e7c2189d34ec728d49736\nFiles changed: 2\nAdditions: +9, Deletions: -9\ndiff --git a/jax/experimental/pallas/ops/gpu/attention.py b/jax/experimental/pallas/ops/gpu/attention.py\nindex ae429be5d73a..4782fc31226e 100644\n--- a/jax/experimental/pallas/ops/gpu/attention.py\n+++ b/jax/experimental/pallas/ops/gpu/attention.py\n@@ -57,10 +57,10 @@ def get_default(cls):\n     return BlockSizes(\n         block_q=128,\n         block_k=128,\n-        block_q_dkv=128,\n-        block_kv_dkv=128,\n-        block_q_dq=128,\n-        block_kv_dq=128,\n+        block_q_dkv=32,\n+        block_kv_dkv=32,\n+        block_q_dq=32,\n+        block_kv_dq=32,\n     )\n \n   @property\ndiff --git a/tests/pallas/gpu_ops_test.py b/tests/pallas/gpu_ops_test.py\nindex cc2d15a8fdee..edda5cf686db 100644\n--- a/tests/pallas/gpu_ops_test.py\n+++ b/tests/pallas/gpu_ops_test.py\n@@ -231,9 +231,9 @@ def impl(q, k, v):\n           (\n               (\"block_q\", 128),\n               (\"block_k\", 128),\n-              (\"block_q_dkv\", 128),\n-              (\"block_kv_dkv\", 128),\n-              (\"block_q_dq\", 128),\n+              (\"block_q_dkv\", 32),\n+              (\"block_kv_dkv\", 32),\n+              (\"block_q_dq\", 32),\n               (\"block_kv_dq\", 128),\n           ),\n           (\n@@ -248,8 +248,8 @@ def impl(q, k, v):\n               (\"block_q\", 64),\n               (\"block_k\", 128),\n               (\"block_q_dkv\", 64),\n-              (\"block_kv_dkv\", 128),\n-              (\"block_q_dq\", 128),\n+              (\"block_kv_dkv\", 32),\n+              (\"block_q_dq\", 32),\n               (\"block_kv_dq\", 64),\n           ),\n       ),\n\n```",
        "from_id": [
            "rdyro",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Fix `bitcast` logic in `shfl_bfly`.\n\nWe were not testing the logic for non-32-bit-wide dtypes, and as a result\nmissed that one of the `bitcast`s was converting between two types with\ndifferent bitwidths.\n\nPiperOrigin-RevId: 766775563",
        "output": "```diff\nCommit: 7e0913f8149e4542e020256640c6ed3fd185f759\nDate: 2025-06-03T19:32:29Z\nURL: https://github.com/jax-ml/jax/commit/7e0913f8149e4542e020256640c6ed3fd185f759\nFiles changed: 2\nAdditions: +8, Deletions: -6\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex a76e077ff463..ac002aa8bffe 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -1398,7 +1398,7 @@ def shfl_bfly(x: ir.Value, distance: int | ir.Value):\n   )\n   if (x_bitwidth := bitwidth(result_type)) < 32:\n     bits_ty = ir.IntegerType.get_signless(x_bitwidth)\n-    y_vec = bitcast(y, ir.VectorType.get((32 // x_bitwidth,), x.type))\n+    y_vec = bitcast(y, ir.VectorType.get((32 // x_bitwidth,), bits_ty))\n     y = vector.extractelement(y_vec, position=c(0, index))\n   return bitcast(y, result_type)\n \ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 3bb0c1b9fe78..31262dcae44d 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -3732,20 +3732,22 @@ def strategy(draw):\n         shape, layout = draw(shape_and_tiled_layout(vector_transfer=True))\n         rank = len(shape)\n         reduced_dims = draw(hps.sets(hps.integers(0, rank - 1), min_size=1))\n-        return shape, layout, tuple(reduced_dims)\n+        dtype = draw(hps.sampled_from([jnp.float32, jnp.bfloat16]))\n+        return shape, layout, tuple(reduced_dims), dtype\n \n       @hp.given(strategy())\n       def run(args):\n-        shape, layout, reduced_dims = args\n+        shape, layout, reduced_dims, dtype = args\n         out_shape = list(shape)\n         for d in sorted(reduced_dims, reverse=True):\n           del out_shape[d]\n         def kernel(ctx, src, dst, scratch):\n+          del ctx\n           arr = fa.FragmentedArray.load_untiled(src, layout=layout, optimized=False)\n           arr.reduce(\"max\", reduced_dims, scratch).store_untiled(dst, optimized=False)\n-        x = jax.random.normal(jax.random.key(1234), shape, jnp.float32)\n-        out_type = jax.ShapeDtypeStruct(out_shape, jnp.float32)\n-        scratch_type = jax.ShapeDtypeStruct((2048,), jnp.float32)\n+        x = jax.random.normal(jax.random.key(1234), shape, dtype)\n+        out_type = jax.ShapeDtypeStruct(out_shape, dtype)\n+        scratch_type = jax.ShapeDtypeStruct((2048,), dtype)\n         hp.assume(layout.vector_length <= 16)  # Otherwise we run out of scratch\n         try:\n           result = mgpu.as_gpu_kernel(\n\n```",
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Prototype of cross-host device transfers in IFRT-PJRT.\n\nFor now it only works with the TFRT TPU runtime, because other PjRt plugins don't implement the necessary APIs. The per-shard indices of the source and destination shardings must be the same, and all shards must require cross-host transfers (support for a mixture of cross-host and host-local transfers is forthcoming).\n\nTransfers take place via the xla::ifrt::PjRtClient::CopyArrays API, which copies the buffers from a set of arrays to a new device list. The distributed KV store from the coordination service is used to store metadata for cross-host transfers. The receiving process populates the store with a descriptor, and the sending process reads it and completes the send.\n\nPiperOrigin-RevId: 766765989",
        "output": "```diff\nCommit: 6cd196a5db22b8db0ed4000e4cf67ad748bf52f3\nDate: 2025-06-03T19:08:49Z\nURL: https://github.com/jax-ml/jax/commit/6cd196a5db22b8db0ed4000e4cf67ad748bf52f3\nFiles changed: 1\nAdditions: +46, Deletions: -11\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex b5e588cbc10e..b9ef8f49f801 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -356,16 +356,6 @@ def _different_device_order_reshard(x, target_sharding, copy: CopySemantics):\n     return api.jit(_identity_fn, out_shardings=target_sharding,\n                    donate_argnums=donate_argnums)(x)\n \n-  if inp_sharding.device_set != target_sharding.device_set:\n-    inp_ids = [d.id for d in inp_sharding._device_assignment]\n-    inp_plat = inp_sharding._device_assignment[0].platform.upper()\n-    target_ids = [d.id for d in target_sharding._device_assignment]\n-    target_plat = target_sharding._device_assignment[0].platform.upper()\n-    raise ValueError(\"Input and target sharding should have the same set of \"\n-                     f\"devices. Got input's device set ids: {inp_ids} on \"\n-                     f\"platform {inp_plat} and target sharding's device set \"\n-                     f\"ids: {target_ids} on platform {target_plat}\")\n-\n   if inp_sharding.is_fully_replicated:\n     permute_order = None\n   else:\n@@ -389,6 +379,25 @@ def _reorder_shards(x, new_s, copy_semantics: CopySemantics):\n   return xc.reorder_shards(x, new_s, xc_copy_semantics)  # type: ignore\n \n \n+@util.cache()\n+def _is_supported_cross_host_transfer(ndim, src_sharding, dst_sharding):\n+  \"\"\"Returns True if src->dst is a supported cross-host transfer.\"\"\"\n+  backend = xla_bridge.get_backend()\n+  # There is experimental support for cross-host device transfers on TFRT TPU\n+  # backends only.\n+  if (xla_bridge.process_count() == 1 or backend.platform != \"tpu\" or\n+      \"TFRT TPU\" not in backend.platform_version):\n+    return False\n+  if (src_sharding._to_xla_hlo_sharding(ndim) !=\n+      dst_sharding._to_xla_hlo_sharding(ndim)):\n+    return False\n+  # This check excludes the case where the source and destination shardings\n+  # have the same process index sets but there are shards that require\n+  # cross-host transfers. This case is supportable but expensive to check for.\n+  return (src_sharding._internal_device_list.process_indices !=\n+          dst_sharding._internal_device_list.process_indices)\n+\n+\n @dataclasses.dataclass(frozen=True)\n class _DeferredShardArg:\n   \"\"\"Deferred call to `pxla.shard_args`.\n@@ -419,7 +428,8 @@ def _device_put_sharding_impl(x, aval, device, copy):\n       return x\n \n     if (not s.is_fully_addressable and\n-        isinstance(x, array.ArrayImpl) and not x.is_fully_addressable):\n+        isinstance(x, array.ArrayImpl) and not x.is_fully_addressable and\n+        s.device_set == x.sharding.device_set):\n       assert isinstance(s, Sharding)\n       return _different_device_order_reshard(x, s, copy)\n \n@@ -430,7 +440,32 @@ def _device_put_sharding_impl(x, aval, device, copy):\n       assert isinstance(s, Sharding)\n       return _different_device_order_reshard(x, s, copy)\n \n+    # There is experimental support for cross-host device transfers on TFRT TPU.\n+    if (isinstance(x, array.ArrayImpl) and x._committed\n+        and _is_supported_cross_host_transfer(x.ndim, x.sharding, s)):\n+      return xc.batched_copy_array_to_devices_with_sharding(\n+          [x], [s._internal_device_list], [s],  # pytype: disable=attribute-error\n+          pxla.to_xc_copy_semantics([copy]))[0]\n+\n     if not s.is_fully_addressable:\n+      # If both the source and target shardings are not fully addressable and\n+      # one of the above conditions has not been met, then assume that the user\n+      # is attempting a different device order reshard.\n+      if (isinstance(x, array.ArrayImpl) and not x.is_fully_addressable\n+          and s.device_set != x.sharding.device_set):\n+        inp_ids = [d.id for d in x.sharding._device_assignment]\n+        inp_plat = x.sharding._device_assignment[0].platform.upper()\n+        target_ids = [d.id for d in s._device_assignment]\n+        target_plat = s._device_assignment[0].platform.upper()\n+        raise ValueError(\n+            \"For a cross-host reshard in multi-controller JAX, input and target\"\n+            \" sharding should have the same set of devices. Got input's device\"\n+            f\" set ids: {inp_ids} on platform {inp_plat} and target sharding's\"\n+            f\" device set ids: {target_ids} on platform {target_plat}.\\n\\n\"\n+            \"There is experimental support for cross-host transfers with \"\n+            \"different device sets, when input/output shardings have the same \"\n+            \"indices and layouts, in the TFRT TPU runtime only.\")\n+\n       if ((isinstance(x, array.ArrayImpl) and not x._committed) or\n           type(x) in array_types or type(x) in dtypes.python_scalar_dtypes):\n         # If all hosts participate in the sharding, assert that the input is the\n\n```",
        "from_id": [
            "emilyfertig",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[jaxlib] Add `PyClient::Compile` method that returns an unloaded `PyExecutable`.\n\n- Introduce `xla::PyExecutable` so we have a public constructor for returning an `xla::nb_class_ptr` from `PyClient::Compile`. There might be other acceptable ways of accomplishing this, but we have a `PyLoadedExecutable` object, so going for consistency.\n- Migrate uses of `ifrt::Executable` to `ifrt::ExecutableRef` (an alias for `std::shared_ptr<ifrt::Executable>`). There might be undesirable consequences for doing this (i.e., a reason why this wasn't migrated before).\n\nPiperOrigin-RevId: 766757937",
        "output": "```diff\nCommit: 7dd0344f9db12ef676280acc51c30d6496ffc2ac\nDate: 2025-06-03T18:48:57Z\nURL: https://github.com/jax-ml/jax/commit/7dd0344f9db12ef676280acc51c30d6496ffc2ac\nFiles changed: 6\nAdditions: +113, Deletions: -31\ndiff --git a/jaxlib/py_client.cc b/jaxlib/py_client.cc\nindex 98bde8c27396..8e78f024e1ae 100644\n--- a/jaxlib/py_client.cc\n+++ b/jaxlib/py_client.cc\n@@ -83,6 +83,7 @@ limitations under the License.\n #include \"xla/python/nb_numpy.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_array.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_client.h\"\n+#include \"xla/python/pjrt_ifrt/pjrt_executable.h\"\n #include \"xla/python/pjrt_ifrt/xla_compiler.h\"\n #include \"xla/python/pprof_profile_builder.h\"\n #include \"xla/python/types.h\"\n@@ -451,6 +452,29 @@ PyClient::CompileAndLoadIfrtProgram(\n       std::move(traceback), std::move(fingerprint));\n }\n \n+/* static */ absl::StatusOr<nb_class_ptr<PyExecutable>> PyClient::Compile(\n+    nb_class_ptr<PyClient> client, std::string mlir_module,\n+    ifrt::DeviceListRef executable_devices, CompileOptions options) {\n+  ifrt::ExecutableRef executable_ref;\n+  {\n+    mlir::MLIRContext context;\n+    nb::gil_scoped_release gil_release;\n+    TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n+                        ParseMlirModuleString(mlir_module, context));\n+    TF_ASSIGN_OR_RETURN(\n+        auto topology,\n+        client->ifrt_client()->GetTopologyForDevices(executable_devices));\n+    auto xla_options = std::make_unique<ifrt::XlaCompileOptions>(\n+        options, std::move(executable_devices));\n+    TF_ASSIGN_OR_RETURN(auto pjrt_executable,\n+                        PjRtCompile(std::move(options), module.get(),\n+                                    *topology->description()));\n+    TF_ASSIGN_OR_RETURN(executable_ref, ifrt::PjRtExecutable::Create(\n+                                            std::move(pjrt_executable)));\n+  }\n+  return make_nb_class<PyExecutable>(executable_ref);\n+}\n+\n /* static */ absl::StatusOr<nb_class_ptr<PyLoadedExecutable>>\n PyClient::CompileAndLoad(nb_class_ptr<PyClient> client, std::string mlir_module,\n                          ifrt::DeviceListRef executable_devices,\ndiff --git a/jaxlib/py_client.h b/jaxlib/py_client.h\nindex 7f70fa4f111b..520fbf8b1e59 100644\n--- a/jaxlib/py_client.h\n+++ b/jaxlib/py_client.h\n@@ -50,6 +50,7 @@ namespace xla {\n \n class PyClient;\n class PyLoadedExecutable;\n+class PyExecutable;\n class PyArray;\n class PyDevice;\n class PyMemorySpace;\n@@ -167,6 +168,10 @@ class PyClient {\n                             std::unique_ptr<ifrt::Program> ifrt_program,\n                             std::unique_ptr<ifrt::CompileOptions> ifrt_options);\n \n+  static absl::StatusOr<nb_class_ptr<PyExecutable>> Compile(\n+      nb_class_ptr<PyClient> client, std::string mlir_module,\n+      ifrt::DeviceListRef executable_devices, CompileOptions options);\n+\n   static absl::StatusOr<nb_class_ptr<PyLoadedExecutable>> CompileAndLoad(\n       nb_class_ptr<PyClient> client, std::string mlir_module,\n       ifrt::DeviceListRef executable_devices, CompileOptions options,\ndiff --git a/jaxlib/py_compile_only_client.cc b/jaxlib/py_compile_only_client.cc\nindex 274f57acba00..bcae15cd6438 100644\n--- a/jaxlib/py_compile_only_client.cc\n+++ b/jaxlib/py_compile_only_client.cc\n@@ -33,6 +33,7 @@ limitations under the License.\n #include \"jaxlib/nb_class_ptr.h\"\n #include \"jaxlib/py_client.h\"\n #include \"jaxlib/py_device_list.h\"\n+#include \"jaxlib/py_executable.h\"\n #include \"xla/pjrt/mlir_to_hlo.h\"\n #include \"xla/pjrt/pjrt_compiler.h\"\n #include \"xla/pjrt/pjrt_executable.h\"\n@@ -70,7 +71,7 @@ class CompileOnlyPyClient : public PyClient {\n     return client;\n   }\n \n-  absl::StatusOr<ifrt::ExecutableRef> CompileUnloaded(\n+  absl::StatusOr<nb_class_ptr<PyExecutable>> CompileUnloaded(\n       absl::string_view mlir_module, ifrt::DeviceListRef executable_devices,\n       CompileOptions options, std::vector<nb::capsule> host_callbacks) {\n     if (!host_callbacks.empty()) {\n@@ -78,22 +79,26 @@ class CompileOnlyPyClient : public PyClient {\n           \"Compiling with host_callbacks not available with compile-only \"\n           \"client.\");\n     }\n-    nb::gil_scoped_release gil_release;\n-    mlir::MLIRContext context;\n-    TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n-                        ParseMlirModuleString(mlir_module, context));\n-    auto* ifrt_client =\n-        llvm::dyn_cast_or_null<CompileOnlyIfRtClient>(this->ifrt_client());\n-    CHECK(ifrt_client) << \"CompileOnlyPyClient requires ifrt_client be a \"\n-                          \"CompileOnlyIfRtClient\";\n-    auto xla_options = std::make_unique<ifrt::XlaCompileOptions>(\n-        options, std::move(executable_devices));\n-    TF_ASSIGN_OR_RETURN(auto executable,\n-                        PjRtCompile(std::move(options), module.get(),\n-                                    *ifrt_client->topology().description()));\n-    TF_ASSIGN_OR_RETURN(auto ifrt_executable,\n-                        ifrt::PjRtExecutable::Create(std::move(executable)));\n-    return ifrt::ExecutableRef(std::move(ifrt_executable));\n+    ifrt::ExecutableRef ifrt_executable;\n+    {\n+      nb::gil_scoped_release gil_release;\n+      mlir::MLIRContext context;\n+      TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n+                          ParseMlirModuleString(mlir_module, context));\n+      auto* ifrt_client =\n+          llvm::dyn_cast_or_null<CompileOnlyIfRtClient>(this->ifrt_client());\n+      CHECK(ifrt_client) << \"CompileOnlyPyClient requires ifrt_client be a \"\n+                            \"CompileOnlyIfRtClient\";\n+\n+      auto xla_options = std::make_unique<ifrt::XlaCompileOptions>(\n+          options, std::move(executable_devices));\n+      TF_ASSIGN_OR_RETURN(auto executable,\n+                          PjRtCompile(std::move(options), module.get(),\n+                                      *ifrt_client->topology().description()));\n+      TF_ASSIGN_OR_RETURN(ifrt_executable,\n+                          ifrt::PjRtExecutable::Create(std::move(executable)));\n+    }\n+    return make_nb_class<PyExecutable>(ifrt_executable);\n   }\n \n  private:\ndiff --git a/jaxlib/py_executable.h b/jaxlib/py_executable.h\nindex 6354edaf9a3e..fed6552a9eb5 100644\n--- a/jaxlib/py_executable.h\n+++ b/jaxlib/py_executable.h\n@@ -126,7 +126,55 @@ class PyExecuteResults {\n \n using ExecuteShardedArg = std::variant<PyArray, std::vector<PyArray>>;\n \n-// Python wrapper around PjRtExecutable. We use a wrapper class:\n+// Thin Python wrapper around ifrt::ExecutableRef. We use a wrapper class:\n+// a) Standardize around ifrt::ExecutableRef, which is\n+//    std::shared_ptr<ifrt::Executable>.\n+// b) Concrete subclasses of ifrt::Executable have protected constructors.\n+class PyExecutable {\n+ public:\n+  PyExecutable(ifrt::ExecutableRef ifrt_executable)\n+      : ifrt_executable_(std::move(ifrt_executable)) {};\n+  ~PyExecutable() = default;\n+\n+  // NOTE(dsuo): For now, we only expose the ifrt::Executable members required\n+  // by the Python bindings.\n+  absl::StatusOr<std::vector<std::shared_ptr<HloModule>>> GetHloModules()\n+      const {\n+    return ifrt_executable_->GetHloModules();\n+  }\n+  absl::StatusOr<std::vector<std::vector<absl::string_view>>>\n+  GetOutputMemoryKinds() const {\n+    return ifrt_executable_->GetOutputMemoryKinds();\n+  }\n+  std::optional<std::vector<OpSharding>> GetOutputShardings() const {\n+    return ifrt_executable_->GetOutputShardings();\n+  }\n+  absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+  GetParameterLayouts() const {\n+    return ifrt_executable_->GetParameterLayouts();\n+  }\n+  absl::StatusOr<std::vector<std::shared_ptr<const xla::PjRtLayout>>>\n+  GetOutputLayouts() const {\n+    return ifrt_executable_->GetOutputLayouts();\n+  }\n+  std::optional<std::vector<OpSharding>> GetParameterShardings() const {\n+    return ifrt_executable_->GetParameterShardings();\n+  }\n+  absl::StatusOr<CompiledMemoryStats> GetCompiledMemoryStats() const {\n+    return ifrt_executable_->GetCompiledMemoryStats();\n+  }\n+  absl::StatusOr<std::string> Serialize() const {\n+    return ifrt_executable_->Serialize();\n+  }\n+  absl::StatusOr<ifrt::AttributeMap> GetCostAnalysis() const {\n+    return ifrt_executable_->GetCostAnalysis();\n+  }\n+\n+ private:\n+  ifrt::ExecutableRef ifrt_executable_;\n+};\n+\n+// Python wrapper around ifrt::LoadedExecutableRef. We use a wrapper class:\n // a) to keep the PyClient alive via a std::shared_ptr<>\n // b) to add Python-specific functionality.\n class PyLoadedExecutable {\n@@ -162,8 +210,8 @@ class PyLoadedExecutable {\n   }\n \n   // Takes args indexed by argid then deviceid, transposes them, and passes to\n-  // PjRtExecutable::Execute. The result is similarly transposed back into the\n-  // argid,deviceid format.\n+  // ifrt::LoadedExecutable::Execute. The result is similarly transposed back\n+  // into the argid,deviceid format.\n   // args is [num_args x num_devices].\n   absl::StatusOr<PyExecuteResults> ExecuteSharded(\n       std::vector<ExecuteShardedArg> args, bool with_tokens);\ndiff --git a/jaxlib/xla.cc b/jaxlib/xla.cc\nindex d97c6868a04b..186bcaf2efd3 100644\n--- a/jaxlib/xla.cc\n+++ b/jaxlib/xla.cc\n@@ -894,24 +894,24 @@ NB_MODULE(_jax, m) {\n                  absl::StrCat(\"Unknown attribute \", name).c_str());\n            });\n \n-  nb::class_<ifrt::Executable>(m, \"Executable\")\n-      .def(\"hlo_modules\", ValueOrThrowWrapper(&ifrt::Executable::GetHloModules))\n+  nb::class_<PyExecutable>(m, \"Executable\")\n+      .def(\"hlo_modules\", ValueOrThrowWrapper(&PyExecutable::GetHloModules))\n       .def(\"get_output_memory_kinds\",\n-           xla::ValueOrThrowWrapper(&ifrt::Executable::GetOutputMemoryKinds))\n-      .def(\"get_output_shardings\", &ifrt::Executable::GetOutputShardings)\n+           xla::ValueOrThrowWrapper(&PyExecutable::GetOutputMemoryKinds))\n+      .def(\"get_output_shardings\", &PyExecutable::GetOutputShardings)\n       .def(\"get_parameter_layouts\",\n-           ValueOrThrowWrapper(&ifrt::Executable::GetParameterLayouts))\n+           ValueOrThrowWrapper(&PyExecutable::GetParameterLayouts))\n       .def(\"get_output_layouts\",\n-           xla::ValueOrThrowWrapper(&ifrt::Executable::GetOutputLayouts))\n-      .def(\"get_parameter_shardings\", &ifrt::Executable::GetParameterShardings)\n+           xla::ValueOrThrowWrapper(&PyExecutable::GetOutputLayouts))\n+      .def(\"get_parameter_shardings\", &PyExecutable::GetParameterShardings)\n       .def(\"get_compiled_memory_stats\",\n-           xla::ValueOrThrowWrapper(&ifrt::Executable::GetCompiledMemoryStats))\n+           xla::ValueOrThrowWrapper(&PyExecutable::GetCompiledMemoryStats))\n       .def(\"serialize\",\n-           [](const ifrt::Executable& exec) -> nb::bytes {\n+           [](const PyExecutable& exec) -> nb::bytes {\n              std::string serialized = ValueOrThrow(exec.Serialize());\n              return nb::bytes(serialized.data(), serialized.size());\n            })\n-      .def(\"cost_analysis\", [](const ifrt::Executable& exec) {\n+      .def(\"cost_analysis\", [](const PyExecutable& exec) {\n         auto attrs = ValueOrThrow(exec.GetCostAnalysis());\n         return ifrt::ToPjRtAttributeMap(std::move(attrs));\n       });\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex b9497b71dcb1..85de5f947e49 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 346\n+_version = 347\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\n\n```",
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add BUILD rules for blackwell matmul kernel\n\nPiperOrigin-RevId: 766748530",
        "output": "```diff\nCommit: 554cc01f76970bf3ed18e04a488577bedcf912ba\nDate: 2025-06-03T18:24:55Z\nURL: https://github.com/jax-ml/jax/commit/554cc01f76970bf3ed18e04a488577bedcf912ba\nFiles changed: 1\nAdditions: +24, Deletions: -1\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 48eddae69a60..5580cb2abd73 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -809,9 +809,13 @@ jax_multiplatform_test(\n     name = \"mgpu_matmul_test\",\n     srcs = [\"mgpu_matmul_test.py\"],\n     enable_backends = [],\n-    enable_configs = [],  # TODO(justinfu): Enable B200 when available.\n+    enable_configs = [\"gpu_b200\"],\n     env = {\"XLA_FLAGS\": \"--xla_gpu_autotune_level=0\"},\n     shard_count = 8,\n+    tags = [\n+        # TODO(b/330364373): Remove when B200 is fully supported.\n+        \"notap\",\n+    ],\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_experimental_gpu_ops\",\n@@ -822,6 +826,25 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"blackwell_matmul_mgpu_run\",\n+    srcs = [\"//jax/experimental/pallas/ops/gpu:blackwell_matmul_mgpu.py\"],\n+    enable_backends = [],\n+    enable_configs = [\"gpu_b200\"],\n+    env = {\"XLA_FLAGS\": \"--xla_gpu_autotune_level=0\"},\n+    tags = [\n+        \"manual\",\n+        \"notap\",\n+    ],\n+    deps = [\n+        \"//jax:pallas\",\n+        \"//jax:pallas_mosaic_gpu\",\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n+)\n+\n jax_multiplatform_test(\n     name = \"mgpu_ragged_dot_run\",\n     srcs = [\"//jax/experimental/pallas/ops/gpu:ragged_dot_mgpu.py\"],\n\n```",
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[JAX] Remove the redundant pjit BUILD target.\n\nPiperOrigin-RevId: 766747093",
        "output": "```diff\nCommit: cda50f5dbd4cd35b26bc1489847b37a977509f40\nDate: 2025-06-03T18:22:30Z\nURL: https://github.com/jax-ml/jax/commit/cda50f5dbd4cd35b26bc1489847b37a977509f40\nFiles changed: 1\nAdditions: +0, Deletions: -11\ndiff --git a/jax/BUILD b/jax/BUILD\nindex b577abcabf5f..80add9c096bd 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -1501,17 +1501,6 @@ pytype_library(\n     deps = [\":jax\"],\n )\n \n-# TODO(apaszke): Remove this target\n-pytype_library(\n-    name = \"pjit\",\n-    srcs = [\"experimental/pjit.py\"],\n-    visibility = [\"//visibility:public\"],\n-    deps = [\n-        \":experimental\",\n-        \":jax\",\n-    ],\n-)\n-\n pytype_library(\n     name = \"jet\",\n     srcs = [\"experimental/jet.py\"],\n\n```",
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add lowering for `2xf32 -> 2xf8e4m3fn` conversions.\n\nThe conversion uses the `cvt.rn.satfinite.e4m3x2.f32` intrinsics, which means\nthat the saturation behaviour is different from XLA's default.\n\nThis does ask the question of which numerical behaviour we expect Mosaic GPU to\nuphold---but we probably don't want to propagate NaNs in this case anyway.\n\nPiperOrigin-RevId: 766737083",
        "output": "```diff\nCommit: e24f7807650d93c70ef7be3d779f64a287a113e8\nDate: 2025-06-03T17:57:35Z\nURL: https://github.com/jax-ml/jax/commit/e24f7807650d93c70ef7be3d779f64a287a113e8\nFiles changed: 2\nAdditions: +78, Deletions: -0\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex 7278af5d7a91..925aa1575e2d 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -1402,11 +1402,14 @@ def __getitem__(self, idx):\n \n   # TODO(apaszke): Support JAX dtypes here as well?\n   def astype(self, new_dtype: ir.Type, *, is_signed: bool | None = None):\n+    index = ir.IndexType.get()\n     i4 = ir.IntegerType.get_signless(4)\n     i8 = ir.IntegerType.get_signless(8)\n     i16 = ir.IntegerType.get_signless(16)\n     i32 = ir.IntegerType.get_signless(32)\n     bf16 = ir.BF16Type.get()\n+    f32 = ir.F32Type.get()\n+    f8e4m3fn = ir.Float8E4M3FNType.get()\n \n     cur_dtype = self.mlir_dtype\n     if cur_dtype == new_dtype:\n@@ -1540,6 +1543,34 @@ def upcast_to_bf16(reg, high):\n       return FragmentedArray(\n           _registers=new_registers, _layout=self.layout, _is_signed=is_signed\n       )\n+    # TODO(bchetioui): handle conversions to/from other float8 types.\n+    if cur_dtype == f32 and new_dtype == f8e4m3fn:\n+      if vector_len != 2:\n+        raise NotImplementedError(vector_len)\n+      new_registers = np.empty_like(self.registers)\n+      empty_vec_32 = llvm.mlir_undef(ir.VectorType.get((1,), i32))\n+      empty_result_vec = llvm.mlir_undef(ir.VectorType.get((2,), i8))\n+      for idx, reg in np.ndenumerate(self.registers):\n+        e0 = vector.extractelement(reg, position=c(0, index))\n+        e1 = vector.extractelement(reg, position=c(1, index))\n+        new_reg_32 = llvm.inline_asm(\n+            i32,\n+            [e1, e0],\n+            \"cvt.rn.satfinite.e4m3x2.f32 $0, $1, $2;\",\n+            \"=h,f,f\",\n+        )\n+        new_vec_32 = llvm.insertelement(empty_vec_32, new_reg_32, c(0, i32))\n+        new_vec_f8 = vector.bitcast(ir.VectorType.get((4,), i8), new_vec_32)\n+        res = llvm.insertelement(\n+            empty_result_vec,\n+            vector.extractelement(new_vec_f8, position=c(0, i32)), c(0, i32))\n+        res = llvm.insertelement(\n+            res,\n+            vector.extractelement(new_vec_f8, position=c(1, i32)), c(1, i32))\n+        new_registers[idx] = vector.bitcast(ir.VectorType.get((2,), f8e4m3fn), res)\n+      return FragmentedArray(\n+          _registers=new_registers, _layout=self.layout, _is_signed=is_signed\n+      )\n     # Generic path.\n     from_float = ir.FloatType.isinstance(cur_dtype)\n     to_float = ir.FloatType.isinstance(new_dtype)\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 314dc8f8f41d..3bb0c1b9fe78 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -587,6 +587,53 @@ def kernel(ctx, inp, out, smem):\n     f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, y, (x, y))\n     np.testing.assert_array_equal(f(x), y)\n \n+  def test_f8_conversions(self):\n+    jax_dtype_from, jax_dtype_to = jnp.float32, jnp.float8_e4m3fn\n+    mlir_dtype_to = utils.dtype_to_ir_type(jax_dtype_to)\n+    def kernel(ctx, inp, out, smem):\n+      del ctx\n+      smem_from, smem_to = smem\n+      copy(inp, smem_from, swizzle=128)\n+      t = mgpu.FragmentedArray.load_tiled(\n+          smem_from,\n+          swizzle=128,\n+          is_signed=None,\n+          layout=fa.WGMMA_LAYOUT,\n+      )\n+      t = t.astype(mlir_dtype_to, is_signed=utils.is_signed(jax_dtype_to))\n+      t.store_tiled(smem_to, swizzle=128)\n+      copy(smem_to, out, swizzle=128)\n+\n+    # These generative shenanigans are to ensure that we don't generate values\n+    # that are too large for the target type. That is because the saturation\n+    # behavior of the conversion is different between XLA and Mosaic GPU here\n+    # (to use the NVIDIA internal, we allow Mosaic GPU to use the .satfinite\n+    # modifier, which saturates to the largest finite value---while XLA would\n+    # give us NaNs in this case).\n+    max_finite_val = 0b111_1110\n+\n+    expected = jax.lax.bitcast_convert_type(\n+        jax.random.randint(\n+            jax.random.key(42),\n+            (1, 1, 64, 128),\n+            -max_finite_val,\n+            max_finite_val + 1,\n+            dtype=jnp.uint8,\n+        ),\n+        jax_dtype_to,\n+    )\n+    x = expected.astype(jax_dtype_from)\n+\n+    res = mgpu.as_gpu_kernel(\n+        kernel,\n+        (1, 1, 1),\n+        (128, 1, 1),\n+        x,\n+        expected,\n+        (x, expected),\n+    )(x)\n+    np.testing.assert_array_equal(res, expected)\n+\n   @parameterized.product(\n       jax_dtype_from_to=(\n           (jnp.int8, jnp.bfloat16),\n\n```",
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29186 from jakevdp:jax-numpy-imports\n\nPiperOrigin-RevId: 766730509",
        "output": "```diff\nCommit: 0b89b2342785de236886f3efd7f89c5158637d5e\nDate: 2025-06-03T17:45:16Z\nURL: https://github.com/jax-ml/jax/commit/0b89b2342785de236886f3efd7f89c5158637d5e\nFiles changed: 19\nAdditions: +182, Deletions: -167\ndiff --git a/jax/_src/lax/fft.py b/jax/_src/lax/fft.py\nindex 2eebe6d91f22..08e06287b784 100644\n--- a/jax/_src/lax/fft.py\n+++ b/jax/_src/lax/fft.py\n@@ -21,8 +21,6 @@\n \n import numpy as np\n \n-from jax import lax\n-\n from jax._src import dispatch\n from jax._src import dtypes\n from jax._src.api import jit, linear_transpose, ShapeDtypeStruct\n@@ -30,6 +28,7 @@\n from jax._src.interpreters import ad\n from jax._src.interpreters import batching\n from jax._src.interpreters import mlir\n+from jax._src.lax import lax\n from jax._src.lib.mlir.dialects import hlo\n \n __all__ = [\ndiff --git a/jax/_src/numpy/array_api_metadata.py b/jax/_src/numpy/array_api_metadata.py\nindex d634a2856a1b..af4e27cd5f8d 100644\n--- a/jax/_src/numpy/array_api_metadata.py\n+++ b/jax/_src/numpy/array_api_metadata.py\n@@ -21,7 +21,6 @@\n \n from types import ModuleType\n \n-import jax\n from jax._src.sharding import Sharding\n from jax._src.lib import xla_client as xc\n from jax._src import config\n@@ -40,6 +39,7 @@ def __array_namespace__(self, *, api_version: None | str = None) -> ModuleType:\n   if api_version is not None and api_version != __array_api_version__:\n     raise ValueError(f\"{api_version=!r} is not available; \"\n                      f\"available versions are: {[__array_api_version__]}\")\n+  import jax.numpy  # pytype: disable=import-error\n   return jax.numpy\n \n \n@@ -77,7 +77,7 @@ def default_device(self):\n   def devices(self):\n     out = [None]  # None indicates \"uncommitted\"\n     for backend in xb.backends():\n-        out.extend(jax.devices(backend))\n+        out.extend(xb.devices(backend))\n     return out\n \n   def capabilities(self):\ndiff --git a/jax/_src/numpy/array_creation.py b/jax/_src/numpy/array_creation.py\nindex 86bcfb2c02f6..b14c2fe73faa 100644\n--- a/jax/_src/numpy/array_creation.py\n+++ b/jax/_src/numpy/array_creation.py\n@@ -19,18 +19,16 @@\n \n import numpy as np\n \n-import jax\n-from jax import lax\n-from jax._src.api import jit\n+from jax._src.api import device_put, jit\n from jax._src import core\n from jax._src import dtypes\n-from jax._src.lax import lax as lax_internal\n+from jax._src.lax import lax\n from jax._src.lib import xla_client as xc\n from jax._src.numpy import ufuncs\n from jax._src.numpy import util\n+from jax._src.sharding import Sharding\n from jax._src.typing import Array, ArrayLike, DuckTypedArray, DTypeLike\n from jax._src.util import canonicalize_axis, set_module\n-from jax.sharding import Sharding\n \n \n export = set_module('jax.numpy')\n@@ -205,6 +203,8 @@ def full(shape: Any, fill_value: ArrayLike,\n     Array([[0, 1, 2],\n            [0, 1, 2]], dtype=int32)\n   \"\"\"\n+  from jax._src.numpy.lax_numpy import asarray  # pytype: disable=import-error\n+\n   dtypes.check_user_dtype_supported(dtype, \"full\")\n   util.check_arraylike(\"full\", fill_value)\n \n@@ -212,8 +212,8 @@ def full(shape: Any, fill_value: ArrayLike,\n     shape = canonicalize_shape(shape)\n     return lax.full(shape, fill_value, dtype, sharding=util.normalize_device_to_sharding(device))\n   else:\n-    return jax.device_put(\n-        util._broadcast_to(jax.numpy.asarray(fill_value, dtype=dtype), shape), device)\n+    return device_put(\n+        util._broadcast_to(asarray(fill_value, dtype=dtype), shape), device)\n \n \n @export\n@@ -394,6 +394,8 @@ def full_like(a: ArrayLike | DuckTypedArray,\n     Array([[1, 1, 1],\n            [2, 2, 2]], dtype=int32)\n   \"\"\"\n+  from jax._src.numpy.lax_numpy import asarray  # pytype: disable=import-error\n+\n   if hasattr(a, 'dtype') and hasattr(a, 'shape'):  # support duck typing\n     util.check_arraylike(\"full_like\", 0, fill_value)\n   else:\n@@ -408,8 +410,8 @@ def full_like(a: ArrayLike | DuckTypedArray,\n   else:\n     shape = np.shape(a) if shape is None else shape  # type: ignore[arg-type]\n     dtype = dtypes.result_type(a) if dtype is None else dtype\n-    return jax.device_put(\n-        util._broadcast_to(jax.numpy.asarray(fill_value, dtype=dtype), shape), device)\n+    return device_put(\n+        util._broadcast_to(asarray(fill_value, dtype=dtype), shape), device)\n \n @overload\n def linspace(start: ArrayLike, stop: ArrayLike, num: int = 50,\n@@ -510,6 +512,8 @@ def _linspace(start: ArrayLike, stop: ArrayLike, num: int = 50,\n               axis: int = 0,\n               *, device: xc.Device | Sharding | None = None) -> Array | tuple[Array, Array]:\n   \"\"\"Implementation of linspace differentiable in start and stop args.\"\"\"\n+  from jax._src.numpy.lax_numpy import asarray  # pytype: disable=import-error\n+\n   dtypes.check_user_dtype_supported(dtype, \"linspace\")\n   if num < 0:\n     raise ValueError(f\"Number of samples, {num}, must be non-negative.\")\n@@ -529,13 +533,13 @@ def _linspace(start: ArrayLike, stop: ArrayLike, num: int = 50,\n   bounds_shape.insert(axis, 1)\n   div = (num - 1) if endpoint else num\n   if num > 1:\n-    delta: Array = lax.convert_element_type(stop - start, computation_dtype) / jax.numpy.array(div, dtype=computation_dtype)\n+    delta: Array = lax.convert_element_type(stop - start, computation_dtype) / asarray(div, dtype=computation_dtype)\n     iota_shape = [1,] * len(bounds_shape)\n     iota_shape[axis] = div\n     # This approach recovers the endpoints with float32 arithmetic,\n     # but can lead to rounding errors for integer outputs.\n     real_dtype = dtypes.finfo(computation_dtype).dtype\n-    step = lax.iota(real_dtype, div).reshape(iota_shape) / jax.numpy.array(div, real_dtype)\n+    step = lax.iota(real_dtype, div).reshape(iota_shape) / asarray(div, real_dtype)\n     step = step.astype(computation_dtype)\n     out = (broadcast_start.reshape(bounds_shape) * (1 - step) +\n       broadcast_stop.reshape(bounds_shape) * step)\n@@ -545,7 +549,7 @@ def _linspace(start: ArrayLike, stop: ArrayLike, num: int = 50,\n                             canonicalize_axis(axis, out.ndim))\n \n   elif num == 1:\n-    delta = jax.numpy.asarray(np.nan if endpoint else stop - start, dtype=computation_dtype)\n+    delta = asarray(np.nan if endpoint else stop - start, dtype=computation_dtype)\n     out = broadcast_start.reshape(bounds_shape)\n   else:  # num == 0 degenerate case, match numpy behavior\n     empty_shape = list(lax.broadcast_shapes(np.shape(start), np.shape(stop)))\n@@ -557,7 +561,7 @@ def _linspace(start: ArrayLike, stop: ArrayLike, num: int = 50,\n     out = lax.floor(out)\n \n   sharding = util.canonicalize_device_to_sharding(device)\n-  result = lax_internal._convert_element_type(out, dtype, sharding=sharding)\n+  result = lax._convert_element_type(out, dtype, sharding=sharding)\n   return (result, delta) if retstep else result\n \n \ndiff --git a/jax/_src/numpy/array_methods.py b/jax/_src/numpy/array_methods.py\nindex b29b95219325..958085e19a53 100644\n--- a/jax/_src/numpy/array_methods.py\n+++ b/jax/_src/numpy/array_methods.py\n@@ -29,9 +29,8 @@\n from typing import Any, Callable, Sequence\n \n import numpy as np\n-import jax\n+\n from jax import lax\n-from jax.sharding import Sharding\n from jax._src import api\n from jax._src import core\n from jax._src import dtypes\n@@ -44,6 +43,7 @@\n from jax._src.numpy import lax_numpy\n from jax._src.numpy import tensor_contractions\n from jax._src.pjit import PartitionSpec\n+from jax._src.sharding import Sharding\n from jax._src.sharding_impls import canonicalize_sharding, NamedSharding\n from jax._src.numpy import reductions\n from jax._src.numpy import ufuncs\n@@ -612,12 +612,13 @@ def _deepcopy(self: Array, memo: Any) -> Array:\n \n def __array_module__(self, types):\n   if all(issubclass(t, _HANDLED_ARRAY_TYPES) for t in types):\n+    import jax.numpy  # pytype: disable=import-error\n     return jax.numpy\n   else:\n     return NotImplemented\n \n \n-@partial(jax.jit, static_argnums=(1,2,3))\n+@partial(api.jit, static_argnums=(1,2,3))\n def _multi_slice(self: Array,\n                  start_indices: tuple[tuple[int, ...]],\n                  limit_indices: tuple[tuple[int, ...]],\n@@ -637,7 +638,7 @@ def _multi_slice(self: Array,\n \n # The next two functions are related to iter(array), implemented here to\n # avoid circular imports.\n-@jax.jit\n+@api.jit\n def _unstack(x: Array) -> list[Array]:\n   dims = (0,)\n   return [lax.squeeze(t, dims) for t in lax.split(x, (1,) * x.shape[0])]\n@@ -776,7 +777,7 @@ def __repr__(self) -> str:\n     return f\"_IndexUpdateRef({self.array!r}, {self.index!r})\"\n \n   def get(self, *, indices_are_sorted: bool = False, unique_indices: bool = False,\n-          mode: str | jax.lax.GatherScatterMode | None = None,\n+          mode: str | lax.GatherScatterMode | None = None,\n           fill_value: ArrayLike | None = None, out_sharding: Sharding | None = None):\n     \"\"\"Equivalent to ``x[idx]``.\n \n@@ -798,7 +799,7 @@ def get(self, *, indices_are_sorted: bool = False, unique_indices: bool = False,\n \n   def set(self, values: ArrayLike, *, indices_are_sorted: bool = False,\n           unique_indices: bool = False,\n-          mode: str | jax.lax.GatherScatterMode | None = None) -> None:\n+          mode: str | lax.GatherScatterMode | None = None) -> None:\n     \"\"\"Pure equivalent of ``x[idx] = y``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -816,7 +817,7 @@ def set(self, values: ArrayLike, *, indices_are_sorted: bool = False,\n \n   def apply(self, func: Callable[[ArrayLike], Array], *,\n             indices_are_sorted: bool = False, unique_indices: bool = False,\n-            mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+            mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``func.at(x, idx)`` for a unary ufunc ``func``.\n \n     Returns the value of ``x`` that would result from applying the unary\n@@ -840,7 +841,7 @@ def _scatter_apply(x, indices, y, dims, **kwargs):\n \n   def add(self, values: ArrayLike, *,\n           indices_are_sorted: bool = False, unique_indices: bool = False,\n-          mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+          mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] += y``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -855,7 +856,7 @@ def add(self, values: ArrayLike, *,\n \n   def subtract(self, values: ArrayLike, *,\n                indices_are_sorted: bool = False, unique_indices: bool = False,\n-               mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+               mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] -= y``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -870,7 +871,7 @@ def subtract(self, values: ArrayLike, *,\n \n   def multiply(self, values: ArrayLike, *,\n                indices_are_sorted: bool = False, unique_indices: bool = False,\n-               mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+               mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] *= y``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -887,7 +888,7 @@ def multiply(self, values: ArrayLike, *,\n \n   def divide(self, values: ArrayLike, *,\n              indices_are_sorted: bool = False, unique_indices: bool = False,\n-             mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+             mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] /= y``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -904,7 +905,7 @@ def divide(self, values: ArrayLike, *,\n \n   def power(self, values: ArrayLike, *,\n             indices_are_sorted: bool = False, unique_indices: bool = False,\n-            mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+            mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] **= y``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -921,7 +922,7 @@ def power(self, values: ArrayLike, *,\n \n   def min(self, values: ArrayLike, *,\n           indices_are_sorted: bool = False, unique_indices: bool = False,\n-          mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+          mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] = minimum(x[idx], y)``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -937,7 +938,7 @@ def min(self, values: ArrayLike, *,\n \n   def max(self, values: ArrayLike, *,\n           indices_are_sorted: bool = False, unique_indices: bool = False,\n-          mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+          mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] = maximum(x[idx], y)``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\ndiff --git a/jax/_src/numpy/error.py b/jax/_src/numpy/error.py\nindex e2c23b43bdf8..8af0c52566b5 100644\n--- a/jax/_src/numpy/error.py\n+++ b/jax/_src/numpy/error.py\n@@ -15,9 +15,11 @@\n import contextlib\n from typing import Literal, Sequence\n \n-import jax\n+import numpy as np\n+\n from jax._src import config\n-from jax._src.typing import ArrayLike\n+from jax._src import dtypes\n+from jax._src.typing import Array, ArrayLike\n \n Category = Literal[\"nan\", \"divide\", \"oob\"]\n \n@@ -40,7 +42,7 @@ def _is_category_disabled(\n \n \n def _set_error_if_with_category(\n-    pred: jax.Array,\n+    pred: Array,\n     /,\n     msg: str,\n     category: Category | None = None,\n@@ -65,7 +67,7 @@ def _set_error_if_with_category(\n   error_check_lib.set_error_if(pred, msg)\n \n \n-def _set_error_if_nan(pred: jax.Array, /):\n+def _set_error_if_nan(pred: Array, /):\n   \"\"\"Set the internal error state if any element of `pred` is `NaN`.\n \n   This function is disabled if the `jax_error_checking_behavior_nan` flag is\n@@ -74,17 +76,17 @@ def _set_error_if_nan(pred: jax.Array, /):\n   if config.error_checking_behavior_nan.value == \"ignore\":\n     return\n \n-  # TODO(mattjj): fix the circular import issue.\n-  import jax.numpy as jnp\n-  if not jnp.issubdtype(pred.dtype, jnp.floating):  # only check floats\n+  if not dtypes.issubdtype(pred.dtype, np.floating):  # only check floats\n     return\n \n   # TODO(mattjj): fix the circular import issue.\n   from jax._src import error_check as error_check_lib\n+  import jax.numpy as jnp\n+\n   error_check_lib.set_error_if(jnp.isnan(pred), \"NaN encountered\")\n \n \n-def _set_error_if_divide_by_zero(pred: jax.Array, /):\n+def _set_error_if_divide_by_zero(pred: Array, /):\n   \"\"\"Set the internal error state if any element of `pred` is zero.\n \n   This function is intended for checking if the denominator of a division is\ndiff --git a/jax/_src/numpy/fft.py b/jax/_src/numpy/fft.py\nindex 2316ad73ffeb..21da91ce613f 100644\n--- a/jax/_src/numpy/fft.py\n+++ b/jax/_src/numpy/fft.py\n@@ -18,8 +18,8 @@\n import operator\n import numpy as np\n \n-from jax import lax\n from jax._src import dtypes\n+from jax._src.lax import fft as lax_fft\n from jax._src.lib import xla_client\n from jax._src.util import safe_zip\n from jax._src.numpy.util import ensure_arraylike, promote_dtypes_inexact\n@@ -45,7 +45,7 @@ def _fft_norm(s: Array, func_name: str, norm: str) -> Array:\n                     '\"ortho\" or \"forward\".')\n \n \n-def _fft_core(func_name: str, fft_type: lax.FftType, a: ArrayLike,\n+def _fft_core(func_name: str, fft_type: lax_fft.FftType, a: ArrayLike,\n               s: Shape | None, axes: Sequence[int] | None,\n               norm: str | None) -> Array:\n   full_name = f\"jax.numpy.fft.{func_name}\"\n@@ -80,14 +80,14 @@ def _fft_core(func_name: str, fft_type: lax.FftType, a: ArrayLike,\n     in_s = list(arr.shape)\n     for axis, x in safe_zip(axes, s):\n       in_s[axis] = x\n-    if fft_type == lax.FftType.IRFFT:\n+    if fft_type == lax_fft.FftType.IRFFT:\n       in_s[-1] = (in_s[-1] // 2 + 1)\n     # Cropping\n     arr = arr[tuple(map(slice, in_s))]\n     # Padding\n     arr = jnp.pad(arr, [(0, x-y) for x, y in zip(in_s, arr.shape)])\n   else:\n-    if fft_type == lax.FftType.IRFFT:\n+    if fft_type == lax_fft.FftType.IRFFT:\n       s = [arr.shape[axis] for axis in axes[:-1]]\n       if axes:\n         s += [max(0, 2 * (arr.shape[axes[-1]] - 1))]\n@@ -103,10 +103,10 @@ def _fft_core(func_name: str, fft_type: lax.FftType, a: ArrayLike,\n   return transformed\n \n \n-def _fft_core_nd(arr: Array, fft_type: lax.FftType, s: Shape) -> Array:\n+def _fft_core_nd(arr: Array, fft_type: lax_fft.FftType, s: Shape) -> Array:\n   # XLA supports N-D transforms up to N=3 so we use XLA's FFT N-D directly.\n   if len(s) <= 3:\n-    return lax.fft(arr, fft_type, tuple(s))\n+    return lax_fft.fft(arr, fft_type, tuple(s))\n \n   # For larger N, we repeatedly apply N<=3 transforms until we reach the\n   # requested dimension. We special case N=4 to use two 2-D transforms instead\n@@ -115,16 +115,16 @@ def _fft_core_nd(arr: Array, fft_type: lax.FftType, s: Shape) -> Array:\n   n = 2 if len(s) == 4 else 3\n   src = tuple(range(arr.ndim - len(s), arr.ndim - n))\n   dst = tuple(range(arr.ndim - len(s) + n, arr.ndim))\n-  if fft_type in {lax.FftType.RFFT, lax.FftType.FFT}:\n-    arr = lax.fft(arr, fft_type, tuple(s)[-n:])\n+  if fft_type in {lax_fft.FftType.RFFT, lax_fft.FftType.FFT}:\n+    arr = lax_fft.fft(arr, fft_type, tuple(s)[-n:])\n     arr = jnp.moveaxis(arr, src, dst)\n-    arr = _fft_core_nd(arr, lax.FftType.FFT, s[:-n])\n+    arr = _fft_core_nd(arr, lax_fft.FftType.FFT, s[:-n])\n     arr = jnp.moveaxis(arr, dst, src)\n   else:\n     arr = jnp.moveaxis(arr, src, dst)\n-    arr = _fft_core_nd(arr, lax.FftType.IFFT, s[:-n])\n+    arr = _fft_core_nd(arr, lax_fft.FftType.IFFT, s[:-n])\n     arr = jnp.moveaxis(arr, dst, src)\n-    arr = lax.fft(arr, fft_type, tuple(s)[-n:])\n+    arr = lax_fft.fft(arr, fft_type, tuple(s)[-n:])\n   return arr\n \n \n@@ -199,7 +199,7 @@ def fftn(a: ArrayLike, s: Shape | None = None,\n     >>> jnp.allclose(x, jnp.fft.ifftn(x_fftn))\n     Array(True, dtype=bool)\n   \"\"\"\n-  return _fft_core('fftn', lax.FftType.FFT, a, s, axes, norm)\n+  return _fft_core('fftn', lax_fft.FftType.FFT, a, s, axes, norm)\n \n \n def ifftn(a: ArrayLike, s: Shape | None = None,\n@@ -267,7 +267,7 @@ def ifftn(a: ArrayLike, s: Shape | None = None,\n     [[ 2.5 +0.j    0.  -0.58j  0.  +0.58j]\n      [ 0.17+0.j   -0.83-0.29j -0.83+0.29j]]\n   \"\"\"\n-  return _fft_core('ifftn', lax.FftType.IFFT, a, s, axes, norm)\n+  return _fft_core('ifftn', lax_fft.FftType.IFFT, a, s, axes, norm)\n \n \n def rfftn(a: ArrayLike, s: Shape | None = None,\n@@ -358,7 +358,7 @@ def rfftn(a: ArrayLike, s: Shape | None = None,\n     >>> jnp.fft.rfftn(x1)\n     Array([10.+0.j, -2.+2.j, -2.+0.j], dtype=complex64)\n   \"\"\"\n-  return _fft_core('rfftn', lax.FftType.RFFT, a, s, axes, norm)\n+  return _fft_core('rfftn', lax_fft.FftType.RFFT, a, s, axes, norm)\n \n \n def irfftn(a: ArrayLike, s: Shape | None = None,\n@@ -435,7 +435,7 @@ def irfftn(a: ArrayLike, s: Shape | None = None,\n            [[-2., -2., -2.],\n             [-2., -2., -2.]]], dtype=float32)\n   \"\"\"\n-  return _fft_core('irfftn', lax.FftType.IRFFT, a, s, axes, norm)\n+  return _fft_core('irfftn', lax_fft.FftType.IRFFT, a, s, axes, norm)\n \n \n def _axis_check_1d(func_name: str, axis: int | None):\n@@ -446,7 +446,7 @@ def _axis_check_1d(func_name: str, axis: int | None):\n         \"Got axis = %r.\" % (full_name, full_name, axis)\n     )\n \n-def _fft_core_1d(func_name: str, fft_type: lax.FftType,\n+def _fft_core_1d(func_name: str, fft_type: lax_fft.FftType,\n                  a: ArrayLike, n: int | None, axis: int | None,\n                  norm: str | None) -> Array:\n   _axis_check_1d(func_name, axis)\n@@ -514,7 +514,7 @@ def fft(a: ArrayLike, n: int | None = None,\n     >>> jnp.allclose(x, jnp.fft.ifft(x_fft))\n     Array(True, dtype=bool)\n   \"\"\"\n-  return _fft_core_1d('fft', lax.FftType.FFT, a, n=n, axis=axis,\n+  return _fft_core_1d('fft', lax_fft.FftType.FFT, a, n=n, axis=axis,\n                       norm=norm)\n \n \n@@ -570,7 +570,7 @@ def ifft(a: ArrayLike, n: int | None = None,\n      [ 0.67+0.58j -0.5 +1.44j  0.17+2.02j  1.83+0.29j]\n      [ 0.67-0.58j -0.5 -1.44j  0.17-2.02j  1.83-0.29j]]\n   \"\"\"\n-  return _fft_core_1d('ifft', lax.FftType.IFFT, a, n=n, axis=axis,\n+  return _fft_core_1d('ifft', lax_fft.FftType.IFFT, a, n=n, axis=axis,\n                       norm=norm)\n \n \n@@ -631,7 +631,7 @@ def rfft(a: ArrayLike, n: int | None = None,\n            [ 1.-2.j,  3.-4.j,  5.-6.j],\n            [-1.+0.j, -1.+0.j, -1.+0.j]], dtype=complex64)\n   \"\"\"\n-  return _fft_core_1d('rfft', lax.FftType.RFFT, a, n=n, axis=axis,\n+  return _fft_core_1d('rfft', lax_fft.FftType.RFFT, a, n=n, axis=axis,\n                       norm=norm)\n \n \n@@ -691,7 +691,7 @@ def irfft(a: ArrayLike, n: int | None = None,\n            [-0.75, -1.25, -1.75],\n            [ 0.25,  0.75,  1.25]], dtype=float32)\n   \"\"\"\n-  return _fft_core_1d('irfft', lax.FftType.IRFFT, a, n=n, axis=axis,\n+  return _fft_core_1d('irfft', lax_fft.FftType.IRFFT, a, n=n, axis=axis,\n                       norm=norm)\n \n \n@@ -781,7 +781,7 @@ def hfft(a: ArrayLike, n: int | None = None,\n   conj_a = ufuncs.conj(a)\n   _axis_check_1d('hfft', axis)\n   nn = (conj_a.shape[axis] - 1) * 2 if n is None else n\n-  return _fft_core_1d('hfft', lax.FftType.IRFFT, conj_a, n=n, axis=axis,\n+  return _fft_core_1d('hfft', lax_fft.FftType.IRFFT, conj_a, n=n, axis=axis,\n                       norm=norm) * nn\n \n \n@@ -831,12 +831,12 @@ def ihfft(a: ArrayLike, n: int | None = None,\n   _axis_check_1d('ihfft', axis)\n   arr = jnp.asarray(a)\n   nn = arr.shape[axis] if n is None else n\n-  output = _fft_core_1d('ihfft', lax.FftType.RFFT, arr, n=n, axis=axis,\n+  output = _fft_core_1d('ihfft', lax_fft.FftType.RFFT, arr, n=n, axis=axis,\n                         norm=norm)\n   return ufuncs.conj(output) * (1 / nn)\n \n \n-def _fft_core_2d(func_name: str, fft_type: lax.FftType, a: ArrayLike,\n+def _fft_core_2d(func_name: str, fft_type: lax_fft.FftType, a: ArrayLike,\n                  s: Shape | None, axes: Sequence[int],\n                  norm: str | None) -> Array:\n   full_name = f\"jax.numpy.fft.{func_name}\"\n@@ -923,7 +923,7 @@ def fft2(a: ArrayLike, s: Shape | None = None, axes: Sequence[int] = (-2,-1),\n     >>> jnp.allclose(x, jnp.fft.ifft2(x_fft2))\n     Array(True, dtype=bool)\n   \"\"\"\n-  return _fft_core_2d('fft2', lax.FftType.FFT, a, s=s, axes=axes,\n+  return _fft_core_2d('fft2', lax_fft.FftType.FFT, a, s=s, axes=axes,\n                       norm=norm)\n \n \n@@ -995,7 +995,7 @@ def ifft2(a: ArrayLike, s: Shape | None = None, axes: Sequence[int] = (-2,-1),\n             [-0.33-0.58j, -0.33-0.58j],\n             [-0.33+0.58j, -0.33+0.58j]]], dtype=complex64)\n   \"\"\"\n-  return _fft_core_2d('ifft2', lax.FftType.IFFT, a, s=s, axes=axes,\n+  return _fft_core_2d('ifft2', lax_fft.FftType.IFFT, a, s=s, axes=axes,\n                       norm=norm)\n \n \n@@ -1074,7 +1074,7 @@ def rfft2(a: ArrayLike, s: Shape | None = None, axes: Sequence[int] = (-2,-1),\n             [  3.47+10.11j,   6.43+11.42j,   9.38+12.74j],\n             [  3.19 +1.63j,   4.4  +1.38j,   5.61 +1.12j]]], dtype=complex64)\n   \"\"\"\n-  return _fft_core_2d('rfft2', lax.FftType.RFFT, a, s=s, axes=axes,\n+  return _fft_core_2d('rfft2', lax_fft.FftType.RFFT, a, s=s, axes=axes,\n                       norm=norm)\n \n \n@@ -1149,7 +1149,7 @@ def irfft2(a: ArrayLike, s: Shape | None = None, axes: Sequence[int] = (-2,-1),\n             [ 0.  ,  0.  ,  0.  ],\n             [ 0.  ,  0.  ,  0.  ]]], dtype=float32)\n   \"\"\"\n-  return _fft_core_2d('irfft2', lax.FftType.IRFFT, a, s=s, axes=axes,\n+  return _fft_core_2d('irfft2', lax_fft.FftType.IRFFT, a, s=s, axes=axes,\n                       norm=norm)\n \n \ndiff --git a/jax/_src/numpy/index_tricks.py b/jax/_src/numpy/index_tricks.py\nindex ec67d7489f30..ab07ecad0cf5 100644\n--- a/jax/_src/numpy/index_tricks.py\n+++ b/jax/_src/numpy/index_tricks.py\n@@ -17,7 +17,9 @@\n from collections.abc import Iterable\n from typing import Any, Union\n \n-import jax\n+import numpy as np\n+\n+from jax._src import config\n from jax._src import core\n from jax._src.numpy.util import promote_dtypes\n from jax._src.numpy.lax_numpy import (\n@@ -26,8 +28,6 @@\n from jax._src.typing import Array, ArrayLike\n from jax._src.util import set_module\n \n-import numpy as np\n-\n \n export = set_module('jax.numpy')\n \n@@ -83,7 +83,7 @@ def __getitem__(self, key: slice | tuple[slice, ...]) -> Array:\n     if isinstance(key, slice):\n       return _make_1d_grid_from_slice(key, op_name=\"mgrid\")\n     output: Iterable[Array] = (_make_1d_grid_from_slice(k, op_name=\"mgrid\") for k in key)\n-    with jax.numpy_dtype_promotion('standard'):\n+    with config.numpy_dtype_promotion('standard'):\n       output = promote_dtypes(*output)\n     output_arr = meshgrid(*output, indexing='ij', sparse=False)\n     if len(output_arr) == 0:\n@@ -128,7 +128,7 @@ def __getitem__(\n     if isinstance(key, slice):\n       return _make_1d_grid_from_slice(key, op_name=\"ogrid\")\n     output: Iterable[Array] = (_make_1d_grid_from_slice(k, op_name=\"ogrid\") for k in key)\n-    with jax.numpy_dtype_promotion('standard'):\n+    with config.numpy_dtype_promotion('standard'):\n       output = promote_dtypes(*output)\n     return meshgrid(*output, indexing='ij', sparse=True)\n \ndiff --git a/jax/_src/numpy/indexing.py b/jax/_src/numpy/indexing.py\nindex 6aa5d6b87ef4..573352135806 100644\n--- a/jax/_src/numpy/indexing.py\n+++ b/jax/_src/numpy/indexing.py\n@@ -20,7 +20,8 @@\n import string\n from typing import Any, NamedTuple, Sequence\n \n-import jax\n+import numpy as np\n+\n from jax import lax\n from jax._src import array\n from jax._src import config\n@@ -39,7 +40,6 @@\n from jax._src.tree_util import tree_flatten\n from jax._src.typing import Array, ArrayLike, StaticScalar\n from jax._src.util import canonicalize_axis, safe_zip, set_module, tuple_update\n-import numpy as np\n \n export = set_module('jax.numpy')\n \n@@ -314,8 +314,10 @@ def replace(tup, val):\n     return lax.full(out_shape, 0, a.dtype)\n \n   if mode == \"one_hot\":\n+    from jax import nn  # pytype: disable=import-error\n+\n     indices = _normalize_index(indices, axis_size)\n-    hot = jax.nn.one_hot(indices, axis_size, dtype=np.bool_)\n+    hot = nn.one_hot(indices, axis_size, dtype=np.bool_)\n     if a.ndim == 1:\n       return einsum.einsum(\"...b,b->...\", hot, a, preferred_element_type=a.dtype)\n     if axis_int > len(string.ascii_letters) - 2:\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex 171a64a758ad..f323bc64718b 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -35,9 +35,9 @@\n from typing import (Any, IO, Literal, Protocol, TypeVar, Union, overload)\n import warnings\n \n-import jax\n-from jax import jit\n from jax import lax\n+from jax._src.api import jit\n+from jax._src import api\n from jax._src import config\n from jax._src import core\n from jax._src import deprecations\n@@ -508,7 +508,7 @@ def isscalar(element: Any) -> bool:\n   \"\"\"\n   if np.isscalar(element):\n     return True\n-  elif isinstance(element, (np.ndarray, jax.Array)):\n+  elif isinstance(element, (np.ndarray, Array)):\n     return element.ndim == 0\n   elif hasattr(element, '__jax_array__'):\n     return asarray(element).ndim == 0\n@@ -3418,7 +3418,7 @@ def clip(\n     )\n \n   util.check_arraylike(\"clip\", arr)\n-  if any(jax.numpy.iscomplexobj(t) for t in (arr, min, max)):\n+  if any(iscomplexobj(t) for t in (arr, min, max)):\n     raise ValueError(\n       \"Clip received a complex value either through the input or the min/max \"\n       \"keywords. Complex values have no ordering and cannot be clipped. \"\n@@ -4676,7 +4676,7 @@ def concat(arrays: Sequence[ArrayLike], /, *, axis: int | None = 0) -> Array:\n            [1., 1., 1., 0.]], dtype=float32)\n   \"\"\"\n   util.check_arraylike(\"concat\", *arrays)\n-  return jax.numpy.concatenate(arrays, axis=axis)\n+  return concatenate(arrays, axis=axis)\n \n \n @export\n@@ -4732,7 +4732,7 @@ def vstack(tup: np.ndarray | Array | Sequence[ArrayLike],\n   \"\"\"\n   arrs: Array | list[Array]\n   if isinstance(tup, (np.ndarray, Array)):\n-    arrs = jax.vmap(atleast_2d)(tup)\n+    arrs = api.vmap(atleast_2d)(tup)\n   else:\n     # TODO(jakevdp): Non-array input deprecated 2023-09-22; change to error.\n     util.check_arraylike(\"vstack\", *tup, emit_warning=True)\n@@ -4791,7 +4791,7 @@ def hstack(tup: np.ndarray | Array | Sequence[ArrayLike],\n   \"\"\"\n   arrs: Array | list[Array]\n   if isinstance(tup, (np.ndarray, Array)):\n-    arrs = jax.vmap(atleast_1d)(tup)\n+    arrs = api.vmap(atleast_1d)(tup)\n     arr0_ndim = arrs.ndim - 1\n   else:\n     # TODO(jakevdp): Non-array input deprecated 2023-09-22; change to error.\n@@ -4854,7 +4854,7 @@ def dstack(tup: np.ndarray | Array | Sequence[ArrayLike],\n   \"\"\"\n   arrs: Array | list[Array]\n   if isinstance(tup, (np.ndarray, Array)):\n-    arrs = jax.vmap(atleast_3d)(tup)\n+    arrs = api.vmap(atleast_3d)(tup)\n   else:\n     # TODO(jakevdp): Non-array input deprecated 2023-09-22; change to error.\n     util.check_arraylike(\"dstack\", *tup, emit_warning=True)\n@@ -4916,7 +4916,7 @@ def column_stack(tup: np.ndarray | Array | Sequence[ArrayLike]) -> Array:\n   \"\"\"\n   arrs: Array | list[Array] | np.ndarray\n   if isinstance(tup, (np.ndarray, Array)):\n-    arrs = jax.vmap(lambda x: atleast_2d(x).T)(tup) if tup.ndim < 3 else tup\n+    arrs = api.vmap(lambda x: atleast_2d(x).T)(tup) if tup.ndim < 3 else tup\n   else:\n     # TODO(jakevdp): Non-array input deprecated 2023-09-22; change to error.\n     util.check_arraylike(\"column_stack\", *tup, emit_warning=True)\n@@ -5354,7 +5354,7 @@ def _make_string_array(\n     )\n \n   # Just do a device_put since XLA does not support string as a data type.\n-  return jax.device_put(x=object, device=device)\n+  return api.device_put(x=object, device=device)\n \n \n @export\n@@ -5447,7 +5447,7 @@ def array(object: Any, dtype: DTypeLike | None = None, copy: bool = True,\n       (dtype is None or dtype == object.dtype) and (ndmin <= object.ndim) and\n       device is None):\n     # Keep the output uncommitted.\n-    return jax.device_put(object)\n+    return api.device_put(object)\n \n   # String arrays need separate handling because XLA does not support string\n   # as a data type.\n@@ -5551,7 +5551,7 @@ def _get_platform(\n     return device_or_sharding\n   elif device_or_sharding is None:\n     if config.default_device.value is None:\n-      return jax.default_backend()\n+      return xla_bridge.default_backend()\n     else:\n       return _get_platform(config.default_device.value)\n   else:\n@@ -6077,7 +6077,7 @@ def fromfunction(function: Callable[..., Array], shape: Any,\n   shape = core.canonicalize_shape(shape, context=\"shape argument of jnp.fromfunction()\")\n   for i in range(len(shape)):\n     in_axes = [0 if i == j else None for j in range(len(shape))]\n-    function = jax.vmap(function, in_axes=tuple(in_axes[::-1]))\n+    function = api.vmap(function, in_axes=tuple(in_axes[::-1]))\n   return function(*(arange(s, dtype=dtype) for s in shape), **kwargs)\n \n \n@@ -6166,7 +6166,7 @@ def eye(N: DimSize, M: DimSize | None = None,\n   # instead of putting it on default device and then on the specific device\n   output = _eye(N, M=M, k=k, dtype=dtype)\n   if device is not None:\n-    return jax.device_put(output, device=device)\n+    return api.device_put(output, device=device)\n   return output\n \n \n@@ -6299,7 +6299,7 @@ def arange(start: ArrayLike | DimSize, stop: ArrayLike | DimSize | None = None,\n   # instead of putting it on default device and then on the specific device\n   output = _arange(start, stop=stop, step=step, dtype=dtype)\n   if device is not None:\n-    return jax.device_put(output, device=device)\n+    return api.device_put(output, device=device)\n   return output\n \n \n@@ -6496,7 +6496,7 @@ def _i0(x):\n \n @_i0.defjvp\n def _i0_jvp(primals, tangents):\n-  primal_out, tangent_out = jax.jvp(_i0.fun, primals, tangents)\n+  primal_out, tangent_out = api.jvp(_i0.fun, primals, tangents)\n   return primal_out, where(primals[0] == 0, 0.0, tangent_out)\n \n @export\n@@ -7792,7 +7792,7 @@ def trim_zeros(filt: ArrayLike, trim: str ='fb') -> Array:\n   util.check_arraylike(\"trim_zeros\", filt, emit_warning=True)\n   core.concrete_or_error(None, filt,\n                          \"Error arose in the `filt` argument of trim_zeros()\")\n-  filt_arr = jax.numpy.asarray(filt)\n+  filt_arr = asarray(filt)\n   del filt\n   if filt_arr.ndim != 1:\n     # Added on 2024-09-11\n@@ -8173,9 +8173,9 @@ def apply_along_axis(\n   axis = _canonicalize_axis(axis, num_dims)\n   func = lambda arr: func1d(arr, *args, **kwargs)\n   for i in range(1, num_dims - axis):\n-    func = jax.vmap(func, in_axes=i, out_axes=-1)\n+    func = api.vmap(func, in_axes=i, out_axes=-1)\n   for i in range(axis):\n-    func = jax.vmap(func, in_axes=0, out_axes=0)\n+    func = api.vmap(func, in_axes=0, out_axes=0)\n   return func(arr)\n \n \n@@ -9623,7 +9623,7 @@ def _rank(x):\n \n def _searchsorted_via_compare_all(sorted_arr: Array, query: Array, side: str, dtype: type) -> Array:\n   op = _sort_lt_comparator if side == 'left' else _sort_le_comparator\n-  comparisons = jax.vmap(op, in_axes=(0, None))(sorted_arr, query)\n+  comparisons = api.vmap(op, in_axes=(0, None))(sorted_arr, query)\n   return comparisons.sum(dtype=dtype, axis=0)\n \n \ndiff --git a/jax/_src/numpy/linalg.py b/jax/_src/numpy/linalg.py\nindex 0e20e5b2a416..f2deddd52f05 100644\n--- a/jax/_src/numpy/linalg.py\n+++ b/jax/_src/numpy/linalg.py\n@@ -23,10 +23,11 @@\n import operator\n from typing import Literal, NamedTuple, overload\n \n-import jax\n-from jax import jit, custom_jvp\n from jax import lax\n \n+from jax._src.api import jit\n+from jax._src import config\n+from jax._src.custom_derivatives import custom_jvp\n from jax._src import deprecations\n from jax._src.lax import lax as lax_internal\n from jax._src.lax.lax import PrecisionLike\n@@ -44,24 +45,24 @@\n \n \n class EighResult(NamedTuple):\n-  eigenvalues: jax.Array\n-  eigenvectors: jax.Array\n+  eigenvalues: Array\n+  eigenvectors: Array\n \n \n class QRResult(NamedTuple):\n-  Q: jax.Array\n-  R: jax.Array\n+  Q: Array\n+  R: Array\n \n \n class SlogdetResult(NamedTuple):\n-  sign: jax.Array\n-  logabsdet: jax.Array\n+  sign: Array\n+  logabsdet: Array\n \n \n class SVDResult(NamedTuple):\n-  U: jax.Array\n-  S: jax.Array\n-  Vh: jax.Array\n+  U: Array\n+  S: Array\n+  Vh: Array\n \n \n def _H(x: ArrayLike) -> Array:\n@@ -995,7 +996,7 @@ def _pinv(a: ArrayLike, rtol: ArrayLike | None = None, hermitian: bool = False)\n \n \n @_pinv.defjvp\n-@jax.default_matmul_precision(\"float32\")\n+@config.default_matmul_precision(\"float32\")\n def _pinv_jvp(rtol, hermitian, primals, tangents):\n   # The Differentiation of Pseudo-Inverses and Nonlinear Least Squares Problems\n   # Whose Variables Separate. Author(s): G. H. Golub and V. Pereyra. SIAM\n@@ -1617,7 +1618,7 @@ def matrix_transpose(x: ArrayLike, /) -> Array:\n   ndim = x_arr.ndim\n   if ndim < 2:\n     raise ValueError(f\"matrix_transpose requres at least 2 dimensions; got {ndim=}\")\n-  return jax.lax.transpose(x_arr, (*range(ndim - 2), ndim - 1, ndim - 2))\n+  return lax.transpose(x_arr, (*range(ndim - 2), ndim - 1, ndim - 2))\n \n \n @export\ndiff --git a/jax/_src/numpy/polynomial.py b/jax/_src/numpy/polynomial.py\nindex 2b2923ba93ce..2f7a32c3f52d 100644\n--- a/jax/_src/numpy/polynomial.py\n+++ b/jax/_src/numpy/polynomial.py\n@@ -19,8 +19,8 @@\n \n import numpy as np\n \n-from jax import jit\n from jax import lax\n+from jax._src.api import jit\n from jax._src import dtypes\n from jax._src import core\n from jax._src.lax import lax as lax_internal\ndiff --git a/jax/_src/numpy/reductions.py b/jax/_src/numpy/reductions.py\nindex cbfda25eafcf..e1f499ccc530 100644\n--- a/jax/_src/numpy/reductions.py\n+++ b/jax/_src/numpy/reductions.py\n@@ -23,9 +23,9 @@\n \n import numpy as np\n \n-import jax\n from jax import lax\n from jax._src import api\n+from jax._src import config\n from jax._src import core\n from jax._src import deprecations\n from jax._src import dtypes\n@@ -793,7 +793,7 @@ def _axis_size(a: ArrayLike, axis: int | Sequence[int]):\n   size = 1\n   a_shape = np.shape(a)\n   for a in axis_seq:\n-    size *= maybe_named_axis(a, lambda i: a_shape[i], jax.lax.axis_size)\n+    size *= maybe_named_axis(a, lambda i: a_shape[i], lax.axis_size)\n   return size\n \n \n@@ -1136,7 +1136,7 @@ def _var(a: Array, axis: Axis = None, dtype: DTypeLike | None = None,\n   normalizer = lax.sub(normalizer, lax.convert_element_type(correction, computation_dtype))\n   result = sum(centered, axis, dtype=computation_dtype, keepdims=keepdims, where=where)\n   result = lax.div(result, normalizer).astype(dtype)\n-  with jax.debug_nans(False):\n+  with config.debug_nans(False):\n     result = _where(normalizer > 0, result, np.nan)\n   return result\n \n@@ -2513,7 +2513,7 @@ def _quantile(a: Array, q: Array, axis: int | tuple[int, ...] | None,\n     index[axis] = high\n     high_value = a[tuple(index)]\n   else:\n-    with jax.debug_nans(False):\n+    with config.debug_nans(False):\n       a = _where(any(lax_internal._isnan(a), axis=axis, keepdims=True), np.nan, a)\n     a = lax.sort(a, dimension=axis)\n     n = lax.convert_element_type(a_shape[axis], lax_internal._dtype(q))\ndiff --git a/jax/_src/numpy/scalar_types.py b/jax/_src/numpy/scalar_types.py\nindex 2b0e04adc997..1abe7cf66c15 100644\n--- a/jax/_src/numpy/scalar_types.py\n+++ b/jax/_src/numpy/scalar_types.py\n@@ -22,11 +22,11 @@\n \n from typing import Any\n \n-import jax\n+import numpy as np\n+\n from jax._src.typing import Array\n from jax._src import core\n from jax._src import dtypes\n-import numpy as np\n \n \n # Some objects below rewrite their __module__ attribute to this name.\n@@ -46,7 +46,8 @@ def __ne__(self, other: Any) -> bool:\n     return not (self == other)\n \n   def __call__(self, x: Any) -> Array:\n-    return jax.numpy.asarray(x, dtype=self.dtype)\n+    from jax._src.numpy.lax_numpy import asarray\n+    return asarray(x, dtype=self.dtype)\n \n   def __instancecheck__(self, instance: Any) -> bool:\n     return isinstance(instance, self.dtype.type)\ndiff --git a/jax/_src/numpy/setops.py b/jax/_src/numpy/setops.py\nindex d4a8e41dd317..ef1d44ae01b1 100644\n--- a/jax/_src/numpy/setops.py\n+++ b/jax/_src/numpy/setops.py\n@@ -21,10 +21,9 @@\n \n import numpy as np\n \n-import jax\n-from jax import jit\n from jax import lax\n \n+from jax._src.api import jit\n from jax._src import core\n from jax._src import dtypes\n from jax._src.lax import lax as lax_internal\n@@ -59,8 +58,10 @@ def _in1d(ar1: ArrayLike, ar2: ArrayLike, invert: bool,\n     else:\n       return (arr1[:, None] == arr2[None, :]).any(-1)\n   elif method == 'binary_search':\n+    from jax._src.numpy.lax_numpy import searchsorted\n+\n     arr2 = lax.sort(arr2)\n-    ind = jax.numpy.searchsorted(arr2, arr1)\n+    ind = searchsorted(arr2, arr1)\n     if invert:\n       return arr1 != arr2[ind]\n     else:\ndiff --git a/jax/_src/numpy/sorting.py b/jax/_src/numpy/sorting.py\nindex a0f368e2ef07..be8f42ce6145 100644\n--- a/jax/_src/numpy/sorting.py\n+++ b/jax/_src/numpy/sorting.py\n@@ -17,14 +17,14 @@\n \n import numpy as np\n \n-import jax\n+from jax import lax\n+\n from jax._src import api\n from jax._src import core\n from jax._src import dtypes\n from jax._src.numpy import util\n from jax._src.util import canonicalize_axis, set_module\n from jax._src.typing import Array, ArrayLike\n-from jax import lax\n \n export = set_module('jax.numpy')\n \n@@ -226,7 +226,7 @@ def partition(a: ArrayLike, kth: int, axis: int = -1) -> Array:\n   axis = canonicalize_axis(axis, arr.ndim)\n   kth = canonicalize_axis(kth, arr.shape[axis])\n \n-  arr = jax.numpy.swapaxes(arr, axis, -1)\n+  arr = arr.swapaxes(axis, -1)\n   if dtypes.isdtype(arr.dtype, \"unsigned integer\"):\n     # Here, we apply a trick to handle correctly 0 values for unsigned integers\n     bottom = -lax.top_k(-(arr + 1), kth + 1)[0] - 1\n@@ -234,7 +234,7 @@ def partition(a: ArrayLike, kth: int, axis: int = -1) -> Array:\n     bottom = -lax.top_k(-arr, kth + 1)[0]\n   top = lax.top_k(arr, arr.shape[-1] - kth - 1)[0]\n   out = lax.concatenate([bottom, top], dimension=arr.ndim - 1)\n-  return jax.numpy.swapaxes(out, -1, axis)\n+  return out.swapaxes(-1, axis)\n \n \n @export\n@@ -297,7 +297,7 @@ def argpartition(a: ArrayLike, kth: int, axis: int = -1) -> Array:\n   axis = canonicalize_axis(axis, arr.ndim)\n   kth = canonicalize_axis(kth, arr.shape[axis])\n \n-  arr = jax.numpy.swapaxes(arr, axis, -1)\n+  arr = arr.swapaxes(axis, -1)\n   if dtypes.isdtype(arr.dtype, \"unsigned integer\"):\n     # Here, we apply a trick to handle correctly 0 values for unsigned integers\n     bottom_ind = lax.top_k(-(arr + 1), kth + 1)[1]\n@@ -307,11 +307,11 @@ def argpartition(a: ArrayLike, kth: int, axis: int = -1) -> Array:\n   # To avoid issues with duplicate values, we compute the top indices via a proxy\n   set_to_zero = lambda a, i: a.at[i].set(0)\n   for _ in range(arr.ndim - 1):\n-    set_to_zero = jax.vmap(set_to_zero)\n-  proxy = set_to_zero(jax.numpy.ones(arr.shape), bottom_ind)\n+    set_to_zero = api.vmap(set_to_zero)\n+  proxy = set_to_zero(lax.full(arr.shape, 1.0), bottom_ind)\n   top_ind = lax.top_k(proxy, arr.shape[-1] - kth - 1)[1]\n   out = lax.concatenate([bottom_ind, top_ind], dimension=arr.ndim - 1)\n-  return jax.numpy.swapaxes(out, -1, axis)\n+  return out.swapaxes(-1, axis)\n \n \n @export\n@@ -421,7 +421,7 @@ def lexsort(keys: Array | np.ndarray | Sequence[ArrayLike], axis: int = -1) -> A\n   if len({np.shape(key) for key in key_arrays}) > 1:\n     raise ValueError(\"all keys need to be the same shape\")\n   if np.ndim(key_arrays[0]) == 0:\n-    return jax.numpy.array(0, dtype=dtypes.canonicalize_dtype(dtypes.int_))\n+    return lax.full((), 0, dtypes.canonicalize_dtype(dtypes.int_))\n   axis = canonicalize_axis(axis, np.ndim(key_arrays[0]))\n   use_64bit_index = key_arrays[0].shape[axis] >= (1 << 31)\n   iota = lax.broadcasted_iota(np.dtype('int64') if use_64bit_index else dtypes.int_,\ndiff --git a/jax/_src/numpy/tensor_contractions.py b/jax/_src/numpy/tensor_contractions.py\nindex 979f68e28f6d..255da08e1816 100644\n--- a/jax/_src/numpy/tensor_contractions.py\n+++ b/jax/_src/numpy/tensor_contractions.py\n@@ -20,7 +20,6 @@\n \n import numpy as np\n \n-import jax\n from jax import lax\n from jax._src import core\n from jax._src import dtypes\n@@ -378,7 +377,7 @@ def vdot(\n   a, b = util.ensure_arraylike(\"vdot\", a, b)\n   if dtypes.issubdtype(dtypes.dtype(a, canonicalize=True), np.complexfloating):\n     a = ufuncs.conj(a)\n-  return dot(jax.numpy.ravel(a), jax.numpy.ravel(b), precision=precision,\n+  return dot(a.ravel(), b.ravel(), precision=precision,\n              preferred_element_type=preferred_element_type)\n \n \n@@ -429,11 +428,13 @@ def vecdot(x1: ArrayLike, x2: ArrayLike, /, *, axis: int = -1,\n     >>> jnp.linalg.vecdot(a, b, axis=-1)\n     Array([20, 47], dtype=int32)\n   \"\"\"\n+  from jax._src.numpy.lax_numpy import moveaxis\n+\n   x1_arr, x2_arr = util.ensure_arraylike(\"jnp.vecdot\", x1, x2)\n   if x1_arr.shape[axis] != x2_arr.shape[axis]:\n     raise ValueError(f\"axes must match; got shapes {x1_arr.shape} and {x2_arr.shape} with {axis=}\")\n-  x1_arr = jax.numpy.moveaxis(x1_arr, axis, -1)\n-  x2_arr = jax.numpy.moveaxis(x2_arr, axis, -1)\n+  x1_arr = moveaxis(x1_arr, axis, -1)\n+  x2_arr = moveaxis(x2_arr, axis, -1)\n   return vectorize(partial(vdot, precision=precision, preferred_element_type=preferred_element_type),\n                    signature=\"(n),(n)->()\")(x1_arr, x2_arr)\n \n@@ -604,8 +605,9 @@ def inner(\n   \"\"\"\n   a, b = util.ensure_arraylike(\"inner\", a, b)\n   if np.ndim(a) == 0 or np.ndim(b) == 0:\n-    a = jax.numpy.asarray(a, dtype=preferred_element_type)\n-    b = jax.numpy.asarray(b, dtype=preferred_element_type)\n+    if preferred_element_type is not None:\n+      a = a.astype(preferred_element_type)\n+      b = b.astype(preferred_element_type)\n     return a * b\n   return tensordot(a, b, (-1, -1), precision=precision,\n                    preferred_element_type=preferred_element_type)\n@@ -643,4 +645,4 @@ def outer(a: ArrayLike, b: ArrayLike, out: None = None) -> Array:\n     raise NotImplementedError(\"The 'out' argument to jnp.outer is not supported.\")\n   a, b = util.ensure_arraylike(\"outer\", a, b)\n   a, b = util.promote_dtypes(a, b)\n-  return jax.numpy.ravel(a)[:, None] * jax.numpy.ravel(b)[None, :]\n+  return a.ravel()[:, None] * b.ravel()[None, :]\ndiff --git a/jax/_src/numpy/ufunc_api.py b/jax/_src/numpy/ufunc_api.py\nindex da55212bae1f..243ab9aa0878 100644\n--- a/jax/_src/numpy/ufunc_api.py\n+++ b/jax/_src/numpy/ufunc_api.py\n@@ -22,9 +22,11 @@\n import operator\n from typing import Any\n \n-import jax\n+from jax._src import api\n from jax._src.typing import Array, ArrayLike, DTypeLike\n-from jax._src.lax import lax as lax_internal\n+from jax._src.lax import control_flow\n+from jax._src.lax import slicing\n+from jax._src.lax import lax\n from jax._src.numpy import indexing\n import jax._src.numpy.lax_numpy as jnp\n from jax._src.numpy.reductions import _moveaxis\n@@ -179,11 +181,11 @@ def __call__(self, *args: ArrayLike, out: None = None, where: None = None) -> An\n     call = self.__static_props['call'] or self._call_vectorized\n     return call(*args)\n \n-  @partial(jax.jit, static_argnames=['self'])\n+  @partial(api.jit, static_argnames=['self'])\n   def _call_vectorized(self, *args):\n     return vectorize(self._func)(*args)\n \n-  @partial(jax.jit, static_argnames=['self', 'axis', 'dtype', 'out', 'keepdims'])\n+  @partial(api.jit, static_argnames=['self', 'axis', 'dtype', 'out', 'keepdims'])\n   def reduce(self, a: ArrayLike, axis: int | None = 0,\n              dtype: DTypeLike | None = None,\n              out: None = None, keepdims: bool = False, initial: ArrayLike | None = None,\n@@ -249,8 +251,8 @@ def reduce(self, a: ArrayLike, axis: int | None = 0,\n       if self.identity is None and initial is None:\n         raise ValueError(f\"reduction operation {self.__name__!r} does not have an identity, \"\n                          \"so to use a where mask one has to specify 'initial'.\")\n-      if lax_internal._dtype(where) != bool:\n-        raise ValueError(f\"where argument must have dtype=bool; got dtype={lax_internal._dtype(where)}\")\n+      if lax._dtype(where) != bool:\n+        raise ValueError(f\"where argument must have dtype=bool; got dtype={lax._dtype(where)}\")\n     reduce = self.__static_props['reduce'] or self._reduce_via_scan\n     return reduce(a, axis=axis, dtype=dtype, keepdims=keepdims, initial=initial, where=where)\n \n@@ -258,11 +260,11 @@ def _reduce_via_scan(self, arr: ArrayLike, axis: int | None = 0, dtype: DTypeLik\n                        keepdims: bool = False, initial: ArrayLike | None = None,\n                        where: ArrayLike | None = None) -> Array:\n     assert self.nin == 2 and self.nout == 1\n-    arr = lax_internal.asarray(arr)\n+    arr = lax.asarray(arr)\n     if initial is None:\n       initial = self.identity\n     if dtype is None:\n-      dtype = jax.eval_shape(self._func, lax_internal._one(arr), lax_internal._one(arr)).dtype\n+      dtype = api.eval_shape(self._func, lax._one(arr), lax._one(arr)).dtype\n     if where is not None:\n       where = _broadcast_to(where, arr.shape)\n     if isinstance(axis, tuple):\n@@ -306,15 +308,15 @@ def body_fun(i, val):\n     else:\n       start_index = 0\n       start_value = initial\n-    start_value = _broadcast_to(lax_internal.asarray(start_value).astype(dtype), arr.shape[1:])\n+    start_value = _broadcast_to(lax.asarray(start_value).astype(dtype), arr.shape[1:])\n \n-    result = jax.lax.fori_loop(start_index, arr.shape[0], body_fun, start_value)\n+    result = control_flow.fori_loop(start_index, arr.shape[0], body_fun, start_value)\n \n     if keepdims:\n       result = result.reshape(final_shape)\n     return result\n \n-  @partial(jax.jit, static_argnames=['self', 'axis', 'dtype'])\n+  @partial(api.jit, static_argnames=['self', 'axis', 'dtype'])\n   def accumulate(self, a: ArrayLike, axis: int = 0, dtype: DTypeLike | None = None,\n                  out: None = None) -> Array:\n     \"\"\"Accumulate operation derived from binary ufunc.\n@@ -376,10 +378,10 @@ def _accumulate_via_scan(self, arr: ArrayLike, axis: int = 0,\n                            dtype: DTypeLike | None = None) -> Array:\n     assert self.nin == 2 and self.nout == 1\n     check_arraylike(f\"{self.__name__}.accumulate\", arr)\n-    arr = lax_internal.asarray(arr)\n+    arr = lax.asarray(arr)\n \n     if dtype is None:\n-      dtype = jax.eval_shape(self._func, lax_internal._one(arr), lax_internal._one(arr)).dtype\n+      dtype = api.eval_shape(self._func, lax._one(arr), lax._one(arr)).dtype\n \n     if axis is None or isinstance(axis, tuple):\n       raise ValueError(\"accumulate does not allow multiple axes\")\n@@ -390,10 +392,10 @@ def scan_fun(carry, _):\n       i, x = carry\n       y = _where(i == 0, arr[0].astype(dtype), self(x.astype(dtype), arr[i].astype(dtype)))\n       return (i + 1, y), y\n-    _, result = jax.lax.scan(scan_fun, (0, arr[0].astype(dtype)), None, length=arr.shape[0])\n+    _, result = control_flow.scan(scan_fun, (0, arr[0].astype(dtype)), None, length=arr.shape[0])\n     return _moveaxis(result, 0, axis)\n \n-  @partial(jax.jit, static_argnums=[0], static_argnames=['inplace'])\n+  @partial(api.jit, static_argnums=[0], static_argnames=['inplace'])\n   def at(self, a: ArrayLike, indices: Any, b: ArrayLike | None = None, /, *,\n          inplace: bool = True) -> Array:\n     \"\"\"Update elements of an array via the specified unary or binary ufunc.\n@@ -440,15 +442,15 @@ def at(self, a: ArrayLike, indices: Any, b: ArrayLike | None = None, /, *,\n   def _at_via_scan(self, a: ArrayLike, indices: Any, *args: Any) -> Array:\n     assert len(args) in {0, 1}\n     check_arraylike(f\"{self.__name__}.at\", a, *args)\n-    dtype = jax.eval_shape(self._func, lax_internal._one(a), *(lax_internal._one(arg) for arg in args)).dtype\n-    a = lax_internal.asarray(a).astype(dtype)\n-    args = tuple(lax_internal.asarray(arg).astype(dtype) for arg in args)\n+    dtype = api.eval_shape(self._func, lax._one(a), *(lax._one(arg) for arg in args)).dtype\n+    a = lax.asarray(a).astype(dtype)\n+    args = tuple(lax.asarray(arg).astype(dtype) for arg in args)\n     indices = indexing.eliminate_deprecated_list_indexing(indices)\n     if not indices:\n       return a\n \n     shapes = [np.shape(i) for i in indices if not isinstance(i, slice)]\n-    shape = shapes and jax.lax.broadcast_shapes(*shapes)\n+    shape = shapes and lax.broadcast_shapes(*shapes)\n     if not shape:\n       return a.at[indices].set(self(a.at[indices].get(), *args))\n \n@@ -462,10 +464,10 @@ def scan_fun(carry, x):\n       idx = tuple(ind if isinstance(ind, slice) else ind[i] for ind in indices)\n       a = a.at[idx].set(self(a.at[idx].get(), *(arg[i] for arg in args)))\n       return (i + 1, a), x\n-    carry, _ = jax.lax.scan(scan_fun, (0, a), None, len(indices[0]))  # type: ignore[arg-type]\n+    carry, _ = control_flow.scan(scan_fun, (0, a), None, len(indices[0]))  # type: ignore[arg-type]\n     return carry[1]\n \n-  @partial(jax.jit, static_argnames=['self', 'axis', 'dtype'])\n+  @partial(api.jit, static_argnames=['self', 'axis', 'dtype'])\n   def reduceat(self, a: ArrayLike, indices: Any, axis: int = 0,\n                dtype: DTypeLike | None = None, out: None = None) -> Array:\n     \"\"\"Reduce an array between specified indices via a binary ufunc.\n@@ -517,7 +519,7 @@ def reduceat(self, a: ArrayLike, indices: Any, axis: int = 0,\n   def _reduceat_via_scan(self, a: ArrayLike, indices: Any, axis: int = 0,\n                          dtype: DTypeLike | None = None) -> Array:\n     check_arraylike(f\"{self.__name__}.reduceat\", a, indices)\n-    a = lax_internal.asarray(a)\n+    a = lax.asarray(a)\n     idx_tuple = indexing.eliminate_deprecated_list_indexing(indices)\n     assert len(idx_tuple) == 1\n     indices = idx_tuple[0]\n@@ -531,17 +533,17 @@ def _reduceat_via_scan(self, a: ArrayLike, indices: Any, axis: int = 0,\n       raise ValueError(\"reduceat requires a single integer axis.\")\n     axis = canonicalize_axis(axis, a.ndim)\n     out = indexing.take(a, indices, axis=axis)\n-    ind = jax.lax.expand_dims(jnp.append(indices, a.shape[axis]),\n-                              list(np.delete(np.arange(out.ndim), axis)))\n-    ind_start = jax.lax.slice_in_dim(ind, 0, ind.shape[axis] - 1, axis=axis)\n-    ind_end = jax.lax.slice_in_dim(ind, 1, ind.shape[axis], axis=axis)\n+    ind = lax.expand_dims(jnp.append(indices, a.shape[axis]),\n+                                   list(np.delete(np.arange(out.ndim), axis)))\n+    ind_start = slicing.slice_in_dim(ind, 0, ind.shape[axis] - 1, axis=axis)\n+    ind_end = slicing.slice_in_dim(ind, 1, ind.shape[axis], axis=axis)\n     def loop_body(i, out):\n       return _where((i > ind_start) & (i < ind_end),\n-                    self(out, indexing.take(a, jax.lax.expand_dims(i, (0,)), axis=axis)),\n+                    self(out, indexing.take(a, lax.expand_dims(i, (0,)), axis=axis)),\n                     out)\n-    return jax.lax.fori_loop(0, a.shape[axis], loop_body, out)\n+    return control_flow.fori_loop(0, a.shape[axis], loop_body, out)\n \n-  @partial(jax.jit, static_argnums=[0])\n+  @partial(api.jit, static_argnums=[0])\n   def outer(self, A: ArrayLike, B: ArrayLike, /) -> Array:\n     \"\"\"Apply the function to all pairs of values in ``A`` and ``B``.\n \n@@ -584,8 +586,8 @@ def outer(self, A: ArrayLike, B: ArrayLike, /) -> Array:\n     if self.nout != 1:\n       raise ValueError(\"outer only supported for functions returning a single value\")\n     check_arraylike(f\"{self.__name__}.outer\", A, B)\n-    _ravel = lambda A: jax.lax.reshape(A, (np.size(A),))\n-    result = jax.vmap(jax.vmap(self, (None, 0)), (0, None))(_ravel(A), _ravel(B))\n+    _ravel = lambda A: lax.reshape(A, (np.size(A),))\n+    result = api.vmap(api.vmap(self, (None, 0)), (0, None))(_ravel(A), _ravel(B))\n     return result.reshape(*np.shape(A), *np.shape(B))\n \n \ndiff --git a/jax/_src/numpy/vectorize.py b/jax/_src/numpy/vectorize.py\nindex 5ea9d697d27d..f166a96a4693 100644\n--- a/jax/_src/numpy/vectorize.py\n+++ b/jax/_src/numpy/vectorize.py\n@@ -23,7 +23,7 @@\n \n from jax._src import api\n from jax._src import config\n-from jax import lax\n+from jax._src.lax import lax\n from jax._src.numpy import lax_numpy as jnp\n from jax._src.util import set_module, safe_map as map, safe_zip as zip\n \ndiff --git a/jax/_src/numpy/window_functions.py b/jax/_src/numpy/window_functions.py\nindex 96a15db777a8..6d1bfb245272 100644\n--- a/jax/_src/numpy/window_functions.py\n+++ b/jax/_src/numpy/window_functions.py\n@@ -16,11 +16,11 @@\n \n from jax._src import core\n from jax._src import dtypes\n+from jax._src.lax import lax\n from jax._src.numpy import lax_numpy\n from jax._src.numpy import ufuncs\n from jax._src.typing import Array, ArrayLike\n from jax._src.util import set_module\n-from jax import lax\n \n export = set_module('jax.numpy')\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[imports] avoid top-level imports in jax.numpy sources",
        "output": "```diff\nCommit: cecf2f6bede94324c4fc159c428a3d024ce431fe\nDate: 2025-06-03T17:13:39Z\nURL: https://github.com/jax-ml/jax/commit/cecf2f6bede94324c4fc159c428a3d024ce431fe\nFiles changed: 19\nAdditions: +182, Deletions: -167\ndiff --git a/jax/_src/lax/fft.py b/jax/_src/lax/fft.py\nindex 2eebe6d91f22..08e06287b784 100644\n--- a/jax/_src/lax/fft.py\n+++ b/jax/_src/lax/fft.py\n@@ -21,8 +21,6 @@\n \n import numpy as np\n \n-from jax import lax\n-\n from jax._src import dispatch\n from jax._src import dtypes\n from jax._src.api import jit, linear_transpose, ShapeDtypeStruct\n@@ -30,6 +28,7 @@\n from jax._src.interpreters import ad\n from jax._src.interpreters import batching\n from jax._src.interpreters import mlir\n+from jax._src.lax import lax\n from jax._src.lib.mlir.dialects import hlo\n \n __all__ = [\ndiff --git a/jax/_src/numpy/array_api_metadata.py b/jax/_src/numpy/array_api_metadata.py\nindex d634a2856a1b..af4e27cd5f8d 100644\n--- a/jax/_src/numpy/array_api_metadata.py\n+++ b/jax/_src/numpy/array_api_metadata.py\n@@ -21,7 +21,6 @@\n \n from types import ModuleType\n \n-import jax\n from jax._src.sharding import Sharding\n from jax._src.lib import xla_client as xc\n from jax._src import config\n@@ -40,6 +39,7 @@ def __array_namespace__(self, *, api_version: None | str = None) -> ModuleType:\n   if api_version is not None and api_version != __array_api_version__:\n     raise ValueError(f\"{api_version=!r} is not available; \"\n                      f\"available versions are: {[__array_api_version__]}\")\n+  import jax.numpy  # pytype: disable=import-error\n   return jax.numpy\n \n \n@@ -77,7 +77,7 @@ def default_device(self):\n   def devices(self):\n     out = [None]  # None indicates \"uncommitted\"\n     for backend in xb.backends():\n-        out.extend(jax.devices(backend))\n+        out.extend(xb.devices(backend))\n     return out\n \n   def capabilities(self):\ndiff --git a/jax/_src/numpy/array_creation.py b/jax/_src/numpy/array_creation.py\nindex 86bcfb2c02f6..b14c2fe73faa 100644\n--- a/jax/_src/numpy/array_creation.py\n+++ b/jax/_src/numpy/array_creation.py\n@@ -19,18 +19,16 @@\n \n import numpy as np\n \n-import jax\n-from jax import lax\n-from jax._src.api import jit\n+from jax._src.api import device_put, jit\n from jax._src import core\n from jax._src import dtypes\n-from jax._src.lax import lax as lax_internal\n+from jax._src.lax import lax\n from jax._src.lib import xla_client as xc\n from jax._src.numpy import ufuncs\n from jax._src.numpy import util\n+from jax._src.sharding import Sharding\n from jax._src.typing import Array, ArrayLike, DuckTypedArray, DTypeLike\n from jax._src.util import canonicalize_axis, set_module\n-from jax.sharding import Sharding\n \n \n export = set_module('jax.numpy')\n@@ -205,6 +203,8 @@ def full(shape: Any, fill_value: ArrayLike,\n     Array([[0, 1, 2],\n            [0, 1, 2]], dtype=int32)\n   \"\"\"\n+  from jax._src.numpy.lax_numpy import asarray  # pytype: disable=import-error\n+\n   dtypes.check_user_dtype_supported(dtype, \"full\")\n   util.check_arraylike(\"full\", fill_value)\n \n@@ -212,8 +212,8 @@ def full(shape: Any, fill_value: ArrayLike,\n     shape = canonicalize_shape(shape)\n     return lax.full(shape, fill_value, dtype, sharding=util.normalize_device_to_sharding(device))\n   else:\n-    return jax.device_put(\n-        util._broadcast_to(jax.numpy.asarray(fill_value, dtype=dtype), shape), device)\n+    return device_put(\n+        util._broadcast_to(asarray(fill_value, dtype=dtype), shape), device)\n \n \n @export\n@@ -394,6 +394,8 @@ def full_like(a: ArrayLike | DuckTypedArray,\n     Array([[1, 1, 1],\n            [2, 2, 2]], dtype=int32)\n   \"\"\"\n+  from jax._src.numpy.lax_numpy import asarray  # pytype: disable=import-error\n+\n   if hasattr(a, 'dtype') and hasattr(a, 'shape'):  # support duck typing\n     util.check_arraylike(\"full_like\", 0, fill_value)\n   else:\n@@ -408,8 +410,8 @@ def full_like(a: ArrayLike | DuckTypedArray,\n   else:\n     shape = np.shape(a) if shape is None else shape  # type: ignore[arg-type]\n     dtype = dtypes.result_type(a) if dtype is None else dtype\n-    return jax.device_put(\n-        util._broadcast_to(jax.numpy.asarray(fill_value, dtype=dtype), shape), device)\n+    return device_put(\n+        util._broadcast_to(asarray(fill_value, dtype=dtype), shape), device)\n \n @overload\n def linspace(start: ArrayLike, stop: ArrayLike, num: int = 50,\n@@ -510,6 +512,8 @@ def _linspace(start: ArrayLike, stop: ArrayLike, num: int = 50,\n               axis: int = 0,\n               *, device: xc.Device | Sharding | None = None) -> Array | tuple[Array, Array]:\n   \"\"\"Implementation of linspace differentiable in start and stop args.\"\"\"\n+  from jax._src.numpy.lax_numpy import asarray  # pytype: disable=import-error\n+\n   dtypes.check_user_dtype_supported(dtype, \"linspace\")\n   if num < 0:\n     raise ValueError(f\"Number of samples, {num}, must be non-negative.\")\n@@ -529,13 +533,13 @@ def _linspace(start: ArrayLike, stop: ArrayLike, num: int = 50,\n   bounds_shape.insert(axis, 1)\n   div = (num - 1) if endpoint else num\n   if num > 1:\n-    delta: Array = lax.convert_element_type(stop - start, computation_dtype) / jax.numpy.array(div, dtype=computation_dtype)\n+    delta: Array = lax.convert_element_type(stop - start, computation_dtype) / asarray(div, dtype=computation_dtype)\n     iota_shape = [1,] * len(bounds_shape)\n     iota_shape[axis] = div\n     # This approach recovers the endpoints with float32 arithmetic,\n     # but can lead to rounding errors for integer outputs.\n     real_dtype = dtypes.finfo(computation_dtype).dtype\n-    step = lax.iota(real_dtype, div).reshape(iota_shape) / jax.numpy.array(div, real_dtype)\n+    step = lax.iota(real_dtype, div).reshape(iota_shape) / asarray(div, real_dtype)\n     step = step.astype(computation_dtype)\n     out = (broadcast_start.reshape(bounds_shape) * (1 - step) +\n       broadcast_stop.reshape(bounds_shape) * step)\n@@ -545,7 +549,7 @@ def _linspace(start: ArrayLike, stop: ArrayLike, num: int = 50,\n                             canonicalize_axis(axis, out.ndim))\n \n   elif num == 1:\n-    delta = jax.numpy.asarray(np.nan if endpoint else stop - start, dtype=computation_dtype)\n+    delta = asarray(np.nan if endpoint else stop - start, dtype=computation_dtype)\n     out = broadcast_start.reshape(bounds_shape)\n   else:  # num == 0 degenerate case, match numpy behavior\n     empty_shape = list(lax.broadcast_shapes(np.shape(start), np.shape(stop)))\n@@ -557,7 +561,7 @@ def _linspace(start: ArrayLike, stop: ArrayLike, num: int = 50,\n     out = lax.floor(out)\n \n   sharding = util.canonicalize_device_to_sharding(device)\n-  result = lax_internal._convert_element_type(out, dtype, sharding=sharding)\n+  result = lax._convert_element_type(out, dtype, sharding=sharding)\n   return (result, delta) if retstep else result\n \n \ndiff --git a/jax/_src/numpy/array_methods.py b/jax/_src/numpy/array_methods.py\nindex b29b95219325..958085e19a53 100644\n--- a/jax/_src/numpy/array_methods.py\n+++ b/jax/_src/numpy/array_methods.py\n@@ -29,9 +29,8 @@\n from typing import Any, Callable, Sequence\n \n import numpy as np\n-import jax\n+\n from jax import lax\n-from jax.sharding import Sharding\n from jax._src import api\n from jax._src import core\n from jax._src import dtypes\n@@ -44,6 +43,7 @@\n from jax._src.numpy import lax_numpy\n from jax._src.numpy import tensor_contractions\n from jax._src.pjit import PartitionSpec\n+from jax._src.sharding import Sharding\n from jax._src.sharding_impls import canonicalize_sharding, NamedSharding\n from jax._src.numpy import reductions\n from jax._src.numpy import ufuncs\n@@ -612,12 +612,13 @@ def _deepcopy(self: Array, memo: Any) -> Array:\n \n def __array_module__(self, types):\n   if all(issubclass(t, _HANDLED_ARRAY_TYPES) for t in types):\n+    import jax.numpy  # pytype: disable=import-error\n     return jax.numpy\n   else:\n     return NotImplemented\n \n \n-@partial(jax.jit, static_argnums=(1,2,3))\n+@partial(api.jit, static_argnums=(1,2,3))\n def _multi_slice(self: Array,\n                  start_indices: tuple[tuple[int, ...]],\n                  limit_indices: tuple[tuple[int, ...]],\n@@ -637,7 +638,7 @@ def _multi_slice(self: Array,\n \n # The next two functions are related to iter(array), implemented here to\n # avoid circular imports.\n-@jax.jit\n+@api.jit\n def _unstack(x: Array) -> list[Array]:\n   dims = (0,)\n   return [lax.squeeze(t, dims) for t in lax.split(x, (1,) * x.shape[0])]\n@@ -776,7 +777,7 @@ def __repr__(self) -> str:\n     return f\"_IndexUpdateRef({self.array!r}, {self.index!r})\"\n \n   def get(self, *, indices_are_sorted: bool = False, unique_indices: bool = False,\n-          mode: str | jax.lax.GatherScatterMode | None = None,\n+          mode: str | lax.GatherScatterMode | None = None,\n           fill_value: ArrayLike | None = None, out_sharding: Sharding | None = None):\n     \"\"\"Equivalent to ``x[idx]``.\n \n@@ -798,7 +799,7 @@ def get(self, *, indices_are_sorted: bool = False, unique_indices: bool = False,\n \n   def set(self, values: ArrayLike, *, indices_are_sorted: bool = False,\n           unique_indices: bool = False,\n-          mode: str | jax.lax.GatherScatterMode | None = None) -> None:\n+          mode: str | lax.GatherScatterMode | None = None) -> None:\n     \"\"\"Pure equivalent of ``x[idx] = y``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -816,7 +817,7 @@ def set(self, values: ArrayLike, *, indices_are_sorted: bool = False,\n \n   def apply(self, func: Callable[[ArrayLike], Array], *,\n             indices_are_sorted: bool = False, unique_indices: bool = False,\n-            mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+            mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``func.at(x, idx)`` for a unary ufunc ``func``.\n \n     Returns the value of ``x`` that would result from applying the unary\n@@ -840,7 +841,7 @@ def _scatter_apply(x, indices, y, dims, **kwargs):\n \n   def add(self, values: ArrayLike, *,\n           indices_are_sorted: bool = False, unique_indices: bool = False,\n-          mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+          mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] += y``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -855,7 +856,7 @@ def add(self, values: ArrayLike, *,\n \n   def subtract(self, values: ArrayLike, *,\n                indices_are_sorted: bool = False, unique_indices: bool = False,\n-               mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+               mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] -= y``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -870,7 +871,7 @@ def subtract(self, values: ArrayLike, *,\n \n   def multiply(self, values: ArrayLike, *,\n                indices_are_sorted: bool = False, unique_indices: bool = False,\n-               mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+               mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] *= y``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -887,7 +888,7 @@ def multiply(self, values: ArrayLike, *,\n \n   def divide(self, values: ArrayLike, *,\n              indices_are_sorted: bool = False, unique_indices: bool = False,\n-             mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+             mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] /= y``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -904,7 +905,7 @@ def divide(self, values: ArrayLike, *,\n \n   def power(self, values: ArrayLike, *,\n             indices_are_sorted: bool = False, unique_indices: bool = False,\n-            mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+            mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] **= y``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -921,7 +922,7 @@ def power(self, values: ArrayLike, *,\n \n   def min(self, values: ArrayLike, *,\n           indices_are_sorted: bool = False, unique_indices: bool = False,\n-          mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+          mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] = minimum(x[idx], y)``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\n@@ -937,7 +938,7 @@ def min(self, values: ArrayLike, *,\n \n   def max(self, values: ArrayLike, *,\n           indices_are_sorted: bool = False, unique_indices: bool = False,\n-          mode: str | jax.lax.GatherScatterMode | None = None) -> Array:\n+          mode: str | lax.GatherScatterMode | None = None) -> Array:\n     \"\"\"Pure equivalent of ``x[idx] = maximum(x[idx], y)``.\n \n     Returns the value of ``x`` that would result from the NumPy-style\ndiff --git a/jax/_src/numpy/error.py b/jax/_src/numpy/error.py\nindex e2c23b43bdf8..8af0c52566b5 100644\n--- a/jax/_src/numpy/error.py\n+++ b/jax/_src/numpy/error.py\n@@ -15,9 +15,11 @@\n import contextlib\n from typing import Literal, Sequence\n \n-import jax\n+import numpy as np\n+\n from jax._src import config\n-from jax._src.typing import ArrayLike\n+from jax._src import dtypes\n+from jax._src.typing import Array, ArrayLike\n \n Category = Literal[\"nan\", \"divide\", \"oob\"]\n \n@@ -40,7 +42,7 @@ def _is_category_disabled(\n \n \n def _set_error_if_with_category(\n-    pred: jax.Array,\n+    pred: Array,\n     /,\n     msg: str,\n     category: Category | None = None,\n@@ -65,7 +67,7 @@ def _set_error_if_with_category(\n   error_check_lib.set_error_if(pred, msg)\n \n \n-def _set_error_if_nan(pred: jax.Array, /):\n+def _set_error_if_nan(pred: Array, /):\n   \"\"\"Set the internal error state if any element of `pred` is `NaN`.\n \n   This function is disabled if the `jax_error_checking_behavior_nan` flag is\n@@ -74,17 +76,17 @@ def _set_error_if_nan(pred: jax.Array, /):\n   if config.error_checking_behavior_nan.value == \"ignore\":\n     return\n \n-  # TODO(mattjj): fix the circular import issue.\n-  import jax.numpy as jnp\n-  if not jnp.issubdtype(pred.dtype, jnp.floating):  # only check floats\n+  if not dtypes.issubdtype(pred.dtype, np.floating):  # only check floats\n     return\n \n   # TODO(mattjj): fix the circular import issue.\n   from jax._src import error_check as error_check_lib\n+  import jax.numpy as jnp\n+\n   error_check_lib.set_error_if(jnp.isnan(pred), \"NaN encountered\")\n \n \n-def _set_error_if_divide_by_zero(pred: jax.Array, /):\n+def _set_error_if_divide_by_zero(pred: Array, /):\n   \"\"\"Set the internal error state if any element of `pred` is zero.\n \n   This function is intended for checking if the denominator of a division is\ndiff --git a/jax/_src/numpy/fft.py b/jax/_src/numpy/fft.py\nindex 2316ad73ffeb..21da91ce613f 100644\n--- a/jax/_src/numpy/fft.py\n+++ b/jax/_src/numpy/fft.py\n@@ -18,8 +18,8 @@\n import operator\n import numpy as np\n \n-from jax import lax\n from jax._src import dtypes\n+from jax._src.lax import fft as lax_fft\n from jax._src.lib import xla_client\n from jax._src.util import safe_zip\n from jax._src.numpy.util import ensure_arraylike, promote_dtypes_inexact\n@@ -45,7 +45,7 @@ def _fft_norm(s: Array, func_name: str, norm: str) -> Array:\n                     '\"ortho\" or \"forward\".')\n \n \n-def _fft_core(func_name: str, fft_type: lax.FftType, a: ArrayLike,\n+def _fft_core(func_name: str, fft_type: lax_fft.FftType, a: ArrayLike,\n               s: Shape | None, axes: Sequence[int] | None,\n               norm: str | None) -> Array:\n   full_name = f\"jax.numpy.fft.{func_name}\"\n@@ -80,14 +80,14 @@ def _fft_core(func_name: str, fft_type: lax.FftType, a: ArrayLike,\n     in_s = list(arr.shape)\n     for axis, x in safe_zip(axes, s):\n       in_s[axis] = x\n-    if fft_type == lax.FftType.IRFFT:\n+    if fft_type == lax_fft.FftType.IRFFT:\n       in_s[-1] = (in_s[-1] // 2 + 1)\n     # Cropping\n     arr = arr[tuple(map(slice, in_s))]\n     # Padding\n     arr = jnp.pad(arr, [(0, x-y) for x, y in zip(in_s, arr.shape)])\n   else:\n-    if fft_type == lax.FftType.IRFFT:\n+    if fft_type == lax_fft.FftType.IRFFT:\n       s = [arr.shape[axis] for axis in axes[:-1]]\n       if axes:\n         s += [max(0, 2 * (arr.shape[axes[-1]] - 1))]\n@@ -103,10 +103,10 @@ def _fft_core(func_name: str, fft_type: lax.FftType, a: ArrayLike,\n   return transformed\n \n \n-def _fft_core_nd(arr: Array, fft_type: lax.FftType, s: Shape) -> Array:\n+def _fft_core_nd(arr: Array, fft_type: lax_fft.FftType, s: Shape) -> Array:\n   # XLA supports N-D transforms up to N=3 so we use XLA's FFT N-D directly.\n   if len(s) <= 3:\n-    return lax.fft(arr, fft_type, tuple(s))\n+    return lax_fft.fft(arr, fft_type, tuple(s))\n \n   # For larger N, we repeatedly apply N<=3 transforms until we reach the\n   # requested dimension. We special case N=4 to use two 2-D transforms instead\n@@ -115,16 +115,16 @@ def _fft_core_nd(arr: Array, fft_type: lax.FftType, s: Shape) -> Array:\n   n = 2 if len(s) == 4 else 3\n   src = tuple(range(arr.ndim - len(s), arr.ndim - n))\n   dst = tuple(range(arr.ndim - len(s) + n, arr.ndim))\n-  if fft_type in {lax.FftType.RFFT, lax.FftType.FFT}:\n-    arr = lax.fft(arr, fft_type, tuple(s)[-n:])\n+  if fft_type in {lax_fft.FftType.RFFT, lax_fft.FftType.FFT}:\n+    arr = lax_fft.fft(arr, fft_type, tuple(s)[-n:])\n     arr = jnp.moveaxis(arr, src, dst)\n-    arr = _fft_core_nd(arr, lax.FftType.FFT, s[:-n])\n+    arr = _fft_core_nd(arr, lax_fft.FftType.FFT, s[:-n])\n     arr = jnp.moveaxis(arr, dst, src)\n   else:\n     arr = jnp.moveaxis(arr, src, dst)\n-    arr = _fft_core_nd(arr, lax.FftType.IFFT, s[:-n])\n+    arr = _fft_core_nd(arr, lax_fft.FftType.IFFT, s[:-n])\n     arr = jnp.moveaxis(arr, dst, src)\n-    arr = lax.fft(arr, fft_type, tuple(s)[-n:])\n+    arr = lax_fft.fft(arr, fft_type, tuple(s)[-n:])\n   return arr\n \n \n@@ -199,7 +199,7 @@ def fftn(a: ArrayLike, s: Shape | None = None,\n     >>> jnp.allclose(x, jnp.fft.ifftn(x_fftn))\n     Array(True, dtype=bool)\n   \"\"\"\n-  return _fft_core('fftn', lax.FftType.FFT, a, s, axes, norm)\n+  return _fft_core('fftn', lax_fft.FftType.FFT, a, s, axes, norm)\n \n \n def ifftn(a: ArrayLike, s: Shape | None = None,\n@@ -267,7 +267,7 @@ def ifftn(a: ArrayLike, s: Shape | None = None,\n     [[ 2.5 +0.j    0.  -0.58j  0.  +0.58j]\n      [ 0.17+0.j   -0.83-0.29j -0.83+0.29j]]\n   \"\"\"\n-  return _fft_core('ifftn', lax.FftType.IFFT, a, s, axes, norm)\n+  return _fft_core('ifftn', lax_fft.FftType.IFFT, a, s, axes, norm)\n \n \n def rfftn(a: ArrayLike, s: Shape | None = None,\n@@ -358,7 +358,7 @@ def rfftn(a: ArrayLike, s: Shape | None = None,\n     >>> jnp.fft.rfftn(x1)\n     Array([10.+0.j, -2.+2.j, -2.+0.j], dtype=complex64)\n   \"\"\"\n-  return _fft_core('rfftn', lax.FftType.RFFT, a, s, axes, norm)\n+  return _fft_core('rfftn', lax_fft.FftType.RFFT, a, s, axes, norm)\n \n \n def irfftn(a: ArrayLike, s: Shape | None = None,\n@@ -435,7 +435,7 @@ def irfftn(a: ArrayLike, s: Shape | None = None,\n            [[-2., -2., -2.],\n             [-2., -2., -2.]]], dtype=float32)\n   \"\"\"\n-  return _fft_core('irfftn', lax.FftType.IRFFT, a, s, axes, norm)\n+  return _fft_core('irfftn', lax_fft.FftType.IRFFT, a, s, axes, norm)\n \n \n def _axis_check_1d(func_name: str, axis: int | None):\n@@ -446,7 +446,7 @@ def _axis_check_1d(func_name: str, axis: int | None):\n         \"Got axis = %r.\" % (full_name, full_name, axis)\n     )\n \n-def _fft_core_1d(func_name: str, fft_type: lax.FftType,\n+def _fft_core_1d(func_name: str, fft_type: lax_fft.FftType,\n                  a: ArrayLike, n: int | None, axis: int | None,\n                  norm: str | None) -> Array:\n   _axis_check_1d(func_name, axis)\n@@ -514,7 +514,7 @@ def fft(a: ArrayLike, n: int | None = None,\n     >>> jnp.allclose(x, jnp.fft.ifft(x_fft))\n     Array(True, dtype=bool)\n   \"\"\"\n-  return _fft_core_1d('fft', lax.FftType.FFT, a, n=n, axis=axis,\n+  return _fft_core_1d('fft', lax_fft.FftType.FFT, a, n=n, axis=axis,\n                       norm=norm)\n \n \n@@ -570,7 +570,7 @@ def ifft(a: ArrayLike, n: int | None = None,\n      [ 0.67+0.58j -0.5 +1.44j  0.17+2.02j  1.83+0.29j]\n      [ 0.67-0.58j -0.5 -1.44j  0.17-2.02j  1.83-0.29j]]\n   \"\"\"\n-  return _fft_core_1d('ifft', lax.FftType.IFFT, a, n=n, axis=axis,\n+  return _fft_core_1d('ifft', lax_fft.FftType.IFFT, a, n=n, axis=axis,\n                       norm=norm)\n \n \n@@ -631,7 +631,7 @@ def rfft(a: ArrayLike, n: int | None = None,\n            [ 1.-2.j,  3.-4.j,  5.-6.j],\n            [-1.+0.j, -1.+0.j, -1.+0.j]], dtype=complex64)\n   \"\"\"\n-  return _fft_core_1d('rfft', lax.FftType.RFFT, a, n=n, axis=axis,\n+  return _fft_core_1d('rfft', lax_fft.FftType.RFFT, a, n=n, axis=axis,\n                       norm=norm)\n \n \n@@ -691,7 +691,7 @@ def irfft(a: ArrayLike, n: int | None = None,\n            [-0.75, -1.25, -1.75],\n            [ 0.25,  0.75,  1.25]], dtype=float32)\n   \"\"\"\n-  return _fft_core_1d('irfft', lax.FftType.IRFFT, a, n=n, axis=axis,\n+  return _fft_core_1d('irfft', lax_fft.FftType.IRFFT, a, n=n, axis=axis,\n                       norm=norm)\n \n \n@@ -781,7 +781,7 @@ def hfft(a: ArrayLike, n: int | None = None,\n   conj_a = ufuncs.conj(a)\n   _axis_check_1d('hfft', axis)\n   nn = (conj_a.shape[axis] - 1) * 2 if n is None else n\n-  return _fft_core_1d('hfft', lax.FftType.IRFFT, conj_a, n=n, axis=axis,\n+  return _fft_core_1d('hfft', lax_fft.FftType.IRFFT, conj_a, n=n, axis=axis,\n                       norm=norm) * nn\n \n \n@@ -831,12 +831,12 @@ def ihfft(a: ArrayLike, n: int | None = None,\n   _axis_check_1d('ihfft', axis)\n   arr = jnp.asarray(a)\n   nn = arr.shape[axis] if n is None else n\n-  output = _fft_core_1d('ihfft', lax.FftType.RFFT, arr, n=n, axis=axis,\n+  output = _fft_core_1d('ihfft', lax_fft.FftType.RFFT, arr, n=n, axis=axis,\n                         norm=norm)\n   return ufuncs.conj(output) * (1 / nn)\n \n \n-def _fft_core_2d(func_name: str, fft_type: lax.FftType, a: ArrayLike,\n+def _fft_core_2d(func_name: str, fft_type: lax_fft.FftType, a: ArrayLike,\n                  s: Shape | None, axes: Sequence[int],\n                  norm: str | None) -> Array:\n   full_name = f\"jax.numpy.fft.{func_name}\"\n@@ -923,7 +923,7 @@ def fft2(a: ArrayLike, s: Shape | None = None, axes: Sequence[int] = (-2,-1),\n     >>> jnp.allclose(x, jnp.fft.ifft2(x_fft2))\n     Array(True, dtype=bool)\n   \"\"\"\n-  return _fft_core_2d('fft2', lax.FftType.FFT, a, s=s, axes=axes,\n+  return _fft_core_2d('fft2', lax_fft.FftType.FFT, a, s=s, axes=axes,\n                       norm=norm)\n \n \n@@ -995,7 +995,7 @@ def ifft2(a: ArrayLike, s: Shape | None = None, axes: Sequence[int] = (-2,-1),\n             [-0.33-0.58j, -0.33-0.58j],\n             [-0.33+0.58j, -0.33+0.58j]]], dtype=complex64)\n   \"\"\"\n-  return _fft_core_2d('ifft2', lax.FftType.IFFT, a, s=s, axes=axes,\n+  return _fft_core_2d('ifft2', lax_fft.FftType.IFFT, a, s=s, axes=axes,\n                       norm=norm)\n \n \n@@ -1074,7 +1074,7 @@ def rfft2(a: ArrayLike, s: Shape | None = None, axes: Sequence[int] = (-2,-1),\n             [  3.47+10.11j,   6.43+11.42j,   9.38+12.74j],\n             [  3.19 +1.63j,   4.4  +1.38j,   5.61 +1.12j]]], dtype=complex64)\n   \"\"\"\n-  return _fft_core_2d('rfft2', lax.FftType.RFFT, a, s=s, axes=axes,\n+  return _fft_core_2d('rfft2', lax_fft.FftType.RFFT, a, s=s, axes=axes,\n                       norm=norm)\n \n \n@@ -1149,7 +1149,7 @@ def irfft2(a: ArrayLike, s: Shape | None = None, axes: Sequence[int] = (-2,-1),\n             [ 0.  ,  0.  ,  0.  ],\n             [ 0.  ,  0.  ,  0.  ]]], dtype=float32)\n   \"\"\"\n-  return _fft_core_2d('irfft2', lax.FftType.IRFFT, a, s=s, axes=axes,\n+  return _fft_core_2d('irfft2', lax_fft.FftType.IRFFT, a, s=s, axes=axes,\n                       norm=norm)\n \n \ndiff --git a/jax/_src/numpy/index_tricks.py b/jax/_src/numpy/index_tricks.py\nindex ec67d7489f30..ab07ecad0cf5 100644\n--- a/jax/_src/numpy/index_tricks.py\n+++ b/jax/_src/numpy/index_tricks.py\n@@ -17,7 +17,9 @@\n from collections.abc import Iterable\n from typing import Any, Union\n \n-import jax\n+import numpy as np\n+\n+from jax._src import config\n from jax._src import core\n from jax._src.numpy.util import promote_dtypes\n from jax._src.numpy.lax_numpy import (\n@@ -26,8 +28,6 @@\n from jax._src.typing import Array, ArrayLike\n from jax._src.util import set_module\n \n-import numpy as np\n-\n \n export = set_module('jax.numpy')\n \n@@ -83,7 +83,7 @@ def __getitem__(self, key: slice | tuple[slice, ...]) -> Array:\n     if isinstance(key, slice):\n       return _make_1d_grid_from_slice(key, op_name=\"mgrid\")\n     output: Iterable[Array] = (_make_1d_grid_from_slice(k, op_name=\"mgrid\") for k in key)\n-    with jax.numpy_dtype_promotion('standard'):\n+    with config.numpy_dtype_promotion('standard'):\n       output = promote_dtypes(*output)\n     output_arr = meshgrid(*output, indexing='ij', sparse=False)\n     if len(output_arr) == 0:\n@@ -128,7 +128,7 @@ def __getitem__(\n     if isinstance(key, slice):\n       return _make_1d_grid_from_slice(key, op_name=\"ogrid\")\n     output: Iterable[Array] = (_make_1d_grid_from_slice(k, op_name=\"ogrid\") for k in key)\n-    with jax.numpy_dtype_promotion('standard'):\n+    with config.numpy_dtype_promotion('standard'):\n       output = promote_dtypes(*output)\n     return meshgrid(*output, indexing='ij', sparse=True)\n \ndiff --git a/jax/_src/numpy/indexing.py b/jax/_src/numpy/indexing.py\nindex 6aa5d6b87ef4..573352135806 100644\n--- a/jax/_src/numpy/indexing.py\n+++ b/jax/_src/numpy/indexing.py\n@@ -20,7 +20,8 @@\n import string\n from typing import Any, NamedTuple, Sequence\n \n-import jax\n+import numpy as np\n+\n from jax import lax\n from jax._src import array\n from jax._src import config\n@@ -39,7 +40,6 @@\n from jax._src.tree_util import tree_flatten\n from jax._src.typing import Array, ArrayLike, StaticScalar\n from jax._src.util import canonicalize_axis, safe_zip, set_module, tuple_update\n-import numpy as np\n \n export = set_module('jax.numpy')\n \n@@ -314,8 +314,10 @@ def replace(tup, val):\n     return lax.full(out_shape, 0, a.dtype)\n \n   if mode == \"one_hot\":\n+    from jax import nn  # pytype: disable=import-error\n+\n     indices = _normalize_index(indices, axis_size)\n-    hot = jax.nn.one_hot(indices, axis_size, dtype=np.bool_)\n+    hot = nn.one_hot(indices, axis_size, dtype=np.bool_)\n     if a.ndim == 1:\n       return einsum.einsum(\"...b,b->...\", hot, a, preferred_element_type=a.dtype)\n     if axis_int > len(string.ascii_letters) - 2:\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex 171a64a758ad..f323bc64718b 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -35,9 +35,9 @@\n from typing import (Any, IO, Literal, Protocol, TypeVar, Union, overload)\n import warnings\n \n-import jax\n-from jax import jit\n from jax import lax\n+from jax._src.api import jit\n+from jax._src import api\n from jax._src import config\n from jax._src import core\n from jax._src import deprecations\n@@ -508,7 +508,7 @@ def isscalar(element: Any) -> bool:\n   \"\"\"\n   if np.isscalar(element):\n     return True\n-  elif isinstance(element, (np.ndarray, jax.Array)):\n+  elif isinstance(element, (np.ndarray, Array)):\n     return element.ndim == 0\n   elif hasattr(element, '__jax_array__'):\n     return asarray(element).ndim == 0\n@@ -3418,7 +3418,7 @@ def clip(\n     )\n \n   util.check_arraylike(\"clip\", arr)\n-  if any(jax.numpy.iscomplexobj(t) for t in (arr, min, max)):\n+  if any(iscomplexobj(t) for t in (arr, min, max)):\n     raise ValueError(\n       \"Clip received a complex value either through the input or the min/max \"\n       \"keywords. Complex values have no ordering and cannot be clipped. \"\n@@ -4676,7 +4676,7 @@ def concat(arrays: Sequence[ArrayLike], /, *, axis: int | None = 0) -> Array:\n            [1., 1., 1., 0.]], dtype=float32)\n   \"\"\"\n   util.check_arraylike(\"concat\", *arrays)\n-  return jax.numpy.concatenate(arrays, axis=axis)\n+  return concatenate(arrays, axis=axis)\n \n \n @export\n@@ -4732,7 +4732,7 @@ def vstack(tup: np.ndarray | Array | Sequence[ArrayLike],\n   \"\"\"\n   arrs: Array | list[Array]\n   if isinstance(tup, (np.ndarray, Array)):\n-    arrs = jax.vmap(atleast_2d)(tup)\n+    arrs = api.vmap(atleast_2d)(tup)\n   else:\n     # TODO(jakevdp): Non-array input deprecated 2023-09-22; change to error.\n     util.check_arraylike(\"vstack\", *tup, emit_warning=True)\n@@ -4791,7 +4791,7 @@ def hstack(tup: np.ndarray | Array | Sequence[ArrayLike],\n   \"\"\"\n   arrs: Array | list[Array]\n   if isinstance(tup, (np.ndarray, Array)):\n-    arrs = jax.vmap(atleast_1d)(tup)\n+    arrs = api.vmap(atleast_1d)(tup)\n     arr0_ndim = arrs.ndim - 1\n   else:\n     # TODO(jakevdp): Non-array input deprecated 2023-09-22; change to error.\n@@ -4854,7 +4854,7 @@ def dstack(tup: np.ndarray | Array | Sequence[ArrayLike],\n   \"\"\"\n   arrs: Array | list[Array]\n   if isinstance(tup, (np.ndarray, Array)):\n-    arrs = jax.vmap(atleast_3d)(tup)\n+    arrs = api.vmap(atleast_3d)(tup)\n   else:\n     # TODO(jakevdp): Non-array input deprecated 2023-09-22; change to error.\n     util.check_arraylike(\"dstack\", *tup, emit_warning=True)\n@@ -4916,7 +4916,7 @@ def column_stack(tup: np.ndarray | Array | Sequence[ArrayLike]) -> Array:\n   \"\"\"\n   arrs: Array | list[Array] | np.ndarray\n   if isinstance(tup, (np.ndarray, Array)):\n-    arrs = jax.vmap(lambda x: atleast_2d(x).T)(tup) if tup.ndim < 3 else tup\n+    arrs = api.vmap(lambda x: atleast_2d(x).T)(tup) if tup.ndim < 3 else tup\n   else:\n     # TODO(jakevdp): Non-array input deprecated 2023-09-22; change to error.\n     util.check_arraylike(\"column_stack\", *tup, emit_warning=True)\n@@ -5354,7 +5354,7 @@ def _make_string_array(\n     )\n \n   # Just do a device_put since XLA does not support string as a data type.\n-  return jax.device_put(x=object, device=device)\n+  return api.device_put(x=object, device=device)\n \n \n @export\n@@ -5447,7 +5447,7 @@ def array(object: Any, dtype: DTypeLike | None = None, copy: bool = True,\n       (dtype is None or dtype == object.dtype) and (ndmin <= object.ndim) and\n       device is None):\n     # Keep the output uncommitted.\n-    return jax.device_put(object)\n+    return api.device_put(object)\n \n   # String arrays need separate handling because XLA does not support string\n   # as a data type.\n@@ -5551,7 +5551,7 @@ def _get_platform(\n     return device_or_sharding\n   elif device_or_sharding is None:\n     if config.default_device.value is None:\n-      return jax.default_backend()\n+      return xla_bridge.default_backend()\n     else:\n       return _get_platform(config.default_device.value)\n   else:\n@@ -6077,7 +6077,7 @@ def fromfunction(function: Callable[..., Array], shape: Any,\n   shape = core.canonicalize_shape(shape, context=\"shape argument of jnp.fromfunction()\")\n   for i in range(len(shape)):\n     in_axes = [0 if i == j else None for j in range(len(shape))]\n-    function = jax.vmap(function, in_axes=tuple(in_axes[::-1]))\n+    function = api.vmap(function, in_axes=tuple(in_axes[::-1]))\n   return function(*(arange(s, dtype=dtype) for s in shape), **kwargs)\n \n \n@@ -6166,7 +6166,7 @@ def eye(N: DimSize, M: DimSize | None = None,\n   # instead of putting it on default device and then on the specific device\n   output = _eye(N, M=M, k=k, dtype=dtype)\n   if device is not None:\n-    return jax.device_put(output, device=device)\n+    return api.device_put(output, device=device)\n   return output\n \n \n@@ -6299,7 +6299,7 @@ def arange(start: ArrayLike | DimSize, stop: ArrayLike | DimSize | None = None,\n   # instead of putting it on default device and then on the specific device\n   output = _arange(start, stop=stop, step=step, dtype=dtype)\n   if device is not None:\n-    return jax.device_put(output, device=device)\n+    return api.device_put(output, device=device)\n   return output\n \n \n@@ -6496,7 +6496,7 @@ def _i0(x):\n \n @_i0.defjvp\n def _i0_jvp(primals, tangents):\n-  primal_out, tangent_out = jax.jvp(_i0.fun, primals, tangents)\n+  primal_out, tangent_out = api.jvp(_i0.fun, primals, tangents)\n   return primal_out, where(primals[0] == 0, 0.0, tangent_out)\n \n @export\n@@ -7792,7 +7792,7 @@ def trim_zeros(filt: ArrayLike, trim: str ='fb') -> Array:\n   util.check_arraylike(\"trim_zeros\", filt, emit_warning=True)\n   core.concrete_or_error(None, filt,\n                          \"Error arose in the `filt` argument of trim_zeros()\")\n-  filt_arr = jax.numpy.asarray(filt)\n+  filt_arr = asarray(filt)\n   del filt\n   if filt_arr.ndim != 1:\n     # Added on 2024-09-11\n@@ -8173,9 +8173,9 @@ def apply_along_axis(\n   axis = _canonicalize_axis(axis, num_dims)\n   func = lambda arr: func1d(arr, *args, **kwargs)\n   for i in range(1, num_dims - axis):\n-    func = jax.vmap(func, in_axes=i, out_axes=-1)\n+    func = api.vmap(func, in_axes=i, out_axes=-1)\n   for i in range(axis):\n-    func = jax.vmap(func, in_axes=0, out_axes=0)\n+    func = api.vmap(func, in_axes=0, out_axes=0)\n   return func(arr)\n \n \n@@ -9623,7 +9623,7 @@ def _rank(x):\n \n def _searchsorted_via_compare_all(sorted_arr: Array, query: Array, side: str, dtype: type) -> Array:\n   op = _sort_lt_comparator if side == 'left' else _sort_le_comparator\n-  comparisons = jax.vmap(op, in_axes=(0, None))(sorted_arr, query)\n+  comparisons = api.vmap(op, in_axes=(0, None))(sorted_arr, query)\n   return comparisons.sum(dtype=dtype, axis=0)\n \n \ndiff --git a/jax/_src/numpy/linalg.py b/jax/_src/numpy/linalg.py\nindex 0e20e5b2a416..f2deddd52f05 100644\n--- a/jax/_src/numpy/linalg.py\n+++ b/jax/_src/numpy/linalg.py\n@@ -23,10 +23,11 @@\n import operator\n from typing import Literal, NamedTuple, overload\n \n-import jax\n-from jax import jit, custom_jvp\n from jax import lax\n \n+from jax._src.api import jit\n+from jax._src import config\n+from jax._src.custom_derivatives import custom_jvp\n from jax._src import deprecations\n from jax._src.lax import lax as lax_internal\n from jax._src.lax.lax import PrecisionLike\n@@ -44,24 +45,24 @@\n \n \n class EighResult(NamedTuple):\n-  eigenvalues: jax.Array\n-  eigenvectors: jax.Array\n+  eigenvalues: Array\n+  eigenvectors: Array\n \n \n class QRResult(NamedTuple):\n-  Q: jax.Array\n-  R: jax.Array\n+  Q: Array\n+  R: Array\n \n \n class SlogdetResult(NamedTuple):\n-  sign: jax.Array\n-  logabsdet: jax.Array\n+  sign: Array\n+  logabsdet: Array\n \n \n class SVDResult(NamedTuple):\n-  U: jax.Array\n-  S: jax.Array\n-  Vh: jax.Array\n+  U: Array\n+  S: Array\n+  Vh: Array\n \n \n def _H(x: ArrayLike) -> Array:\n@@ -995,7 +996,7 @@ def _pinv(a: ArrayLike, rtol: ArrayLike | None = None, hermitian: bool = False)\n \n \n @_pinv.defjvp\n-@jax.default_matmul_precision(\"float32\")\n+@config.default_matmul_precision(\"float32\")\n def _pinv_jvp(rtol, hermitian, primals, tangents):\n   # The Differentiation of Pseudo-Inverses and Nonlinear Least Squares Problems\n   # Whose Variables Separate. Author(s): G. H. Golub and V. Pereyra. SIAM\n@@ -1617,7 +1618,7 @@ def matrix_transpose(x: ArrayLike, /) -> Array:\n   ndim = x_arr.ndim\n   if ndim < 2:\n     raise ValueError(f\"matrix_transpose requres at least 2 dimensions; got {ndim=}\")\n-  return jax.lax.transpose(x_arr, (*range(ndim - 2), ndim - 1, ndim - 2))\n+  return lax.transpose(x_arr, (*range(ndim - 2), ndim - 1, ndim - 2))\n \n \n @export\ndiff --git a/jax/_src/numpy/polynomial.py b/jax/_src/numpy/polynomial.py\nindex 2b2923ba93ce..2f7a32c3f52d 100644\n--- a/jax/_src/numpy/polynomial.py\n+++ b/jax/_src/numpy/polynomial.py\n@@ -19,8 +19,8 @@\n \n import numpy as np\n \n-from jax import jit\n from jax import lax\n+from jax._src.api import jit\n from jax._src import dtypes\n from jax._src import core\n from jax._src.lax import lax as lax_internal\ndiff --git a/jax/_src/numpy/reductions.py b/jax/_src/numpy/reductions.py\nindex cbfda25eafcf..e1f499ccc530 100644\n--- a/jax/_src/numpy/reductions.py\n+++ b/jax/_src/numpy/reductions.py\n@@ -23,9 +23,9 @@\n \n import numpy as np\n \n-import jax\n from jax import lax\n from jax._src import api\n+from jax._src import config\n from jax._src import core\n from jax._src import deprecations\n from jax._src import dtypes\n@@ -793,7 +793,7 @@ def _axis_size(a: ArrayLike, axis: int | Sequence[int]):\n   size = 1\n   a_shape = np.shape(a)\n   for a in axis_seq:\n-    size *= maybe_named_axis(a, lambda i: a_shape[i], jax.lax.axis_size)\n+    size *= maybe_named_axis(a, lambda i: a_shape[i], lax.axis_size)\n   return size\n \n \n@@ -1136,7 +1136,7 @@ def _var(a: Array, axis: Axis = None, dtype: DTypeLike | None = None,\n   normalizer = lax.sub(normalizer, lax.convert_element_type(correction, computation_dtype))\n   result = sum(centered, axis, dtype=computation_dtype, keepdims=keepdims, where=where)\n   result = lax.div(result, normalizer).astype(dtype)\n-  with jax.debug_nans(False):\n+  with config.debug_nans(False):\n     result = _where(normalizer > 0, result, np.nan)\n   return result\n \n@@ -2513,7 +2513,7 @@ def _quantile(a: Array, q: Array, axis: int | tuple[int, ...] | None,\n     index[axis] = high\n     high_value = a[tuple(index)]\n   else:\n-    with jax.debug_nans(False):\n+    with config.debug_nans(False):\n       a = _where(any(lax_internal._isnan(a), axis=axis, keepdims=True), np.nan, a)\n     a = lax.sort(a, dimension=axis)\n     n = lax.convert_element_type(a_shape[axis], lax_internal._dtype(q))\ndiff --git a/jax/_src/numpy/scalar_types.py b/jax/_src/numpy/scalar_types.py\nindex 2b0e04adc997..1abe7cf66c15 100644\n--- a/jax/_src/numpy/scalar_types.py\n+++ b/jax/_src/numpy/scalar_types.py\n@@ -22,11 +22,11 @@\n \n from typing import Any\n \n-import jax\n+import numpy as np\n+\n from jax._src.typing import Array\n from jax._src import core\n from jax._src import dtypes\n-import numpy as np\n \n \n # Some objects below rewrite their __module__ attribute to this name.\n@@ -46,7 +46,8 @@ def __ne__(self, other: Any) -> bool:\n     return not (self == other)\n \n   def __call__(self, x: Any) -> Array:\n-    return jax.numpy.asarray(x, dtype=self.dtype)\n+    from jax._src.numpy.lax_numpy import asarray\n+    return asarray(x, dtype=self.dtype)\n \n   def __instancecheck__(self, instance: Any) -> bool:\n     return isinstance(instance, self.dtype.type)\ndiff --git a/jax/_src/numpy/setops.py b/jax/_src/numpy/setops.py\nindex d4a8e41dd317..ef1d44ae01b1 100644\n--- a/jax/_src/numpy/setops.py\n+++ b/jax/_src/numpy/setops.py\n@@ -21,10 +21,9 @@\n \n import numpy as np\n \n-import jax\n-from jax import jit\n from jax import lax\n \n+from jax._src.api import jit\n from jax._src import core\n from jax._src import dtypes\n from jax._src.lax import lax as lax_internal\n@@ -59,8 +58,10 @@ def _in1d(ar1: ArrayLike, ar2: ArrayLike, invert: bool,\n     else:\n       return (arr1[:, None] == arr2[None, :]).any(-1)\n   elif method == 'binary_search':\n+    from jax._src.numpy.lax_numpy import searchsorted\n+\n     arr2 = lax.sort(arr2)\n-    ind = jax.numpy.searchsorted(arr2, arr1)\n+    ind = searchsorted(arr2, arr1)\n     if invert:\n       return arr1 != arr2[ind]\n     else:\ndiff --git a/jax/_src/numpy/sorting.py b/jax/_src/numpy/sorting.py\nindex a0f368e2ef07..be8f42ce6145 100644\n--- a/jax/_src/numpy/sorting.py\n+++ b/jax/_src/numpy/sorting.py\n@@ -17,14 +17,14 @@\n \n import numpy as np\n \n-import jax\n+from jax import lax\n+\n from jax._src import api\n from jax._src import core\n from jax._src import dtypes\n from jax._src.numpy import util\n from jax._src.util import canonicalize_axis, set_module\n from jax._src.typing import Array, ArrayLike\n-from jax import lax\n \n export = set_module('jax.numpy')\n \n@@ -226,7 +226,7 @@ def partition(a: ArrayLike, kth: int, axis: int = -1) -> Array:\n   axis = canonicalize_axis(axis, arr.ndim)\n   kth = canonicalize_axis(kth, arr.shape[axis])\n \n-  arr = jax.numpy.swapaxes(arr, axis, -1)\n+  arr = arr.swapaxes(axis, -1)\n   if dtypes.isdtype(arr.dtype, \"unsigned integer\"):\n     # Here, we apply a trick to handle correctly 0 values for unsigned integers\n     bottom = -lax.top_k(-(arr + 1), kth + 1)[0] - 1\n@@ -234,7 +234,7 @@ def partition(a: ArrayLike, kth: int, axis: int = -1) -> Array:\n     bottom = -lax.top_k(-arr, kth + 1)[0]\n   top = lax.top_k(arr, arr.shape[-1] - kth - 1)[0]\n   out = lax.concatenate([bottom, top], dimension=arr.ndim - 1)\n-  return jax.numpy.swapaxes(out, -1, axis)\n+  return out.swapaxes(-1, axis)\n \n \n @export\n@@ -297,7 +297,7 @@ def argpartition(a: ArrayLike, kth: int, axis: int = -1) -> Array:\n   axis = canonicalize_axis(axis, arr.ndim)\n   kth = canonicalize_axis(kth, arr.shape[axis])\n \n-  arr = jax.numpy.swapaxes(arr, axis, -1)\n+  arr = arr.swapaxes(axis, -1)\n   if dtypes.isdtype(arr.dtype, \"unsigned integer\"):\n     # Here, we apply a trick to handle correctly 0 values for unsigned integers\n     bottom_ind = lax.top_k(-(arr + 1), kth + 1)[1]\n@@ -307,11 +307,11 @@ def argpartition(a: ArrayLike, kth: int, axis: int = -1) -> Array:\n   # To avoid issues with duplicate values, we compute the top indices via a proxy\n   set_to_zero = lambda a, i: a.at[i].set(0)\n   for _ in range(arr.ndim - 1):\n-    set_to_zero = jax.vmap(set_to_zero)\n-  proxy = set_to_zero(jax.numpy.ones(arr.shape), bottom_ind)\n+    set_to_zero = api.vmap(set_to_zero)\n+  proxy = set_to_zero(lax.full(arr.shape, 1.0), bottom_ind)\n   top_ind = lax.top_k(proxy, arr.shape[-1] - kth - 1)[1]\n   out = lax.concatenate([bottom_ind, top_ind], dimension=arr.ndim - 1)\n-  return jax.numpy.swapaxes(out, -1, axis)\n+  return out.swapaxes(-1, axis)\n \n \n @export\n@@ -421,7 +421,7 @@ def lexsort(keys: Array | np.ndarray | Sequence[ArrayLike], axis: int = -1) -> A\n   if len({np.shape(key) for key in key_arrays}) > 1:\n     raise ValueError(\"all keys need to be the same shape\")\n   if np.ndim(key_arrays[0]) == 0:\n-    return jax.numpy.array(0, dtype=dtypes.canonicalize_dtype(dtypes.int_))\n+    return lax.full((), 0, dtypes.canonicalize_dtype(dtypes.int_))\n   axis = canonicalize_axis(axis, np.ndim(key_arrays[0]))\n   use_64bit_index = key_arrays[0].shape[axis] >= (1 << 31)\n   iota = lax.broadcasted_iota(np.dtype('int64') if use_64bit_index else dtypes.int_,\ndiff --git a/jax/_src/numpy/tensor_contractions.py b/jax/_src/numpy/tensor_contractions.py\nindex 979f68e28f6d..255da08e1816 100644\n--- a/jax/_src/numpy/tensor_contractions.py\n+++ b/jax/_src/numpy/tensor_contractions.py\n@@ -20,7 +20,6 @@\n \n import numpy as np\n \n-import jax\n from jax import lax\n from jax._src import core\n from jax._src import dtypes\n@@ -378,7 +377,7 @@ def vdot(\n   a, b = util.ensure_arraylike(\"vdot\", a, b)\n   if dtypes.issubdtype(dtypes.dtype(a, canonicalize=True), np.complexfloating):\n     a = ufuncs.conj(a)\n-  return dot(jax.numpy.ravel(a), jax.numpy.ravel(b), precision=precision,\n+  return dot(a.ravel(), b.ravel(), precision=precision,\n              preferred_element_type=preferred_element_type)\n \n \n@@ -429,11 +428,13 @@ def vecdot(x1: ArrayLike, x2: ArrayLike, /, *, axis: int = -1,\n     >>> jnp.linalg.vecdot(a, b, axis=-1)\n     Array([20, 47], dtype=int32)\n   \"\"\"\n+  from jax._src.numpy.lax_numpy import moveaxis\n+\n   x1_arr, x2_arr = util.ensure_arraylike(\"jnp.vecdot\", x1, x2)\n   if x1_arr.shape[axis] != x2_arr.shape[axis]:\n     raise ValueError(f\"axes must match; got shapes {x1_arr.shape} and {x2_arr.shape} with {axis=}\")\n-  x1_arr = jax.numpy.moveaxis(x1_arr, axis, -1)\n-  x2_arr = jax.numpy.moveaxis(x2_arr, axis, -1)\n+  x1_arr = moveaxis(x1_arr, axis, -1)\n+  x2_arr = moveaxis(x2_arr, axis, -1)\n   return vectorize(partial(vdot, precision=precision, preferred_element_type=preferred_element_type),\n                    signature=\"(n),(n)->()\")(x1_arr, x2_arr)\n \n@@ -604,8 +605,9 @@ def inner(\n   \"\"\"\n   a, b = util.ensure_arraylike(\"inner\", a, b)\n   if np.ndim(a) == 0 or np.ndim(b) == 0:\n-    a = jax.numpy.asarray(a, dtype=preferred_element_type)\n-    b = jax.numpy.asarray(b, dtype=preferred_element_type)\n+    if preferred_element_type is not None:\n+      a = a.astype(preferred_element_type)\n+      b = b.astype(preferred_element_type)\n     return a * b\n   return tensordot(a, b, (-1, -1), precision=precision,\n                    preferred_element_type=preferred_element_type)\n@@ -643,4 +645,4 @@ def outer(a: ArrayLike, b: ArrayLike, out: None = None) -> Array:\n     raise NotImplementedError(\"The 'out' argument to jnp.outer is not supported.\")\n   a, b = util.ensure_arraylike(\"outer\", a, b)\n   a, b = util.promote_dtypes(a, b)\n-  return jax.numpy.ravel(a)[:, None] * jax.numpy.ravel(b)[None, :]\n+  return a.ravel()[:, None] * b.ravel()[None, :]\ndiff --git a/jax/_src/numpy/ufunc_api.py b/jax/_src/numpy/ufunc_api.py\nindex da55212bae1f..243ab9aa0878 100644\n--- a/jax/_src/numpy/ufunc_api.py\n+++ b/jax/_src/numpy/ufunc_api.py\n@@ -22,9 +22,11 @@\n import operator\n from typing import Any\n \n-import jax\n+from jax._src import api\n from jax._src.typing import Array, ArrayLike, DTypeLike\n-from jax._src.lax import lax as lax_internal\n+from jax._src.lax import control_flow\n+from jax._src.lax import slicing\n+from jax._src.lax import lax\n from jax._src.numpy import indexing\n import jax._src.numpy.lax_numpy as jnp\n from jax._src.numpy.reductions import _moveaxis\n@@ -179,11 +181,11 @@ def __call__(self, *args: ArrayLike, out: None = None, where: None = None) -> An\n     call = self.__static_props['call'] or self._call_vectorized\n     return call(*args)\n \n-  @partial(jax.jit, static_argnames=['self'])\n+  @partial(api.jit, static_argnames=['self'])\n   def _call_vectorized(self, *args):\n     return vectorize(self._func)(*args)\n \n-  @partial(jax.jit, static_argnames=['self', 'axis', 'dtype', 'out', 'keepdims'])\n+  @partial(api.jit, static_argnames=['self', 'axis', 'dtype', 'out', 'keepdims'])\n   def reduce(self, a: ArrayLike, axis: int | None = 0,\n              dtype: DTypeLike | None = None,\n              out: None = None, keepdims: bool = False, initial: ArrayLike | None = None,\n@@ -249,8 +251,8 @@ def reduce(self, a: ArrayLike, axis: int | None = 0,\n       if self.identity is None and initial is None:\n         raise ValueError(f\"reduction operation {self.__name__!r} does not have an identity, \"\n                          \"so to use a where mask one has to specify 'initial'.\")\n-      if lax_internal._dtype(where) != bool:\n-        raise ValueError(f\"where argument must have dtype=bool; got dtype={lax_internal._dtype(where)}\")\n+      if lax._dtype(where) != bool:\n+        raise ValueError(f\"where argument must have dtype=bool; got dtype={lax._dtype(where)}\")\n     reduce = self.__static_props['reduce'] or self._reduce_via_scan\n     return reduce(a, axis=axis, dtype=dtype, keepdims=keepdims, initial=initial, where=where)\n \n@@ -258,11 +260,11 @@ def _reduce_via_scan(self, arr: ArrayLike, axis: int | None = 0, dtype: DTypeLik\n                        keepdims: bool = False, initial: ArrayLike | None = None,\n                        where: ArrayLike | None = None) -> Array:\n     assert self.nin == 2 and self.nout == 1\n-    arr = lax_internal.asarray(arr)\n+    arr = lax.asarray(arr)\n     if initial is None:\n       initial = self.identity\n     if dtype is None:\n-      dtype = jax.eval_shape(self._func, lax_internal._one(arr), lax_internal._one(arr)).dtype\n+      dtype = api.eval_shape(self._func, lax._one(arr), lax._one(arr)).dtype\n     if where is not None:\n       where = _broadcast_to(where, arr.shape)\n     if isinstance(axis, tuple):\n@@ -306,15 +308,15 @@ def body_fun(i, val):\n     else:\n       start_index = 0\n       start_value = initial\n-    start_value = _broadcast_to(lax_internal.asarray(start_value).astype(dtype), arr.shape[1:])\n+    start_value = _broadcast_to(lax.asarray(start_value).astype(dtype), arr.shape[1:])\n \n-    result = jax.lax.fori_loop(start_index, arr.shape[0], body_fun, start_value)\n+    result = control_flow.fori_loop(start_index, arr.shape[0], body_fun, start_value)\n \n     if keepdims:\n       result = result.reshape(final_shape)\n     return result\n \n-  @partial(jax.jit, static_argnames=['self', 'axis', 'dtype'])\n+  @partial(api.jit, static_argnames=['self', 'axis', 'dtype'])\n   def accumulate(self, a: ArrayLike, axis: int = 0, dtype: DTypeLike | None = None,\n                  out: None = None) -> Array:\n     \"\"\"Accumulate operation derived from binary ufunc.\n@@ -376,10 +378,10 @@ def _accumulate_via_scan(self, arr: ArrayLike, axis: int = 0,\n                            dtype: DTypeLike | None = None) -> Array:\n     assert self.nin == 2 and self.nout == 1\n     check_arraylike(f\"{self.__name__}.accumulate\", arr)\n-    arr = lax_internal.asarray(arr)\n+    arr = lax.asarray(arr)\n \n     if dtype is None:\n-      dtype = jax.eval_shape(self._func, lax_internal._one(arr), lax_internal._one(arr)).dtype\n+      dtype = api.eval_shape(self._func, lax._one(arr), lax._one(arr)).dtype\n \n     if axis is None or isinstance(axis, tuple):\n       raise ValueError(\"accumulate does not allow multiple axes\")\n@@ -390,10 +392,10 @@ def scan_fun(carry, _):\n       i, x = carry\n       y = _where(i == 0, arr[0].astype(dtype), self(x.astype(dtype), arr[i].astype(dtype)))\n       return (i + 1, y), y\n-    _, result = jax.lax.scan(scan_fun, (0, arr[0].astype(dtype)), None, length=arr.shape[0])\n+    _, result = control_flow.scan(scan_fun, (0, arr[0].astype(dtype)), None, length=arr.shape[0])\n     return _moveaxis(result, 0, axis)\n \n-  @partial(jax.jit, static_argnums=[0], static_argnames=['inplace'])\n+  @partial(api.jit, static_argnums=[0], static_argnames=['inplace'])\n   def at(self, a: ArrayLike, indices: Any, b: ArrayLike | None = None, /, *,\n          inplace: bool = True) -> Array:\n     \"\"\"Update elements of an array via the specified unary or binary ufunc.\n@@ -440,15 +442,15 @@ def at(self, a: ArrayLike, indices: Any, b: ArrayLike | None = None, /, *,\n   def _at_via_scan(self, a: ArrayLike, indices: Any, *args: Any) -> Array:\n     assert len(args) in {0, 1}\n     check_arraylike(f\"{self.__name__}.at\", a, *args)\n-    dtype = jax.eval_shape(self._func, lax_internal._one(a), *(lax_internal._one(arg) for arg in args)).dtype\n-    a = lax_internal.asarray(a).astype(dtype)\n-    args = tuple(lax_internal.asarray(arg).astype(dtype) for arg in args)\n+    dtype = api.eval_shape(self._func, lax._one(a), *(lax._one(arg) for arg in args)).dtype\n+    a = lax.asarray(a).astype(dtype)\n+    args = tuple(lax.asarray(arg).astype(dtype) for arg in args)\n     indices = indexing.eliminate_deprecated_list_indexing(indices)\n     if not indices:\n       return a\n \n     shapes = [np.shape(i) for i in indices if not isinstance(i, slice)]\n-    shape = shapes and jax.lax.broadcast_shapes(*shapes)\n+    shape = shapes and lax.broadcast_shapes(*shapes)\n     if not shape:\n       return a.at[indices].set(self(a.at[indices].get(), *args))\n \n@@ -462,10 +464,10 @@ def scan_fun(carry, x):\n       idx = tuple(ind if isinstance(ind, slice) else ind[i] for ind in indices)\n       a = a.at[idx].set(self(a.at[idx].get(), *(arg[i] for arg in args)))\n       return (i + 1, a), x\n-    carry, _ = jax.lax.scan(scan_fun, (0, a), None, len(indices[0]))  # type: ignore[arg-type]\n+    carry, _ = control_flow.scan(scan_fun, (0, a), None, len(indices[0]))  # type: ignore[arg-type]\n     return carry[1]\n \n-  @partial(jax.jit, static_argnames=['self', 'axis', 'dtype'])\n+  @partial(api.jit, static_argnames=['self', 'axis', 'dtype'])\n   def reduceat(self, a: ArrayLike, indices: Any, axis: int = 0,\n                dtype: DTypeLike | None = None, out: None = None) -> Array:\n     \"\"\"Reduce an array between specified indices via a binary ufunc.\n@@ -517,7 +519,7 @@ def reduceat(self, a: ArrayLike, indices: Any, axis: int = 0,\n   def _reduceat_via_scan(self, a: ArrayLike, indices: Any, axis: int = 0,\n                          dtype: DTypeLike | None = None) -> Array:\n     check_arraylike(f\"{self.__name__}.reduceat\", a, indices)\n-    a = lax_internal.asarray(a)\n+    a = lax.asarray(a)\n     idx_tuple = indexing.eliminate_deprecated_list_indexing(indices)\n     assert len(idx_tuple) == 1\n     indices = idx_tuple[0]\n@@ -531,17 +533,17 @@ def _reduceat_via_scan(self, a: ArrayLike, indices: Any, axis: int = 0,\n       raise ValueError(\"reduceat requires a single integer axis.\")\n     axis = canonicalize_axis(axis, a.ndim)\n     out = indexing.take(a, indices, axis=axis)\n-    ind = jax.lax.expand_dims(jnp.append(indices, a.shape[axis]),\n-                              list(np.delete(np.arange(out.ndim), axis)))\n-    ind_start = jax.lax.slice_in_dim(ind, 0, ind.shape[axis] - 1, axis=axis)\n-    ind_end = jax.lax.slice_in_dim(ind, 1, ind.shape[axis], axis=axis)\n+    ind = lax.expand_dims(jnp.append(indices, a.shape[axis]),\n+                                   list(np.delete(np.arange(out.ndim), axis)))\n+    ind_start = slicing.slice_in_dim(ind, 0, ind.shape[axis] - 1, axis=axis)\n+    ind_end = slicing.slice_in_dim(ind, 1, ind.shape[axis], axis=axis)\n     def loop_body(i, out):\n       return _where((i > ind_start) & (i < ind_end),\n-                    self(out, indexing.take(a, jax.lax.expand_dims(i, (0,)), axis=axis)),\n+                    self(out, indexing.take(a, lax.expand_dims(i, (0,)), axis=axis)),\n                     out)\n-    return jax.lax.fori_loop(0, a.shape[axis], loop_body, out)\n+    return control_flow.fori_loop(0, a.shape[axis], loop_body, out)\n \n-  @partial(jax.jit, static_argnums=[0])\n+  @partial(api.jit, static_argnums=[0])\n   def outer(self, A: ArrayLike, B: ArrayLike, /) -> Array:\n     \"\"\"Apply the function to all pairs of values in ``A`` and ``B``.\n \n@@ -584,8 +586,8 @@ def outer(self, A: ArrayLike, B: ArrayLike, /) -> Array:\n     if self.nout != 1:\n       raise ValueError(\"outer only supported for functions returning a single value\")\n     check_arraylike(f\"{self.__name__}.outer\", A, B)\n-    _ravel = lambda A: jax.lax.reshape(A, (np.size(A),))\n-    result = jax.vmap(jax.vmap(self, (None, 0)), (0, None))(_ravel(A), _ravel(B))\n+    _ravel = lambda A: lax.reshape(A, (np.size(A),))\n+    result = api.vmap(api.vmap(self, (None, 0)), (0, None))(_ravel(A), _ravel(B))\n     return result.reshape(*np.shape(A), *np.shape(B))\n \n \ndiff --git a/jax/_src/numpy/vectorize.py b/jax/_src/numpy/vectorize.py\nindex 5ea9d697d27d..f166a96a4693 100644\n--- a/jax/_src/numpy/vectorize.py\n+++ b/jax/_src/numpy/vectorize.py\n@@ -23,7 +23,7 @@\n \n from jax._src import api\n from jax._src import config\n-from jax import lax\n+from jax._src.lax import lax\n from jax._src.numpy import lax_numpy as jnp\n from jax._src.util import set_module, safe_map as map, safe_zip as zip\n \ndiff --git a/jax/_src/numpy/window_functions.py b/jax/_src/numpy/window_functions.py\nindex 96a15db777a8..6d1bfb245272 100644\n--- a/jax/_src/numpy/window_functions.py\n+++ b/jax/_src/numpy/window_functions.py\n@@ -16,11 +16,11 @@\n \n from jax._src import core\n from jax._src import dtypes\n+from jax._src.lax import lax\n from jax._src.numpy import lax_numpy\n from jax._src.numpy import ufuncs\n from jax._src.typing import Array, ArrayLike\n from jax._src.util import set_module\n-from jax import lax\n \n export = set_module('jax.numpy')\n \n\n```",
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Don't canonicalize in `__eq__` if `other` is a PartitionSpec since it is already canonicalized\n\nPiperOrigin-RevId: 766714598",
        "output": "```diff\nCommit: d17b29230605673f44c72e0617c31a198d6a809f\nDate: 2025-06-03T17:09:06Z\nURL: https://github.com/jax-ml/jax/commit/d17b29230605673f44c72e0617c31a198d6a809f\nFiles changed: 1\nAdditions: +5, Deletions: -5\ndiff --git a/jax/_src/partition_spec.py b/jax/_src/partition_spec.py\nindex 040db35ccb2b..2c833e6544e4 100644\n--- a/jax/_src/partition_spec.py\n+++ b/jax/_src/partition_spec.py\n@@ -111,18 +111,18 @@ def __len__(self):\n     return len(self._partitions)\n \n   def __eq__(self, other):\n-    if not isinstance(other, (PartitionSpec, tuple)):\n-      return False\n-    other_p = tuple(_canonicalize_partition(o) for o in other)\n     if isinstance(other, PartitionSpec):\n-      return (self._partitions == other_p and\n+      return (self._partitions == other._partitions and\n               self._unreduced == other._unreduced)\n-    else:\n+    elif isinstance(other, tuple):\n       if self._unreduced:\n         raise TypeError(\n             f\"other {other} cannot be of instance `tuple` when self {self} has\"\n             \" unreduced in `__eq__` of PartitionSpec.\")\n+      other_p = tuple(_canonicalize_partition(o) for o in other)\n       return self._partitions == other_p\n+    else:\n+      return False\n \n   def __hash__(self):\n     return hash((self._partitions, self._unreduced))\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic] Dropped the `TPU` prefix from the recently added `TPUInterpreterParams`\n\nWe no longer use platform prefixes in all other Pallas APIs.\n\nPiperOrigin-RevId: 766639632",
        "output": "```diff\nCommit: 87641ccb80bad1845bf8abb35a6d71ef79b1fae2\nDate: 2025-06-03T13:39:37Z\nURL: https://github.com/jax-ml/jax/commit/87641ccb80bad1845bf8abb35a6d71ef79b1fae2\nFiles changed: 6\nAdditions: +31, Deletions: -31\ndiff --git a/jax/_src/pallas/core.py b/jax/_src/pallas/core.py\nindex a05f97eb122f..047edd2b8435 100644\n--- a/jax/_src/pallas/core.py\n+++ b/jax/_src/pallas/core.py\n@@ -1250,7 +1250,7 @@ def _core_map_abstract_eval(*args, jaxpr, mesh, **kwargs):\n   if interpret:\n     try:\n       from jax._src.pallas.mosaic import interpret as mosaic_tpu_interpret  # Avoid circular dependency.\n-      if isinstance(interpret, mosaic_tpu_interpret.TPUInterpretParams):\n+      if isinstance(interpret, mosaic_tpu_interpret.InterpretParams):\n         effs = mosaic_tpu_interpret.get_interpret_effects()\n     except ImportError:\n       pass\n@@ -1353,7 +1353,7 @@ def _core_map_typecheck_rule(_, *in_atoms, jaxpr, mesh, **kwargs):\n   if interpret:\n     try:\n       from jax._src.pallas.mosaic import interpret as mosaic_tpu_interpret  # Avoid circular dependency.\n-      if isinstance(interpret, mosaic_tpu_interpret.TPUInterpretParams):\n+      if isinstance(interpret, mosaic_tpu_interpret.InterpretParams):\n         effs = mosaic_tpu_interpret.get_interpret_effects()\n     except ImportError:\n       pass\ndiff --git a/jax/_src/pallas/mosaic/interpret.py b/jax/_src/pallas/mosaic/interpret.py\nindex 03c99c794ac7..3be718aa0aa6 100644\n--- a/jax/_src/pallas/mosaic/interpret.py\n+++ b/jax/_src/pallas/mosaic/interpret.py\n@@ -67,7 +67,7 @@\n \n \n @dataclasses.dataclass(frozen=True)\n-class TPUInterpretParams:\n+class InterpretParams:\n   \"\"\"Parameters for Mosaic TPU interpret mode.\n \n   Attributes:\n@@ -524,7 +524,7 @@ def check_write(device_id, local_core_id, clock, buffer_key, rnge, source_info=N\n \n @dataclasses.dataclass\n class SharedMemory:\n-  interpret_params: TPUInterpretParams\n+  interpret_params: InterpretParams\n   num_devices: int\n   num_cores_per_device: int\n   clocks: list[VectorClock]\n@@ -1926,7 +1926,7 @@ def interpret_pallas_call(\n     compiler_params: dict[str, Any],\n     cost_estimate: CostEstimate,\n     out_avals: tuple[jax_core.AbstractValue, ...],\n-    interpret_params: TPUInterpretParams,\n+    interpret_params: InterpretParams,\n ):\n   del debug, cost_estimate, out_avals\n \ndiff --git a/jax/_src/pallas/pallas_call.py b/jax/_src/pallas/pallas_call.py\nindex 93b929d219c9..b14259556faf 100644\n--- a/jax/_src/pallas/pallas_call.py\n+++ b/jax/_src/pallas/pallas_call.py\n@@ -96,7 +96,7 @@ def _pallas_call_abstract_eval(\n ):\n   del avals\n \n-  if isinstance(interpret, mosaic_tpu_interpret.TPUInterpretParams):\n+  if isinstance(interpret, mosaic_tpu_interpret.InterpretParams):\n     # Report effects that will be introduced when running/lowering\n     # mosaic_tpu_interpret.mosaic_tpu_interpret.interpret_pallas_call .\n     effs = mosaic_tpu_interpret.get_interpret_effects()\n@@ -1261,7 +1261,7 @@ def _pallas_call_lowering(\n   if params['jaxpr'].constvars:\n     raise ValueError('Cannot lower a pallas_call with constants.')\n   if interpret:\n-    if isinstance(interpret, mosaic_tpu_interpret.TPUInterpretParams):\n+    if isinstance(interpret, mosaic_tpu_interpret.InterpretParams):\n       impl = partial(mosaic_tpu_interpret.interpret_pallas_call,\n                      interpret_params=interpret,\n                      **params)\n@@ -1774,5 +1774,5 @@ def in_path_to_input_origin(\n   from jax._src.pallas.mosaic import interpret as mosaic_tpu_interpret\n except ImportError:\n   mosaic_tpu_interpret = types.SimpleNamespace(  # type: ignore\n-      TPUInterpretParams=types.new_class('_NoInstances', (enum.Enum,)),\n+      InterpretParams=types.new_class('_NoInstances', (enum.Enum,)),\n   )\ndiff --git a/jax/experimental/pallas/tpu.py b/jax/experimental/pallas/tpu.py\nindex 5ac79558b11d..eceb2e4f0383 100644\n--- a/jax/experimental/pallas/tpu.py\n+++ b/jax/experimental/pallas/tpu.py\n@@ -28,7 +28,7 @@\n from jax._src.pallas.mosaic.helpers import sync_copy as sync_copy\n from jax._src.pallas.mosaic.helpers import core_barrier as core_barrier\n from jax._src.pallas.mosaic.helpers import run_on_first_core as run_on_first_core\n-from jax._src.pallas.mosaic.interpret import TPUInterpretParams as TPUInterpretParams\n+from jax._src.pallas.mosaic.interpret import InterpretParams as InterpretParams\n from jax._src.pallas.mosaic.lowering import LoweringException as LoweringException\n from jax._src.pallas.mosaic.pipeline import BufferedRef as BufferedRef\n from jax._src.pallas.mosaic.pipeline import BufferedRefBase as BufferedRefBase\ndiff --git a/tests/pallas/tpu_pallas_interpret_distributed_test.py b/tests/pallas/tpu_pallas_interpret_distributed_test.py\nindex 70ed3dc576e5..ddfe8bcde4f4 100644\n--- a/tests/pallas/tpu_pallas_interpret_distributed_test.py\n+++ b/tests/pallas/tpu_pallas_interpret_distributed_test.py\n@@ -107,7 +107,7 @@ def right_permute_kernel(input_ref, output_ref, send_sem, recv_sem):\n         out_shape=out_shape,\n         grid_spec=grid_spec,\n         compiler_params=pltpu.CompilerParams(collective_id=13),\n-        interpret=pltpu.TPUInterpretParams(\n+        interpret=pltpu.InterpretParams(\n             dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n     )\n     # Wrap the kernel within a shard_map to call.\n@@ -228,7 +228,7 @@ def _():\n       all_gather_kernel,\n       out_shape=out_shape,\n       grid_spec=grid_spec,\n-      interpret=pltpu.TPUInterpretParams(\n+      interpret=pltpu.InterpretParams(\n           dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n       compiler_params=pltpu.CompilerParams(collective_id=0),\n     )\n@@ -388,7 +388,7 @@ def _():\n       all_reduce_kernel,\n       out_shape=out_shape,\n       grid_spec=grid_spec,\n-      interpret=pltpu.TPUInterpretParams(\n+      interpret=pltpu.InterpretParams(\n           dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n       compiler_params=pltpu.CompilerParams(collective_id=0),\n     )\n@@ -672,7 +672,7 @@ def pallas_reduce_scatter(input_arr):\n         reduce_scatter_kernel,\n         out_shape=out_shape,\n         grid_spec=grid_spec,\n-        interpret=pltpu.TPUInterpretParams(\n+        interpret=pltpu.InterpretParams(\n             dma_execution_mode=dma_execution_mode, detect_races=True),\n         compiler_params=pltpu.CompilerParams(collective_id=7),\n       )(input_arr)[0]\n@@ -976,7 +976,7 @@ def pallas_reduce_scatter(input_arr):\n         reduce_scatter_kernel,\n         out_shape=out_shape,\n         grid_spec=grid_spec,\n-        interpret=pltpu.TPUInterpretParams(\n+        interpret=pltpu.InterpretParams(\n             dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n         compiler_params=pltpu.CompilerParams(collective_id=19),\n       )(input_arr)[0]\n@@ -1064,7 +1064,7 @@ def run(src_dst_ids):\n               ],\n               out_specs=pl.BlockSpec(memory_space=pltpu.ANY),\n               scratch_shapes=[pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA],\n-              interpret=pltpu.TPUInterpretParams(\n+              interpret=pltpu.InterpretParams(\n                   dma_execution_mode='eager',\n                   detect_races=True,\n               ),\ndiff --git a/tests/pallas/tpu_pallas_interpret_test.py b/tests/pallas/tpu_pallas_interpret_test.py\nindex 9cf98e6c1dd4..5bfca2270aa1 100644\n--- a/tests/pallas/tpu_pallas_interpret_test.py\n+++ b/tests/pallas/tpu_pallas_interpret_test.py\n@@ -124,7 +124,7 @@ def matmul(x: jax.Array, y: jax.Array):\n               (x.shape[0] // 2, y.shape[1] // 2),\n               lambda i, j: (i, j),\n           ),\n-          interpret=pltpu.TPUInterpretParams(),\n+          interpret=pltpu.InterpretParams(),\n       )(x, y)\n \n     k1, k2 = jax.random.split(jax.random.key(0))\n@@ -155,7 +155,7 @@ def block_dynamic_slice(x, starts, sizes):\n           dynamic_slice_kernel,\n           grid_spec=grid_spec,\n           out_shape=jax.ShapeDtypeStruct(shape=sizes, dtype=x.dtype),\n-          interpret=pltpu.TPUInterpretParams(),\n+          interpret=pltpu.InterpretParams(),\n       )\n       block_idx = jnp.array([starts[0] // sizes[0], starts[1] // sizes[1]])\n       return kernel(block_idx, x)\n@@ -189,7 +189,7 @@ def f(s, x):\n           ],\n           out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)),\n           input_output_aliases={1: 0},\n-          interpret=pltpu.TPUInterpretParams(),\n+          interpret=pltpu.InterpretParams(),\n       )(s, x)\n \n     s = jnp.array([1], dtype=jnp.int32)\n@@ -224,7 +224,7 @@ def _():\n         ),\n         scratch_shapes=(pltpu.SMEM((1,), jnp.int32),),\n         input_output_aliases={0: 0},\n-        interpret=pltpu.TPUInterpretParams(),\n+        interpret=pltpu.InterpretParams(),\n     )(x)\n \n     expected = np.zeros((4, 4))\n@@ -264,7 +264,7 @@ def kernel_with_race(x_ref, o_ref, t_ref, sem):\n             pltpu.VMEM(x.shape, x.dtype),\n             pltpu.SemaphoreType.DMA,\n         ],\n-        interpret=pltpu.TPUInterpretParams(\n+        interpret=pltpu.InterpretParams(\n             detect_races=True, dma_execution_mode=dma_execution_mode\n         ),\n     )(x).block_until_ready()\n@@ -279,7 +279,7 @@ def kernel_with_race(x_ref, o_ref, t_ref, sem):\n             pltpu.VMEM(x.shape, x.dtype),\n             pltpu.SemaphoreType.DMA,\n         ],\n-        interpret=pltpu.TPUInterpretParams(\n+        interpret=pltpu.InterpretParams(\n             detect_races=True, dma_execution_mode=dma_execution_mode\n         ),\n     )(x).block_until_ready()\n@@ -293,7 +293,7 @@ def matmul(x: jax.Array, y: jax.Array):\n       return pl.pallas_call(\n           matmul_kernel,\n           out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype),\n-          interpret=pltpu.TPUInterpretParams(\n+          interpret=pltpu.InterpretParams(\n               skip_floating_point_ops=True\n           ),\n       )(x, y)\n@@ -325,7 +325,7 @@ def kernel(o1_ref, o2_ref, o3_ref, t1_ref, t2_ref):\n             pltpu.VMEM((8, 128), jnp.bfloat16),\n             pltpu.VMEM((8, 128), jnp.int16),\n         ],\n-        interpret=pltpu.TPUInterpretParams(\n+        interpret=pltpu.InterpretParams(\n             uninitialized_memory=uninitialized_memory\n         ),\n     )()\n@@ -355,7 +355,7 @@ def kernel_call(x, s):\n               pl.BlockSpec(memory_space=pltpu.SMEM),\n           ],\n           out_specs=pl.BlockSpec((8, 256), lambda i, j: (i, 0)),\n-          interpret=pltpu.TPUInterpretParams(),\n+          interpret=pltpu.InterpretParams(),\n       )(x, s)\n \n     with CountStoreCallbacksContext() as store_callbacks_counter:\n@@ -378,7 +378,7 @@ def kernel_call_dimensions_parallel_arbitrary(s, grid_point_recorder):\n           grid=(4, 4),\n           in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n           out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n-          interpret=pltpu.TPUInterpretParams(\n+          interpret=pltpu.InterpretParams(\n               random_seed=12345, grid_point_recorder=grid_point_recorder\n           ),\n           compiler_params=pltpu.CompilerParams(\n@@ -436,7 +436,7 @@ def kernel(s_ref, o_ref):\n           grid=(4, 4),\n           in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n           out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n-          interpret=pltpu.TPUInterpretParams(random_seed=12345),\n+          interpret=pltpu.InterpretParams(random_seed=12345),\n           compiler_params=pltpu.CompilerParams(\n               dimension_semantics=('arbitrary', 'parallel')\n           ),\n@@ -462,7 +462,7 @@ def kernel_call_dynamic_parallel_dimension():\n           grid=(dim_size,),\n           in_specs=[],\n           out_specs=pl.BlockSpec((1,), lambda _: (0,)),\n-          interpret=pltpu.TPUInterpretParams(),\n+          interpret=pltpu.InterpretParams(),\n           compiler_params=pltpu.CompilerParams(\n               dimension_semantics=('parallel',)\n           ),\n@@ -479,7 +479,7 @@ def f(x):\n       y = jnp.zeros_like(x)\n       def inner(refs):\n         x_ref, y_ref = refs\n-        @pl.core_map(mesh, interpret=pltpu.TPUInterpretParams())\n+        @pl.core_map(mesh, interpret=pltpu.InterpretParams())\n         def _():\n           num_cores = jax.lax.psum(1, \"x\")\n           slc_size = 16 // num_cores\n@@ -520,7 +520,7 @@ def kernel(x_ref, o_ref, vmem_ref):\n         scratch_shapes=[\n             pltpu.VMEM(x.shape, x.dtype),\n         ],\n-        interpret=pltpu.TPUInterpretParams(\n+        interpret=pltpu.InterpretParams(\n             num_cores_per_device=2,\n             detect_races=True,\n         ),\n@@ -554,7 +554,7 @@ def kernel(x_ref, o_ref, vmem_ref):\n         scratch_shapes=[\n             pltpu.VMEM((8, 128), x.dtype),\n         ],\n-        interpret=pltpu.TPUInterpretParams(\n+        interpret=pltpu.InterpretParams(\n             num_cores_per_device=2,\n             detect_races=True,\n         ),\n@@ -578,7 +578,7 @@ def kernel_call(s, num_cores_per_device, grid_point_recorder):\n           grid=(4, 4),\n           in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n           out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n-          interpret=pltpu.TPUInterpretParams(\n+          interpret=pltpu.InterpretParams(\n               random_seed=12345,\n               num_cores_per_device=num_cores_per_device,\n               grid_point_recorder=grid_point_recorder,\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic] Removed the `TPU` prefix from `TPUCompilerParams` and `TPUMemorySpace`\n\nAll TPU-specific APIs are always used qualified, e.g. `pltpu.TPUCompilerParams`,\nso the prefix is redundant.\n\nPiperOrigin-RevId: 766576735",
        "output": "```diff\nCommit: d0d081564953d90ead6121356b6835f868d9afba\nDate: 2025-06-03T10:15:41Z\nURL: https://github.com/jax-ml/jax/commit/d0d081564953d90ead6121356b6835f868d9afba\nFiles changed: 33\nAdditions: +209, Deletions: -183\ndiff --git a/docs/pallas/CHANGELOG.md b/docs/pallas/CHANGELOG.md\nindex 5c916c66ed86..e3589b87b720 100644\n--- a/docs/pallas/CHANGELOG.md\n+++ b/docs/pallas/CHANGELOG.md\n@@ -23,6 +23,11 @@ Remember to align the itemized text with the first line of an item within a list\n   * {class}`jax.experimental.pallas.triton.TritonCompilerParams` has been\n     renamed to {class}`jax.experimental.pallas.triton.CompilerParams`. The\n     old name is deprecated and will be removed in a future release.\n+  * {class}`jax.experimental.pallas.tpu.TPUCompilerParams`\n+    and {class}`jax.experimental.pallas.tpu.TPUMemorySpace` have been\n+    renamed to {class}`jax.experimental.pallas.tpu.CompilerParams`\n+    and {class}`jax.experimental.pallas.tpu.MemorySpace`. The\n+    old names are deprecated and will be removed in a future release.\n \n ## Released with jax 0.6.1\n \ndiff --git a/docs/pallas/quickstart.ipynb b/docs/pallas/quickstart.ipynb\nindex 6460c1d5e739..ffdf715e984a 100644\n--- a/docs/pallas/quickstart.ipynb\n+++ b/docs/pallas/quickstart.ipynb\n@@ -280,7 +280,7 @@\n    \"metadata\": {},\n    \"source\": [\n     \"TPUs distinguish between vector and scalar memory spaces and in this case the\\n\",\n-    \"output must be placed in scalar memory (`TPUMemorySpace.SMEM`) since `i` is\\n\",\n+    \"output must be placed in scalar memory (`MemorySpace.SMEM`) since `i` is\\n\",\n     \"a scalar. For more details read {ref}`tpu_and_its_memory_spaces`.\\n\",\n     \"To call the above kernel on TPU, run:\"\n    ]\n@@ -297,7 +297,7 @@\n     \"\\n\",\n     \"def iota(size: int):\\n\",\n     \"  return pl.pallas_call(iota_kernel,\\n\",\n-    \"                        out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM),\\n\",\n+    \"                        out_specs=pl.BlockSpec(memory_space=pltpu.MemorySpace.SMEM),\\n\",\n     \"                        out_shape=jax.ShapeDtypeStruct((size,), jnp.int32),\\n\",\n     \"                        grid=(size,))()\\n\",\n     \"iota(8)\"\ndiff --git a/docs/pallas/quickstart.md b/docs/pallas/quickstart.md\nindex d4865488a15b..5f1832f2a2f0 100644\n--- a/docs/pallas/quickstart.md\n+++ b/docs/pallas/quickstart.md\n@@ -186,7 +186,7 @@ iota(8)\n ```\n \n TPUs distinguish between vector and scalar memory spaces and in this case the\n-output must be placed in scalar memory (`TPUMemorySpace.SMEM`) since `i` is\n+output must be placed in scalar memory (`MemorySpace.SMEM`) since `i` is\n a scalar. For more details read {ref}`tpu_and_its_memory_spaces`.\n To call the above kernel on TPU, run:\n \n@@ -196,7 +196,7 @@ from jax.experimental.pallas import tpu as pltpu\n \n def iota(size: int):\n   return pl.pallas_call(iota_kernel,\n-                        out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.SMEM),\n+                        out_specs=pl.BlockSpec(memory_space=pltpu.MemorySpace.SMEM),\n                         out_shape=jax.ShapeDtypeStruct((size,), jnp.int32),\n                         grid=(size,))()\n iota(8)\ndiff --git a/docs/pallas/tpu/details.rst b/docs/pallas/tpu/details.rst\nindex 0575806e6037..a961c376f5bc 100644\n--- a/docs/pallas/tpu/details.rst\n+++ b/docs/pallas/tpu/details.rst\n@@ -170,7 +170,7 @@ grid axes over cores. This is an opt-in procedure. To allow that,\n ..\n   pallas_call(\n       ...,\n-      compiler_params=pltpu.TPUCompilerParams(\n+      compiler_params=pltpu.CompilerParams(\n           dimension_semantics=[\"parallel\", \"parallel\", \"arbitrary\"]\n       ),\n     )\ndiff --git a/docs/pallas/tpu/distributed.ipynb b/docs/pallas/tpu/distributed.ipynb\nindex 75aeeb92ca43..ae82b7a80ac6 100644\n--- a/docs/pallas/tpu/distributed.ipynb\n+++ b/docs/pallas/tpu/distributed.ipynb\n@@ -271,11 +271,11 @@\n     \"out_shape = jax.ShapeDtypeStruct((8, 128), jnp.float32)\\n\",\n     \"grid_spec = pltpu.PrefetchScalarGridSpec(\\n\",\n     \"    num_scalar_prefetch=0,\\n\",\n-    \"    # TPUMemorySpace.ANY will (usually) place the tensor in HBM.\\n\",\n+    \"    # MemorySpace.ANY will (usually) place the tensor in HBM.\\n\",\n     \"    in_specs=[\\n\",\n-    \"        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\\n\",\n+    \"        pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\\n\",\n     \"    ],\\n\",\n-    \"    out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\\n\",\n+    \"    out_specs=pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\\n\",\n     \"    scratch_shapes=(\\n\",\n     \"        # We allocate DMA semaphores in scratch memory.\\n\",\n     \"        [pltpu.SemaphoreType.DMA] * 2\\n\",\n@@ -420,10 +420,10 @@\n     \"grid_spec = pltpu.PrefetchScalarGridSpec(\\n\",\n     \"            num_scalar_prefetch=0,\\n\",\n     \"            in_specs=[\\n\",\n-    \"                # TPUMemorySpace.ANY will (usually) place the tensor in HBM.\\n\",\n-    \"                pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\\n\",\n+    \"                # MemorySpace.ANY will (usually) place the tensor in HBM.\\n\",\n+    \"                pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\\n\",\n     \"            ],\\n\",\n-    \"            out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\\n\",\n+    \"            out_specs=pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\\n\",\n     \"            scratch_shapes=(\\n\",\n     \"              # DMA semaphores are allocated in scratch memory.\\n\",\n     \"              # We allocated one semaphore for a local HBM-VMEM copy,\\n\",\n@@ -569,7 +569,7 @@\n     \"kernel = pl.pallas_call(\\n\",\n     \"      example_kernel,\\n\",\n     \"      ...,\\n\",\n-    \"      compiler_params=pltpu.TPUCompilerParams(collective_id=0),\\n\",\n+    \"      compiler_params=pltpu.CompilerParams(collective_id=0),\\n\",\n     \")\\n\",\n     \"```\"\n    ]\n@@ -809,13 +809,13 @@\n     \"    num_scalar_prefetch=0,\\n\",\n     \"    in_specs=[\\n\",\n     \"        # Our input lives in VMEM\\n\",\n-    \"        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\\n\",\n+    \"        pl.BlockSpec(memory_space=pltpu.MemorySpace.VMEM),\\n\",\n     \"    ],\\n\",\n     \"    out_specs=[\\n\",\n     \"        # Our output lives in VMEM\\n\",\n-    \"        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\\n\",\n+    \"        pl.BlockSpec(memory_space=pltpu.MemorySpace.VMEM),\\n\",\n     \"        # Our double-buffer lives in HBM\\n\",\n-    \"        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\\n\",\n+    \"        pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\\n\",\n     \"    ],\\n\",\n     \"    grid=(num_devices,),\\n\",\n     \"    scratch_shapes=(\\n\",\n@@ -829,7 +829,7 @@\n     \"    all_reduce_kernel,\\n\",\n     \"    out_shape=out_shape,\\n\",\n     \"    grid_spec=grid_spec,\\n\",\n-    \"    compiler_params=pltpu.TPUCompilerParams(collective_id=0),\\n\",\n+    \"    compiler_params=pltpu.CompilerParams(collective_id=0),\\n\",\n     \")\\n\",\n     \"\\n\",\n     \"pallas_result = jax.jit(\\n\",\n@@ -1146,11 +1146,11 @@\n     \"grid_spec = pltpu.PrefetchScalarGridSpec(\\n\",\n     \"    num_scalar_prefetch=0,\\n\",\n     \"    in_specs=[\\n\",\n-    \"        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\\n\",\n+    \"        pl.BlockSpec(memory_space=pltpu.MemorySpace.VMEM),\\n\",\n     \"    ],\\n\",\n     \"    out_specs=[\\n\",\n-    \"        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\\n\",\n-    \"        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\\n\",\n+    \"        pl.BlockSpec(memory_space=pltpu.MemorySpace.VMEM),\\n\",\n+    \"        pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\\n\",\n     \"    ],\\n\",\n     \"    grid=(num_devices, 2),\\n\",\n     \"    scratch_shapes=(\\n\",\n@@ -1169,7 +1169,7 @@\n     \"      reduce_scatter_kernel,\\n\",\n     \"      out_shape=out_shape,\\n\",\n     \"      grid_spec=grid_spec,\\n\",\n-    \"      compiler_params=pltpu.TPUCompilerParams(collective_id=0),\\n\",\n+    \"      compiler_params=pltpu.CompilerParams(collective_id=0),\\n\",\n     \"  )(input_arr)[0]\\n\",\n     \"\\n\",\n     \"\\n\",\n@@ -1307,7 +1307,7 @@\n     \"\\n\",\n     \"In this next example we will modify our previous reduce-scatter example to utilize a nested inner pipeline. Note that the communication and computation costs of `reduce_scatter` both scale linearly with the size of the input, so we do not necessarily expect to see the operation become compute-bound with larger block sizes. This example is purely for demonstration purposes on how to use the pipeline emitter.\\n\",\n     \"\\n\",\n-    \"We will increase the block sizes of the outer kernel such that they would be undesirable to place inside of VMEM, and allocate all inputs and outputs in HBM (`memory_space=TPUMemorySpace.Any`). The only major change from our previous kernel is the body of the kernel where accumulation is done. Rather than manually copying from HBM to VMEM, accumulating, and copying back to HBM, we use `emit_pipeline` to handle the memory transfers for us. Accumulation is done in an inner kernel with a much smaller, VMEM-friendly block size.\\n\",\n+    \"We will increase the block sizes of the outer kernel such that they would be undesirable to place inside of VMEM, and allocate all inputs and outputs in HBM (`memory_space=MemorySpace.ANY`). The only major change from our previous kernel is the body of the kernel where accumulation is done. Rather than manually copying from HBM to VMEM, accumulating, and copying back to HBM, we use `emit_pipeline` to handle the memory transfers for us. Accumulation is done in an inner kernel with a much smaller, VMEM-friendly block size.\\n\",\n     \"\\n\",\n     \"In our previous kernel we had the following kernel body to copy data from HBM to the VMEM accumulator, increment, and then copy the results back to HBM:\\n\",\n     \"\\n\",\n@@ -1408,7 +1408,7 @@\n     \"inner_block_spec = pl.BlockSpec(\\n\",\n     \"    index_map=lambda i, j: (i, j),\\n\",\n     \"    block_shape=inner_block_size,\\n\",\n-    \"    memory_space=pltpu.TPUMemorySpace.ANY,\\n\",\n+    \"    memory_space=pltpu.MemorySpace.ANY,\\n\",\n     \")\\n\",\n     \"\\n\",\n     \"\\n\",\n@@ -1590,11 +1590,11 @@\n     \"grid_spec = pltpu.PrefetchScalarGridSpec(\\n\",\n     \"    num_scalar_prefetch=0,\\n\",\n     \"    in_specs=[\\n\",\n-    \"        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\\n\",\n+    \"        pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\\n\",\n     \"    ],\\n\",\n     \"    out_specs=[\\n\",\n-    \"        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\\n\",\n-    \"        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\\n\",\n+    \"        pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\\n\",\n+    \"        pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\\n\",\n     \"    ],\\n\",\n     \"    grid=(num_devices, 2),\\n\",\n     \"    scratch_shapes=(\\n\",\n@@ -1612,7 +1612,7 @@\n     \"      reduce_scatter_kernel,\\n\",\n     \"      out_shape=out_shape,\\n\",\n     \"      grid_spec=grid_spec,\\n\",\n-    \"      compiler_params=pltpu.TPUCompilerParams(collective_id=0),\\n\",\n+    \"      compiler_params=pltpu.CompilerParams(collective_id=0),\\n\",\n     \"  )(input_arr)[0]\\n\",\n     \"\\n\",\n     \"\\n\",\ndiff --git a/docs/pallas/tpu/distributed.md b/docs/pallas/tpu/distributed.md\nindex 7b1f26bccf89..b16116549972 100644\n--- a/docs/pallas/tpu/distributed.md\n+++ b/docs/pallas/tpu/distributed.md\n@@ -233,11 +233,11 @@ def right_permute_kernel(input_ref, output_ref, send_sem, recv_sem):\n out_shape = jax.ShapeDtypeStruct((8, 128), jnp.float32)\n grid_spec = pltpu.PrefetchScalarGridSpec(\n     num_scalar_prefetch=0,\n-    # TPUMemorySpace.ANY will (usually) place the tensor in HBM.\n+    # MemorySpace.ANY will (usually) place the tensor in HBM.\n     in_specs=[\n-        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n+        pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\n     ],\n-    out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n+    out_specs=pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\n     scratch_shapes=(\n         # We allocate DMA semaphores in scratch memory.\n         [pltpu.SemaphoreType.DMA] * 2\n@@ -356,10 +356,10 @@ out_shape = jax.ShapeDtypeStruct((num_devices, 8, 128), jnp.float32)\n grid_spec = pltpu.PrefetchScalarGridSpec(\n             num_scalar_prefetch=0,\n             in_specs=[\n-                # TPUMemorySpace.ANY will (usually) place the tensor in HBM.\n-                pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n+                # MemorySpace.ANY will (usually) place the tensor in HBM.\n+                pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\n             ],\n-            out_specs=pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n+            out_specs=pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\n             scratch_shapes=(\n               # DMA semaphores are allocated in scratch memory.\n               # We allocated one semaphore for a local HBM-VMEM copy,\n@@ -491,7 +491,7 @@ When using barrier semaphores, the `collective_id` compiler parameter must be pa\n kernel = pl.pallas_call(\n       example_kernel,\n       ...,\n-      compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n+      compiler_params=pltpu.CompilerParams(collective_id=0),\n )\n ```\n \n@@ -703,13 +703,13 @@ grid_spec = pltpu.PrefetchScalarGridSpec(\n     num_scalar_prefetch=0,\n     in_specs=[\n         # Our input lives in VMEM\n-        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\n+        pl.BlockSpec(memory_space=pltpu.MemorySpace.VMEM),\n     ],\n     out_specs=[\n         # Our output lives in VMEM\n-        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\n+        pl.BlockSpec(memory_space=pltpu.MemorySpace.VMEM),\n         # Our double-buffer lives in HBM\n-        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n+        pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\n     ],\n     grid=(num_devices,),\n     scratch_shapes=(\n@@ -723,7 +723,7 @@ kernel = pl.pallas_call(\n     all_reduce_kernel,\n     out_shape=out_shape,\n     grid_spec=grid_spec,\n-    compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n+    compiler_params=pltpu.CompilerParams(collective_id=0),\n )\n \n pallas_result = jax.jit(\n@@ -1019,11 +1019,11 @@ out_shape = (\n grid_spec = pltpu.PrefetchScalarGridSpec(\n     num_scalar_prefetch=0,\n     in_specs=[\n-        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\n+        pl.BlockSpec(memory_space=pltpu.MemorySpace.VMEM),\n     ],\n     out_specs=[\n-        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.VMEM),\n-        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n+        pl.BlockSpec(memory_space=pltpu.MemorySpace.VMEM),\n+        pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\n     ],\n     grid=(num_devices, 2),\n     scratch_shapes=(\n@@ -1042,7 +1042,7 @@ def pallas_reduce_scatter(input_arr):\n       reduce_scatter_kernel,\n       out_shape=out_shape,\n       grid_spec=grid_spec,\n-      compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n+      compiler_params=pltpu.CompilerParams(collective_id=0),\n   )(input_arr)[0]\n \n \n@@ -1148,7 +1148,7 @@ pl.pallas_call(\n \n In this next example we will modify our previous reduce-scatter example to utilize a nested inner pipeline. Note that the communication and computation costs of `reduce_scatter` both scale linearly with the size of the input, so we do not necessarily expect to see the operation become compute-bound with larger block sizes. This example is purely for demonstration purposes on how to use the pipeline emitter.\n \n-We will increase the block sizes of the outer kernel such that they would be undesirable to place inside of VMEM, and allocate all inputs and outputs in HBM (`memory_space=TPUMemorySpace.Any`). The only major change from our previous kernel is the body of the kernel where accumulation is done. Rather than manually copying from HBM to VMEM, accumulating, and copying back to HBM, we use `emit_pipeline` to handle the memory transfers for us. Accumulation is done in an inner kernel with a much smaller, VMEM-friendly block size.\n+We will increase the block sizes of the outer kernel such that they would be undesirable to place inside of VMEM, and allocate all inputs and outputs in HBM (`memory_space=MemorySpace.ANY`). The only major change from our previous kernel is the body of the kernel where accumulation is done. Rather than manually copying from HBM to VMEM, accumulating, and copying back to HBM, we use `emit_pipeline` to handle the memory transfers for us. Accumulation is done in an inner kernel with a much smaller, VMEM-friendly block size.\n \n In our previous kernel we had the following kernel body to copy data from HBM to the VMEM accumulator, increment, and then copy the results back to HBM:\n \n@@ -1242,7 +1242,7 @@ inner_grid = (\n inner_block_spec = pl.BlockSpec(\n     index_map=lambda i, j: (i, j),\n     block_shape=inner_block_size,\n-    memory_space=pltpu.TPUMemorySpace.ANY,\n+    memory_space=pltpu.MemorySpace.ANY,\n )\n \n \n@@ -1424,11 +1424,11 @@ out_shape = (\n grid_spec = pltpu.PrefetchScalarGridSpec(\n     num_scalar_prefetch=0,\n     in_specs=[\n-        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n+        pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\n     ],\n     out_specs=[\n-        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n-        pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY),\n+        pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\n+        pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY),\n     ],\n     grid=(num_devices, 2),\n     scratch_shapes=(\n@@ -1446,7 +1446,7 @@ def pallas_reduce_scatter(input_arr):\n       reduce_scatter_kernel,\n       out_shape=out_shape,\n       grid_spec=grid_spec,\n-      compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n+      compiler_params=pltpu.CompilerParams(collective_id=0),\n   )(input_arr)[0]\n \n \ndiff --git a/docs/pallas/tpu/matmul.ipynb b/docs/pallas/tpu/matmul.ipynb\nindex 9c90add16ab0..3ae5f95c204a 100644\n--- a/docs/pallas/tpu/matmul.ipynb\n+++ b/docs/pallas/tpu/matmul.ipynb\n@@ -210,7 +210,7 @@\n     \"                pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))],\\n\",\n     \"      out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),\\n\",\n     \"      grid=(m // bm, n // bn, k // bk),\\n\",\n-    \"      compiler_params=pltpu.TPUCompilerParams(\\n\",\n+    \"      compiler_params=pltpu.CompilerParams(\\n\",\n     \"          dimension_semantics=(\\\"parallel\\\", \\\"parallel\\\", \\\"arbitrary\\\")),\\n\",\n     \"  )(x, y)\"\n    ]\n@@ -466,7 +466,7 @@\n     \"        grid=(m // bm, n // bn, k // bk),\\n\",\n     \"      ),\\n\",\n     \"      out_shape=jax.ShapeDtypeStruct((m, n), x.dtype),\\n\",\n-    \"      compiler_params=pltpu.TPUCompilerParams(\\n\",\n+    \"      compiler_params=pltpu.CompilerParams(\\n\",\n     \"          dimension_semantics=(\\\"parallel\\\", \\\"parallel\\\", \\\"arbitrary\\\")),\\n\",\n     \"  )(x, y)\"\n    ]\n@@ -741,7 +741,7 @@\n     \"        grid=(m // bm, n // bn, k // bk),\\n\",\n     \"      ),\\n\",\n     \"      out_shape=jax.ShapeDtypeStruct((m, n), x.dtype),\\n\",\n-    \"      compiler_params=pltpu.TPUCompilerParams(\\n\",\n+    \"      compiler_params=pltpu.CompilerParams(\\n\",\n     \"          dimension_semantics=(\\\"parallel\\\", \\\"parallel\\\", \\\"arbitrary\\\")),\\n\",\n     \"  )(x, y)\"\n    ]\n@@ -929,7 +929,7 @@\n     \"          grid=(m // bm, n // bn, k // bk),\\n\",\n     \"      ),\\n\",\n     \"      out_shape=jax.ShapeDtypeStruct((m, n), x.dtype),\\n\",\n-    \"      compiler_params=pltpu.TPUCompilerParams(\\n\",\n+    \"      compiler_params=pltpu.CompilerParams(\\n\",\n     \"          dimension_semantics=(\\\"parallel\\\", \\\"parallel\\\", \\\"arbitrary\\\")),\\n\",\n     \"  )(x, y)\"\n    ]\ndiff --git a/docs/pallas/tpu/matmul.md b/docs/pallas/tpu/matmul.md\nindex 42084f12d5f5..7ac157b4a2e9 100644\n--- a/docs/pallas/tpu/matmul.md\n+++ b/docs/pallas/tpu/matmul.md\n@@ -167,7 +167,7 @@ def matmul(\n                 pl.BlockSpec((bk, bn), lambda i, j, k: (k, j))],\n       out_specs=pl.BlockSpec((bm, bn), lambda i, j, k: (i, j)),\n       grid=(m // bm, n // bn, k // bk),\n-      compiler_params=pltpu.TPUCompilerParams(\n+      compiler_params=pltpu.CompilerParams(\n           dimension_semantics=(\"parallel\", \"parallel\", \"arbitrary\")),\n   )(x, y)\n ```\n@@ -321,7 +321,7 @@ def matmul(\n         grid=(m // bm, n // bn, k // bk),\n       ),\n       out_shape=jax.ShapeDtypeStruct((m, n), x.dtype),\n-      compiler_params=pltpu.TPUCompilerParams(\n+      compiler_params=pltpu.CompilerParams(\n           dimension_semantics=(\"parallel\", \"parallel\", \"arbitrary\")),\n   )(x, y)\n ```\n@@ -489,7 +489,7 @@ def matmul(\n         grid=(m // bm, n // bn, k // bk),\n       ),\n       out_shape=jax.ShapeDtypeStruct((m, n), x.dtype),\n-      compiler_params=pltpu.TPUCompilerParams(\n+      compiler_params=pltpu.CompilerParams(\n           dimension_semantics=(\"parallel\", \"parallel\", \"arbitrary\")),\n   )(x, y)\n ```\n@@ -613,7 +613,7 @@ def matmul(\n           grid=(m // bm, n // bn, k // bk),\n       ),\n       out_shape=jax.ShapeDtypeStruct((m, n), x.dtype),\n-      compiler_params=pltpu.TPUCompilerParams(\n+      compiler_params=pltpu.CompilerParams(\n           dimension_semantics=(\"parallel\", \"parallel\", \"arbitrary\")),\n   )(x, y)\n ```\ndiff --git a/docs/pallas/tpu/pipelining.ipynb b/docs/pallas/tpu/pipelining.ipynb\nindex 68932f4d1e40..829cda000e5d 100644\n--- a/docs/pallas/tpu/pipelining.ipynb\n+++ b/docs/pallas/tpu/pipelining.ipynb\n@@ -123,15 +123,15 @@\n     \"\\n\",\n     \"| Pallas Enum | TPU Memory Space | Type (DRAM/SRAM) |\\n\",\n     \"| --- | --- | --- |\\n\",\n-    \"| `pltpu.TPUMemorySpace.ANY` | HBM (usually) or VMEM | DRAM |\\n\",\n-    \"| `pltpu.TPUMemorySpace.VMEM` | VMEM | SRAM |\\n\",\n-    \"| `pltpu.TPUMemorySpace.SMEM` | SMEM | SRAM |\\n\",\n-    \"| `pltpu.TPUMemorySpace.SEMAPHORE` | Semaphore | SRAM |\\n\",\n+    \"| `pltpu.MemorySpace.ANY` | HBM (usually) or VMEM | DRAM |\\n\",\n+    \"| `pltpu.MemorySpace.VMEM` | VMEM | SRAM |\\n\",\n+    \"| `pltpu.MemorySpace.SMEM` | SMEM | SRAM |\\n\",\n+    \"| `pltpu.MemorySpace.SEMAPHORE` | Semaphore | SRAM |\\n\",\n     \"\\n\",\n-    \"- `TPUMemorySpace.VMEM` denotes vector SRAM. It is the default memory space if nothing is specified.\\n\",\n-    \"- `TPUMemorySpace.SMEM` denotes scalar SRAM. Only scalar loads and stores can be performed to/from SMEM.\\n\",\n-    \"- `TPUMemorySpace.ANY` is a hint to the compiler that the memory space is unconstrained. In most cases, XLA will place this buffer in HBM. A buffer assigned to the `ANY` memory space cannot be dereferenced normally using array indexing syntax (e.g. `x[...]`). Instead, we must first copy the values into a VMEM or SMEM buffer using `pltpu.sync_copy` or `pltpu.async_copy`.\\n\",\n-    \"- `TPUMemorySpace.SEMAPHORE` is used to allocate semaphores for constructing barriers or tracking asynchronous operations. It is also possible to return semaphores from the kernel for building asynchronous kernels - this is an experimental feature; see {ref}`pallas_async` for more details.\\n\",\n+    \"- `MemorySpace.VMEM` denotes vector SRAM. It is the default memory space if nothing is specified.\\n\",\n+    \"- `MemorySpace.SMEM` denotes scalar SRAM. Only scalar loads and stores can be performed to/from SMEM.\\n\",\n+    \"- `MemorySpace.ANY` is a hint to the compiler that the memory space is unconstrained. In most cases, XLA will place this buffer in HBM. A buffer assigned to the `ANY` memory space cannot be dereferenced normally using array indexing syntax (e.g. `x[...]`). Instead, we must first copy the values into a VMEM or SMEM buffer using `pltpu.sync_copy` or `pltpu.async_copy`.\\n\",\n+    \"- `MemorySpace.SEMAPHORE` is used to allocate semaphores for constructing barriers or tracking asynchronous operations. It is also possible to return semaphores from the kernel for building asynchronous kernels - this is an experimental feature; see {ref}`pallas_async` for more details.\\n\",\n     \"\\n\",\n     \"Pipelining on TPUs is typically done between HBM (DRAM) to VMEM (Vector SRAM). The default behavior for `pallas_call` on TPU is that arguments to `pallas_call` are assumed to live in HBM, and inputs to the user kernel body are stored in VMEM.\\n\",\n     \"\\n\",\n@@ -164,9 +164,9 @@\n     \"\\n\",\n     \"x = jax.random.uniform(jax.random.key(0), (8, 128), jnp.float32)\\n\",\n     \"out = pl.pallas_call(hbm_vmem_kernel,\\n\",\n-    \"  in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)],\\n\",\n+    \"  in_specs=[pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY)],\\n\",\n     \"  out_shape=jax.ShapeDtypeStruct((1, 128), jnp.float32),\\n\",\n-    \"  scratch_shapes=(pltpu.TPUMemorySpace.VMEM(shape=(1, 128), dtype=jnp.float32),)\\n\",\n+    \"  scratch_shapes=(pltpu.MemorySpace.VMEM(shape=(1, 128), dtype=jnp.float32),)\\n\",\n     \")(x)\\n\",\n     \"\\n\",\n     \"np.testing.assert_allclose(out, x[0:1] + 1)\"\n@@ -259,7 +259,7 @@\n     \"      in_specs=[block_spec, block_spec],\\n\",\n     \"      out_specs=block_spec,\\n\",\n     \"      grid=(2,),\\n\",\n-    \"      compiler_params=pltpu.TPUCompilerParams(\\n\",\n+    \"      compiler_params=pltpu.CompilerParams(\\n\",\n     \"          dimension_semantics=(\\\"parallel\\\",))\\n\",\n     \"  )(x, y)\\n\",\n     \"\\n\",\ndiff --git a/docs/pallas/tpu/pipelining.md b/docs/pallas/tpu/pipelining.md\nindex b9ed41f937c8..44a252410151 100644\n--- a/docs/pallas/tpu/pipelining.md\n+++ b/docs/pallas/tpu/pipelining.md\n@@ -94,15 +94,15 @@ Pallas exposes all levels of the TPU memory hierarchy to users. The following ta\n \n | Pallas Enum | TPU Memory Space | Type (DRAM/SRAM) |\n | --- | --- | --- |\n-| `pltpu.TPUMemorySpace.ANY` | HBM (usually) or VMEM | DRAM |\n-| `pltpu.TPUMemorySpace.VMEM` | VMEM | SRAM |\n-| `pltpu.TPUMemorySpace.SMEM` | SMEM | SRAM |\n-| `pltpu.TPUMemorySpace.SEMAPHORE` | Semaphore | SRAM |\n+| `pltpu.MemorySpace.ANY` | HBM (usually) or VMEM | DRAM |\n+| `pltpu.MemorySpace.VMEM` | VMEM | SRAM |\n+| `pltpu.MemorySpace.SMEM` | SMEM | SRAM |\n+| `pltpu.MemorySpace.SEMAPHORE` | Semaphore | SRAM |\n \n-- `TPUMemorySpace.VMEM` denotes vector SRAM. It is the default memory space if nothing is specified.\n-- `TPUMemorySpace.SMEM` denotes scalar SRAM. Only scalar loads and stores can be performed to/from SMEM.\n-- `TPUMemorySpace.ANY` is a hint to the compiler that the memory space is unconstrained. In most cases, XLA will place this buffer in HBM. A buffer assigned to the `ANY` memory space cannot be dereferenced normally using array indexing syntax (e.g. `x[...]`). Instead, we must first copy the values into a VMEM or SMEM buffer using `pltpu.sync_copy` or `pltpu.async_copy`.\n-- `TPUMemorySpace.SEMAPHORE` is used to allocate semaphores for constructing barriers or tracking asynchronous operations. It is also possible to return semaphores from the kernel for building asynchronous kernels - this is an experimental feature; see {ref}`pallas_async` for more details.\n+- `MemorySpace.VMEM` denotes vector SRAM. It is the default memory space if nothing is specified.\n+- `MemorySpace.SMEM` denotes scalar SRAM. Only scalar loads and stores can be performed to/from SMEM.\n+- `MemorySpace.ANY` is a hint to the compiler that the memory space is unconstrained. In most cases, XLA will place this buffer in HBM. A buffer assigned to the `ANY` memory space cannot be dereferenced normally using array indexing syntax (e.g. `x[...]`). Instead, we must first copy the values into a VMEM or SMEM buffer using `pltpu.sync_copy` or `pltpu.async_copy`.\n+- `MemorySpace.SEMAPHORE` is used to allocate semaphores for constructing barriers or tracking asynchronous operations. It is also possible to return semaphores from the kernel for building asynchronous kernels - this is an experimental feature; see {ref}`pallas_async` for more details.\n \n Pipelining on TPUs is typically done between HBM (DRAM) to VMEM (Vector SRAM). The default behavior for `pallas_call` on TPU is that arguments to `pallas_call` are assumed to live in HBM, and inputs to the user kernel body are stored in VMEM.\n \n@@ -128,9 +128,9 @@ def hbm_vmem_kernel(x_hbm_ref, out_vmem_ref, scratch_vmem_ref):\n \n x = jax.random.uniform(jax.random.key(0), (8, 128), jnp.float32)\n out = pl.pallas_call(hbm_vmem_kernel,\n-  in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)],\n+  in_specs=[pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY)],\n   out_shape=jax.ShapeDtypeStruct((1, 128), jnp.float32),\n-  scratch_shapes=(pltpu.TPUMemorySpace.VMEM(shape=(1, 128), dtype=jnp.float32),)\n+  scratch_shapes=(pltpu.MemorySpace.VMEM(shape=(1, 128), dtype=jnp.float32),)\n )(x)\n \n np.testing.assert_allclose(out, x[0:1] + 1)\n@@ -190,7 +190,7 @@ def add_matrices_pipelined_megacore(x: jax.Array, y: jax.Array) -> jax.Array:\n       in_specs=[block_spec, block_spec],\n       out_specs=block_spec,\n       grid=(2,),\n-      compiler_params=pltpu.TPUCompilerParams(\n+      compiler_params=pltpu.CompilerParams(\n           dimension_semantics=(\"parallel\",))\n   )(x, y)\n \ndiff --git a/jax/_src/pallas/mosaic/core.py b/jax/_src/pallas/mosaic/core.py\nindex 49ff632f5c14..b864a1df2ec5 100644\n--- a/jax/_src/pallas/mosaic/core.py\n+++ b/jax/_src/pallas/mosaic/core.py\n@@ -66,7 +66,7 @@ class GridDimensionSemantics(enum.Enum):\n \n \n @dataclasses.dataclass(frozen=True)\n-class TPUCompilerParams(pallas_core.CompilerParams):\n+class CompilerParams(pallas_core.CompilerParams):\n   \"\"\"Mosaic TPU compiler parameters.\n \n   Attributes:\n@@ -102,7 +102,7 @@ class TPUCompilerParams(pallas_core.CompilerParams):\n   # Replace is a method, not a field.\n   replace = dataclasses.replace\n \n-class TPUMemorySpace(enum.Enum):\n+class MemorySpace(enum.Enum):\n   ANY = \"any\"  # TODO(b/368401328): Remove this and just use pl.ANY.\n   VMEM = \"vmem\"\n   SMEM = \"smem\"\n@@ -135,7 +135,7 @@ def __call__(self, shape: tuple[int, ...]):\n       dtype = pallas_core.BarrierSemaphore()\n     else:\n       dtype = pallas_core.Semaphore()\n-    return pallas_core.MemoryRef(shape, dtype, TPUMemorySpace.SEMAPHORE)\n+    return pallas_core.MemoryRef(shape, dtype, MemorySpace.SEMAPHORE)\n \n   def get_array_aval(self) -> pallas_core.ShapedArrayWithMemorySpace:\n     return self(()).get_array_aval()\n@@ -166,7 +166,7 @@ def __init__(\n \n   def _make_scalar_ref_aval(self, aval):\n     return AbstractMemoryRef(jax_core.ShapedArray(aval.shape, aval.dtype),\n-                             TPUMemorySpace.SMEM)\n+                             MemorySpace.SMEM)\n \n \n @dataclasses.dataclass(frozen=True)\n@@ -223,12 +223,12 @@ def _tensorcore_mesh_discharge_rule(\n     name: str,\n ):\n   assert isinstance(mesh, TensorCoreMesh)\n-  if compiler_params and not isinstance(compiler_params, TPUCompilerParams):\n+  if compiler_params and not isinstance(compiler_params, CompilerParams):\n     raise ValueError(\n-        \"compiler_params must be a pltpu.TPUCompilerParams\"\n+        \"compiler_params must be a pltpu.CompilerParams\"\n     )\n   if not compiler_params:\n-    compiler_params = TPUCompilerParams()\n+    compiler_params = CompilerParams()\n   if len(mesh.shape) > 1:\n     raise NotImplementedError(\"Mesh must be 1D\")\n   if compiler_params.dimension_semantics is not None:\ndiff --git a/jax/_src/pallas/mosaic/interpret.py b/jax/_src/pallas/mosaic/interpret.py\nindex 401ed02288bc..03c99c794ac7 100644\n--- a/jax/_src/pallas/mosaic/interpret.py\n+++ b/jax/_src/pallas/mosaic/interpret.py\n@@ -817,16 +817,16 @@ def _allocate_semaphores(\n   ).reshape(shape)\n \n \n-TPU_MEMORY_SPACE_IDXS : dict[mosaic_core.TPUMemorySpace | pallas_core.MemorySpace | None, int] = {\n-    v: i for i, v in enumerate(mosaic_core.TPUMemorySpace)}\n+TPU_MEMORY_SPACE_IDXS : dict[mosaic_core.MemorySpace | pallas_core.MemorySpace | None, int] = {\n+    v: i for i, v in enumerate(mosaic_core.MemorySpace)}\n TPU_MEMORY_SPACE_IDXS[pallas_core.MemorySpace.ANY] = (\n-    TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY])\n+    TPU_MEMORY_SPACE_IDXS[mosaic_core.MemorySpace.ANY])\n TPU_MEMORY_SPACE_NAMES = {\n-    i: v.value for i, v in enumerate(mosaic_core.TPUMemorySpace)}\n+    i: v.value for i, v in enumerate(mosaic_core.MemorySpace)}\n \n # Default to VMEM when no memory space is specified.\n TPU_MEMORY_SPACE_IDXS[None] = (\n-    TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.VMEM])\n+    TPU_MEMORY_SPACE_IDXS[mosaic_core.MemorySpace.VMEM])\n \n def get_barrier_semaphore(device_id, collective_id):\n   del device_id\n@@ -1340,7 +1340,7 @@ def _to_jaxpr(flat_fun, in_avals):\n   return new_jaxpr\n \n def _is_any(memory_space):\n-  return ((memory_space == mosaic_core.TPUMemorySpace.ANY) or\n+  return ((memory_space == mosaic_core.MemorySpace.ANY) or\n           (memory_space == pallas_core.MemorySpace.ANY))\n \n def _is_float(dtype):\n@@ -1521,7 +1521,7 @@ def f(*args, jaxpr):\n         # runs the same sequence of `run_scoped`s.\n         allocs = []\n         for v in eqn.params['jaxpr'].invars:\n-          if v.aval.memory_space == mosaic_core.TPUMemorySpace.SEMAPHORE:\n+          if v.aval.memory_space == mosaic_core.MemorySpace.SEMAPHORE:\n             allocs.append(callback.io_callback(\n                 _allocate_semaphores,\n                 jax.ShapeDtypeStruct(v.aval.shape, jnp.int16),\n@@ -1543,7 +1543,7 @@ def f(*args, jaxpr):\n         out = _interpret(eqn.params['jaxpr'], *deferred_invals(), *allocs)\n \n         for a, v in zip(allocs, eqn.params['jaxpr'].invars):\n-          if v.aval.memory_space == mosaic_core.TPUMemorySpace.SEMAPHORE:\n+          if v.aval.memory_space == mosaic_core.MemorySpace.SEMAPHORE:\n             # TODO(jburnim): De-allocate semaphores.\n             # callback.io_callback(\n             #     _deallocate_semaphores,\n@@ -1609,9 +1609,9 @@ def f(*args, jaxpr):\n             (),\n             device_id,\n             local_core_id,\n-            TPU_MEMORY_SPACE_IDXS[getattr(orig_src_ref.aval, 'memory_space', mosaic_core.TPUMemorySpace.ANY)],\n+            TPU_MEMORY_SPACE_IDXS[getattr(orig_src_ref.aval, 'memory_space', mosaic_core.MemorySpace.ANY)],\n             src, src_transforms,\n-            TPU_MEMORY_SPACE_IDXS[getattr(orig_dst_ref.aval, 'memory_space', mosaic_core.TPUMemorySpace.ANY)],\n+            TPU_MEMORY_SPACE_IDXS[getattr(orig_dst_ref.aval, 'memory_space', mosaic_core.MemorySpace.ANY)],\n             dst, dst_transforms,\n             state_discharge.transform_array(dst_sem, dst_sem_transforms),\n             state_discharge.transform_array(src_sem, src_sem_transforms),\n@@ -1749,11 +1749,11 @@ def _get_next_indices(grid, indices):\n   return tuple(reversed(next_indices))\n \n \n-def _get_mosaic_params(compiler_params: dict[str, pallas_core.CompilerParams]) -> tpu_core.TPUCompilerParams:\n+def _get_mosaic_params(compiler_params: dict[str, pallas_core.CompilerParams]) -> tpu_core.CompilerParams:\n   try:\n-    return cast(tpu_core.TPUCompilerParams, compiler_params['mosaic_tpu'])\n+    return cast(tpu_core.CompilerParams, compiler_params['mosaic_tpu'])\n   except KeyError:\n-    return tpu_core.TPUCompilerParams()\n+    return tpu_core.CompilerParams()\n \n \n def _get_parallel_dim_semantics(\n@@ -1762,7 +1762,7 @@ def _get_parallel_dim_semantics(\n   \"\"\"Returns a tuple indicating which grid dimensions have parallel semantics.\n \n   Args:\n-    compiler_params: Representation of a `mosaic_core.TPUCompilerParams` object\n+    compiler_params: Representation of a `mosaic_core.CompilerParams` object\n       as a dictionary.\n     num_dimensions_in_grid: The number of dimensions in the grid.\n \n@@ -1818,7 +1818,7 @@ def _get_randomized_grid_coordinates(\n \n   Args:\n     grid: Tuple of sizes of the dimensions in the grid.\n-    compiler_params: Representation of a `mosaic_core.TPUCompilerParams` object\n+    compiler_params: Representation of a `mosaic_core.CompilerParams` object\n       as a dictionary.\n     parallel_semantics_per_dim: A tuple of booleans indicating whether the\n       corresponding dimension in the grid has parallel semantics.\n@@ -1986,7 +1986,7 @@ def interpret_pallas_call(\n         jax.ShapeDtypeStruct((), jnp.int16),\n         device_id,\n         None,  # local_core_id\n-        TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n+        TPU_MEMORY_SPACE_IDXS[mosaic_core.MemorySpace.ANY],\n         input_args[i],\n         ordered=True))\n \n@@ -2016,7 +2016,7 @@ def interpret_pallas_call(\n               jax.ShapeDtypeStruct((), jnp.int16),\n               device_id,\n               None,  # local_core_id\n-              TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n+              TPU_MEMORY_SPACE_IDXS[mosaic_core.MemorySpace.ANY],\n               padded_val,\n               ordered=True,\n           )\n@@ -2036,7 +2036,7 @@ def interpret_pallas_call(\n             jax.ShapeDtypeStruct((), jnp.int16),\n             device_id,\n             None,  # local_core_id,\n-            TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.SMEM],\n+            TPU_MEMORY_SPACE_IDXS[mosaic_core.MemorySpace.SMEM],\n             val,\n             ordered=True,\n         )\n@@ -2047,7 +2047,7 @@ def interpret_pallas_call(\n     output_idx = i - grid_mapping.num_inputs\n     is_input = i < grid_mapping.num_inputs\n     is_output = (output_idx >= 0) and (output_idx < grid_mapping.num_outputs)\n-    if var.aval.memory_space == mosaic_core.TPUMemorySpace.SEMAPHORE:\n+    if var.aval.memory_space == mosaic_core.MemorySpace.SEMAPHORE:\n       kernel_buffer_ids.append(\n           callback.io_callback(\n               _allocate_semaphores,\n@@ -2241,7 +2241,7 @@ def _store_slice_to_kernel_input(index, input_var):\n             jax.ShapeDtypeStruct(input_var.aval.shape, input_var.aval.dtype),\n             device_id,\n             cur_local_core_id,\n-            TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n+            TPU_MEMORY_SPACE_IDXS[mosaic_core.MemorySpace.ANY],\n             input_buffer_ids[index],\n             (transform,),\n             ordered=True,\n@@ -2318,7 +2318,7 @@ def _store_to_output_buffer(index, output_var):\n             (),\n             device_id,\n             cur_local_core_id,\n-            TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n+            TPU_MEMORY_SPACE_IDXS[mosaic_core.MemorySpace.ANY],\n             output_buffer_ids[index],\n             (transform,),\n             kernel_output_val,\n@@ -2398,7 +2398,7 @@ def _store_to_output_buffer(index, output_var):\n           val,\n           device_id,\n           0,  # local_core_id\n-          TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n+          TPU_MEMORY_SPACE_IDXS[mosaic_core.MemorySpace.ANY],\n           output_buffer_id,\n           (indexing.NDIndexer.from_indices_shape(\n               tuple(indexing.ds(0, s) for s in val.shape),\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 873bf587093a..b1a2c186e2c9 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -15,12 +15,12 @@\n \"\"\"Module for lowering JAX to Mosaic-compatible MLIR dialects.\"\"\"\n from __future__ import annotations\n \n-from collections.abc import Callable, Collection, Sequence\n+from collections.abc import Callable, Collection, Hashable, Sequence\n import contextlib\n import dataclasses\n import functools\n import string\n-from typing import Any, Hashable, TypeVar\n+from typing import Any, TypeVar\n \n import jax\n from jax import api_util\n@@ -86,10 +86,10 @@\n # mypy: ignore-errors\n \n NDIndexer = indexing.NDIndexer\n-TPUMemorySpace = tpu_core.TPUMemorySpace\n-MemorySpace = pallas_core.MemorySpace | TPUMemorySpace\n-VMEM = tpu_core.TPUMemorySpace.VMEM\n-SMEM = tpu_core.TPUMemorySpace.SMEM\n+TPUMemorySpace = tpu_core.MemorySpace\n+AnyMemorySpace = pallas_core.MemorySpace | TPUMemorySpace\n+VMEM = TPUMemorySpace.VMEM\n+SMEM = TPUMemorySpace.SMEM\n # Booleans are stored as the following type in memrefs.\n BOOL_MEMREF_TYPE = np.dtype('int32')\n \n@@ -212,7 +212,7 @@ def forward_compatible(self):\n     return self.lowering_context.forward_compatible\n \n \n-def _memory_space_to_tpu_memory_space(memory_space: MemorySpace | None\n+def _memory_space_to_tpu_memory_space(memory_space: AnyMemorySpace | None\n                                      ) -> TPUMemorySpace:\n   match memory_space:\n     case None:\n@@ -235,7 +235,7 @@ def _memory_space_to_tpu_memory_space(memory_space: MemorySpace | None\n       raise ValueError(f\"Invalid memory space: {memory_space}\")\n \n \n-def _memory_space_to_mosaic_attribute(memory_space: MemorySpace | None\n+def _memory_space_to_mosaic_attribute(memory_space: AnyMemorySpace | None\n                                       ) -> ir.Attribute:\n   tpu_memory_space = _memory_space_to_tpu_memory_space(memory_space)\n   return ir.Attribute.parse(f\"#tpu.memory_space<{tpu_memory_space}>\")\n@@ -266,7 +266,7 @@ def aval_to_ir_type(\n     dynamic_shape_replacement_fn,\n     aval,\n     shape=None,\n-    memory_space: MemorySpace | None = None,\n+    memory_space: AnyMemorySpace | None = None,\n     is_kernel_boundary: bool = False,\n ):\n   if isinstance(aval, tpu_core.AbstractSemaphore):\n@@ -600,9 +600,9 @@ def _check_block_mappings(\n     # TODO(necula): add tests for SMEM blocks with trivial windowing\n     # We support scalars too\n     memory_space = _memory_space_to_tpu_memory_space(bm.block_aval.memory_space)\n-    if memory_space == tpu_core.TPUMemorySpace.SMEM and bm.has_trivial_window():\n+    if memory_space == tpu_core.MemorySpace.SMEM and bm.has_trivial_window():\n       continue\n-    if memory_space == tpu_core.TPUMemorySpace.SEMAPHORE:\n+    if memory_space == tpu_core.MemorySpace.SEMAPHORE:\n       continue\n \n     def err_details():\n@@ -619,7 +619,7 @@ def err_details():\n           \"rank >= 1. \" + err_details())\n \n     if (\n-        memory_space == tpu_core.TPUMemorySpace.ANY\n+        memory_space == tpu_core.MemorySpace.ANY\n         and not bm.has_trivial_window()\n     ):\n       raise ValueError(\n@@ -761,8 +761,8 @@ def dynamic_shape_replacement_fn(\n       tpu_memory_space = _memory_space_to_tpu_memory_space(\n           bm.block_aval.memory_space)\n       if (\n-          tpu_memory_space == tpu_core.TPUMemorySpace.ANY\n-          or tpu_memory_space == tpu_core.TPUMemorySpace.SEMAPHORE\n+          tpu_memory_space == tpu_core.MemorySpace.ANY\n+          or tpu_memory_space == tpu_core.MemorySpace.SEMAPHORE\n       ):\n         # We checked above that the block does not require windowing.\n         window_params.append(ir.DictAttr.get())\n@@ -784,7 +784,7 @@ def dynamic_shape_replacement_fn(\n       # Force single-buffering pipelining for trivial windowing in VMEM.\n       pipeline_mode = bm.pipeline_mode\n       if (\n-          tpu_memory_space == tpu_core.TPUMemorySpace.VMEM\n+          tpu_memory_space == tpu_core.MemorySpace.VMEM\n           and bm.has_trivial_window()\n       ):\n         pipeline_mode = pallas_core.Buffered(1)\n@@ -1520,7 +1520,7 @@ def _load_lowering_rule(ctx: LoweringRuleContext, *args_flat, args_tree, **_):\n   ):\n     if not is_smem_load:\n       raise ValueError(\"PRNG keys must be loaded from SMEM. Did you set \"\n-                       \"the memory space to TPUMemorySpace.SMEM in the \"\n+                       \"the memory space to MemorySpace.SMEM in the \"\n                        \"BlockSpec for the PRNG key input?\")\n     return _prng_key_load_lowering_rule(ctx, *args_flat, args_tree=args_tree)\n   if not is_smem_load and not ref_block_shape:\ndiff --git a/jax/_src/pallas/mosaic/pallas_call_registration.py b/jax/_src/pallas/mosaic/pallas_call_registration.py\nindex 74253e809a35..528f897edf74 100644\n--- a/jax/_src/pallas/mosaic/pallas_call_registration.py\n+++ b/jax/_src/pallas/mosaic/pallas_call_registration.py\n@@ -80,13 +80,13 @@ def _get_memory_space_from_aval(\n   match out_aval.memory_space:\n     case None:\n       return None\n-    case tpu_core.TPUMemorySpace.ANY:\n+    case tpu_core.MemorySpace.ANY:\n       return None\n-    case tpu_core.TPUMemorySpace.VMEM:\n+    case tpu_core.MemorySpace.VMEM:\n       return tpu_custom_call.MemorySpace.VMEM\n-    case tpu_core.TPUMemorySpace.SMEM:\n+    case tpu_core.MemorySpace.SMEM:\n       return tpu_custom_call.MemorySpace.SMEM\n-    case tpu_core.TPUMemorySpace.SEMAPHORE:\n+    case tpu_core.MemorySpace.SEMAPHORE:\n       return tpu_custom_call.MemorySpace.SEMAPHORE_MEM\n   return None\n \n@@ -126,10 +126,10 @@ def pallas_call_tpu_lowering_rule(\n \n   if \"mosaic_tpu\" in compiler_params:\n     mosaic_params = cast(\n-        tpu_core.TPUCompilerParams, compiler_params[\"mosaic_tpu\"]\n+        tpu_core.CompilerParams, compiler_params[\"mosaic_tpu\"]\n     )\n   else:\n-    mosaic_params = tpu_core.TPUCompilerParams()\n+    mosaic_params = tpu_core.CompilerParams()\n \n   jax_mesh = None\n   axis_context = ctx.module_context.axis_context\ndiff --git a/jax/_src/pallas/mosaic/pipeline.py b/jax/_src/pallas/mosaic/pipeline.py\nindex f4dab313fb6f..659146a4ad7e 100644\n--- a/jax/_src/pallas/mosaic/pipeline.py\n+++ b/jax/_src/pallas/mosaic/pipeline.py\n@@ -36,8 +36,8 @@\n import numpy as np\n \n \n-SMEM = tpu_core.TPUMemorySpace.SMEM\n-VMEM = tpu_core.TPUMemorySpace.VMEM\n+SMEM = tpu_core.MemorySpace.SMEM\n+VMEM = tpu_core.MemorySpace.VMEM\n REF = pallas_core.MemoryRef\n GridDimensionSemantics = tpu_core.GridDimensionSemantics\n PARALLEL = tpu_core.PARALLEL\ndiff --git a/jax/_src/pallas/mosaic/primitives.py b/jax/_src/pallas/mosaic/primitives.py\nindex 33a1de12ebde..c9cdcbf56f85 100644\n--- a/jax/_src/pallas/mosaic/primitives.py\n+++ b/jax/_src/pallas/mosaic/primitives.py\n@@ -642,7 +642,7 @@ def async_remote_copy(src_ref, dst_ref, send_sem, recv_sem, device_id,\n def _get_barrier_semaphore_abstract_eval():\n   return pl_core.AbstractMemoryRef(\n       jax_core.ShapedArray((), pl_core.BarrierSemaphore()),\n-      tpu_core.TPUMemorySpace.SEMAPHORE,\n+      tpu_core.MemorySpace.SEMAPHORE,\n   )\n \n def get_barrier_semaphore():\ndiff --git a/jax/_src/pallas/pallas_call.py b/jax/_src/pallas/pallas_call.py\nindex 016bac96424e..93b929d219c9 100644\n--- a/jax/_src/pallas/pallas_call.py\n+++ b/jax/_src/pallas/pallas_call.py\n@@ -1545,7 +1545,7 @@ def pallas_call(\n       {file}:{line}`.\n     compiler_params: Optional compiler parameters. The value should either be a\n       backend-specific dataclass\n-      (:class:`jax.experimental.pallas.tpu.TPUCompilerParams`,\n+      (:class:`jax.experimental.pallas.tpu.CompilerParams`,\n       :class:`jax.experimental.pallas.triton.CompilerParams`,\n       :class:`jax.experimental.pallas.mosaic_gpu.CompilerParams`) or a dict\n       mapping backend name to the corresponding platform-specific dataclass.\ndiff --git a/jax/experimental/pallas/ops/tpu/all_gather.py b/jax/experimental/pallas/ops/tpu/all_gather.py\nindex a0eb07f719ec..ce80a443547e 100644\n--- a/jax/experimental/pallas/ops/tpu/all_gather.py\n+++ b/jax/experimental/pallas/ops/tpu/all_gather.py\n@@ -120,7 +120,7 @@ def ag_kernel(x_ref, o_ref, send_sem, recv_sem, *, axis_name: str,\n     jax.jit, static_argnames=[\"mesh\", \"axis_name\", \"memory_space\"]\n )\n def all_gather(x, *, mesh: jax.sharding.Mesh, axis_name: str | Sequence[str],\n-               memory_space: pltpu.TPUMemorySpace = pltpu.VMEM):\n+               memory_space: pltpu.MemorySpace = pltpu.VMEM):\n   if isinstance(axis_name, str):\n     axis_name = (axis_name,)\n   # TODO(sharadmv): enable all gather over multiple axes\n@@ -136,7 +136,7 @@ def ag_local(x_shard):\n     out = pl.pallas_call(\n         functools.partial(ag_kernel, axis_name=axis_name, mesh=mesh),\n         out_shape=out_shape,\n-        compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n+        compiler_params=pltpu.CompilerParams(collective_id=0),\n         grid_spec=pltpu.PrefetchScalarGridSpec(\n             num_scalar_prefetch=0,\n             scratch_shapes=(\ndiff --git a/jax/experimental/pallas/ops/tpu/flash_attention.py b/jax/experimental/pallas/ops/tpu/flash_attention.py\nindex 06746986a15e..27f66d34e354 100644\n--- a/jax/experimental/pallas/ops/tpu/flash_attention.py\n+++ b/jax/experimental/pallas/ops/tpu/flash_attention.py\n@@ -767,7 +767,7 @@ def kv_segment_ids_index_map(\n       ),\n       out_shape=out_shape,\n       debug=debug,\n-      compiler_params=pltpu.TPUCompilerParams(\n+      compiler_params=pltpu.CompilerParams(\n           dimension_semantics=(\n               \"parallel\",\n               \"parallel\",\n@@ -1130,7 +1130,7 @@ def dkv_index_map(batch_index, head_index, kv_seq_index, _):\n         ),\n         out_shape=out_shapes,\n         debug=debug,\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n                 dimension_semantics=(\n                     \"parallel\",\n                     \"parallel\",\n@@ -1465,7 +1465,7 @@ def kv_segment_ids_index_map(\n         ),\n         out_shape=out_shapes,\n         debug=debug,\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n                 dimension_semantics=(\n                     \"parallel\",\n                     \"parallel\",\ndiff --git a/jax/experimental/pallas/ops/tpu/matmul.py b/jax/experimental/pallas/ops/tpu/matmul.py\nindex 06d868168f9e..341aa93fa258 100644\n--- a/jax/experimental/pallas/ops/tpu/matmul.py\n+++ b/jax/experimental/pallas/ops/tpu/matmul.py\n@@ -78,7 +78,7 @@ def matmul(\n           grid=(x.shape[0] // l, y.shape[1] // r, x.shape[1] // block_k),\n           scratch_shapes=[pltpu.VMEM((l, r), acc_dtype)],\n       ),\n-      compiler_params=pltpu.TPUCompilerParams(\n+      compiler_params=pltpu.CompilerParams(\n           dimension_semantics=(\"parallel\", \"parallel\", \"arbitrary\")),\n       debug=debug,\n   )(x, y)\ndiff --git a/jax/experimental/pallas/ops/tpu/megablox/gmm.py b/jax/experimental/pallas/ops/tpu/megablox/gmm.py\nindex 5c2f938597e7..cb185fc45f1d 100644\n--- a/jax/experimental/pallas/ops/tpu/megablox/gmm.py\n+++ b/jax/experimental/pallas/ops/tpu/megablox/gmm.py\n@@ -538,7 +538,7 @@ def out_transform_indices(n_i, grid_id, k_i, group_metadata, group_offset):\n           scratch_shapes=[pltpu.VMEM((tm, tn), jnp.float32)],\n       ),\n       input_output_aliases=input_output_aliases,\n-      compiler_params=pltpu.TPUCompilerParams(\n+      compiler_params=pltpu.CompilerParams(\n               dimension_semantics=(\"parallel\", \"arbitrary\", \"arbitrary\")),\n       interpret=interpret,\n       cost_estimate=cost_estimate,\n@@ -777,7 +777,7 @@ def out_transform_indices(n_i, k_i, grid_id, group_metadata, group_offset):\n           scratch_shapes=[pltpu.VMEM((tk, tn), jnp.float32)],\n       ),\n       input_output_aliases=input_output_aliases,\n-      compiler_params=pltpu.TPUCompilerParams(\n+      compiler_params=pltpu.CompilerParams(\n               dimension_semantics=(\"parallel\", \"arbitrary\", \"arbitrary\")),\n       interpret=interpret,\n       cost_estimate=cost_estimate,\ndiff --git a/jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py b/jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py\nindex 9c02679c45ea..309858368896 100644\n--- a/jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py\n+++ b/jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py\n@@ -648,7 +648,7 @@ def paged_attention(\n           grid=grid,\n           scratch_shapes=scratch_shapes,\n       ),\n-      compiler_params=pltpu.TPUCompilerParams(\n+      compiler_params=pltpu.CompilerParams(\n           dimension_semantics=dimension_semantics\n       ),\n       out_shape=[\ndiff --git a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\nindex 67c0b376ecc6..e7bc599b2b2b 100644\n--- a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n+++ b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n@@ -891,7 +891,7 @@ def q_index_map(heads_blk_idx, q_blk_idx, *_):\n           grid=grid,\n           scratch_shapes=scratch_shapes,\n       ),\n-      compiler_params=pltpu.TPUCompilerParams(\n+      compiler_params=pltpu.CompilerParams(\n           dimension_semantics=(\n               \"arbitrary\",\n               \"arbitrary\",\ndiff --git a/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py b/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py\nindex b69b0e36f177..34d8847e6193 100644\n--- a/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py\n+++ b/jax/experimental/pallas/ops/tpu/splash_attention/splash_attention_kernel.py\n@@ -1118,7 +1118,7 @@ def logsumexp_index_map(h, i, *_):\n             out_specs=out_specs,\n             grid=grid,\n         ),\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n           dimension_semantics=(\"parallel\", \"arbitrary\", \"arbitrary\"),\n         ),\n         out_shape=out_shapes,\n@@ -1577,7 +1577,7 @@ def logsumexp_index_map(h, i, *_):\n             grid=grid,\n         ),\n         out_shape=out_shapes,\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n           dimension_semantics=(\"arbitrary\", \"arbitrary\", \"arbitrary\"),\n         ),\n         name=kernel_name,\n@@ -2126,7 +2126,7 @@ def logsumexp_index_map(\n         #    megacore\n         # 2) for heads, we are reducing over heads\n         # 3) for q_seq_len, we are reducing over it to compute dkv\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n           dimension_semantics=(\"arbitrary\", \"arbitrary\", \"arbitrary\"),\n         ),\n         name=kernel_name,\ndiff --git a/jax/experimental/pallas/tpu.py b/jax/experimental/pallas/tpu.py\nindex c4d21023a6e6..5ac79558b11d 100644\n--- a/jax/experimental/pallas/tpu.py\n+++ b/jax/experimental/pallas/tpu.py\n@@ -23,8 +23,8 @@\n from jax._src.pallas.mosaic.core import PARALLEL as PARALLEL\n from jax._src.pallas.mosaic.core import PrefetchScalarGridSpec as PrefetchScalarGridSpec\n from jax._src.pallas.mosaic.core import SemaphoreType as SemaphoreType\n-from jax._src.pallas.mosaic.core import TPUMemorySpace as TPUMemorySpace\n-from jax._src.pallas.mosaic.core import TPUCompilerParams as TPUCompilerParams\n+from jax._src.pallas.mosaic.core import MemorySpace as MemorySpace\n+from jax._src.pallas.mosaic.core import CompilerParams as CompilerParams\n from jax._src.pallas.mosaic.helpers import sync_copy as sync_copy\n from jax._src.pallas.mosaic.helpers import core_barrier as core_barrier\n from jax._src.pallas.mosaic.helpers import run_on_first_core as run_on_first_core\n@@ -68,8 +68,29 @@\n )\n del types, assume, pretend, skip, define_model  # Clean up.\n \n-ANY = TPUMemorySpace.ANY\n-CMEM = TPUMemorySpace.CMEM\n-SMEM = TPUMemorySpace.SMEM\n-VMEM = TPUMemorySpace.VMEM\n-SEMAPHORE = TPUMemorySpace.SEMAPHORE\n+ANY = MemorySpace.ANY\n+CMEM = MemorySpace.CMEM\n+SMEM = MemorySpace.SMEM\n+VMEM = MemorySpace.VMEM\n+SEMAPHORE = MemorySpace.SEMAPHORE\n+\n+import typing as _typing  # pylint: disable=g-import-not-at-top\n+if _typing.TYPE_CHECKING:\n+  TPUCompilerParams = CompilerParams\n+  TPUMemorySpace = MemorySpace\n+else:\n+  from jax._src.deprecations import deprecation_getattr as _deprecation_getattr\n+  _deprecations = {\n+      # Deprecated on May 30th 2025.\n+      \"TPUCompilerParams\": (\n+          \"TPUCompilerParams is deprecated, use CompilerParams instead.\",\n+          CompilerParams,\n+      ),\n+      \"TPUMemorySpace\": (\n+          \"TPUMemorySpace is deprecated, use MemorySpace instead.\",\n+          MemorySpace,\n+      ),\n+  }\n+  __getattr__ = _deprecation_getattr(__name__, _deprecations)\n+  del _deprecation_getattr\n+del _typing\ndiff --git a/tests/pallas/tpu_fusible_matmul_test.py b/tests/pallas/tpu_fusible_matmul_test.py\nindex 2382c09f26ac..4bde9b95483b 100644\n--- a/tests/pallas/tpu_fusible_matmul_test.py\n+++ b/tests/pallas/tpu_fusible_matmul_test.py\n@@ -177,7 +177,7 @@ def z_index_map(i, j, k, *_):\n           ],\n           out_specs=[z_out_block_spec],\n       ),\n-      compiler_params=pltpu.TPUCompilerParams(\n+      compiler_params=pltpu.CompilerParams(\n           dimension_semantics=dimension_semantics,\n       ),\n       out_shape=[z_out_type],\ndiff --git a/tests/pallas/tpu_ops_test.py b/tests/pallas/tpu_ops_test.py\nindex 3f6dc593e333..a67e74d617b6 100644\n--- a/tests/pallas/tpu_ops_test.py\n+++ b/tests/pallas/tpu_ops_test.py\n@@ -534,7 +534,7 @@ def kernel(src, tgt):\n     run = pl.pallas_call(\n         kernel,\n         jax.ShapeDtypeStruct(tgt_shape, jnp.float32),\n-        compiler_params=pltpu.TPUCompilerParams(disable_bounds_checks=True),\n+        compiler_params=pltpu.CompilerParams(disable_bounds_checks=True),\n     )\n     output = run(x)\n     np.testing.assert_array_equal(\ndiff --git a/tests/pallas/tpu_pallas_async_test.py b/tests/pallas/tpu_pallas_async_test.py\nindex e464214928e4..c70fb6ea2ff5 100644\n--- a/tests/pallas/tpu_pallas_async_test.py\n+++ b/tests/pallas/tpu_pallas_async_test.py\n@@ -436,7 +436,7 @@ def copy_start_kernel(x_ref, aliased_x_ref, o_ref, send_sem, recv_sem):\n             pl.BlockSpec(memory_space=pltpu.SEMAPHORE),\n         ),\n         input_output_aliases={0: 0},\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n             collective_id=0, has_side_effects=True\n         ),\n     )(x)\n@@ -537,7 +537,7 @@ def copy_start_kernel(x_ref, aliased_x_ref, o_ref, left_sems, right_sems):\n             (pl.BlockSpec(memory_space=pltpu.SEMAPHORE),) * 2,\n         ),\n         input_output_aliases={0: 0},\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n             collective_id=0, has_side_effects=False\n         ),\n     )(x)\ndiff --git a/tests/pallas/tpu_pallas_distributed_test.py b/tests/pallas/tpu_pallas_distributed_test.py\nindex aa4488b778a8..11b159dbec4c 100644\n--- a/tests/pallas/tpu_pallas_distributed_test.py\n+++ b/tests/pallas/tpu_pallas_distributed_test.py\n@@ -235,7 +235,7 @@ def body(x):\n           in_specs=[pl.BlockSpec(memory_space=pltpu.VMEM)],\n           out_specs=pl.BlockSpec(memory_space=pltpu.VMEM),\n           out_shape=x,\n-          compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n+          compiler_params=pltpu.CompilerParams(collective_id=0),\n       )(x)\n \n     device_mesh = mesh_utils.create_device_mesh(\ndiff --git a/tests/pallas/tpu_pallas_interpret_distributed_test.py b/tests/pallas/tpu_pallas_interpret_distributed_test.py\nindex c5f1b29fd6bc..70ed3dc576e5 100644\n--- a/tests/pallas/tpu_pallas_interpret_distributed_test.py\n+++ b/tests/pallas/tpu_pallas_interpret_distributed_test.py\n@@ -92,7 +92,7 @@ def right_permute_kernel(input_ref, output_ref, send_sem, recv_sem):\n     out_shape = jax.ShapeDtypeStruct((8, 128), jnp.float32)\n     grid_spec = pltpu.PrefetchScalarGridSpec(\n         num_scalar_prefetch=0,\n-        # TPUMemorySpace.ANY will (usually) place the tensor in HBM.\n+        # MemorySpace.ANY will (usually) place the tensor in HBM.\n         in_specs=[\n             pl.BlockSpec(memory_space=pltpu.ANY),\n         ],\n@@ -106,7 +106,7 @@ def right_permute_kernel(input_ref, output_ref, send_sem, recv_sem):\n         right_permute_kernel,\n         out_shape=out_shape,\n         grid_spec=grid_spec,\n-        compiler_params=pltpu.TPUCompilerParams(collective_id=13),\n+        compiler_params=pltpu.CompilerParams(collective_id=13),\n         interpret=pltpu.TPUInterpretParams(\n             dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n     )\n@@ -206,7 +206,7 @@ def _():\n     grid_spec = pltpu.PrefetchScalarGridSpec(\n       num_scalar_prefetch=0,\n       in_specs=[\n-        # TPUMemorySpace.ANY will (usually) place the tensor in HBM.\n+        # MemorySpace.ANY will (usually) place the tensor in HBM.\n         pl.BlockSpec(memory_space=pltpu.ANY),\n       ],\n       out_specs=pl.BlockSpec(memory_space=pltpu.ANY),\n@@ -230,7 +230,7 @@ def _():\n       grid_spec=grid_spec,\n       interpret=pltpu.TPUInterpretParams(\n           dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n-      compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n+      compiler_params=pltpu.CompilerParams(collective_id=0),\n     )\n \n     # Wrap the kernel within a shard_map to call.\n@@ -390,7 +390,7 @@ def _():\n       grid_spec=grid_spec,\n       interpret=pltpu.TPUInterpretParams(\n           dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n-      compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n+      compiler_params=pltpu.CompilerParams(collective_id=0),\n     )\n \n     pallas_result = jax.jit(\n@@ -674,7 +674,7 @@ def pallas_reduce_scatter(input_arr):\n         grid_spec=grid_spec,\n         interpret=pltpu.TPUInterpretParams(\n             dma_execution_mode=dma_execution_mode, detect_races=True),\n-        compiler_params=pltpu.TPUCompilerParams(collective_id=7),\n+        compiler_params=pltpu.CompilerParams(collective_id=7),\n       )(input_arr)[0]\n \n     pallas_result = jax.jit(\n@@ -978,7 +978,7 @@ def pallas_reduce_scatter(input_arr):\n         grid_spec=grid_spec,\n         interpret=pltpu.TPUInterpretParams(\n             dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n-        compiler_params=pltpu.TPUCompilerParams(collective_id=19),\n+        compiler_params=pltpu.CompilerParams(collective_id=19),\n       )(input_arr)[0]\n \n     pallas_result = jax.jit(\ndiff --git a/tests/pallas/tpu_pallas_interpret_test.py b/tests/pallas/tpu_pallas_interpret_test.py\nindex 871f66d71c53..9cf98e6c1dd4 100644\n--- a/tests/pallas/tpu_pallas_interpret_test.py\n+++ b/tests/pallas/tpu_pallas_interpret_test.py\n@@ -381,7 +381,7 @@ def kernel_call_dimensions_parallel_arbitrary(s, grid_point_recorder):\n           interpret=pltpu.TPUInterpretParams(\n               random_seed=12345, grid_point_recorder=grid_point_recorder\n           ),\n-          compiler_params=pltpu.TPUCompilerParams(\n+          compiler_params=pltpu.CompilerParams(\n               dimension_semantics=('parallel', 'arbitrary')\n           ),\n       )(s)\n@@ -437,7 +437,7 @@ def kernel(s_ref, o_ref):\n           in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n           out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n           interpret=pltpu.TPUInterpretParams(random_seed=12345),\n-          compiler_params=pltpu.TPUCompilerParams(\n+          compiler_params=pltpu.CompilerParams(\n               dimension_semantics=('arbitrary', 'parallel')\n           ),\n       )(s)\n@@ -463,7 +463,7 @@ def kernel_call_dynamic_parallel_dimension():\n           in_specs=[],\n           out_specs=pl.BlockSpec((1,), lambda _: (0,)),\n           interpret=pltpu.TPUInterpretParams(),\n-          compiler_params=pltpu.TPUCompilerParams(\n+          compiler_params=pltpu.CompilerParams(\n               dimension_semantics=('parallel',)\n           ),\n       )()\n@@ -516,7 +516,7 @@ def kernel(x_ref, o_ref, vmem_ref):\n         kernel,\n         grid=(2,),\n         out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype),\n-        in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)],\n+        in_specs=[pl.BlockSpec(memory_space=pltpu.MemorySpace.ANY)],\n         scratch_shapes=[\n             pltpu.VMEM(x.shape, x.dtype),\n         ],\n@@ -524,7 +524,7 @@ def kernel(x_ref, o_ref, vmem_ref):\n             num_cores_per_device=2,\n             detect_races=True,\n         ),\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n             dimension_semantics=('parallel',),\n         ),\n     )(x).block_until_ready()\n@@ -558,7 +558,7 @@ def kernel(x_ref, o_ref, vmem_ref):\n             num_cores_per_device=2,\n             detect_races=True,\n         ),\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n             dimension_semantics=('parallel',)\n         ),\n     )(x).block_until_ready()\n@@ -583,7 +583,7 @@ def kernel_call(s, num_cores_per_device, grid_point_recorder):\n               num_cores_per_device=num_cores_per_device,\n               grid_point_recorder=grid_point_recorder,\n           ),\n-          compiler_params=pltpu.TPUCompilerParams(\n+          compiler_params=pltpu.CompilerParams(\n               dimension_semantics=('parallel', 'arbitrary')\n           ),\n       )(s)\ndiff --git a/tests/pallas/tpu_pallas_pipeline_test.py b/tests/pallas/tpu_pallas_pipeline_test.py\nindex 59ac680d3ac3..1c10fa4b73e5 100644\n--- a/tests/pallas/tpu_pallas_pipeline_test.py\n+++ b/tests/pallas/tpu_pallas_pipeline_test.py\n@@ -481,7 +481,7 @@ def _wait_on_prev_dma():\n             + [pltpu.SemaphoreType.DMA] * 4\n             + inner_allocs\n         ),\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n                     collective_id=0,\n                     # must set scoped vmem flag *larger* than below! e.g.:\n                     # flags.FLAGS.xla_tpu_scoped_vmem_limit_kib = 131072\n@@ -724,7 +724,7 @@ def _wait_on_prev_dma():\n             + [pltpu.SemaphoreType.DMA] * 4\n             + inner_allocs,\n         ),\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n             collective_id=0,\n             # must set scoped vmem flag *larger* than below! e.g.:\n             # flags.FLAGS.xla_tpu_scoped_vmem_limit_kib = 131072\n@@ -1007,7 +1007,7 @@ def _loop_epilogue():\n             + [pltpu.SemaphoreType.DMA] * 4\n             + inner_allocs,\n         ),\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n             collective_id=0,\n             # must set scoped vmem flag *larger* than below!\n             # e.g. flags.FLAGS.xla_tpu_scoped_vmem_limit_kib = 131072\n@@ -1268,7 +1268,7 @@ def _prefetch_accumulator():\n             + [pltpu.SemaphoreType.DMA] * 4\n             + inner_allocs,\n         ),\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n             collective_id=0,\n             # must set scoped vmem flag *larger* than below!\n             # e.g. flags.FLAGS.xla_tpu_scoped_vmem_limit_kib = 131072\n@@ -1353,7 +1353,7 @@ def mul_kernel(iters_ref, x_ref, y_ref):\n             out_specs=pl.BlockSpec(memory_space=pltpu.ANY),\n             grid=(num_cores,),\n         ),\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n             dimension_semantics=('parallel',)\n         ),\n     )\n@@ -1389,7 +1389,7 @@ def matmul_kernel(x_ref, y_ref):\n         ],\n         out_specs=pl.BlockSpec(memory_space=pltpu.ANY),\n         grid=(num_cores,),\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n             dimension_semantics=('parallel',)\n         ),\n     )\n@@ -1440,7 +1440,7 @@ def matmul_kernel(x_ref, y_ref, z_ref, *, bm, bk, bn):\n         ],\n         out_specs=pl.BlockSpec(memory_space=pltpu.ANY),\n         grid=(num_cores,),\n-        compiler_params=pltpu.TPUCompilerParams(\n+        compiler_params=pltpu.CompilerParams(\n             dimension_semantics=('parallel',)\n         ),\n     )\ndiff --git a/tests/pallas/tpu_pallas_test.py b/tests/pallas/tpu_pallas_test.py\nindex aac249251e2b..c232ebeedb38 100644\n--- a/tests/pallas/tpu_pallas_test.py\n+++ b/tests/pallas/tpu_pallas_test.py\n@@ -477,7 +477,7 @@ def kernel(s, x):\n               ),\n               grid=8,\n           ),\n-          compiler_params=pltpu.TPUCompilerParams(\n+          compiler_params=pltpu.CompilerParams(\n               allow_input_fusion=[False, True]\n           ),\n       )(s, x)\n@@ -1913,12 +1913,12 @@ def kernel(x_ref, y_ref):\n       self.pallas_call(\n           kernel,\n           out_shape=x,\n-          compiler_params=pltpu.TPUCompilerParams(vmem_limit_bytes=256),\n+          compiler_params=pltpu.CompilerParams(vmem_limit_bytes=256),\n       )(x)\n     self.pallas_call(\n         kernel,\n         out_shape=x,\n-        compiler_params=pltpu.TPUCompilerParams(vmem_limit_bytes=int(2**18)),\n+        compiler_params=pltpu.CompilerParams(vmem_limit_bytes=int(2**18)),\n     )(x)\n \n   def test_allow_input_fusion(self):\n@@ -1935,7 +1935,7 @@ def f(x, y):\n           in_specs=[pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0))],\n           out_specs=pl.BlockSpec((1, 128, 128), lambda i: (i, 0, 0)),\n           out_shape=x,\n-          compiler_params=pltpu.TPUCompilerParams(allow_input_fusion=[True]),\n+          compiler_params=pltpu.CompilerParams(allow_input_fusion=[True]),\n       )(z)\n \n     x = jnp.arange(np.prod(shape), dtype=np.float32).reshape(shape)\n@@ -1963,7 +1963,7 @@ def kernel(x_ref, y_ref):\n       self.pallas_call(\n           kernel,\n           out_shape=jax.ShapeDtypeStruct(shape, jnp.float32),\n-          compiler_params=pltpu.TPUCompilerParams(\n+          compiler_params=pltpu.CompilerParams(\n               internal_scratch_in_bytes=requested_bytes,\n           ),\n       )(x)\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Dropped the `GPU` prefix from `GPUShapeDtypeStruct`\n\nI also slightly tweaked the docstring of `inline_mgpu`, since it references\n`GPUShapeDtypeStruct`\n\nPiperOrigin-RevId: 766575998",
        "output": "```diff\nCommit: 0a5924c81d303460ec76d4b249eba04dd5d734e0\nDate: 2025-06-03T10:13:01Z\nURL: https://github.com/jax-ml/jax/commit/0a5924c81d303460ec76d4b249eba04dd5d734e0\nFiles changed: 3\nAdditions: +31, Deletions: -40\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 9e40d046af13..2bd191b859ba 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -1796,7 +1796,7 @@ def jaxpr_call(\n \n \n @dataclasses.dataclass(frozen=True)\n-class GPUShapeDtypeStruct:\n+class ShapeDtypeStruct:\n   shape: tuple[int, ...]\n   dtype: jnp.dtype\n   layout: ParameterizedLayout | Layout\n@@ -1821,52 +1821,43 @@ def _undo_transforms(\n   return tmp_ref.transforms\n \n \n-def inline_mgpu(arg_types=(), return_type=None):\n-  \"\"\"Decorate a function that inlines mgpu code.\n+def inline_mgpu(*, arg_types=(), return_type=None):\n+  r\"\"\"Returns a decorator that inlines Mosaic GPU code.\n \n-  Arguments provided to the decorated function may be Pallas\n-  references or array values. The body will accept the corresponding\n-  mgpu values.\n+  This allows using lower-level Mosaic GPU abstractions and operations, which\n+  are otherwise not directly exposed in Pallas.\n \n-  The decorated function may return a tree of `FragmentedArray`s.\n+  Example::\n \n-  ```\n-  layout = plgpu.Layout.WG_STRIDED(x_ref.shape, vec_size=4)\n-  @plgpu.inline_mgpu(\n-      arg_types=(plgpu.RefType(),),\n-      return_type=plgpu.GPUShapeDtypeStruct(\n-          (128, 128), dtype, layout=layout\n-      ),\n-  )\n-  def foo(ctx, smem_ref):\n-    del ctx\n-    x = mgpu.FragmentedArray.load_tiled(smem_ref, )\n-    y = mgpu.FragmentedArray.splat(\n-        mgpu.c(1, x.mlir_dtype), shape=x.shape, layout=x.layout\n-    )\n-    return (x + y)\n+      layout = plgpu.Layout.WG_STRIDED(x_ref.shape, vec_size=4)\n \n-  arr = foo(smem_ref)\n-  ```\n+      @plgpu.inline_mgpu(\n+          arg_types=(plgpu.RefType(),),\n+          return_type=plgpu.ShapeDtypeStruct(\n+              (128, 128), dtype, layout=layout\n+          ),\n+      )\n+      def add_one(ctx, smem_ref):\n+        x = mgpu.FragmentedArray.load_tiled(smem_ref)\n+        y = mgpu.FragmentedArray.splat(\n+            mgpu.c(1, x.mlir_dtype), shape=x.shape, layout=x.layout\n+        )\n+        return x + y\n \n   Args:\n-\n-    arg_types: a sequence of pytrees where the leaves are `RefType` or\n-      `Layout` for references or arrays respectively as the return\n-      type.\n-\n-    return_type: A pytree where the leaves are `GPUShapeDtypeStruct`\n-      represeinting the arrays returned by the decorated function.\n-\n-  Returns:\n-    A decorator that creates a function that inlines mgpu code.\n-\n+    arg_types: A sequence of pytrees where the leaves are\n+      {class}`~jax.experimental.pallas.mosaic_gpu.RefType`\\s or\n+      {class}`~jax.experimental.pallas.mosaic_gpu.Layout`\\s for reference or\n+      array arguments respectively.\n+    return_type: A pytree where the leaves are\n+      {class}`~jax.experimental.pallas.mosaic_gpu.ShapeDtypeStruct`\\s\n+      representing the arrays returned by the decorated function.\n   \"\"\"\n   flat_arg_types, treedef_ty = jax.tree.flatten(tuple(arg_types))\n   flat_ret_ty, pytree_ret_ty = jax.tree.flatten(return_type)\n-  if return_type and not all(isinstance(r, GPUShapeDtypeStruct) for r in flat_ret_ty):\n+  if return_type and not all(isinstance(r, ShapeDtypeStruct) for r in flat_ret_ty):\n     raise ValueError(\n-        \"inline_mgpu_p only supports GPUShapeDtypeStructx return types.\"\n+        \"inline_mgpu_p only supports plgpu.ShapeDtypeStruct return types.\"\n     )\n   if not all(isinstance(r, (Layout, ParameterizedLayout, RefType)) for r in flat_arg_types):\n     raise ValueError(\n@@ -1951,7 +1942,7 @@ def _type_check_mgpu(v, ty):\n   match (ty, v):\n     case (RefType(), ir.Value()) if ir.MemRefType.isinstance(v.type):\n       pass\n-    case (GPUShapeDtypeStruct(), mgpu.FragmentedArray()):\n+    case (ShapeDtypeStruct(), mgpu.FragmentedArray()):\n       mlir_dtype = mgpu_utils.dtype_to_ir_type(ty.dtype)\n       if v.mlir_dtype != mlir_dtype:\n         raise ValueError(\ndiff --git a/jax/experimental/pallas/mosaic_gpu.py b/jax/experimental/pallas/mosaic_gpu.py\nindex 8c7870412403..1c47d391aa65 100644\n--- a/jax/experimental/pallas/mosaic_gpu.py\n+++ b/jax/experimental/pallas/mosaic_gpu.py\n@@ -46,7 +46,7 @@\n from jax._src.pallas.mosaic_gpu.primitives import broadcasted_iota as broadcasted_iota\n from jax._src.pallas.mosaic_gpu.primitives import commit_smem as commit_smem\n from jax._src.pallas.mosaic_gpu.primitives import commit_smem_to_gmem_group as commit_smem_to_gmem_group\n-from jax._src.pallas.mosaic_gpu.primitives import GPUShapeDtypeStruct as GPUShapeDtypeStruct\n+from jax._src.pallas.mosaic_gpu.primitives import ShapeDtypeStruct as ShapeDtypeStruct\n from jax._src.pallas.mosaic_gpu.primitives import copy_gmem_to_smem as copy_gmem_to_smem\n from jax._src.pallas.mosaic_gpu.primitives import copy_smem_to_gmem as copy_smem_to_gmem\n from jax._src.pallas.mosaic_gpu.primitives import inline_mgpu as inline_mgpu\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 61a9f18ef26d..6cedbc6ae14c 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -423,7 +423,7 @@ def kernel(x_ref, o_ref, smem_ref, barrier):\n               plgpu.TransposeTransform((1, 0, 2, 3)),\n               plgpu.SwizzleTransform(128),\n           )),),\n-          return_type=plgpu.GPUShapeDtypeStruct(\n+          return_type=plgpu.ShapeDtypeStruct(\n               shape, dtype, layout=plgpu.Layout.WGMMA\n           ),\n       )\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add support for tiled loads and stores of `f8` data types.\n\nPiperOrigin-RevId: 766570588",
        "output": "```diff\nCommit: d30b176c8620ac8e9081f38e549bafa03484c53e\nDate: 2025-06-03T09:55:18Z\nURL: https://github.com/jax-ml/jax/commit/d30b176c8620ac8e9081f38e549bafa03484c53e\nFiles changed: 2\nAdditions: +57, Deletions: -4\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex 04dd30023293..7278af5d7a91 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -2063,10 +2063,18 @@ def load_tiled(\n             ),\n         )\n         registers = np.full(layout.registers_shape(shape), zero, dtype=object)\n-        reg_ty = ir.VectorType.get((layout.vector_length,), ref_ty.element_type)\n+        is_f8 = ir.FloatType.isinstance(dtype) and utils.bitwidth(dtype) == 8\n+        i8 = ir.IntegerType.get_signless(8)\n+        reg_ty = ir.VectorType.get((layout.vector_length,), dtype)\n+        # f8 data types are not handled by the LLVM dialect, so we need to\n+        # transfer them as i8 and bitcast them back to f8.\n+        transfer_ty = ir.VectorType.get((layout.vector_length,), i8 if is_f8 else dtype)\n         loads = cls.transfer_tiled2(ref, swizzle, layout, shape, optimized)\n         for _, update, ptr in loads:\n-          update(registers, llvm.load(reg_ty, ptr))\n+          loaded_reg = llvm.load(transfer_ty, ptr)\n+          if is_f8:\n+            loaded_reg = vector.bitcast(reg_ty, loaded_reg)\n+          update(registers, loaded_reg)\n       case _:\n         raise NotImplementedError(layout)\n     return cls(_registers=registers, _layout=layout, _is_signed=is_signed)\n@@ -2259,7 +2267,12 @@ def transfer_tiled2(\n     # Technically we should keep the vector_dim set to 1, but its shape is 1\n     # so it does not matter.\n     transfer_tiled_strides = [s // layout.vector_length for s in elem_tiled_strides]\n-    transfer_dtype = ir.VectorType.get((layout.vector_length,), dtype)\n+    is_f8 = ir.FloatType.isinstance(dtype) and element_bits == 8\n+    i8 = ir.IntegerType.get_signless(8)\n+    if is_f8:\n+      transfer_dtype = ir.VectorType.get((layout.vector_length,), i8)\n+    else:\n+      transfer_dtype = ir.VectorType.get((layout.vector_length,), dtype)\n \n     if ref_ty.memory_space is None:\n       llvm_memory_space = None\n@@ -2327,7 +2340,13 @@ def mem_idx_to_reg_idx(idx):\n         return (*reg_tiled_idx, *idx[base_idx:])\n       reg_idxs = [mem_idx_to_reg_idx(idx) for idx in indices.tolist()]\n       def get_register(regs, reg_idxs=reg_idxs):\n-        return plan.select([regs[reg_idx] for reg_idx in reg_idxs])\n+        def cast_if_f8(x):\n+          if is_f8:\n+            return vector.bitcast(transfer_dtype, x)\n+          return x\n+        # f8 data types are not handled by the LLVM dialect, so we need to\n+        # transfer them as i8 and bitcast them back to f8.\n+        return plan.select([cast_if_f8(regs[reg_idx]) for reg_idx in reg_idxs])\n       def update_registers(regs, new, reg_idxs=reg_idxs):\n         # TODO(apaszke): If the staggering forms a permutation with a small\n         # cycle length, then instead of blending at each step we could construct\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex a56aa04f6f60..314dc8f8f41d 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -507,6 +507,40 @@ def kernel(ctx, out, _):\n     )()\n     np.testing.assert_array_equal(iota, expected)\n \n+  @parameterized.product(\n+      dtype=[jnp.float8_e5m2fnuz, jnp.float8_e5m2, jnp.float8_e4m3b11fnuz,\n+             jnp.float8_e4m3fn, jnp.float8_e4m3fnuz],\n+      swizzle=(32, 64, 128),\n+      num_col_tiles=(1, 2, 3),\n+  )\n+  def test_load_and_store_tiled_f8(self, dtype, swizzle, num_col_tiles):\n+    # We use a different test than `test_store_tiled` because converting\n+    # `iota` to `f8` type requires additional specialized logic that is not\n+    # yet available.\n+    col_tiling = swizzle\n+    m = 128\n+    n = col_tiling * num_col_tiles\n+    tiling = (64, col_tiling)\n+    def kernel(ctx, inp, out, smem):\n+      del ctx\n+      smem_inp, smem_out = smem\n+      copy(inp, smem_inp, swizzle=swizzle)\n+      arr = mgpu.FragmentedArray.load_tiled(smem_inp, swizzle=swizzle)\n+      arr.store_tiled(smem_out, swizzle=swizzle)\n+      copy(smem_out, out, swizzle=swizzle)\n+    expected = (\n+        jax.random.randint(\n+            jax.random.key(42), (m * n,), -16, 15, dtype=jnp.int8\n+        )\n+        .reshape(m // tiling[0], tiling[0], n // tiling[1], tiling[1])\n+        .astype(dtype)\n+        .transpose(0, 2, 1, 3)\n+    )\n+    res = mgpu.as_gpu_kernel(\n+        kernel, (1, 1, 1), (128, 1, 1), expected, expected, (expected,) * 2\n+    )(expected)\n+    np.testing.assert_array_equal(res, expected)\n+\n   @parameterized.product(\n       dtype=[jnp.float32, jnp.float16, jnp.int8],\n       swizzle=(32, 64, 128),\n\n```",
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move jax/_src/custom_partitioning_sharding_rule.py to its own build rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nPiperOrigin-RevId: 766447138",
        "output": "```diff\nCommit: 41fd7a70dc48fca18dc84665d6d1c8f39fa6880f\nDate: 2025-06-03T03:03:46Z\nURL: https://github.com/jax-ml/jax/commit/41fd7a70dc48fca18dc84665d6d1c8f39fa6880f\nFiles changed: 2\nAdditions: +10, Deletions: -1\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 1ec0ddd655f2..b577abcabf5f 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -303,7 +303,6 @@ py_library_providing_imports_info(\n         \"_src/checkify.py\",\n         \"_src/custom_batching.py\",\n         \"_src/custom_partitioning.py\",\n-        \"_src/custom_partitioning_sharding_rule.py\",\n         \"_src/debugging.py\",\n         \"_src/dispatch.py\",  # TODO(vanderplas): remove this and depend on :api instead\n         \"_src/dlpack.py\",\n@@ -388,6 +387,7 @@ py_library_providing_imports_info(\n         \":custom_api_util\",\n         \":custom_dce\",\n         \":custom_derivatives\",\n+        \":custom_partitioning_sharding_rule\",\n         \":custom_transpose\",\n         \":deprecations\",\n         \":dtypes\",\n@@ -700,6 +700,14 @@ pytype_strict_library(\n     ],\n )\n \n+pytype_strict_library(\n+    name = \"custom_partitioning_sharding_rule\",\n+    srcs = [\"_src/custom_partitioning_sharding_rule.py\"],\n+    deps = [\n+        \"//jax/_src/lib\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"custom_transpose\",\n     srcs = [\"_src/custom_transpose.py\"],\ndiff --git a/tests/BUILD b/tests/BUILD\nindex 75946713a49e..faa3367aecba 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -2158,6 +2158,7 @@ jax_py_test(\n     srcs = [\"custom_partitioning_sharding_rule_test.py\"],\n     deps = [\n         \"//jax\",\n+        \"//jax:custom_partitioning_sharding_rule\",\n         \"//jax:experimental\",\n         \"//jax:test_util\",\n     ] + py_deps(\"absl/testing\"),\n\n```",
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move jax/_src/extend/* to its own build rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nPiperOrigin-RevId: 766414704",
        "output": "```diff\nCommit: 4367d7c7e94fe54cb704ed95ba2495c2233bd07f\nDate: 2025-06-03T00:54:10Z\nURL: https://github.com/jax-ml/jax/commit/4367d7c7e94fe54cb704ed95ba2495c2233bd07f\nFiles changed: 2\nAdditions: +10, Deletions: -2\ndiff --git a/jax/BUILD b/jax/BUILD\nindex aaa25db88beb..1ec0ddd655f2 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -324,7 +324,6 @@ py_library_providing_imports_info(\n             \"*.py\",\n             \"_src/cudnn/**/*.py\",\n             \"_src/debugger/**/*.py\",\n-            \"_src/extend/**/*.py\",\n             \"_src/image/**/*.py\",\n             \"_src/export/**/*.py\",\n             \"_src/lax/**/*.py\",\n@@ -1544,6 +1543,12 @@ pytype_library(\n     ],\n )\n \n+pytype_strict_library(\n+    name = \"extend_src\",\n+    srcs = glob(include = [\"_src/extend/**/*.py\"]),\n+    deps = [\":jax\"],\n+)\n+\n # TODO(phawkins): remove this target in favor of the finer-grained targets in jax/extend/...\n pytype_strict_library(\n     name = \"extend\",\ndiff --git a/jax/extend/BUILD b/jax/extend/BUILD\nindex f466f1748654..1147e6bf502f 100644\n--- a/jax/extend/BUILD\n+++ b/jax/extend/BUILD\n@@ -70,7 +70,10 @@ pytype_strict_library(\n pytype_strict_library(\n     name = \"random\",\n     srcs = [\"random.py\"],\n-    deps = [\"//jax\"],\n+    deps = [\n+        \"//jax\",\n+        \"//jax:extend_src\",\n+    ],\n )\n \n pytype_strict_library(\n\n```",
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29153 from soraros:simplify-isclose\n\nPiperOrigin-RevId: 766395465",
        "output": "```diff\nCommit: 35453396c5dbcde4fd5062e3d06f829085c199bc\nDate: 2025-06-02T23:51:00Z\nURL: https://github.com/jax-ml/jax/commit/35453396c5dbcde4fd5062e3d06f829085c199bc\nFiles changed: 2\nAdditions: +21, Deletions: -23\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex 792954dc1cc8..171a64a758ad 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -2602,31 +2602,13 @@ def isclose(a: ArrayLike, b: ArrayLike, rtol: ArrayLike = 1e-05, atol: ArrayLike\n     dtype = np.array(0, dtype).real.dtype\n   rtol = lax.convert_element_type(rtol, dtype)\n   atol = lax.convert_element_type(atol, dtype)\n-  out = lax.le(\n+  both_nan = ufuncs.logical_and(ufuncs.isnan(a), ufuncs.isnan(b))\n+  check_fin = ufuncs.isfinite(b)\n+  in_range = lax.le(\n     lax.abs(lax.sub(a, b)),\n     lax.add(atol, lax.mul(rtol, lax.abs(b))))\n-  # This corrects the comparisons for infinite and nan values\n-  a_inf = ufuncs.isinf(a)\n-  b_inf = ufuncs.isinf(b)\n-  any_inf = ufuncs.logical_or(a_inf, b_inf)\n-  both_inf = ufuncs.logical_and(a_inf, b_inf)\n-  # Make all elements where either a or b are infinite to False\n-  out = ufuncs.logical_and(out, ufuncs.logical_not(any_inf))\n-  # Make all elements where both a or b are the same inf to True\n-  same_value = lax.eq(a, b)\n-  same_inf = ufuncs.logical_and(both_inf, same_value)\n-  out = ufuncs.logical_or(out, same_inf)\n-\n-  # Make all elements where either a or b is NaN to False\n-  a_nan = ufuncs.isnan(a)\n-  b_nan = ufuncs.isnan(b)\n-  any_nan = ufuncs.logical_or(a_nan, b_nan)\n-  out = ufuncs.logical_and(out, ufuncs.logical_not(any_nan))\n-  if equal_nan:\n-    # Make all elements where both a and b is NaN to True\n-    both_nan = ufuncs.logical_and(a_nan, b_nan)\n-    out = ufuncs.logical_or(out, both_nan)\n-  return out\n+  out = ufuncs.logical_or(lax.eq(a, b), ufuncs.logical_and(check_fin, in_range))\n+  return ufuncs.logical_or(out, both_nan) if equal_nan else out\n \n \n def _interp(x: ArrayLike, xp: ArrayLike, fp: ArrayLike,\ndiff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py\nindex 80d1d4161cc5..e98ac6986cae 100644\n--- a/tests/lax_numpy_test.py\n+++ b/tests/lax_numpy_test.py\n@@ -3963,6 +3963,22 @@ def testIsClose(self):\n     key = jax.random.key(0)\n     self.assertTrue(jnp.isclose(key, key))\n \n+  @jtu.sample_product(\n+    atol=[0.0, 1E-4, np.inf],\n+    rtol=[0.0, 1E-4, np.inf],\n+    equal_nan=[True, False]\n+  )\n+  def testIsCloseCornerCases(self, atol, rtol, equal_nan):\n+    if jtu.numpy_version() < (2, 0, 0) and (np.isinf(atol) or np.isinf(rtol)):\n+      self.skipTest(\"fails on older NumPy\")\n+    vals = np.array([-np.nan, -np.inf, -1.00001, -1.0, -0.00001, -0.0,\n+                     0.0, 0.00001, 1.0, 1.00001, np.inf, np.nan])\n+    x, y = np.meshgrid(vals, vals)\n+    self.assertArraysEqual(\n+      np.isclose(x, y, atol=atol, rtol=rtol, equal_nan=equal_nan),\n+      jnp.isclose(x, y, atol=atol, rtol=rtol, equal_nan=equal_nan)\n+    )\n+\n   @jtu.sample_product(\n     x=[1, [1], [1, 1 + 1E-4], [1, np.nan]],\n     y=[1, [1], [1, 1 + 1E-4], [1, np.nan]],\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "When the size of the remainder array is 0, don't append it to the remainder_leaves list. This fixes usage of lax.map in sharding-in-types mode.\n\nPiperOrigin-RevId: 766367547",
        "output": "```diff\nCommit: 31017c559b2bde21e1e4198befaf6c23dee1eb3b\nDate: 2025-06-02T22:33:15Z\nURL: https://github.com/jax-ml/jax/commit/31017c559b2bde21e1e4198befaf6c23dee1eb3b\nFiles changed: 2\nAdditions: +38, Deletions: -17\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 47808ee3c423..146f27e5d2e7 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -2509,19 +2509,23 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n def _batch_and_remainder(x, batch_size: int):\n   leaves, treedef = tree_flatten(x)\n-\n-  scan_leaves = []\n-  remainder_leaves = []\n-\n-  for leaf in leaves:\n-    num_batches, _ = divmod(leaf.shape[0], batch_size)\n-    total_batch_elems = num_batches * batch_size\n-    scan_leaves.append(leaf[:total_batch_elems].reshape(num_batches, batch_size, *leaf.shape[1:]))\n-    remainder_leaves.append(leaf[total_batch_elems:])\n-\n-  scan_tree = treedef.unflatten(scan_leaves)\n-  remainder_tree = treedef.unflatten(remainder_leaves)\n-  return scan_tree, remainder_tree\n+  if not leaves:\n+    return x, None\n+  num_batches, remainder = divmod(leaves[0].shape[0], batch_size)\n+  total_batch_elems = num_batches * batch_size\n+  if remainder:\n+    scan_leaves, remainder_leaves = [], []\n+    for leaf in leaves:\n+      scan_leaves.append(leaf[:total_batch_elems].reshape(\n+          num_batches, batch_size, *leaf.shape[1:]))\n+      remainder_leaves.append(leaf[total_batch_elems:])\n+    return treedef.unflatten(scan_leaves), treedef.unflatten(remainder_leaves)\n+  else:\n+    scan_leaves = [\n+        leaf[:total_batch_elems].reshape(num_batches, batch_size, *leaf.shape[1:])\n+        for leaf in leaves\n+    ]\n+    return treedef.unflatten(scan_leaves), None\n \n @api_boundary\n def map(f, xs, *, batch_size: int | None = None):\n@@ -2576,11 +2580,14 @@ def map(f, xs):\n     scan_xs, remainder_xs = _batch_and_remainder(xs, batch_size)\n     g = lambda _, x: ((), api.vmap(f)(x))\n     _, scan_ys = scan(g, (), scan_xs)\n-    remainder_ys = api.vmap(f)(remainder_xs)\n     flatten = lambda x: x.reshape(-1, *x.shape[2:])\n-    ys = tree_map(\n-      lambda x, y: lax.concatenate([flatten(x), y], dimension=0), scan_ys, remainder_ys,\n-    )\n+    if remainder_xs is not None:\n+      remainder_ys = api.vmap(f)(remainder_xs)\n+      ys = tree_map(\n+        lambda x, y: lax.concatenate([flatten(x), y], dimension=0), scan_ys,\n+        remainder_ys)\n+    else:\n+      ys = tree_map(flatten, scan_ys)\n   else:\n     g = lambda _, x: ((), f(x))\n     _, ys = scan(g, (), xs)\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex dd5c5d46e62f..c4d36ab78d10 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7893,6 +7893,20 @@ def test_nn_constant(self, mesh):\n     self.assertArraysEqual(out, jnp.full((8, 2), -7, dtype=jnp.float32))\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('x', None)))\n \n+  @config.numpy_rank_promotion('allow')\n+  @jtu.with_explicit_mesh((2, 2), ('x', 'y'))\n+  def test_lax_map(self, mesh):\n+    def simple_func(w, x):\n+      return jnp.sum(w * x, axis=-1)\n+\n+    w = jax.device_put(np.arange(4, dtype=np.float32), P('x'))\n+    x = jax.device_put(np.ones((4, 2, 4), dtype=np.float32),\n+                       P(None, 'y', None))\n+\n+    jax.lax.map(lambda _x: simple_func(w, _x), x)  # doesn't crash\n+\n+    jax.lax.map(lambda _x: simple_func(w, _x), x, batch_size=2)  # doesn't crash\n+\n \n @jtu.pytest_mark_if_available('multiaccelerator')\n class PJitErrorTest(jtu.JaxTestCase):\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Clean up unused GPU RNN kernels.\n\nThese kernels aren't covered by the export compatibility policy, and their FFI counterparts have been targeted by JAX for several releases.\n\nPiperOrigin-RevId: 766351572",
        "output": "```diff\nCommit: 2f32a794717ee220dcf1dea0679c6cea1c30dd38\nDate: 2025-06-02T21:49:18Z\nURL: https://github.com/jax-ml/jax/commit/2f32a794717ee220dcf1dea0679c6cea1c30dd38\nFiles changed: 6\nAdditions: +3, Deletions: -32\ndiff --git a/jaxlib/cuda/BUILD b/jaxlib/cuda/BUILD\nindex 33ed84b08753..5cc401e14eb0 100644\n--- a/jaxlib/cuda/BUILD\n+++ b/jaxlib/cuda/BUILD\n@@ -113,7 +113,6 @@ cc_library(\n         \"@com_google_absl//absl/synchronization\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@xla//xla/ffi/api:ffi\",\n-        \"@xla//xla/service:custom_call_status\",\n         \"@xla//xla/tsl/cuda:cudart\",\n         \"@xla//xla/tsl/cuda:cudnn\",\n     ],\ndiff --git a/jaxlib/gpu/gpu_kernels.cc b/jaxlib/gpu/gpu_kernels.cc\nindex 8428562e3248..1f6e5f75315d 100644\n--- a/jaxlib/gpu/gpu_kernels.cc\n+++ b/jaxlib/gpu/gpu_kernels.cc\n@@ -31,8 +31,9 @@ namespace jax {\n namespace JAX_GPU_NAMESPACE {\n namespace {\n \n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cudnn_rnn\", RNNForward, \"CUDA\");\n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cudnn_rnn_bwd\", RNNBackward, \"CUDA\");\n+XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cudnn_rnn\", \"CUDA\", RNNForwardFfi);\n+XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cudnn_rnn_bwd\", \"CUDA\",\n+                         RNNBackwardFfi);\n XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusolver_getrf_ffi\", \"CUDA\",\n                          GetrfFfi);\n XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusolver_syrk_ffi\", \"CUDA\",\ndiff --git a/jaxlib/gpu/rnn.cc b/jaxlib/gpu/rnn.cc\nindex 32e0842e3038..c235aa9fecfb 100644\n--- a/jaxlib/gpu/rnn.cc\n+++ b/jaxlib/gpu/rnn.cc\n@@ -39,8 +39,6 @@ nb::bytes BuildRnnDescriptor(int input_size, int hidden_size, int num_layers,\n \n nb::dict Registrations() {\n   nb::dict dict;\n-  dict[JAX_GPU_PREFIX \"dnn_rnn\"] = EncapsulateFunction(RNNForward);\n-  dict[JAX_GPU_PREFIX \"dnn_rnn_bwd\"] = EncapsulateFunction(RNNBackward);\n   dict[JAX_GPU_PREFIX \"dnn_rnn_ffi\"] = EncapsulateFfiHandler(RNNForwardFfi);\n   dict[JAX_GPU_PREFIX \"dnn_rnn_bwd_ffi\"] =\n       EncapsulateFfiHandler(RNNBackwardFfi);\ndiff --git a/jaxlib/gpu/rnn_kernels.cc b/jaxlib/gpu/rnn_kernels.cc\nindex d06535a668ac..44864d6a2663 100644\n--- a/jaxlib/gpu/rnn_kernels.cc\n+++ b/jaxlib/gpu/rnn_kernels.cc\n@@ -30,7 +30,6 @@ limitations under the License.\n #include \"jaxlib/gpu/handle_pool.h\"\n #include \"jaxlib/gpu/vendor.h\"\n #include \"jaxlib/kernel_helpers.h\"\n-#include \"xla/service/custom_call_status.h\"\n \n namespace jax {\n \n@@ -541,24 +540,6 @@ static absl::Status DnnRNNBackward_(gpuStream_t stream, void** buffers,\n   return absl::OkStatus();\n }\n \n-void RNNForward(gpuStream_t stream, void** buffers, const char* opaque,\n-                size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = DnnRNNForward_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n-void RNNBackward(gpuStream_t stream, void** buffers, const char* opaque,\n-                 size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = DnnRNNBackward_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n JAX_GPU_REGISTER_WRAPPED_LEGACY_KERNEL(RNNForwardFfi, DnnRNNForward_);\n JAX_GPU_REGISTER_WRAPPED_LEGACY_KERNEL(RNNBackwardFfi, DnnRNNBackward_);\n \ndiff --git a/jaxlib/gpu/rnn_kernels.h b/jaxlib/gpu/rnn_kernels.h\nindex 36d8c25c6a9f..c1d6712a9eac 100644\n--- a/jaxlib/gpu/rnn_kernels.h\n+++ b/jaxlib/gpu/rnn_kernels.h\n@@ -22,7 +22,6 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"jaxlib/gpu/vendor.h\"\n #include \"xla/ffi/api/ffi.h\"\n-#include \"xla/service/custom_call_status.h\"\n \n namespace jax {\n namespace JAX_GPU_NAMESPACE {\n@@ -47,12 +46,6 @@ absl::StatusOr<std::pair<size_t, size_t>> RnnComputeWorkspaceReserveSpaceSizes(\n     int max_seq_length, float dropout, bool bidirectional,\n     bool cudnn_allow_tf32);\n \n-void RNNForward(gpuStream_t stream, void **buffers, const char *opaque,\n-                size_t opaque_len, XlaCustomCallStatus *status);\n-\n-void RNNBackward(gpuStream_t stream, void **buffers, const char *opaque,\n-                 size_t opaque_len, XlaCustomCallStatus *status);\n-\n XLA_FFI_DECLARE_HANDLER_SYMBOL(RNNForwardFfi);\n XLA_FFI_DECLARE_HANDLER_SYMBOL(RNNBackwardFfi);\n \ndiff --git a/jaxlib/rocm/BUILD b/jaxlib/rocm/BUILD\nindex a24a1617d309..f265e6714c8e 100644\n--- a/jaxlib/rocm/BUILD\n+++ b/jaxlib/rocm/BUILD\n@@ -104,7 +104,6 @@ cc_library(\n         \"@local_config_rocm//rocm:miopen\",\n         \"@local_config_rocm//rocm:rocm_headers\",\n         \"@xla//xla/ffi/api:ffi\",\n-        \"@xla//xla/service:custom_call_status\",\n     ],\n )\n \n\n```",
        "from_id": [
            "dfm",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add reduction support for TCGEN05 layout.\n\nPiperOrigin-RevId: 766350008",
        "output": "```diff\nCommit: 3ede957e587695dd0f5a1e7feff70ab8f4f5a8a1\nDate: 2025-06-02T21:44:50Z\nURL: https://github.com/jax-ml/jax/commit/3ede957e587695dd0f5a1e7feff70ab8f4f5a8a1\nFiles changed: 2\nAdditions: +38, Deletions: -2\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex ce74a5ba7c05..5695da4cc8b1 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -2035,7 +2035,7 @@ def _reduce_sum_lowering_rule(ctx: LoweringRuleContext, x, *, axes):\n       scratch_ty = jax.ShapeDtypeStruct(shape=(4,), dtype=x_aval.dtype)\n       with ctx.module_ctx.scratch_view([scratch_ty]) as [scratch]:\n         return x.reduce(\"add\", axes, scratch)\n-    case mgpu.WGMMA_LAYOUT:\n+    case mgpu.TiledLayout():\n       if axes != (x_aval.ndim - 1,):\n         raise NotImplementedError\n       if not jnp.issubdtype(x_aval.dtype, jnp.floating):\n@@ -2049,7 +2049,7 @@ def _reduce_sum_lowering_rule(ctx: LoweringRuleContext, x, *, axes):\n def _reduce_max_lowering_rule(ctx: LoweringRuleContext, x, *, axes):\n   [x_aval] = ctx.avals_in\n   match x.layout:\n-    case mgpu.WGMMA_LAYOUT:\n+    case mgpu.TiledLayout():\n       if axes != (x_aval.ndim - 1,):\n         raise NotImplementedError\n       if not jnp.issubdtype(x_aval.dtype, jnp.floating):\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex f7bc83fc460d..61a9f18ef26d 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -2368,6 +2368,42 @@ def kernel(x_ref, y_ref, tmem_ref, tmem_ref2, smem_ref, barrier_ref):\n     x_result = jax.block_until_ready(kernel(x))\n     np.testing.assert_array_equal(x_result, x + 1)\n \n+  @parameterized.parameters(\n+      (jnp.sum,),\n+      (jnp.max,)\n+  )\n+  def test_reduce_with_tcgen05_layout(self, op):\n+    axis = -1\n+    swizzle_elems = 128 // jnp.dtype(jnp.float32).itemsize\n+    transforms = (\n+        plgpu.TilingTransform((8, swizzle_elems)),\n+        plgpu.SwizzleTransform(128),\n+    )\n+    @functools.partial(\n+        self.kernel,\n+        out_shape=jnp.zeros((128,), jnp.float32),\n+        scratch_shapes=[\n+            plgpu.SMEM((128, 128), jnp.float32, transforms=transforms),\n+            plgpu.SMEM((128,), jnp.float32),\n+            plgpu.Barrier(),\n+        ],\n+        num_threads=1,\n+        thread_name=\"x\",\n+    )\n+    def kernel(x_ref, y_ref, smem_ref, smem_reduced_ref, barrier_ref):\n+      plgpu.copy_gmem_to_smem(x_ref, smem_ref, barrier_ref)\n+      plgpu.barrier_wait(barrier_ref)\n+      x_val = plgpu.load(smem_ref, (), layout=plgpu.Layout.TCGEN05)\n+      smem_reduced_ref[...] = op(x_val, axis=axis)\n+      plgpu.commit_smem()\n+      plgpu.copy_smem_to_gmem(smem_reduced_ref, y_ref)\n+      plgpu.wait_smem_to_gmem(0)\n+\n+    x = jax.random.uniform(\n+        jax.random.key(0), shape=(128, 128), dtype=jnp.float32)\n+    x_result = jax.block_until_ready(kernel(x))\n+    np.testing.assert_allclose(x_result, op(x, axis=axis), atol=1e-5)\n+\n   @parameterized.product(shape=[(128, 128)],\n                          swizzle=[128, 64, 32],\n                          dtype=[jnp.float16, jnp.bfloat16],\n\n```",
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] `plgpu.nd_loop` is now a decorator similar to `pl.loop`\n\nThe name was too similar to `pl.loop`, so having a different calling convention\nwas confusing.\n\nPiperOrigin-RevId: 766330213",
        "output": "```diff\nCommit: 81de911a86655551f43b352b8106530dab763407\nDate: 2025-06-02T20:55:49Z\nURL: https://github.com/jax-ml/jax/commit/81de911a86655551f43b352b8106530dab763407\nFiles changed: 3\nAdditions: +26, Deletions: -28\ndiff --git a/jax/_src/pallas/mosaic_gpu/helpers.py b/jax/_src/pallas/mosaic_gpu/helpers.py\nindex 54c4910059d5..939f3d0382e7 100644\n--- a/jax/_src/pallas/mosaic_gpu/helpers.py\n+++ b/jax/_src/pallas/mosaic_gpu/helpers.py\n@@ -26,11 +26,9 @@\n \n def nd_loop(\n     grid: Sequence[int],\n-    body: Callable[[Sequence[jax.Array], _T], _T],\n-    init_val: _T,\n     *,\n     collective_axes: Sequence[Hashable] | Hashable,\n-) -> _T:\n+) -> Callable[[Callable[[Sequence[jax.Array]], None]], None]:\n   \"\"\"A loop over a multi-dimensional grid partitioned along the given axes.\n \n   For example, if ``collective_axes`` is ``\"x\"`` with :func:`lax.axis_size`\n@@ -61,26 +59,29 @@ def nd_loop(\n           3         (1, 0)\n \n   See also:\n-    - :func:`jax.lax.fori_loop`: A single-dimensional indexed loop.\n+    - :func:`jax.experimental.pallas.loop`: A loop over a single dimension.\n   \"\"\"\n   axis_index = lax.axis_index(collective_axes)\n   axis_size = lax.axis_size(collective_axes)\n   grid_size = math.prod(grid)\n \n-  def wrapper(step, carry):\n-    step = step * axis_size + axis_index\n-    # The loop below is conceptually ``jnp.unravel_index``, but it uses\n-    # ``lax`` APIs instead of ``jax.numpy`` to minimize the number of\n-    # primitives used.\n-    index = []\n-    for grid_dim in reversed(grid):\n-      grid_dim = lax.convert_element_type(grid_dim, step.dtype)\n-      index.append(lax.rem(step, grid_dim))\n-      step = lax.div(step, grid_dim)\n-    index.reverse()\n-    return body(tuple(index), carry)\n-\n-  upper = lax.div(grid_size, axis_size) + lax.convert_element_type(\n-      axis_index < grid_size % axis_size, axis_index.dtype\n-  )\n-  return lax.fori_loop(0, upper, wrapper, init_val)\n+  def decorator(body):\n+    def wrapper(step, _):\n+      step = step * axis_size + axis_index\n+      # The loop below is conceptually ``jnp.unravel_index``, but it uses\n+      # ``lax`` APIs instead of ``jax.numpy`` to minimize the number of\n+      # primitives used.\n+      index = []\n+      for grid_dim in reversed(grid):\n+        grid_dim = lax.convert_element_type(grid_dim, step.dtype)\n+        index.append(lax.rem(step, grid_dim))\n+        step = lax.div(step, grid_dim)\n+      index.reverse()\n+      return body(tuple(index))\n+\n+    upper = lax.div(grid_size, axis_size) + lax.convert_element_type(\n+        axis_index < grid_size % axis_size, axis_index.dtype\n+    )\n+    return lax.fori_loop(0, upper, wrapper, None)\n+\n+  return decorator\ndiff --git a/jax/experimental/pallas/ops/gpu/ragged_dot_mgpu.py b/jax/experimental/pallas/ops/gpu/ragged_dot_mgpu.py\nindex 6d295a36f435..9a1514b9827c 100644\n--- a/jax/experimental/pallas/ops/gpu/ragged_dot_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/ragged_dot_mgpu.py\n@@ -140,10 +140,8 @@ def body(rows_per_expert_gmem, lhs_gmem, rhs_gmem, o_gmem):\n         pl.cdiv(n, grid_block_n * block_n),\n     )\n \n-    @functools.partial(\n-        plgpu.nd_loop, grid, init_val=None, collective_axes=\"sm\"\n-    )\n-    def mn_loop(idx, _):  # pylint: disable=unused-variable\n+    @plgpu.nd_loop(grid, collective_axes=\"sm\")\n+    def mn_loop(idx):  # pylint: disable=unused-variable\n       block_ni, mi, remainder_ni = idx\n       ni = block_ni * pl.cdiv(n, block_n * grid_block_n) + remainder_ni\n       group_info = GroupInfo.create(rows_per_expert_gmem, block_m, mi)\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex f9a23e7be9d0..f7bc83fc460d 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1747,7 +1747,8 @@ def test_nd_loop(self, sm_steps):\n         grid_names=(\"sm\",),\n     )\n     def kernel(o_ref):\n-      def body(idx, _):\n+      @plgpu.nd_loop((sm_steps, 4, 33), collective_axes=\"sm\")\n+      def _(idx):\n         assert len(idx) == 3\n         # We need to use `mode=\"clip\"`, because the indices are not static.\n         flat_idx = jnp.ravel_multi_index(idx, (sm_steps, 4, 33), mode=\"clip\")\n@@ -1758,8 +1759,6 @@ def body(idx, _):\n             flat_idx, o_ref.shape[-1:]\n         )\n \n-      plgpu.nd_loop((sm_steps, 4, 33), body, None, collective_axes=\"sm\")\n-\n     result = kernel()\n     for sm_step in range(sm_steps):\n       np.testing.assert_array_equal(\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] Added a note on `pl.loop` to the changelog\n\nPiperOrigin-RevId: 766324349",
        "output": "```diff\nCommit: 9e4ff925bb9f562a2e5be9d82499ac276892734a\nDate: 2025-06-02T20:39:33Z\nURL: https://github.com/jax-ml/jax/commit/9e4ff925bb9f562a2e5be9d82499ac276892734a\nFiles changed: 1\nAdditions: +5, Deletions: -0\ndiff --git a/docs/pallas/CHANGELOG.md b/docs/pallas/CHANGELOG.md\nindex 40a30057354d..5c916c66ed86 100644\n--- a/docs/pallas/CHANGELOG.md\n+++ b/docs/pallas/CHANGELOG.md\n@@ -13,6 +13,11 @@ Remember to align the itemized text with the first line of an item within a list\n \n ## Unreleased\n \n+* New functionality\n+\n+  * Added a new decorator {func}`jax.experimental.pallas.loop` which allows\n+    to write stateless loops as functions.\n+\n * Deprecations\n \n   * {class}`jax.experimental.pallas.triton.TritonCompilerParams` has been\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Maintain the dtype of the input on the output in `broadcast_one_to_all`.\n\nPiperOrigin-RevId: 766317087",
        "output": "```diff\nCommit: 94037a8186a956deb7d46b20ee6554d359bc9ef0\nDate: 2025-06-02T20:23:44Z\nURL: https://github.com/jax-ml/jax/commit/94037a8186a956deb7d46b20ee6554d359bc9ef0\nFiles changed: 2\nAdditions: +9, Deletions: -2\ndiff --git a/jax/experimental/multihost_utils.py b/jax/experimental/multihost_utils.py\nindex 7be349f0fc8f..3a83ff16d612 100644\n--- a/jax/experimental/multihost_utils.py\n+++ b/jax/experimental/multihost_utils.py\n@@ -39,8 +39,8 @@\n import numpy as np\n \n \n-def _psum(x: Any) -> Any:\n-  return jax.tree.map(partial(jnp.sum, axis=0), x)\n+def _psum(xs: Any) -> Any:\n+  return jax.tree.map(lambda x: jnp.sum(x, dtype=x.dtype, axis=0), xs)\n \n \n def broadcast_one_to_all(in_tree: Any, is_source: bool | None = None) -> Any:\ndiff --git a/tests/array_test.py b/tests/array_test.py\nindex 10a17b557dea..44734e64a995 100644\n--- a/tests/array_test.py\n+++ b/tests/array_test.py\n@@ -712,6 +712,13 @@ def test_process_allgather_single_host(self):\n     self.assertEqual(out.shape, (1, x.shape[0]))\n     self.assertArraysEqual(out, np.expand_dims(x, axis=0))\n \n+  def test_broadcast_one_to_all_single_host(self):\n+    x = jnp.arange(8, dtype=jnp.uint8)\n+    out = multihost_utils.broadcast_one_to_all(x)\n+    self.assertEqual(out.shape, x.shape)\n+    self.assertEqual(out.dtype, x.dtype)\n+    self.assertArraysEqual(out, x)\n+\n   @jtu.sample_product(\n     dtype=jtu.dtypes.all,\n     shape=[(), (10), (2, 3)],\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Clean up some unused GPU sparse kernels.\n\nThese kernels aren't covered by the export compatibility policy, and their FFI counterparts have been targeted by JAX for several releases.\n\nPiperOrigin-RevId: 766310963",
        "output": "```diff\nCommit: 6f0c2a8d644a4ae4ee3cbade66ddaee2ac6b32a5\nDate: 2025-06-02T20:07:53Z\nURL: https://github.com/jax-ml/jax/commit/6f0c2a8d644a4ae4ee3cbade66ddaee2ac6b32a5\nFiles changed: 6\nAdditions: +22, Deletions: -362\ndiff --git a/jaxlib/cuda/BUILD b/jaxlib/cuda/BUILD\nindex 7bcb526e6e38..33ed84b08753 100644\n--- a/jaxlib/cuda/BUILD\n+++ b/jaxlib/cuda/BUILD\n@@ -244,7 +244,6 @@ cc_library(\n         \"@com_google_absl//absl/synchronization\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@xla//xla/ffi/api:ffi\",\n-        \"@xla//xla/service:custom_call_status\",\n         \"@xla//xla/tsl/cuda:cudart\",\n         \"@xla//xla/tsl/cuda:cusparse\",\n     ],\n@@ -264,7 +263,6 @@ nanobind_extension(\n         \":cuda_vendor\",\n         \":cusparse_kernels\",\n         \"//jaxlib:absl_status_casters\",\n-        \"//jaxlib:kernel_helpers\",\n         \"//jaxlib:kernel_nanobind_helpers\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base\",\n@@ -272,13 +270,11 @@ nanobind_extension(\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/hash\",\n         \"@com_google_absl//absl/memory\",\n-        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@nanobind\",\n-        \"@xla//xla/service:custom_call_status\",\n         \"@xla//xla/tsl/cuda:cudart\",\n         \"@xla//xla/tsl/cuda:cusparse\",\n         \"@xla//xla/tsl/python/lib/core:numpy\",\n@@ -354,7 +350,6 @@ cc_library(\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@xla//xla/ffi/api:c_api\",\n         \"@xla//xla/ffi/api:ffi\",\n-        \"@xla//xla/service:custom_call_status\",\n     ],\n )\n \n@@ -370,7 +365,6 @@ cuda_library(\n         \"//jaxlib:kernel_helpers\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@xla//xla/ffi/api:ffi\",\n-        \"@xla//xla/service:custom_call_status\",\n     ],\n )\n \ndiff --git a/jaxlib/gpu/gpu_kernels.cc b/jaxlib/gpu/gpu_kernels.cc\nindex 3204053b8822..8428562e3248 100644\n--- a/jaxlib/gpu/gpu_kernels.cc\n+++ b/jaxlib/gpu/gpu_kernels.cc\n@@ -59,28 +59,26 @@ XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cu_lu_pivots_to_permutation\",\n XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cu_threefry2x32_ffi\", \"CUDA\",\n                          ThreeFry2x32Ffi);\n \n-#if JAX_CUSPARSE_11300\n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cusparse_csr_todense\", CsrToDense,\n-                                         \"CUDA\");\n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cusparse_csr_fromdense\", CsrFromDense,\n-                                         \"CUDA\");\n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cusparse_csr_matvec\", CsrMatvec,\n-                                         \"CUDA\");\n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cusparse_csr_matmat\", CsrMatmat,\n-                                         \"CUDA\");\n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cusparse_coo_todense\", CooToDense,\n-                                         \"CUDA\");\n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cusparse_coo_fromdense\", CooFromDense,\n-                                         \"CUDA\");\n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cusparse_coo_matvec\", CooMatvec,\n-                                         \"CUDA\");\n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cusparse_coo_matmat\", CooMatmat,\n-                                         \"CUDA\");\n+#if JAX_GPU_HAVE_SPARSE\n+XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusparse_csr_todense_ffi\", \"CUDA\",\n+                         CsrToDenseFfi);\n+XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusparse_csr_fromdense_ffi\", \"CUDA\",\n+                         CsrFromDenseFfi);\n+XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusparse_csr_matvec_ffi\", \"CUDA\",\n+                         CsrMatvecFfi);\n+XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusparse_csr_matmat_ffi\", \"CUDA\",\n+                         CsrMatmatFfi);\n+XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusparse_coo_todense_ffi\", \"CUDA\",\n+                         CooToDenseFfi);\n+XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusparse_coo_fromdense_ffi\", \"CUDA\",\n+                         CooFromDenseFfi);\n+XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusparse_coo_matvec_ffi\", \"CUDA\",\n+                         CooMatvecFfi);\n+XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusparse_coo_matmat_ffi\", \"CUDA\",\n+                         CooMatmatFfi);\n #endif\n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cusparse_gtsv2_f32\", gtsv2_f32,\n-                                         \"CUDA\");\n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cusparse_gtsv2_f64\", gtsv2_f64,\n-                                         \"CUDA\");\n+XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusparse_gtsv2_ffi\", \"CUDA\",\n+                         kGtsv2);\n \n XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"triton_kernel_call\", TritonKernelCall,\n                                          \"CUDA\");\ndiff --git a/jaxlib/gpu/sparse.cc b/jaxlib/gpu/sparse.cc\nindex 0190ba776de5..21f567e79f92 100644\n--- a/jaxlib/gpu/sparse.cc\n+++ b/jaxlib/gpu/sparse.cc\n@@ -15,11 +15,9 @@ limitations under the License.\n \n #include <cstddef>\n #include <stdexcept>\n-#include <string>\n #include <utility>\n \n #include \"absl/container/flat_hash_map.h\"\n-#include \"absl/status/status.h\"\n #include \"absl/strings/str_format.h\"\n #include \"nanobind/nanobind.h\"\n #include \"nanobind/stl/pair.h\"  // IWYU pragma: keep\n@@ -27,9 +25,7 @@ limitations under the License.\n #include \"jaxlib/gpu/gpu_kernel_helpers.h\"\n #include \"jaxlib/gpu/sparse_kernels.h\"\n #include \"jaxlib/gpu/vendor.h\"\n-#include \"jaxlib/kernel_helpers.h\"\n #include \"jaxlib/kernel_nanobind_helpers.h\"\n-#include \"xla/service/custom_call_status.h\"\n #include \"xla/tsl/python/lib/core/numpy.h\"\n \n namespace nb = nanobind;\n@@ -146,45 +142,6 @@ std::pair<size_t, nb::bytes> BuildCsrToDenseDescriptor(const dtype& data_dtype,\n   return {buffer_size, PackDescriptor(d)};\n }\n \n-absl::Status CsrToDense_(gpuStream_t stream, void** buffers, const char* opaque,\n-                         size_t opaque_len) {\n-  auto s = UnpackDescriptor<SparseMatDescriptor>(opaque, opaque_len);\n-  JAX_RETURN_IF_ERROR(s.status());\n-  const SparseMatDescriptor& d = **s;\n-  auto h = SparseHandlePool::Borrow(stream);\n-  JAX_RETURN_IF_ERROR(h.status());\n-  auto& handle = *h;\n-\n-  gpusparseSpMatDescr_t mat_a = 0;\n-  gpusparseDnMatDescr_t mat_b = 0;\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(\n-      gpusparseCreateCsr(&mat_a, d.rows, d.cols, d.nnz,\n-                         /*csrRowOffsets=*/buffers[2],\n-                         /*csrColInd=*/buffers[1],\n-                         /*csrValues=*/buffers[0], d.index_type, d.index_type,\n-                         GPUSPARSE_INDEX_BASE_ZERO, d.value_type)));\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpusparseCreateDnMat(\n-      &mat_b, d.rows, d.cols,\n-      /*ld=*/d.cols, buffers[3], d.value_type, GPUSPARSE_ORDER_ROW)));\n-\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(\n-      gpusparseSparseToDense(handle.get(), mat_a, mat_b,\n-                             GPUSPARSE_SPARSETODENSE_ALG_DEFAULT, buffers[4])));\n-\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpusparseDestroySpMat(mat_a)));\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpusparseDestroyDnMat(mat_b)));\n-  return absl::OkStatus();\n-}\n-\n-void CsrToDense(gpuStream_t stream, void** buffers, const char* opaque,\n-                size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = CsrToDense_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n // CsrFromDense: Convert dense matrix to CSR matrix\n \n // Returns the descriptor for a CsrFromDense operation.\n@@ -221,46 +178,6 @@ std::pair<size_t, nb::bytes> BuildCsrFromDenseDescriptor(\n   return {buffer_size, PackDescriptor(d)};\n }\n \n-absl::Status CsrFromDense_(gpuStream_t stream, void** buffers,\n-                           const char* opaque, size_t opaque_len) {\n-  auto s = UnpackDescriptor<SparseMatDescriptor>(opaque, opaque_len);\n-  JAX_RETURN_IF_ERROR(s.status());\n-  const SparseMatDescriptor& d = **s;\n-  auto h = SparseHandlePool::Borrow(stream);\n-  JAX_RETURN_IF_ERROR(h.status());\n-  auto& handle = *h;\n-\n-  gpusparseDnMatDescr_t mat_a = 0;\n-  gpusparseSpMatDescr_t mat_b = 0;\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpusparseCreateDnMat(\n-      &mat_a, d.rows, d.cols,\n-      /*ld=*/d.cols, buffers[0], d.value_type, GPUSPARSE_ORDER_ROW)));\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(\n-      gpusparseCreateCsr(&mat_b, d.rows, d.cols, d.nnz,\n-                         /*csrRowOffsets=*/buffers[3],\n-                         /*csrColInd=*/buffers[2],\n-                         /*csrValues=*/buffers[1], d.index_type, d.index_type,\n-                         GPUSPARSE_INDEX_BASE_ZERO, d.value_type)));\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpusparseDenseToSparse_analysis(\n-      handle.get(), mat_a, mat_b, GPUSPARSE_DENSETOSPARSE_ALG_DEFAULT,\n-      buffers[4])));\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpusparseDenseToSparse_convert(\n-      handle.get(), mat_a, mat_b, GPUSPARSE_DENSETOSPARSE_ALG_DEFAULT,\n-      buffers[4])));\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpusparseDestroyDnMat(mat_a)));\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpusparseDestroySpMat(mat_b)));\n-  return absl::OkStatus();\n-}\n-\n-void CsrFromDense(gpuStream_t stream, void** buffers, const char* opaque,\n-                  size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = CsrFromDense_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n // CsrMatvec: Product of CSR matrix and dense vector.\n \n // Returns the descriptor for a CsrMatvec operation.\n@@ -553,44 +470,9 @@ std::pair<size_t, nb::bytes> BuildCooMatmatDescriptor(\n \n #endif  // if JAX_GPU_HAVE_SPARSE\n \n-nb::bytes BuildGtsv2Descriptor(int b, int m, int n, int ldb) {\n-  return PackDescriptor(Gtsv2Descriptor{b, m, n, ldb});\n-}\n-\n-template <typename F>\n-size_t Gtsv2BufferSize(F f, int m, int n, int ldb) {\n-  auto h = SparseHandlePool::Borrow(/*stream=*/nullptr);\n-  JAX_THROW_IF_ERROR(h.status());\n-  auto& handle = *h;\n-  size_t size;\n-  JAX_THROW_IF_ERROR(\n-      JAX_AS_STATUS(f(handle.get(), m, n, /*dl=*/nullptr, /*d=*/nullptr,\n-                      /*du=*/nullptr, /*B=*/nullptr, ldb, &size)));\n-  return size;\n-}\n-\n-size_t Gtsv2BufferSizeF32(int m, int n, int ldb) {\n-  return Gtsv2BufferSize(gpusparseSgtsv2_bufferSizeExt, m, n, ldb);\n-}\n-\n-size_t Gtsv2BufferSizeF64(int m, int n, int ldb) {\n-  return Gtsv2BufferSize(gpusparseDgtsv2_bufferSizeExt, m, n, ldb);\n-}\n-\n nb::dict Registrations() {\n   nb::dict dict;\n #if JAX_GPU_HAVE_SPARSE\n-  dict[JAX_GPU_PREFIX \"sparse_csr_todense\"] = EncapsulateFunction(CsrToDense);\n-  dict[JAX_GPU_PREFIX \"sparse_csr_fromdense\"] =\n-      EncapsulateFunction(CsrFromDense);\n-  dict[JAX_GPU_PREFIX \"sparse_csr_matvec\"] = EncapsulateFunction(CsrMatvec);\n-  dict[JAX_GPU_PREFIX \"sparse_csr_matmat\"] = EncapsulateFunction(CsrMatmat);\n-  dict[JAX_GPU_PREFIX \"sparse_coo_todense\"] = EncapsulateFunction(CooToDense);\n-  dict[JAX_GPU_PREFIX \"sparse_coo_fromdense\"] =\n-      EncapsulateFunction(CooFromDense);\n-  dict[JAX_GPU_PREFIX \"sparse_coo_matvec\"] = EncapsulateFunction(CooMatvec);\n-  dict[JAX_GPU_PREFIX \"sparse_coo_matmat\"] = EncapsulateFunction(CooMatmat);\n-\n   dict[JAX_GPU_PREFIX \"sparse_csr_todense_ffi\"] =\n       EncapsulateFfiHandler(CsrToDenseFfi);\n   dict[JAX_GPU_PREFIX \"sparse_csr_fromdense_ffi\"] =\n@@ -608,12 +490,6 @@ nb::dict Registrations() {\n   dict[JAX_GPU_PREFIX \"sparse_coo_matmat_ffi\"] =\n       EncapsulateFfiHandler(CooMatmatFfi);\n #endif\n-  dict[JAX_GPU_PREFIX \"sparse_gtsv2_f32\"] = EncapsulateFunction(gtsv2_f32);\n-  dict[JAX_GPU_PREFIX \"sparse_gtsv2_f64\"] = EncapsulateFunction(gtsv2_f64);\n-  dict[JAX_GPU_PREFIX \"sparse_gtsv2_f32_ffi\"] =\n-      EncapsulateFfiHandler(gtsv2_f32_ffi);\n-  dict[JAX_GPU_PREFIX \"sparse_gtsv2_f64_ffi\"] =\n-      EncapsulateFfiHandler(gtsv2_f64_ffi);\n   dict[JAX_GPU_PREFIX \"sparse_gtsv2_ffi\"] = EncapsulateFfiHandler(kGtsv2);\n \n   // TODO(tomhennigan): Add support for gtsv2 complex 32/64.\n@@ -634,9 +510,6 @@ NB_MODULE(_sparse, m) {\n   m.def(\"build_coo_matvec_descriptor\", &BuildCooMatvecDescriptor);\n   m.def(\"build_coo_matmat_descriptor\", &BuildCooMatmatDescriptor);\n #endif\n-  m.def(\"gtsv2_f32_buffer_size\", &Gtsv2BufferSizeF32);\n-  m.def(\"gtsv2_f64_buffer_size\", &Gtsv2BufferSizeF64);\n-  m.def(\"build_gtsv2_descriptor\", &BuildGtsv2Descriptor);\n }\n \n }  // namespace\ndiff --git a/jaxlib/gpu/sparse_kernels.cc b/jaxlib/gpu/sparse_kernels.cc\nindex 363321e3ca8b..139fbc73f8ce 100644\n--- a/jaxlib/gpu/sparse_kernels.cc\n+++ b/jaxlib/gpu/sparse_kernels.cc\n@@ -32,7 +32,6 @@ limitations under the License.\n #include \"jaxlib/gpu/vendor.h\"\n #include \"jaxlib/kernel_helpers.h\"\n #include \"xla/ffi/api/ffi.h\"\n-#include \"xla/service/custom_call_status.h\"\n \n #define JAX_FFI_RETURN_IF_GPU_ERROR(...) \\\n   FFI_RETURN_IF_ERROR_STATUS(JAX_AS_STATUS(__VA_ARGS__))\n@@ -189,15 +188,6 @@ static absl::Status CsrToDense_(gpuStream_t stream, void** buffers,\n \n JAX_GPU_REGISTER_WRAPPED_LEGACY_KERNEL(CsrToDenseFfi, CsrToDense_);\n \n-void CsrToDense(gpuStream_t stream, void** buffers, const char* opaque,\n-                size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = CsrToDense_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n // CsrFromDense: Convert dense matrix to CSR matrix\n \n static absl::Status CsrFromDense_(gpuStream_t stream, void** buffers,\n@@ -233,15 +223,6 @@ static absl::Status CsrFromDense_(gpuStream_t stream, void** buffers,\n \n JAX_GPU_REGISTER_WRAPPED_LEGACY_KERNEL(CsrFromDenseFfi, CsrFromDense_);\n \n-void CsrFromDense(gpuStream_t stream, void** buffers, const char* opaque,\n-                  size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = CsrFromDense_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n // CsrMatvec: Product of CSR matrix and dense vector.\n \n static absl::Status CsrMatvec_(gpuStream_t stream, void** buffers,\n@@ -292,15 +273,6 @@ static absl::Status CsrMatvec_(gpuStream_t stream, void** buffers,\n \n JAX_GPU_REGISTER_WRAPPED_LEGACY_KERNEL(CsrMatvecFfi, CsrMatvec_);\n \n-void CsrMatvec(gpuStream_t stream, void** buffers, const char* opaque,\n-               size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = CsrMatvec_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n // CsrMatmat: Product of CSR matrix and dense matrix.\n \n static absl::Status CsrMatmat_(gpuStream_t stream, void** buffers,\n@@ -352,15 +324,6 @@ static absl::Status CsrMatmat_(gpuStream_t stream, void** buffers,\n \n JAX_GPU_REGISTER_WRAPPED_LEGACY_KERNEL(CsrMatmatFfi, CsrMatmat_);\n \n-void CsrMatmat(gpuStream_t stream, void** buffers, const char* opaque,\n-               size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = CsrMatmat_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n // CooToDense: Convert COO matrix to dense matrix\n \n static absl::Status CooToDense_(gpuStream_t stream, void** buffers,\n@@ -395,15 +358,6 @@ static absl::Status CooToDense_(gpuStream_t stream, void** buffers,\n \n JAX_GPU_REGISTER_WRAPPED_LEGACY_KERNEL(CooToDenseFfi, CooToDense_);\n \n-void CooToDense(gpuStream_t stream, void** buffers, const char* opaque,\n-                size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = CooToDense_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n // CooFromDense: Convert dense matrix to COO matrix\n \n static absl::Status CooFromDense_(gpuStream_t stream, void** buffers,\n@@ -439,15 +393,6 @@ static absl::Status CooFromDense_(gpuStream_t stream, void** buffers,\n \n JAX_GPU_REGISTER_WRAPPED_LEGACY_KERNEL(CooFromDenseFfi, CooFromDense_);\n \n-void CooFromDense(gpuStream_t stream, void** buffers, const char* opaque,\n-                  size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = CooFromDense_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n // CooMatvec: Product of COO matrix and dense vector.\n \n static absl::Status CooMatvec_(gpuStream_t stream, void** buffers,\n@@ -497,15 +442,6 @@ static absl::Status CooMatvec_(gpuStream_t stream, void** buffers,\n \n JAX_GPU_REGISTER_WRAPPED_LEGACY_KERNEL(CooMatvecFfi, CooMatvec_);\n \n-void CooMatvec(gpuStream_t stream, void** buffers, const char* opaque,\n-               size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = CooMatvec_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n // CooMatmat: Product of COO matrix and dense matrix.\n \n static absl::Status CooMatmat_(gpuStream_t stream, void** buffers,\n@@ -564,92 +500,8 @@ static absl::Status CooMatmat_(gpuStream_t stream, void** buffers,\n }\n \n JAX_GPU_REGISTER_WRAPPED_LEGACY_KERNEL(CooMatmatFfi, CooMatmat_);\n-\n-void CooMatmat(gpuStream_t stream, void** buffers, const char* opaque,\n-               size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = CooMatmat_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n #endif  // if JAX_GPU_HAVE_SPARSE\n \n-template <typename T, typename F>\n-static absl::Status gtsv2(F computeGtsv2, gpuStream_t stream, void** buffers,\n-                          const char* opaque, std::size_t opaque_len) {\n-  auto h = SparseHandlePool::Borrow(stream);\n-  JAX_RETURN_IF_ERROR(h.status());\n-  auto& handle = *h;\n-\n-  auto s = UnpackDescriptor<Gtsv2Descriptor>(opaque, opaque_len);\n-  JAX_RETURN_IF_ERROR(s.status());\n-  const Gtsv2Descriptor& descriptor = **s;\n-  int batch = descriptor.batch;\n-  int m = descriptor.m;\n-  int n = descriptor.n;\n-  int ldb = descriptor.ldb;\n-\n-  T* dl = static_cast<T*>(buffers[0]);\n-  T* d = static_cast<T*>(buffers[1]);\n-  T* du = static_cast<T*>(buffers[2]);\n-  T* B = static_cast<T*>(buffers[3]);\n-  T* X = static_cast<T*>(buffers[4]);\n-  void* buffer = static_cast<void*>(buffers[5]);\n-\n-  // The solution X is written in place to B. We need to therefore copy the\n-  // contents of B into the output buffer X and pass that into the kernel as B.\n-  // Once copy insertion is supported for custom call aliasing, we could alias B\n-  // with X and avoid the copy, the code below is written defensively assuming B\n-  // and X might alias, but today we know they will not.\n-  // TODO(b/182906199): Update the comment here once copy insertion is WAI.\n-  if (X != B) {\n-    size_t B_bytes = ldb * n * sizeof(T) * batch;\n-    JAX_RETURN_IF_ERROR(JAX_AS_STATUS(\n-        gpuMemcpyAsync(X, B, B_bytes, gpuMemcpyDeviceToDevice, stream)));\n-  }\n-  for (int i = 0; i < batch; ++i) {\n-    JAX_RETURN_IF_ERROR(JAX_AS_STATUS(\n-        computeGtsv2(handle.get(), m, n, dl, d, du, X, ldb, buffer)));\n-    dl += m;\n-    d += m;\n-    du += m;\n-    X += m * n;\n-  }\n-  return absl::OkStatus();\n-}\n-\n-JAX_GPU_REGISTER_WRAPPED_LEGACY_KERNEL(\n-    gtsv2_f32_ffi, [](gpuStream_t stream, void** buffers, const char* opaque,\n-                      std::size_t opaque_len) {\n-      return gtsv2<float>(gpusparseSgtsv2, stream, buffers, opaque, opaque_len);\n-    });\n-\n-JAX_GPU_REGISTER_WRAPPED_LEGACY_KERNEL(\n-    gtsv2_f64_ffi, [](gpuStream_t stream, void** buffers, const char* opaque,\n-                      std::size_t opaque_len) {\n-      return gtsv2<double>(gpusparseDgtsv2, stream, buffers, opaque,\n-                           opaque_len);\n-    });\n-\n-void gtsv2_f32(gpuStream_t stream, void** buffers, const char* opaque,\n-               std::size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = gtsv2<float>(gpusparseSgtsv2, stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n-void gtsv2_f64(gpuStream_t stream, void** buffers, const char* opaque,\n-               std::size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = gtsv2<double>(gpusparseDgtsv2, stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n template <typename T, typename BufferSizeF, typename KernelF>\n ffi::Error Gtsv2Impl(BufferSizeF getBufferSize, KernelF kernel, int64_t batch,\n                      int64_t rows, int64_t cols, gpuStream_t stream,\ndiff --git a/jaxlib/gpu/sparse_kernels.h b/jaxlib/gpu/sparse_kernels.h\nindex 3b365872f591..75f83752be15 100644\n--- a/jaxlib/gpu/sparse_kernels.h\n+++ b/jaxlib/gpu/sparse_kernels.h\n@@ -16,14 +16,12 @@ limitations under the License.\n #ifndef JAXLIB_GPU_SPARSE_KERNELS_H_\n #define JAXLIB_GPU_SPARSE_KERNELS_H_\n \n-#include <cstddef>\n #include <cstdint>\n \n #include \"absl/status/statusor.h\"\n #include \"jaxlib/gpu/handle_pool.h\"\n #include \"jaxlib/gpu/vendor.h\"\n #include \"xla/ffi/api/ffi.h\"\n-#include \"xla/service/custom_call_status.h\"\n \n namespace jax {\n \n@@ -72,17 +70,6 @@ struct DenseVecDescriptor {\n };\n \n #if JAX_GPU_HAVE_SPARSE\n-// CsrToDense: Convert CSR matrix to dense matrix\n-\n-void CsrToDense(gpuStream_t stream, void** buffers, const char* opaque,\n-                size_t opaque_len, XlaCustomCallStatus* status);\n-\n-// CsrFromDense: Convert dense matrix to CSR matrix\n-\n-void CsrFromDense(gpuStream_t stream, void** buffers, const char* opaque,\n-                  size_t opaque_len, XlaCustomCallStatus* status);\n-\n-// CsrMatvec: Product of CSR matrix and dense vector.\n \n struct CsrMatvecDescriptor {\n   SparseMatDescriptor A;\n@@ -90,63 +77,24 @@ struct CsrMatvecDescriptor {\n   gpusparseOperation_t op;\n };\n \n-void CsrMatvec(gpuStream_t stream, void** buffers, const char* opaque,\n-               size_t opaque_len, XlaCustomCallStatus* status);\n-\n-// CsrMatmat: Product of CSR matrix and dense matrix.\n-\n struct CsrMatmatDescriptor {\n   SparseMatDescriptor A;\n   DenseMatDescriptor B, C;\n   gpusparseOperation_t op_A;\n };\n \n-void CsrMatmat(gpuStream_t stream, void** buffers, const char* opaque,\n-               size_t opaque_len, XlaCustomCallStatus* status);\n-\n-// CooToDense: Convert COO matrix to dense matrix\n-\n-void CooToDense(gpuStream_t stream, void** buffers, const char* opaque,\n-                size_t opaque_len, XlaCustomCallStatus* status);\n-\n-// CooFromDense: Convert dense matrix to COO matrix\n-\n-void CooFromDense(gpuStream_t stream, void** buffers, const char* opaque,\n-                  size_t opaque_len, XlaCustomCallStatus* status);\n-\n-// CooMatvec: Product of COO matrix and dense vector.\n-\n struct CooMatvecDescriptor {\n   SparseMatDescriptor A;\n   DenseVecDescriptor x, y;\n   gpusparseOperation_t op;\n };\n \n-void CooMatvec(gpuStream_t stream, void** buffers, const char* opaque,\n-               size_t opaque_len, XlaCustomCallStatus* status);\n-\n-// CooMatmat: Product of COO matrix and dense matrix.\n-\n struct CooMatmatDescriptor {\n   SparseMatDescriptor A;\n   DenseMatDescriptor B, C;\n   gpusparseOperation_t op_A;\n };\n \n-void CooMatmat(gpuStream_t stream, void** buffers, const char* opaque,\n-               size_t opaque_len, XlaCustomCallStatus* status);\n-#endif  // JAX_GPU_HAVE_SPARSE\n-\n-struct Gtsv2Descriptor {\n-  int batch, m, n, ldb;\n-};\n-\n-void gtsv2_f32(gpuStream_t stream, void** buffers, const char* opaque,\n-               std::size_t opaque_len, XlaCustomCallStatus* status);\n-\n-void gtsv2_f64(gpuStream_t stream, void** buffers, const char* opaque,\n-               std::size_t opaque_len, XlaCustomCallStatus* status);\n-\n XLA_FFI_DECLARE_HANDLER_SYMBOL(CsrToDenseFfi);\n XLA_FFI_DECLARE_HANDLER_SYMBOL(CsrFromDenseFfi);\n XLA_FFI_DECLARE_HANDLER_SYMBOL(CsrMatvecFfi);\n@@ -155,8 +103,9 @@ XLA_FFI_DECLARE_HANDLER_SYMBOL(CooToDenseFfi);\n XLA_FFI_DECLARE_HANDLER_SYMBOL(CooFromDenseFfi);\n XLA_FFI_DECLARE_HANDLER_SYMBOL(CooMatvecFfi);\n XLA_FFI_DECLARE_HANDLER_SYMBOL(CooMatmatFfi);\n-XLA_FFI_DECLARE_HANDLER_SYMBOL(gtsv2_f32_ffi);\n-XLA_FFI_DECLARE_HANDLER_SYMBOL(gtsv2_f64_ffi);\n+\n+#endif  // JAX_GPU_HAVE_SPARSE\n+\n XLA_FFI_DECLARE_HANDLER_SYMBOL(kGtsv2);\n \n }  // namespace JAX_GPU_NAMESPACE\ndiff --git a/jaxlib/rocm/BUILD b/jaxlib/rocm/BUILD\nindex 76e3ef01563c..a24a1617d309 100644\n--- a/jaxlib/rocm/BUILD\n+++ b/jaxlib/rocm/BUILD\n@@ -229,7 +229,6 @@ cc_library(\n         \"@local_config_rocm//rocm:hipsparse\",\n         \"@local_config_rocm//rocm:rocm_headers\",\n         \"@xla//xla/ffi/api:ffi\",\n-        \"@xla//xla/service:custom_call_status\",\n     ],\n )\n \n@@ -247,7 +246,6 @@ nanobind_extension(\n         \":hip_vendor\",\n         \":hipsparse_kernels\",\n         \"//jaxlib:absl_status_casters\",\n-        \"//jaxlib:kernel_helpers\",\n         \"//jaxlib:kernel_nanobind_helpers\",\n         \"@com_google_absl//absl/algorithm:container\",\n         \"@com_google_absl//absl/base\",\n@@ -255,14 +253,12 @@ nanobind_extension(\n         \"@com_google_absl//absl/container:flat_hash_map\",\n         \"@com_google_absl//absl/hash\",\n         \"@com_google_absl//absl/memory\",\n-        \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/strings\",\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@local_config_rocm//rocm:hipsparse\",\n         \"@local_config_rocm//rocm:rocm_headers\",\n         \"@nanobind\",\n-        \"@xla//xla/service:custom_call_status\",\n         \"@xla//xla/tsl/python/lib/core:numpy\",\n     ],\n )\n@@ -330,7 +326,6 @@ cc_library(\n         \"@local_config_rocm//rocm:rocm_headers\",\n         \"@xla//xla/ffi/api:c_api\",\n         \"@xla//xla/ffi/api:ffi\",\n-        \"@xla//xla/service:custom_call_status\",\n     ],\n )\n \n@@ -344,7 +339,6 @@ rocm_library(\n         \"//jaxlib:kernel_helpers\",\n         \"@local_config_rocm//rocm:rocm_headers\",\n         \"@xla//xla/ffi/api:ffi\",\n-        \"@xla//xla/service:custom_call_status\",\n     ],\n )\n \n\n```",
        "from_id": [
            "dfm",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Simplify `jnp.isclose`\n\n- Clean up implementation of `isclose` to match NumPy 2.* behavior\n- Add tests for corner cases, kindly provided by @jakevdp",
        "output": "```diff\nCommit: 674fb5b57797f2686f1502b4e41cd2ffa3adbb26\nDate: 2025-06-02T19:47:53Z\nURL: https://github.com/jax-ml/jax/commit/674fb5b57797f2686f1502b4e41cd2ffa3adbb26\nFiles changed: 2\nAdditions: +21, Deletions: -23\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex ad2b3ad6aa75..f42f11844783 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -2602,31 +2602,13 @@ def isclose(a: ArrayLike, b: ArrayLike, rtol: ArrayLike = 1e-05, atol: ArrayLike\n     dtype = np.array(0, dtype).real.dtype\n   rtol = lax.convert_element_type(rtol, dtype)\n   atol = lax.convert_element_type(atol, dtype)\n-  out = lax.le(\n+  both_nan = ufuncs.logical_and(ufuncs.isnan(a), ufuncs.isnan(b))\n+  check_fin = ufuncs.isfinite(b)\n+  in_range = lax.le(\n     lax.abs(lax.sub(a, b)),\n     lax.add(atol, lax.mul(rtol, lax.abs(b))))\n-  # This corrects the comparisons for infinite and nan values\n-  a_inf = ufuncs.isinf(a)\n-  b_inf = ufuncs.isinf(b)\n-  any_inf = ufuncs.logical_or(a_inf, b_inf)\n-  both_inf = ufuncs.logical_and(a_inf, b_inf)\n-  # Make all elements where either a or b are infinite to False\n-  out = ufuncs.logical_and(out, ufuncs.logical_not(any_inf))\n-  # Make all elements where both a or b are the same inf to True\n-  same_value = lax.eq(a, b)\n-  same_inf = ufuncs.logical_and(both_inf, same_value)\n-  out = ufuncs.logical_or(out, same_inf)\n-\n-  # Make all elements where either a or b is NaN to False\n-  a_nan = ufuncs.isnan(a)\n-  b_nan = ufuncs.isnan(b)\n-  any_nan = ufuncs.logical_or(a_nan, b_nan)\n-  out = ufuncs.logical_and(out, ufuncs.logical_not(any_nan))\n-  if equal_nan:\n-    # Make all elements where both a and b is NaN to True\n-    both_nan = ufuncs.logical_and(a_nan, b_nan)\n-    out = ufuncs.logical_or(out, both_nan)\n-  return out\n+  out = ufuncs.logical_or(lax.eq(a, b), ufuncs.logical_and(check_fin, in_range))\n+  return ufuncs.logical_or(out, both_nan) if equal_nan else out\n \n \n def _interp(x: ArrayLike, xp: ArrayLike, fp: ArrayLike,\ndiff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py\nindex 16234463d795..60f3bbb09edf 100644\n--- a/tests/lax_numpy_test.py\n+++ b/tests/lax_numpy_test.py\n@@ -3963,6 +3963,22 @@ def testIsClose(self):\n     key = jax.random.key(0)\n     self.assertTrue(jnp.isclose(key, key))\n \n+  @jtu.sample_product(\n+    atol=[0.0, 1E-4, np.inf],\n+    rtol=[0.0, 1E-4, np.inf],\n+    equal_nan=[True, False]\n+  )\n+  def testIsCloseCornerCases(self, atol, rtol, equal_nan):\n+    if jtu.numpy_version() < (2, 0, 0) and (np.isinf(atol) or np.isinf(rtol)):\n+      self.skipTest(\"fails on older NumPy\")\n+    vals = np.array([-np.nan, -np.inf, -1.00001, -1.0, -0.00001, -0.0,\n+                     0.0, 0.00001, 1.0, 1.00001, np.inf, np.nan])\n+    x, y = np.meshgrid(vals, vals)\n+    self.assertArraysEqual(\n+      np.isclose(x, y, atol=atol, rtol=rtol, equal_nan=equal_nan),\n+      jnp.isclose(x, y, atol=atol, rtol=rtol, equal_nan=equal_nan)\n+    )\n+\n   @jtu.sample_product(\n     x=[1, [1], [1, 1 + 1E-4], [1, np.nan]],\n     y=[1, [1], [1, 1 + 1E-4], [1, np.nan]],\n\n```",
        "from_id": [
            "soraros"
        ]
    },
    {
        "text_input": "Clean up some unused GPU linear algebra kernels.\n\nThis change removes the legacy `csrlsvqr` and `sytrd` custom calls from jaxlib. These were never covered by the export compatibility policy, and their FFI counterparts have been targeted by JAX for several releases.\n\nPiperOrigin-RevId: 766298494",
        "output": "```diff\nCommit: 3e52872cb6fcb73455553cb5ebfe51c268f4b19c\nDate: 2025-06-02T19:30:29Z\nURL: https://github.com/jax-ml/jax/commit/3e52872cb6fcb73455553cb5ebfe51c268f4b19c\nFiles changed: 7\nAdditions: +0, Deletions: -464\ndiff --git a/jaxlib/cuda/BUILD b/jaxlib/cuda/BUILD\nindex eabb3157ecca..7bcb526e6e38 100644\n--- a/jaxlib/cuda/BUILD\n+++ b/jaxlib/cuda/BUILD\n@@ -155,24 +155,6 @@ cc_library(\n     ],\n )\n \n-cc_library(\n-    name = \"cusolver_kernels\",\n-    srcs = [\"//jaxlib/gpu:solver_kernels.cc\"],\n-    hdrs = [\"//jaxlib/gpu:solver_kernels.h\"],\n-    deps = [\n-        \":cuda_gpu_kernel_helpers\",\n-        \":cuda_solver_handle_pool\",\n-        \":cuda_vendor\",\n-        \"//jaxlib:kernel_helpers\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@local_config_cuda//cuda:cuda_headers\",\n-        \"@xla//xla/service:custom_call_status\",\n-        \"@xla//xla/tsl/cuda:cudart\",\n-        \"@xla//xla/tsl/cuda:cusolver\",\n-    ],\n-)\n-\n cc_library(\n     name = \"cusolver_interface\",\n     srcs = [\"//jaxlib/gpu:solver_interface.cc\"],\n@@ -223,21 +205,14 @@ nanobind_extension(\n     features = [\"-use_header_modules\"],\n     module_name = \"_solver\",\n     deps = [\n-        \":cuda_gpu_kernel_helpers\",\n-        \":cuda_solver_handle_pool\",\n         \":cuda_vendor\",\n-        \":cusolver_kernels\",\n         \":cusolver_kernels_ffi\",\n         \"//jaxlib:kernel_nanobind_helpers\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings:str_format\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@nanobind\",\n         \"@xla//xla/tsl/cuda:cublas\",\n         \"@xla//xla/tsl/cuda:cudart\",\n         \"@xla//xla/tsl/cuda:cusolver\",\n-        \"@xla//xla/tsl/python/lib/core:numpy\",\n     ],\n )\n \n@@ -472,7 +447,6 @@ cc_library(\n         \":cuda_prng_kernels\",\n         \":cuda_vendor\",\n         \":cudnn_rnn_kernels\",\n-        \":cusolver_kernels\",\n         \":cusolver_kernels_ffi\",\n         \":cusparse_kernels\",\n         \":triton_kernels\",\ndiff --git a/jaxlib/gpu/BUILD b/jaxlib/gpu/BUILD\nindex e153e0588cf6..98f0f6cfe624 100644\n--- a/jaxlib/gpu/BUILD\n+++ b/jaxlib/gpu/BUILD\n@@ -59,8 +59,6 @@ exports_files(srcs = [\n     \"solver_handle_pool.h\",\n     \"solver_interface.cc\",\n     \"solver_interface.h\",\n-    \"solver_kernels.cc\",\n-    \"solver_kernels.h\",\n     \"solver_kernels_ffi.cc\",\n     \"solver_kernels_ffi.h\",\n     \"sparse.cc\",\ndiff --git a/jaxlib/gpu/gpu_kernels.cc b/jaxlib/gpu/gpu_kernels.cc\nindex c59cc7d8076b..3204053b8822 100644\n--- a/jaxlib/gpu/gpu_kernels.cc\n+++ b/jaxlib/gpu/gpu_kernels.cc\n@@ -19,7 +19,6 @@ limitations under the License.\n #include \"jaxlib/gpu/linalg_kernels.h\"\n #include \"jaxlib/gpu/prng_kernels.h\"\n #include \"jaxlib/gpu/rnn_kernels.h\"\n-#include \"jaxlib/gpu/solver_kernels.h\"\n #include \"jaxlib/gpu/solver_kernels_ffi.h\"\n #include \"jaxlib/gpu/sparse_kernels.h\"\n #include \"jaxlib/gpu/triton_kernels.h\"\n@@ -40,14 +39,12 @@ XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusolver_syrk_ffi\", \"CUDA\",\n                          SyrkFfi);\n XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusolver_geqrf_ffi\", \"CUDA\",\n                          GeqrfFfi);\n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cusolver_csrlsvqr\", Csrlsvqr, \"CUDA\");\n XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusolver_csrlsvqr_ffi\", \"CUDA\",\n                          CsrlsvqrFfi);\n XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusolver_orgqr_ffi\", \"CUDA\",\n                          OrgqrFfi);\n XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusolver_syevd_ffi\", \"CUDA\",\n                          SyevdFfi);\n-XLA_REGISTER_CUSTOM_CALL_TARGET_WITH_SYM(\"cusolver_sytrd\", Sytrd, \"CUDA\");\n XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusolver_sytrd_ffi\", \"CUDA\",\n                          SytrdFfi);\n XLA_FFI_REGISTER_HANDLER(XLA_FFI_GetApi(), \"cusolver_gesvd_ffi\", \"CUDA\",\ndiff --git a/jaxlib/gpu/solver.cc b/jaxlib/gpu/solver.cc\nindex e4d6b5d4dedf..08d25948d893 100644\n--- a/jaxlib/gpu/solver.cc\n+++ b/jaxlib/gpu/solver.cc\n@@ -13,21 +13,10 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n-#include <stdexcept>\n-#include <utility>\n-\n-#include \"absl/container/flat_hash_map.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"absl/strings/str_format.h\"\n #include \"nanobind/nanobind.h\"\n-#include \"nanobind/stl/pair.h\"  // IWYU pragma: keep\n-#include \"jaxlib/gpu/gpu_kernel_helpers.h\"\n-#include \"jaxlib/gpu/solver_handle_pool.h\"\n-#include \"jaxlib/gpu/solver_kernels.h\"\n #include \"jaxlib/gpu/solver_kernels_ffi.h\"\n #include \"jaxlib/gpu/vendor.h\"\n #include \"jaxlib/kernel_nanobind_helpers.h\"\n-#include \"xla/tsl/python/lib/core/numpy.h\"\n \n namespace jax {\n namespace JAX_GPU_NAMESPACE {\n@@ -35,79 +24,8 @@ namespace {\n \n namespace nb = nanobind;\n \n-// Converts a NumPy dtype to a Type.\n-SolverType DtypeToSolverType(const dtype& np_type) {\n-  static auto* types =\n-      new absl::flat_hash_map<std::pair<char, int>, SolverType>({\n-          {{'f', 4}, SolverType::F32},\n-          {{'f', 8}, SolverType::F64},\n-          {{'c', 8}, SolverType::C64},\n-          {{'c', 16}, SolverType::C128},\n-      });\n-  auto it = types->find({np_type.kind(), np_type.itemsize()});\n-  if (it == types->end()) {\n-    nb::str repr = nb::repr(np_type);\n-    throw std::invalid_argument(\n-        absl::StrFormat(\"Unsupported dtype %s\", repr.c_str()));\n-  }\n-  return it->second;\n-}\n-\n-#ifdef JAX_GPU_CUDA\n-\n-// csrlsvqr: Linear system solve via Sparse QR\n-\n-// Returns a descriptor for a csrlsvqr operation.\n-nb::bytes BuildCsrlsvqrDescriptor(const dtype& dtype, int n, int nnzA,\n-                                  int reorder, double tol) {\n-  SolverType type = DtypeToSolverType(dtype);\n-  return PackDescriptor(CsrlsvqrDescriptor{type, n, nnzA, reorder, tol});\n-}\n-\n-#endif  // JAX_GPU_CUDA\n-\n-// Returns the workspace size and a descriptor for a geqrf operation.\n-std::pair<int, nb::bytes> BuildSytrdDescriptor(const dtype& dtype, bool lower,\n-                                               int b, int n) {\n-  SolverType type = DtypeToSolverType(dtype);\n-  auto h = SolverHandlePool::Borrow(/*stream=*/nullptr);\n-  JAX_THROW_IF_ERROR(h.status());\n-  auto& handle = *h;\n-  int lwork;\n-  gpusolverFillMode_t uplo =\n-      lower ? GPUSOLVER_FILL_MODE_LOWER : GPUSOLVER_FILL_MODE_UPPER;\n-  switch (type) {\n-    case SolverType::F32:\n-      JAX_THROW_IF_ERROR(JAX_AS_STATUS(gpusolverDnSsytrd_bufferSize(\n-          handle.get(), uplo, n, /*A=*/nullptr, /*lda=*/n, /*D=*/nullptr,\n-          /*E=*/nullptr, /*tau=*/nullptr, &lwork)));\n-      break;\n-    case SolverType::F64:\n-      JAX_THROW_IF_ERROR(JAX_AS_STATUS(gpusolverDnDsytrd_bufferSize(\n-          handle.get(), uplo, n, /*A=*/nullptr, /*lda=*/n, /*D=*/nullptr,\n-          /*E=*/nullptr, /*tau=*/nullptr, &lwork)));\n-      break;\n-    case SolverType::C64:\n-      JAX_THROW_IF_ERROR(JAX_AS_STATUS(gpusolverDnChetrd_bufferSize(\n-          handle.get(), uplo, n, /*A=*/nullptr, /*lda=*/n, /*D=*/nullptr,\n-          /*E=*/nullptr, /*tau=*/nullptr, &lwork)));\n-      break;\n-    case SolverType::C128:\n-      JAX_THROW_IF_ERROR(JAX_AS_STATUS(gpusolverDnZhetrd_bufferSize(\n-          handle.get(), uplo, n, /*A=*/nullptr, /*lda=*/n, /*D=*/nullptr,\n-          /*E=*/nullptr, /*tau=*/nullptr, &lwork)));\n-      break;\n-  }\n-  return {lwork, PackDescriptor(SytrdDescriptor{type, uplo, b, n, n, lwork})};\n-}\n-\n nb::dict Registrations() {\n   nb::dict dict;\n-  dict[JAX_GPU_PREFIX \"solver_sytrd\"] = EncapsulateFunction(Sytrd);\n-\n-#ifdef JAX_GPU_CUDA\n-  dict[\"cusolver_csrlsvqr\"] = EncapsulateFunction(Csrlsvqr);\n-#endif  // JAX_GPU_CUDA\n \n   dict[JAX_GPU_PREFIX \"solver_getrf_ffi\"] = EncapsulateFfiHandler(GetrfFfi);\n   dict[JAX_GPU_PREFIX \"solver_geqrf_ffi\"] = EncapsulateFfiHandler(GeqrfFfi);\n@@ -127,12 +45,7 @@ nb::dict Registrations() {\n }\n \n NB_MODULE(_solver, m) {\n-  tsl::ImportNumpy();\n   m.def(\"registrations\", &Registrations);\n-  m.def(\"build_sytrd_descriptor\", &BuildSytrdDescriptor);\n-#ifdef JAX_GPU_CUDA\n-  m.def(\"build_csrlsvqr_descriptor\", &BuildCsrlsvqrDescriptor);\n-#endif  // JAX_GPU_CUDA\n }\n \n }  // namespace\ndiff --git a/jaxlib/gpu/solver_kernels.cc b/jaxlib/gpu/solver_kernels.cc\ndeleted file mode 100644\nindex d054e77d2102..000000000000\n--- a/jaxlib/gpu/solver_kernels.cc\n+++ /dev/null\n@@ -1,255 +0,0 @@\n-/* Copyright 2019 The JAX Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#include \"jaxlib/gpu/solver_kernels.h\"\n-\n-#include <cstddef>\n-#include <cstdint>\n-#include <string>\n-\n-#include \"absl/status/status.h\"\n-#include \"absl/status/statusor.h\"\n-#include \"jaxlib/gpu/gpu_kernel_helpers.h\"\n-#include \"jaxlib/gpu/solver_handle_pool.h\"\n-#include \"jaxlib/gpu/vendor.h\"\n-#include \"jaxlib/kernel_helpers.h\"\n-#include \"xla/service/custom_call_status.h\"\n-\n-#ifdef JAX_GPU_CUDA\n-#include \"third_party/gpus/cuda/include/cusolverSp.h\"\n-#endif  // JAX_GPU_CUDA\n-\n-namespace jax {\n-\n-namespace JAX_GPU_NAMESPACE {\n-\n-static int SizeOfSolverType(SolverType type) {\n-  switch (type) {\n-    case SolverType::F32:\n-      return sizeof(float);\n-    case SolverType::F64:\n-      return sizeof(double);\n-    case SolverType::C64:\n-      return sizeof(gpuComplex);\n-    case SolverType::C128:\n-      return sizeof(gpuDoubleComplex);\n-  }\n-}\n-\n-#ifdef JAX_GPU_CUDA\n-\n-// csrlsvqr: Linear system solve via Sparse QR\n-\n-static absl::Status Csrlsvqr_(gpuStream_t stream, void** buffers,\n-                              const char* opaque, size_t opaque_len,\n-                              int& singularity) {\n-  auto s = UnpackDescriptor<CsrlsvqrDescriptor>(opaque, opaque_len);\n-  JAX_RETURN_IF_ERROR(s.status());\n-  const CsrlsvqrDescriptor& d = **s;\n-\n-  // This is the handle to the CUDA session. Gets a cusolverSp handle.\n-  auto h = SpSolverHandlePool::Borrow(stream);\n-  JAX_RETURN_IF_ERROR(h.status());\n-  auto& handle = *h;\n-\n-  cusparseMatDescr_t matdesc = nullptr;\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(cusparseCreateMatDescr(&matdesc)));\n-  JAX_RETURN_IF_ERROR(\n-      JAX_AS_STATUS(cusparseSetMatType(matdesc, CUSPARSE_MATRIX_TYPE_GENERAL)));\n-  JAX_RETURN_IF_ERROR(JAX_AS_STATUS(\n-      cusparseSetMatIndexBase(matdesc, CUSPARSE_INDEX_BASE_ZERO)));\n-\n-  switch (d.type) {\n-    case SolverType::F32: {\n-      float* csrValA = static_cast<float*>(buffers[0]);\n-      int* csrRowPtrA = static_cast<int*>(buffers[1]);\n-      int* csrColIndA = static_cast<int*>(buffers[2]);\n-      float* b = static_cast<float*>(buffers[3]);\n-      float* x = static_cast<float*>(buffers[4]);\n-\n-      JAX_RETURN_IF_ERROR(JAX_AS_STATUS(cusolverSpScsrlsvqr(\n-          handle.get(), d.n, d.nnz, matdesc, csrValA, csrRowPtrA, csrColIndA, b,\n-          (float)d.tol, d.reorder, x, &singularity)));\n-\n-      break;\n-    }\n-    case SolverType::F64: {\n-      double* csrValA = static_cast<double*>(buffers[0]);\n-      int* csrRowPtrA = static_cast<int*>(buffers[1]);\n-      int* csrColIndA = static_cast<int*>(buffers[2]);\n-      double* b = static_cast<double*>(buffers[3]);\n-      double* x = static_cast<double*>(buffers[4]);\n-\n-      JAX_RETURN_IF_ERROR(JAX_AS_STATUS(cusolverSpDcsrlsvqr(\n-          handle.get(), d.n, d.nnz, matdesc, csrValA, csrRowPtrA, csrColIndA, b,\n-          d.tol, d.reorder, x, &singularity)));\n-\n-      break;\n-    }\n-    case SolverType::C64: {\n-      gpuComplex* csrValA = static_cast<gpuComplex*>(buffers[0]);\n-      int* csrRowPtrA = static_cast<int*>(buffers[1]);\n-      int* csrColIndA = static_cast<int*>(buffers[2]);\n-      gpuComplex* b = static_cast<gpuComplex*>(buffers[3]);\n-      gpuComplex* x = static_cast<gpuComplex*>(buffers[4]);\n-\n-      JAX_RETURN_IF_ERROR(JAX_AS_STATUS(cusolverSpCcsrlsvqr(\n-          handle.get(), d.n, d.nnz, matdesc, csrValA, csrRowPtrA, csrColIndA, b,\n-          (float)d.tol, d.reorder, x, &singularity)));\n-\n-      break;\n-    }\n-    case SolverType::C128: {\n-      gpuDoubleComplex* csrValA = static_cast<gpuDoubleComplex*>(buffers[0]);\n-      int* csrRowPtrA = static_cast<int*>(buffers[1]);\n-      int* csrColIndA = static_cast<int*>(buffers[2]);\n-      gpuDoubleComplex* b = static_cast<gpuDoubleComplex*>(buffers[3]);\n-      gpuDoubleComplex* x = static_cast<gpuDoubleComplex*>(buffers[4]);\n-\n-      JAX_RETURN_IF_ERROR(JAX_AS_STATUS(cusolverSpZcsrlsvqr(\n-          handle.get(), d.n, d.nnz, matdesc, csrValA, csrRowPtrA, csrColIndA, b,\n-          (float)d.tol, d.reorder, x, &singularity)));\n-\n-      break;\n-    }\n-  }\n-\n-  cusparseDestroyMatDescr(matdesc);\n-  return absl::OkStatus();\n-}\n-\n-void Csrlsvqr(gpuStream_t stream, void** buffers, const char* opaque,\n-              size_t opaque_len, XlaCustomCallStatus* status) {\n-  // Is >= 0 if A is singular.\n-  int singularity = -1;\n-\n-  auto s = Csrlsvqr_(stream, buffers, opaque, opaque_len, singularity);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-\n-  if (singularity >= 0) {\n-    auto s = std::string(\"Singular matrix in linear solve.\");\n-    XlaCustomCallStatusSetFailure(status, s.c_str(), s.length());\n-  }\n-}\n-\n-#endif  // JAX_GPU_CUDA\n-\n-// sytrd/hetrd: symmetric (Hermitian) tridiagonal reduction\n-\n-static absl::Status Sytrd_(gpuStream_t stream, void** buffers,\n-                           const char* opaque, size_t opaque_len) {\n-  auto s = UnpackDescriptor<SytrdDescriptor>(opaque, opaque_len);\n-  JAX_RETURN_IF_ERROR(s.status());\n-  const SytrdDescriptor& d = **s;\n-  auto h = SolverHandlePool::Borrow(stream);\n-  JAX_RETURN_IF_ERROR(h.status());\n-  auto& handle = *h;\n-  if (buffers[1] != buffers[0]) {\n-    JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpuMemcpyAsync(\n-        buffers[1], buffers[0],\n-        SizeOfSolverType(d.type) * static_cast<std::int64_t>(d.batch) *\n-            static_cast<std::int64_t>(d.n) * static_cast<std::int64_t>(d.lda),\n-        gpuMemcpyDeviceToDevice, stream)));\n-  }\n-\n-  int* info = static_cast<int*>(buffers[5]);\n-  void* workspace = buffers[6];\n-  switch (d.type) {\n-    case SolverType::F32: {\n-      float* a = static_cast<float*>(buffers[1]);\n-      float* d_out = static_cast<float*>(buffers[2]);\n-      float* e_out = static_cast<float*>(buffers[3]);\n-      float* tau = static_cast<float*>(buffers[4]);\n-      for (int i = 0; i < d.batch; ++i) {\n-        JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpusolverDnSsytrd(\n-            handle.get(), d.uplo, d.n, a, d.lda, d_out, e_out, tau,\n-            static_cast<float*>(workspace), d.lwork, info)));\n-        a += d.lda * d.n;\n-        d_out += d.n;\n-        e_out += d.n - 1;\n-        tau += d.n - 1;\n-        ++info;\n-      }\n-      break;\n-    }\n-    case SolverType::F64: {\n-      double* a = static_cast<double*>(buffers[1]);\n-      double* d_out = static_cast<double*>(buffers[2]);\n-      double* e_out = static_cast<double*>(buffers[3]);\n-      double* tau = static_cast<double*>(buffers[4]);\n-      for (int i = 0; i < d.batch; ++i) {\n-        JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpusolverDnDsytrd(\n-            handle.get(), d.uplo, d.n, a, d.lda, d_out, e_out, tau,\n-            static_cast<double*>(workspace), d.lwork, info)));\n-        a += d.lda * d.n;\n-        d_out += d.n;\n-        e_out += d.n - 1;\n-        tau += d.n - 1;\n-        ++info;\n-      }\n-      break;\n-    }\n-    case SolverType::C64: {\n-      gpuComplex* a = static_cast<gpuComplex*>(buffers[1]);\n-      float* d_out = static_cast<float*>(buffers[2]);\n-      float* e_out = static_cast<float*>(buffers[3]);\n-      gpuComplex* tau = static_cast<gpuComplex*>(buffers[4]);\n-      for (int i = 0; i < d.batch; ++i) {\n-        JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpusolverDnChetrd(\n-            handle.get(), d.uplo, d.n, a, d.lda, d_out, e_out, tau,\n-            static_cast<gpuComplex*>(workspace), d.lwork, info)));\n-        a += d.lda * d.n;\n-        d_out += d.n;\n-        e_out += d.n - 1;\n-        tau += d.n - 1;\n-        ++info;\n-      }\n-      break;\n-    }\n-    case SolverType::C128: {\n-      gpuDoubleComplex* a = static_cast<gpuDoubleComplex*>(buffers[1]);\n-      double* d_out = static_cast<double*>(buffers[2]);\n-      double* e_out = static_cast<double*>(buffers[3]);\n-      gpuDoubleComplex* tau = static_cast<gpuDoubleComplex*>(buffers[4]);\n-      for (int i = 0; i < d.batch; ++i) {\n-        JAX_RETURN_IF_ERROR(JAX_AS_STATUS(gpusolverDnZhetrd(\n-            handle.get(), d.uplo, d.n, a, d.lda, d_out, e_out, tau,\n-            static_cast<gpuDoubleComplex*>(workspace), d.lwork, info)));\n-        a += d.lda * d.n;\n-        d_out += d.n;\n-        e_out += d.n - 1;\n-        tau += d.n - 1;\n-        ++info;\n-      }\n-      break;\n-    }\n-  }\n-  return absl::OkStatus();\n-}\n-\n-void Sytrd(gpuStream_t stream, void** buffers, const char* opaque,\n-           size_t opaque_len, XlaCustomCallStatus* status) {\n-  auto s = Sytrd_(stream, buffers, opaque, opaque_len);\n-  if (!s.ok()) {\n-    XlaCustomCallStatusSetFailure(status, std::string(s.message()).c_str(),\n-                                  s.message().length());\n-  }\n-}\n-\n-}  // namespace JAX_GPU_NAMESPACE\n-}  // namespace jax\ndiff --git a/jaxlib/gpu/solver_kernels.h b/jaxlib/gpu/solver_kernels.h\ndeleted file mode 100644\nindex c325e746b709..000000000000\n--- a/jaxlib/gpu/solver_kernels.h\n+++ /dev/null\n@@ -1,65 +0,0 @@\n-/* Copyright 2019 The JAX Authors.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-==============================================================================*/\n-\n-#ifndef JAXLIB_CUSOLVER_KERNELS_H_\n-#define JAXLIB_CUSOLVER_KERNELS_H_\n-\n-#include <cstddef>\n-\n-#include \"jaxlib/gpu/vendor.h\"\n-#include \"xla/service/custom_call_status.h\"\n-\n-namespace jax {\n-\n-namespace JAX_GPU_NAMESPACE {\n-\n-// Set of types known to Cusolver.\n-enum class SolverType {\n-  F32,\n-  F64,\n-  C64,\n-  C128,\n-};\n-\n-#ifdef JAX_GPU_CUDA\n-\n-// csrlsvpr: Linear system solve via Sparse QR\n-\n-struct CsrlsvqrDescriptor {\n-  SolverType type;\n-  int n, nnz, reorder;\n-  double tol;\n-};\n-\n-void Csrlsvqr(gpuStream_t stream, void** buffers, const char* opaque,\n-              size_t opaque_len, XlaCustomCallStatus* status);\n-\n-#endif  // JAX_GPU_CUDA\n-\n-// sytrd/hetrd: Reduction of a symmetric (Hermitian) matrix to tridiagonal form.\n-struct SytrdDescriptor {\n-  SolverType type;\n-  gpusolverFillMode_t uplo;\n-  int batch, n, lda, lwork;\n-};\n-\n-void Sytrd(gpuStream_t stream, void** buffers, const char* opaque,\n-           size_t opaque_len, XlaCustomCallStatus* status);\n-\n-\n-}  // namespace JAX_GPU_NAMESPACE\n-}  // namespace jax\n-\n-#endif  // JAXLIB_CUSOLVER_KERNELS_H_\ndiff --git a/jaxlib/rocm/BUILD b/jaxlib/rocm/BUILD\nindex d0468d72d1b3..76e3ef01563c 100644\n--- a/jaxlib/rocm/BUILD\n+++ b/jaxlib/rocm/BUILD\n@@ -143,24 +143,6 @@ cc_library(\n     ],\n )\n \n-cc_library(\n-    name = \"hipsolver_kernels\",\n-    srcs = [\"//jaxlib/gpu:solver_kernels.cc\"],\n-    hdrs = [\"//jaxlib/gpu:solver_kernels.h\"],\n-    deps = [\n-        \":hip_gpu_kernel_helpers\",\n-        \":hip_solver_handle_pool\",\n-        \":hip_vendor\",\n-        \"//jaxlib:kernel_helpers\",\n-        \"@com_google_absl//absl/status\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/synchronization\",\n-        \"@local_config_rocm//rocm:hipsolver\",\n-        \"@local_config_rocm//rocm:rocm_headers\",\n-        \"@xla//xla/service:custom_call_status\",\n-    ],\n-)\n-\n cc_library(\n     name = \"hipsolver_interface\",\n     srcs = [\"//jaxlib/gpu:solver_interface.cc\"],\n@@ -195,7 +177,6 @@ cc_library(\n         \"@local_config_rocm//rocm:hipsolver\",\n         \"@local_config_rocm//rocm:rocm_headers\",\n         \"@xla//xla/ffi/api:ffi\",\n-        \"@xla//xla/service:custom_call_status\",\n     ],\n )\n \n@@ -209,20 +190,13 @@ nanobind_extension(\n     features = [\"-use_header_modules\"],\n     module_name = \"_solver\",\n     deps = [\n-        \":hip_gpu_kernel_helpers\",\n-        \":hip_solver_handle_pool\",\n         \":hip_vendor\",\n-        \":hipsolver_kernels\",\n         \":hipsolver_kernels_ffi\",\n         \"//jaxlib:kernel_nanobind_helpers\",\n-        \"@com_google_absl//absl/container:flat_hash_map\",\n-        \"@com_google_absl//absl/status:statusor\",\n-        \"@com_google_absl//absl/strings:str_format\",\n         \"@local_config_rocm//rocm:hipblas\",\n         \"@local_config_rocm//rocm:hipsolver\",\n         \"@local_config_rocm//rocm:rocm_headers\",\n         \"@nanobind\",\n-        \"@xla//xla/tsl/python/lib/core:numpy\",\n     ],\n )\n \n\n```",
        "from_id": [
            "dfm",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29144 from DanisNone:main\n\nPiperOrigin-RevId: 766277157",
        "output": "```diff\nCommit: f6e6118f488c9fbb88b66cfd92b92272ec7b1478\nDate: 2025-06-02T18:36:15Z\nURL: https://github.com/jax-ml/jax/commit/f6e6118f488c9fbb88b66cfd92b92272ec7b1478\nFiles changed: 5\nAdditions: +77, Deletions: -20\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex b21fcfeb5772..792954dc1cc8 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -3445,7 +3445,7 @@ def clip(\n   if min is not None:\n     arr = ufuncs.maximum(min, arr)\n   if max is not None:\n-    arr = ufuncs.minimum(max, arr)\n+    arr = ufuncs.minimum(max, arr) # type: ignore\n   return asarray(arr)\n \n \ndiff --git a/jax/_src/numpy/reductions.py b/jax/_src/numpy/reductions.py\nindex 9cb543d5d869..cbfda25eafcf 100644\n--- a/jax/_src/numpy/reductions.py\n+++ b/jax/_src/numpy/reductions.py\n@@ -33,6 +33,7 @@\n     _broadcast_to, ensure_arraylike,\n     promote_dtypes_inexact, promote_dtypes_numeric, _where)\n from jax._src.lax import lax as lax_internal\n+from jax._src.lax import other as lax_other\n from jax._src.typing import Array, ArrayLike, DType, DTypeLike, DeprecatedArg\n from jax._src.util import (\n     canonicalize_axis as _canonicalize_axis, maybe_named_axis,\n@@ -398,11 +399,11 @@ def prod(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None = None,\n \n \n @partial(api.jit, static_argnames=('axis', 'keepdims'), inline=True)\n-def _reduce_max(a: ArrayLike, axis: Axis = None, out: None = None,\n-                keepdims: bool = False, initial: ArrayLike | None = None,\n-                where: ArrayLike | None = None) -> Array:\n+def _reduce_max(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None = None,\n+                out: None = None, keepdims: bool = False,\n+                initial: ArrayLike | None = None, where: ArrayLike | None = None) -> Array:\n   return _reduction(a, \"max\", lax.max, -np.inf, has_identity=False,\n-                    axis=axis, out=out, keepdims=keepdims,\n+                    axis=axis, dtype=dtype, out=out, keepdims=keepdims,\n                     initial=initial, where_=where, parallel_reduce=lax.pmax)\n \n \n@@ -480,12 +481,12 @@ def max(a: ArrayLike, axis: Axis = None, out: None = None,\n   return _reduce_max(a, axis=_ensure_optional_axes(axis), out=out,\n                      keepdims=keepdims, initial=initial, where=where)\n \n-@partial(api.jit, static_argnames=('axis', 'keepdims'), inline=True)\n-def _reduce_min(a: ArrayLike, axis: Axis = None, out: None = None,\n-                keepdims: bool = False, initial: ArrayLike | None = None,\n-                where: ArrayLike | None = None) -> Array:\n+@partial(api.jit, static_argnames=('axis', 'keepdims', 'dtype'), inline=True)\n+def _reduce_min(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None = None,\n+                out: None = None, keepdims: bool = False,\n+                initial: ArrayLike | None = None, where: ArrayLike | None = None) -> Array:\n   return _reduction(a, \"min\", lax.min, np.inf, has_identity=False,\n-                    axis=axis, out=out, keepdims=keepdims,\n+                    axis=axis, dtype=dtype, out=out, keepdims=keepdims,\n                     initial=initial, where_=where, parallel_reduce=lax.pmin)\n \n \n@@ -682,7 +683,7 @@ def _reduce_bitwise_and(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None\n                         out: None = None, keepdims: bool = False,\n                         initial: ArrayLike | None = None, where: ArrayLike | None = None) -> Array:\n   arr = lax_internal.asarray(a)\n-  init_val = np.array(-1, dtype=dtype or arr.dtype)\n+  init_val = np.array(-1).astype(dtype or arr.dtype)\n   return _reduction(arr, name=\"reduce_bitwise_and\", op=lax.bitwise_and, init_val=init_val, preproc=_require_integer,\n                     axis=_ensure_optional_axes(axis), dtype=dtype, out=out, keepdims=keepdims,\n                     initial=initial, where_=where)\n@@ -750,7 +751,7 @@ def _logsumexp(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None = None,\n   exp_a = lax.exp(lax.sub(a_arr, amax_with_dims.astype(a_arr.dtype)))\n   sumexp = exp_a.sum(axis=dims, keepdims=keepdims, where=where)\n   result = lax.add(lax.log(sumexp), amax.astype(sumexp.dtype))\n-  return result if initial is None else lax.logaddexp(initial, result)\n+  return result if initial is None else lax_other.logaddexp(initial, result)\n \n \n def _logsumexp2(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None = None,\n@@ -768,7 +769,6 @@ def _logsumexp2(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None = None,\n   return _logsumexp(a * ln2, axis=axis, dtype=dtype, keepdims=keepdims,\n                     where=where, initial=initial) / ln2\n \n-\n @export\n def amin(a: ArrayLike, axis: Axis = None, out: None = None,\n         keepdims: bool = False, initial: ArrayLike | None = None,\ndiff --git a/jax/_src/numpy/ufuncs.py b/jax/_src/numpy/ufuncs.py\nindex d722534e3136..486d3f15e17c 100644\n--- a/jax/_src/numpy/ufuncs.py\n+++ b/jax/_src/numpy/ufuncs.py\n@@ -1626,8 +1626,7 @@ def arctan2(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n   return lax.atan2(*promote_args_inexact(\"arctan2\", x1, x2))\n \n \n-@export\n-@partial(jit, inline=True)\n+@binary_ufunc(identity=None, reduce=reductions._reduce_min)\n def minimum(x: ArrayLike, y: ArrayLike, /) -> Array:\n   \"\"\"Return element-wise minimum of the input arrays.\n \n@@ -1687,8 +1686,7 @@ def minimum(x: ArrayLike, y: ArrayLike, /) -> Array:\n   return lax.min(*promote_args(\"minimum\", x, y))\n \n \n-@export\n-@partial(jit, inline=True)\n+@binary_ufunc(identity=None, reduce=reductions._reduce_max)\n def maximum(x: ArrayLike, y: ArrayLike, /) -> Array:\n   \"\"\"Return element-wise maximum of the input arrays.\n \ndiff --git a/jax/numpy/__init__.pyi b/jax/numpy/__init__.pyi\nindex 4db407861f34..e81d97765121 100644\n--- a/jax/numpy/__init__.pyi\n+++ b/jax/numpy/__init__.pyi\n@@ -653,7 +653,7 @@ def matvec(x1: ArrayLike, x2: ArrayLike, /) -> Array: ...\n def max(a: ArrayLike, axis: _Axis = ..., out: None = ...,\n         keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n         where: ArrayLike | None = ...) -> Array: ...\n-def maximum(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n+maximum: BinaryUfunc\n def mean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n          out: None = ..., keepdims: builtins.bool = ..., *,\n          where: ArrayLike | None = ...) -> Array: ...\n@@ -666,7 +666,7 @@ mgrid: _Mgrid\n def min(a: ArrayLike, axis: _Axis = ..., out: None = ...,\n         keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n         where: ArrayLike | None = ...) -> Array: ...\n-def minimum(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n+minimum: BinaryUfunc\n def mod(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n def modf(x: ArrayLike, /, out=None) -> tuple[Array, Array]: ...\n def moveaxis(a: ArrayLike, source: int | Sequence[int],\ndiff --git a/tests/lax_numpy_ufuncs_test.py b/tests/lax_numpy_ufuncs_test.py\nindex fd5050a5829b..905d7eed1acd 100644\n--- a/tests/lax_numpy_ufuncs_test.py\n+++ b/tests/lax_numpy_ufuncs_test.py\n@@ -56,7 +56,7 @@ def _jnp_ufunc_props(name):\n   jnp_func = getattr(jnp, name)\n   assert isinstance(jnp_func, jnp.ufunc)\n   np_func = getattr(np, name)\n-  dtypes = [np.dtype(c) for c in \"Ffi?\" if f\"{c}{c}->{c}\" in np_func.types or f\"{c}->{c}\" in np_func.types]\n+  dtypes = [np.dtype(c) for c in \"FfIi?\" if f\"{c}{c}->{c}\" in np_func.types or f\"{c}->{c}\" in np_func.types]\n   return [dict(name=name, dtype=dtype) for dtype in dtypes]\n \n \n@@ -242,6 +242,7 @@ def test_frompyfunc_reduce(self, func, nin, nout, identity, shape, axis, dtype):\n     self._CheckAgainstNumpy(jnp_fun, np_fun, args_maker)\n     self._CompileAndCheck(jnp_fun, args_maker)\n \n+\n   @jtu.sample_product(\n       BINARY_UFUNCS_WITH_DTYPES,\n       [{'shape': shape, 'axis': axis}\n@@ -324,6 +325,64 @@ def test_binary_ufunc_reduce_where(self, name, shape, axis, dtype):\n     self._CheckAgainstNumpy(jnp_fun_reduce, np_fun_reduce, args_maker, tol=tol)\n     self._CompileAndCheck(jnp_fun_reduce, args_maker)\n \n+  @jtu.sample_product(\n+      BINARY_UFUNCS_WITH_DTYPES,\n+      [{'shape': shape, 'axis': axis}\n+       for shape in nonscalar_shapes\n+       for axis in [None, *range(-len(shape), len(shape))]],\n+  )\n+  def test_binary_ufunc_reduce_initial(self, name, shape, axis, dtype):\n+    jnp_fun = getattr(jnp, name)\n+    np_fun = getattr(np, name)\n+\n+    if jnp_fun.identity is None and axis is None and len(shape) > 1:\n+      self.skipTest(\"Multiple-axis reduction over non-reorderable ufunc.\")\n+\n+    jnp_fun_reduce = lambda a, initial: jnp_fun.reduce(a, axis=axis, initial=initial)\n+    np_fun_reduce = lambda a, initial: np_fun.reduce(a, axis=axis, initial=initial)\n+\n+    rng = jtu.rand_default(self.rng())\n+    rng_initial = jtu.rand_default(self.rng())\n+    args_maker = lambda: [rng(shape, dtype), rng_initial((), dtype)]\n+\n+    tol = {np.float32: 1E-4} if jtu.test_device_matches(['tpu']) else None\n+\n+    self._CheckAgainstNumpy(jnp_fun_reduce, np_fun_reduce, args_maker, tol=tol)\n+    self._CompileAndCheck(jnp_fun_reduce, args_maker)\n+\n+  @jtu.sample_product(\n+      BINARY_UFUNCS_WITH_DTYPES,\n+      [{'shape': shape, 'axis': axis}\n+      for shape in nonscalar_shapes\n+      for axis in [None, *range(-len(shape), len(shape))]],\n+  )\n+  def test_binary_ufunc_reduce_where_initial(self, name, shape, axis, dtype):\n+      jnp_fun = getattr(jnp, name)\n+      np_fun = getattr(np, name)\n+\n+      # Skip if the ufunc doesn't have an identity and we're doing a multi-axis reduction\n+      if jnp_fun.identity is None and axis is None and len(shape) > 1:\n+          self.skipTest(\"Multiple-axis reduction over non-reorderable ufunc.\")\n+\n+      jnp_fun_reduce = lambda a, where, initial: jnp_fun.reduce(\n+          a, axis=axis, where=where, initial=initial)\n+      np_fun_reduce = lambda a, where, initial: np_fun.reduce(\n+          a, axis=axis, where=where, initial=initial)\n+\n+      rng = jtu.rand_default(self.rng())\n+      rng_where = jtu.rand_bool(self.rng())\n+      rng_initial = jtu.rand_default(self.rng())\n+      args_maker = lambda: [\n+          rng(shape, dtype),\n+          rng_where(shape, bool),\n+          rng_initial((), dtype)\n+      ]\n+\n+      tol = {np.float32: 1E-4} if jtu.test_device_matches(['tpu']) else None\n+\n+      self._CheckAgainstNumpy(jnp_fun_reduce, np_fun_reduce, args_maker, tol=tol)\n+      self._CompileAndCheck(jnp_fun_reduce, args_maker)\n+\n   @jtu.sample_product(\n       SCALAR_FUNCS,\n       [{'shape': shape, 'axis': axis}\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update workflow files to use new ml-build containers.\n\nPiperOrigin-RevId: 766270515",
        "output": "```diff\nCommit: 62ab725780bd4f3bf0af54bd60a7360de00205aa\nDate: 2025-06-02T18:20:38Z\nURL: https://github.com/jax-ml/jax/commit/62ab725780bd4f3bf0af54bd60a7360de00205aa\nFiles changed: 5\nAdditions: +6, Deletions: -6\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex cc3ae89d97f9..65a7b7b6a01f 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -38,7 +38,7 @@ jobs:\n         shell: bash\n     runs-on: ${{ inputs.runner }}\n     container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest') ||\n-                   (contains(inputs.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') }}\n+                   (contains(inputs.runner, 'linux-arm64') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-arm64:latest') }}\n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: ${{ inputs.python }}\n       JAXCI_ENABLE_X64: ${{ inputs.enable-x64 }}\ndiff --git a/.github/workflows/bazel_cpu_rbe.yml b/.github/workflows/bazel_cpu_rbe.yml\nindex 3eff0932adcb..71c140464454 100644\n--- a/.github/workflows/bazel_cpu_rbe.yml\n+++ b/.github/workflows/bazel_cpu_rbe.yml\n@@ -29,7 +29,7 @@ jobs:\n     if: github.event.repository.fork == false\n     runs-on: ${{ matrix.runner }}\n     container: ${{ (contains(matrix.runner, 'linux-x86') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest') ||\n-                   (contains(matrix.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') }}\n+                   (contains(matrix.runner, 'linux-arm64') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-arm64:latest') }}\n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: ${{ matrix.python }}\n       JAXCI_ENABLE_X64: ${{ matrix.enable-x_64 }}\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 95ab90412494..ece2237eeead 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -90,7 +90,7 @@ jobs:\n     runs-on: ${{ inputs.runner }}\n \n     container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest') ||\n-                   (contains(inputs.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') ||\n+                   (contains(inputs.runner, 'linux-arm64') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-arm64:latest') ||\n                    (contains(inputs.runner, 'windows-x86') && null) }}\n \n     env:\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex 95086257c62b..d23c1f543827 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -47,7 +47,7 @@ jobs:\n         shell: bash\n     runs-on: ${{ inputs.runner }}\n     container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest') ||\n-                   (contains(inputs.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') ||\n+                   (contains(inputs.runner, 'linux-arm64') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-arm64:latest') ||\n                    (contains(inputs.runner, 'windows-x86') && null) }}\n \n     name: \"${{ (contains(inputs.runner, 'linux-x86') && 'linux x86') ||\ndiff --git a/ci/envs/docker.env b/ci/envs/docker.env\nindex 5135b61ac45b..d556cb82d74d 100644\n--- a/ci/envs/docker.env\n+++ b/ci/envs/docker.env\n@@ -35,11 +35,11 @@ fi\n # Linux Aarch64 image for building JAX artifacts, running Pytests CPU tests, and\n # Bazel tests\n if [[ $os == \"linux\" ]] && [[ $arch == \"aarch64\" ]]; then\n-  export JAXCI_DOCKER_IMAGE=\"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest\"\n+  export JAXCI_DOCKER_IMAGE=\"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-arm64:latest\"\n fi\n \n # Windows image for building JAX artifacts, running Pytests CPU tests, and Bazel\n # tests\n if [[ $os =~ \"msys_nt\" ]]; then\n-  export JAXCI_DOCKER_IMAGE=\"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/tf-test-windows:latest\"\n+  export JAXCI_DOCKER_IMAGE=\"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/tf-test-windows:latest\"\n fi\n\\ No newline at end of file\n\n```",
        "from_id": [
            "quoctruong",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29168 from froystig:pallas-call-eager\n\nPiperOrigin-RevId: 766268347",
        "output": "```diff\nCommit: 0347b668ecac629bc88cb9644df864794d74ac8f\nDate: 2025-06-02T18:15:57Z\nURL: https://github.com/jax-ml/jax/commit/0347b668ecac629bc88cb9644df864794d74ac8f\nFiles changed: 2\nAdditions: +19, Deletions: -32\ndiff --git a/jax/_src/pallas/pallas_call.py b/jax/_src/pallas/pallas_call.py\nindex 6f8c96a4591c..016bac96424e 100644\n--- a/jax/_src/pallas/pallas_call.py\n+++ b/jax/_src/pallas/pallas_call.py\n@@ -77,16 +77,12 @@\n \n def _pallas_call_impl(*args, **params):\n   # Call the lowering path\n-  if config.disable_jit.value:\n-    raise NotImplementedError(\n-        \"pallas_call not supported with disable_jit. Consider invoking under a\"\n-        \" local context of `jax.disable_jit(False)`.\"\n-    )\n-\n   @partial(jax.jit, inline=True)\n   def _jit_run(*args):\n     return pallas_call_p.bind(*args, **params)\n-  return _jit_run(*args)\n+\n+  with config.disable_jit(False):\n+    return _jit_run(*args)\n \n pallas_call_p.def_impl(_pallas_call_impl)\n \ndiff --git a/tests/pallas/pallas_test.py b/tests/pallas/pallas_test.py\nindex 03399e12b609..cb61d5648912 100644\n--- a/tests/pallas/pallas_test.py\n+++ b/tests/pallas/pallas_test.py\n@@ -692,6 +692,22 @@ def f(x):\n     self.assertEqual(f(x), 2.)\n     self.assertEqual(trace_count, 1)\n \n+  def test_pallas_call_under_disable_jit(self):\n+    @functools.partial(\n+        self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.float32),\n+    )\n+    def add_one(x_ref, o_ref):\n+      o_ref[...] = x_ref[...] + 1.\n+\n+    x = jnp.arange(8, dtype=jnp.float32)\n+\n+    result = add_one(x)\n+    np.testing.assert_array_equal(result, x + 1.)\n+\n+    with jax.disable_jit():\n+      result = add_one(x)\n+      np.testing.assert_array_equal(result, x + 1.)\n+\n   @parameterized.parameters(\n       (\"float32\", None),\n       (\"float32\", jax.lax.Precision.DEFAULT),\n@@ -1261,31 +1277,6 @@ def dot_general_kernel(x_ref, y_ref, o_ref):\n     ):\n       dot_general_kernel(x, y)\n \n-  def test_jax_disable_jit(self):\n-    def add_vectors_kernel(x_ref, y_ref, o_ref):\n-      x, y = x_ref[...], y_ref[...]\n-      o_ref[...] = x + y\n-\n-    @jax.jit\n-    def add_vectors(x: jax.Array, y: jax.Array) -> jax.Array:\n-      return self.pallas_call(\n-          add_vectors_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype)\n-      )(x, y)\n-\n-    # Prove kernel works fine without disable_jit.\n-    add_vectors(jnp.arange(8), jnp.arange(8))\n-\n-    with self.assertRaisesRegex(\n-        NotImplementedError, \"pallas_call not supported with disable_jit.\"\n-    ):\n-      with jax.disable_jit():\n-        add_vectors(jnp.arange(8.0), jnp.arange(8.0))\n-\n-    with jax.disable_jit():\n-      # We instructed the user to do this, so this should not raise an error.\n-      with jax.disable_jit(False):\n-        add_vectors(jnp.arange(8.0), jnp.arange(8.0))\n-\n \n class ApiErrorInterpretTest(ApiErrorTest):\n   INTERPRET = True\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29170 from jakevdp:complex-warning\n\nPiperOrigin-RevId: 766265412",
        "output": "```diff\nCommit: 9a32fabcfdf74216f7314d2bb12eceeb4a27c1e7\nDate: 2025-06-02T18:08:14Z\nURL: https://github.com/jax-ml/jax/commit/9a32fabcfdf74216f7314d2bb12eceeb4a27c1e7\nFiles changed: 9\nAdditions: +34, Deletions: -40\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex 68363d10bc04..e03951eb4730 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -70,7 +70,7 @@\n                                      ShardingContext, SPMDAxisContext,\n                                      PartitionSpec as P, canonicalize_sharding)\n from jax._src.typing import Array, ArrayLike, DimSize, DuckTypedArray, DTypeLike, Shape\n-from jax._src.util import (NumpyComplexWarning, cache, canonicalize_axis,\n+from jax._src.util import (cache, canonicalize_axis,\n                            safe_map, safe_zip, split_list, weakref_lru_cache,\n                            foreach)\n \n@@ -1706,7 +1706,7 @@ def _convert_element_type(\n       dtypes.issubdtype(old_dtype, np.complexfloating) and\n       not dtypes.issubdtype(new_dtype, np.complexfloating)):\n     msg = \"Casting complex values to real discards the imaginary part\"\n-    warnings.warn(msg, NumpyComplexWarning, stacklevel=2)\n+    warnings.warn(msg, np.exceptions.ComplexWarning, stacklevel=2)\n \n   # Python has big integers, but convert_element_type(2 ** 100, np.float32) need\n   # not be an error since the target dtype fits the value. Handle this case by\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex ad2b3ad6aa75..b21fcfeb5772 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -62,7 +62,7 @@\n   Array, ArrayLike, DType, DTypeLike, DeprecatedArg, DimSize, Shape, SupportsShape\n )\n from jax._src.util import (\n-    NumpyComplexWarning, canonicalize_axis as _canonicalize_axis,\n+    canonicalize_axis as _canonicalize_axis,\n     ceil_of_ratio, safe_zip, set_module, unzip2)\n from jax.sharding import Sharding\n from jax._src.sharding_impls import NamedSharding, PartitionSpec as P\n@@ -171,7 +171,7 @@ def _dtype(x: Any) -> DType:\n can_cast = dtypes.can_cast\n promote_types = dtypes.promote_types\n \n-ComplexWarning = NumpyComplexWarning\n+ComplexWarning = np.exceptions.ComplexWarning\n \n _lax_const = lax_internal._const\n \ndiff --git a/jax/_src/util.py b/jax/_src/util.py\nindex 4100ac21dc00..71d8f8bfa6a1 100644\n--- a/jax/_src/util.py\n+++ b/jax/_src/util.py\n@@ -642,9 +642,6 @@ def decorator(f):\n   return decorator\n \n \n-NumpyComplexWarning: type[Warning] = np.exceptions.ComplexWarning\n-\n-\n class StrictABCMeta(abc.ABCMeta):\n   \"\"\"A variant of `abc.ABCMeta` which does not allow virtual subclasses.\n \ndiff --git a/tests/lax_autodiff_test.py b/tests/lax_autodiff_test.py\nindex aea9d2ad3dff..a6398e402df9 100644\n--- a/tests/lax_autodiff_test.py\n+++ b/tests/lax_autodiff_test.py\n@@ -28,7 +28,6 @@\n from jax import dtypes\n from jax import lax\n from jax._src import test_util as jtu\n-from jax._src.util import NumpyComplexWarning\n from jax.test_util import check_grads\n \n jax.config.parse_flags_with_absl()\n@@ -244,7 +243,7 @@ def testConvertElementTypeGrad(self, from_dtype, to_dtype):\n               jtu.tolerance(from_dtype, jtu.default_gradient_tolerance))\n     args = (rng((2, 3), from_dtype),)\n     convert_element_type = lambda x: lax.convert_element_type(x, to_dtype)\n-    convert_element_type = jtu.ignore_warning(category=NumpyComplexWarning)(\n+    convert_element_type = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(\n       convert_element_type)\n     check_grads(convert_element_type, args, 2, [\"fwd\", \"rev\"], tol, tol, eps=1.)\n \ndiff --git a/tests/lax_metal_test.py b/tests/lax_metal_test.py\nindex 5f1781c3be06..e44ff9ebc930 100644\n--- a/tests/lax_metal_test.py\n+++ b/tests/lax_metal_test.py\n@@ -48,7 +48,7 @@\n from jax._src import test_util as jtu\n from jax._src.lax import lax as lax_internal\n \n-from jax._src.util import safe_zip, NumpyComplexWarning\n+from jax._src.util import safe_zip\n \n try:\n   from jax_plugins import metal_plugin\n@@ -2099,11 +2099,11 @@ def testCumSumProd(self, axis, shape, dtype, out_dtype, op):\n     np_op = getattr(np, op)\n     rng = jtu.rand_default(self.rng())\n     np_fun = lambda arg: np_op(arg, axis=axis, dtype=out_dtype)\n-    np_fun = jtu.ignore_warning(category=NumpyComplexWarning)(np_fun)\n+    np_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(np_fun)\n     np_fun = jtu.ignore_warning(category=RuntimeWarning,\n                                 message=\"overflow encountered.*\")(np_fun)\n     jnp_fun = lambda arg: jnp_op(arg, axis=axis, dtype=out_dtype)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n \n     args_maker = lambda: [rng(shape, dtype)]\n \n@@ -2127,11 +2127,11 @@ def testNanCumSumProd(self, axis, shape, dtype, out_dtype, op):\n     np_op = getattr(np, op)\n     rng = jtu.rand_some_nan(self.rng())\n     np_fun = partial(np_op, axis=axis, dtype=out_dtype)\n-    np_fun = jtu.ignore_warning(category=NumpyComplexWarning)(np_fun)\n+    np_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(np_fun)\n     np_fun = jtu.ignore_warning(category=RuntimeWarning,\n                                 message=\"overflow encountered.*\")(np_fun)\n     jnp_fun = partial(jnp_op, axis=axis, dtype=out_dtype)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n \n     args_maker = lambda: [rng(shape, dtype)]\n \ndiff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py\nindex ca9ba9c88806..745cab59cf1b 100644\n--- a/tests/lax_numpy_indexing_test.py\n+++ b/tests/lax_numpy_indexing_test.py\n@@ -35,7 +35,6 @@\n from jax._src import test_util as jtu\n from jax._src import util\n from jax._src.lax import lax as lax_internal\n-from jax._src.util import NumpyComplexWarning\n \n config.parse_flags_with_absl()\n \n@@ -1186,7 +1185,7 @@ def _check(x_type, y_type):\n       out = x.at[0].set(y)\n       self.assertEqual(x.dtype, out.dtype)\n \n-    @jtu.ignore_warning(category=NumpyComplexWarning,\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning,\n                         message=\"Casting complex values to real\")\n     def _check_warns(x_type, y_type, msg):\n       with self.assertWarnsRegex(FutureWarning, msg):\ndiff --git a/tests/lax_numpy_reducers_test.py b/tests/lax_numpy_reducers_test.py\nindex aa5e08e96a3e..93aff25c6f8e 100644\n--- a/tests/lax_numpy_reducers_test.py\n+++ b/tests/lax_numpy_reducers_test.py\n@@ -29,7 +29,6 @@\n from jax._src import config\n from jax._src import dtypes\n from jax._src import test_util as jtu\n-from jax._src.util import NumpyComplexWarning\n \n config.parse_flags_with_absl()\n \n@@ -209,7 +208,7 @@ def testReducer(self, name, rng_factory, shape, dtype, out_dtype,\n     np_op = getattr(np, name)\n     jnp_op = getattr(jnp, name)\n     rng = rng_factory(self.rng())\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Mean of empty slice.*\")\n     @jtu.ignore_warning(category=RuntimeWarning,\n@@ -225,7 +224,7 @@ def np_fun(x):\n       return np_op(x_cast, axis, dtype=t, keepdims=keepdims)\n \n     jnp_fun = lambda x: jnp_op(x, axis, dtype=out_dtype, keepdims=keepdims)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n     args_maker = lambda: [rng(shape, dtype)]\n     tol_spec = {np.float16: 1e-2, np.int16: 2e-7, np.int32: 1E-3,\n                 np.uint32: 3e-7, np.float32: 1e-3, np.complex64: 1e-3,\n@@ -313,7 +312,7 @@ def testReducerInitial(self, name, rng_factory, shape, dtype, axis,\n     is_bf16_nan_test = dtype == jnp.bfloat16 and rng_factory.__name__ == 'rand_some_nan'\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Degrees of freedom <= 0 for slice.*\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       x = np.asarray(x)\n       if inexact:\n@@ -324,7 +323,7 @@ def np_fun(x):\n       return res.astype(_reducer_output_dtype(name, x.dtype))\n \n     jnp_fun = lambda x: jnp_op(x, axis, keepdims=keepdims, initial=initial)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n     args_maker = lambda: [rng(shape, dtype)]\n     tol = {jnp.bfloat16: 3E-2}\n     self._CheckAgainstNumpy(np_fun, jnp_fun, args_maker, rtol=tol, atol=tol)\n@@ -353,7 +352,7 @@ def testReducerPromoteInt(self, name, rng_factory, shape, dtype, axis,\n                         rng_factory.__name__ == 'rand_some_nan')\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Degrees of freedom <= 0 for slice.*\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       x = np.asarray(x)\n       if inexact:\n@@ -364,7 +363,7 @@ def np_fun(x):\n       return res.astype(_reducer_output_dtype(name, x.dtype, promote_integers))\n \n     jnp_fun = lambda x: jnp_op(x, axis, keepdims=keepdims, initial=initial, promote_integers=promote_integers)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n     args_maker = lambda: [rng(shape, dtype)]\n     tol = {jnp.bfloat16: 3E-2, jnp.float16: 5e-3}\n     self._CheckAgainstNumpy(np_fun, jnp_fun, args_maker, rtol=tol)\n@@ -390,7 +389,7 @@ def testReducerNoInitialZeroDims(self, name, rng_factory, shape, dtype, axis,\n     is_bf16_nan_test = dtype == jnp.bfloat16 and rng_factory.__name__ == 'rand_some_nan'\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Degrees of freedom <= 0 for slice.*\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       x = np.asarray(x)\n       if inexact:\n@@ -401,7 +400,7 @@ def np_fun(x):\n       return res.astype(_reducer_output_dtype(name, x.dtype))\n \n     jnp_fun = lambda x: jnp_op(x, axis, keepdims=keepdims)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n     args_maker = lambda: [rng(shape, dtype)]\n     tol = {jnp.bfloat16: 3E-2}\n     self._CheckAgainstNumpy(np_fun, jnp_fun, args_maker, rtol=tol)\n@@ -436,7 +435,7 @@ def testReducerWhere(self, name, rng_factory, shape, dtype, axis,\n     where = jtu.rand_bool(self.rng())(whereshape, np.bool_)\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Degrees of freedom <= 0 for slice.*\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       x = np.asarray(x)\n       if inexact:\n@@ -447,7 +446,7 @@ def np_fun(x):\n       return res.astype(_reducer_output_dtype(name, x.dtype))\n \n     jnp_fun = lambda x: jnp_op(x, axis, keepdims=keepdims, initial=initial, where=where)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n     args_maker = lambda: [rng(shape, dtype)]\n     self._CheckAgainstNumpy(np_fun, jnp_fun, args_maker, atol=tol, rtol=tol)\n     self._CompileAndCheck(jnp_fun, args_maker)\n@@ -499,7 +498,7 @@ def testReducerWhereNoInitial(self, name, rng_factory, shape, dtype, axis,\n                         message=\"Mean of empty slice.*\")\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"invalid value encountered.*\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       x = np.asarray(x)\n       if inexact:\n@@ -510,7 +509,7 @@ def np_fun(x):\n       return res\n \n     jnp_fun = lambda x: jnp_op(x, axis, keepdims=keepdims, where=where)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n     args_maker = lambda: [rng(shape, dtype)]\n     self._CheckAgainstNumpy(np_fun, jnp_fun, args_maker, atol=tol, rtol=tol)\n     self._CompileAndCheck(jnp_fun, args_maker)\n@@ -574,7 +573,7 @@ def testStdOrVar(self, test_fns, shape, dtype, out_dtype, axis, ddof_correction,\n     args_maker = self._GetArgsMaker(rng, [shape], [dtype])\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Degrees of freedom <= 0 for slice.\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       # setup ddof and correction kwargs excluding case when correction is not specified\n       ddof_correction_kwargs = {\"ddof\": ddof}\n@@ -625,7 +624,7 @@ def testNanVar(self, shape, dtype, out_dtype, axis, ddof, keepdims):\n     args_maker = self._GetArgsMaker(rng, [shape], [dtype])\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Degrees of freedom <= 0 for slice.\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       # Numpy fails with bfloat16 inputs\n       out = np.nanvar(x.astype(np.float32 if dtype == dtypes.bfloat16 else dtype),\n@@ -834,7 +833,7 @@ def test_f16_mean(self, dtype):\n     ],\n     include_initial=[False, True],\n   )\n-  @jtu.ignore_warning(category=NumpyComplexWarning)\n+  @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n   @jax.numpy_dtype_promotion('standard')  # This test explicitly exercises mixed type promotion\n   def testCumulativeSum(self, shape, axis, dtype, out_dtype, include_initial):\n     rng = jtu.rand_some_zero(self.rng())\n@@ -902,7 +901,7 @@ def testCumulativeSumBool(self):\n     ],\n     include_initial=[False, True],\n   )\n-  @jtu.ignore_warning(category=NumpyComplexWarning)\n+  @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n   @jax.numpy_dtype_promotion('standard')  # This test explicitly exercises mixed type promotion\n   def testCumulativeProd(self, shape, axis, dtype, out_dtype, include_initial):\n     if jtu.is_device_tpu_at_least(6):\ndiff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py\nindex 16234463d795..80d1d4161cc5 100644\n--- a/tests/lax_numpy_test.py\n+++ b/tests/lax_numpy_test.py\n@@ -50,7 +50,7 @@\n from jax._src import dtypes\n from jax._src import test_util as jtu\n from jax._src.lax import lax as lax_internal\n-from jax._src.util import safe_zip, NumpyComplexWarning, tuple_update\n+from jax._src.util import safe_zip, tuple_update\n \n config.parse_flags_with_absl()\n \n@@ -2354,11 +2354,11 @@ def testCumSumProd(self, axis, shape, dtype, out_dtype, op):\n     np_op = getattr(np, op)\n     rng = jtu.rand_default(self.rng())\n     np_fun = lambda arg: np_op(arg, axis=axis, dtype=out_dtype)\n-    np_fun = jtu.ignore_warning(category=NumpyComplexWarning)(np_fun)\n+    np_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(np_fun)\n     np_fun = jtu.ignore_warning(category=RuntimeWarning,\n                                 message=\"overflow encountered.*\")(np_fun)\n     jnp_fun = lambda arg: jnp_op(arg, axis=axis, dtype=out_dtype)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n \n     args_maker = lambda: [rng(shape, dtype)]\n \n@@ -2382,11 +2382,11 @@ def testNanCumSumProd(self, axis, shape, dtype, out_dtype, op):\n     np_op = getattr(np, op)\n     rng = jtu.rand_some_nan(self.rng())\n     np_fun = partial(np_op, axis=axis, dtype=out_dtype)\n-    np_fun = jtu.ignore_warning(category=NumpyComplexWarning)(np_fun)\n+    np_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(np_fun)\n     np_fun = jtu.ignore_warning(category=RuntimeWarning,\n                                 message=\"overflow encountered.*\")(np_fun)\n     jnp_fun = partial(jnp_op, axis=axis, dtype=out_dtype)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n \n     args_maker = lambda: [rng(shape, dtype)]\n \ndiff --git a/tests/lax_test.py b/tests/lax_test.py\nindex 6792b08c37fa..a11c989fc9c5 100644\n--- a/tests/lax_test.py\n+++ b/tests/lax_test.py\n@@ -48,7 +48,7 @@\n from jax._src.interpreters import pxla\n from jax._src.internal_test_util import lax_test_util\n from jax._src.lax import lax as lax_internal\n-from jax._src.util import NumpyComplexWarning, safe_zip\n+from jax._src.util import safe_zip\n from jax._src.tree_util import tree_map\n \n config.parse_flags_with_absl()\n@@ -3744,7 +3744,7 @@ def testConvertElementReturnType(self, input_type, dtype, value, jit):\n \n   @jtu.sample_product(\n       dtype_in=lax_test_util.all_dtypes, dtype_out=lax_test_util.all_dtypes)\n-  @jtu.ignore_warning(category=NumpyComplexWarning)\n+  @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n   def testConvertElementTypeAvoidsCopies(self, dtype_in, dtype_out):\n     x = jax.device_put(np.zeros(5, dtype_in))\n     self.assertEqual(x.dtype, dtype_in)\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix native tiling logic in infer_vector_layout.\nFor the pattern `arith.trunci` ->`tpu.bitcast` -> `tpu.matmul`, there will be a `tpu.relayout` op after `arith.trunci` before the fix, which has a negative impact on the performance e2e.\n\nPiperOrigin-RevId: 766256767",
        "output": "```diff\nCommit: a43ccbb9df2a8ed6acb5b9837116cbe3b6a298df\nDate: 2025-06-02T17:48:51Z\nURL: https://github.com/jax-ml/jax/commit/a43ccbb9df2a8ed6acb5b9837116cbe3b6a298df\nFiles changed: 3\nAdditions: +35, Deletions: -6\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex 14d1fb2104fa..17575183bd81 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -1952,12 +1952,11 @@ class VectorLayoutInferer {\n   }\n \n   bool allUsersRequireNativeTiling(Value x) {\n-    for (OpOperand &operand : x.getUses()) {\n-      if (isa<tpu::MatmulOp>(operand.getOwner())) {\n+    for (Operation *user : getNontrivialTransitiveUsers(x)) {\n+      if (isa<tpu::MatmulOp>(user)) {\n         continue;\n       }\n-      if (auto reduce =\n-              dyn_cast<vector::MultiDimReductionOp>(operand.getOwner())) {\n+      if (auto reduce = dyn_cast<vector::MultiDimReductionOp>(user)) {\n         bool reduces_tiled_dims = false;\n         for (int64_t dim : reduce.getReductionDims()) {\n           if (dim >= reduce.getSourceVectorType().getRank() - 2) {\n@@ -1969,7 +1968,7 @@ class VectorLayoutInferer {\n           continue;\n         }\n       }\n-      if (auto transpose = dyn_cast<tpu::TransposeOp>(operand.getOwner())) {\n+      if (auto transpose = dyn_cast<tpu::TransposeOp>(user)) {\n         auto perm = transpose.getPermutation();\n         auto rank = perm.size();\n         // Only permutations that actually swap the last two dims need it.\n@@ -1979,7 +1978,7 @@ class VectorLayoutInferer {\n         }\n         // Fall through.\n       }\n-      if (auto store = dyn_cast<vector::StoreOp>(operand.getOwner())) {\n+      if (auto store = dyn_cast<vector::StoreOp>(user)) {\n         auto maybe_tiling = verifyMemoryTiling(\n             store, getMemRefLayout(store.getBase()).getTiles(),\n             store.getMemRefType().getRank(),\ndiff --git a/jaxlib/mosaic/dialect/tpu/util.cc b/jaxlib/mosaic/dialect/tpu/util.cc\nindex 02598bd16f9a..0e67b4299f7e 100644\n--- a/jaxlib/mosaic/dialect/tpu/util.cc\n+++ b/jaxlib/mosaic/dialect/tpu/util.cc\n@@ -32,6 +32,7 @@ limitations under the License.\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n+#include \"mlir/IR/OpDefinition.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/IR/ValueRange.h\"\n #include \"mlir/Support/LLVM.h\"\n@@ -313,4 +314,27 @@ bool canFoldMinorDimsToSize(ArrayRef<int64_t> shape, int64_t target_size) {\n   return product == target_size;\n }\n \n+SmallVector<Operation *> getNontrivialTransitiveUsers(Value v) {\n+  auto isUnaryElementwise = [](Operation *op) {\n+    if (!op->hasTrait<mlir::OpTrait::Elementwise>()) {\n+      return false;\n+    }\n+    return op->getNumOperands() == 1 && op->getNumResults() == 1;\n+  };\n+  SmallVector<Operation *> users;\n+  SmallVector<Value> candidates;\n+  candidates.push_back(v);\n+  while (!candidates.empty()) {\n+    Value candidate = candidates.back();\n+    candidates.pop_back();\n+    for (const auto &user : candidate.getUsers()) {\n+      if (isa<tpu::BitcastOp>(user) || isUnaryElementwise(user))\n+        candidates.push_back(user->getResult(0));\n+      else\n+        users.push_back(user);\n+    }\n+  }\n+  return users;\n+}\n+\n }  // namespace mlir::tpu\ndiff --git a/jaxlib/mosaic/dialect/tpu/util.h b/jaxlib/mosaic/dialect/tpu/util.h\nindex 2a7325ee7b24..af590f45f619 100644\n--- a/jaxlib/mosaic/dialect/tpu/util.h\n+++ b/jaxlib/mosaic/dialect/tpu/util.h\n@@ -290,6 +290,12 @@ std::optional<int64_t> getIntConst(Value v);\n // Precondition: `shape` has at least 2 dimensions.\n bool canFoldMinorDimsToSize(ArrayRef<int64_t> shape, int64_t target_size);\n \n+// Recursively finds all non-trivial users of a given value, including those\n+// accessed via `tpu.bitcast` or unary elementwise operations. However,\n+// `tpu.bitcast` and unary element-wise operations are excluded from the\n+// results.\n+SmallVector<Operation *> getNontrivialTransitiveUsers(Value v);\n+\n }  // namespace mlir::tpu\n \n #endif  // THIRD_PARTY_PY_JAX_JAXLIB_MOSAIC_DIALECT_TPU_UTIL_H_\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "always compile Pallas calls, enabling `pallas_call` under `disable_jit`\n\nHow should `pallas_call` behave under `disable_jit`? We could:\n\na) Error because we don't know what's expected.\nb) Run `pallas_call` as we would in eager mode.\nc) Run `pallas_call` in interpret mode.\n\nToday we do (a). This change implements (b) instead, where by \"eager\nmode\" we mean the behavior under no `jit` (and no `disable_jit` under\nit). Choice (c) seems to take things too far. On the one hand, it\nwould execute the Pallas kernel \"op-by-op,\" which may seem\ndesirable. On the other hand, it would execute each individual such op\non the host (CPU) rather than the device (accelerator). If we could do\nthe first \"half\" of (c) alone somehow -- executing op-by-op,\ndispatching each op on the device, that may be ideal. Until then, (b)\nseems closer to expectations.\n\nTo implement (b), this change simply re-enables jit only for the\nduration of the pallas call.",
        "output": "```diff\nCommit: 980f5dc49a09a3b4dc8e4870dfd838854f356b85\nDate: 2025-06-02T17:42:41Z\nURL: https://github.com/jax-ml/jax/commit/980f5dc49a09a3b4dc8e4870dfd838854f356b85\nFiles changed: 2\nAdditions: +19, Deletions: -32\ndiff --git a/jax/_src/pallas/pallas_call.py b/jax/_src/pallas/pallas_call.py\nindex 6f8c96a4591c..016bac96424e 100644\n--- a/jax/_src/pallas/pallas_call.py\n+++ b/jax/_src/pallas/pallas_call.py\n@@ -77,16 +77,12 @@\n \n def _pallas_call_impl(*args, **params):\n   # Call the lowering path\n-  if config.disable_jit.value:\n-    raise NotImplementedError(\n-        \"pallas_call not supported with disable_jit. Consider invoking under a\"\n-        \" local context of `jax.disable_jit(False)`.\"\n-    )\n-\n   @partial(jax.jit, inline=True)\n   def _jit_run(*args):\n     return pallas_call_p.bind(*args, **params)\n-  return _jit_run(*args)\n+\n+  with config.disable_jit(False):\n+    return _jit_run(*args)\n \n pallas_call_p.def_impl(_pallas_call_impl)\n \ndiff --git a/tests/pallas/pallas_test.py b/tests/pallas/pallas_test.py\nindex 03399e12b609..cb61d5648912 100644\n--- a/tests/pallas/pallas_test.py\n+++ b/tests/pallas/pallas_test.py\n@@ -692,6 +692,22 @@ def f(x):\n     self.assertEqual(f(x), 2.)\n     self.assertEqual(trace_count, 1)\n \n+  def test_pallas_call_under_disable_jit(self):\n+    @functools.partial(\n+        self.pallas_call, out_shape=jax.ShapeDtypeStruct((8,), jnp.float32),\n+    )\n+    def add_one(x_ref, o_ref):\n+      o_ref[...] = x_ref[...] + 1.\n+\n+    x = jnp.arange(8, dtype=jnp.float32)\n+\n+    result = add_one(x)\n+    np.testing.assert_array_equal(result, x + 1.)\n+\n+    with jax.disable_jit():\n+      result = add_one(x)\n+      np.testing.assert_array_equal(result, x + 1.)\n+\n   @parameterized.parameters(\n       (\"float32\", None),\n       (\"float32\", jax.lax.Precision.DEFAULT),\n@@ -1261,31 +1277,6 @@ def dot_general_kernel(x_ref, y_ref, o_ref):\n     ):\n       dot_general_kernel(x, y)\n \n-  def test_jax_disable_jit(self):\n-    def add_vectors_kernel(x_ref, y_ref, o_ref):\n-      x, y = x_ref[...], y_ref[...]\n-      o_ref[...] = x + y\n-\n-    @jax.jit\n-    def add_vectors(x: jax.Array, y: jax.Array) -> jax.Array:\n-      return self.pallas_call(\n-          add_vectors_kernel, out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype)\n-      )(x, y)\n-\n-    # Prove kernel works fine without disable_jit.\n-    add_vectors(jnp.arange(8), jnp.arange(8))\n-\n-    with self.assertRaisesRegex(\n-        NotImplementedError, \"pallas_call not supported with disable_jit.\"\n-    ):\n-      with jax.disable_jit():\n-        add_vectors(jnp.arange(8.0), jnp.arange(8.0))\n-\n-    with jax.disable_jit():\n-      # We instructed the user to do this, so this should not raise an error.\n-      with jax.disable_jit(False):\n-        add_vectors(jnp.arange(8.0), jnp.arange(8.0))\n-\n \n class ApiErrorInterpretTest(ApiErrorTest):\n   INTERPRET = True\n\n```",
        "from_id": [
            "froystig"
        ]
    },
    {
        "text_input": "[cleanup] inline uses of NumpyComplexWarning",
        "output": "```diff\nCommit: 8eaa9bf19ae0245bb518a3ef9479842cf761be67\nDate: 2025-06-02T17:23:02Z\nURL: https://github.com/jax-ml/jax/commit/8eaa9bf19ae0245bb518a3ef9479842cf761be67\nFiles changed: 9\nAdditions: +34, Deletions: -40\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex 68363d10bc04..e03951eb4730 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -70,7 +70,7 @@\n                                      ShardingContext, SPMDAxisContext,\n                                      PartitionSpec as P, canonicalize_sharding)\n from jax._src.typing import Array, ArrayLike, DimSize, DuckTypedArray, DTypeLike, Shape\n-from jax._src.util import (NumpyComplexWarning, cache, canonicalize_axis,\n+from jax._src.util import (cache, canonicalize_axis,\n                            safe_map, safe_zip, split_list, weakref_lru_cache,\n                            foreach)\n \n@@ -1706,7 +1706,7 @@ def _convert_element_type(\n       dtypes.issubdtype(old_dtype, np.complexfloating) and\n       not dtypes.issubdtype(new_dtype, np.complexfloating)):\n     msg = \"Casting complex values to real discards the imaginary part\"\n-    warnings.warn(msg, NumpyComplexWarning, stacklevel=2)\n+    warnings.warn(msg, np.exceptions.ComplexWarning, stacklevel=2)\n \n   # Python has big integers, but convert_element_type(2 ** 100, np.float32) need\n   # not be an error since the target dtype fits the value. Handle this case by\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex ad2b3ad6aa75..b21fcfeb5772 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -62,7 +62,7 @@\n   Array, ArrayLike, DType, DTypeLike, DeprecatedArg, DimSize, Shape, SupportsShape\n )\n from jax._src.util import (\n-    NumpyComplexWarning, canonicalize_axis as _canonicalize_axis,\n+    canonicalize_axis as _canonicalize_axis,\n     ceil_of_ratio, safe_zip, set_module, unzip2)\n from jax.sharding import Sharding\n from jax._src.sharding_impls import NamedSharding, PartitionSpec as P\n@@ -171,7 +171,7 @@ def _dtype(x: Any) -> DType:\n can_cast = dtypes.can_cast\n promote_types = dtypes.promote_types\n \n-ComplexWarning = NumpyComplexWarning\n+ComplexWarning = np.exceptions.ComplexWarning\n \n _lax_const = lax_internal._const\n \ndiff --git a/jax/_src/util.py b/jax/_src/util.py\nindex 4100ac21dc00..71d8f8bfa6a1 100644\n--- a/jax/_src/util.py\n+++ b/jax/_src/util.py\n@@ -642,9 +642,6 @@ def decorator(f):\n   return decorator\n \n \n-NumpyComplexWarning: type[Warning] = np.exceptions.ComplexWarning\n-\n-\n class StrictABCMeta(abc.ABCMeta):\n   \"\"\"A variant of `abc.ABCMeta` which does not allow virtual subclasses.\n \ndiff --git a/tests/lax_autodiff_test.py b/tests/lax_autodiff_test.py\nindex aea9d2ad3dff..a6398e402df9 100644\n--- a/tests/lax_autodiff_test.py\n+++ b/tests/lax_autodiff_test.py\n@@ -28,7 +28,6 @@\n from jax import dtypes\n from jax import lax\n from jax._src import test_util as jtu\n-from jax._src.util import NumpyComplexWarning\n from jax.test_util import check_grads\n \n jax.config.parse_flags_with_absl()\n@@ -244,7 +243,7 @@ def testConvertElementTypeGrad(self, from_dtype, to_dtype):\n               jtu.tolerance(from_dtype, jtu.default_gradient_tolerance))\n     args = (rng((2, 3), from_dtype),)\n     convert_element_type = lambda x: lax.convert_element_type(x, to_dtype)\n-    convert_element_type = jtu.ignore_warning(category=NumpyComplexWarning)(\n+    convert_element_type = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(\n       convert_element_type)\n     check_grads(convert_element_type, args, 2, [\"fwd\", \"rev\"], tol, tol, eps=1.)\n \ndiff --git a/tests/lax_metal_test.py b/tests/lax_metal_test.py\nindex 5f1781c3be06..e44ff9ebc930 100644\n--- a/tests/lax_metal_test.py\n+++ b/tests/lax_metal_test.py\n@@ -48,7 +48,7 @@\n from jax._src import test_util as jtu\n from jax._src.lax import lax as lax_internal\n \n-from jax._src.util import safe_zip, NumpyComplexWarning\n+from jax._src.util import safe_zip\n \n try:\n   from jax_plugins import metal_plugin\n@@ -2099,11 +2099,11 @@ def testCumSumProd(self, axis, shape, dtype, out_dtype, op):\n     np_op = getattr(np, op)\n     rng = jtu.rand_default(self.rng())\n     np_fun = lambda arg: np_op(arg, axis=axis, dtype=out_dtype)\n-    np_fun = jtu.ignore_warning(category=NumpyComplexWarning)(np_fun)\n+    np_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(np_fun)\n     np_fun = jtu.ignore_warning(category=RuntimeWarning,\n                                 message=\"overflow encountered.*\")(np_fun)\n     jnp_fun = lambda arg: jnp_op(arg, axis=axis, dtype=out_dtype)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n \n     args_maker = lambda: [rng(shape, dtype)]\n \n@@ -2127,11 +2127,11 @@ def testNanCumSumProd(self, axis, shape, dtype, out_dtype, op):\n     np_op = getattr(np, op)\n     rng = jtu.rand_some_nan(self.rng())\n     np_fun = partial(np_op, axis=axis, dtype=out_dtype)\n-    np_fun = jtu.ignore_warning(category=NumpyComplexWarning)(np_fun)\n+    np_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(np_fun)\n     np_fun = jtu.ignore_warning(category=RuntimeWarning,\n                                 message=\"overflow encountered.*\")(np_fun)\n     jnp_fun = partial(jnp_op, axis=axis, dtype=out_dtype)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n \n     args_maker = lambda: [rng(shape, dtype)]\n \ndiff --git a/tests/lax_numpy_indexing_test.py b/tests/lax_numpy_indexing_test.py\nindex ca9ba9c88806..745cab59cf1b 100644\n--- a/tests/lax_numpy_indexing_test.py\n+++ b/tests/lax_numpy_indexing_test.py\n@@ -35,7 +35,6 @@\n from jax._src import test_util as jtu\n from jax._src import util\n from jax._src.lax import lax as lax_internal\n-from jax._src.util import NumpyComplexWarning\n \n config.parse_flags_with_absl()\n \n@@ -1186,7 +1185,7 @@ def _check(x_type, y_type):\n       out = x.at[0].set(y)\n       self.assertEqual(x.dtype, out.dtype)\n \n-    @jtu.ignore_warning(category=NumpyComplexWarning,\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning,\n                         message=\"Casting complex values to real\")\n     def _check_warns(x_type, y_type, msg):\n       with self.assertWarnsRegex(FutureWarning, msg):\ndiff --git a/tests/lax_numpy_reducers_test.py b/tests/lax_numpy_reducers_test.py\nindex aa5e08e96a3e..93aff25c6f8e 100644\n--- a/tests/lax_numpy_reducers_test.py\n+++ b/tests/lax_numpy_reducers_test.py\n@@ -29,7 +29,6 @@\n from jax._src import config\n from jax._src import dtypes\n from jax._src import test_util as jtu\n-from jax._src.util import NumpyComplexWarning\n \n config.parse_flags_with_absl()\n \n@@ -209,7 +208,7 @@ def testReducer(self, name, rng_factory, shape, dtype, out_dtype,\n     np_op = getattr(np, name)\n     jnp_op = getattr(jnp, name)\n     rng = rng_factory(self.rng())\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Mean of empty slice.*\")\n     @jtu.ignore_warning(category=RuntimeWarning,\n@@ -225,7 +224,7 @@ def np_fun(x):\n       return np_op(x_cast, axis, dtype=t, keepdims=keepdims)\n \n     jnp_fun = lambda x: jnp_op(x, axis, dtype=out_dtype, keepdims=keepdims)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n     args_maker = lambda: [rng(shape, dtype)]\n     tol_spec = {np.float16: 1e-2, np.int16: 2e-7, np.int32: 1E-3,\n                 np.uint32: 3e-7, np.float32: 1e-3, np.complex64: 1e-3,\n@@ -313,7 +312,7 @@ def testReducerInitial(self, name, rng_factory, shape, dtype, axis,\n     is_bf16_nan_test = dtype == jnp.bfloat16 and rng_factory.__name__ == 'rand_some_nan'\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Degrees of freedom <= 0 for slice.*\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       x = np.asarray(x)\n       if inexact:\n@@ -324,7 +323,7 @@ def np_fun(x):\n       return res.astype(_reducer_output_dtype(name, x.dtype))\n \n     jnp_fun = lambda x: jnp_op(x, axis, keepdims=keepdims, initial=initial)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n     args_maker = lambda: [rng(shape, dtype)]\n     tol = {jnp.bfloat16: 3E-2}\n     self._CheckAgainstNumpy(np_fun, jnp_fun, args_maker, rtol=tol, atol=tol)\n@@ -353,7 +352,7 @@ def testReducerPromoteInt(self, name, rng_factory, shape, dtype, axis,\n                         rng_factory.__name__ == 'rand_some_nan')\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Degrees of freedom <= 0 for slice.*\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       x = np.asarray(x)\n       if inexact:\n@@ -364,7 +363,7 @@ def np_fun(x):\n       return res.astype(_reducer_output_dtype(name, x.dtype, promote_integers))\n \n     jnp_fun = lambda x: jnp_op(x, axis, keepdims=keepdims, initial=initial, promote_integers=promote_integers)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n     args_maker = lambda: [rng(shape, dtype)]\n     tol = {jnp.bfloat16: 3E-2, jnp.float16: 5e-3}\n     self._CheckAgainstNumpy(np_fun, jnp_fun, args_maker, rtol=tol)\n@@ -390,7 +389,7 @@ def testReducerNoInitialZeroDims(self, name, rng_factory, shape, dtype, axis,\n     is_bf16_nan_test = dtype == jnp.bfloat16 and rng_factory.__name__ == 'rand_some_nan'\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Degrees of freedom <= 0 for slice.*\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       x = np.asarray(x)\n       if inexact:\n@@ -401,7 +400,7 @@ def np_fun(x):\n       return res.astype(_reducer_output_dtype(name, x.dtype))\n \n     jnp_fun = lambda x: jnp_op(x, axis, keepdims=keepdims)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n     args_maker = lambda: [rng(shape, dtype)]\n     tol = {jnp.bfloat16: 3E-2}\n     self._CheckAgainstNumpy(np_fun, jnp_fun, args_maker, rtol=tol)\n@@ -436,7 +435,7 @@ def testReducerWhere(self, name, rng_factory, shape, dtype, axis,\n     where = jtu.rand_bool(self.rng())(whereshape, np.bool_)\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Degrees of freedom <= 0 for slice.*\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       x = np.asarray(x)\n       if inexact:\n@@ -447,7 +446,7 @@ def np_fun(x):\n       return res.astype(_reducer_output_dtype(name, x.dtype))\n \n     jnp_fun = lambda x: jnp_op(x, axis, keepdims=keepdims, initial=initial, where=where)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n     args_maker = lambda: [rng(shape, dtype)]\n     self._CheckAgainstNumpy(np_fun, jnp_fun, args_maker, atol=tol, rtol=tol)\n     self._CompileAndCheck(jnp_fun, args_maker)\n@@ -499,7 +498,7 @@ def testReducerWhereNoInitial(self, name, rng_factory, shape, dtype, axis,\n                         message=\"Mean of empty slice.*\")\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"invalid value encountered.*\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       x = np.asarray(x)\n       if inexact:\n@@ -510,7 +509,7 @@ def np_fun(x):\n       return res\n \n     jnp_fun = lambda x: jnp_op(x, axis, keepdims=keepdims, where=where)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n     args_maker = lambda: [rng(shape, dtype)]\n     self._CheckAgainstNumpy(np_fun, jnp_fun, args_maker, atol=tol, rtol=tol)\n     self._CompileAndCheck(jnp_fun, args_maker)\n@@ -574,7 +573,7 @@ def testStdOrVar(self, test_fns, shape, dtype, out_dtype, axis, ddof_correction,\n     args_maker = self._GetArgsMaker(rng, [shape], [dtype])\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Degrees of freedom <= 0 for slice.\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       # setup ddof and correction kwargs excluding case when correction is not specified\n       ddof_correction_kwargs = {\"ddof\": ddof}\n@@ -625,7 +624,7 @@ def testNanVar(self, shape, dtype, out_dtype, axis, ddof, keepdims):\n     args_maker = self._GetArgsMaker(rng, [shape], [dtype])\n     @jtu.ignore_warning(category=RuntimeWarning,\n                         message=\"Degrees of freedom <= 0 for slice.\")\n-    @jtu.ignore_warning(category=NumpyComplexWarning)\n+    @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n     def np_fun(x):\n       # Numpy fails with bfloat16 inputs\n       out = np.nanvar(x.astype(np.float32 if dtype == dtypes.bfloat16 else dtype),\n@@ -834,7 +833,7 @@ def test_f16_mean(self, dtype):\n     ],\n     include_initial=[False, True],\n   )\n-  @jtu.ignore_warning(category=NumpyComplexWarning)\n+  @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n   @jax.numpy_dtype_promotion('standard')  # This test explicitly exercises mixed type promotion\n   def testCumulativeSum(self, shape, axis, dtype, out_dtype, include_initial):\n     rng = jtu.rand_some_zero(self.rng())\n@@ -902,7 +901,7 @@ def testCumulativeSumBool(self):\n     ],\n     include_initial=[False, True],\n   )\n-  @jtu.ignore_warning(category=NumpyComplexWarning)\n+  @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n   @jax.numpy_dtype_promotion('standard')  # This test explicitly exercises mixed type promotion\n   def testCumulativeProd(self, shape, axis, dtype, out_dtype, include_initial):\n     if jtu.is_device_tpu_at_least(6):\ndiff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py\nindex 16234463d795..80d1d4161cc5 100644\n--- a/tests/lax_numpy_test.py\n+++ b/tests/lax_numpy_test.py\n@@ -50,7 +50,7 @@\n from jax._src import dtypes\n from jax._src import test_util as jtu\n from jax._src.lax import lax as lax_internal\n-from jax._src.util import safe_zip, NumpyComplexWarning, tuple_update\n+from jax._src.util import safe_zip, tuple_update\n \n config.parse_flags_with_absl()\n \n@@ -2354,11 +2354,11 @@ def testCumSumProd(self, axis, shape, dtype, out_dtype, op):\n     np_op = getattr(np, op)\n     rng = jtu.rand_default(self.rng())\n     np_fun = lambda arg: np_op(arg, axis=axis, dtype=out_dtype)\n-    np_fun = jtu.ignore_warning(category=NumpyComplexWarning)(np_fun)\n+    np_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(np_fun)\n     np_fun = jtu.ignore_warning(category=RuntimeWarning,\n                                 message=\"overflow encountered.*\")(np_fun)\n     jnp_fun = lambda arg: jnp_op(arg, axis=axis, dtype=out_dtype)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n \n     args_maker = lambda: [rng(shape, dtype)]\n \n@@ -2382,11 +2382,11 @@ def testNanCumSumProd(self, axis, shape, dtype, out_dtype, op):\n     np_op = getattr(np, op)\n     rng = jtu.rand_some_nan(self.rng())\n     np_fun = partial(np_op, axis=axis, dtype=out_dtype)\n-    np_fun = jtu.ignore_warning(category=NumpyComplexWarning)(np_fun)\n+    np_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(np_fun)\n     np_fun = jtu.ignore_warning(category=RuntimeWarning,\n                                 message=\"overflow encountered.*\")(np_fun)\n     jnp_fun = partial(jnp_op, axis=axis, dtype=out_dtype)\n-    jnp_fun = jtu.ignore_warning(category=jnp.ComplexWarning)(jnp_fun)\n+    jnp_fun = jtu.ignore_warning(category=np.exceptions.ComplexWarning)(jnp_fun)\n \n     args_maker = lambda: [rng(shape, dtype)]\n \ndiff --git a/tests/lax_test.py b/tests/lax_test.py\nindex 6792b08c37fa..a11c989fc9c5 100644\n--- a/tests/lax_test.py\n+++ b/tests/lax_test.py\n@@ -48,7 +48,7 @@\n from jax._src.interpreters import pxla\n from jax._src.internal_test_util import lax_test_util\n from jax._src.lax import lax as lax_internal\n-from jax._src.util import NumpyComplexWarning, safe_zip\n+from jax._src.util import safe_zip\n from jax._src.tree_util import tree_map\n \n config.parse_flags_with_absl()\n@@ -3744,7 +3744,7 @@ def testConvertElementReturnType(self, input_type, dtype, value, jit):\n \n   @jtu.sample_product(\n       dtype_in=lax_test_util.all_dtypes, dtype_out=lax_test_util.all_dtypes)\n-  @jtu.ignore_warning(category=NumpyComplexWarning)\n+  @jtu.ignore_warning(category=np.exceptions.ComplexWarning)\n   def testConvertElementTypeAvoidsCopies(self, dtype_in, dtype_out):\n     x = jax.device_put(np.zeros(5, dtype_in))\n     self.assertEqual(x.dtype, dtype_in)\n\n```",
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Merge pull request #29169 from dfm:pp-custom-lin\n\nPiperOrigin-RevId: 766245312",
        "output": "```diff\nCommit: 0edfc7294f64544974ce357d0df079958e6062da\nDate: 2025-06-02T17:22:29Z\nURL: https://github.com/jax-ml/jax/commit/0edfc7294f64544974ce357d0df079958e6062da\nFiles changed: 2\nAdditions: +37, Deletions: -0\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 0cd99a197f66..a77e93bb0696 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -1243,6 +1243,14 @@ def _custom_lin_transpose(cts_out, *invals, num_res,\n   return [None] * num_res + nz_cts_in\n primitive_transposes[custom_lin_p] = _custom_lin_transpose\n \n+def _custom_lin_pp_rule(eqn: core.JaxprEqn, context: core.JaxprPpContext,\n+                        settings: core.JaxprPpSettings) -> core.pp.Doc:\n+  params = dict(eqn.params)\n+  params.pop(\"out_avals\")\n+  params[\"bwd\"] = params.pop(\"bwd\").debug_info.func_name\n+  return core._pp_eqn(eqn.replace(params=params), context, settings)\n+core.pp_eqn_rules[custom_lin_p] = _custom_lin_pp_rule\n+\n \n class CustomJVPException(Exception):\n   def __init__(self):\ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 45434923d543..61b0129aca3e 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3199,6 +3199,35 @@ def f_bwd(_, g):\n         \"\"\").strip()\n     self.assertEqual(actual, expected)\n \n+  def test_custom_lin_pretty_print(self):\n+    @jax.custom_vjp\n+    def f(x):\n+      return x + 1\n+\n+    def f_fwd(x):\n+      return f(x), ()\n+\n+    def f_bwd(_, g):\n+      return g\n+    f.defvjp(f_fwd, f_bwd)\n+\n+    x = jnp.array([4.2], dtype=jnp.float32)\n+    jaxpr = jax.make_jaxpr(lambda x: jax.jvp(f, (x,), (x,)))(x)\n+    jaxpr, _ = pe.dce_jaxpr(jaxpr.jaxpr, [False, True])\n+    actual = jaxpr.pretty_print(use_color=False)\n+    expected = textwrap.dedent(\n+        \"\"\"\n+        { lambda ; a:f32[1]. let\n+            b:f32[1] = custom_lin[\n+              bwd=f_bwd\n+              in_zeros=[False]\n+              num_res=0\n+              symbolic_zeros=False\n+            ] a\n+          in (b,) }\n+        \"\"\").strip()\n+    self.assertEqual(actual, expected)\n+\n \n def transpose_unary(f, x_example):\n   def transposed(y):\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29166 from hawkinsp:minver\n\nPiperOrigin-RevId: 766242578",
        "output": "```diff\nCommit: 2c3018dc65be0cdaa58da7541ebcf29e339ec786\nDate: 2025-06-02T17:16:11Z\nURL: https://github.com/jax-ml/jax/commit/2c3018dc65be0cdaa58da7541ebcf29e339ec786\nFiles changed: 8\nAdditions: +14, Deletions: -54\ndiff --git a/.github/workflows/oldest_supported_numpy.yml b/.github/workflows/oldest_supported_numpy.yml\nindex 67fc9f10e5ce..fbf881a84a9c 100644\n--- a/.github/workflows/oldest_supported_numpy.yml\n+++ b/.github/workflows/oldest_supported_numpy.yml\n@@ -42,7 +42,7 @@ jobs:\n           $JAXCI_PYTHON -m uv pip install -r build/test-requirements.txt\n \n           # Install NumPy and SciPy with the oldest supported versions\n-          $JAXCI_PYTHON -m uv pip install numpy==1.25.2 scipy==1.11.1\n+          $JAXCI_PYTHON -m uv pip install numpy==1.26.4 scipy==1.12.0\n \n           # Install JAX using the changes in the PR\n           $JAXCI_PYTHON -m uv pip install -e .[minimum-jaxlib]\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex b34bf36997af..afd15a357b48 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -19,6 +19,9 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n * New features:\n   * Added {func}`jax.tree.broadcast` which implements a pytree prefix broadcasting helper.\n \n+* Changes\n+  * The minimum NumPy version is 1.26 and the minimum SciPy version is 1.12.\n+\n ## JAX 0.6.1 (May 21, 2025)\n \n * New features:\ndiff --git a/jax/_src/util.py b/jax/_src/util.py\nindex dbdc746713fb..4100ac21dc00 100644\n--- a/jax/_src/util.py\n+++ b/jax/_src/util.py\n@@ -642,12 +642,7 @@ def decorator(f):\n   return decorator\n \n \n-try:\n-  # numpy 1.25.0 or newer\n-  NumpyComplexWarning: type[Warning] = np.exceptions.ComplexWarning\n-except AttributeError:\n-  # legacy numpy\n-  NumpyComplexWarning = np.ComplexWarning\n+NumpyComplexWarning: type[Warning] = np.exceptions.ComplexWarning\n \n \n class StrictABCMeta(abc.ABCMeta):\ndiff --git a/jaxlib/setup.py b/jaxlib/setup.py\nindex 30e81c9ad671..ef0fcb205fb1 100644\n--- a/jaxlib/setup.py\n+++ b/jaxlib/setup.py\n@@ -61,9 +61,9 @@ def has_ext_modules(self):\n     packages=['jaxlib'],\n     python_requires='>=3.10',\n     install_requires=[\n-        'scipy>=1.11.1',\n-        'numpy>=1.25',\n-        'ml_dtypes>=0.2.0',\n+        'scipy>=1.12',\n+        'numpy>=1.26',\n+        'ml_dtypes>=0.5.0',\n     ],\n     url='https://github.com/jax-ml/jax',\n     license='Apache-2.0',\ndiff --git a/setup.py b/setup.py\nindex 2b50b041008d..6f552b1cf2f4 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -63,10 +63,9 @@ def load_version_module(pkg_path):\n     install_requires=[\n         f'jaxlib >={_minimum_jaxlib_version}, <={_jax_version}',\n         'ml_dtypes>=0.5.0',\n-        'numpy>=1.25',\n-        \"numpy>=1.26.0; python_version>='3.12'\",\n+        'numpy>=1.26',\n         'opt_einsum',\n-        'scipy>=1.11.1',\n+        'scipy>=1.12',\n     ],\n     extras_require={\n         # Minimum jaxlib version; used in testing.\ndiff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py\nindex 29e6586ffa18..16234463d795 100644\n--- a/tests/lax_numpy_test.py\n+++ b/tests/lax_numpy_test.py\n@@ -1938,9 +1938,6 @@ def testDeleteMaskArray(self, shape, dtype, axis):\n     rng = jtu.rand_default(self.rng())\n     mask_size = np.zeros(shape).size if axis is None else np.zeros(shape).shape[axis]\n     mask = jtu.rand_int(self.rng(), low=0, high=2)(mask_size, bool)\n-    if numpy_version == (1, 23, 0) and mask.shape == (1,):\n-      # https://github.com/numpy/numpy/issues/21840\n-      self.skipTest(\"test fails for numpy v1.23.0\")\n     args_maker = lambda: [rng(shape, dtype)]\n     np_fun = lambda arg: np.delete(arg, mask, axis=axis)\n     jnp_fun = lambda arg: jnp.delete(arg, mask, axis=axis)\ndiff --git a/tests/scipy_spatial_test.py b/tests/scipy_spatial_test.py\nindex 3da98efce884..6b1c042b049e 100644\n--- a/tests/scipy_spatial_test.py\n+++ b/tests/scipy_spatial_test.py\n@@ -123,8 +123,6 @@ def testRotationAsQuat(self, shape, dtype):\n     shape=[(4,), (num_samples, 4)],\n   )\n   def testRotationAsQuatCanonical(self, shape, dtype):\n-    if scipy_version < (1, 11, 0):\n-      self.skipTest(\"Scipy 1.11.0 added the `canonical` arg.\")\n     rng = jtu.rand_default(self.rng())\n     args_maker = lambda: (rng(shape, dtype),)\n     jnp_fn = lambda q: jsp_Rotation.from_quat(q).as_quat(canonical=True)\n@@ -152,8 +150,6 @@ def testRotationAsQuatScalarFirst(self, shape, dtype):\n     other_shape=[(num_samples, 4)],\n   )\n   def testRotationConcatenate(self, shape, other_shape, dtype):\n-    if scipy_version < (1, 8, 0):\n-      self.skipTest(\"Scipy 1.8.0 needed for concatenate.\")\n     rng = jtu.rand_default(self.rng())\n     args_maker = lambda: (rng(shape, dtype), rng(other_shape, dtype),)\n     jnp_fn = lambda q, o: jsp_Rotation.concatenate([jsp_Rotation.from_quat(q), jsp_Rotation.from_quat(o)]).as_rotvec()\n@@ -297,8 +293,6 @@ def testRotationInv(self, shape, dtype):\n     shape=[(4,), (num_samples, 4)],\n   )\n   def testRotationInvConjugate(self, shape, dtype):\n-    if scipy_version < (1, 11, 0):\n-      self.skipTest(\"Scipy prior to 1.11.0 used a negative conjugate.\")\n     rng = jtu.rand_default(self.rng())\n     args_maker = lambda: (rng(shape, dtype),)\n     jnp_fn = lambda q: jsp_Rotation.from_quat(q).inv().as_quat()\ndiff --git a/tests/scipy_stats_test.py b/tests/scipy_stats_test.py\nindex 796d4490daea..e9021b86bb7a 100644\n--- a/tests/scipy_stats_test.py\n+++ b/tests/scipy_stats_test.py\n@@ -20,18 +20,15 @@\n \n import numpy as np\n import scipy.stats as osp_stats\n-import scipy.version\n \n import jax\n import jax.numpy as jnp\n-from jax._src import dtypes, test_util as jtu\n+from jax._src import test_util as jtu\n from jax.scipy import stats as lsp_stats\n from jax.scipy.special import expit\n \n jax.config.parse_flags_with_absl()\n \n-scipy_version = jtu.parse_version(scipy.version.version)\n-\n all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]\n one_and_two_dim_shapes = [(4,), (3, 4), (3, 1), (1, 4)]\n \n@@ -217,9 +214,6 @@ def testBernoulliPpf(self, shapes, dtypes):\n     scipy_fun = osp_stats.bernoulli.ppf\n     lax_fun = lsp_stats.bernoulli.ppf\n \n-    if scipy_version < (1, 9, 2):\n-      self.skipTest(\"Scipy 1.9.2 needed for fix https://github.com/scipy/scipy/pull/17166.\")\n-\n     def args_maker():\n       q, p = map(rng, shapes, dtypes)\n       q = expit(q)\n@@ -1664,9 +1658,6 @@ def evaluate_kde(kde, x):\n       message=\"All axis-slices of one or more sample arguments are too small\",\n   )\n   def testMode(self, shape, dtype, axis, contains_nans, keepdims):\n-    if scipy_version < (1, 9, 0) and keepdims != True:\n-      self.skipTest(\"scipy < 1.9.0 only support keepdims == True\")\n-\n     if contains_nans:\n       rng = jtu.rand_some_nan(self.rng())\n     else:\n@@ -1675,25 +1666,7 @@ def testMode(self, shape, dtype, axis, contains_nans, keepdims):\n \n     def scipy_mode_wrapper(a, axis=0, nan_policy='propagate', keepdims=None):\n       \"\"\"Wrapper to manage the shape discrepancies between scipy and jax\"\"\"\n-      if scipy_version < (1, 11, 0) and a.size == 0:\n-        if keepdims:\n-          if axis == None:\n-            output_shape = tuple(1 for _ in a.shape)\n-          else:\n-            output_shape = tuple(1 if i == axis else s for i, s in enumerate(a.shape))\n-        else:\n-          if axis == None:\n-            output_shape = ()\n-          else:\n-            output_shape = np.delete(np.array(a.shape, dtype=np.int64), axis)\n-        t = dtypes.canonicalize_dtype(jax.numpy.float_)\n-        return (np.full(output_shape, np.nan, dtype=t),\n-                np.zeros(output_shape, dtype=t))\n-\n-      if scipy_version < (1, 9, 0):\n-        result = osp_stats.mode(a, axis=axis, nan_policy=nan_policy)\n-      else:\n-        result = osp_stats.mode(a, axis=axis, nan_policy=nan_policy, keepdims=keepdims)\n+      result = osp_stats.mode(a, axis=axis, nan_policy=nan_policy, keepdims=keepdims)\n \n       if a.size != 0 and axis == None and keepdims == True:\n         output_shape = tuple(1 for _ in a.shape)\n@@ -1748,11 +1721,10 @@ def testSEM(self, shape, dtype, axis, ddof, nan_policy, keepdims):\n     rng = jtu.rand_default(self.rng())\n     args_maker = lambda: [rng(shape, dtype)]\n \n-    kwds = {} if scipy_version < (1, 11) else {'keepdims': keepdims}\n     scipy_fun = partial(osp_stats.sem, axis=axis, ddof=ddof, nan_policy=nan_policy,\n-                        **kwds)\n+                        keepdims=keepdims)\n     lax_fun = partial(lsp_stats.sem, axis=axis, ddof=ddof, nan_policy=nan_policy,\n-                      **kwds)\n+                      keepdims=keepdims)\n     tol_spec = {np.float32: 2e-4, np.float64: 5e-6}\n     tol = jtu.tolerance(dtype, tol_spec)\n     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=False,\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add a pretty printing rule for custom_lin_p.",
        "output": "```diff\nCommit: d62d94cb8578c37303e6a071bbe30b3395f84847\nDate: 2025-06-02T16:32:26Z\nURL: https://github.com/jax-ml/jax/commit/d62d94cb8578c37303e6a071bbe30b3395f84847\nFiles changed: 2\nAdditions: +37, Deletions: -0\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 0cd99a197f66..a77e93bb0696 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -1243,6 +1243,14 @@ def _custom_lin_transpose(cts_out, *invals, num_res,\n   return [None] * num_res + nz_cts_in\n primitive_transposes[custom_lin_p] = _custom_lin_transpose\n \n+def _custom_lin_pp_rule(eqn: core.JaxprEqn, context: core.JaxprPpContext,\n+                        settings: core.JaxprPpSettings) -> core.pp.Doc:\n+  params = dict(eqn.params)\n+  params.pop(\"out_avals\")\n+  params[\"bwd\"] = params.pop(\"bwd\").debug_info.func_name\n+  return core._pp_eqn(eqn.replace(params=params), context, settings)\n+core.pp_eqn_rules[custom_lin_p] = _custom_lin_pp_rule\n+\n \n class CustomJVPException(Exception):\n   def __init__(self):\ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 45434923d543..61b0129aca3e 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3199,6 +3199,35 @@ def f_bwd(_, g):\n         \"\"\").strip()\n     self.assertEqual(actual, expected)\n \n+  def test_custom_lin_pretty_print(self):\n+    @jax.custom_vjp\n+    def f(x):\n+      return x + 1\n+\n+    def f_fwd(x):\n+      return f(x), ()\n+\n+    def f_bwd(_, g):\n+      return g\n+    f.defvjp(f_fwd, f_bwd)\n+\n+    x = jnp.array([4.2], dtype=jnp.float32)\n+    jaxpr = jax.make_jaxpr(lambda x: jax.jvp(f, (x,), (x,)))(x)\n+    jaxpr, _ = pe.dce_jaxpr(jaxpr.jaxpr, [False, True])\n+    actual = jaxpr.pretty_print(use_color=False)\n+    expected = textwrap.dedent(\n+        \"\"\"\n+        { lambda ; a:f32[1]. let\n+            b:f32[1] = custom_lin[\n+              bwd=f_bwd\n+              in_zeros=[False]\n+              num_res=0\n+              symbolic_zeros=False\n+            ] a\n+          in (b,) }\n+        \"\"\").strip()\n+    self.assertEqual(actual, expected)\n+\n \n def transpose_unary(f, x_example):\n   def transposed(y):\n\n```",
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Merge pull request #29127 from mattjj:scan-vjp-mutable-hoist\n\nPiperOrigin-RevId: 766224792",
        "output": "```diff\nCommit: a99ca731e2c8bf54150b783dff72cfbf6de85345\nDate: 2025-06-02T16:31:04Z\nURL: https://github.com/jax-ml/jax/commit/a99ca731e2c8bf54150b783dff72cfbf6de85345\nFiles changed: 2\nAdditions: +43, Deletions: -5\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 4df4c517090b..47808ee3c423 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -854,6 +854,8 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n   # want to broadcast the matrix!). So, outside the loop we perform a partial\n   # evaluation with known 'const' inputs (but all other inputs unknown).\n   const_pvals = [pe.PartialVal.known(t.pval.get_known())\n+                 if not isinstance(t.aval, state.AbstractRef)\n+                 else pe.PartialVal.unknown(t.aval)\n                  for t in tracers[:num_consts] if t.pval.is_known()]\n   other_pvals = [pe.PartialVal.unknown(aval)\n                  for aval in jaxpr_known.in_avals[len(const_pvals):]]\n@@ -898,7 +900,9 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n   # We use `fwds_known` below when forming the output of scanning jaxpr_known.\n \n   # Run the known part of the scan (if it has any outputs or effects).\n-  known_inputs = (list(jaxpr_known_consts) +\n+  known_mutable_consts = [t.pval.get_known() for t in tracers[:num_consts]\n+                          if t.pval.is_known() and isinstance(t.aval, state.AbstractRef)]\n+  known_inputs = (list(jaxpr_known_consts) + known_mutable_consts +\n                   [t.pval.get_known() for t in tracers[num_consts:]\n                    if t.pval.is_known()])\n   if not jaxpr_known.out_avals and not jaxpr_known.effects:\n@@ -907,7 +911,8 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n     linear_known = [False] * len(known_inputs)  # conservative!\n     out_known = scan_p.bind(\n         *known_inputs, reverse=reverse, length=length, jaxpr=jaxpr_known,\n-        num_consts=len(jaxpr_known_consts), num_carry=num_carry - sum(carry_uk),\n+        num_consts=len(jaxpr_known_consts) + len(known_mutable_consts),\n+        num_carry=num_carry - sum(carry_uk),\n         linear=tuple(linear_known), unroll=unroll,\n         _split_transpose=_split_transpose)\n     del linear_known\n@@ -1292,10 +1297,12 @@ def _scan_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n   num_const_known = len(const_uk) - sum(const_uk)\n   num_carry_known = len(carry_uk) - sum(carry_uk)\n   num_xs_known    = len(   xs_uk) - sum(   xs_uk)\n+  const_donthoist = [isinstance(a, state.AbstractRef)\n+                     for a in jaxpr_known.in_avals[:num_const_known]]\n   jaxpr_known_hoist, jaxpr_known_loop, loop_dep, consts_known_lp_avals = \\\n       pe.partial_eval_jaxpr_nounits(\n           jaxpr_known,\n-          [False] * num_const_known + [True] * (num_carry_known + num_xs_known),\n+          const_donthoist + [True] * (num_carry_known + num_xs_known),\n           [True] * (len(unks_out) - sum(unks_out)) + [False] * num_res)\n   # jaxpr_known_hoist produces intensive residuals followed by the constants for\n   # jaxpr_known_loop. We adjust jaxpr_staged to accept intensive res as consts.\n@@ -1328,10 +1335,13 @@ def _scan_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n                       linear=tuple(linear_known))\n \n   def known(*ins_known):\n-    consts_known_hoist, ins_known_lp = split_list(ins_known, [num_const_known])\n+    consts_known_maybehoist, ins_known_lp = split_list(ins_known, [num_const_known])\n+    consts_known_hoist, consts_known_donthoist = \\\n+        partition_list(const_donthoist, consts_known_maybehoist)\n     out_hoist = core.jaxpr_as_fun(jaxpr_known_hoist)(*consts_known_hoist)\n     intensive_res, consts_known_lp = split_list(out_hoist, [num_intensive_res])\n-    out_loop = scan_p.bind(*consts_known_lp, *ins_known_lp, **params_known)\n+    out_loop = scan_p.bind(*consts_known_lp, *consts_known_donthoist,\n+                           *ins_known_lp, **params_known)\n     return [*intensive_res, *out_loop]\n   call_jaxpr_, _, call_jaxpr_consts, () = pe.trace_to_jaxpr_dynamic(\n       lu.wrap_init(known, debug_info=jaxpr_known_hoist.jaxpr.debug_info),\ndiff --git a/tests/mutable_array_test.py b/tests/mutable_array_test.py\nindex 95ebdd818fa6..e350a242548a 100644\n--- a/tests/mutable_array_test.py\n+++ b/tests/mutable_array_test.py\n@@ -289,6 +289,34 @@ def test_rng_key(self):\n     # test read/write\n     key[...] = jax.random.fold_in(key[...], 1) # don't crash\n \n+  def test_scan_grad_doesnt_hoist_mutable_stuff(self):\n+    x_ref = core.mutable_array(0)\n+\n+    def f(x):\n+      def body(c, _):\n+        x_ref[...] += 1\n+        return c, ()\n+      x, () = jax.lax.scan(body, x, (), length=3)\n+      return x\n+\n+    jax.grad(f)(1.0)\n+    self.assertAllClose(x_ref[...], 3, check_dtypes=False)\n+\n+  def test_scan_grad_doesnt_hoist_mutable_stuff2(self):\n+    x_ref = core.mutable_array(0)\n+    const = jnp.arange(3)\n+    const2 = jnp.zeros(())\n+\n+    def f(x):\n+      def body(c, _):\n+        x_ref[...] += const.sum()\n+        return c + const2, ()\n+      x, () = jax.lax.scan(body, x, (), length=4)\n+      return x\n+\n+    jax.grad(f)(1.0)\n+    self.assertAllClose(x_ref[...], 12, check_dtypes=False)\n+\n \n @jtu.with_config(jax_mutable_array_checks=True)\n class MutableArrayErrorsTest(jtu.JaxTestCase):\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Raise a better error when inputs sharded on explicit mesh axes are closed over in a shard_map instead of a crash. Fixes https://github.com/jax-ml/jax/issues/29162\n\nPiperOrigin-RevId: 766199302",
        "output": "```diff\nCommit: 8625207fc634b171e68ac03e62dc22c562e60e2d\nDate: 2025-06-02T15:17:51Z\nURL: https://github.com/jax-ml/jax/commit/8625207fc634b171e68ac03e62dc22c562e60e2d\nFiles changed: 2\nAdditions: +42, Deletions: -6\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex 24150aba6584..1355fc10472f 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -1883,14 +1883,20 @@ def canonicalize_value(val):\n   cur_mesh = mesh_lib.get_abstract_mesh()\n   if cur_mesh == aval.sharding.mesh:\n     return val\n-  # Atleast 1 mesh axis should be Manual and all other axes should be\n-  # Manual or Auto to allow casting.\n   # TODO(yashkatariy): Casting to Explicit is not yet allowed. Maybe we need\n   # cast_and_slice_p for it since shape might change?\n-  if (cur_mesh._any_axis_manual and cur_mesh._are_all_axes_auto_or_manual and\n-      aval.sharding.mesh._are_all_axes_auto):\n-    from jax._src.pjit import mesh_cast  # pytype: disable=import-error\n-    return mesh_cast(val, NamedSharding(cur_mesh, P(*[None] * aval.ndim)))\n+  # Atleast 1 mesh axis should be Manual and all other axes should be\n+  # Manual or Auto to allow casting.\n+  if cur_mesh._any_axis_manual and cur_mesh._are_all_axes_auto_or_manual:\n+    if aval.sharding.mesh._are_all_axes_auto:\n+      from jax._src.pjit import mesh_cast  # pytype: disable=import-error\n+      return mesh_cast(val, NamedSharding(cur_mesh, P(*[None] * aval.ndim)))\n+    elif aval.sharding.mesh._any_axis_explicit:\n+      raise NotImplementedError(\n+          \"Closing over inputs to shard_map where the input is sharded on\"\n+          \" `Explicit` axes is not implemented. As a workaround, please pass\"\n+          \" those inputs as an argument to shard_map. Got input with shape\"\n+          f\" {aval.str_short(True, True)}\")\n   return val\n \n \ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex f473a4dc0547..f3f5641be1b6 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -2259,6 +2259,36 @@ def grad_fn(batch):\n     params = jnp.copy(arr_sharded)\n     update_fn(params, arr_sharded)  # doesn't crash\n \n+  @jtu.with_explicit_mesh((2,), ('x',))\n+  def test_close_over_explicit_sharded_input_error(self, mesh):\n+    def simple_func(w, x):\n+      return jnp.sum(w * x, axis=-1)\n+\n+    w = jnp.ones((2, 4), dtype=np.float32)\n+    x = jnp.ones((4, 4), dtype=np.float32)\n+\n+    shard_map(simple_func, in_specs=(P(), P('x')), out_specs=P('x'))(w, x)\n+\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        'Closing over inputs to shard_map where the input is sharded on'\n+        ' `Explicit` axes is not implemented'):\n+      shard_map(lambda xi: simple_func(w, xi),\n+                in_specs=P('x'), out_specs=P('x'))(x)\n+\n+  def test_close_over_input_explict_ctx_mesh(self):\n+    mesh = jtu.create_mesh((2,), 'x', axis_types=(AxisType.Explicit,))\n+    w = jnp.ones((2, 4), dtype=np.float32)\n+    x = jnp.ones((4, 4), dtype=np.float32)\n+\n+    def simple_func(w, x):\n+      return jnp.sum(w * x, axis=-1)\n+\n+    shard_map(simple_func, mesh=mesh, in_specs=(P(), P('x')),\n+              out_specs=P('x'))(w, x)\n+    shard_map(lambda xi: simple_func(w, xi), mesh=mesh,\n+              in_specs=P('x'), out_specs=P('x'))(x)\n+\n   def test_shmap_close_over_unused_params_vmap(self):\n     mesh = jtu.create_mesh((2,), (\"data\",))\n \n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29084 from MichaelHudgins:actions\n\nPiperOrigin-RevId: 766195126",
        "output": "```diff\nCommit: e1b59e5d70a0b45925f63fd5811eae4709c1e4cb\nDate: 2025-06-02T15:06:12Z\nURL: https://github.com/jax-ml/jax/commit/e1b59e5d70a0b45925f63fd5811eae4709c1e4cb\nFiles changed: 4\nAdditions: +6, Deletions: -2\ndiff --git a/.github/workflows/k8s.yaml b/.github/workflows/k8s.yaml\nindex 81552f9bb43b..86bc5e6c168b 100644\n--- a/.github/workflows/k8s.yaml\n+++ b/.github/workflows/k8s.yaml\n@@ -4,6 +4,8 @@ on:\n     branches:\n       - main\n     paths:\n+      - '.github/workflows/k8s.yaml'\n+      - 'ci/k8s/**'\n       - 'jax/distributed.py'\n       - 'jax/_src/distributed.py'\n       - 'jax/_src/clusters/**'\n@@ -11,6 +13,8 @@ on:\n     branches:\n       - main\n     paths:\n+      - '.github/workflows/k8s.yaml'\n+      - 'ci/k8s/**'\n       - 'jax/distributed.py'\n       - 'jax/_src/distributed.py'\n       - 'jax/_src/clusters/**'\n@@ -61,7 +65,7 @@ jobs:\n         run: kubectl apply -f jax/examples/k8s/svc-acct.yaml\n \n       - name: Submit test job\n-        run: kubectl apply -f jax/.github/workflows/k8s/${{ matrix.controller }}.yaml\n+        run: kubectl apply -f jax/ci/k8s/${{ matrix.controller }}.yaml\n \n       - name: Check job status\n         shell: bash -e -o pipefail {0}\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 71b3d51caaa7..8cc28c9fe4ac 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -18,7 +18,7 @@ repos:\n     exclude: |\n       (?x)^(\n           examples/k8s/svc-acct\\.yaml |\n-          \\.github/workflows/k8s/indexed-job\\.yaml\n+          ci/k8s/indexed-job\\.yaml\n       )$\n   - id: end-of-file-fixer\n     # only include python files\ndiff --git a/.github/workflows/k8s/indexed-job.yaml b/ci/k8s/indexed-job.yaml\nsimilarity index 100%\nrename from .github/workflows/k8s/indexed-job.yaml\nrename to ci/k8s/indexed-job.yaml\ndiff --git a/.github/workflows/k8s/jobset.yaml b/ci/k8s/jobset.yaml\nsimilarity index 100%\nrename from .github/workflows/k8s/jobset.yaml\nrename to ci/k8s/jobset.yaml\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29165 from dfm:pe-effects\n\nPiperOrigin-RevId: 766195082",
        "output": "```diff\nCommit: 432de6283cd74c1491be684a7becbee045f2a797\nDate: 2025-06-02T15:04:14Z\nURL: https://github.com/jax-ml/jax/commit/432de6283cd74c1491be684a7becbee045f2a797\nFiles changed: 6\nAdditions: +15, Deletions: -14\ndiff --git a/jax/_src/debugging.py b/jax/_src/debugging.py\nindex 3490de5118e1..e587d48cda68 100644\n--- a/jax/_src/debugging.py\n+++ b/jax/_src/debugging.py\n@@ -69,6 +69,8 @@ class OrderedDebugEffect(effects.Effect):\n effects.remat_allowed_effects.add_type(OrderedDebugEffect)\n effects.custom_derivatives_allowed_effects.add_type(DebugEffect)\n effects.custom_derivatives_allowed_effects.add_type(OrderedDebugEffect)\n+effects.partial_eval_kept_effects.add_type(DebugEffect)\n+effects.partial_eval_kept_effects.add_type(OrderedDebugEffect)\n \n # `debug_callback_p` is the main primitive for staging out Python callbacks.\n debug_callback_p = core.Primitive('debug_callback')\n@@ -126,10 +128,10 @@ def debug_callback_jvp_rule(primals, tangents, **params):\n   return debug_callback_p.bind(*primals, **params), []\n ad.primitive_jvps[debug_callback_p] = debug_callback_jvp_rule\n \n-def debug_callback_transpose_rule(*flat_args, callback: Callable[..., Any],\n+def debug_callback_transpose_rule(_, *flat_args, callback: Callable[..., Any],\n                                   effect: DebugEffect, partitioned):\n-  del flat_args, callback, effect\n-  raise ValueError(\"Transpose doesn't support debugging callbacks.\")\n+  del callback, effect, partitioned\n+  return [None for _ in flat_args]\n ad.primitive_transposes[debug_callback_p] = debug_callback_transpose_rule\n \n def _debug_callback_partial_auto(axis_context, *args, **params):\ndiff --git a/jax/_src/effects.py b/jax/_src/effects.py\nindex d55333540355..fb79c542e78b 100644\n--- a/jax/_src/effects.py\n+++ b/jax/_src/effects.py\n@@ -118,3 +118,5 @@ def filter_not_in(self, effects: Iterable[Effect]) -> list[Effect]:\n control_flow_allowed_effects: EffectTypeSet = EffectTypeSet()\n custom_derivatives_allowed_effects: EffectTypeSet = EffectTypeSet()\n remat_allowed_effects: EffectTypeSet = EffectTypeSet()\n+\n+partial_eval_kept_effects: EffectTypeSet = EffectTypeSet()\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 444b60f15fa5..1bcd3f00321c 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -43,7 +43,7 @@\n                            mapped_aval, unmapped_aval, DBIdx, InDBIdx, OutDBIdx,\n                            InputType, OutputType, get_referent, JaxprEqnContext)\n from jax._src.source_info_util import SourceInfo\n-from jax._src.state.types import AbstractRef, ReadEffect, RefEffect\n+from jax._src.state.types import AbstractRef, ReadEffect\n from jax._src.tree_util import (PyTreeDef, treedef_tuple, tree_flatten,\n                                 tree_structure, register_static)\n from jax._src.util import (unzip2, safe_zip, safe_map, toposort, split_list,\n@@ -240,23 +240,23 @@ def default_process_primitive(self, primitive, tracers, params):\n       return primitive.bind_with_trace(self.parent_trace, consts, params)\n     tracers = map(self.instantiate_const, tracers)\n     avals = [t.aval for t in tracers]\n-    out_aval, effects = primitive.abstract_eval(*avals, **params)\n+    out_aval, effs = primitive.abstract_eval(*avals, **params)\n     name_stack = self._current_truncated_name_stack()\n     source = source_info_util.current().replace(name_stack=name_stack)\n     if primitive.multiple_results:\n       out_tracers = [JaxprTracer(self, PartialVal.unknown(aval), None)\n                      for aval in out_aval]\n-      eqn = new_eqn_recipe(self, tracers, out_tracers, primitive, params, effects,\n+      eqn = new_eqn_recipe(self, tracers, out_tracers, primitive, params, effs,\n                            source)\n-      if any(isinstance(e, RefEffect) for e in effects):\n+      if effects.partial_eval_kept_effects.filter_in(effs):\n         self.effect_handles.append(EffectHandle(tracers, eqn))\n       for t in out_tracers: t.recipe = eqn\n       return out_tracers\n     else:\n       out_tracer = JaxprTracer(self, PartialVal.unknown(out_aval), None)\n       eqn = new_eqn_recipe(self, tracers, [out_tracer], primitive,\n-                           params, effects, source)\n-      if any(isinstance(e, RefEffect) for e in effects):\n+                           params, effs, source)\n+      if effects.partial_eval_kept_effects.filter_in(effs):\n         self.effect_handles.append(EffectHandle(tracers, eqn))\n       out_tracer.recipe = eqn\n       return out_tracer\ndiff --git a/jax/_src/state/types.py b/jax/_src/state/types.py\nindex af5bec49a6d5..e3a86e241bf2 100644\n--- a/jax/_src/state/types.py\n+++ b/jax/_src/state/types.py\n@@ -75,6 +75,7 @@ class AccumEffect(RefEffect):\n   name: str = \"Accum\"\n \n effects.control_flow_allowed_effects.add_type(RefEffect)\n+effects.partial_eval_kept_effects.add_type(RefEffect)\n \n StateEffect = Union[ReadEffect, WriteEffect, AccumEffect]\n \ndiff --git a/tests/debugging_primitives_test.py b/tests/debugging_primitives_test.py\nindex 7985cf841248..9c23f136b825 100644\n--- a/tests/debugging_primitives_test.py\n+++ b/tests/debugging_primitives_test.py\n@@ -442,8 +442,6 @@ def f(x):\n     with jtu.capture_stdout() as output:\n       jax.linear_transpose(f, 1.)(1.)\n       jax.effects_barrier()\n-    # `debug_print` should be dropped by `partial_eval` because of no\n-    # output data-dependence.\n     self.assertEqual(output(), \"\")\n \n   @jtu.sample_product(ordered=[False, True])\ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex 2f1e154627f4..3f950f865735 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -28,7 +28,6 @@\n \n import jax\n from jax._src import core\n-from jax._src import config\n from jax import dtypes\n from jax import lax\n from jax import random\n@@ -3359,8 +3358,7 @@ def f(c, _):\n       return c + 1, None\n     def g(x):\n       return jax.lax.scan(f, x, length=2)[0]\n-    with config.use_direct_linearize(True):\n-      jaxpr = jax.make_jaxpr(jax.value_and_grad(g))(1.0)\n+    jaxpr = jax.make_jaxpr(jax.value_and_grad(g))(1.0)\n     eqn_jaxpr = jaxpr.eqns[0].params[\"jaxpr\"]\n     self.assertIn(\"debug_callback\", [e.primitive.name for e in eqn_jaxpr.eqns])\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Bump the minimum NumPy and SciPy versions.\n\nPer Spec 0, we can drop NumPy 1.25 and SciPy 1.11 in June 2025.",
        "output": "```diff\nCommit: 73aabb46c51251199ee1059c50c5c5ae3ea133d1\nDate: 2025-06-02T15:01:22Z\nURL: https://github.com/jax-ml/jax/commit/73aabb46c51251199ee1059c50c5c5ae3ea133d1\nFiles changed: 8\nAdditions: +14, Deletions: -54\ndiff --git a/.github/workflows/oldest_supported_numpy.yml b/.github/workflows/oldest_supported_numpy.yml\nindex 67fc9f10e5ce..fbf881a84a9c 100644\n--- a/.github/workflows/oldest_supported_numpy.yml\n+++ b/.github/workflows/oldest_supported_numpy.yml\n@@ -42,7 +42,7 @@ jobs:\n           $JAXCI_PYTHON -m uv pip install -r build/test-requirements.txt\n \n           # Install NumPy and SciPy with the oldest supported versions\n-          $JAXCI_PYTHON -m uv pip install numpy==1.25.2 scipy==1.11.1\n+          $JAXCI_PYTHON -m uv pip install numpy==1.26.4 scipy==1.12.0\n \n           # Install JAX using the changes in the PR\n           $JAXCI_PYTHON -m uv pip install -e .[minimum-jaxlib]\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex b34bf36997af..afd15a357b48 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -19,6 +19,9 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n * New features:\n   * Added {func}`jax.tree.broadcast` which implements a pytree prefix broadcasting helper.\n \n+* Changes\n+  * The minimum NumPy version is 1.26 and the minimum SciPy version is 1.12.\n+\n ## JAX 0.6.1 (May 21, 2025)\n \n * New features:\ndiff --git a/jax/_src/util.py b/jax/_src/util.py\nindex dbdc746713fb..4100ac21dc00 100644\n--- a/jax/_src/util.py\n+++ b/jax/_src/util.py\n@@ -642,12 +642,7 @@ def decorator(f):\n   return decorator\n \n \n-try:\n-  # numpy 1.25.0 or newer\n-  NumpyComplexWarning: type[Warning] = np.exceptions.ComplexWarning\n-except AttributeError:\n-  # legacy numpy\n-  NumpyComplexWarning = np.ComplexWarning\n+NumpyComplexWarning: type[Warning] = np.exceptions.ComplexWarning\n \n \n class StrictABCMeta(abc.ABCMeta):\ndiff --git a/jaxlib/setup.py b/jaxlib/setup.py\nindex 30e81c9ad671..ef0fcb205fb1 100644\n--- a/jaxlib/setup.py\n+++ b/jaxlib/setup.py\n@@ -61,9 +61,9 @@ def has_ext_modules(self):\n     packages=['jaxlib'],\n     python_requires='>=3.10',\n     install_requires=[\n-        'scipy>=1.11.1',\n-        'numpy>=1.25',\n-        'ml_dtypes>=0.2.0',\n+        'scipy>=1.12',\n+        'numpy>=1.26',\n+        'ml_dtypes>=0.5.0',\n     ],\n     url='https://github.com/jax-ml/jax',\n     license='Apache-2.0',\ndiff --git a/setup.py b/setup.py\nindex 2b50b041008d..6f552b1cf2f4 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -63,10 +63,9 @@ def load_version_module(pkg_path):\n     install_requires=[\n         f'jaxlib >={_minimum_jaxlib_version}, <={_jax_version}',\n         'ml_dtypes>=0.5.0',\n-        'numpy>=1.25',\n-        \"numpy>=1.26.0; python_version>='3.12'\",\n+        'numpy>=1.26',\n         'opt_einsum',\n-        'scipy>=1.11.1',\n+        'scipy>=1.12',\n     ],\n     extras_require={\n         # Minimum jaxlib version; used in testing.\ndiff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py\nindex 29e6586ffa18..16234463d795 100644\n--- a/tests/lax_numpy_test.py\n+++ b/tests/lax_numpy_test.py\n@@ -1938,9 +1938,6 @@ def testDeleteMaskArray(self, shape, dtype, axis):\n     rng = jtu.rand_default(self.rng())\n     mask_size = np.zeros(shape).size if axis is None else np.zeros(shape).shape[axis]\n     mask = jtu.rand_int(self.rng(), low=0, high=2)(mask_size, bool)\n-    if numpy_version == (1, 23, 0) and mask.shape == (1,):\n-      # https://github.com/numpy/numpy/issues/21840\n-      self.skipTest(\"test fails for numpy v1.23.0\")\n     args_maker = lambda: [rng(shape, dtype)]\n     np_fun = lambda arg: np.delete(arg, mask, axis=axis)\n     jnp_fun = lambda arg: jnp.delete(arg, mask, axis=axis)\ndiff --git a/tests/scipy_spatial_test.py b/tests/scipy_spatial_test.py\nindex 3da98efce884..6b1c042b049e 100644\n--- a/tests/scipy_spatial_test.py\n+++ b/tests/scipy_spatial_test.py\n@@ -123,8 +123,6 @@ def testRotationAsQuat(self, shape, dtype):\n     shape=[(4,), (num_samples, 4)],\n   )\n   def testRotationAsQuatCanonical(self, shape, dtype):\n-    if scipy_version < (1, 11, 0):\n-      self.skipTest(\"Scipy 1.11.0 added the `canonical` arg.\")\n     rng = jtu.rand_default(self.rng())\n     args_maker = lambda: (rng(shape, dtype),)\n     jnp_fn = lambda q: jsp_Rotation.from_quat(q).as_quat(canonical=True)\n@@ -152,8 +150,6 @@ def testRotationAsQuatScalarFirst(self, shape, dtype):\n     other_shape=[(num_samples, 4)],\n   )\n   def testRotationConcatenate(self, shape, other_shape, dtype):\n-    if scipy_version < (1, 8, 0):\n-      self.skipTest(\"Scipy 1.8.0 needed for concatenate.\")\n     rng = jtu.rand_default(self.rng())\n     args_maker = lambda: (rng(shape, dtype), rng(other_shape, dtype),)\n     jnp_fn = lambda q, o: jsp_Rotation.concatenate([jsp_Rotation.from_quat(q), jsp_Rotation.from_quat(o)]).as_rotvec()\n@@ -297,8 +293,6 @@ def testRotationInv(self, shape, dtype):\n     shape=[(4,), (num_samples, 4)],\n   )\n   def testRotationInvConjugate(self, shape, dtype):\n-    if scipy_version < (1, 11, 0):\n-      self.skipTest(\"Scipy prior to 1.11.0 used a negative conjugate.\")\n     rng = jtu.rand_default(self.rng())\n     args_maker = lambda: (rng(shape, dtype),)\n     jnp_fn = lambda q: jsp_Rotation.from_quat(q).inv().as_quat()\ndiff --git a/tests/scipy_stats_test.py b/tests/scipy_stats_test.py\nindex 796d4490daea..e9021b86bb7a 100644\n--- a/tests/scipy_stats_test.py\n+++ b/tests/scipy_stats_test.py\n@@ -20,18 +20,15 @@\n \n import numpy as np\n import scipy.stats as osp_stats\n-import scipy.version\n \n import jax\n import jax.numpy as jnp\n-from jax._src import dtypes, test_util as jtu\n+from jax._src import test_util as jtu\n from jax.scipy import stats as lsp_stats\n from jax.scipy.special import expit\n \n jax.config.parse_flags_with_absl()\n \n-scipy_version = jtu.parse_version(scipy.version.version)\n-\n all_shapes = [(), (4,), (3, 4), (3, 1), (1, 4), (2, 1, 4)]\n one_and_two_dim_shapes = [(4,), (3, 4), (3, 1), (1, 4)]\n \n@@ -217,9 +214,6 @@ def testBernoulliPpf(self, shapes, dtypes):\n     scipy_fun = osp_stats.bernoulli.ppf\n     lax_fun = lsp_stats.bernoulli.ppf\n \n-    if scipy_version < (1, 9, 2):\n-      self.skipTest(\"Scipy 1.9.2 needed for fix https://github.com/scipy/scipy/pull/17166.\")\n-\n     def args_maker():\n       q, p = map(rng, shapes, dtypes)\n       q = expit(q)\n@@ -1664,9 +1658,6 @@ def evaluate_kde(kde, x):\n       message=\"All axis-slices of one or more sample arguments are too small\",\n   )\n   def testMode(self, shape, dtype, axis, contains_nans, keepdims):\n-    if scipy_version < (1, 9, 0) and keepdims != True:\n-      self.skipTest(\"scipy < 1.9.0 only support keepdims == True\")\n-\n     if contains_nans:\n       rng = jtu.rand_some_nan(self.rng())\n     else:\n@@ -1675,25 +1666,7 @@ def testMode(self, shape, dtype, axis, contains_nans, keepdims):\n \n     def scipy_mode_wrapper(a, axis=0, nan_policy='propagate', keepdims=None):\n       \"\"\"Wrapper to manage the shape discrepancies between scipy and jax\"\"\"\n-      if scipy_version < (1, 11, 0) and a.size == 0:\n-        if keepdims:\n-          if axis == None:\n-            output_shape = tuple(1 for _ in a.shape)\n-          else:\n-            output_shape = tuple(1 if i == axis else s for i, s in enumerate(a.shape))\n-        else:\n-          if axis == None:\n-            output_shape = ()\n-          else:\n-            output_shape = np.delete(np.array(a.shape, dtype=np.int64), axis)\n-        t = dtypes.canonicalize_dtype(jax.numpy.float_)\n-        return (np.full(output_shape, np.nan, dtype=t),\n-                np.zeros(output_shape, dtype=t))\n-\n-      if scipy_version < (1, 9, 0):\n-        result = osp_stats.mode(a, axis=axis, nan_policy=nan_policy)\n-      else:\n-        result = osp_stats.mode(a, axis=axis, nan_policy=nan_policy, keepdims=keepdims)\n+      result = osp_stats.mode(a, axis=axis, nan_policy=nan_policy, keepdims=keepdims)\n \n       if a.size != 0 and axis == None and keepdims == True:\n         output_shape = tuple(1 for _ in a.shape)\n@@ -1748,11 +1721,10 @@ def testSEM(self, shape, dtype, axis, ddof, nan_policy, keepdims):\n     rng = jtu.rand_default(self.rng())\n     args_maker = lambda: [rng(shape, dtype)]\n \n-    kwds = {} if scipy_version < (1, 11) else {'keepdims': keepdims}\n     scipy_fun = partial(osp_stats.sem, axis=axis, ddof=ddof, nan_policy=nan_policy,\n-                        **kwds)\n+                        keepdims=keepdims)\n     lax_fun = partial(lsp_stats.sem, axis=axis, ddof=ddof, nan_policy=nan_policy,\n-                      **kwds)\n+                      keepdims=keepdims)\n     tol_spec = {np.float32: 2e-4, np.float64: 5e-6}\n     tol = jtu.tolerance(dtype, tol_spec)\n     self._CheckAgainstNumpy(scipy_fun, lax_fun, args_maker, check_dtypes=False,\n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Update partial eval to avoid DCEing a specific set of effects.",
        "output": "```diff\nCommit: a964f54dd1a2ff615771d23298eb1e8e6bdc82ef\nDate: 2025-06-02T14:29:02Z\nURL: https://github.com/jax-ml/jax/commit/a964f54dd1a2ff615771d23298eb1e8e6bdc82ef\nFiles changed: 6\nAdditions: +15, Deletions: -14\ndiff --git a/jax/_src/debugging.py b/jax/_src/debugging.py\nindex 3490de5118e1..e587d48cda68 100644\n--- a/jax/_src/debugging.py\n+++ b/jax/_src/debugging.py\n@@ -69,6 +69,8 @@ class OrderedDebugEffect(effects.Effect):\n effects.remat_allowed_effects.add_type(OrderedDebugEffect)\n effects.custom_derivatives_allowed_effects.add_type(DebugEffect)\n effects.custom_derivatives_allowed_effects.add_type(OrderedDebugEffect)\n+effects.partial_eval_kept_effects.add_type(DebugEffect)\n+effects.partial_eval_kept_effects.add_type(OrderedDebugEffect)\n \n # `debug_callback_p` is the main primitive for staging out Python callbacks.\n debug_callback_p = core.Primitive('debug_callback')\n@@ -126,10 +128,10 @@ def debug_callback_jvp_rule(primals, tangents, **params):\n   return debug_callback_p.bind(*primals, **params), []\n ad.primitive_jvps[debug_callback_p] = debug_callback_jvp_rule\n \n-def debug_callback_transpose_rule(*flat_args, callback: Callable[..., Any],\n+def debug_callback_transpose_rule(_, *flat_args, callback: Callable[..., Any],\n                                   effect: DebugEffect, partitioned):\n-  del flat_args, callback, effect\n-  raise ValueError(\"Transpose doesn't support debugging callbacks.\")\n+  del callback, effect, partitioned\n+  return [None for _ in flat_args]\n ad.primitive_transposes[debug_callback_p] = debug_callback_transpose_rule\n \n def _debug_callback_partial_auto(axis_context, *args, **params):\ndiff --git a/jax/_src/effects.py b/jax/_src/effects.py\nindex d55333540355..fb79c542e78b 100644\n--- a/jax/_src/effects.py\n+++ b/jax/_src/effects.py\n@@ -118,3 +118,5 @@ def filter_not_in(self, effects: Iterable[Effect]) -> list[Effect]:\n control_flow_allowed_effects: EffectTypeSet = EffectTypeSet()\n custom_derivatives_allowed_effects: EffectTypeSet = EffectTypeSet()\n remat_allowed_effects: EffectTypeSet = EffectTypeSet()\n+\n+partial_eval_kept_effects: EffectTypeSet = EffectTypeSet()\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 444b60f15fa5..1bcd3f00321c 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -43,7 +43,7 @@\n                            mapped_aval, unmapped_aval, DBIdx, InDBIdx, OutDBIdx,\n                            InputType, OutputType, get_referent, JaxprEqnContext)\n from jax._src.source_info_util import SourceInfo\n-from jax._src.state.types import AbstractRef, ReadEffect, RefEffect\n+from jax._src.state.types import AbstractRef, ReadEffect\n from jax._src.tree_util import (PyTreeDef, treedef_tuple, tree_flatten,\n                                 tree_structure, register_static)\n from jax._src.util import (unzip2, safe_zip, safe_map, toposort, split_list,\n@@ -240,23 +240,23 @@ def default_process_primitive(self, primitive, tracers, params):\n       return primitive.bind_with_trace(self.parent_trace, consts, params)\n     tracers = map(self.instantiate_const, tracers)\n     avals = [t.aval for t in tracers]\n-    out_aval, effects = primitive.abstract_eval(*avals, **params)\n+    out_aval, effs = primitive.abstract_eval(*avals, **params)\n     name_stack = self._current_truncated_name_stack()\n     source = source_info_util.current().replace(name_stack=name_stack)\n     if primitive.multiple_results:\n       out_tracers = [JaxprTracer(self, PartialVal.unknown(aval), None)\n                      for aval in out_aval]\n-      eqn = new_eqn_recipe(self, tracers, out_tracers, primitive, params, effects,\n+      eqn = new_eqn_recipe(self, tracers, out_tracers, primitive, params, effs,\n                            source)\n-      if any(isinstance(e, RefEffect) for e in effects):\n+      if effects.partial_eval_kept_effects.filter_in(effs):\n         self.effect_handles.append(EffectHandle(tracers, eqn))\n       for t in out_tracers: t.recipe = eqn\n       return out_tracers\n     else:\n       out_tracer = JaxprTracer(self, PartialVal.unknown(out_aval), None)\n       eqn = new_eqn_recipe(self, tracers, [out_tracer], primitive,\n-                           params, effects, source)\n-      if any(isinstance(e, RefEffect) for e in effects):\n+                           params, effs, source)\n+      if effects.partial_eval_kept_effects.filter_in(effs):\n         self.effect_handles.append(EffectHandle(tracers, eqn))\n       out_tracer.recipe = eqn\n       return out_tracer\ndiff --git a/jax/_src/state/types.py b/jax/_src/state/types.py\nindex af5bec49a6d5..e3a86e241bf2 100644\n--- a/jax/_src/state/types.py\n+++ b/jax/_src/state/types.py\n@@ -75,6 +75,7 @@ class AccumEffect(RefEffect):\n   name: str = \"Accum\"\n \n effects.control_flow_allowed_effects.add_type(RefEffect)\n+effects.partial_eval_kept_effects.add_type(RefEffect)\n \n StateEffect = Union[ReadEffect, WriteEffect, AccumEffect]\n \ndiff --git a/tests/debugging_primitives_test.py b/tests/debugging_primitives_test.py\nindex 7985cf841248..9c23f136b825 100644\n--- a/tests/debugging_primitives_test.py\n+++ b/tests/debugging_primitives_test.py\n@@ -442,8 +442,6 @@ def f(x):\n     with jtu.capture_stdout() as output:\n       jax.linear_transpose(f, 1.)(1.)\n       jax.effects_barrier()\n-    # `debug_print` should be dropped by `partial_eval` because of no\n-    # output data-dependence.\n     self.assertEqual(output(), \"\")\n \n   @jtu.sample_product(ordered=[False, True])\ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex 2f1e154627f4..3f950f865735 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -28,7 +28,6 @@\n \n import jax\n from jax._src import core\n-from jax._src import config\n from jax import dtypes\n from jax import lax\n from jax import random\n@@ -3359,8 +3358,7 @@ def f(c, _):\n       return c + 1, None\n     def g(x):\n       return jax.lax.scan(f, x, length=2)[0]\n-    with config.use_direct_linearize(True):\n-      jaxpr = jax.make_jaxpr(jax.value_and_grad(g))(1.0)\n+    jaxpr = jax.make_jaxpr(jax.value_and_grad(g))(1.0)\n     eqn_jaxpr = jaxpr.eqns[0].params[\"jaxpr\"]\n     self.assertIn(\"debug_callback\", [e.primitive.name for e in eqn_jaxpr.eqns])\n \n\n```",
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "[jaxlib] Use SafeStaticInit in more places.\n\nFixes a deadlock in free threading mode.\n\nPiperOrigin-RevId: 766165343",
        "output": "```diff\nCommit: 8f5dae48a30545ec631c4ea9c0c68869ad6856ab\nDate: 2025-06-02T13:21:15Z\nURL: https://github.com/jax-ml/jax/commit/8f5dae48a30545ec631c4ea9c0c68869ad6856ab\nFiles changed: 4\nAdditions: +29, Deletions: -22\ndiff --git a/jaxlib/BUILD b/jaxlib/BUILD\nindex 7be24ce5e825..834103063aae 100644\n--- a/jaxlib/BUILD\n+++ b/jaxlib/BUILD\n@@ -801,6 +801,7 @@ cc_library(\n         \"@xla//xla/pjrt:status_casters\",\n         \"@xla//xla/python:nb_helpers\",\n         \"@xla//xla/python:nb_numpy\",\n+        \"@xla//xla/python:safe_static_init\",\n         \"@xla//xla/python:types\",\n         \"@xla//xla/python/ifrt\",\n         \"@xla//xla/tsl/concurrency:ref_count\",\ndiff --git a/jaxlib/pmap_lib.cc b/jaxlib/pmap_lib.cc\nindex e18dd8b4637a..4a4e20f8f55b 100644\n--- a/jaxlib/pmap_lib.cc\n+++ b/jaxlib/pmap_lib.cc\n@@ -67,6 +67,7 @@ limitations under the License.\n #include \"xla/python/ifrt/sharding.h\"\n #include \"xla/python/nb_helpers.h\"\n #include \"xla/python/nb_numpy.h\"\n+#include \"xla/python/safe_static_init.h\"\n #include \"xla/python/types.h\"\n #include \"xla/status_macros.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n@@ -289,9 +290,10 @@ class PmapFunction {\n                                   size_t nargs, PyObject* kwnames);\n \n   nb::object PythonSignature() {\n-    static const auto* inspect =\n-        new nb::module_(nb::module_::import_(\"inspect\"));\n-    return inspect->attr(\"signature\")(fun_);\n+    const nb::module_& inspect = xla::SafeStaticInit<nb::module_>([]() {\n+      return std::make_unique<nb::module_>(nb::module_::import_(\"inspect\"));\n+    });\n+    return inspect.attr(\"signature\")(fun_);\n   }\n \n   int cache_size() {\ndiff --git a/jaxlib/py_array.cc b/jaxlib/py_array.cc\nindex 37932a2aed45..8659cba49dea 100644\n--- a/jaxlib/py_array.cc\n+++ b/jaxlib/py_array.cc\n@@ -94,6 +94,7 @@ limitations under the License.\n #include \"xla/python/pjrt_ifrt/pjrt_client.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_device.h\"\n #include \"xla/python/pjrt_ifrt/pjrt_dtype.h\"\n+#include \"xla/python/safe_static_init.h\"\n #include \"xla/python/types.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n@@ -393,16 +394,16 @@ nb::object MakeShapedArrayCached(const ShapedArrayCacheKey& key) {\n   static auto* lru_list = new CacheT::LRUList(4096);\n   static auto* cache = new CacheT(lru_list);\n \n-  static const nb::object* shaped_array = []() -> nb::object* {\n+  const nb::object& shaped_array = SafeStaticInit<nb::object>([]() {\n     nb::object jax_core;\n     try {\n       jax_core = nb::module_::import_(\"jax.core\");\n     } catch (nb::python_error& e) {\n-      return nullptr;\n+      return std::make_unique<nb::object>();\n     }\n-    return new nb::object(jax_core.attr(\"ShapedArray\"));\n-  }();\n-  if (!shaped_array) {\n+    return std::make_unique<nb::object>(jax_core.attr(\"ShapedArray\"));\n+  });\n+  if (!shaped_array.ptr()) {\n     return nb::none();\n   }\n \n@@ -415,7 +416,7 @@ nb::object MakeShapedArrayCached(const ShapedArrayCacheKey& key) {\n   if (!value->has_value()) {\n     nb_dtype dtype =\n         IfrtDtypeToDtypeWithTokenCanonicalization(key.dtype).value();\n-    nb::object aval = (*shaped_array)(\n+    nb::object aval = shaped_array(\n         SpanToNbTuple(absl::Span<const int64_t>(\n             key.dtype.kind() == ifrt::DType::kToken ? std::vector<int64_t>{0}\n                                                     : key.dims)),\ndiff --git a/jaxlib/py_values.cc b/jaxlib/py_values.cc\nindex 6ea5c272eea3..987a51eb67cf 100644\n--- a/jaxlib/py_values.cc\n+++ b/jaxlib/py_values.cc\n@@ -712,11 +712,12 @@ absl::StatusOr<ShardFn> MakeShardFn(nb::handle arg, ifrt::Client* client,\n }  // namespace\n \n bool IsFloat0(xla::nb_numpy_ndarray arg) {\n-  static const auto* dtypes_module =\n-      new nb::module_(nb::module_::import_(\"jax.dtypes\"));\n-  static const auto* float0_dtype =\n-      new nb::handle(dtypes_module->attr(\"float0\"));\n-  return float0_dtype->is(arg.attr(\"dtype\"));\n+  const nb::object& float0_dtype = SafeStaticInit<nb::object>([] {\n+    nb::module_ dtypes_module = nb::module_::import_(\"jax.dtypes\");\n+    nb::object float0_dtype = dtypes_module.attr(\"float0\");\n+    return std::make_unique<nb::object>(float0_dtype);\n+  });\n+  return float0_dtype.is(arg.attr(\"dtype\"));\n }\n \n std::string PyArgSignature::DebugString() const {\n@@ -734,9 +735,11 @@ using ToPyArgSignatureHandler =\n \n absl::StatusOr<PyArgSignature> PyArgSignatureOfValue(nb::handle arg,\n                                                      bool jax_enable_x64) {\n-  static const absl::flat_hash_map<PyObject*, ToPyArgSignatureHandler>* const\n-      handlers = [] {\n-        auto p = new absl::flat_hash_map<PyObject*, ToPyArgSignatureHandler>();\n+  const absl::flat_hash_map<PyObject*, ToPyArgSignatureHandler>& handlers =\n+      SafeStaticInit<\n+          absl::flat_hash_map<PyObject*, ToPyArgSignatureHandler>>([] {\n+        auto p = std::make_unique<\n+            absl::flat_hash_map<PyObject*, ToPyArgSignatureHandler>>();\n \n         const NumpyScalarTypes& dtypes = GetNumpyScalarTypes();\n \n@@ -881,7 +884,7 @@ absl::StatusOr<PyArgSignature> PyArgSignatureOfValue(nb::handle arg,\n         (*p)[dtypes.np_intc.ptr()] = numpy_array_handler;\n \n         return p;\n-      }();\n+      });\n \n   if (arg.type().ptr() == PyArray::type().ptr()) {\n     auto array = nb::borrow<PyArray>(arg);\n@@ -894,12 +897,12 @@ absl::StatusOr<PyArgSignature> PyArgSignatureOfValue(nb::handle arg,\n     return PyArgSignature(primitive_type, array.shape(), array.weak_type());\n   }\n \n-  auto res = handlers->find(arg.type().ptr());\n-  if (res == handlers->end()) {\n+  auto res = handlers.find(arg.type().ptr());\n+  if (res == handlers.end()) {\n     // We attempt to look at the MRO classes\n     for (auto base_class : arg.type().attr(\"__mro__\")) {\n-      res = handlers->find(base_class.ptr());\n-      if (res != handlers->end()) {\n+      res = handlers.find(base_class.ptr());\n+      if (res != handlers.end()) {\n         return res->second(arg, jax_enable_x64);\n       }\n     }\n\n```",
        "from_id": [
            "hawkinsp",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[JAX] Use `util.fun_name` to determine `WrappedFun.__name__` instead of trying to get the `f.__name__` attribute, which won't always exists.\n\nCo-authored-by: Keith Rush <krush@google.com>\nPiperOrigin-RevId: 766152892",
        "output": "```diff\nCommit: 27e454ddb81e2a382aafb4d5c89266df0be1bf2a\nDate: 2025-06-02T12:37:37Z\nURL: https://github.com/jax-ml/jax/commit/27e454ddb81e2a382aafb4d5c89266df0be1bf2a\nFiles changed: 3\nAdditions: +66, Deletions: -13\ndiff --git a/jax/_src/linear_util.py b/jax/_src/linear_util.py\nindex bfe87430554e..41af7644d361 100644\n--- a/jax/_src/linear_util.py\n+++ b/jax/_src/linear_util.py\n@@ -75,8 +75,8 @@ def trans1(static_arg, *dynamic_args, **kwargs):\n from jax._src import config\n from jax._src import core\n from jax._src import traceback_util\n-from jax._src.tree_util import keystr, KeyPath, generate_key_paths\n-from jax._src.util import curry, cache_clearing_funs, HashableFunction\n+from jax._src.tree_util import KeyPath, generate_key_paths, keystr\n+from jax._src.util import HashableFunction, cache_clearing_funs, curry, fun_name\n \n \n traceback_util.register_exclusion(__file__)\n@@ -186,7 +186,7 @@ def __init__(self, f: Callable,\n \n   @property\n   def __name__(self):\n-    return getattr(self.f, '__name__', '<unnamed wrapped function>')\n+    return fun_name(self.f, \"<unnamed wrapped function>\")\n \n   def wrap(self, gen, gen_static_args,\n            out_store: Store | EqualStore | None) -> WrappedFun:\n@@ -266,12 +266,6 @@ def transformation_with_aux2(\n   out_thunk = lambda: out_store.val\n   return fun.wrap(gen, gen_static_args, out_store), out_thunk\n \n-def fun_name(f):\n-  try:\n-    return f.__name__\n-  except:\n-    return str(f)\n-\n \n class DebugInfo(NamedTuple):\n   \"\"\"Debugging info about a func, its arguments, and results.\"\"\"\ndiff --git a/jax/_src/util.py b/jax/_src/util.py\nindex 34f748544d6d..dbdc746713fb 100644\n--- a/jax/_src/util.py\n+++ b/jax/_src/util.py\n@@ -367,14 +367,16 @@ def __eq__(self, other):\n def wrap_name(name: str, transform_name: str) -> str:\n   return transform_name + '(' + name + ')'\n \n-def fun_name(fun: Callable) -> str:\n+\n+def fun_name(fun: Callable, default_name: str = \"<unnamed function>\") -> str:\n   name = getattr(fun, \"__name__\", None)\n   if name is not None:\n     return name\n   if isinstance(fun, partial):\n     return fun_name(fun.func)\n   else:\n-    return \"<unnamed function>\"\n+    return default_name\n+\n \n def fun_qual_name(fun: Callable) -> str:\n   qual_name = getattr(fun, \"__qualname__\", None)\ndiff --git a/tests/util_test.py b/tests/util_test.py\nindex 90506117af8f..923240b69242 100644\n--- a/tests/util_test.py\n+++ b/tests/util_test.py\n@@ -12,16 +12,15 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from functools import partial\n import operator\n \n from absl.testing import absltest\n-\n import jax\n from jax import api_util\n from jax._src import linear_util as lu\n from jax._src import test_util as jtu\n from jax._src import util\n-\n from jax._src.util import weakref_lru_cache\n jax.config.parse_flags_with_absl()\n \n@@ -74,6 +73,64 @@ def kw_to_positional(f, store, factor, *args, **kwargs):\n     self.assertEqual(dict(three=6, four=8), scaled_kwargs)\n     self.assertEqual(2, out_thunk())\n \n+  def test_wrapped_fun_name(self):\n+    def my_function():\n+      return\n+\n+    with self.subTest(\"function\"):\n+      wrapped = lu.wrap_init(\n+          my_function,\n+          debug_info=api_util.debug_info(\"test\", my_function, (), {}),\n+      )\n+      self.assertEqual(wrapped.__name__, my_function.__name__)\n+\n+    with self.subTest(\"default_partial\"):\n+      my_partial = partial(my_function)\n+      wrapped = lu.wrap_init(\n+          my_partial,\n+          debug_info=api_util.debug_info(\"test\", my_partial, (), {}),\n+      )\n+      self.assertEqual(wrapped.__name__, my_function.__name__)\n+\n+    with self.subTest(\"nested_default_partial\"):\n+      my_partial = partial(partial(my_function))\n+      wrapped = lu.wrap_init(\n+          my_partial,\n+          debug_info=api_util.debug_info(\"test\", my_partial, (), {}),\n+      )\n+      self.assertEqual(wrapped.__name__, my_function.__name__)\n+\n+    with self.subTest(\"named_partial\"):\n+      my_partial = partial(my_function)\n+      my_partial.__name__ = \"my_partial\"\n+      wrapped = lu.wrap_init(\n+          my_partial,\n+          debug_info=api_util.debug_info(\"test\", my_partial, (), {}),\n+      )\n+      self.assertEqual(wrapped.__name__, my_partial.__name__)\n+\n+    with self.subTest(\"lambda\"):\n+      l = lambda: my_function()\n+      wrapped = lu.wrap_init(\n+          l,\n+          debug_info=api_util.debug_info(\"test\", l, (), {}),\n+      )\n+      self.assertEqual(wrapped.__name__, \"<lambda>\")\n+\n+    with self.subTest(\"unnamed_callable\"):\n+\n+      class MyCallable:\n+\n+        def __call__(self):\n+          return\n+\n+      my_callable = MyCallable()\n+      wrapped = lu.wrap_init(\n+          my_callable,\n+          debug_info=api_util.debug_info(\"test\", my_callable, (), {}),\n+      )\n+      self.assertEqual(wrapped.__name__, \"<unnamed wrapped function>\")\n+\n   def test_weakref_lru_cache(self):\n     @weakref_lru_cache\n     def example_cached_fn(key):\n\n```",
        "from_id": [
            "ZacharyGarrett",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Automated Code Change\n\nPiperOrigin-RevId: 766096798",
        "output": "```diff\nCommit: 1914815d763fc802f4efbbd90b059245aaa57bd5\nDate: 2025-06-02T09:19:19Z\nURL: https://github.com/jax-ml/jax/commit/1914815d763fc802f4efbbd90b059245aaa57bd5\nFiles changed: 2\nAdditions: +5, Deletions: -2\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 214521ce3764..01b7e015e461 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -20,9 +20,11 @@ limitations under the License.\n \n #include <algorithm>\n #include <array>\n+#include <cerrno>\n #include <cstdint>\n #include <cstdio>\n #include <cstdlib>\n+#include <cstring>\n #include <fstream>\n #include <iostream>\n #include <memory>\n@@ -40,9 +42,10 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n+#include \"absl/strings/numbers.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n-#include \"absl/strings/str_replace.h\"\n+#include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n // Leave this comment here. Internal Google business.\ndiff --git a/jaxlib/mosaic/gpu/target.cc b/jaxlib/mosaic/gpu/target.cc\nindex dfb119b410af..d26b1f1ccbf7 100644\n--- a/jaxlib/mosaic/gpu/target.cc\n+++ b/jaxlib/mosaic/gpu/target.cc\n@@ -16,7 +16,6 @@ limitations under the License.\n \n #include <memory>\n #include <string>\n-#include <utility>\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n@@ -24,6 +23,7 @@ limitations under the License.\n #include \"absl/strings/numbers.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/strings/string_view.h\"\n #include \"absl/strings/strip.h\"\n #include \"llvm/MC/MCSubtargetInfo.h\"\n #include \"llvm/MC/TargetRegistry.h\"\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Introduce profiler_options in the documentation.\n\nPiperOrigin-RevId: 766058161",
        "output": "```diff\nCommit: 52e5a87d785db75eadbf172d813495f8bfea7101\nDate: 2025-06-02T07:04:36Z\nURL: https://github.com/jax-ml/jax/commit/52e5a87d785db75eadbf172d813495f8bfea7101\nFiles changed: 1\nAdditions: +93, Deletions: -4\ndiff --git a/docs/profiling.md b/docs/profiling.md\nindex c33e79c1dc0c..c4a340684cd3 100644\n--- a/docs/profiling.md\n+++ b/docs/profiling.md\n@@ -100,10 +100,10 @@ pip install tb-nightly tbp-nightly\n ### Programmatic capture\n \n You can instrument your code to capture a profiler trace via the\n-{func}`jax.profiler.start_trace` and {func}`jax.profiler.stop_trace`\n-methods. Call {func}`~jax.profiler.start_trace` with the directory to write\n-trace files to. This should be the same `--logdir` directory used to start\n-TensorBoard. Then, you can use TensorBoard to view the traces.\n+{func}`jax.profiler.start_trace` and {func}`jax.profiler.stop_trace` methods.\n+Call {func}`~jax.profiler.start_trace` with the directory to write trace files\n+to. This should be the same `--logdir` directory used to start TensorBoard.\n+Then, you can use TensorBoard to view the traces.\n \n For example, to take a profiler trace:\n \n@@ -229,6 +229,95 @@ functions. You can add your own events and functions by using\n {class}`jax.profiler.TraceAnnotation` and {func}`jax.profiler.annotate_function` in\n your code.\n \n+### Configuring profiler options\n+\n+The `start_trace` method accepts an optional `profiler_options` parameter, which\n+allows for fine-grained control over the profiler's behavior. This parameter\n+should be an instance of `jax.profiler.ProfileOptions`.\n+<!-- TODO: Add API documentation for jax.profiler.ProfileOptions -->\n+\n+For example, to disable all python and host traces:\n+\n+```python\n+import jax\n+\n+options = jax.profiler.ProfileOptions()\n+options.python_tracer_level = 0\n+options.host_tracer_level = 0\n+jax.profiler.start_trace(\"/tmp/tensorboard\", profiler_options=options)\n+\n+# Run the operations to be profiled\n+key = jax.random.key(0)\n+x = jax.random.normal(key, (5000, 5000))\n+y = x @ x\n+y.block_until_ready()\n+\n+jax.profiler.stop_trace()\n+```\n+\n+#### General options\n+\n+1.  `host_tracer_level`: Sets the trace level for host-side activities.\n+\n+    Supported Values:\n+\n+    `0`: Disables host (CPU) tracing entirely.\n+\n+    `1`: Enables tracing of only user-instrumented TraceMe events (this is the\n+    default).\n+\n+    `2`: Includes level 1 traces plus high-level program execution details like\n+    expensive TensorFlow or XLA operations.\n+\n+    `3`: Includes level 2 traces plus more verbose, low-level program execution\n+    details such as cheap TensorFlow operations.\n+\n+2.  `python_tracer_level`: Controls whether Python tracing is enabled.\n+\n+    Supported Values:\n+\n+    `0`: Disables Python function call tracing.\n+\n+    `> 0`: Enables Python tracing (this is the default).\n+\n+#### Advanced configuration options\n+\n+1.  `tpu_trace_mode`: Specifies the mode for TPU tracing.\n+\n+    Supported Values:\n+\n+    `TRACE_ONLY_HOST`: This means only host-side (CPU) activities are traced,\n+    and no device (TPU/GPU) traces are collected.\n+\n+    `TRACE_ONLY_XLA`: This means only XLA-level operations on the device are\n+    traced.\n+\n+    `TRACE_COMPUTE`: This traces compute operations on the device.\n+\n+    `TRACE_COMPUTE_AND_SYNC`: This traces both compute operations and\n+    synchronization events on the device.\n+\n+    If \"tpu_trace_mode\" is not provided the trace_mode defaults to\n+    TRACE_ONLY_XLA.\n+\n+2.  `tpu_num_sparse_cores_to_trace`: Specifies the number of sparse cores to\n+    trace on the TPU.\n+3.  `tpu_num_sparse_core_tiles_to_trace`: Specifies the number of tiles within\n+    each sparse core to trace on the TPU.\n+4.  `tpu_num_chips_to_profile_per_task`: Specifies the number of TPU chips to\n+    profile per task.\n+\n+For example:\n+\n+```\n+options = ProfileOptions()\n+options.advanced_configuration = {\"tpu_trace_mode\" : \"TRACE_ONLY_HOST\", \"tpu_num_sparse_cores_to_trace\" : 2}\n+\n+```\n+\n+Returns InvalidArgumentError if any unrecognized keys or option values are\n+found.\n+\n ### Troubleshooting\n \n #### GPU profiling\n\n```",
        "from_id": [
            "sannidhyachauhan",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reverts 73c016a534af51614741d70d36c2c75ca59f2dcc\n\nPiperOrigin-RevId: 765852528",
        "output": "```diff\nCommit: 107efde069779d5b66821c867b725503fe48340e\nDate: 2025-06-01T14:26:54Z\nURL: https://github.com/jax-ml/jax/commit/107efde069779d5b66821c867b725503fe48340e\nFiles changed: 1\nAdditions: +12, Deletions: -3\ndiff --git a/jax/_src/named_sharding.py b/jax/_src/named_sharding.py\nindex 0b4efdb41d25..faf0b2a9f2b2 100644\n--- a/jax/_src/named_sharding.py\n+++ b/jax/_src/named_sharding.py\n@@ -288,6 +288,13 @@ def _custom_repr(self):\n     priority_repr = '' if self.priority is None else f'p{self.priority}'\n     return f'{{{axes_repr}{open_repr}}}{priority_repr}'\n \n+def _get_axes(axes, mesh_shape):\n+  if not axes:\n+    return ()\n+  assert mesh_shape is not None\n+  # Sort wrt mesh axis names so order is deterministic and doesn't hang in\n+  # McJAX.\n+  return tuple(n for n, _ in mesh_shape if n in axes)\n \n @dataclasses.dataclass(kw_only=True)\n class SdyArray:\n@@ -307,11 +314,13 @@ def build(self) -> sdy.TensorShardingAttr:\n           [sdy.MeshAxisAttr.get(name, size) for name, size in self.mesh_shape],\n           ldi)\n \n+    replicated_axes = _get_axes(self.replicated_axes, self.mesh_shape)\n+    unreduced_axes = _get_axes(self.unreduced_axes, self.mesh_shape)\n     return sdy.TensorShardingAttr.get(\n         mesh_attr,\n         [dim_sharding.build() for dim_sharding in self.dim_shardings],\n-        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in self.replicated_axes],\n-        unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in self.unreduced_axes])\n+        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes],\n+        unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in unreduced_axes])\n \n   def __repr__(self):\n     dim_sharding_repr = ', '.join(\n@@ -333,7 +342,7 @@ def modify_sdy_sharding_wrt_axis_types(sdy_sharding: SdyArray, mesh):\n       dim_shardings.append(SdyDim(axes=[], is_open=True)\n                            if not d.axes and not d.is_open else d)\n       used_axes.extend(d.axes)\n-    remaining_axes = tuple(n for n in mesh.axis_names if n not in used_axes)\n+    remaining_axes = set(mesh.axis_names) - set(used_axes)\n     replicated_axes = tuple(r for r in remaining_axes\n                             if mesh._name_to_type[r] == mesh_lib.AxisType.Explicit)\n     return SdyArray(mesh_shape=sdy_sharding.mesh_shape,\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Allow specifying non-differentiable arguments by name\n(`nondiff_argnames`) in addition to by index (`nondiff_argnums`).\nThe implementation normalizes `nondiff_argnames` to indices in the constructor\nand merges them with `nondiff_argnums`, allowing the rest of the custom derivative logic to continue using a unified list of indices.\n\nPiperOrigin-RevId: 765730837",
        "output": "```diff\nCommit: 0a1ada83ec979dfeba713d5f81f9d7684c52afa6\nDate: 2025-06-01T04:21:04Z\nURL: https://github.com/jax-ml/jax/commit/0a1ada83ec979dfeba713d5f81f9d7684c52afa6\nFiles changed: 2\nAdditions: +92, Deletions: -11\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex 2a09665f6285..87407efebd3a 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -31,7 +31,8 @@\n     stop_gradient_p, SymbolicZero, Zero, zeros_like_aval)\n from jax._src.api_util import (\n   argnums_partial, flatten_fun_nokwargs, resolve_kwargs,\n-  prepend_static_args, debug_info)\n+  prepend_static_args, debug_info, fun_signature,\n+  infer_argnums_and_argnames)\n from jax._src.errors import UnexpectedTracerError\n from jax._src.state.types import AbstractRef\n from jax._src.interpreters import ad\n@@ -133,16 +134,31 @@ def f_jvp(primals, tangents):\n   \"\"\"\n   fun: Callable[..., ReturnValue]\n   nondiff_argnums: Sequence[int]\n+  nondiff_argnames: Sequence[str]\n   jvp: Callable[..., tuple[ReturnValue, ReturnValue]] | None = None\n   symbolic_zeros: bool = False\n \n   def __init__(self,\n                fun: Callable[..., ReturnValue],\n                nondiff_argnums: Sequence[int] = (),\n+               nondiff_argnames: Sequence[str] = (),\n                ):\n     update_wrapper(self, fun)\n     self.fun = fun\n-    self.nondiff_argnums = nondiff_argnums\n+\n+    nondiff_argnums_: set[int] = set()\n+    if nondiff_argnames:\n+      sig = fun_signature(self.fun)\n+      assert sig is not None\n+      inferred_nondiff_argnums, _ = infer_argnums_and_argnames(\n+          sig, None, nondiff_argnames\n+      )\n+      nondiff_argnums_.update(inferred_nondiff_argnums)\n+\n+    if nondiff_argnums:\n+      nondiff_argnums_.update(nondiff_argnums)\n+\n+    self.nondiff_argnums = tuple(sorted(nondiff_argnums_))\n \n   __getattr__ = custom_api_util.forward_attr\n \n@@ -259,10 +275,9 @@ def __call__(self, *args: Any, **kwargs: Any) -> ReturnValue:  # pytype: disable\n       ) from e\n \n     if self.nondiff_argnums:\n-      nondiff_argnums = set(self.nondiff_argnums)\n-      args = tuple(_stop_gradient(x) if i in nondiff_argnums else x\n+      args = tuple(_stop_gradient(x) if i in self.nondiff_argnums else x\n                    for i, x in enumerate(args))\n-      diff_argnums = [i for i in range(len(args)) if i not in nondiff_argnums]\n+      diff_argnums = [i for i in range(len(args)) if i not in self.nondiff_argnums]\n       f_, dyn_args = argnums_partial(lu.wrap_init(self.fun, debug_info=debug),\n                                      diff_argnums, args,\n                                      require_static_args_hashable=False)\n@@ -536,10 +551,24 @@ def f_bwd(res, g):\n \n   def __init__(self,\n                fun: Callable[..., ReturnValue],\n-               nondiff_argnums: Sequence[int] = ()):\n+               nondiff_argnums: Sequence[int] = (),\n+               nondiff_argnames: Sequence[str] = ()):\n     update_wrapper(self, fun)\n     self.fun = fun\n-    self.nondiff_argnums = nondiff_argnums\n+\n+    nondiff_argnums_: set[int] = set()\n+    if nondiff_argnames:\n+      sig = fun_signature(self.fun)\n+      assert sig is not None\n+      inferred_nondiff_argnums, _ = infer_argnums_and_argnames(\n+          sig, None, nondiff_argnames\n+      )\n+      nondiff_argnums_.update(inferred_nondiff_argnums)\n+\n+    if nondiff_argnums:\n+      nondiff_argnums_.update(nondiff_argnums)\n+\n+    self.nondiff_argnums = tuple(sorted(nondiff_argnums_))\n     self.fwd: Callable[..., tuple[ReturnValue, Any]] | None = None\n     self.bwd: Callable[..., tuple[Any, ...]] | None = None\n     self.symbolic_zeros = False\n@@ -681,8 +710,7 @@ def __call__(self, *args: Any, **kwargs: Any) -> ReturnValue:  # pytype: disable\n     else:\n       if self.nondiff_argnums:\n         for i in self.nondiff_argnums: _check_for_tracers(args[i])\n-        nondiff_argnums = set(self.nondiff_argnums)\n-        dyn_argnums = [i for i in range(len(args)) if i not in nondiff_argnums]\n+        dyn_argnums = [i for i in range(len(args)) if i not in self.nondiff_argnums]\n         f_, dyn_args = argnums_partial(\n             lu.wrap_init(self.fun, debug_info=debug_fun), dyn_argnums,\n             args, require_static_args_hashable=False)\ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex bfe391797920..45434923d543 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -330,7 +330,7 @@ def g_jvp(primals, tangents):\n     self.assertRaises(UnexpectedTracerError, lambda: api.jvp(f, (3.,), (1.,)))\n     self.assertRaises(UnexpectedTracerError, lambda: api.grad(f)(3.))\n \n-  def test_nondiff_arg(self):\n+  def test_nondiff_argnums(self):\n     @partial(jax.custom_jvp, nondiff_argnums=(0,))\n     def app(f, x):\n       return f(x)\n@@ -347,6 +347,21 @@ def app_jvp(f, primals, tangents):\n     expected = (2., 3.)\n     self.assertAllClose(ans, expected, check_dtypes=False)\n \n+  def test_nondiff_argnames(self):\n+    @partial(jax.custom_jvp, nondiff_argnames=('f',))\n+    def app(f, x):\n+      return f(x)\n+\n+    def app_jvp(f, primals, tangents):\n+      (x,), (t,) = primals, tangents\n+      return app(f, x), 3 * t\n+\n+    app.defjvp(app_jvp)\n+\n+    ans = app(lambda x: 2 * x, 1)\n+    expected = 2\n+    self.assertAllClose(ans, expected, check_dtypes=False)\n+\n   def test_nondiff_arg_jit_tracer(self):\n     # This test would pass with \"final-style\" JIT tracing, but that was\n     # misleading: it doesn't work with \"initial-style\" staging, i.e. control\n@@ -1655,7 +1670,7 @@ def foo(x):\n     expected = 2. * jnp.cos(jnp.arange(3.))\n     self.assertAllClose(ans, expected, check_dtypes=False)\n \n-  def test_nondiff_arg(self):\n+  def test_nondiff_argnums(self):\n     @partial(jax.custom_vjp, nondiff_argnums=(0,))\n     def app(f, x):\n       return f(x)\n@@ -1673,6 +1688,44 @@ def app_rev(f, cos_x, g):\n     expected = (2., jnp.cos(1.))\n     self.assertAllClose(ans, expected, check_dtypes=False)\n \n+  def test_nondiff_argnames(self):\n+    @partial(jax.custom_vjp, nondiff_argnames=('f',))\n+    def app(f, x):\n+      return f(x)\n+    def app_fwd(f, x):\n+      return app(f, x), jnp.cos(x)\n+    def app_rev(f, cos_x, g):\n+      return (cos_x * g,)\n+    app.defvjp(app_fwd, app_rev)\n+\n+    ans = app(lambda x: 2 * x, 1)\n+    expected = 2\n+    self.assertAllClose(ans, expected, check_dtypes=False)\n+\n+    ans = api.value_and_grad(lambda x: app(lambda y: 2 * y, x))(1.)\n+    expected = (2., jnp.cos(1.))\n+    self.assertAllClose(ans, expected, check_dtypes=False)\n+\n+  def test_nondiff_argnums_argnames(self):\n+    @partial(jax.custom_vjp, nondiff_argnums=(0,), nondiff_argnames=('g',))\n+    def app(f, g, x):\n+      return f(x) + g(x)\n+    def app_fwd(f, g, x):\n+      return app(f, g, x), jnp.cos(x)\n+    def app_rev(f, g, cos_x, v):\n+      return (cos_x * v,)\n+    app.defvjp(app_fwd, app_rev)\n+\n+    f = lambda x: 2 * x\n+    g = lambda x: 2 * x\n+    ans = app(f, g, 1)\n+    expected = 4\n+    self.assertAllClose(ans, expected, check_dtypes=False)\n+\n+    ans = api.value_and_grad(lambda x: app(f, g, x))(1.)\n+    expected = (4., jnp.cos(1.))\n+    self.assertAllClose(ans, expected, check_dtypes=False)\n+\n   def test_closed_over_jit_tracer(self):\n     # See the comment in CustomJVPTest.test_nondiff_arg_jit_tracer.\n     raise unittest.SkipTest(\"behavior no longer supported\")\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "test_binary_ufunc_reduce now also tests behavior with the initial and where parameters.\ntest_binary_ufunc_reduce_where has been removed.\nadd uint32 to tests\n\ncorrect _logsumexp and _reduce_bitwise_and\n\nimplement _logsumexp2 without _logsumexp\n\nNow jnp.minimum and jnp.maximum are ufuncs.\n\nfix for ruff and mypy\n\nfix\n\nfix mypy\n\nfix",
        "output": "```diff\nCommit: 5cca31fa2b411f332c19b972ed966983917d55e7\nDate: 2025-05-31T16:03:34Z\nURL: https://github.com/jax-ml/jax/commit/5cca31fa2b411f332c19b972ed966983917d55e7\nFiles changed: 5\nAdditions: +77, Deletions: -20\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex ad2b3ad6aa75..b926662fd777 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -3445,7 +3445,7 @@ def clip(\n   if min is not None:\n     arr = ufuncs.maximum(min, arr)\n   if max is not None:\n-    arr = ufuncs.minimum(max, arr)\n+    arr = ufuncs.minimum(max, arr) # type: ignore\n   return asarray(arr)\n \n \ndiff --git a/jax/_src/numpy/reductions.py b/jax/_src/numpy/reductions.py\nindex 9cb543d5d869..cbfda25eafcf 100644\n--- a/jax/_src/numpy/reductions.py\n+++ b/jax/_src/numpy/reductions.py\n@@ -33,6 +33,7 @@\n     _broadcast_to, ensure_arraylike,\n     promote_dtypes_inexact, promote_dtypes_numeric, _where)\n from jax._src.lax import lax as lax_internal\n+from jax._src.lax import other as lax_other\n from jax._src.typing import Array, ArrayLike, DType, DTypeLike, DeprecatedArg\n from jax._src.util import (\n     canonicalize_axis as _canonicalize_axis, maybe_named_axis,\n@@ -398,11 +399,11 @@ def prod(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None = None,\n \n \n @partial(api.jit, static_argnames=('axis', 'keepdims'), inline=True)\n-def _reduce_max(a: ArrayLike, axis: Axis = None, out: None = None,\n-                keepdims: bool = False, initial: ArrayLike | None = None,\n-                where: ArrayLike | None = None) -> Array:\n+def _reduce_max(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None = None,\n+                out: None = None, keepdims: bool = False,\n+                initial: ArrayLike | None = None, where: ArrayLike | None = None) -> Array:\n   return _reduction(a, \"max\", lax.max, -np.inf, has_identity=False,\n-                    axis=axis, out=out, keepdims=keepdims,\n+                    axis=axis, dtype=dtype, out=out, keepdims=keepdims,\n                     initial=initial, where_=where, parallel_reduce=lax.pmax)\n \n \n@@ -480,12 +481,12 @@ def max(a: ArrayLike, axis: Axis = None, out: None = None,\n   return _reduce_max(a, axis=_ensure_optional_axes(axis), out=out,\n                      keepdims=keepdims, initial=initial, where=where)\n \n-@partial(api.jit, static_argnames=('axis', 'keepdims'), inline=True)\n-def _reduce_min(a: ArrayLike, axis: Axis = None, out: None = None,\n-                keepdims: bool = False, initial: ArrayLike | None = None,\n-                where: ArrayLike | None = None) -> Array:\n+@partial(api.jit, static_argnames=('axis', 'keepdims', 'dtype'), inline=True)\n+def _reduce_min(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None = None,\n+                out: None = None, keepdims: bool = False,\n+                initial: ArrayLike | None = None, where: ArrayLike | None = None) -> Array:\n   return _reduction(a, \"min\", lax.min, np.inf, has_identity=False,\n-                    axis=axis, out=out, keepdims=keepdims,\n+                    axis=axis, dtype=dtype, out=out, keepdims=keepdims,\n                     initial=initial, where_=where, parallel_reduce=lax.pmin)\n \n \n@@ -682,7 +683,7 @@ def _reduce_bitwise_and(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None\n                         out: None = None, keepdims: bool = False,\n                         initial: ArrayLike | None = None, where: ArrayLike | None = None) -> Array:\n   arr = lax_internal.asarray(a)\n-  init_val = np.array(-1, dtype=dtype or arr.dtype)\n+  init_val = np.array(-1).astype(dtype or arr.dtype)\n   return _reduction(arr, name=\"reduce_bitwise_and\", op=lax.bitwise_and, init_val=init_val, preproc=_require_integer,\n                     axis=_ensure_optional_axes(axis), dtype=dtype, out=out, keepdims=keepdims,\n                     initial=initial, where_=where)\n@@ -750,7 +751,7 @@ def _logsumexp(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None = None,\n   exp_a = lax.exp(lax.sub(a_arr, amax_with_dims.astype(a_arr.dtype)))\n   sumexp = exp_a.sum(axis=dims, keepdims=keepdims, where=where)\n   result = lax.add(lax.log(sumexp), amax.astype(sumexp.dtype))\n-  return result if initial is None else lax.logaddexp(initial, result)\n+  return result if initial is None else lax_other.logaddexp(initial, result)\n \n \n def _logsumexp2(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None = None,\n@@ -768,7 +769,6 @@ def _logsumexp2(a: ArrayLike, axis: Axis = None, dtype: DTypeLike | None = None,\n   return _logsumexp(a * ln2, axis=axis, dtype=dtype, keepdims=keepdims,\n                     where=where, initial=initial) / ln2\n \n-\n @export\n def amin(a: ArrayLike, axis: Axis = None, out: None = None,\n         keepdims: bool = False, initial: ArrayLike | None = None,\ndiff --git a/jax/_src/numpy/ufuncs.py b/jax/_src/numpy/ufuncs.py\nindex d722534e3136..486d3f15e17c 100644\n--- a/jax/_src/numpy/ufuncs.py\n+++ b/jax/_src/numpy/ufuncs.py\n@@ -1626,8 +1626,7 @@ def arctan2(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n   return lax.atan2(*promote_args_inexact(\"arctan2\", x1, x2))\n \n \n-@export\n-@partial(jit, inline=True)\n+@binary_ufunc(identity=None, reduce=reductions._reduce_min)\n def minimum(x: ArrayLike, y: ArrayLike, /) -> Array:\n   \"\"\"Return element-wise minimum of the input arrays.\n \n@@ -1687,8 +1686,7 @@ def minimum(x: ArrayLike, y: ArrayLike, /) -> Array:\n   return lax.min(*promote_args(\"minimum\", x, y))\n \n \n-@export\n-@partial(jit, inline=True)\n+@binary_ufunc(identity=None, reduce=reductions._reduce_max)\n def maximum(x: ArrayLike, y: ArrayLike, /) -> Array:\n   \"\"\"Return element-wise maximum of the input arrays.\n \ndiff --git a/jax/numpy/__init__.pyi b/jax/numpy/__init__.pyi\nindex 4db407861f34..e81d97765121 100644\n--- a/jax/numpy/__init__.pyi\n+++ b/jax/numpy/__init__.pyi\n@@ -653,7 +653,7 @@ def matvec(x1: ArrayLike, x2: ArrayLike, /) -> Array: ...\n def max(a: ArrayLike, axis: _Axis = ..., out: None = ...,\n         keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n         where: ArrayLike | None = ...) -> Array: ...\n-def maximum(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n+maximum: BinaryUfunc\n def mean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n          out: None = ..., keepdims: builtins.bool = ..., *,\n          where: ArrayLike | None = ...) -> Array: ...\n@@ -666,7 +666,7 @@ mgrid: _Mgrid\n def min(a: ArrayLike, axis: _Axis = ..., out: None = ...,\n         keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n         where: ArrayLike | None = ...) -> Array: ...\n-def minimum(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n+minimum: BinaryUfunc\n def mod(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n def modf(x: ArrayLike, /, out=None) -> tuple[Array, Array]: ...\n def moveaxis(a: ArrayLike, source: int | Sequence[int],\ndiff --git a/tests/lax_numpy_ufuncs_test.py b/tests/lax_numpy_ufuncs_test.py\nindex fd5050a5829b..905d7eed1acd 100644\n--- a/tests/lax_numpy_ufuncs_test.py\n+++ b/tests/lax_numpy_ufuncs_test.py\n@@ -56,7 +56,7 @@ def _jnp_ufunc_props(name):\n   jnp_func = getattr(jnp, name)\n   assert isinstance(jnp_func, jnp.ufunc)\n   np_func = getattr(np, name)\n-  dtypes = [np.dtype(c) for c in \"Ffi?\" if f\"{c}{c}->{c}\" in np_func.types or f\"{c}->{c}\" in np_func.types]\n+  dtypes = [np.dtype(c) for c in \"FfIi?\" if f\"{c}{c}->{c}\" in np_func.types or f\"{c}->{c}\" in np_func.types]\n   return [dict(name=name, dtype=dtype) for dtype in dtypes]\n \n \n@@ -242,6 +242,7 @@ def test_frompyfunc_reduce(self, func, nin, nout, identity, shape, axis, dtype):\n     self._CheckAgainstNumpy(jnp_fun, np_fun, args_maker)\n     self._CompileAndCheck(jnp_fun, args_maker)\n \n+\n   @jtu.sample_product(\n       BINARY_UFUNCS_WITH_DTYPES,\n       [{'shape': shape, 'axis': axis}\n@@ -324,6 +325,64 @@ def test_binary_ufunc_reduce_where(self, name, shape, axis, dtype):\n     self._CheckAgainstNumpy(jnp_fun_reduce, np_fun_reduce, args_maker, tol=tol)\n     self._CompileAndCheck(jnp_fun_reduce, args_maker)\n \n+  @jtu.sample_product(\n+      BINARY_UFUNCS_WITH_DTYPES,\n+      [{'shape': shape, 'axis': axis}\n+       for shape in nonscalar_shapes\n+       for axis in [None, *range(-len(shape), len(shape))]],\n+  )\n+  def test_binary_ufunc_reduce_initial(self, name, shape, axis, dtype):\n+    jnp_fun = getattr(jnp, name)\n+    np_fun = getattr(np, name)\n+\n+    if jnp_fun.identity is None and axis is None and len(shape) > 1:\n+      self.skipTest(\"Multiple-axis reduction over non-reorderable ufunc.\")\n+\n+    jnp_fun_reduce = lambda a, initial: jnp_fun.reduce(a, axis=axis, initial=initial)\n+    np_fun_reduce = lambda a, initial: np_fun.reduce(a, axis=axis, initial=initial)\n+\n+    rng = jtu.rand_default(self.rng())\n+    rng_initial = jtu.rand_default(self.rng())\n+    args_maker = lambda: [rng(shape, dtype), rng_initial((), dtype)]\n+\n+    tol = {np.float32: 1E-4} if jtu.test_device_matches(['tpu']) else None\n+\n+    self._CheckAgainstNumpy(jnp_fun_reduce, np_fun_reduce, args_maker, tol=tol)\n+    self._CompileAndCheck(jnp_fun_reduce, args_maker)\n+\n+  @jtu.sample_product(\n+      BINARY_UFUNCS_WITH_DTYPES,\n+      [{'shape': shape, 'axis': axis}\n+      for shape in nonscalar_shapes\n+      for axis in [None, *range(-len(shape), len(shape))]],\n+  )\n+  def test_binary_ufunc_reduce_where_initial(self, name, shape, axis, dtype):\n+      jnp_fun = getattr(jnp, name)\n+      np_fun = getattr(np, name)\n+\n+      # Skip if the ufunc doesn't have an identity and we're doing a multi-axis reduction\n+      if jnp_fun.identity is None and axis is None and len(shape) > 1:\n+          self.skipTest(\"Multiple-axis reduction over non-reorderable ufunc.\")\n+\n+      jnp_fun_reduce = lambda a, where, initial: jnp_fun.reduce(\n+          a, axis=axis, where=where, initial=initial)\n+      np_fun_reduce = lambda a, where, initial: np_fun.reduce(\n+          a, axis=axis, where=where, initial=initial)\n+\n+      rng = jtu.rand_default(self.rng())\n+      rng_where = jtu.rand_bool(self.rng())\n+      rng_initial = jtu.rand_default(self.rng())\n+      args_maker = lambda: [\n+          rng(shape, dtype),\n+          rng_where(shape, bool),\n+          rng_initial((), dtype)\n+      ]\n+\n+      tol = {np.float32: 1E-4} if jtu.test_device_matches(['tpu']) else None\n+\n+      self._CheckAgainstNumpy(jnp_fun_reduce, np_fun_reduce, args_maker, tol=tol)\n+      self._CompileAndCheck(jnp_fun_reduce, args_maker)\n+\n   @jtu.sample_product(\n       SCALAR_FUNCS,\n       [{'shape': shape, 'axis': axis}\n\n```",
        "from_id": [
            "DanisNone"
        ]
    },
    {
        "text_input": "[Mosaic] Move i1 broadcast lowering logic to Mosaic.\n\nAnd relax the test skip conditions. Somehow we skipped everything before. Also, this should fix https://github.com/jax-ml/jax/issues/29092.\n\nPiperOrigin-RevId: 765380392",
        "output": "```diff\nCommit: 6c18aa8a468e35b8c11b101dceaa43d05b497177\nDate: 2025-05-30T23:33:32Z\nURL: https://github.com/jax-ml/jax/commit/6c18aa8a468e35b8c11b101dceaa43d05b497177\nFiles changed: 3\nAdditions: +69, Deletions: -25\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex e2dfe526ea14..873bf587093a 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -1928,20 +1928,6 @@ def _broadcast_in_dim_lowering_rule(\n   if aval_in.shape == shape:\n     return val\n \n-  if jnp.issubdtype(aval_in.dtype, jnp.bool_):\n-    # Direct broadcasts for bools are not supported in Mosaic due to booleans\n-    # living in mask registers and broadcast operating on vregs. Broadcast as an\n-    # integer instead and cast back to a bool.\n-    # TODO(b/351019164): Implement this logic in Mosaic BroadcastOp instead.\n-    def _proxy_fun(val, *, shape, broadcast_dimensions):\n-      int_val = jnp.where(val, 1, 0)\n-      bcast_val = jax.lax.broadcast_in_dim(int_val, shape, broadcast_dimensions)\n-      return bcast_val == 1\n-    proxy_lowering = lower_fun(\n-        _proxy_fun, multiple_results=False)\n-    return proxy_lowering(\n-        ctx, val, shape=shape, broadcast_dimensions=broadcast_dimensions)\n-\n   if broadcast_dimensions:\n     out_shape_list = [1] * len(shape)\n     for i, s in zip(broadcast_dimensions, aval_in.shape):\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\nindex c963cff0be50..733863546935 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n@@ -566,6 +566,44 @@ LogicalResult canonicalize_extract(const CanonicalizeContext &ctx,\n   return success();\n }\n \n+LogicalResult canonicalize_broadcast(const CanonicalizeContext &ctx,\n+                                     Operation &raw_op) {\n+  auto op = dyn_cast<vector::BroadcastOp>(raw_op);\n+  auto src_ty = op.getSource().getType();\n+  auto src_vty = dyn_cast<VectorType>(src_ty);\n+  if ((src_vty && src_vty.getElementType().isSignlessInteger(1)) ||\n+      op.getSource().getType().isSignlessInteger(1)) {\n+    // Canonicalize i1 broadcast.\n+    // i1 represents vmsk in Mosaic and TPU doesn't support vmsk replication\n+    // directly.\n+    // Instead, convert i1 to i32 vector, broadcast i32, and then convert it\n+    // back to i1.\n+    ImplicitLocOpBuilder builder(op->getLoc(), op.getOperation());\n+    Value i32_src;\n+    if (src_vty) {\n+      i32_src = builder.create<arith::ExtUIOp>(\n+          VectorType::get(src_vty.getShape(), builder.getI32Type()),\n+          op.getSource());\n+    } else {\n+      i32_src =\n+          builder.create<arith::ExtUIOp>(builder.getI32Type(), op.getSource());\n+    }\n+    auto i32_res_vty =\n+        VectorType::get(op.getType().getShape(), builder.getI32Type());\n+    auto bcast = builder.create<vector::BroadcastOp>(i32_res_vty, i32_src);\n+    auto ones = builder.create<arith::ConstantOp>(\n+        i32_res_vty,\n+        SplatElementsAttr::get(i32_res_vty,\n+                               builder.getOneAttr(builder.getI32Type())));\n+    auto cmp =\n+        builder.create<arith::CmpIOp>(arith::CmpIPredicate::eq, bcast, ones);\n+    op.replaceAllUsesWith(cmp.getResult());\n+    op.erase();\n+    return success();\n+  }\n+  return success();\n+}\n+\n LogicalResult canonicalize_select(const CanonicalizeContext &ctx,\n                                   Operation &raw_op) {\n   auto op = dyn_cast<arith::SelectOp>(raw_op);\n@@ -920,6 +958,7 @@ const llvm::StringMap<canonicalize_rule_type> &rules() {\n        canonicalize_multi_dim_reduction},\n       {vector::TransposeOp::getOperationName(), canonicalize_vector_transpose},\n       {vector::ShapeCastOp::getOperationName(), canonicalize_reshape},\n+      {vector::BroadcastOp::getOperationName(), canonicalize_broadcast},\n       {arith::SelectOp::getOperationName(), canonicalize_select},\n       {arith::FPToSIOp::getOperationName(), canonicalize_fptosi},\n       {arith::SIToFPOp::getOperationName(), canonicalize_sitofp},\ndiff --git a/tests/pallas/ops_test.py b/tests/pallas/ops_test.py\nindex c819d050c8a5..e951a9fda827 100644\n--- a/tests/pallas/ops_test.py\n+++ b/tests/pallas/ops_test.py\n@@ -1822,30 +1822,49 @@ def copyitem(x_ref, in_idx_ref, out_idx_ref, o_ref):\n         np.testing.assert_allclose(out[oi], x[ii])\n         np.testing.assert_allclose(out[oi + 1 :], jnp.zeros_like(out[oi + 1 :]))\n \n-  @parameterized.parameters(\n-      ((), (2,), ()),\n-      ((1,), (2,), (0,)),\n-      ((1, 1), (2, 2), (0, 1)),\n-      ((), (2, 2), ()),\n+  @parameterized.product(\n+      shape_spec=[\n+          ((), (2,), ()),\n+          ((1,), (2,), (0,)),\n+          ((1, 128), (8, 128), (0, 1)),  # row broadcasting\n+          ((), (2, 2), ()),\n+      ],\n+      dtype=[jnp.int32, jnp.int16, jnp.int8, jnp.bool_],\n   )\n-  def test_broadcast_in_dim(self, in_shape, out_shape, dims):\n+  def test_broadcast_in_dim(self, shape_spec, dtype):\n     self.skip_if_mosaic_gpu()\n \n-    # The Pallas TPU lowering currently supports only blocks of rank >= 1\n+    in_shape, out_shape, dims = shape_spec\n     if jtu.test_device_matches([\"tpu\"]):\n-      self.skipTest(\"Not supported on TPU\")\n+      if not in_shape:\n+        self.skipTest(\n+            \"The Pallas TPU lowering currently supports only blocks of rank\"\n+            \" >= 1\"\n+        )\n+      if dtype is jnp.bool_ and not jtu.if_cloud_tpu_at_least(2025, 6, 5):\n+        self.skipTest(\"Requires libtpu built after 2025-06-05\")\n+      if (\n+          len(in_shape) == 1\n+          and len(out_shape) == 1\n+          and dtype not in {jnp.int32, jnp.bool_}\n+      ):\n+        self.skipTest(\"Unsupported tiling\")\n \n     @functools.partial(\n         self.pallas_call,\n-        out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32),\n+        out_shape=jax.ShapeDtypeStruct(out_shape, dtype),\n     )\n     def f(x_ref, o_ref):\n       x = x_ref[...]\n       o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n \n-    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n+    x = (\n+        jnp.arange(math.prod(in_shape), dtype=jnp.int32)\n+        .reshape(in_shape)\n+        .astype(dtype)\n+    )\n     expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n-    np.testing.assert_allclose(f(x), expected)\n+    np.testing.assert_array_equal(f(x), expected)\n \n   @parameterized.product(\n       lhs_and_rhs_shape=[\n\n```",
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Allow setting non-string TPU runtime flags. For example:\njax.config.update(\"jax_pjrt_client_create_options\", {\"max_inflight_computations\": 64})\n\nPiperOrigin-RevId: 765357524",
        "output": "```diff\nCommit: 26228f5a0c5dc45147805bd5535443f2f2213dbe\nDate: 2025-05-30T22:24:31Z\nURL: https://github.com/jax-ml/jax/commit/26228f5a0c5dc45147805bd5535443f2f2213dbe\nFiles changed: 2\nAdditions: +23, Deletions: -14\ndiff --git a/jax/_src/config.py b/jax/_src/config.py\nindex e79993958349..9d19ceb8b261 100644\n--- a/jax/_src/config.py\n+++ b/jax/_src/config.py\n@@ -940,11 +940,17 @@ def enum_flag(name, default, *args, **kwargs) -> Flag[str]:\n         'otherwise.'\n         ))\n \n-jax_pjrt_client_create_options = optional_string_state(\n+def _validate_jax_pjrt_client_create_options(new_val):\n+  if new_val is not None and not isinstance(new_val, (str, dict)):\n+      raise ValueError('new string config value must be None or of type dict'\n+                       f' | str, got {new_val} of type {type(new_val)}.')\n+\n+jax_pjrt_client_create_options = string_or_object_state(\n     name='jax_pjrt_client_create_options',\n     default=None,\n     help=('A set of key-value pairs in the format of \"k1:v1;k2:v2\" strings '\n-          'provided to a device platform pjrt client as extra arguments.'))\n+          'provided to a device platform pjrt client as extra arguments.'),\n+    validator=_validate_jax_pjrt_client_create_options)\n \n enable_checks = bool_state(\n     name='jax_enable_checks',\ndiff --git a/jax/_src/xla_bridge.py b/jax/_src/xla_bridge.py\nindex ce0c36fdcca4..22e71d7d9ce6 100644\n--- a/jax/_src/xla_bridge.py\n+++ b/jax/_src/xla_bridge.py\n@@ -449,18 +449,21 @@ def _options_from_jax_configs(plugin_name):\n   options = {}\n \n   pjrt_client_options = config.jax_pjrt_client_create_options.value\n-  pjrt_client_option_list = []\n-  if pjrt_client_options:\n-    pjrt_client_option_list = pjrt_client_options.split(\";\")\n-\n-  for option in pjrt_client_option_list:\n-    option_list = option.split(\":\")\n-    if (len(option_list) != 2):\n-      raise RuntimeError(\n-          \"Multiple ':' separators for option in \"\n-          f\"jax_pjrt_client_create_options: '{option}'. \"\n-          \"Should be in format 'key:value'\")\n-    options[option_list[0]] = option_list[1]\n+  if isinstance(pjrt_client_options, str):\n+    pjrt_client_option_list = []\n+    if pjrt_client_options:\n+      pjrt_client_option_list = pjrt_client_options.split(\";\")\n+\n+    for option in pjrt_client_option_list:\n+      option_list = option.split(\":\")\n+      if (len(option_list) != 2):\n+        raise RuntimeError(\n+            \"Multiple ':' separators for option in \"\n+            f\"jax_pjrt_client_create_options: '{option}'. \"\n+            \"Should be in format 'key:value'\")\n+      options[option_list[0]] = option_list[1]\n+  elif isinstance(pjrt_client_options, dict):\n+    options.update(pjrt_client_options)\n \n   if plugin_name in (\"cuda\", \"rocm\"):\n     visible_devices = (CUDA_VISIBLE_DEVICES.value if plugin_name == \"cuda\"\n\n```",
        "from_id": [
            "pschuh",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Refactor jax/_src/api.py and associated files in preparation for moving them to their own BUILD rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nThis change stops short of actually making the main jax package build rule depend on the new api build rule, because some downstream targets need to be migrated and pytype errors need to be fixed before we can land the final change.\n\nPiperOrigin-RevId: 765341918",
        "output": "```diff\nCommit: 22f04d92fc1c6c9fa85dd486862f88fce36964f9\nDate: 2025-05-30T21:38:04Z\nURL: https://github.com/jax-ml/jax/commit/22f04d92fc1c6c9fa85dd486862f88fce36964f9\nFiles changed: 11\nAdditions: +135, Deletions: -79\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 91a6e5926e42..67fc208f7841 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -295,8 +295,8 @@ py_library_providing_imports_info(\n     srcs = [\n         \"_src/__init__.py\",\n         \"_src/ad_checkpoint.py\",\n-        \"_src/api.py\",\n-        \"_src/array.py\",\n+        \"_src/api.py\",  # TODO(vanderplas): remove this and depend on :api instead\n+        \"_src/array.py\",  # TODO(vanderplas): remove this and depend on :api instead\n         \"_src/blocked_sampler.py\",\n         \"_src/buffer_callback.py\",\n         \"_src/callback.py\",\n@@ -305,14 +305,14 @@ py_library_providing_imports_info(\n         \"_src/custom_partitioning.py\",\n         \"_src/custom_partitioning_sharding_rule.py\",\n         \"_src/debugging.py\",\n-        \"_src/dispatch.py\",\n+        \"_src/dispatch.py\",  # TODO(vanderplas): remove this and depend on :api instead\n         \"_src/dlpack.py\",\n         \"_src/earray.py\",\n         \"_src/error_check.py\",\n         \"_src/ffi.py\",\n         \"_src/flatten_util.py\",\n-        \"_src/interpreters/__init__.py\",\n-        \"_src/interpreters/pxla.py\",\n+        \"_src/interpreters/__init__.py\",  # TODO(vanderplas): remove this and depend on :api instead\n+        \"_src/interpreters/pxla.py\",  # TODO(vanderplas): remove this and depend on :api instead\n         \"_src/pjit.py\",\n         \"_src/prng.py\",\n         \"_src/public_test_util.py\",\n@@ -375,6 +375,7 @@ py_library_providing_imports_info(\n         \":abstract_arrays\",\n         \":ad\",\n         \":ad_util\",\n+        # \":api\",   # TODO(vanderplas): add this dependency once downstream targets are fixed\n         \":api_util\",\n         \":attrs\",\n         \":basearray\",\n@@ -450,6 +451,53 @@ pytype_strict_library(\n     ],\n )\n \n+pytype_strict_library(\n+    name = \"api\",\n+    srcs = [\n+        \"_src/api.py\",\n+        \"_src/array.py\",\n+        \"_src/dispatch.py\",\n+        \"_src/interpreters/pxla.py\",\n+        \"_src/pjit.py\",\n+    ],\n+    visibility = [\":internal\"] + jax_visibility(\"api\"),\n+    deps = [\n+        \":abstract_arrays\",\n+        \":ad\",\n+        \":api_util\",\n+        \":attrs\",\n+        \":basearray\",\n+        \":batching\",\n+        \":compiler\",\n+        \":config\",\n+        \":core\",\n+        \":deprecations\",\n+        \":dtypes\",\n+        \":effects\",\n+        \":layout\",\n+        \":mesh\",\n+        \":mlir\",\n+        \":monitoring\",\n+        \":op_shardings\",\n+        \":partial_eval\",\n+        \":partition_spec\",\n+        \":profiler\",\n+        \":sharding\",\n+        \":sharding_impls\",\n+        \":sharding_specs\",\n+        \":source_info_util\",\n+        \":stages\",\n+        \":state_types\",\n+        \":traceback_util\",\n+        \":tree_util\",\n+        \":typing\",\n+        \":util\",\n+        \":xla\",\n+        \":xla_bridge\",\n+        \"//jax/_src/lib\",\n+    ] + py_deps(\"numpy\"),\n+)\n+\n pytype_strict_library(\n     name = \"api_util\",\n     srcs = [\"_src/api_util.py\"],\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex c21e8248d52d..229dee979d06 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -66,7 +66,6 @@\n   rebase_donate_argnums, _ensure_index, _ensure_index_tuple,\n   apply_flat_fun_nokwargs, check_callable, debug_info,\n   flat_out_axes)\n-from jax._src.lax import lax as lax_internal\n from jax._src.lib import jax_jit\n from jax._src.lib import xla_client as xc\n from jax._src.lib import pmap_lib\n@@ -477,6 +476,8 @@ def value_and_grad(fun: Callable, argnums: int | Sequence[int] = 0,\n     shapes and types as the corresponding arguments. If ``has_aux`` is True\n     then a tuple of ((value, auxiliary_data), gradient) is returned.\n   \"\"\"\n+  from jax._src.lax import lax as lax_internal  # pytype: disable=import-error\n+\n   if reduce_axes:\n     raise NotImplementedError(\"reduce_axes argument to grad is deprecated\")\n   del reduce_axes\n@@ -889,7 +890,7 @@ def hessian(fun: Callable, argnums: int | Sequence[int] = 0,\n                 argnums, has_aux=has_aux, holomorphic=holomorphic)\n \n def _std_basis(pytree):\n-  import jax.numpy as jnp\n+  import jax.numpy as jnp  # pytype: disable=import-error\n   leaves, _ = tree_flatten(pytree)\n   ndim = sum(map(np.size, leaves))\n   dtype = dtypes.result_type(*leaves)\n@@ -905,6 +906,7 @@ def _jacrev_unravel(output_pytree, input_pytree_leaf, arr):\n     output_pytree, 0, input_pytree_leaf, arr)\n \n def _possible_downcast(x, example):\n+  from jax._src.lax import lax as lax_internal  # pytype: disable=import-error\n   if (dtypes.issubdtype(x.dtype, np.complexfloating) and\n       not dtypes.issubdtype(_dtype(example), np.complexfloating)):\n     x = x.real\n@@ -1483,7 +1485,7 @@ def pmap(\n         \" from pmap.\")\n \n   if config.pmap_shmap_merge.value:\n-    from jax._src.shard_map import pmap\n+    from jax._src.shard_map import pmap  # pytype: disable=import-error\n     return pmap(fun, axis_name, in_axes=in_axes, out_axes=out_axes,\n                 static_broadcasted_argnums=static_broadcasted_argnums,\n                 devices=devices, backend=backend,\ndiff --git a/jax/_src/array.py b/jax/_src/array.py\nindex 9a71b12ed1a8..2514502c27d0 100644\n--- a/jax/_src/array.py\n+++ b/jax/_src/array.py\n@@ -343,8 +343,8 @@ def __format__(self, format_spec):\n       return format(self._value, format_spec)\n \n   def __getitem__(self, idx):\n-    from jax._src.lax import lax\n-    from jax._src.numpy import indexing\n+    from jax._src.lax import lax  # pytype: disable=import-error\n+    from jax._src.numpy import indexing  # pytype: disable=import-error\n     self._check_if_deleted()\n \n     if isinstance(self.sharding, PmapSharding):\n@@ -444,7 +444,7 @@ def __dlpack__(self, *, stream: int | Any | None = None,\n                  max_version: tuple[int, int] | None = None,\n                  dl_device: tuple[DLDeviceType, int] | None = None,\n                  copy: bool | None = None):\n-    from jax._src.dlpack import to_dlpack  # pylint: disable=g-import-not-at-top\n+    from jax._src.dlpack import to_dlpack  # pytype: disable=import-error  # pylint: disable=g-import-not-at-top\n \n     device_set = self.sharding.device_set\n     if len(device_set) > 1:\n@@ -464,7 +464,7 @@ def __dlpack_device__(self) -> tuple[enum.Enum, int]:\n     if len(self._arrays) != 1:\n       raise BufferError(\"__dlpack__ only supported for unsharded arrays.\")\n \n-    from jax._src.dlpack import DLDeviceType  # pylint: disable=g-import-not-at-top\n+    from jax._src.dlpack import DLDeviceType  # pytype: disable=import-error  # pylint: disable=g-import-not-at-top\n \n     if self.platform() == \"cpu\":\n       return DLDeviceType.kDLCPU, 0\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex 028c2cfa125e..b5e588cbc10e 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -26,7 +26,6 @@\n import time\n from typing import Any\n \n-import jax\n from jax._src import api\n from jax._src import array\n from jax._src import basearray\n@@ -34,8 +33,11 @@\n from jax._src import core\n from jax._src import dtypes\n from jax._src import lib\n+from jax._src import pjit\n from jax._src import traceback_util\n from jax._src import util\n+\n+from jax._src import xla_bridge\n from jax._src.abstract_arrays import array_types\n from jax._src.interpreters import ad\n from jax._src.interpreters import batching\n@@ -133,7 +135,7 @@ def get_token_input(\n       # TODO(yueshengys): This might still be buggy in a multi-process SPMD\n       # scenario. Revise the logic later. A distributed shutdown barrier inside\n       # the XLA program may be needed.\n-      return jax.device_put(\n+      return api.device_put(\n           tok, NamedSharding(Mesh(devices, 'x'), PartitionSpec('x')))\n \n     # We only use replicated sharding for the first time when the token for the\n@@ -244,8 +246,7 @@ def jaxpr_has_prim_requiring_devices(jaxpr: core.Jaxpr) -> bool:\n @util.weakref_lru_cache\n def get_intermediate_shardings(\n     jaxpr: core.Jaxpr) -> Sequence[tuple[Sharding, SourceInfo]]:\n-  from jax._src import pjit\n-  from jax._src import shard_map\n+  from jax._src import shard_map  # pytype: disable=import-error\n \n   out = []\n   for eqn in jaxpr.eqns:\n@@ -409,7 +410,7 @@ def result_handler(self, shard_arg_result):\n \n \n def _device_put_sharding_impl(x, aval, device, copy):\n-  from jax.experimental import multihost_utils\n+  from jax.experimental import multihost_utils  # pytype: disable=import-error\n \n   if isinstance(device, Sharding):\n     s = device\n@@ -440,7 +441,7 @@ def _device_put_sharding_impl(x, aval, device, copy):\n         # sharding do not transfer data) or (2) the sharding contains a\n         # different subset of devices on each host. For (1), the input should be\n         # the same on all hosts, but for (2) it need not be.\n-        if jax.process_count() == len(s._internal_device_list.process_indices):  # pytype: disable=attribute-error\n+        if xla_bridge.process_count() == len(s._internal_device_list.process_indices):  # pytype: disable=attribute-error\n           multihost_utils.assert_equal(\n               x, fail_message=(\n                   f\"{type(x)} passed to device_put is not the same on each\"\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex 74f2e028c555..0530e313f310 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -29,9 +29,8 @@\n \n import numpy as np\n \n-import jax\n-\n from jax._src import api\n+from jax._src import array\n from jax._src import compiler\n from jax._src import config\n from jax._src import core\n@@ -41,11 +40,13 @@\n from jax._src import linear_util as lu\n from jax._src import op_shardings\n from jax._src import sharding_specs\n+from jax._src import pjit\n from jax._src import profiler\n from jax._src import sharding_impls\n from jax._src import source_info_util\n from jax._src import stages\n from jax._src import tree_util\n+from jax._src import typing\n from jax._src import util\n from jax._src import xla_bridge as xb\n from jax._src.abstract_arrays import array_types\n@@ -154,7 +155,7 @@ def shard_args(shardings: Sequence[JSharding], layouts, copy_semantics,\n   # from each call in the same order as `args`. Since `batches` is grouped by\n   # types, we cannot simply flatten the results and we have to use the original\n   # indices to put each array back to its original position.\n-  results: list[jax.Array | None] = [None] * len(args)\n+  results: list[typing.Array | None] = [None] * len(args)\n   for t, (indices, a, s, l, cs) in batches.items():\n     outs = shard_arg_handlers[t](a, s, l, cs)\n     for i, out in safe_zip(indices, outs):\n@@ -230,11 +231,9 @@ def _shard_mutable_array(xs, shardings, layouts, copy_semantics):\n \n def batched_device_put(aval: core.ShapedArray,\n                        sharding: JSharding, xs: Sequence[Any],\n-                       devices: Sequence[jax.Device], committed: bool = True):\n+                       devices: Sequence[xc.Device], committed: bool = True):\n   util.test_event(\"batched_device_put_start\")\n   try:\n-    from jax._src import array\n-\n     bufs = [x for x, d in safe_zip(xs, devices)\n             if (isinstance(x, array.ArrayImpl) and\n                 dispatch.is_single_device_sharding(x.sharding) and\n@@ -385,7 +384,6 @@ def _emap_impl(fun: lu.WrappedFun, *args,\n                donated_invars: Sequence[bool],\n                is_explicit_global_axis_size: bool,\n                ):\n-  from jax._src import array\n   # TODO(sharadmv,mattjj): implement these cases\n   if any(d for d in donated_invars):\n     raise NotImplementedError(\"Buffer donation not supported in eager pmap.\")\n@@ -410,12 +408,12 @@ def _emap_impl(fun: lu.WrappedFun, *args,\n   donate_argnums = (1,) if platform in {\"cuda\", \"rocm\", \"tpu\"} else ()\n   new_outvals = []\n   for out_axis_src, out_axis, outval in zip(out_axes_src, out_axes, outvals):\n-    with jax.disable_jit(False):\n+    with api.disable_jit(False):\n       donate_argnums_ = donate_argnums\n       if isinstance(outval, array.ArrayImpl):\n         # We don't want to donate if it's already sharded.\n         donate_argnums_ = ()\n-      out = jax.pmap(\n+      out = api.pmap(\n           lambda _, x: x,\n           in_axes=(0, out_axis_src.get(axis_name)),\n           out_axes=out_axis,\n@@ -448,7 +446,7 @@ def _multi_pmap(f: Callable, info: EmapInfo, names: list[core.AxisName],\n   for i, name in reversed(list(enumerate(names))):\n     in_axes = tuple(arg_axis[i] for arg_axis in all_axes)\n     if any(in_axis is not None for in_axis in in_axes):\n-      f = jax.pmap(\n+      f = api.pmap(\n           f,\n           in_axes=in_axes,\n           axis_name=name,\n@@ -476,11 +474,12 @@ def to_map_tracer(self, val):\n       return MapTracer(self, val, {})\n \n   def process_primitive(self, primitive, tracers, params):\n-    if primitive is jax._src.lax.parallel.axis_index_p:\n-      return self.process_axis_index(**params)\n-    if primitive is jax._src.lax.parallel.psum_p:\n+    from jax._src.lax import parallel  # pytype: disable=import-error\n+    if primitive is parallel.axis_index_p:\n+      return self.process_axis_index(**params)  # pytype: disable=missing-parameter\n+    if primitive is parallel.psum_p:\n       f = HashableFunction(\n-          lambda *xs: jax._src.lax.parallel.psum(\n+          lambda *xs: parallel.psum(\n             xs, axis_name=params['axes'], axis_index_groups=params['axis_index_groups']),\n           (primitive, tuple(params.items())))\n     else:\n@@ -492,7 +491,7 @@ def process_primitive(self, primitive, tracers, params):\n     names = core.get_axis_env().axis_names()\n     all_axes = tuple(_map_schedule(map(s.get, names)) for s in shard_axes)  # pytype: disable=wrong-arg-types  # always-use-return-annotations\n     f_mapped, out_shard_axes = _multi_pmap(f, self.emap_info, names, all_axes)\n-    with core.eval_context(), jax.disable_jit(False):\n+    with core.eval_context(), api.disable_jit(False):\n       outvals = f_mapped(*vals)\n     if primitive.multiple_results:\n       return [MapTracer(self, val, out_shard_axes) for val in outvals]\n@@ -546,11 +545,12 @@ def process_custom_vjp_call(self, primitive, fun, fwd, bwd, tracers,\n       return fun.call_wrapped(*tracers)\n \n   def process_axis_index(self, axis_name):\n+    from jax._src.lax import lax, parallel  # pytype: disable=import-error\n     bind = HashableFunction(\n-        lambda _: jax.lax.axis_index(axis_name),\n-        (jax.lax.axis_index, axis_name))\n+        lambda _: parallel.axis_index(axis_name),\n+        (parallel.axis_index, axis_name))\n     fake_primitive = FakePrimitive(multiple_results=False, bind=bind)\n-    range = jax.lax.iota(np.int32, core.get_axis_env().axis_size(axis_name))\n+    range = lax.iota(np.int32, core.get_axis_env().axis_size(axis_name))\n     dummy_tracer = MapTracer(self, range, {axis_name: 0})\n     return self.process_primitive(fake_primitive, (dummy_tracer,), {})\n \n@@ -695,14 +695,15 @@ def find_replicas(\n \n @lu.transformation2\n def _change_argument_ranks(f, in_axes, out_axes_thunk, *args):\n+  from jax._src.lax import lax  # pytype: disable=import-error\n   args = tuple(\n-      arg if in_axis is None else jax.lax.squeeze(arg, dimensions=(in_axis,))\n+      arg if in_axis is None else lax.squeeze(arg, dimensions=(in_axis,))\n       for in_axis, arg in zip(in_axes, args)\n   )\n   results = f(*args)\n   out_axes = out_axes_thunk()\n   return tuple(\n-      x if axis is None else jax.lax.expand_dims(x, dimensions=(axis,))\n+      x if axis is None else lax.expand_dims(x, dimensions=(axis,))\n       for x, axis in zip(results, out_axes)\n   )\n \n@@ -1276,7 +1277,7 @@ def _handle_token_bufs(self, token_bufs, sharded_token):\n           assert isinstance(token.sharding, sharding_impls.SingleDeviceSharding)\n           token_devices.append(token.sharding._device_assignment[0])\n         s = NamedSharding(Mesh(token_devices, 'x'), P('x'))\n-        global_token_array = jax.make_array_from_single_device_arrays(\n+        global_token_array = array.make_array_from_single_device_arrays(\n             (0,), s, token_buf\n         )\n         dispatch.runtime_tokens.set_token_result(\n@@ -1754,7 +1755,7 @@ class MutationData(NamedTuple):\n def _discharge_refs(\n     jaxpr: core.ClosedJaxpr\n ) -> tuple[core.ClosedJaxpr, Sequence[int | None], MutationData]:\n-  from jax._src.state.discharge import discharge_state\n+  from jax._src.state.discharge import discharge_state  # pytype: disable=import-error\n   jaxpr, in_mut = _move_mutable_consts(jaxpr)\n   new_jaxpr = core.ClosedJaxpr(*discharge_state(jaxpr.jaxpr, jaxpr.consts))\n   count = it.count(len(jaxpr.out_avals))  # new outputs are appended to the end\n@@ -1782,7 +1783,7 @@ def _move_mutable_consts(\n \n @weakref_lru_cache\n def _discharge_internal_refs(jaxpr: core.ClosedJaxpr) -> core.ClosedJaxpr:\n-  from jax._src.state.discharge import discharge_state\n+  from jax._src.state.discharge import discharge_state  # pytype: disable=import-error\n   jaxpr_, consts = discharge_state(jaxpr.jaxpr, jaxpr.consts)\n   jaxpr_._debug_info = jaxpr.jaxpr._debug_info\n   return core.ClosedJaxpr(jaxpr_, consts)\n@@ -2016,8 +2017,6 @@ def _default_rule(prim, num_outvars, *_, **__):\n @weakref_lru_cache\n def get_out_layouts_via_propagation(closed_jaxpr: core.ClosedJaxpr\n                                     ) -> tuple[None | DeviceLocalLayout]:\n-  from jax._src import pjit\n-\n   env = {}  # type: ignore\n   jaxpr = closed_jaxpr.jaxpr\n \n@@ -3229,7 +3228,6 @@ def check_array_xla_sharding_layout_match(\n     in_xla_layouts: Sequence[DeviceLocalLayout],\n     jaxpr_debug_info: core.DebugInfo,\n     kept_var_idx: set[int]) -> None:\n-  from jax._src.array import ArrayImpl\n   # jaxpr_debug_info.arg_names are before DCE, so need to DCE them.\n   arg_names = (\n       [a for i, a in enumerate(jaxpr_debug_info.arg_names)\n@@ -3239,7 +3237,7 @@ def check_array_xla_sharding_layout_match(\n   num_errors = 5\n   for arg, xs, xl, name in safe_zip(\n       args_after_dce, in_xla_shardings, in_xla_layouts, arg_names):\n-    if not isinstance(arg, ArrayImpl):\n+    if not isinstance(arg, array.ArrayImpl):\n       continue\n     if isinstance(xs, (UnspecifiedValue, AUTO)):\n       continue\ndiff --git a/jax/_src/pallas/fuser/BUILD b/jax/_src/pallas/fuser/BUILD\nindex d5b5d128241d..951c08d8f4fa 100644\n--- a/jax/_src/pallas/fuser/BUILD\n+++ b/jax/_src/pallas/fuser/BUILD\n@@ -48,6 +48,7 @@ pytype_strict_library(\n         \":fuser_utils\",\n         \"//jax\",\n         \"//jax:ad_util\",\n+        \"//jax:api\",\n         \"//jax:api_util\",\n         \"//jax:core\",\n         \"//jax:custom_derivatives\",\ndiff --git a/jax/_src/pallas/mosaic_gpu/BUILD b/jax/_src/pallas/mosaic_gpu/BUILD\nindex 8a5d087125b7..78a1bd4f0011 100644\n--- a/jax/_src/pallas/mosaic_gpu/BUILD\n+++ b/jax/_src/pallas/mosaic_gpu/BUILD\n@@ -60,6 +60,7 @@ pytype_strict_library(\n     deps = [\n         \":core\",\n         \"//jax\",\n+        \"//jax:api\",\n         \"//jax:core\",\n         \"//jax:mesh\",\n         \"//jax:mlir\",\ndiff --git a/jax/_src/pallas/triton/BUILD b/jax/_src/pallas/triton/BUILD\nindex acbc11a60039..b13967d5b61c 100644\n--- a/jax/_src/pallas/triton/BUILD\n+++ b/jax/_src/pallas/triton/BUILD\n@@ -60,6 +60,7 @@ pytype_strict_library(\n     deps = [\n         \"//jax\",\n         \"//jax:ad_util\",\n+        \"//jax:api\",\n         \"//jax:api_util\",\n         \"//jax:config\",\n         \"//jax:core\",\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 4113f764e888..f2446e9a4939 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -70,7 +70,7 @@\n     prepare_axis_resources, parse_flatten_op_sharding, canonicalize_sharding,\n     flatten_spec, _internal_use_concrete_mesh)\n from jax._src.layout import Format, DeviceLocalLayout, AutoLayout\n-from jax._src.state import discharge as state_discharge, RefEffect, AbstractRef\n+from jax._src.state.types import RefEffect\n from jax._src.traceback_util import api_boundary\n from jax._src.tree_util import (\n     tree_flatten, tree_unflatten, treedef_is_leaf, tree_structure, tree_leaves,\n@@ -1413,7 +1413,7 @@ def _create_pjit_jaxpr(\n \n   if config.debug_key_reuse.value:\n     # Import here to avoid circular imports\n-    from jax.experimental.key_reuse._core import check_key_reuse_jaxpr\n+    from jax.experimental.key_reuse._core import check_key_reuse_jaxpr  # pytype: disable=import-error\n     check_key_reuse_jaxpr(jaxpr)\n \n   if any(isinstance(c, core.Tracer) or core.typeof(c).mutable for c in consts):\n@@ -2682,38 +2682,6 @@ def _pjit_pp_rule(eqn: core.JaxprEqn,\n core.pp_eqn_rules[pjit_p] = _pjit_pp_rule\n \n \n-def _pjit_state_discharge_rule(\n-    in_avals, out_avals, *args, jaxpr, in_shardings, out_shardings,\n-    in_layouts, out_layouts, **params):\n-  if not all(isinstance(s, UnspecifiedValue) for s in (*in_shardings, *out_shardings)):\n-    raise NotImplementedError\n-\n-  if not (all(l is None for l in in_layouts) and\n-          all(l is None for l in out_layouts)):\n-    raise NotImplementedError\n-\n-  jaxpr, consts = jaxpr.jaxpr, jaxpr.consts\n-  num_outs = len(jaxpr.outvars)\n-  discharged_jaxpr, discharged_consts = state_discharge.discharge_state(jaxpr, consts)\n-  discharged_closed_jaxpr = core.ClosedJaxpr(discharged_jaxpr, discharged_consts)\n-  new_in_shardings = (UnspecifiedValue(),) * len(discharged_jaxpr.invars)\n-  new_out_shardings = (UnspecifiedValue(),) * len(discharged_jaxpr.outvars)\n-  new_in_layouts = (None,) * len(discharged_jaxpr.invars)\n-  new_out_layouts = (None,) * len(discharged_jaxpr.outvars)\n-  out_and_ref_vals = pjit_p.bind(\n-      *args, jaxpr=discharged_closed_jaxpr, in_shardings=new_in_shardings,\n-      out_shardings=new_out_shardings, in_layouts=new_in_layouts,\n-      out_layouts=new_out_layouts, **params)\n-  out_vals, ref_vals = split_list(out_and_ref_vals, [num_outs])\n-  ref_vals_iter = iter(ref_vals)\n-  new_invals = tuple(next(ref_vals_iter) if isinstance(aval, AbstractRef)\n-                     else None for aval in in_avals)\n-  sentinel = object()\n-  assert next(ref_vals_iter, sentinel) is sentinel\n-  return new_invals, out_vals\n-state_discharge.register_discharge_rule(pjit_p)(_pjit_state_discharge_rule)\n-\n-\n # -------------------- with_sharding_constraint --------------------\n \n def check_shardings_are_auto(shardings_flat):\ndiff --git a/jax/_src/state/discharge.py b/jax/_src/state/discharge.py\nindex 100447f12d18..9dce3297b947 100644\n--- a/jax/_src/state/discharge.py\n+++ b/jax/_src/state/discharge.py\n@@ -25,6 +25,8 @@\n from jax._src import api_util\n from jax._src import core\n from jax._src import linear_util as lu\n+from jax._src import pjit\n+from jax._src import sharding_impls\n from jax._src import source_info_util\n from jax._src import tree_util\n from jax._src.interpreters import ad\n@@ -1145,3 +1147,35 @@ def wrapped(args):\n     _, out_flat = split_list(out_const_flat, [len(consts)])\n     return in_tree.unflatten(out_flat)\n   return wrapped\n+\n+\n+@register_discharge_rule(pjit.pjit_p)\n+def _pjit_state_discharge_rule(\n+    in_avals, out_avals, *args, jaxpr, in_shardings, out_shardings,\n+    in_layouts, out_layouts, **params):\n+  if not all(isinstance(s, sharding_impls.UnspecifiedValue) for s in (*in_shardings, *out_shardings)):\n+    raise NotImplementedError\n+\n+  if not (all(l is None for l in in_layouts) and\n+          all(l is None for l in out_layouts)):\n+    raise NotImplementedError\n+\n+  jaxpr, consts = jaxpr.jaxpr, jaxpr.consts\n+  num_outs = len(jaxpr.outvars)\n+  discharged_jaxpr, discharged_consts = discharge_state(jaxpr, consts)\n+  discharged_closed_jaxpr = core.ClosedJaxpr(discharged_jaxpr, discharged_consts)\n+  new_in_shardings = (sharding_impls.UNSPECIFIED,) * len(discharged_jaxpr.invars)\n+  new_out_shardings = (sharding_impls.UNSPECIFIED,) * len(discharged_jaxpr.outvars)\n+  new_in_layouts = (None,) * len(discharged_jaxpr.invars)\n+  new_out_layouts = (None,) * len(discharged_jaxpr.outvars)\n+  out_and_ref_vals = pjit.pjit_p.bind(\n+      *args, jaxpr=discharged_closed_jaxpr, in_shardings=new_in_shardings,\n+      out_shardings=new_out_shardings, in_layouts=new_in_layouts,\n+      out_layouts=new_out_layouts, **params)\n+  out_vals, ref_vals = split_list(out_and_ref_vals, [num_outs])\n+  ref_vals_iter = iter(ref_vals)\n+  new_invals = tuple(next(ref_vals_iter) if isinstance(aval, AbstractRef)\n+                     else None for aval in in_avals)\n+  sentinel = object()\n+  assert next(ref_vals_iter, sentinel) is sentinel\n+  return new_invals, out_vals\ndiff --git a/jax/extend/BUILD b/jax/extend/BUILD\nindex c2a5c48bd2b0..f466f1748654 100644\n--- a/jax/extend/BUILD\n+++ b/jax/extend/BUILD\n@@ -45,6 +45,7 @@ py_library_providing_imports_info(\n         \"//jax:abstract_arrays\",\n         \"//jax:ad\",\n         \"//jax:ad_util\",\n+        \"//jax:api\",\n         \"//jax:core\",\n         \"//jax:custom_derivatives\",\n     ],\n@@ -61,6 +62,7 @@ pytype_strict_library(\n     srcs = [\"backend.py\"],\n     deps = [\n         \"//jax\",\n+        \"//jax:api\",\n         \"//jax:xla_bridge\",\n     ],\n )\n\n```",
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29130 from mattjj:mutable-array-vmap2\n\nPiperOrigin-RevId: 765330882",
        "output": "```diff\nCommit: ebf0588ebfc086fccfa4e4fcbea8ec899b619524\nDate: 2025-05-30T21:08:57Z\nURL: https://github.com/jax-ml/jax/commit/ebf0588ebfc086fccfa4e4fcbea8ec899b619524\nFiles changed: 2\nAdditions: +38, Deletions: -0\ndiff --git a/jax/_src/state/primitives.py b/jax/_src/state/primitives.py\nindex dbcc67df18cb..3a54644dbd37 100644\n--- a/jax/_src/state/primitives.py\n+++ b/jax/_src/state/primitives.py\n@@ -618,6 +618,13 @@ def _swap_vmap(batched_args, batched_dims, *, tree):\n   val_is_batched = val_dim is not batching.not_mapped\n   idx_is_batched = any(i_dim is not batching.not_mapped\n                        for i_dim in flat_idx_dims)\n+\n+  if not ref_is_batched:\n+    raise Exception(\"performing a set/swap operation with vmapped value on \"\n+                    \"an unbatched mutable array reference \"\n+                    f\"of type {core.typeof(ref)}. Move the mutable array to be \"\n+                    \"an argument to the vmapped function?\")\n+\n   if len(indexers) > 1:\n     raise NotImplementedError(\"Batching with multiple indexers not supported.\")\n   # TODO(sharadmv): handle vmap of multiple indexers\ndiff --git a/tests/mutable_array_test.py b/tests/mutable_array_test.py\nindex 0da335e2fac5..95ebdd818fa6 100644\n--- a/tests/mutable_array_test.py\n+++ b/tests/mutable_array_test.py\n@@ -253,6 +253,27 @@ def f(x):\n     ys = f(xs)\n     self.assertAllClose(ys, xs ** 2, check_dtypes=False)\n \n+  def test_vmap_extensive_inputs(self):\n+    def f(x_ref, val):\n+      x_ref[...] += val\n+      x_ref[...] += val\n+\n+    xs_ref = core.mutable_array(jnp.array([0, 0, 0]))\n+    vals = jnp.arange(3)\n+    jax.vmap(f)(xs_ref, vals)\n+    self.assertAllClose(xs_ref[...], 2 * vals, check_dtypes=False)\n+\n+  def test_vmap_closed_over_read_only(self):\n+    y_ref = core.mutable_array(1)\n+\n+    def f(x_ref):\n+      x_ref[...] += y_ref[...]\n+      x_ref[...] += y_ref[...]\n+\n+    xs_ref = core.mutable_array(jnp.array([0, 0, 0]))\n+    jax.vmap(f)(xs_ref)\n+    self.assertAllClose(xs_ref[...], jnp.array([2, 2, 2]), check_dtypes=False)\n+\n   def test_implicit_bitcast_regression(self):\n     # https://github.com/jax-ml/jax/issues/27683\n     v = core.mutable_array(jnp.array([0, 0, 0]))\n@@ -417,6 +438,16 @@ def false_fun():\n     out_false = f(False)\n     self.assertAllClose(x_ref[...], 2.)\n \n+  def test_vmap_closed_over_ref_write(self):\n+    x_ref = core.mutable_array(jnp.zeros((), 'int32'))\n+\n+    def f(val):\n+      x_ref[...] += val\n+\n+    vals = jnp.arange(3, dtype='int32')\n+    with self.assertRaisesRegex(Exception, \"unbatched mutable array\"):\n+      jax.vmap(f)(vals)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[mutable-arrays] add basic tests for vmap + mutable array",
        "output": "```diff\nCommit: 581cb628a5f6516cfb63df201e18361dd0af6e96\nDate: 2025-05-30T20:23:34Z\nURL: https://github.com/jax-ml/jax/commit/581cb628a5f6516cfb63df201e18361dd0af6e96\nFiles changed: 2\nAdditions: +38, Deletions: -0\ndiff --git a/jax/_src/state/primitives.py b/jax/_src/state/primitives.py\nindex dbcc67df18cb..3a54644dbd37 100644\n--- a/jax/_src/state/primitives.py\n+++ b/jax/_src/state/primitives.py\n@@ -618,6 +618,13 @@ def _swap_vmap(batched_args, batched_dims, *, tree):\n   val_is_batched = val_dim is not batching.not_mapped\n   idx_is_batched = any(i_dim is not batching.not_mapped\n                        for i_dim in flat_idx_dims)\n+\n+  if not ref_is_batched:\n+    raise Exception(\"performing a set/swap operation with vmapped value on \"\n+                    \"an unbatched mutable array reference \"\n+                    f\"of type {core.typeof(ref)}. Move the mutable array to be \"\n+                    \"an argument to the vmapped function?\")\n+\n   if len(indexers) > 1:\n     raise NotImplementedError(\"Batching with multiple indexers not supported.\")\n   # TODO(sharadmv): handle vmap of multiple indexers\ndiff --git a/tests/mutable_array_test.py b/tests/mutable_array_test.py\nindex 0da335e2fac5..95ebdd818fa6 100644\n--- a/tests/mutable_array_test.py\n+++ b/tests/mutable_array_test.py\n@@ -253,6 +253,27 @@ def f(x):\n     ys = f(xs)\n     self.assertAllClose(ys, xs ** 2, check_dtypes=False)\n \n+  def test_vmap_extensive_inputs(self):\n+    def f(x_ref, val):\n+      x_ref[...] += val\n+      x_ref[...] += val\n+\n+    xs_ref = core.mutable_array(jnp.array([0, 0, 0]))\n+    vals = jnp.arange(3)\n+    jax.vmap(f)(xs_ref, vals)\n+    self.assertAllClose(xs_ref[...], 2 * vals, check_dtypes=False)\n+\n+  def test_vmap_closed_over_read_only(self):\n+    y_ref = core.mutable_array(1)\n+\n+    def f(x_ref):\n+      x_ref[...] += y_ref[...]\n+      x_ref[...] += y_ref[...]\n+\n+    xs_ref = core.mutable_array(jnp.array([0, 0, 0]))\n+    jax.vmap(f)(xs_ref)\n+    self.assertAllClose(xs_ref[...], jnp.array([2, 2, 2]), check_dtypes=False)\n+\n   def test_implicit_bitcast_regression(self):\n     # https://github.com/jax-ml/jax/issues/27683\n     v = core.mutable_array(jnp.array([0, 0, 0]))\n@@ -417,6 +438,16 @@ def false_fun():\n     out_false = f(False)\n     self.assertAllClose(x_ref[...], 2.)\n \n+  def test_vmap_closed_over_ref_write(self):\n+    x_ref = core.mutable_array(jnp.zeros((), 'int32'))\n+\n+    def f(val):\n+      x_ref[...] += val\n+\n+    vals = jnp.arange(3, dtype='int32')\n+    with self.assertRaisesRegex(Exception, \"unbatched mutable array\"):\n+      jax.vmap(f)(vals)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "[mutable-arrays] don't let scan AD hoist mutable operations\n\nWe do that by marking all mutable consts as unkonwn when we do the\nhoisting-via-partial-eval. That is a bit conservative, in that pure math on\nreads would be safe to hoist; what we really don't want to hoist is writing.\n\n**The remat path `_scan_partial_eval_custom` is not tested** because we\ncurrently error out when there are mutable arrays under remat. I did one simple\nmanual test, but there's no checked-in test for it.",
        "output": "```diff\nCommit: 69bcb0d88e24cb736c4fc1224b57075a824447aa\nDate: 2025-05-30T19:36:50Z\nURL: https://github.com/jax-ml/jax/commit/69bcb0d88e24cb736c4fc1224b57075a824447aa\nFiles changed: 2\nAdditions: +43, Deletions: -5\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 4df4c517090b..47808ee3c423 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -854,6 +854,8 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n   # want to broadcast the matrix!). So, outside the loop we perform a partial\n   # evaluation with known 'const' inputs (but all other inputs unknown).\n   const_pvals = [pe.PartialVal.known(t.pval.get_known())\n+                 if not isinstance(t.aval, state.AbstractRef)\n+                 else pe.PartialVal.unknown(t.aval)\n                  for t in tracers[:num_consts] if t.pval.is_known()]\n   other_pvals = [pe.PartialVal.unknown(aval)\n                  for aval in jaxpr_known.in_avals[len(const_pvals):]]\n@@ -898,7 +900,9 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n   # We use `fwds_known` below when forming the output of scanning jaxpr_known.\n \n   # Run the known part of the scan (if it has any outputs or effects).\n-  known_inputs = (list(jaxpr_known_consts) +\n+  known_mutable_consts = [t.pval.get_known() for t in tracers[:num_consts]\n+                          if t.pval.is_known() and isinstance(t.aval, state.AbstractRef)]\n+  known_inputs = (list(jaxpr_known_consts) + known_mutable_consts +\n                   [t.pval.get_known() for t in tracers[num_consts:]\n                    if t.pval.is_known()])\n   if not jaxpr_known.out_avals and not jaxpr_known.effects:\n@@ -907,7 +911,8 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n     linear_known = [False] * len(known_inputs)  # conservative!\n     out_known = scan_p.bind(\n         *known_inputs, reverse=reverse, length=length, jaxpr=jaxpr_known,\n-        num_consts=len(jaxpr_known_consts), num_carry=num_carry - sum(carry_uk),\n+        num_consts=len(jaxpr_known_consts) + len(known_mutable_consts),\n+        num_carry=num_carry - sum(carry_uk),\n         linear=tuple(linear_known), unroll=unroll,\n         _split_transpose=_split_transpose)\n     del linear_known\n@@ -1292,10 +1297,12 @@ def _scan_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n   num_const_known = len(const_uk) - sum(const_uk)\n   num_carry_known = len(carry_uk) - sum(carry_uk)\n   num_xs_known    = len(   xs_uk) - sum(   xs_uk)\n+  const_donthoist = [isinstance(a, state.AbstractRef)\n+                     for a in jaxpr_known.in_avals[:num_const_known]]\n   jaxpr_known_hoist, jaxpr_known_loop, loop_dep, consts_known_lp_avals = \\\n       pe.partial_eval_jaxpr_nounits(\n           jaxpr_known,\n-          [False] * num_const_known + [True] * (num_carry_known + num_xs_known),\n+          const_donthoist + [True] * (num_carry_known + num_xs_known),\n           [True] * (len(unks_out) - sum(unks_out)) + [False] * num_res)\n   # jaxpr_known_hoist produces intensive residuals followed by the constants for\n   # jaxpr_known_loop. We adjust jaxpr_staged to accept intensive res as consts.\n@@ -1328,10 +1335,13 @@ def _scan_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n                       linear=tuple(linear_known))\n \n   def known(*ins_known):\n-    consts_known_hoist, ins_known_lp = split_list(ins_known, [num_const_known])\n+    consts_known_maybehoist, ins_known_lp = split_list(ins_known, [num_const_known])\n+    consts_known_hoist, consts_known_donthoist = \\\n+        partition_list(const_donthoist, consts_known_maybehoist)\n     out_hoist = core.jaxpr_as_fun(jaxpr_known_hoist)(*consts_known_hoist)\n     intensive_res, consts_known_lp = split_list(out_hoist, [num_intensive_res])\n-    out_loop = scan_p.bind(*consts_known_lp, *ins_known_lp, **params_known)\n+    out_loop = scan_p.bind(*consts_known_lp, *consts_known_donthoist,\n+                           *ins_known_lp, **params_known)\n     return [*intensive_res, *out_loop]\n   call_jaxpr_, _, call_jaxpr_consts, () = pe.trace_to_jaxpr_dynamic(\n       lu.wrap_init(known, debug_info=jaxpr_known_hoist.jaxpr.debug_info),\ndiff --git a/tests/mutable_array_test.py b/tests/mutable_array_test.py\nindex 0da335e2fac5..8d80499c0e26 100644\n--- a/tests/mutable_array_test.py\n+++ b/tests/mutable_array_test.py\n@@ -268,6 +268,34 @@ def test_rng_key(self):\n     # test read/write\n     key[...] = jax.random.fold_in(key[...], 1) # don't crash\n \n+  def test_scan_grad_doesnt_hoist_mutable_stuff(self):\n+    x_ref = core.mutable_array(0)\n+\n+    def f(x):\n+      def body(c, _):\n+        x_ref[...] += 1\n+        return c, ()\n+      x, () = jax.lax.scan(body, x, (), length=3)\n+      return x\n+\n+    jax.grad(f)(1.0)\n+    self.assertAllClose(x_ref[...], 3, check_dtypes=False)\n+\n+  def test_scan_grad_doesnt_hoist_mutable_stuff2(self):\n+    x_ref = core.mutable_array(0)\n+    const = jnp.arange(3)\n+    const2 = jnp.zeros(())\n+\n+    def f(x):\n+      def body(c, _):\n+        x_ref[...] += const.sum()\n+        return c + const2, ()\n+      x, () = jax.lax.scan(body, x, (), length=4)\n+      return x\n+\n+    jax.grad(f)(1.0)\n+    self.assertAllClose(x_ref[...], 12, check_dtypes=False)\n+\n \n @jtu.with_config(jax_mutable_array_checks=True)\n class MutableArrayErrorsTest(jtu.JaxTestCase):\n\n```",
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "[pallas] Expose TPUInterpretParams in jax.experimental.pallas.tpu\n\nPiperOrigin-RevId: 765266754",
        "output": "```diff\nCommit: c2e7d61323b17481d213190bb779a4b74e7d5356\nDate: 2025-05-30T18:23:46Z\nURL: https://github.com/jax-ml/jax/commit/c2e7d61323b17481d213190bb779a4b74e7d5356\nFiles changed: 3\nAdditions: +23, Deletions: -22\ndiff --git a/jax/experimental/pallas/tpu.py b/jax/experimental/pallas/tpu.py\nindex e27fdaaadd8f..c4d21023a6e6 100644\n--- a/jax/experimental/pallas/tpu.py\n+++ b/jax/experimental/pallas/tpu.py\n@@ -28,6 +28,7 @@\n from jax._src.pallas.mosaic.helpers import sync_copy as sync_copy\n from jax._src.pallas.mosaic.helpers import core_barrier as core_barrier\n from jax._src.pallas.mosaic.helpers import run_on_first_core as run_on_first_core\n+from jax._src.pallas.mosaic.interpret import TPUInterpretParams as TPUInterpretParams\n from jax._src.pallas.mosaic.lowering import LoweringException as LoweringException\n from jax._src.pallas.mosaic.pipeline import BufferedRef as BufferedRef\n from jax._src.pallas.mosaic.pipeline import BufferedRefBase as BufferedRefBase\ndiff --git a/tests/pallas/tpu_pallas_interpret_distributed_test.py b/tests/pallas/tpu_pallas_interpret_distributed_test.py\nindex 4e4776736cf1..c5f1b29fd6bc 100644\n--- a/tests/pallas/tpu_pallas_interpret_distributed_test.py\n+++ b/tests/pallas/tpu_pallas_interpret_distributed_test.py\n@@ -107,7 +107,7 @@ def right_permute_kernel(input_ref, output_ref, send_sem, recv_sem):\n         out_shape=out_shape,\n         grid_spec=grid_spec,\n         compiler_params=pltpu.TPUCompilerParams(collective_id=13),\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n     )\n     # Wrap the kernel within a shard_map to call.\n@@ -228,7 +228,7 @@ def _():\n       all_gather_kernel,\n       out_shape=out_shape,\n       grid_spec=grid_spec,\n-      interpret=mosaic_interpret.TPUInterpretParams(\n+      interpret=pltpu.TPUInterpretParams(\n           dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n       compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n     )\n@@ -388,7 +388,7 @@ def _():\n       all_reduce_kernel,\n       out_shape=out_shape,\n       grid_spec=grid_spec,\n-      interpret=mosaic_interpret.TPUInterpretParams(\n+      interpret=pltpu.TPUInterpretParams(\n           dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n       compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n     )\n@@ -672,7 +672,7 @@ def pallas_reduce_scatter(input_arr):\n         reduce_scatter_kernel,\n         out_shape=out_shape,\n         grid_spec=grid_spec,\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             dma_execution_mode=dma_execution_mode, detect_races=True),\n         compiler_params=pltpu.TPUCompilerParams(collective_id=7),\n       )(input_arr)[0]\n@@ -976,7 +976,7 @@ def pallas_reduce_scatter(input_arr):\n         reduce_scatter_kernel,\n         out_shape=out_shape,\n         grid_spec=grid_spec,\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n         compiler_params=pltpu.TPUCompilerParams(collective_id=19),\n       )(input_arr)[0]\n@@ -1064,7 +1064,7 @@ def run(src_dst_ids):\n               ],\n               out_specs=pl.BlockSpec(memory_space=pltpu.ANY),\n               scratch_shapes=[pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA],\n-              interpret=mosaic_interpret.TPUInterpretParams(\n+              interpret=pltpu.TPUInterpretParams(\n                   dma_execution_mode='eager',\n                   detect_races=True,\n               ),\ndiff --git a/tests/pallas/tpu_pallas_interpret_test.py b/tests/pallas/tpu_pallas_interpret_test.py\nindex 47d4ba3e1acf..871f66d71c53 100644\n--- a/tests/pallas/tpu_pallas_interpret_test.py\n+++ b/tests/pallas/tpu_pallas_interpret_test.py\n@@ -124,7 +124,7 @@ def matmul(x: jax.Array, y: jax.Array):\n               (x.shape[0] // 2, y.shape[1] // 2),\n               lambda i, j: (i, j),\n           ),\n-          interpret=mosaic_interpret.TPUInterpretParams(),\n+          interpret=pltpu.TPUInterpretParams(),\n       )(x, y)\n \n     k1, k2 = jax.random.split(jax.random.key(0))\n@@ -155,7 +155,7 @@ def block_dynamic_slice(x, starts, sizes):\n           dynamic_slice_kernel,\n           grid_spec=grid_spec,\n           out_shape=jax.ShapeDtypeStruct(shape=sizes, dtype=x.dtype),\n-          interpret=mosaic_interpret.TPUInterpretParams(),\n+          interpret=pltpu.TPUInterpretParams(),\n       )\n       block_idx = jnp.array([starts[0] // sizes[0], starts[1] // sizes[1]])\n       return kernel(block_idx, x)\n@@ -189,7 +189,7 @@ def f(s, x):\n           ],\n           out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)),\n           input_output_aliases={1: 0},\n-          interpret=mosaic_interpret.TPUInterpretParams(),\n+          interpret=pltpu.TPUInterpretParams(),\n       )(s, x)\n \n     s = jnp.array([1], dtype=jnp.int32)\n@@ -224,7 +224,7 @@ def _():\n         ),\n         scratch_shapes=(pltpu.SMEM((1,), jnp.int32),),\n         input_output_aliases={0: 0},\n-        interpret=mosaic_interpret.TPUInterpretParams(),\n+        interpret=pltpu.TPUInterpretParams(),\n     )(x)\n \n     expected = np.zeros((4, 4))\n@@ -264,7 +264,7 @@ def kernel_with_race(x_ref, o_ref, t_ref, sem):\n             pltpu.VMEM(x.shape, x.dtype),\n             pltpu.SemaphoreType.DMA,\n         ],\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             detect_races=True, dma_execution_mode=dma_execution_mode\n         ),\n     )(x).block_until_ready()\n@@ -279,7 +279,7 @@ def kernel_with_race(x_ref, o_ref, t_ref, sem):\n             pltpu.VMEM(x.shape, x.dtype),\n             pltpu.SemaphoreType.DMA,\n         ],\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             detect_races=True, dma_execution_mode=dma_execution_mode\n         ),\n     )(x).block_until_ready()\n@@ -293,7 +293,7 @@ def matmul(x: jax.Array, y: jax.Array):\n       return pl.pallas_call(\n           matmul_kernel,\n           out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype),\n-          interpret=mosaic_interpret.TPUInterpretParams(\n+          interpret=pltpu.TPUInterpretParams(\n               skip_floating_point_ops=True\n           ),\n       )(x, y)\n@@ -325,7 +325,7 @@ def kernel(o1_ref, o2_ref, o3_ref, t1_ref, t2_ref):\n             pltpu.VMEM((8, 128), jnp.bfloat16),\n             pltpu.VMEM((8, 128), jnp.int16),\n         ],\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             uninitialized_memory=uninitialized_memory\n         ),\n     )()\n@@ -355,7 +355,7 @@ def kernel_call(x, s):\n               pl.BlockSpec(memory_space=pltpu.SMEM),\n           ],\n           out_specs=pl.BlockSpec((8, 256), lambda i, j: (i, 0)),\n-          interpret=mosaic_interpret.TPUInterpretParams(),\n+          interpret=pltpu.TPUInterpretParams(),\n       )(x, s)\n \n     with CountStoreCallbacksContext() as store_callbacks_counter:\n@@ -378,7 +378,7 @@ def kernel_call_dimensions_parallel_arbitrary(s, grid_point_recorder):\n           grid=(4, 4),\n           in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n           out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n-          interpret=mosaic_interpret.TPUInterpretParams(\n+          interpret=pltpu.TPUInterpretParams(\n               random_seed=12345, grid_point_recorder=grid_point_recorder\n           ),\n           compiler_params=pltpu.TPUCompilerParams(\n@@ -436,7 +436,7 @@ def kernel(s_ref, o_ref):\n           grid=(4, 4),\n           in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n           out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n-          interpret=mosaic_interpret.TPUInterpretParams(random_seed=12345),\n+          interpret=pltpu.TPUInterpretParams(random_seed=12345),\n           compiler_params=pltpu.TPUCompilerParams(\n               dimension_semantics=('arbitrary', 'parallel')\n           ),\n@@ -462,7 +462,7 @@ def kernel_call_dynamic_parallel_dimension():\n           grid=(dim_size,),\n           in_specs=[],\n           out_specs=pl.BlockSpec((1,), lambda _: (0,)),\n-          interpret=mosaic_interpret.TPUInterpretParams(),\n+          interpret=pltpu.TPUInterpretParams(),\n           compiler_params=pltpu.TPUCompilerParams(\n               dimension_semantics=('parallel',)\n           ),\n@@ -479,7 +479,7 @@ def f(x):\n       y = jnp.zeros_like(x)\n       def inner(refs):\n         x_ref, y_ref = refs\n-        @pl.core_map(mesh, interpret=mosaic_interpret.TPUInterpretParams())\n+        @pl.core_map(mesh, interpret=pltpu.TPUInterpretParams())\n         def _():\n           num_cores = jax.lax.psum(1, \"x\")\n           slc_size = 16 // num_cores\n@@ -520,7 +520,7 @@ def kernel(x_ref, o_ref, vmem_ref):\n         scratch_shapes=[\n             pltpu.VMEM(x.shape, x.dtype),\n         ],\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             num_cores_per_device=2,\n             detect_races=True,\n         ),\n@@ -554,7 +554,7 @@ def kernel(x_ref, o_ref, vmem_ref):\n         scratch_shapes=[\n             pltpu.VMEM((8, 128), x.dtype),\n         ],\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             num_cores_per_device=2,\n             detect_races=True,\n         ),\n@@ -578,7 +578,7 @@ def kernel_call(s, num_cores_per_device, grid_point_recorder):\n           grid=(4, 4),\n           in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n           out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n-          interpret=mosaic_interpret.TPUInterpretParams(\n+          interpret=pltpu.TPUInterpretParams(\n               random_seed=12345,\n               num_cores_per_device=num_cores_per_device,\n               grid_point_recorder=grid_point_recorder,\n\n```",
        "from_id": [
            "jburnim",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Support interleaved packing on TPUv4-.\n\nThis enables row broadcast for int8 and int4 on TPUv4.\n\nPiperOrigin-RevId: 765252479",
        "output": "```diff\nCommit: d15253e7f5e71b18dad93c2a0e3c10234be37550\nDate: 2025-05-30T17:48:24Z\nURL: https://github.com/jax-ml/jax/commit/d15253e7f5e71b18dad93c2a0e3c10234be37550\nFiles changed: 4\nAdditions: +18, Deletions: -19\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 53d8712d5274..1669d1bf1586 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -3655,8 +3655,7 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n               if (packing != 1) {\n                 if (auto new_dst_vreg = broadcastSubelements(\n                         builder, cast<TypedValue<VectorType>>(dst_vreg),\n-                        subelement_offset, ctx.target_shape,\n-                        ctx.hardware_generation);\n+                        subelement_offset, ctx.target_shape);\n                     succeeded(new_dst_vreg)) {\n                   dst_vreg = *new_dst_vreg;\n                 } else {\ndiff --git a/jaxlib/mosaic/dialect/tpu/vreg_util.cc b/jaxlib/mosaic/dialect/tpu/vreg_util.cc\nindex 237bbe5cc722..90efacf0c676 100644\n--- a/jaxlib/mosaic/dialect/tpu/vreg_util.cc\n+++ b/jaxlib/mosaic/dialect/tpu/vreg_util.cc\n@@ -224,8 +224,7 @@ LogicalResult maskNativeTilingVregs(ImplicitLocOpBuilder &builder,\n \n FailureOr<TypedValue<VectorType>> broadcastSubelements(\n     ImplicitLocOpBuilder &builder, TypedValue<VectorType> vec,\n-    int subelement_idx, std::array<int64_t, 2> target_shape,\n-    int hardware_generation) {\n+    int subelement_idx, std::array<int64_t, 2> target_shape) {\n   int bitwidth = vec.getType().getElementTypeBitWidth();\n   int packing = 32 / bitwidth;\n   if (subelement_idx < 0 || subelement_idx >= packing) {\n@@ -247,17 +246,9 @@ FailureOr<TypedValue<VectorType>> broadcastSubelements(\n       src_vreg_int,\n       getFullVector(builder, vreg_native_int_ty,\n                     builder.getI32IntegerAttr(subelement_idx * bitwidth)));\n-  Value vreg_result_int;\n-  if (hardware_generation >= 5) {\n-    SmallVector<Value> packed_vregs(packing, vreg_subelement_low);\n-    vreg_result_int = builder.create<tpu::PackSubelementsOp>(\n-        vreg_packed_int_ty, packed_vregs, tpu::PackFormat::kInterleaved);\n-  } else {\n-    // This can be virtualized as a tree of shifts and ORs.\n-    return builder.emitError()\n-           << \"broadcastSubelements not implemented for hardware generation \"\n-           << hardware_generation;\n-  }\n+  SmallVector<Value> packed_vregs(packing, vreg_subelement_low);\n+  Value vreg_result_int = builder.create<tpu::PackSubelementsOp>(\n+      vreg_packed_int_ty, packed_vregs, tpu::PackFormat::kInterleaved);\n   return cast<TypedValue<VectorType>>(\n       builder.create<tpu::BitcastVregOp>(vec.getType(), vreg_result_int)\n           .getResult());\ndiff --git a/jaxlib/mosaic/dialect/tpu/vreg_util.h b/jaxlib/mosaic/dialect/tpu/vreg_util.h\nindex 8833390ef87b..90e802fcb8fc 100644\n--- a/jaxlib/mosaic/dialect/tpu/vreg_util.h\n+++ b/jaxlib/mosaic/dialect/tpu/vreg_util.h\n@@ -90,8 +90,7 @@ LogicalResult maskNativeTilingVregs(ImplicitLocOpBuilder &builder,\n // subelement_idx must be between 0 and packing.\n FailureOr<TypedValue<VectorType>> broadcastSubelements(\n     ImplicitLocOpBuilder &builder, TypedValue<VectorType> vec,\n-    int subelement_idx, std::array<int64_t, 2> target_shape,\n-    int hardware_generation);\n+    int subelement_idx, std::array<int64_t, 2> target_shape);\n \n }  // namespace mlir::tpu\n \ndiff --git a/tests/pallas/tpu_ops_test.py b/tests/pallas/tpu_ops_test.py\nindex de87126ebd3f..3f6dc593e333 100644\n--- a/tests/pallas/tpu_ops_test.py\n+++ b/tests/pallas/tpu_ops_test.py\n@@ -197,8 +197,18 @@ def kernel(x_ref, y_ref, out_ref):\n   def test_row_broadcast(self, dtype):\n     if not jtu.if_cloud_tpu_at_least(2025, 1, 10):\n       self.skipTest(\"Requires libtpu built after 2025-01-10\")\n-    if not self.INTERPRET and jtu.get_tpu_version() < 5:\n-      self.skipTest(\"Requires TPUv5+\")\n+    bitwidth = pallas_utils.dtype_bitwidth(dtype)\n+    if not self.INTERPRET and jtu.get_tpu_version() < 4 and bitwidth < 8:\n+      self.skipTest(\"Requires TPUv4+ for sub-byte types\")\n+    if (\n+        not self.INTERPRET\n+        and jtu.get_tpu_version() == 4\n+        and bitwidth < 16\n+        and not jtu.if_cloud_tpu_at_least(2025, 6, 2)\n+    ):\n+      self.skipTest(\n+          \"Requires libtpu built after 2025-06-02 for bitwidth < 16 on TPUv4\"\n+      )\n     def kernel(x_ref, y_ref):\n       y_ref[...] = jnp.broadcast_to(x_ref[pl.ds(3, 1)], y_ref.shape).astype(y_ref.dtype)\n     m, n = 4, 1152\n\n```",
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "replace mentions of `Compiled.input_layouts` with `Compiled.input_formats`\n\nThis is part of a broader renaming of \"layout\" to \"format\".\n\nPiperOrigin-RevId: 765205967",
        "output": "```diff\nCommit: 213985aa8d20d0b01113e1f5a337a3649ece0a7c\nDate: 2025-05-30T15:42:37Z\nURL: https://github.com/jax-ml/jax/commit/213985aa8d20d0b01113e1f5a337a3649ece0a7c\nFiles changed: 3\nAdditions: +64, Deletions: -64\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex 2072aaf44b5a..74f2e028c555 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -2976,7 +2976,7 @@ def from_hlo(name: str,\n             xla_executable.local_devices(), len(in_shardings), len(out_shardings))\n \n     # xla_in_layouts are all either None or DeviceLocalLayout. Even default\n-    # layout are concrete layouts and they are used in `compiled.input_layouts`\n+    # layout are concrete layouts and they are used in `compiled.input_formats`\n     # to return concrete layouts to users.\n     # `dispatch_in_layouts` replaces default layouts with `None` to simplify\n     # dispatch logic downstream.\ndiff --git a/jax/experimental/array_serialization/serialization_test.py b/jax/experimental/array_serialization/serialization_test.py\nindex 0611388a1d80..0d7e0a48b6c1 100644\n--- a/jax/experimental/array_serialization/serialization_test.py\n+++ b/jax/experimental/array_serialization/serialization_test.py\n@@ -593,10 +593,10 @@ def test_load_with_layout(self):\n     s = NamedSharding(mesh, P('x', 'y'))\n     arr = jax.device_put(np_inp, s)\n \n-    out_layout = jax.jit(lambda x: x.T, out_shardings=Format(DLL.AUTO)).lower(\n-        arr).compile().output_layouts\n+    out_format = jax.jit(lambda x: x.T, out_shardings=Format(DLL.AUTO)).lower(\n+        arr).compile().output_formats\n     self.assertEqual(arr.format.device_local_layout.major_to_minor,\n-                     out_layout.device_local_layout.major_to_minor[::-1])\n+                     out_format.device_local_layout.major_to_minor[::-1])\n \n     ckpt_dir = pathlib.Path(self.create_tempdir('ckpt').full_path)\n     ckpt_path = pathlib.Path(self.create_tempdir(f'{ckpt_dir}/first').full_path)\n@@ -609,9 +609,9 @@ def test_load_with_layout(self):\n             self._on_commit_callback, ckpt_dir, ckpt_dir))\n     manager.wait_until_finished()\n \n-    out, = serialization.run_deserialization([out_layout], tspecs)\n+    out, = serialization.run_deserialization([out_format], tspecs)\n \n-    self.assertEqual(out.format, out_layout)\n+    self.assertEqual(out.format, out_format)\n     self.assertIsInstance(out, array.ArrayImpl)\n     self.assertArraysEqual(out, np_inp)\n     for s in out.addressable_shards:\ndiff --git a/tests/layout_test.py b/tests/layout_test.py\nindex d7b23c75b313..ce0ca17b05de 100644\n--- a/tests/layout_test.py\n+++ b/tests/layout_test.py\n@@ -55,18 +55,18 @@ def init(x, y):\n                             out_shardings=Format(DLL.AUTO)).lower(sds1, sds2)\n     compiled_apply = lowered_apply.compile()\n \n-    arg_layouts, kw_layouts = compiled_apply.input_layouts\n+    arg_formats, kw_layouts = compiled_apply.input_formats\n     self.assertEmpty(kw_layouts)\n \n-    for i, o in zip(arg_layouts, compiled_apply.output_layouts):\n+    for i, o in zip(arg_formats, compiled_apply.output_formats):\n       self.assertEqual(i.device_local_layout.major_to_minor,\n                        o.device_local_layout.major_to_minor[::-1])\n \n     init_compiled = jax.jit(\n-        init, out_shardings=arg_layouts).lower(sds1, sds2).compile()\n+        init, out_shardings=arg_formats).lower(sds1, sds2).compile()\n \n-    for i, o in zip(init_compiled.input_layouts[0],\n-                    init_compiled.output_layouts):\n+    for i, o in zip(init_compiled.input_formats[0],\n+                    init_compiled.output_formats):\n       self.assertEqual(i, o)\n \n     arr1 = jax.device_put(np_inp1, s1)\n@@ -77,16 +77,16 @@ def init(x, y):\n       init_compiled(arr1, arr2)\n     self.assertEqual(init_count(), 1)\n \n-    self.assertEqual(init_out[0].format, init_compiled.output_layouts[0])\n-    self.assertEqual(init_out[1].format, init_compiled.output_layouts[1])\n+    self.assertEqual(init_out[0].format, init_compiled.output_formats[0])\n+    self.assertEqual(init_out[1].format, init_compiled.output_formats[1])\n \n     with jtu.count_aot_jit_cpp_cache_miss() as apply_count:\n       apply_out = compiled_apply(*init_out)\n       compiled_apply(*init_out)\n     self.assertEqual(apply_count(), 1)\n \n-    self.assertEqual(apply_out[0].format, compiled_apply.output_layouts[0])\n-    self.assertEqual(apply_out[1].format, compiled_apply.output_layouts[1])\n+    self.assertEqual(apply_out[0].format, compiled_apply.output_formats[0])\n+    self.assertEqual(apply_out[1].format, compiled_apply.output_formats[1])\n \n     self.assertTupleEqual(apply_out[0].format.device_local_layout.major_to_minor,\n                           init_out[0].format.device_local_layout.major_to_minor[::-1])\n@@ -114,10 +114,10 @@ def f(x):\n     out = compiled(arr)\n \n     self.assertTupleEqual(\n-        compiled.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n+        compiled.input_formats[0][0].device_local_layout.major_to_minor[::-1],\n         (2, 1, 0))\n     self.assertTupleEqual(\n-        compiled.output_layouts.device_local_layout.major_to_minor[::-1],\n+        compiled.output_formats.device_local_layout.major_to_minor[::-1],\n         (2, 1, 0))\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n@@ -125,10 +125,10 @@ def f(x):\n     compiled_auto = jax.jit(f, in_shardings=Format(DLL.AUTO),\n                             out_shardings=Format(DLL.AUTO)).lower(sds).compile()\n     self.assertTupleEqual(\n-        compiled_auto.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n+        compiled_auto.input_formats[0][0].device_local_layout.major_to_minor[::-1],\n         (2, 1, 0))\n     self.assertTupleEqual(\n-        compiled_auto.output_layouts.device_local_layout.major_to_minor[::-1],\n+        compiled_auto.output_formats.device_local_layout.major_to_minor[::-1],\n         (0, 1, 2))\n \n     with self.assertRaisesRegex(\n@@ -149,15 +149,15 @@ def f(x):\n     compiled = jax.jit(f, in_shardings=Format(),\n                        out_shardings=Format(DLL.AUTO)).lower(arr).compile()\n     self.assertTupleEqual(\n-        compiled.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n+        compiled.input_formats[0][0].device_local_layout.major_to_minor[::-1],\n         (1, 0))\n     self.assertTupleEqual(\n-        compiled.output_layouts.device_local_layout.major_to_minor[::-1],\n+        compiled.output_formats.device_local_layout.major_to_minor[::-1],\n         (0, 1))\n \n     out = compiled(arr)\n     self.assertArraysEqual(out, np_inp.T)\n-    self.assertEqual(out.format, compiled.output_layouts)\n+    self.assertEqual(out.format, compiled.output_formats)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('y', 'x')))\n \n   def test_sharding_and_layouts(self):\n@@ -170,11 +170,11 @@ def test_sharding_and_layouts(self):\n                        out_shardings=Format(DLL.AUTO, s)).lower(np_inp).compile()\n     out = compiled(np_inp)\n     self.assertTupleEqual(\n-        compiled.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n+        compiled.input_formats[0][0].device_local_layout.major_to_minor[::-1],\n         (1, 0))\n     if not jtu.test_device_matches(['cpu']):\n       self.assertTupleEqual(\n-          compiled.output_layouts.device_local_layout.major_to_minor[::-1],\n+          compiled.output_formats.device_local_layout.major_to_minor[::-1],\n           (0, 1))\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, s)\n@@ -187,19 +187,19 @@ def f(x, y, z, a, b, c):\n     inps = [np.arange(math.prod(shape)).reshape(shape)] * 6\n     compiled = jax.jit(f, in_shardings=Format(DLL.AUTO),\n                        out_shardings=Format(DLL.AUTO)).lower(*inps).compile()\n-    arg_layouts, _ = compiled.input_layouts\n+    arg_formats, _ = compiled.input_formats\n     out1, out2 = compiled(*inps)\n \n-    compiled2 = jax.jit(f, in_shardings=arg_layouts).lower(*inps).compile()\n+    compiled2 = jax.jit(f, in_shardings=arg_formats).lower(*inps).compile()\n     out3, out4 = compiled2(*inps)\n \n-    for l1, l2 in safe_zip(arg_layouts, compiled2.input_layouts[0]):\n+    for l1, l2 in safe_zip(arg_formats, compiled2.input_formats[0]):\n       self.assertEqual(l1, l2)\n \n     self.assertArraysEqual(out1, out3)\n     self.assertArraysEqual(out2, out4)\n \n-    arrs = [jax.device_put(i, l) for i, l in zip(inps, arg_layouts)]\n+    arrs = [jax.device_put(i, l) for i, l in zip(inps, arg_formats)]\n     out5, out6 = jax.jit(f)(*arrs)\n     self.assertArraysEqual(out1, out5)\n     self.assertArraysEqual(out2, out6)\n@@ -219,8 +219,8 @@ def f(x, y):\n     jf = jax.jit(f, in_shardings=Format(DLL.AUTO, s),\n                  out_shardings=Format(DLL.AUTO, s))\n     compiled = jf.lower(np_inp, np_inp).compile()\n-    arg_layouts, _ = compiled.input_layouts\n-    arrs = [jax.device_put(i, l) for i, l in zip(arrs, arg_layouts)]\n+    arg_formats, _ = compiled.input_formats\n+    arrs = [jax.device_put(i, l) for i, l in zip(arrs, arg_formats)]\n     compiled(*arrs)\n \n   def test_aot_layout_mismatch(self):\n@@ -274,7 +274,7 @@ def test_device_put_concrete_layout(self):\n \n     compiled = jax.jit(\n         lambda x: x * 2, out_shardings=Format(DLL.AUTO)).lower(arr).compile()\n-    col = compiled.output_layouts\n+    col = compiled.output_formats\n \n     out = jax.device_put(np_inp, col)\n     self.assertEqual(out.format, col)\n@@ -306,7 +306,7 @@ def invalid_layout_spec(self):\n     compiled = jax.jit(lambda x: x).lower(x).compile()\n     with self.assertRaisesRegex(\n         ValueError, 'Sharding has to be concrete when layout.*'):\n-      Format(compiled.output_layouts[0], None)\n+      Format(compiled.output_formats[0], None)\n \n   def test_layout_on_sds(self):\n     mesh = jtu.create_mesh((2, 1), ('x', 'y'))\n@@ -314,12 +314,12 @@ def test_layout_on_sds(self):\n     np_inp = np.arange(16).reshape(8, 2)\n     arr = jax.device_put(np_inp, s)\n \n-    out_layout = jax.jit(jnp.sin, out_shardings=Format(DLL.AUTO)).lower(\n-        arr).compile().output_layouts\n+    out_format = jax.jit(jnp.sin, out_shardings=Format(DLL.AUTO)).lower(\n+        arr).compile().output_formats\n \n-    sds = jax.ShapeDtypeStruct(arr.shape, arr.dtype, sharding=out_layout)\n-    arg_layout, _ = jax.jit(lambda x: x * 2).lower(sds).compile().input_layouts\n-    self.assertEqual(arg_layout[0], out_layout)\n+    sds = jax.ShapeDtypeStruct(arr.shape, arr.dtype, sharding=out_format)\n+    arg_format, _ = jax.jit(lambda x: x * 2).lower(sds).compile().input_formats\n+    self.assertEqual(arg_format[0], out_format)\n \n     with self.assertRaisesRegex(\n         TypeError,\n@@ -333,12 +333,12 @@ def test_make_array_from_callback(self):\n     np_inp = np.arange(16).reshape(8, 2)\n     sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n \n-    layout = jax.jit(lambda x: x * 2).lower(sds).compile().output_layouts\n+    format = jax.jit(lambda x: x * 2).lower(sds).compile().output_formats\n \n-    out = jax.make_array_from_callback(np_inp.shape, layout,\n+    out = jax.make_array_from_callback(np_inp.shape, format,\n                                        lambda idx: np_inp[idx])\n     self.assertArraysEqual(out, np_inp)\n-    self.assertEqual(out.format, layout)\n+    self.assertEqual(out.format, format)\n \n     with self.assertRaisesRegex(\n         TypeError,\n@@ -417,18 +417,18 @@ def test_device_put_user_concrete_layout_multi_device(self):\n     jnp_inp = jnp.arange(math.prod(shape)).reshape(shape)\n     arr = jax.device_put(np_inp, s)\n \n-    custom_layout = Format(DLL(major_to_minor=(0, 1)), s)\n-    out1 = jax.device_put(arr, custom_layout)\n+    custom_format = Format(DLL(major_to_minor=(0, 1)), s)\n+    out1 = jax.device_put(arr, custom_format)\n \n     with jax.sharding.use_mesh(mesh):\n-      out2 = jax.device_put(arr, custom_layout)\n-      out3 = jax.device_put(jnp_inp, custom_layout)\n-      out4 = jax.device_put(np_inp, custom_layout)\n+      out2 = jax.device_put(arr, custom_format)\n+      out3 = jax.device_put(jnp_inp, custom_format)\n+      out4 = jax.device_put(np_inp, custom_format)\n \n     for o in [out1, out2, out3, out4]:\n       self.assertArraysEqual(o, np_inp)\n       self.assertEqual(o.format.device_local_layout.major_to_minor,\n-                       custom_layout.device_local_layout.major_to_minor)\n+                       custom_format.device_local_layout.major_to_minor)\n \n   def test_concrete_layout_jit(self):\n     mesh = jtu.create_mesh((2, 2), ('x', 'y'))\n@@ -619,16 +619,16 @@ def test_sparsecore_compute(self):\n \n     dll = DLL(major_to_minor=(0, 1), _tiling=((8,),))\n     s = SingleDeviceSharding(jax.devices()[0])\n-    sparse_layout = Format(dll, s)\n-    sparecore_arr = jax.device_put(inp, sparse_layout)\n-    dense_layout = Format(DLL(major_to_minor=(0, 1)), s)\n+    sparse_format = Format(dll, s)\n+    sparecore_arr = jax.device_put(inp, sparse_format)\n+    dense_format = Format(DLL(major_to_minor=(0, 1)), s)\n \n     @compute_on('tpu_sparsecore')\n     @jax.jit\n     def sparsecore_compute(x):\n       return x * x\n \n-    @partial(jax.jit, out_shardings=(dense_layout, sparse_layout))\n+    @partial(jax.jit, out_shardings=(dense_format, sparse_format))\n     def f(x, y):\n       return x * 2, sparsecore_compute(y)\n \n@@ -645,8 +645,8 @@ def test_sparsecore_compute_twice(self):\n \n     dll = DLL(major_to_minor=(0, 1), _tiling=((8,),))\n     s = SingleDeviceSharding(jax.devices()[0])\n-    sparse_layout = Format(dll, s)\n-    sparecore_arr = jax.device_put(inp, sparse_layout)\n+    sparse_format = Format(dll, s)\n+    sparecore_arr = jax.device_put(inp, sparse_format)\n \n     @compute_on('tpu_sparsecore')\n     @jax.jit\n@@ -658,7 +658,7 @@ def sparsecore_multiply(x, y):\n     def sparsecore_add(x, y):\n       return x + y\n \n-    @partial(jax.jit, donate_argnums=0, out_shardings=sparse_layout)\n+    @partial(jax.jit, donate_argnums=0, out_shardings=sparse_format)\n     def f(x):\n       return sparsecore_multiply(sparsecore_add(x, x) + 1, x)\n \n@@ -675,12 +675,12 @@ def test_sparsecore_and_host_compute(self):\n     s = SingleDeviceSharding(jax.devices()[0])\n \n     sparse_dll = DLL(major_to_minor=(0, 1), _tiling=((8,),))\n-    sparse_layout = Format(sparse_dll, s)\n-    sparecore_arr = jax.device_put(inp, sparse_layout)\n+    sparse_format = Format(sparse_dll, s)\n+    sparecore_arr = jax.device_put(inp, sparse_format)\n \n     host_dll = DLL(major_to_minor=(0, 1), _tiling=((1,),))\n-    host_layout = Format(host_dll, s)\n-    host_arr = jax.device_put(inp, host_layout)\n+    host_format = Format(host_dll, s)\n+    host_arr = jax.device_put(inp, host_format)\n \n     @compute_on('tpu_sparsecore')\n     @jax.jit\n@@ -694,8 +694,8 @@ def host_compute(x):\n \n     @partial(\n         jax.jit,\n-        in_shardings=(sparse_layout, host_layout),\n-        out_shardings=(sparse_layout, host_layout),\n+        in_shardings=(sparse_format, host_format),\n+        out_shardings=(sparse_format, host_format),\n     )\n     def f(x, y):\n       return sparsecore_compute(x), host_compute(y)\n@@ -710,8 +710,8 @@ def test_cpp_layout_cache_miss(self):\n     arr = jax.device_put(np_inp, s)\n \n     arr_m2m = arr.format.device_local_layout.major_to_minor\n-    custom_layout = Format(DLL(major_to_minor=arr_m2m[::-1]), s)\n-    arr2 = jax.device_put(np_inp, custom_layout)\n+    custom_format = Format(DLL(major_to_minor=arr_m2m[::-1]), s)\n+    arr2 = jax.device_put(np_inp, custom_format)\n \n     @jax.jit\n     def f(x):\n@@ -731,9 +731,9 @@ def test_layout_donation_with_default_layout(self):\n     shape = (16, 16)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     arr = jax.device_put(np_inp, s)\n-    out_layout = Format(arr.format.device_local_layout, s)\n+    out_format = Format(arr.format.device_local_layout, s)\n \n-    @partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\n+    @partial(jax.jit, out_shardings=out_format, donate_argnums=0)\n     def f(x):\n       return x * 2\n \n@@ -743,7 +743,7 @@ def f(x):\n \n     out = f(arr)\n     self.assertArraysEqual(out, np_inp * 2)\n-    self.assertEqual(out.format, out_layout)\n+    self.assertEqual(out.format, out_format)\n \n   def test_with_layout_constraint(self):\n     if not jtu.test_device_matches(['tpu']):\n\n```",
        "from_id": [
            "froystig",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29113 from hawkinsp:install\n\nPiperOrigin-RevId: 765199706",
        "output": "```diff\nCommit: 5a066bccc34ca63daa6b73eb1407f296dcecfea0\nDate: 2025-05-30T15:25:28Z\nURL: https://github.com/jax-ml/jax/commit/5a066bccc34ca63daa6b73eb1407f296dcecfea0\nFiles changed: 2\nAdditions: +16, Deletions: -17\ndiff --git a/README.md b/README.md\nindex 14a0a06ae700..e6af1b344f24 100644\n--- a/README.md\n+++ b/README.md\n@@ -225,14 +225,14 @@ Notebook](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html).\n \n ### Supported platforms\n \n-|            | Linux x86_64 | Linux aarch64 | Mac x86_64   | Mac aarch64  | Windows x86_64 | Windows WSL2 x86_64 |\n-|------------|--------------|---------------|--------------|--------------|----------------|---------------------|\n-| CPU        | yes          | yes           | yes          | yes          | yes            | yes                 |\n-| NVIDIA GPU | yes          | yes           | no           | n/a          | no             | experimental        |\n-| Google TPU | yes          | n/a           | n/a          | n/a          | n/a            | n/a                 |\n-| AMD GPU    | yes          | no            | experimental | n/a          | no             | no                  |\n-| Apple GPU  | n/a          | no            | n/a          | experimental | n/a            | n/a                 |\n-| Intel GPU  | experimental | n/a           | n/a          | n/a          | no             | no                  |\n+|            | Linux x86_64 | Linux aarch64 | Mac aarch64  | Windows x86_64 | Windows WSL2 x86_64 |\n+|------------|--------------|---------------|--------------|----------------|---------------------|\n+| CPU        | yes          | yes           | yes          | yes            | yes                 |\n+| NVIDIA GPU | yes          | yes           | n/a          | no             | experimental        |\n+| Google TPU | yes          | n/a           | n/a          | n/a            | n/a                 |\n+| AMD GPU    | yes          | no            | n/a          | no             | no                  |\n+| Apple GPU  | n/a          | no            | experimental | n/a            | n/a                 |\n+| Intel GPU  | experimental | n/a           | n/a          | no             | no                  |\n \n \n ### Instructions\ndiff --git a/docs/installation.md b/docs/installation.md\nindex 1314a2efa0a8..4019f6461473 100644\n--- a/docs/installation.md\n+++ b/docs/installation.md\n@@ -28,14 +28,14 @@ different builds for different operating systems and accelerators.\n \n The table below shows all supported platforms and installation options. Check if your setup is supported; and if it says _\"yes\"_ or _\"experimental\"_, then click on the corresponding link to learn how to install JAX in greater detail.\n \n-|                  | Linux, x86_64                         | Linux, aarch64                  | Mac, x86_64                           | Mac, aarch64                          | Windows, x86_64          | Windows WSL2, x86_64                     |\n-|------------------|---------------------------------------|---------------------------------|---------------------------------------|---------------------------------------|--------------------------|------------------------------------------|\n-| CPU              | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>`        | {ref}`jax0.4.38 only <install-cpu>`  | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>` | {ref}`yes <install-cpu>`                 |\n-| NVIDIA GPU       | {ref}`yes <install-nvidia-gpu>`       | {ref}`yes <install-nvidia-gpu>` | no                                    | n/a                                   | no                       | {ref}`experimental <install-nvidia-gpu>` |\n-| Google Cloud TPU | {ref}`yes <install-google-tpu>`       | n/a                             | n/a                                   | n/a                                   | n/a                      | n/a                                      |\n-| AMD GPU          | {ref}`yes <install-amd-gpu>`          | no                              | {ref}`experimental <install-mac-gpu>` | n/a                                   | no                       | no                                       |\n-| Apple GPU        | n/a                                   | no                              | n/a                                   | {ref}`experimental <install-mac-gpu>` | n/a                      | n/a                                      |\n-| Intel GPU        | {ref}`experimental <install-intel-gpu>`| n/a                            | n/a                                   | n/a                                     | no                       | no                                       |\n+|                  | Linux, x86_64                         | Linux, aarch64                  | Mac, aarch64                          | Windows, x86_64          | Windows WSL2, x86_64                     |\n+|------------------|---------------------------------------|---------------------------------|---------------------------------------|--------------------------|------------------------------------------|\n+| CPU              | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>`        | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>` | {ref}`yes <install-cpu>`                 |\n+| NVIDIA GPU       | {ref}`yes <install-nvidia-gpu>`       | {ref}`yes <install-nvidia-gpu>` | n/a                                   | no                       | {ref}`experimental <install-nvidia-gpu>` |\n+| Google Cloud TPU | {ref}`yes <install-google-tpu>`       | n/a                             | n/a                                   | n/a                      | n/a                                      |\n+| AMD GPU          | {ref}`yes <install-amd-gpu>`          | no                              | n/a                                   | no                       | no                                       |\n+| Apple GPU        | n/a                                   | no                              | {ref}`experimental <install-mac-gpu>` | n/a                      | n/a                                      |\n+| Intel GPU        | {ref}`experimental <install-intel-gpu>`| n/a                            | n/a                                     | no                       | no                                       |\n \n \n (install-cpu)=\n@@ -48,7 +48,6 @@ operating systems and architectures:\n \n - Linux, x86_64\n - Linux, aarch64\n-- macOS, Intel\n - macOS, Apple ARM-based\n - Windows, x86_64 (*experimental*)\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Don't sort replicated and unreduced axes wrt mesh axis names as they are not set and their order actually matters for all-reduce.\n\nPiperOrigin-RevId: 765199626",
        "output": "```diff\nCommit: 73c016a534af51614741d70d36c2c75ca59f2dcc\nDate: 2025-05-30T15:23:16Z\nURL: https://github.com/jax-ml/jax/commit/73c016a534af51614741d70d36c2c75ca59f2dcc\nFiles changed: 1\nAdditions: +3, Deletions: -12\ndiff --git a/jax/_src/named_sharding.py b/jax/_src/named_sharding.py\nindex faf0b2a9f2b2..0b4efdb41d25 100644\n--- a/jax/_src/named_sharding.py\n+++ b/jax/_src/named_sharding.py\n@@ -288,13 +288,6 @@ def _custom_repr(self):\n     priority_repr = '' if self.priority is None else f'p{self.priority}'\n     return f'{{{axes_repr}{open_repr}}}{priority_repr}'\n \n-def _get_axes(axes, mesh_shape):\n-  if not axes:\n-    return ()\n-  assert mesh_shape is not None\n-  # Sort wrt mesh axis names so order is deterministic and doesn't hang in\n-  # McJAX.\n-  return tuple(n for n, _ in mesh_shape if n in axes)\n \n @dataclasses.dataclass(kw_only=True)\n class SdyArray:\n@@ -314,13 +307,11 @@ def build(self) -> sdy.TensorShardingAttr:\n           [sdy.MeshAxisAttr.get(name, size) for name, size in self.mesh_shape],\n           ldi)\n \n-    replicated_axes = _get_axes(self.replicated_axes, self.mesh_shape)\n-    unreduced_axes = _get_axes(self.unreduced_axes, self.mesh_shape)\n     return sdy.TensorShardingAttr.get(\n         mesh_attr,\n         [dim_sharding.build() for dim_sharding in self.dim_shardings],\n-        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes],\n-        unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in unreduced_axes])\n+        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in self.replicated_axes],\n+        unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in self.unreduced_axes])\n \n   def __repr__(self):\n     dim_sharding_repr = ', '.join(\n@@ -342,7 +333,7 @@ def modify_sdy_sharding_wrt_axis_types(sdy_sharding: SdyArray, mesh):\n       dim_shardings.append(SdyDim(axes=[], is_open=True)\n                            if not d.axes and not d.is_open else d)\n       used_axes.extend(d.axes)\n-    remaining_axes = set(mesh.axis_names) - set(used_axes)\n+    remaining_axes = tuple(n for n in mesh.axis_names if n not in used_axes)\n     replicated_axes = tuple(r for r in remaining_axes\n                             if mesh._name_to_type[r] == mesh_lib.AxisType.Explicit)\n     return SdyArray(mesh_shape=sdy_sharding.mesh_shape,\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29097 from MichaelHudgins:more-actions-fixes\n\nPiperOrigin-RevId: 765167817",
        "output": "```diff\nCommit: 2d4baf4772b3ad2a3832104458aa69bbf1456953\nDate: 2025-05-30T13:38:54Z\nURL: https://github.com/jax-ml/jax/commit/2d4baf4772b3ad2a3832104458aa69bbf1456953\nFiles changed: 6\nAdditions: +4, Deletions: -47\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex 09c9d173e0d0..cc3ae89d97f9 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -16,22 +16,18 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version to test?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n \n jobs:\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 458589199c53..d30e1b56dab8 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -17,33 +17,28 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version to test?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       jaxlib-version:\n         description: \"Which jaxlib version to test? (head/pypi_latest)\"\n         type: string\n-        required: true\n         default: \"head\"\n       gcs_download_uri:\n         description: \"GCS location URI from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 72d554aa5d1b..95ab90412494 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -12,7 +12,6 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: choice\n-        required: true\n         default: \"linux-x86-n2-16\"\n         options:\n         - \"linux-x86-n2-16\"\n@@ -21,7 +20,6 @@ on:\n       artifact:\n         description: \"Which JAX artifact to build?\"\n         type: choice\n-        required: true\n         default: \"jaxlib\"\n         options:\n         - \"jax\"\n@@ -31,7 +29,6 @@ on:\n       python:\n         description: \"Which python version should the artifact be built for?\"\n         type: choice\n-        required: false\n         default: \"3.12\"\n         options:\n         - \"3.10\"\n@@ -41,7 +38,6 @@ on:\n       clone_main_xla:\n         description: \"Should latest XLA be used?\"\n         type: choice\n-        required: false\n         default: \"0\"\n         options:\n         - \"1\"\n@@ -49,7 +45,6 @@ on:\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: choice\n-        required: false\n         default: 'no'\n         options:\n         - 'yes'\n@@ -59,31 +54,25 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       artifact:\n         description: \"Which JAX artifact to build?\"\n         type: string\n-        required: true\n         default: \"jaxlib\"\n       python:\n         description: \"Which python version should the artifact be built for?\"\n         type: string\n-        required: false\n         default: \"3.12\"\n       clone_main_xla:\n         description: \"Should latest XLA be used?\"\n         type: string\n-        required: false\n         default: \"0\"\n       upload_artifacts_to_gcs:\n         description: \"Should the artifacts be uploaded to a GCS bucket?\"\n-        required: true\n         default: true\n         type: boolean\n       gcs_upload_uri:\n         description: \"GCS location prefix to where the artifacts should be uploaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n     outputs:\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex a92f2d96dc89..95086257c62b 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -17,34 +17,28 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version should the artifact be built for?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       download-jax-only-from-gcs:\n         description: \"Whether to download only the jax wheel from GCS (e.g for testing a jax only release)\"\n-        required: false\n         default: '0'\n         type: string\n       gcs_download_uri:\n         description: \"GCS location prefix from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n-\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 6fa4e14f8b85..d576370bb772 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -17,44 +17,36 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version to test?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       cuda-version:\n         description: \"Which CUDA version to test?\"\n         type: string\n-        required: true\n         default: \"12.8\"\n       use-nvidia-pip-wheels:\n         description: \"Whether to download CUDA packages from PyPI?\"\n         type: boolean\n-        required: false\n         default: false\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       download-jax-only-from-gcs:\n         description: \"Whether to download only the jax wheel from GCS (e.g for testing a jax only release)\"\n-        required: false\n         default: '0'\n         type: string\n       gcs_download_uri:\n         description: \"GCS location prefix from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n-\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 5f56b165c295..313bbede52f5 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -22,32 +22,26 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-ct5lp-224-8tpu\"\n       cores:\n         description: \"How many TPU cores should the test use?\"\n         type: string\n-        required: true\n         default: \"8\"\n       tpu-type:\n         description: \"Which TPU type is used for testing?\"\n         type: string\n-        required: true\n         default: \"v5e-8\"\n       python:\n         description: \"Which Python version should be used for testing?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       run-full-tpu-test-suite:\n         description: \"Should the full TPU test suite be run?\"\n         type: string\n-        required: false\n         default: \"0\"\n       libtpu-version-type:\n         description: \"Which libtpu version should be used for testing?\"\n         type: string\n-        required: false\n         # Choices are:\n         # - \"nightly\": Use the nightly libtpu wheel.\n         # - \"pypi_latest\": Use the latest libtpu wheel from PyPI.\n@@ -55,20 +49,17 @@ on:\n         default: \"nightly\"\n       download-jax-only-from-gcs:\n         description: \"Whether to download only the jax wheel from GCS (e.g for testing a jax only release)\"\n-        required: false\n         default: '0'\n         type: string\n       gcs_download_uri:\n         description: \"GCS location prefix from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n-\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Remove Mac x86 from the installation instructions.\n\nWe have not been shipping Mac x86 for some time.",
        "output": "```diff\nCommit: 6564a4bb5f9eac51e449f4403997b53c873ffa75\nDate: 2025-05-30T13:10:51Z\nURL: https://github.com/jax-ml/jax/commit/6564a4bb5f9eac51e449f4403997b53c873ffa75\nFiles changed: 2\nAdditions: +16, Deletions: -17\ndiff --git a/README.md b/README.md\nindex 14a0a06ae700..e6af1b344f24 100644\n--- a/README.md\n+++ b/README.md\n@@ -225,14 +225,14 @@ Notebook](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html).\n \n ### Supported platforms\n \n-|            | Linux x86_64 | Linux aarch64 | Mac x86_64   | Mac aarch64  | Windows x86_64 | Windows WSL2 x86_64 |\n-|------------|--------------|---------------|--------------|--------------|----------------|---------------------|\n-| CPU        | yes          | yes           | yes          | yes          | yes            | yes                 |\n-| NVIDIA GPU | yes          | yes           | no           | n/a          | no             | experimental        |\n-| Google TPU | yes          | n/a           | n/a          | n/a          | n/a            | n/a                 |\n-| AMD GPU    | yes          | no            | experimental | n/a          | no             | no                  |\n-| Apple GPU  | n/a          | no            | n/a          | experimental | n/a            | n/a                 |\n-| Intel GPU  | experimental | n/a           | n/a          | n/a          | no             | no                  |\n+|            | Linux x86_64 | Linux aarch64 | Mac aarch64  | Windows x86_64 | Windows WSL2 x86_64 |\n+|------------|--------------|---------------|--------------|----------------|---------------------|\n+| CPU        | yes          | yes           | yes          | yes            | yes                 |\n+| NVIDIA GPU | yes          | yes           | n/a          | no             | experimental        |\n+| Google TPU | yes          | n/a           | n/a          | n/a            | n/a                 |\n+| AMD GPU    | yes          | no            | n/a          | no             | no                  |\n+| Apple GPU  | n/a          | no            | experimental | n/a            | n/a                 |\n+| Intel GPU  | experimental | n/a           | n/a          | no             | no                  |\n \n \n ### Instructions\ndiff --git a/docs/installation.md b/docs/installation.md\nindex 1314a2efa0a8..4019f6461473 100644\n--- a/docs/installation.md\n+++ b/docs/installation.md\n@@ -28,14 +28,14 @@ different builds for different operating systems and accelerators.\n \n The table below shows all supported platforms and installation options. Check if your setup is supported; and if it says _\"yes\"_ or _\"experimental\"_, then click on the corresponding link to learn how to install JAX in greater detail.\n \n-|                  | Linux, x86_64                         | Linux, aarch64                  | Mac, x86_64                           | Mac, aarch64                          | Windows, x86_64          | Windows WSL2, x86_64                     |\n-|------------------|---------------------------------------|---------------------------------|---------------------------------------|---------------------------------------|--------------------------|------------------------------------------|\n-| CPU              | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>`        | {ref}`jax0.4.38 only <install-cpu>`  | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>` | {ref}`yes <install-cpu>`                 |\n-| NVIDIA GPU       | {ref}`yes <install-nvidia-gpu>`       | {ref}`yes <install-nvidia-gpu>` | no                                    | n/a                                   | no                       | {ref}`experimental <install-nvidia-gpu>` |\n-| Google Cloud TPU | {ref}`yes <install-google-tpu>`       | n/a                             | n/a                                   | n/a                                   | n/a                      | n/a                                      |\n-| AMD GPU          | {ref}`yes <install-amd-gpu>`          | no                              | {ref}`experimental <install-mac-gpu>` | n/a                                   | no                       | no                                       |\n-| Apple GPU        | n/a                                   | no                              | n/a                                   | {ref}`experimental <install-mac-gpu>` | n/a                      | n/a                                      |\n-| Intel GPU        | {ref}`experimental <install-intel-gpu>`| n/a                            | n/a                                   | n/a                                     | no                       | no                                       |\n+|                  | Linux, x86_64                         | Linux, aarch64                  | Mac, aarch64                          | Windows, x86_64          | Windows WSL2, x86_64                     |\n+|------------------|---------------------------------------|---------------------------------|---------------------------------------|--------------------------|------------------------------------------|\n+| CPU              | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>`        | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>` | {ref}`yes <install-cpu>`                 |\n+| NVIDIA GPU       | {ref}`yes <install-nvidia-gpu>`       | {ref}`yes <install-nvidia-gpu>` | n/a                                   | no                       | {ref}`experimental <install-nvidia-gpu>` |\n+| Google Cloud TPU | {ref}`yes <install-google-tpu>`       | n/a                             | n/a                                   | n/a                      | n/a                                      |\n+| AMD GPU          | {ref}`yes <install-amd-gpu>`          | no                              | n/a                                   | no                       | no                                       |\n+| Apple GPU        | n/a                                   | no                              | {ref}`experimental <install-mac-gpu>` | n/a                      | n/a                                      |\n+| Intel GPU        | {ref}`experimental <install-intel-gpu>`| n/a                            | n/a                                     | no                       | no                                       |\n \n \n (install-cpu)=\n@@ -48,7 +48,6 @@ operating systems and architectures:\n \n - Linux, x86_64\n - Linux, aarch64\n-- macOS, Intel\n - macOS, Apple ARM-based\n - Windows, x86_64 (*experimental*)\n \n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Unconditionally emit line info for Mosaic GPU kernels\n\nI also changed the lowering to override --jax_include_full_tracebacks_in_locations\nso that we get a single location per emitted op, since the\nensure-debug-info-scope-on-llvm-func pass in MLIR does not correctly handle\nnested CallSiteLocs.\n\nPiperOrigin-RevId: 765112273",
        "output": "```diff\nCommit: a9407763d9abf80f8636631065f9256cf0238e6d\nDate: 2025-05-30T10:13:38Z\nURL: https://github.com/jax-ml/jax/commit/a9407763d9abf80f8636631065f9256cf0238e6d\nFiles changed: 3\nAdditions: +13, Deletions: -12\ndiff --git a/jax/_src/pallas/mosaic_gpu/BUILD b/jax/_src/pallas/mosaic_gpu/BUILD\nindex 74b44fb8f991..8a5d087125b7 100644\n--- a/jax/_src/pallas/mosaic_gpu/BUILD\n+++ b/jax/_src/pallas/mosaic_gpu/BUILD\n@@ -45,6 +45,7 @@ pytype_strict_library(\n         \":core\",\n         \":lowering\",\n         \"//jax\",\n+        \"//jax:config\",\n         \"//jax:core\",\n         \"//jax:mlir\",\n         \"//jax:mosaic_gpu\",\ndiff --git a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\nindex ccbe4d36edc9..1d55a6e862a0 100644\n--- a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n+++ b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n@@ -24,6 +24,7 @@\n \n import jax\n from jax import lax\n+from jax._src import config\n from jax._src import core as jax_core\n from jax._src import sharding_impls\n from jax._src.interpreters import mlir\n@@ -73,9 +74,12 @@ def pallas_call_lowering(\n     if isinstance(axis_context, sharding_impls.SPMDAxisContext):\n       jax_mesh = axis_context.mesh\n \n-  lowering_result = lowering.lower_pipelined_jaxpr_to_module(\n-      grid_mapping, mesh, jax_mesh, jaxpr, params, cost_estimate\n-  )\n+  # TODO(slebedev): Remove this once the ensure-debug-info-scope-on-llvm-func\n+  # pass correctly handles full tracebacks.\n+  with config.include_full_tracebacks_in_locations(False):\n+    lowering_result = lowering.lower_pipelined_jaxpr_to_module(\n+        grid_mapping, mesh, jax_mesh, jaxpr, params, cost_estimate\n+    )\n   if debug:\n     print(f\"\\nThe Mosaic GPU module for pallas_call {debug_info.func_src_info}:\")\n     print(lowering_result.module.operation)\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 5253d4590658..214521ce3764 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -343,7 +343,6 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n     mlir::LLVM::registerDIScopeForLLVMFuncOpPass();\n     return true;\n   });\n-  bool emit_line_info = getenv(\"MOSAIC_GPU_LINE_INFO\") != nullptr;\n   const char *cuda_root = GetCUDARoot();\n   if (!cuda_root) {\n     return mlir::failure();\n@@ -360,8 +359,8 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n         convert-scf-to-cf,\n         convert-nvvm-to-llvm,\n         expand-strided-metadata,\n-        nvvm-attach-target{O=3 chip=)\", sm, \" fast=false features=+\",\n-      ptx_isa,\n+        nvvm-attach-target{O=3 chip=)\",\n+      sm, \" fast=false features=+\", ptx_isa,\n       R\"( ftz=false  module= triple=nvptx64-nvidia-cuda},\n         lower-affine,\n         convert-arith-to-llvm{index-bitwidth=0},\n@@ -369,7 +368,6 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n         canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true},\n         cse,\n         )\",\n-      emit_line_info ? \"\" : \"gpu.module(strip-debuginfo),\",\n       R\"(\n         gpu.module(convert-gpu-to-nvvm{has-redux=false index-bitwidth=64 use-bare-ptr-memref-call-conv=false}),\n         gpu.module(canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}),\n@@ -377,13 +375,11 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n         gpu.module(mosaic-byval-insertion),\n         gpu.module(reconcile-unrealized-casts),\n         mosaic-convert-gpu-to-llvm,\n-        )\",\n-      emit_line_info ? \"ensure-debug-info-scope-on-llvm-func{emission-kind=DebugDirectivesOnly},\" : \"\",\n-      \"gpu-module-to-binary{format=\",\n+        ensure-debug-info-scope-on-llvm-func{emission-kind=DebugDirectivesOnly},\n+        gpu-module-to-binary{format=)\",\n       mlir::gpu::stringifyCompilationTarget(target).str(),\n       (!nvshmem_path.empty() ? \" l=\" + nvshmem_path : \"\"),\n-      (emit_line_info ? \"  opts=-lineinfo\" : \"\"),\n-      \" toolkit=\", cuda_root,\n+      \"  opts=-lineinfo toolkit=\", cuda_root,\n       R\"(},\n         convert-math-to-llvm{approximate-log1p=true},\n         canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true},\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Move the semaphore implementation to Mosaic\n\nPallas lowering should not be doing any heavy lifting here. The implementation\nis quite low level and should ideally live closer to where other synchronization\nprimitives are implemented.\n\nPiperOrigin-RevId: 765092823",
        "output": "```diff\nCommit: 75b2c7e553e7ad9a141e0d94ff45af31eacfebd3\nDate: 2025-05-30T09:04:19Z\nURL: https://github.com/jax-ml/jax/commit/75b2c7e553e7ad9a141e0d94ff45af31eacfebd3\nFiles changed: 4\nAdditions: +125, Deletions: -36\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 003fa0419f63..ce74a5ba7c05 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -3108,13 +3108,8 @@ def _semaphore_signal_lowering_rule(\n   # receive a signal).\n   if ctx.module_ctx.auto_barriers:\n     mgpu.utils.warpgroup_barrier()\n-  pred = ctx.module_ctx.single_wg_lane_predicate\n-  llvm_dialect.inline_asm(\n-    i32,\n-    [sem_ptr, val, pred],\n-    \"@$3 atom.add.release.sys.global.u32 $0, [$1], $2;\",\n-    \"=r,l,r,b\",\n-    has_side_effects=True,\n+  mgpu_utils.SemaphoreRef(sem_ptr).signal(\n+      val, predicate=ctx.module_ctx.single_wg_lane_predicate\n   )\n   return ()\n \n@@ -3127,35 +3122,9 @@ def _semaphore_wait_lowering_rule(ctx: LoweringRuleContext, *args, args_tree):\n     raise NotImplementedError(\n         f\"Unhandled transforms for semaphore_wait: {transforms}\"\n     )\n-\n-  sem_ptr = mgpu.utils.memref_ptr(sem)\n-  i32_ty = ir.IntegerType.get_signless(32)\n-  ne_pred = arith_dialect.CmpIPredicate.ne\n-  val = _ir_constant(value, i32_ty)\n-\n-  with mgpu.single_thread(scope=mgpu.ThreadSubset.WARPGROUP):\n-    # Create the while loop for busy waiting\n-    while_op = scf_dialect.WhileOp([i32_ty], [val])\n-    before_block = while_op.before.blocks.append(i32_ty)\n-    with ir.InsertionPoint.at_block_begin(before_block):\n-      [expected_in_memory] = before_block.arguments\n-      new_val = arith_dialect.subi(expected_in_memory, val)\n-      in_memory = llvm_dialect.inline_asm(\n-        i32_ty,\n-        [sem_ptr, expected_in_memory, new_val],\n-        \"atom.acquire.sys.global.cas.b32 $0, [$1], $2, $3;\",\n-        \"=r,l,r,r\",\n-        has_side_effects=True,\n-      )\n-      comparison = arith_dialect.cmpi(ne_pred, in_memory, expected_in_memory)\n-      new_expected_in_memory = arith_dialect.maxui(in_memory, val)\n-      scf_dialect.condition(comparison, [new_expected_in_memory])\n-    after_block = while_op.after.blocks.append(i32_ty)\n-    with ir.InsertionPoint.at_block_begin(after_block):\n-      scf_dialect.yield_(after_block.arguments)\n-  # NOTE: This barrier is necessary for a correct lowering of this op and can't\n-  # be removed even if auto_barriers is False.\n-  mgpu_utils.warpgroup_barrier()\n+  i32 = ir.IntegerType.get_signless(32)\n+  val = _ir_constant(value, i32)\n+  mgpu_utils.SemaphoreRef(mgpu.utils.memref_ptr(sem)).wait(val)\n   return ()\n \n \ndiff --git a/jax/experimental/mosaic/gpu/__init__.py b/jax/experimental/mosaic/gpu/__init__.py\nindex 82155f86d9ea..cd207c2b2519 100644\n--- a/jax/experimental/mosaic/gpu/__init__.py\n+++ b/jax/experimental/mosaic/gpu/__init__.py\n@@ -74,6 +74,7 @@\n     DynamicSlice as DynamicSlice,\n     Partition as Partition,\n     Partition1D as Partition1D,\n+    SemaphoreRef as SemaphoreRef,\n     ThreadSubset as ThreadSubset,\n     bitwidth as bitwidth,\n     bytewidth as bytewidth,\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex 51b6ed4612ca..a76e077ff463 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -744,6 +744,9 @@ def warpgroup_barrier():\n       has_side_effects=True,\n   )\n \n+def warp_barrier():\n+  nvvm.bar_warp_sync(c(0xffffffff, ir.IntegerType.get_signless(32)))\n+\n \n @dataclasses.dataclass(frozen=True)\n class BarrierRef:\n@@ -1046,6 +1049,67 @@ def wait_parity(self, *args, **kwargs):\n     self.barrier.wait_parity(*args, **kwargs)\n \n \n+@dataclasses.dataclass(frozen=True)\n+class SemaphoreRef:\n+  ptr: ir.Value\n+\n+  def signal(self, value: ir.Value | int, predicate: ir.Value | None = None):\n+    i32 = ir.IntegerType.get_signless(32)\n+    if not isinstance(value, ir.Value):\n+      value = c(value, i32)\n+    elif value.type != i32:\n+      raise ValueError(f\"Expected a i32 value, got {value.type}\")\n+    if predicate is None:\n+      predicate = single_thread_predicate(ThreadSubset.WARPGROUP)\n+    llvm.inline_asm(\n+      i32,\n+      [self.ptr, value, predicate],\n+      \"@$3 atom.add.release.sys.global.u32 $0, [$1], $2;\",\n+      \"=r,l,r,b\",\n+      has_side_effects=True,\n+    )\n+\n+  def wait(\n+      self,\n+      value: ir.Value | int = 1,\n+      scope: ThreadSubset = ThreadSubset.WARPGROUP,\n+  ):\n+    i32 = ir.IntegerType.get_signless(32)\n+    if not isinstance(value, ir.Value):\n+      value = c(value, i32)\n+    elif value.type != i32:\n+      raise ValueError(f\"Expected a i32 value, got {value.type}\")\n+\n+    ne_pred = arith.CmpIPredicate.ne\n+\n+    with single_thread(scope=scope):\n+      # Create the while loop for busy waiting\n+      while_op = scf.WhileOp([i32], [value])\n+      before_block = while_op.before.blocks.append(i32)\n+      with ir.InsertionPoint.at_block_begin(before_block):\n+        [expected_in_memory] = before_block.arguments\n+        new_val = arith.subi(expected_in_memory, value)\n+        in_memory = llvm.inline_asm(\n+          i32,\n+          [self.ptr, expected_in_memory, new_val],\n+          \"atom.acquire.sys.global.cas.b32 $0, [$1], $2, $3;\",\n+          \"=r,l,r,r\",\n+          has_side_effects=True,\n+        )\n+        comparison = arith.cmpi(ne_pred, in_memory, expected_in_memory)\n+        new_expected_in_memory = arith.maxui(in_memory, value)\n+        scf.condition(comparison, [new_expected_in_memory])\n+      after_block = while_op.after.blocks.append(i32)\n+      with ir.InsertionPoint.at_block_begin(after_block):\n+        scf.yield_(after_block.arguments)\n+    if scope == ThreadSubset.WARPGROUP:\n+      warpgroup_barrier()\n+    elif scope == ThreadSubset.WARP:\n+      warp_barrier()\n+    else:\n+      raise ValueError(f\"Unsupported scope: {scope}\")\n+\n+\n class Partition:\n   source_bounds: tuple[int, ...]\n   target_bounds: tuple[int, ...]\ndiff --git a/tests/mosaic/gpu_test_distributed.py b/tests/mosaic/gpu_test_distributed.py\nindex cf3913771983..c289b27c0be1 100644\n--- a/tests/mosaic/gpu_test_distributed.py\n+++ b/tests/mosaic/gpu_test_distributed.py\n@@ -23,6 +23,7 @@\n from jax._src.interpreters import mlir\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import arith\n+from jax._src.lib.mlir.dialects import memref\n from jax.experimental.mosaic.gpu import dialect as mgpu_dialect  # pylint: disable=g-importing-member\n from jax.experimental import shard\n from jax.experimental import multihost_utils\n@@ -70,6 +71,28 @@ def setUp(self):\n \n class ProfilerTest(TestCase):\n \n+  def test_get_device_id(self):\n+    index = ir.IndexType.get()\n+    def kernel(ctx, dst, _):\n+      device_id = ctx.device_id()\n+      memref.store(device_id, dst, [arith.constant(index, 0)])\n+    mesh = jax.make_mesh(\n+        (jax.device_count(),), (\"x\",), axis_types=(jax.sharding.AxisType.Explicit,)\n+    )\n+    with jax.sharding.use_mesh(mesh):\n+      out_shape = jax.ShapeDtypeStruct((1,), jnp.int32)\n+      y = jax.jit(\n+          jax.shard_map(\n+              mgpu.as_gpu_kernel(\n+                  kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ()\n+              ),\n+              out_specs=P(\"x\"),\n+              check_vma=False,\n+          )\n+      )()\n+      y_np = multihost_utils.process_allgather(y, tiled=True)\n+      np.testing.assert_array_equal(y_np, np.arange(jax.device_count()))\n+\n   def test_remote_async_copy(self):\n     i32 = ir.IntegerType.get_signless(32)\n     def kernel(ctx, src, dst, scratch):\n@@ -99,6 +122,38 @@ def kernel(ctx, src, dst, scratch):\n           y_np, np.concatenate(np.split(x_np, 2)[::-1], axis=0)\n       )\n \n+  def test_remote_semaphore(self):\n+    i32 = ir.IntegerType.get_signless(32)\n+    def kernel(ctx, sem, _):\n+      my_device = ctx.device_id()\n+      other_device = arith.subi(arith.constant(i32, 1), my_device)\n+      my_sem = mgpu.SemaphoreRef(mgpu.utils.memref_ptr(sem))\n+      other_dst = ctx.to_remote(sem, other_device)\n+      other_sem = mgpu.SemaphoreRef(mgpu.utils.memref_ptr(other_dst))\n+      # We signal and wait a different amount on each device to make sure we're\n+      # really communicating here.\n+      other_sem.signal(arith.addi(arith.constant(i32, 1), other_device))\n+      @mgpu.fori(arith.addi(arith.constant(i32, 1), my_device), None)\n+      def wait_loop(i, _):\n+        my_sem.wait(1)\n+\n+    mesh = jax.make_mesh(\n+        (2,), (\"x\",), axis_types=(jax.sharding.AxisType.Explicit,)\n+    )\n+    with jax.sharding.use_mesh(mesh):\n+      sem = shard.reshard(jnp.zeros((1,), dtype=jnp.int32), P())\n+      out_sem = jax.jit(\n+          jax.shard_map(\n+              mgpu.as_gpu_kernel(\n+                  kernel, (1, 1, 1), (128, 1, 1), (), (), (), inout_shape=sem\n+              ),\n+              out_specs=P(\"x\"),\n+              check_vma=False,\n+          )\n+      )(sem)\n+      out_sems = multihost_utils.process_allgather(out_sem, tiled=True)\n+      np.testing.assert_array_equal(out_sems, np.zeros_like(out_sems))\n+\n \n if __name__ == \"__main__\":\n   # This test doesn't work with the platform allocator, so we override it\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "rename `Array.layout` to `Array.format`\n\nThis change renames the attribute and updates the codebase to refer to the new name. It should have minimal external effect, since it keeps a `layout` alias for the attribute.\n\nCo-authored-by: Yash Katariya <yashkatariya@google.com>\nPiperOrigin-RevId: 764967359",
        "output": "```diff\nCommit: 7ff6f0d01a7e324787b238f6998028a4a2686625\nDate: 2025-05-30T01:35:42Z\nURL: https://github.com/jax-ml/jax/commit/7ff6f0d01a7e324787b238f6998028a4a2686625\nFiles changed: 8\nAdditions: +46, Deletions: -43\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 2630fc7ae1be..c21e8248d52d 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -2909,12 +2909,12 @@ def update(self, **kwargs):\n       if self._dll is not None and isinstance(s, Sharding):\n         raise ValueError(\n             f\"You are updating ShapeDtypeStruct with a {type(s)} when the\"\n-            f\" original ShapeDtypeStruct had a concrete layout {self.layout}.\"\n+            f\" original ShapeDtypeStruct had a concrete layout {self.format}.\"\n             \" This might lead to bugs. If you want to do this, create a new\"\n             \" ShapeDtypeStruct via the constructor.\")\n       sharding = s\n     else:\n-      sharding = self.layout\n+      sharding = self.format\n     return ShapeDtypeStruct(\n         shape=kwargs.pop('shape', self.shape),\n         dtype=kwargs.pop('dtype', self.dtype),\ndiff --git a/jax/_src/array.py b/jax/_src/array.py\nindex 29c7a17b07f1..9a71b12ed1a8 100644\n--- a/jax/_src/array.py\n+++ b/jax/_src/array.py\n@@ -547,7 +547,7 @@ def addressable_shards(self) -> Sequence[Shard]:\n     return out\n \n   @property\n-  def layout(self):\n+  def format(self):\n     # TODO(yashkatariya): Remove the deleted check from here.\n     if self.is_deleted():\n       return Format(None, self.sharding)\n@@ -561,6 +561,9 @@ def layout(self):\n       else:\n         raise\n \n+  # TODO(frostig, yashkatariya): remove\n+  layout = format\n+\n   @property\n   def global_shards(self) -> Sequence[Shard]:\n     \"\"\"Returns list of all `Shard`s of the Array across all devices.\n@@ -812,7 +815,7 @@ def get_data(index: Index | None) -> ArrayImpl | np.ndarray:\n         and sharding.is_fully_replicated\n         and first_value.is_fully_replicated\n         and first_value.sharding._device_assignment == tuple(devices)\n-        and first_value.layout.device_local_layout == dll):\n+        and first_value.format.device_local_layout == dll):\n       return first_value\n \n   if dtypes.issubdtype(aval.dtype, dtypes.extended):\n@@ -1197,7 +1200,7 @@ def _array_shard_arg(xs, shardings, layouts, copy_semantics):\n     x._check_if_deleted()\n     indices, same_indices = _sharding_indices_and_eq(x.sharding, x.shape, sharding)\n     same_layout = (True if layout is None else\n-                   x.layout.device_local_layout == layout)\n+                   x.format.device_local_layout == layout)\n \n     if not x.is_fully_addressable:\n       if same_indices and same_layout:\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex a7c8d4ea7380..028c2cfa125e 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -497,7 +497,7 @@ def _device_put_impl(\n   if isinstance(device, Format):\n     l = device\n     dll = l.device_local_layout\n-    x_dll = x.layout.device_local_layout if hasattr(x, 'layout') else None\n+    x_dll = x.format.device_local_layout if hasattr(x, 'format') else None\n     if dll is None and l.sharding is None:\n       return _device_put_sharding_impl(x, aval, l.sharding, copy)\n     if (not isinstance(l.sharding, Sharding) or\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex af1f8217951c..2072aaf44b5a 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -3255,11 +3255,11 @@ def check_array_xla_sharding_layout_match(\n           'sharding'))\n \n     if (not db_xs and arg._committed and\n-        arg.layout.device_local_layout is not None and xl is not None and\n-        arg.layout.device_local_layout != xl):\n+        arg.format.device_local_layout is not None and xl is not None and\n+        arg.format.device_local_layout != xl):\n       errors.append(\n           (\"Got input layout(s) that compiled object was called with: \"\n-          f\"{arg.layout.device_local_layout} and layout(s) the computation was \"\n+          f\"{arg.format.device_local_layout} and layout(s) the computation was \"\n           f\"compiled with: {xl} for arg {name} with \"\n           f\"shape: {arg.aval.str_short()}\",\n           'layout'))\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex f012459296a1..4113f764e888 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -1651,8 +1651,8 @@ def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n     # below. We cannot replace default layout with None to raise nicer errors.\n     # `dispatch_arg_layout` replaces default layouts with `None` to simplify\n     # dispatch and lowering logic downstream.\n-    if hasattr(arg, 'layout'):\n-      arg_layout = arg.layout.device_local_layout\n+    if hasattr(arg, 'format'):\n+      arg_layout = arg.format.device_local_layout\n       dispatch_arg_layout = (None if pxla.is_default_layout(arg_layout, rs, aval)\n                              else arg_layout)\n     else:\n@@ -1670,7 +1670,7 @@ def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n         resolved_in_layouts.append(None)\n     else:\n       # arg_layout can be None because some backends don't implement the\n-      # required layout methods. Hence `arr.layout` can return\n+      # required layout methods. Hence `arr.format` can return\n       # `Format(None, sharding)`\n       if (committed\n           and not is_pmap_sharding\n@@ -2845,7 +2845,7 @@ def _sharding_constraint_impl(x, sharding, layout, context_mesh,\n     # Run a jit here to raise good errors when device assignment don't match.\n     return api.jit(_identity_fn, out_shardings=sharding)(x)\n   else:\n-    if (hasattr(x, 'layout') and x.layout.device_local_layout == layout and\n+    if (hasattr(x, 'format') and x.format.device_local_layout == layout and\n         x.sharding.is_equivalent_to(sharding, x.ndim)):\n       return x\n     return api.jit(_identity_fn, out_shardings=Format(layout, sharding))(x)\n@@ -3193,7 +3193,7 @@ def _layout_constraint_impl(x, *, layout):\n     raise ValueError(\n         'with_layout_constraint in eager mode can only be applied to'\n         f' jax.Arrays. Got {type(x)}')\n-  if x.layout.device_local_layout == layout:  # type: ignore\n+  if x.format.device_local_layout == layout:  # type: ignore\n     return x\n   return api.jit(_identity_fn, out_shardings=Format(layout, x.sharding))(x)\n layout_constraint_p.def_impl(_layout_constraint_impl)\ndiff --git a/jax/experimental/array_serialization/serialization_test.py b/jax/experimental/array_serialization/serialization_test.py\nindex 3bee72967101..0611388a1d80 100644\n--- a/jax/experimental/array_serialization/serialization_test.py\n+++ b/jax/experimental/array_serialization/serialization_test.py\n@@ -595,7 +595,7 @@ def test_load_with_layout(self):\n \n     out_layout = jax.jit(lambda x: x.T, out_shardings=Format(DLL.AUTO)).lower(\n         arr).compile().output_layouts\n-    self.assertEqual(arr.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(arr.format.device_local_layout.major_to_minor,\n                      out_layout.device_local_layout.major_to_minor[::-1])\n \n     ckpt_dir = pathlib.Path(self.create_tempdir('ckpt').full_path)\n@@ -611,7 +611,7 @@ def test_load_with_layout(self):\n \n     out, = serialization.run_deserialization([out_layout], tspecs)\n \n-    self.assertEqual(out.layout, out_layout)\n+    self.assertEqual(out.format, out_layout)\n     self.assertIsInstance(out, array.ArrayImpl)\n     self.assertArraysEqual(out, np_inp)\n     for s in out.addressable_shards:\ndiff --git a/tests/layout_test.py b/tests/layout_test.py\nindex cfec2253dfc8..d7b23c75b313 100644\n--- a/tests/layout_test.py\n+++ b/tests/layout_test.py\n@@ -77,21 +77,21 @@ def init(x, y):\n       init_compiled(arr1, arr2)\n     self.assertEqual(init_count(), 1)\n \n-    self.assertEqual(init_out[0].layout, init_compiled.output_layouts[0])\n-    self.assertEqual(init_out[1].layout, init_compiled.output_layouts[1])\n+    self.assertEqual(init_out[0].format, init_compiled.output_layouts[0])\n+    self.assertEqual(init_out[1].format, init_compiled.output_layouts[1])\n \n     with jtu.count_aot_jit_cpp_cache_miss() as apply_count:\n       apply_out = compiled_apply(*init_out)\n       compiled_apply(*init_out)\n     self.assertEqual(apply_count(), 1)\n \n-    self.assertEqual(apply_out[0].layout, compiled_apply.output_layouts[0])\n-    self.assertEqual(apply_out[1].layout, compiled_apply.output_layouts[1])\n+    self.assertEqual(apply_out[0].format, compiled_apply.output_layouts[0])\n+    self.assertEqual(apply_out[1].format, compiled_apply.output_layouts[1])\n \n-    self.assertTupleEqual(apply_out[0].layout.device_local_layout.major_to_minor,\n-                          init_out[0].layout.device_local_layout.major_to_minor[::-1])\n-    self.assertTupleEqual(apply_out[1].layout.device_local_layout.major_to_minor,\n-                          init_out[1].layout.device_local_layout.major_to_minor[::-1])\n+    self.assertTupleEqual(apply_out[0].format.device_local_layout.major_to_minor,\n+                          init_out[0].format.device_local_layout.major_to_minor[::-1])\n+    self.assertTupleEqual(apply_out[1].format.device_local_layout.major_to_minor,\n+                          init_out[1].format.device_local_layout.major_to_minor[::-1])\n \n     self.assertArraysEqual(init_out[0], np_inp1 * 2)\n     self.assertArraysEqual(init_out[1], np_inp2 * 2)\n@@ -157,7 +157,7 @@ def f(x):\n \n     out = compiled(arr)\n     self.assertArraysEqual(out, np_inp.T)\n-    self.assertEqual(out.layout, compiled.output_layouts)\n+    self.assertEqual(out.format, compiled.output_layouts)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('y', 'x')))\n \n   def test_sharding_and_layouts(self):\n@@ -277,11 +277,11 @@ def test_device_put_concrete_layout(self):\n     col = compiled.output_layouts\n \n     out = jax.device_put(np_inp, col)\n-    self.assertEqual(out.layout, col)\n+    self.assertEqual(out.format, col)\n     self.assertArraysEqual(out, np_inp)\n     for s in out.addressable_shards:\n-      self.assertEqual(out.layout.device_local_layout,\n-                       s.data.layout.device_local_layout)\n+      self.assertEqual(out.format.device_local_layout,\n+                       s.data.format.device_local_layout)\n \n   def test_device_put_non_concrete_layout_error(self):\n     np_inp = np.arange(16).reshape(8, 2)\n@@ -338,7 +338,7 @@ def test_make_array_from_callback(self):\n     out = jax.make_array_from_callback(np_inp.shape, layout,\n                                        lambda idx: np_inp[idx])\n     self.assertArraysEqual(out, np_inp)\n-    self.assertEqual(out.layout, layout)\n+    self.assertEqual(out.format, layout)\n \n     with self.assertRaisesRegex(\n         TypeError,\n@@ -370,9 +370,9 @@ def f(x):\n       return jax.lax.with_sharding_constraint(y, Format(custom_dll, s))\n \n     out = f(arr)\n-    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(out.format.device_local_layout.major_to_minor,\n                      custom_dll.major_to_minor)\n-    self.assertEqual(out.layout, arr.layout)\n+    self.assertEqual(out.format, arr.format)\n     self.assertArraysEqual(out, np_inp.T)\n \n   def test_wsc_bfloat16_concrete_layout(self):\n@@ -393,9 +393,9 @@ def f(x):\n       return jax.lax.with_sharding_constraint(y, Format(custom_dll, s))\n \n     out = f(arr)\n-    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(out.format.device_local_layout.major_to_minor,\n                      custom_dll.major_to_minor)\n-    self.assertEqual(out.layout, arr.layout)\n+    self.assertEqual(out.format, arr.format)\n     self.assertArraysEqual(out, inp.T)\n \n   def test_device_put_user_concrete_layout(self):\n@@ -405,7 +405,7 @@ def test_device_put_user_concrete_layout(self):\n     s = SingleDeviceSharding(jax.devices()[0])\n \n     out = jax.device_put(np_inp, Format(dll, s))\n-    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(out.format.device_local_layout.major_to_minor,\n                      dll.major_to_minor)\n     self.assertArraysEqual(out, np_inp)\n \n@@ -427,7 +427,7 @@ def test_device_put_user_concrete_layout_multi_device(self):\n \n     for o in [out1, out2, out3, out4]:\n       self.assertArraysEqual(o, np_inp)\n-      self.assertEqual(o.layout.device_local_layout.major_to_minor,\n+      self.assertEqual(o.format.device_local_layout.major_to_minor,\n                        custom_layout.device_local_layout.major_to_minor)\n \n   def test_concrete_layout_jit(self):\n@@ -445,7 +445,7 @@ def f(x):\n \n     out = f(arr)\n     self.assertArraysEqual(out, np_inp.T)\n-    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(out.format.device_local_layout.major_to_minor,\n                      custom_dll.major_to_minor)\n \n   def test_compatible_aval_error(self):\n@@ -489,7 +489,7 @@ def f(x):\n \n     out = f(arr)\n     self.assertArraysEqual(out, np_inp.T)\n-    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(out.format.device_local_layout.major_to_minor,\n                      custom_dll.major_to_minor[::-1])\n \n     custom_dll2 = DLL(major_to_minor=(1, 0))\n@@ -709,7 +709,7 @@ def test_cpp_layout_cache_miss(self):\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     arr = jax.device_put(np_inp, s)\n \n-    arr_m2m = arr.layout.device_local_layout.major_to_minor\n+    arr_m2m = arr.format.device_local_layout.major_to_minor\n     custom_layout = Format(DLL(major_to_minor=arr_m2m[::-1]), s)\n     arr2 = jax.device_put(np_inp, custom_layout)\n \n@@ -731,7 +731,7 @@ def test_layout_donation_with_default_layout(self):\n     shape = (16, 16)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     arr = jax.device_put(np_inp, s)\n-    out_layout = Format(arr.layout.device_local_layout, s)\n+    out_layout = Format(arr.format.device_local_layout, s)\n \n     @partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\n     def f(x):\n@@ -743,7 +743,7 @@ def f(x):\n \n     out = f(arr)\n     self.assertArraysEqual(out, np_inp * 2)\n-    self.assertEqual(out.layout, out_layout)\n+    self.assertEqual(out.format, out_layout)\n \n   def test_with_layout_constraint(self):\n     if not jtu.test_device_matches(['tpu']):\n@@ -755,7 +755,7 @@ def test_with_layout_constraint(self):\n     arr = jax.device_put(np_inp, s)\n \n     # Create a custom layout instead of using `arr.layout` to test the API.\n-    custom_dll = DLL(major_to_minor=arr.layout.dll.major_to_minor[::-1])\n+    custom_dll = DLL(major_to_minor=arr.format.dll.major_to_minor[::-1])\n \n     def f(x):\n       y = x.T\n@@ -768,7 +768,7 @@ def f(x):\n \n     f = jax.jit(f)\n     out = f(arr)\n-    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(out.format.device_local_layout.major_to_minor,\n                      custom_dll.major_to_minor)\n     self.assertArraysEqual(out, np_inp.T * 2)\n \ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 751fd63823e3..dd5c5d46e62f 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -5001,7 +5001,7 @@ def test_sds_update(self):\n     new_layout = Format(DLL((1, 0)), NamedSharding(mesh, P('x')))\n     s4_u = s4.update(sharding=new_layout)\n     self.assertEqual(s4_u.sharding, new_layout.sharding)\n-    self.assertEqual(s4_u.layout, new_layout)\n+    self.assertEqual(s4_u.format, new_layout)\n \n     with self.assertRaisesRegex(ValueError, \"updating ShapeDtypeStruct\"):\n       s4.update(sharding=NamedSharding(mesh, P('x')))\n\n```",
        "from_id": [
            "froystig",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Make 1D tiling agnostic to large 2nd minor flags.\n\nPiperOrigin-RevId: 764937293",
        "output": "```diff\nCommit: 663e50f72cc6cf420e78f865837f82112d2425b6\nDate: 2025-05-29T23:47:36Z\nURL: https://github.com/jax-ml/jax/commit/663e50f72cc6cf420e78f865837f82112d2425b6\nFiles changed: 1\nAdditions: +12, Deletions: -6\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_memref_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_memref_layout.cc\nindex bfb9be87dfd0..b772c5c8a114 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_memref_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_memref_layout.cc\n@@ -58,10 +58,12 @@ namespace mlir::tpu {\n //     enabled by XLA for memrefs.\n //   bitwidth: The bitwidth of the element type of the operand.\n //   is_kernel_argument: Whether the operand is a kernel argument.\n+//   is_1d: Whether the operand is 1D.\n int getTilingFactor(const int src_sublane, const int hardware_generation,\n                     const int64_t target_sublane_count,\n                     const TpuTilingFlags &tpu_tiling_flags,\n-                    const int8_t bitwidth, const bool is_kernel_argument) {\n+                    const int8_t bitwidth, const bool is_kernel_argument,\n+                    const bool is_1d) {\n   CHECK(llvm::isPowerOf2_32(bitwidth));\n   CHECK_LE(2, bitwidth);\n   CHECK_LE(bitwidth, 32);\n@@ -76,6 +78,10 @@ int getTilingFactor(const int src_sublane, const int hardware_generation,\n   const int max_normal_tiling = tiling_sublane;\n \n   int large_tiling = [&] {\n+    if (is_1d) {\n+      // 1D tiling is always compact.\n+      return tiling_sublane;\n+    }\n     if (bitwidth == 2) {\n       return target_sublane_count * 16;\n     }\n@@ -151,9 +157,9 @@ FailureOr<TiledLayoutAttr> inferLayout(MemRefType memref_ty,\n       auto src_sublane =\n           llvm::divideCeil(memref_ty.getShape().back(), lane_count);\n       const int64_t leading_tile =\n-          getTilingFactor(src_sublane, hardware_generation,\n-                          sublane_count, tpu_tiling_flags, bitwidth,\n-                          is_kernel_argument) *\n+          getTilingFactor(src_sublane, hardware_generation, sublane_count,\n+                          tpu_tiling_flags, bitwidth, is_kernel_argument,\n+                          /*is_1d=*/true) *\n           lane_count;\n       SmallVector<xla::Tile> tiles{xla::Tile({leading_tile})};\n       if (bitwidth != 32) {\n@@ -173,8 +179,8 @@ FailureOr<TiledLayoutAttr> inferLayout(MemRefType memref_ty,\n     const int64_t src_sublane = shape[shape.size() - 2];\n     if (leading_tile_rows == 0) {\n       leading_tile_rows = getTilingFactor(\n-          src_sublane, hardware_generation, sublane_count,\n-          tpu_tiling_flags, bitwidth, is_kernel_argument);\n+          src_sublane, hardware_generation, sublane_count, tpu_tiling_flags,\n+          bitwidth, is_kernel_argument, /*is_1d=*/false);\n     }\n     SmallVector<xla::Tile> tiles{xla::Tile({leading_tile_rows, lane_count})};\n     if (bitwidth != 32) {\n\n```",
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29101 from mattjj:hijax\n\nPiperOrigin-RevId: 764930280",
        "output": "```diff\nCommit: da106b971af143906c8cbec8f4f2dfb7b57a11ce\nDate: 2025-05-29T23:27:42Z\nURL: https://github.com/jax-ml/jax/commit/da106b971af143906c8cbec8f4f2dfb7b57a11ce\nFiles changed: 8\nAdditions: +665, Deletions: -200\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex b20b85a43b6e..24150aba6584 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,7 +88,8 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info', '_is_high', '_final_typechange_env']\n+               '_effects', '_debug_info', '_is_high',\n+               '_initial_typechange_env', '_final_typechange_env']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -97,6 +98,7 @@ class Jaxpr:\n   _effects: Effects\n   _debug_info: DebugInfo\n   _is_high: bool\n+  _initial_typechange_env: dict[Var, Any]\n   _final_typechange_env: dict[Var, Any]\n \n   @property\n@@ -127,6 +129,10 @@ def debug_info(self) -> DebugInfo:\n   def is_high(self) -> bool:\n     return self._is_high\n \n+  @property\n+  def initial_typechange_env(self) -> dict[Var, Any]:\n+    return self._initial_typechange_env\n+\n   @property\n   def final_typechange_env(self) -> dict[Var, Any]:\n     return self._final_typechange_env\n@@ -139,6 +145,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n                is_high: bool = False,\n+               initial_typechange_env: dict | None = None,\n                final_typechange_env: dict | None = None,\n                ):\n     \"\"\"\n@@ -165,6 +172,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n     self._is_high = is_high\n+    self._initial_typechange_env = initial_typechange_env or {}\n     self._final_typechange_env = final_typechange_env or {}\n \n   def __str__(self):\n@@ -193,6 +201,8 @@ def replace(self, **kwargs):\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n         is_high=kwargs.pop(\"is_high\", self.is_high),\n+        initial_typechange_env=kwargs.pop(\"initial_typechange_env\",\n+                                          self.initial_typechange_env),\n         final_typechange_env=kwargs.pop(\"final_typechange_env\",\n                                         self.final_typechange_env),\n     )\n@@ -222,6 +232,22 @@ def subjaxprs(jaxpr: Jaxpr) -> Iterator[Jaxpr]:\n     yield from jaxprs_in_params(eqn.params)\n \n \n+@dataclass(frozen=True)\n+class TypeChange:\n+  aval: AbstractValue\n+  initial_type_state: Any\n+  final_type_state: Any\n+\n+  def to_tangent_aval(self):\n+    return TypeChange(self.aval.to_tangent_aval(),\n+                      self.initial_type_state.to_tangent_aval(),\n+                      self.final_type_state.to_tangent_aval())\n+\n+  def normalize(self):\n+    return TypeChange(self.aval.normalize(),\n+                      self.initial_type_state.normalize(),\n+                      self.final_type_state.normalize())\n+\n class ClosedJaxpr:\n   __slots__ = ['__weakref__', '_jaxpr', '_consts']\n \n@@ -241,6 +267,13 @@ def __init__(self, jaxpr: Jaxpr, consts: Sequence):\n   def in_avals(self):\n     return [v.aval for v in self.jaxpr.invars]\n \n+  @property\n+  def in_avals_aug(self):\n+    ienv = self.jaxpr.initial_typechange_env\n+    fenv = self.jaxpr.final_typechange_env\n+    return [TypeChange(v.aval, ienv[v], fenv[v]) if v.aval.mutable else v.aval\n+            for v in self.jaxpr.invars]\n+\n   @property\n   def out_avals(self):\n     return [v.aval for v in self.jaxpr.outvars]\n@@ -542,10 +575,6 @@ def _true_bind(self, *args, **params):\n     # is called frequently and it's slightly faster to avoid using a context\n     # manager object.\n     prev_trace = trace_ctx.trace\n-\n-    if self.is_high(**params) and prev_trace.requires_low:\n-      return self.to_lojax(*args, **params)  # type: ignore\n-\n     trace_ctx.set_trace(eval_trace)\n     try:\n       return self.bind_with_trace(prev_trace, args, params)\n@@ -553,6 +582,11 @@ def _true_bind(self, *args, **params):\n       trace_ctx.set_trace(prev_trace)\n \n   def bind_with_trace(self, trace, args, params):\n+    # TODO(mattjj,dougalm): remove this block?\n+    if self.is_high(**params) and trace.requires_low:\n+      with set_current_trace(trace):\n+        return self.to_lojax(*args, **params)  # type: ignore\n+\n     return trace.process_primitive(self, args, params)\n \n   def def_impl(self, impl):\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 7cbdfff01462..0cd99a197f66 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -641,6 +641,9 @@ def to_concrete_value(self):\n   def get_referent(self):\n     return core.get_referent(self.primal)\n \n+  def type_state(self):\n+    return self.primal.type_state()\n+\n def _primal_tangent_shapes_match(primal, tangent):\n   if type(tangent) is not Zero:\n     primal_aval = get_aval(primal).strip_weak_type()\n@@ -1166,8 +1169,9 @@ def _jvp_jaxpr(jaxpr: core.ClosedJaxpr,\n                    debug_info=jaxpr.jaxpr.debug_info)\n   f_jvp, out_nonzeros = f_jvp_traceable(\n       jvp(f, instantiate=instantiate, transform_stack=False), nonzeros)\n-  tangent_avals = [aval.to_tangent_aval() for aval, nz in zip(jaxpr.in_avals, nonzeros) if nz]\n-  avals_in = list(it.chain(jaxpr.in_avals, tangent_avals))\n+  tangent_avals = [aval.to_tangent_aval()\n+                   for aval, nz in zip(jaxpr.in_avals_aug, nonzeros) if nz]\n+  avals_in = list(it.chain(jaxpr.in_avals_aug, tangent_avals))\n   jaxpr_out, avals_out, literals_out, () = pe.trace_to_jaxpr_dynamic(\n       f_jvp, avals_in)\n   return core.ClosedJaxpr(jaxpr_out, literals_out), out_nonzeros()\n@@ -1189,14 +1193,12 @@ def rearrange_binders(jaxpr: core.ClosedJaxpr, primals_in, tangents_in, primals_\n   new_invars = _perm(primals_in, tangents_in, jaxpr.jaxpr.invars)\n   new_outvars = _perm(primals_out, tangents_out, jaxpr.jaxpr.outvars)\n   new_debug_info = jaxpr.jaxpr.debug_info\n-  new_arg_names = tuple(_perm(primals_in, tangents_in,\n-                              jaxpr.jaxpr.debug_info.safe_arg_names(len(jaxpr.jaxpr.invars))))\n-  new_result_paths = tuple(_perm(primals_out, tangents_out,\n-                                  jaxpr.jaxpr.debug_info.safe_result_paths(len(jaxpr.jaxpr.outvars))))\n+  arg_names = jaxpr.jaxpr.debug_info.safe_arg_names(len(jaxpr.in_avals))\n+  result_paths = jaxpr.jaxpr.debug_info.safe_result_paths(len(jaxpr.out_avals))\n+  new_arg_names = tuple(_perm(primals_in, tangents_in, arg_names))\n+  new_result_paths = tuple(_perm(primals_out, tangents_out, result_paths))\n   new_debug_info = new_debug_info._replace(\n-      arg_names=new_arg_names,\n-      result_paths=new_result_paths,\n-  )\n+      arg_names=new_arg_names, result_paths=new_result_paths)\n   constvars = jaxpr.jaxpr.constvars\n   new_effects = pe._renumber_effects(\n       (*constvars, *new_invars), (*constvars, *jaxpr.jaxpr.invars),\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 3c499429a663..444b60f15fa5 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -896,10 +896,8 @@ def convert_constvars_jaxpr(jaxpr: Jaxpr) -> Jaxpr:\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   dbg = jaxpr.debug_info._replace(\n       arg_names=(\"\",) * len(jaxpr.constvars) + jaxpr.debug_info.arg_names)\n-  lifted_jaxpr = Jaxpr(constvars=(),\n-                       invars=jaxpr.constvars + jaxpr.invars,\n-                       outvars=jaxpr.outvars, eqns=jaxpr.eqns,\n-                       effects=jaxpr.effects, debug_info=dbg)\n+  lifted_jaxpr = jaxpr.replace(\n+      constvars=(), invars=jaxpr.constvars + jaxpr.invars, debug_info=dbg)\n   config.enable_checks.value and core.check_jaxpr(lifted_jaxpr)\n   return lifted_jaxpr\n \n@@ -1014,10 +1012,9 @@ def fun(*known_vals_in):\n     known_vals_out = [pval.get_known() for pval in out_pvals if pval.is_known()]\n     return [*known_vals_out, *residuals]\n \n-  known_avals = [a for a, uk in zip(jaxpr.in_avals, in_unknowns) if not uk]\n+  known_avals = [a for a, uk in zip(jaxpr.in_avals_aug, in_unknowns) if not uk]\n   jaxpr_known, _, consts_known, () = trace_to_jaxpr_dynamic(\n-      lu.wrap_init(fun, debug_info=f.debug_info),\n-      known_avals)\n+      lu.wrap_init(fun, debug_info=f.debug_info), known_avals)\n   (out_unknowns, jaxpr_unknown, res_avals), = cell  # pytype: disable=bad-unpacking\n \n   # check jaxpr_known and jaxpr_unknown in isolation\n@@ -1579,6 +1576,20 @@ def dce_jaxpr_closed_call_rule(used_outputs: list[bool], eqn: JaxprEqn\n def close_jaxpr(jaxpr: Jaxpr) -> ClosedJaxpr:\n   return ClosedJaxpr(jaxpr, ())\n \n+def move_invars_right(jaxpr: ClosedJaxpr, to_move: Sequence[bool]):\n+  return _move_invars_right(jaxpr, tuple(to_move))\n+\n+@weakref_lru_cache\n+def _move_invars_right(jaxpr: ClosedJaxpr, to_move: tuple[bool, ...]):\n+  invars, rest = split_list(jaxpr.jaxpr.invars, [len(to_move)])\n+  left_invars, right_invars = partition_list(to_move, invars)\n+  new_invars = [*left_invars, *right_invars, *rest]\n+  new_effs = _renumber_effects(\n+      (*jaxpr.jaxpr.constvars, *new_invars),\n+      (*jaxpr.jaxpr.constvars, *jaxpr.jaxpr.invars),\n+      jaxpr.jaxpr.effects)\n+  return jaxpr.replace(jaxpr=jaxpr.jaxpr.replace(invars=new_invars, effects=new_effs))\n+\n def move_binders_to_front(closed_jaxpr: ClosedJaxpr, to_move: Sequence[bool]\n                           ) -> ClosedJaxpr:\n   \"\"\"Reorder `invars` by moving those indicated in `to_move` to the front.\"\"\"\n@@ -1640,6 +1651,10 @@ def full_lower(self):\n     if val is None: return self\n     return core.full_lower(val)\n \n+  def type_state(self):\n+    var = self._trace.frame.tracer_to_var.get(id(self))\n+    return self._trace.frame.current_typechange_env[var]\n+\n   def _contents(self):\n     return ()\n \n@@ -1735,7 +1750,8 @@ class JaxprStackFrame:\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n   is_high: bool\n-  final_typechange_env: dict\n+  initial_typechange_env: dict\n+  current_typechange_env: dict\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1751,7 +1767,8 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_vars = []\n     self.debug_info = debug_info\n     self.is_high = False\n-    self.final_typechange_env = {}\n+    self.initial_typechange_env = {}\n+    self.current_typechange_env = {}\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1777,8 +1794,11 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n+    final_typechange_env = {v: s for v, s in self.current_typechange_env.items()\n+                            if v in self.initial_typechange_env}\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info, self.is_high, self.final_typechange_env)\n+                  debug_info, self.is_high, self.initial_typechange_env,\n+                  final_typechange_env)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1895,8 +1915,6 @@ def new_arg(self, aval, source_info: SourceInfo):\n     self.frame.tracers.append(tracer)\n     self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n     self.frame.invars.append(var)\n-    if aval.mutable:\n-      self.frame.final_typechange_env[var] = aval\n     return tracer\n \n   def new_const(self, c, source_info: SourceInfo):\n@@ -1921,6 +1939,8 @@ def _new_const(self, aval, c, source_info: SourceInfo) -> DynamicJaxprTracer:\n       self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n       self.frame.constid_to_tracer[id(c)] = tracer\n       self.frame.constvar_to_val[var] = c\n+      if aval.mutable:\n+        self.frame.initial_typechange_env[var] = c.type_state()\n     return tracer\n \n   def get_const(self, tracer) -> Any:\n@@ -2235,18 +2255,24 @@ def trace_to_jaxpr_dynamic(\n            list[tuple[PyTreeDef, PyTreeDef, tuple[Any, str, AttrKind]]]]:\n   keep_inputs = [True] * len(in_avals) if keep_inputs is None else keep_inputs\n   trace = DynamicJaxprTrace(fun.debug_info, lower=lower)\n+  in_avals_ = [a.aval if isinstance(a, core.TypeChange) else a for a in in_avals]\n   with core.ensure_no_leaks(trace), source_info_util.reset_name_stack():\n     source_info = source_info_util.current()\n     in_tracers = _input_type_to_tracers(\n-        partial(trace.new_arg, source_info=source_info), in_avals)\n+        partial(trace.new_arg, source_info=source_info), in_avals_)\n     in_tracers = [t for t, keep in zip(in_tracers, keep_inputs) if keep]\n+    trace.frame.initial_typechange_env = initial_typechange_env = {\n+        v: a.initial_type_state for v, a in zip(trace.frame.invars, in_avals)\n+        if isinstance(a, core.TypeChange)}\n+    trace.frame.current_typechange_env = dict(initial_typechange_env)\n+\n     try:\n       with core.set_current_trace(trace):\n         ans = fun.call_wrapped(*in_tracers)\n       _check_returned_jaxtypes(fun.debug_info, ans)\n       out_tracers = map(partial(trace.to_jaxpr_tracer, source_info=source_info), ans)\n       _check_no_returned_refs(fun.debug_info, out_tracers)\n-      jaxpr, consts, attrs_tracked = trace.to_jaxpr(out_tracers, fun.debug_info)\n+      jaxpr, consts, attrs_tracked = trace.frame.to_jaxpr(trace, out_tracers, fun.debug_info)\n       del fun, in_tracers, out_tracers, ans\n     finally:\n       trace.frame.reset_states(trace)\n@@ -2718,21 +2744,38 @@ def _linearize_of_pmap_hack(f: lu.WrappedFun, jaxpr, consts) -> tuple[Jaxpr, lis\n \n @weakref_lru_cache\n def lower_jaxpr(hi_jaxpr):\n-  in_avals = [lo_ty for t in hi_jaxpr.in_avals for lo_ty in t.lo_ty()]\n+  initial_env = hi_jaxpr.jaxpr.initial_typechange_env\n+  lo_avals = [lo_ty for v in hi_jaxpr.jaxpr.invars\n+              for lo_ty in (v.aval.lo_ty_(initial_env[v]) if v.aval.mutable\n+                            else v.aval.lo_ty())]\n   f = lu.wrap_init(partial(lower_traceable, hi_jaxpr),\n                    debug_info=hi_jaxpr.jaxpr.debug_info)\n-  lo_jaxpr, _, consts, () = trace_to_jaxpr_dynamic(f, in_avals, lower=True)\n-  return core.ClosedJaxpr(lo_jaxpr, consts)\n+  lo_jaxpr, _, lo_consts, () = trace_to_jaxpr_dynamic(f, lo_avals, lower=True)\n+  return core.ClosedJaxpr(lo_jaxpr, lo_consts)\n \n def lower_traceable(jaxpr, *lo_args):\n+  env = jaxpr.jaxpr.initial_typechange_env\n   lo_args_ = iter(lo_args)\n-  hi_args = [t.raise_val(*it.islice(lo_args_, len(t.lo_ty())))\n-             for t in jaxpr.in_avals]\n+  hi_args = [v.aval.raise_val(*it.islice(lo_args_, len(v.aval.lo_ty())))\n+             if not v.aval.mutable else\n+             v.aval.new_from_loval(env[v], *it.islice(lo_args_, len(v.aval.lo_ty_(env[v]))))\n+             for v in jaxpr.jaxpr.invars]\n   assert (problem := next(lo_args_, None)) is None\n   hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n   mut_outs = [lo_val for v, ty in jaxpr.jaxpr.final_typechange_env.items()\n-              for lo_val in ty.get(hi_args[in_idx[v]])]\n-  lo_outs = [lo_val for t, hi_val in zip(jaxpr.out_avals, hi_outs)\n-             for lo_val in t.lower_val(hi_val)]\n+              for lo_val in v.aval.read_loval(ty, hi_args[in_idx[v]])]\n+  lo_outs = [lo_val for v, hi_val in zip(jaxpr.jaxpr.outvars, hi_outs)\n+             for lo_val in v.aval.lower_val(hi_val)]\n   return mut_outs + lo_outs\n+\n+def convert_const_himutables(jaxpr):\n+  move = [core.typeof(c).mutable for c in jaxpr.consts]\n+  constvals, in_mutables = partition_list(move, jaxpr.consts)\n+  constvars, boxvars = partition_list(move, jaxpr.jaxpr.constvars)\n+  invars = *boxvars, *jaxpr.jaxpr.invars\n+  effects = make_jaxpr_effects(constvars, invars, jaxpr.jaxpr.outvars,\n+                               jaxpr.jaxpr.eqns)\n+  new_jaxpr = jaxpr.jaxpr.replace(constvars=constvars, invars=invars,\n+                                  effects=effects)\n+  return jaxpr.replace(jaxpr=new_jaxpr, consts=constvals), in_mutables\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex 8ebf4133a5fd..af1f8217951c 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -1775,6 +1775,7 @@ def _move_mutable_consts(\n   constvars, mutvars = partition_list(hoist, jaxpr.constvars)\n   invars = (*jaxpr.invars, *mutvars)\n   effects = pe.make_jaxpr_effects(constvars, invars, jaxpr.outvars, jaxpr.eqns)\n+  # TODO(mattjj): debug_info must be updated...\n   jaxpr = core.Jaxpr(constvars, invars, jaxpr.outvars, jaxpr.eqns,\n                      effects, closed_jaxpr.jaxpr.debug_info)\n   return core.ClosedJaxpr(jaxpr, consts), in_mut\n@@ -2181,8 +2182,7 @@ def lower_sharding_computation(\n   The caller of this code can pass in a singleton UNSPECIFIED because the\n   number of out_avals might not be known at that time and\n   lower_sharding_computation calculates the number of out_avals so it can apply\n-  the singleton UNSPECIFIED to all out_avals.\n-  \"\"\"\n+  the singleton UNSPECIFIED to all out_avals.\"\"\"\n   auto_spmd_lowering = check_if_any_auto(\n       it.chain.from_iterable([in_shardings, out_shardings]))\n \ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex b9ce8ae09380..4df4c517090b 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -17,7 +17,7 @@\n from collections.abc import Callable, Sequence\n from functools import partial\n import inspect\n-import itertools\n+import itertools as it\n import operator\n from typing import Any, TypeVar\n import weakref\n@@ -438,11 +438,11 @@ def _merge_attrs_out(attrs_tracked, out_state, out_append):\n   out_attrs = []\n   for _, out_tree, (_, _, k) in attrs_tracked:\n     if k in (pe.ReadWrite, pe.BoxAttr):\n-      out_attrs.extend(itertools.islice(out_state_, out_tree.num_leaves))\n+      out_attrs.extend(it.islice(out_state_, out_tree.num_leaves))\n     elif k is pe.Append:\n       out_attrs.append(next(out_append_))\n     elif k is pe.ListAttr:\n-      out_attrs.extend(itertools.islice(out_append_, out_tree.num_leaves))\n+      out_attrs.extend(it.islice(out_append_, out_tree.num_leaves))\n     else:\n       assert False\n   assert next(out_state_, None) is next(out_append_, None) is None\n@@ -931,7 +931,7 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n   ys_avals = [core.unmapped_aval(length, 0, y_aval)\n               for y_aval in y_avals]\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(a), None)\n-                 for a in itertools.chain(carry_avals, ys_avals)]\n+                 for a in it.chain(carry_avals, ys_avals)]\n   del carry_avals, y_avals\n   # Create equation.\n   linear_unknown = tuple([False] * len(intensive_res) +\n@@ -1500,6 +1500,17 @@ def arrange_jaxpr_args_for_wrapped(args):\n   assert len(refs_out_matching_in_avals) == len(in_avals)\n   return refs_out_matching_in_avals, [*carry_out, *ys]\n \n+def _scan_staging(trace, *args, **params):\n+  outs = trace.default_process_primitive(scan_p, args, params)\n+  jaxpr = params['jaxpr']\n+  trace.frame.is_high = jaxpr.jaxpr.is_high\n+  invars = [trace.frame.tracer_to_var[id(t)] for t in args]\n+  var_map = dict(zip(jaxpr.jaxpr.invars, invars))\n+  final_env = {var_map[v]: ty for v, ty in\n+               jaxpr.jaxpr.final_typechange_env.items()}\n+  trace.frame.current_typechange_env.update(final_env)\n+  return outs\n+\n scan_p = core.Primitive(\"scan\")\n scan_p.multiple_results = True\n scan_p.skip_canonicalization = True\n@@ -1518,6 +1529,65 @@ def arrange_jaxpr_args_for_wrapped(args):\n pe.padding_rules[scan_p] = _scan_padding_rule\n pe.dce_rules[scan_p] = _scan_dce_rule\n state_discharge.register_partial_discharge_rule(scan_p)(_scan_state_partial_discharge_rule)\n+pe.custom_staging_rules[scan_p] = _scan_staging\n+\n+def _is_high(jaxpr, **_) -> bool:\n+  return jaxpr.jaxpr.is_high\n+scan_p.is_high = _is_high  # type: ignore\n+\n+def _to_lojax(*hi_args, jaxpr, num_carry, num_consts, linear, **params):\n+  ienv, fenv = jaxpr.jaxpr.initial_typechange_env, jaxpr.jaxpr.final_typechange_env\n+\n+  # move box binders and hi_args from consts slots to carry slots\n+  to_move = [t.mutable for t in jaxpr.in_avals[:num_consts]]\n+  jaxpr = pe.move_invars_right(jaxpr, to_move)\n+  hi_args = _move_right(hi_args, to_move)\n+  num_consts -= sum(to_move)\n+  num_carry += sum(to_move)\n+\n+  # expand num_consts, num_carry, linear according to lo types\n+  const_invars, carry_invars, _ = split_list(jaxpr.jaxpr.invars, [num_consts, num_carry])\n+  num_consts = sum(len(v.aval.lo_ty() if not v.aval.mutable\n+                       else v.aval.lo_ty_(ienv[v])) for v in const_invars)\n+  num_carry = sum(len(v.aval.lo_ty() if not v.aval.mutable\n+                      else v.aval.lo_ty_(ienv[v])) for v in carry_invars)\n+  linear = [l for v, l_ in zip(jaxpr.jaxpr.invars, linear)\n+            for l in (l_,) * len(v.aval.lo_ty() if not v.aval.mutable\n+                                 else v.aval.lo_ty_(ienv[v]))]\n+  lo_muts_out = sum(len(m.leaf_avals) for m in fenv.values())  # TODO hardcoded\n+\n+  # collect lo inputs values\n+  lo_args = [lo_val for v, x in zip(jaxpr.jaxpr.invars, hi_args)\n+             for lo_val in (v.aval.read_loval(ienv[v], x) if v.aval.mutable\n+                            else v.aval.lower_val(x))]\n+\n+  # lower the jaxpr and bind it using lo input values\n+  lo_jaxpr = pe.lower_jaxpr(jaxpr)\n+  all_outs = scan_p.bind(*lo_args, jaxpr=lo_jaxpr, num_consts=num_consts,\n+                         num_carry=num_carry, linear=tuple(linear), **params)\n+  out_mut, lo_outs = split_list(all_outs, [lo_muts_out])\n+\n+  # collect and apply mutations\n+  out_mut_ = iter(out_mut)\n+  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n+  for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n+    lo_vals = it.islice(out_mut_, len(var.aval.lo_ty_(ty)))\n+    var.aval.update_from_loval(ty, hi_args[in_idx[var]], *lo_vals)\n+  assert next(out_mut_, None) is None\n+\n+  # collect output values into hi types\n+  lo_outs_ = iter(lo_outs)\n+  hi_outs = [t.raise_val(*it.islice(lo_outs_, len(t.lo_ty())))\n+             for t in jaxpr.out_avals]\n+  assert next(lo_outs_, None) is None\n+\n+  return hi_outs\n+scan_p.to_lojax = _to_lojax\n+\n+def _move_right(lst, to_move):\n+  lst, rest = split_list(lst, [len(to_move)])\n+  left, right = partition_list(to_move, lst)\n+  return [*left, *right, *rest]\n \n def _propagate_mem_kind_scan(*xm, reverse, length, num_consts, num_carry, jaxpr,\n                              linear, unroll, _split_transpose):\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 94a754a1a597..f012459296a1 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -590,6 +590,8 @@ def _infer_params_impl(\n     in_type = in_avals = tuple(core.shaped_abstractify(x) for x in explicit_args)  # type: ignore\n   else:\n     in_type = in_avals  # type: ignore\n+    in_type = tuple(core.TypeChange(a, x.type_state(), None) if a.mutable  # type: ignore\n+                    else a for a, x in zip(in_type, explicit_args))\n   assert in_avals is not None\n \n   in_shardings_flat, in_layouts_flat = _process_in_axis_resources(\n@@ -705,7 +707,7 @@ def _infer_params_internal(\n   if entry.pjit_params is None:\n     p, args_flat = _infer_params_impl(\n         fun, ji, ctx_mesh, dbg, args, kwargs, in_avals=avals)\n-    if p.attrs_tracked or p.box_data:  # if attrs/boxes, don't populate cache\n+    if p.attrs_tracked or p.box_data or p.params['jaxpr'].jaxpr.is_high:\n       return p, p.consts + args_flat\n     entry.pjit_params = p\n   return entry.pjit_params, entry.pjit_params.consts + dynargs\n@@ -1407,16 +1409,14 @@ def _create_pjit_jaxpr(\n           lu.annotate(fun, cast(core.InputType, in_type)))\n       attrs_tracked = []\n     else:\n-      jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n-          fun, in_type)\n-      # assert attr_data is sentinel or attr_data matches attrs_tracked\n+      jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(fun, in_type)\n \n   if config.debug_key_reuse.value:\n     # Import here to avoid circular imports\n     from jax.experimental.key_reuse._core import check_key_reuse_jaxpr\n     check_key_reuse_jaxpr(jaxpr)\n \n-  if any(isinstance(c, core.Tracer) for c in consts):\n+  if any(isinstance(c, core.Tracer) or core.typeof(c).mutable for c in consts):\n     closed_jaxpr = pe.close_jaxpr(pe.convert_constvars_jaxpr(jaxpr))\n     final_consts = consts\n   else:\n@@ -1561,21 +1561,41 @@ def _is_high(jaxpr, **_) -> bool:\n   return jaxpr.jaxpr.is_high\n pjit_p.is_high = _is_high  # type: ignore\n \n-def _to_lojax( *hi_args, jaxpr, **params):\n-  params, num_mutants = _lojax_expand_params(jaxpr, **params)\n+def _to_lojax(*hi_args, jaxpr, **params):\n+  ienv, fenv = jaxpr.jaxpr.initial_typechange_env, jaxpr.jaxpr.final_typechange_env\n \n-  lo_args = [lo_val for t, hi_val in zip(jaxpr.in_avals, hi_args)\n-             for lo_val in t.lower_val(hi_val)]\n+  # convert closed-over boxes to explicit args\n+  jaxpr, closed_over_himutables = pe.convert_const_himutables(jaxpr)\n+  hi_args = [*closed_over_himutables, *hi_args]\n+  params = _converted_mutables_add_params(len(closed_over_himutables), **params)\n+\n+  # expand pjit params that must match number of lo inputs/outputs\n+  lo_nums_in = [len(v.aval.lo_ty() if not v.aval.mutable\n+                    else v.aval.lo_ty_(ienv[v]))\n+                for v in jaxpr.jaxpr.invars]\n+  lo_nums_out = [len(t.lo_ty()) for t in jaxpr.out_avals]\n+  lo_muts_out = sum(len(m.leaf_avals) for m in fenv.values())  # TODO hardcoded\n+  params = _lojax_expand_params(lo_nums_in, lo_nums_out, lo_muts_out, **params)\n+\n+  # collect lo input values\n+  lo_args = [lo_val for v, x in zip(jaxpr.jaxpr.invars, hi_args)\n+             for lo_val in (v.aval.read_loval(ienv[v], x) if v.aval.mutable\n+                            else v.aval.lower_val(x))]\n+\n+  # lower the jaxpr and bind it using lo input values\n   lo_jaxpr = pe.lower_jaxpr(jaxpr)\n   all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, **params)\n-  out_mut, lo_outs = split_list(all_outs, [num_mutants])\n+  out_mut, lo_outs = split_list(all_outs, [lo_muts_out])\n \n+  # collect and apply mutations\n   out_mut_ = iter(out_mut)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n   for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n-    ty.set(hi_args[in_idx[var]], *it.islice(out_mut_, len(ty.lo_ty())))\n+    lo_vals = it.islice(out_mut_, len(var.aval.lo_ty_(ty)))\n+    var.aval.update_from_loval(ty, hi_args[in_idx[var]], *lo_vals)\n   assert next(out_mut_, None) is None\n \n+  # collect output values into hi types\n   lo_outs_ = iter(lo_outs)\n   hi_outs = [t.raise_val(*it.islice(lo_outs_, len(t.lo_ty())))\n              for t in jaxpr.out_avals]\n@@ -1584,29 +1604,35 @@ def _to_lojax( *hi_args, jaxpr, **params):\n   return hi_outs\n pjit_p.to_lojax = _to_lojax\n \n+def _converted_mutables_add_params(\n+    n, *, donated_invars, in_shardings, in_layouts, **params):\n+  donated_invars = (False,) * n + donated_invars\n+  in_shardings = (UNSPECIFIED,) * n + in_shardings\n+  in_layouts = (None,) * n + in_layouts\n+  return dict(params, donated_invars=donated_invars, in_shardings=in_shardings,\n+              in_layouts=in_layouts)\n+\n def _lojax_expand_params(\n-    hi_jaxpr, *, donated_invars, in_shardings, in_layouts, out_shardings,\n-    out_layouts, **params):\n+    nums_in, nums_out, muts_out, *, donated_invars, in_shardings, in_layouts,\n+    out_shardings, out_layouts, **params):\n   # some pjit params match the length of hi_jaxpr.invars/outvars, so when\n   # lowering we must expand them to match their number of lojax types\n-  def expand(hi_tys, xs):\n-    return tuple(y for hi, x in zip(hi_tys, xs) for y in (x,) * len(hi.lo_ty()))\n-  donated_invars = expand(hi_jaxpr.in_avals , donated_invars)\n-  in_shardings   = expand(hi_jaxpr.in_avals , in_shardings  )\n-  in_layouts     = expand(hi_jaxpr.in_avals , in_layouts    )\n-  out_shardings  = expand(hi_jaxpr.out_avals, out_shardings )\n-  out_layouts    = expand(hi_jaxpr.out_avals, out_layouts   )\n+  def expand(ns, xs):\n+    return tuple(y for n, x in zip(ns, xs) for y in (x,) * n)\n+  donated_invars = expand(nums_in , donated_invars)\n+  in_shardings   = expand(nums_in , in_shardings  )\n+  in_layouts     = expand(nums_in , in_layouts    )\n+  out_shardings  = expand(nums_out, out_shardings )\n+  out_layouts    = expand(nums_out, out_layouts   )\n \n   # also, the lo_jaxpr has pure outputs corresponding to mutable hi_jaxpr types\n-  num_mutants = sum(len(hi_ty.lo_ty()) for hi_ty in\n-                    hi_jaxpr.jaxpr.final_typechange_env.values())\n-  out_shardings = (UNSPECIFIED,) * num_mutants + out_shardings\n-  out_layouts = (None,) * num_mutants + out_layouts\n+  out_shardings = (UNSPECIFIED,) * muts_out + out_shardings\n+  out_layouts = (None,) * muts_out + out_layouts\n \n   new_params = dict(params, donated_invars=donated_invars,\n                     in_shardings=in_shardings, in_layouts=in_layouts,\n                     out_shardings=out_shardings, out_layouts=out_layouts)\n-  return new_params, num_mutants\n+  return new_params\n \n \n def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n@@ -1948,6 +1974,7 @@ def pjit_staging_rule(trace, *args, **params):\n \n   jaxpr = params['jaxpr']\n   source_info = source_info_util.current()\n+  consts = []\n   if config.dynamic_shapes.value:\n     jaxpr, in_fwd, out_shardings, out_layouts = _pjit_forwarding(\n         jaxpr, params['out_shardings'], params['out_layouts'])\n@@ -1981,6 +2008,14 @@ def pjit_staging_rule(trace, *args, **params):\n         pjit_p, (*args, *consts), new_params)\n   else:\n     out_tracers = trace.default_process_primitive(pjit_p, args, params)\n+\n+  trace.frame.is_high = jaxpr.jaxpr.is_high\n+  invars = [trace.frame.tracer_to_var[id(t)] for t in it.chain(args, consts)]\n+  var_map = dict(zip(jaxpr.jaxpr.invars, invars))\n+  final_env = {var_map[v]: ty for v, ty in\n+               jaxpr.jaxpr.final_typechange_env.items()}\n+  trace.frame.current_typechange_env.update(final_env)\n+\n   return out_tracers\n pe.custom_staging_rules[pjit_p] = pjit_staging_rule\n \ndiff --git a/tests/attrs_test.py b/tests/attrs_test.py\nindex 60a3753a7ba5..90083626fb8e 100644\n--- a/tests/attrs_test.py\n+++ b/tests/attrs_test.py\n@@ -1056,7 +1056,7 @@ def f(x):\n     self.assertAllClose(box.get(), 2.0)\n \n   @parameterized.parameters([False, True])\n-  def test_grad_closrue_stop_gradient(self, jit):\n+  def test_grad_closure_stop_gradient(self, jit):\n     box = Box(0.0)\n \n     def f(x):\n@@ -1124,7 +1124,6 @@ def f(lst, x):\n       lst.append(2.0)\n       lst.append({'c': x + 3.0})\n \n-\n     tracing_ok = True\n     lst1 = List()\n     f(lst1, 0)\ndiff --git a/tests/hijax_test.py b/tests/hijax_test.py\nindex 21034d164d28..8b7d045c6c5a 100644\n--- a/tests/hijax_test.py\n+++ b/tests/hijax_test.py\n@@ -17,18 +17,19 @@\n from dataclasses import dataclass\n from functools import partial\n import itertools as it\n+from typing import Any\n import unittest\n \n-from absl.testing import absltest\n+from absl.testing import absltest, parameterized\n \n import jax\n import jax.numpy as jnp\n \n from jax._src import config\n from jax._src import core\n-from jax._src import dtypes\n from jax._src.interpreters import ad\n from jax._src.interpreters import partial_eval as pe\n+from jax._src import ad_util\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip, safe_map\n \n@@ -37,6 +38,8 @@\n map, unsafe_map = safe_map, map\n zip, unsafe_zip = safe_zip, zip\n \n+PyTreeDef = Any\n+\n \n # TODO(mattjj,dougalm): move HiPrimitive, Box, etc out of tests and into library\n class HiPrimitive(core.Primitive):\n@@ -65,124 +68,6 @@ def jvp(self, primals, tangents, **params):\n   def transpose(self, *args, **params):\n     assert False  # TODO\n \n-\n-class BoxTy(core.AbstractValue):\n-  mutable = True\n-\n-  def __init__(self, leaf_avals, treedef):\n-    self._leaf_avals = leaf_avals  # hijax avals\n-    self._treedef = treedef\n-\n-  # aval interface: hashability and str_short\n-  def __hash__(self):\n-    return hash((self._leaf_avals, self._treedef))\n-\n-  def __eq__(self, other):\n-    return (isinstance(other, BoxTy) and self._leaf_avals == other._leaf_avals\n-            and self._treedef == other._treedef)\n-\n-  def str_short(self, short_dtypes=False):\n-    return 'BoxTy'\n-\n-  # hijax interface: lower val, raise val, and low type\n-  def lo_ty(self):\n-    return [lo_aval for hi_aval in self._leaf_avals for lo_aval in hi_aval.lo_ty()]\n-\n-  def lower_val(self, box):\n-    leaf_vals, treedef = jax.tree.flatten(box._val)\n-    assert treedef == self._treedef\n-    return [lo_val for hi_aval, hi_val in zip(self._leaf_avals, leaf_vals)\n-            for lo_val in hi_aval.lower_val(hi_val)]\n-\n-  def raise_val(self, *lo_vals):\n-    lo_vals_ = iter(lo_vals)\n-    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n-               for hi_ty in self._leaf_avals]\n-    assert next(lo_vals_, None) is None\n-    return Box(jax.tree.unflatten(self._treedef, hi_vals))  # will be mutated\n-\n-  # mutable interface: get/set\n-  def get(self, box):\n-    leaf_vals, treedef = jax.tree.flatten(box._val)\n-    assert treedef == self._treedef\n-    return [lo_val for hi_ty, hi_val in zip(self._leaf_avals, leaf_vals)\n-            for lo_val in hi_ty.lower_val(hi_val)]\n-\n-  def set(self, box, *lo_vals):\n-    lo_vals_ = iter(lo_vals)\n-    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n-               for hi_ty in self._leaf_avals]\n-    assert next(lo_vals_, None) is None\n-    box._val = jax.tree.unflatten(self._treedef, hi_vals)\n-\n-  # TODO placeholder thing\n-  def to_tangent_aval(self):\n-    return core.ShapedArray((), dtypes.float0)  # TODO revise placeholder\n-\n-class Box:  # noqa: F811\n-  def __init__(self, val):\n-    self._val = val\n-\n-  @property\n-  def ty(self):\n-    leaves, treedef = jax.tree.flatten(self._val)\n-    leaf_avals = tuple(map(core.typeof, leaves))\n-    return BoxTy(leaf_avals, treedef)\n-core.pytype_aval_mappings[Box] = lambda b: b.ty\n-\n-\n-class BoxSet(HiPrimitive):\n-  multiple_results = True\n-\n-  def is_high(self, *, treedef) -> bool: return True\n-\n-  def staging(self, trace, box, *leaves, treedef):\n-    super().staging(trace, box, *leaves, treedef=treedef)\n-    avals = tuple(t.aval for t in leaves)\n-    trace.frame.final_typechange_env[trace.getvar(box)] = BoxTy(avals, treedef)\n-\n-  def abstract_eval(self, box_ty, *leaf_avals, treedef):\n-    return [], set()  # TODO better typechecking...\n-\n-  def to_lojax(_, box, *leaves, treedef):\n-    box._val = jax.tree.unflatten(treedef, leaves)\n-    return []\n-\n-  def jvp(_, primals, tangents, *, treedef):\n-    assert False  # TODO\n-\n-  def transpose(_, *args, treedef):\n-    assert False  # TODO\n-box_set_p = BoxSet('box_set')\n-\n-def box_set(box, val):\n-  leaves, treedef = jax.tree.flatten(val)\n-  box_set_p.bind(box, *leaves, treedef=treedef)\n-\n-\n-class BoxGet(HiPrimitive):\n-  multiple_results = True\n-\n-  def is_high(self) -> bool: return True\n-\n-  def abstract_eval(self, box_ty):\n-    return box_ty._leaf_avals, set()\n-\n-  def to_lojax(_, box):\n-    return jax.tree.leaves(box._val)\n-\n-  def jvp(_, primals, tangents):\n-    assert False  # TODO\n-\n-  def transpose(_, *args):\n-    assert False  # TODO\n-box_get_p = BoxGet('box_get')\n-\n-def box_get(box):\n-  leaf_vals = box_get_p.bind(box)\n-  return jax.tree.unflatten(core.typeof(box)._treedef, leaf_vals)\n-\n-\n class HijaxTest(jtu.JaxTestCase):\n \n   def test_custom_types_and_primitive(self):\n@@ -194,8 +79,6 @@ class MyArray:\n \n     @dataclass(frozen=True)\n     class MyTy(core.AbstractValue):\n-      mutable = False\n-\n       def to_tangent_aval(self):\n         return MyTy()\n       def str_short(self, short_dtypes=False):\n@@ -324,6 +207,392 @@ def f(x):\n     self.assertIsInstance(a_grad, MyArray)\n     self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n \n+\n+def new_box():\n+  (), treedef = jax.tree.flatten(None)\n+  return new_box_p.bind(treedef=treedef)\n+\n+def box_get(box):\n+  tys = box.type_state()\n+  leaf_vals = box_get_p.bind(box, avals=tys.leaf_avals)\n+  return jax.tree.unflatten(tys.treedef, leaf_vals)\n+\n+def box_set(box, val):\n+  leaves, treedef = jax.tree.flatten(val)\n+  box_set_p.bind(box, *leaves, treedef=treedef)\n+\n+@dataclass(frozen=True)\n+class BoxTypeState:\n+  leaf_avals: tuple[core.AbstractValue, ...]\n+  treedef: PyTreeDef\n+\n+  def to_tangent_aval(self):\n+    return BoxTypeState(tuple(a.to_tangent_aval() for a in self.leaf_avals),\n+                        self.treedef)\n+\n+  def normalize(self):\n+    return BoxTypeState(tuple(a.normalize() for a in self.leaf_avals),\n+                        self.treedef)\n+\n+class BoxTy(core.AbstractValue):\n+  mutable = True\n+\n+  # forwarded to value\n+  get = core.aval_method(box_get)\n+  set = core.aval_method(box_set)\n+\n+  # aval interface: hashability and str_short\n+  def __hash__(self): return hash(BoxTy)\n+  def __eq__(self, other): return isinstance(other, BoxTy)\n+\n+  def str_short(self, short_dtypes=False):\n+    return 'BoxTy'\n+\n+  # mutable interface\n+  def lo_ty_(self, box_state):\n+    return [lo_ty for t in box_state.leaf_avals for lo_ty in t.lo_ty()]\n+\n+  def new_from_loval(self, box_state: BoxTypeState, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in box_state.leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    return Box(jax.tree.unflatten(box_state.treedef, hi_vals))  # will be mutated\n+\n+  def read_loval(self, box_state: BoxTypeState, box):\n+    leaf_vals, treedef = jax.tree.flatten(box_get(box))\n+    assert treedef == box_state.treedef\n+    return [lo_val for hi_ty, hi_val in zip(box_state.leaf_avals, leaf_vals)\n+            for lo_val in hi_ty.lower_val(hi_val)]\n+\n+  def update_from_loval(self, box_state: BoxTypeState, box, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in box_state.leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    box_set(box, jax.tree.unflatten(box_state.treedef, hi_vals))\n+\n+  def to_tangent_aval(self):\n+    return BoxTy()\n+\n+class Box:  # noqa: F811\n+  def __init__(self, val):\n+    self._val = val\n+\n+  def get(self):\n+    return box_get(self)\n+\n+  def set(self, val):\n+    box_set(self, val)\n+\n+  @property\n+  def ty(self):\n+    return BoxTy()\n+\n+  def type_state(self):\n+    leaves, treedef = jax.tree.flatten(self._val)\n+    leaf_avals = tuple(map(core.typeof, leaves))\n+    return BoxTypeState(leaf_avals, treedef)\n+core.pytype_aval_mappings[Box] = lambda b: b.ty\n+\n+\n+class NewBox(HiPrimitive):\n+  def is_high(self, *, treedef) -> bool: return True\n+\n+  def staging(self, trace, *, treedef):\n+    tracer = super().staging(trace, treedef=treedef)\n+    var = trace.frame.tracer_to_var[id(tracer)]\n+    leaves, treedef = jax.tree.flatten(None)\n+    trace.frame.current_typechange_env[var] = BoxTypeState(leaves, treedef)\n+    return tracer\n+\n+  def abstract_eval(self, *, treedef):\n+    return BoxTy(), set()\n+\n+  def to_lojax(_, *, treedef):\n+    return Box(None)\n+\n+  def jvp(_, primals, tangents, *, treedef):\n+    assert False  # TODO\n+\n+  def transpose(_, *args, treedef):\n+    assert False  # TODO\n+new_box_p = NewBox('new_box')\n+\n+\n+class BoxSet(HiPrimitive):\n+  multiple_results = True\n+\n+  def is_high(self, *, treedef) -> bool: return True\n+\n+  def staging(self, trace, box_tracer, *leaves, treedef):\n+    super().staging(trace, box_tracer, *leaves, treedef=treedef)\n+    var = trace.getvar(box_tracer)\n+    avals = tuple(t.aval for t in leaves)\n+    trace.frame.current_typechange_env[var] = BoxTypeState(avals, treedef)\n+    return []\n+\n+  def abstract_eval(self, box_ty, *leaf_avals, treedef):\n+    return [], set()  # TODO better typechecking...\n+\n+  def to_lojax(_, box, *leaves, treedef):\n+    box._val = jax.tree.unflatten(treedef, leaves)\n+    return []\n+\n+  def jvp(_, primals, tangents, *, treedef):\n+    box, *vals = primals\n+    box_dot, *val_dots = tangents\n+    if type(box_dot) is ad_util.Zero:\n+      raise Exception(\"you're an idiot\")\n+    box_set_p.bind(box, *vals, treedef=treedef)\n+    box_set_p.bind(box_dot, *val_dots, treedef=treedef)\n+    return [], []\n+\n+  def transpose(_, *args, treedef):\n+    assert False  # TODO\n+box_set_p = BoxSet('box_set')\n+\n+\n+class BoxGet(HiPrimitive):\n+  multiple_results = True\n+\n+  def abstract_eval(self, box_ty, *, avals):\n+    return avals, set()\n+\n+  def to_lojax(_, box, *, avals):\n+    return jax.tree.leaves(box._val)\n+\n+  def jvp(_, primals, tangents, *, avals):\n+    (box,), (box_dot,) = primals, tangents\n+    return (box_get_p.bind(box, avals=avals),\n+            box_get_p.bind(box_dot, avals=[a.to_tangent_aval() for a in avals]))\n+\n+  def transpose(_, *args):\n+    assert False  # TODO\n+box_get_p = BoxGet('box_get')\n+\n+\n+\n+class BoxTest(jtu.JaxTestCase):\n+\n+  def test_jit_arg(self):\n+    @jax.jit\n+    def f(box, x):\n+      assert tracing_ok\n+      box.set(box.get() + x)\n+\n+    tracing_ok = True\n+    box1 = Box(1.0)\n+    f(box1, 1.)\n+    self.assertAllClose(box1.get(), 2.0)\n+\n+    tracing_ok = False\n+    box2 = Box(2.0)\n+    f(box2, 2.)\n+    self.assertAllClose(box2.get(), 4.0)\n+\n+  def test_jit_arg2(self):\n+    # set without get\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    box = Box(0.0)\n+    f(box, 1.)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+\n+  def test_jit_arg_in_pytree(self):\n+    @jax.jit\n+    def f(dct, x):\n+      assert tracing_ok\n+      box = dct['box']\n+      box.set(box.get() + x)\n+\n+    tracing_ok = True\n+    box1 = Box(1.0)\n+    f({'box': box1, 'a': 1.0}, 1.)\n+    self.assertAllClose(box1.get(), 2.0)\n+\n+    tracing_ok = False\n+    box2 = Box(2.0)\n+    f({'box': box2, 'a': 2.0}, 2.)\n+    self.assertAllClose(box2.get(), 4.0)\n+\n+    tracing_ok = True\n+    box3 = Box(3)  # int, dtype changed\n+    f({'box': box3, 'a': 2.0}, 2.)\n+    self.assertAllClose(box3.get(), 5.0)\n+\n+  def test_jit_closure(self):\n+    box = Box(1.0)\n+\n+    @jax.jit\n+    def f(x):\n+      assert tracing_ok\n+      box.set(box.get() + x)\n+\n+    tracing_ok = True\n+    f(2.0)\n+    self.assertAllClose(box.get(), 3.0)\n+    tracing_ok = False\n+    f(5.0)\n+    self.assertAllClose(box.get(), 8.0)\n+\n+  def test_jit_closure_nested(self):\n+    box = Box(5.0)\n+\n+    @jax.jit\n+    def f(x):\n+      box.set(box.get() + x)\n+\n+    @jax.jit\n+    def g(x):\n+      f(x)\n+\n+    g(3.0)\n+    self.assertAllClose(box.get(), 8.0)\n+\n+  def test_jit_closure_nested2(self):\n+    @jax.jit\n+    def h(x):\n+      box = new_box()\n+      box.set(x)\n+\n+      @jax.jit\n+      def k(x):\n+        box.set(box.get() + x)\n+\n+      k(1.0)\n+      k(1.0)\n+      return box.get()\n+\n+    ans = h(2.0)\n+    self.assertAllClose(ans, 4.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_jvp_closure_stop_gradient(self, jit):\n+    box = Box(1.0)\n+\n+    def f(x):\n+      y = 2 * x\n+      box.set(box.get() + jax.lax.stop_gradient(y))\n+      return y\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    y, y_dot = jax.jvp(f, (1.0,), (1.0,))\n+    self.assertAllClose(y, 2.0)\n+    self.assertAllClose(y_dot, 2.0)\n+    self.assertAllClose(box.get(), 3.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_jvp_arg(self, jit):\n+    def f(box, x):\n+      box.set(box.get() + x)\n+      return x\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    box = Box(5.0)\n+    box_dot = Box(1.0)\n+    y, y_dot = jax.jvp(f, (box, 2.), (box_dot, 1.))\n+    self.assertAllClose(y, 2.0)\n+    self.assertAllClose(y_dot, 1.0)\n+    self.assertAllClose(box.get(), 7.0)\n+    self.assertAllClose(box_dot.get(), 2.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_custom_vjp_plumbing(self, jit):\n+    box = Box(0.0)\n+\n+    @jax.custom_vjp\n+    def foo(x):\n+      return x\n+    def foo_fwd(x):\n+      return foo(x), None\n+    def foo_bwd(_, g):\n+      box.set(g)\n+      return g,\n+    foo.defvjp(foo_fwd, foo_bwd)\n+\n+    def f(x):\n+      x = 2 * x\n+      x = foo(x)\n+      x = 2 * x\n+      return x\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    jax.grad(f)(1.0)\n+    self.assertAllClose(box.get(), 2.0)\n+\n+  # TODO(mattjj,dougalm): make this work...\n+  # @parameterized.parameters([False, True])\n+  # def test_custom_vjp_plumbing_abstracted(self, jit):\n+  #   box = Box(0.0)\n+\n+  #   @jax.custom_vjp\n+  #   def foo(box, x):\n+  #     return x\n+  #   def foo_fwd(box, x):\n+  #     return x, box\n+  #   def foo_bwd(box, g):\n+  #     box.set(g)\n+  #     return None, g\n+  #   foo.defvjp(foo_fwd, foo_bwd)\n+\n+  #   def f(box, x):\n+  #     x = 2 * x\n+  #     x = foo(box, x)\n+  #     x = 2 * x\n+  #     return x\n+\n+  #   if jit:\n+  #     f = jax.jit(f)\n+\n+  #   jax.grad(partial(f, box))(1.0)\n+  #   self.assertAllClose(box.get(), 2.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_grad_closure_stop_gradient(self, jit):\n+    box = Box(0.0)\n+\n+    def f(x):\n+      y = x * 2\n+      box.set(box.get() + jax.lax.stop_gradient(y))\n+      return y\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    g = jax.grad(f)(1.0)\n+    self.assertAllClose(g, 2.0)\n+    self.assertAllClose(box.get(), 2.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_scan_basic(self, jit):\n+    box = Box(1.0)\n+\n+    def double_it_10():\n+      def body(_, __):\n+        box.set(box.get() * 2)\n+        return None, None\n+      _, _ = jax.lax.scan(body, None, None, length=10)\n+\n+    if jit:\n+      double_it_10 = jax.jit(double_it_10)\n+\n+    double_it_10()\n+    self.assertAllClose(box.get(), 1024., check_dtypes=False)\n+\n+  # TODO error-checking tests from attrs_test.py\n+\n+  ###\n+\n   def test_box_autodiff(self):\n     if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n \n@@ -336,7 +605,7 @@ def abstract_eval(_, box_aval, x_aval):\n         return x_aval, set()\n \n       def to_lojax(_, box, x):\n-        assert False  # TODO\n+        return x\n \n       def jvp(_, primals, tangents):\n         box, x = primals\n@@ -351,14 +620,6 @@ def transpose(self, *args):\n     def stash_tangents(box, x):\n       return stash_tangents_p.bind(box, x)\n \n-    @jax.jit\n-    def f(box, x):\n-      box_set(box, x)\n-\n-    box = Box(0.0)\n-    f(box, 1.)\n-    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n-\n     @jax.jit\n     def f(box, x):\n       x = stash_tangents(box, x)\n@@ -449,5 +710,26 @@ def f(box):\n     self.assertAllClose(b_.arr, 2, check_dtypes=False)\n \n \n+class ListTy(core.AbstractValue):\n+  mutable = True\n+\n+  # forwarded to value\n+  get = core.aval_method(box_get)\n+  set = core.aval_method(box_set)\n+\n+  # aval interface: hashability and str_short\n+  def __hash__(self): return hash(BoxTy)\n+  def __eq__(self, other): return isinstance(other, BoxTy)\n+\n+  def str_short(self, short_dtypes=False):\n+    return 'ListTy'\n+\n+  # TODO\n+\n+class ListTest(jtu.JaxTestCase):\n+  ...\n+\n+\n+\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "rename layout to format, part 1\n\nWe want to rename layouts to formats, since layout is overloaded, both with its own device-local layout, and with XLA's layout (which corresponds to the device-local layout).\n\nThis change specifically focuses on renaming the `Layout` type to `Format` and using the new type constructor throughout the codebase. It should have minimal external effect, since it sets up `Layout` as a public alias of the newly renamed `Format`.\n\nThis change does not yet change most variable and attribute names, so it leaves around various names like `layout` that now have type `Format`. Next up, we should rename these for clarity, among other things.\n\nCo-authored-by: Yash Katariya <yashkatariya@google.com>\nPiperOrigin-RevId: 764922667",
        "output": "```diff\nCommit: 2c838d4ae3fff34242d5a2993e6892c8214c9177\nDate: 2025-05-29T23:06:06Z\nURL: https://github.com/jax-ml/jax/commit/2c838d4ae3fff34242d5a2993e6892c8214c9177\nFiles changed: 14\nAdditions: +159, Deletions: -131\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 3ff103997dc7..2630fc7ae1be 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -74,7 +74,7 @@\n from jax._src.mesh import get_concrete_mesh\n from jax._src.sharding_impls import (\n     PmapSharding, TransferToMemoryKind, PartitionSpec as P, NamedSharding)\n-from jax._src.layout import Layout, AutoLayout\n+from jax._src.layout import Format, AutoLayout\n from jax._src.traceback_util import api_boundary\n from jax._src import tree_util\n from jax._src.util import unzip2, safe_map, safe_zip, wraps, split_list\n@@ -2501,10 +2501,10 @@ def _check_string_compatible_sharding(s):\n @lru_cache(maxsize=2048)\n def _check_sharding(aval, s):\n   if (s is not None and\n-      not isinstance(s, (xc.Device, Sharding, Layout, TransferToMemoryKind))):\n+      not isinstance(s, (xc.Device, Sharding, Format, TransferToMemoryKind))):\n     raise ValueError(\n         \"`jax.device_put` only accepts `None`, `jax.sharding.Sharding`,\"\n-        \" `jax.Device`, `Layout` or a pytree of these values. Received\"\n+        \" `jax.Device`, `Format` or a pytree of these values. Received\"\n         f\" invalid value: {s}\")\n \n   if isinstance(aval, core.ShapedArray) and dtypes.is_string_dtype(aval.dtype):\n@@ -2530,8 +2530,8 @@ def pspec_to_sharding(val):\n \n def device_put(\n     x,\n-    device: None | xc.Device | Sharding | P | Layout | Any | TransferToMemoryKind = None,\n-    *, src: None | xc.Device | Sharding | P | Layout | Any | TransferToMemoryKind = None,\n+    device: None | xc.Device | Sharding | P | Format | Any | TransferToMemoryKind = None,\n+    *, src: None | xc.Device | Sharding | P | Format | Any | TransferToMemoryKind = None,\n     donate: bool | Any = False, may_alias: bool | None | Any = None):\n   \"\"\"Transfers ``x`` to ``device``.\n \n@@ -2827,18 +2827,18 @@ def __init__(self, shape, dtype, *, sharding=None, weak_type=False):\n     if dtype is None:\n       raise ValueError(\"ShapeDtypeStruct: dtype must be specified.\")\n     self.dtype = dtype if dtypes.issubdtype(dtype, dtypes.extended) else np.dtype(dtype)\n-    if sharding is not None and not isinstance(sharding, (Sharding, Layout, P)):\n+    if sharding is not None and not isinstance(sharding, (Sharding, Format, P)):\n       raise ValueError(\n           \"sharding should be an instance of `jax.sharding.Sharding`, \"\n           \"`jax.sharding.PartitionSpec` or\"\n-          f\" `jax.experimental.layout.Layout`. Got {sharding} of type\"\n+          f\" `jax.experimental.layout.Format`. Got {sharding} of type\"\n           f\" {type(sharding)}.\")\n-    if (isinstance(sharding, Layout) and\n+    if (isinstance(sharding, Format) and\n         isinstance(sharding.device_local_layout, AutoLayout)):\n       raise TypeError(\n           \"`DeviceLocalLayout.AUTO` cannot be used in place of a device-local\"\n           f\" layout in a `ShapeDtypeStruct`. Got {sharding}\")\n-    if isinstance(sharding, Layout):\n+    if isinstance(sharding, Format):\n       self.sharding = sharding.sharding\n     elif isinstance(sharding, P):\n       # TODO(yashkatariya): Should this be abstract mesh?\n@@ -2851,15 +2851,18 @@ def __init__(self, shape, dtype, *, sharding=None, weak_type=False):\n       self.sharding = NamedSharding(cur_mesh, sharding)\n     else:\n       self.sharding = sharding\n-    self._dll = sharding.device_local_layout if isinstance(sharding, Layout) else None\n+    self._dll = (sharding.device_local_layout if isinstance(sharding, Format)\n+                 else None)\n     self.weak_type = weak_type\n \n   size = property(lambda self: math.prod(self.shape))\n   ndim = property(lambda self: len(self.shape))\n \n   @property\n-  def layout(self):\n-    return Layout(self._dll, self.sharding)\n+  def format(self):\n+    return Format(self._dll, self.sharding)\n+\n+  layout = format\n \n   def __len__(self):\n     try:\n@@ -2869,7 +2872,7 @@ def __len__(self):\n \n   def __repr__(self):\n     sh = f\", sharding={self.sharding}\" if self.sharding is not None else \"\"\n-    l = f\", layout={self.layout}\" if self._dll is not None else \"\"\n+    l = f\", format={self._dll}\" if self._dll is not None else \"\"\n     wt = f\", weak_type={self.weak_type}\" if self.weak_type else \"\"\n     return (f\"{type(self).__name__}(shape={self.shape}, \"\n             f\"dtype={self.dtype.name}{sh}{l}{wt})\")\n@@ -2880,11 +2883,13 @@ def __eq__(self, other):\n     if not isinstance(other, ShapeDtypeStruct):\n       return False\n     else:\n-      return ((self.shape, self.dtype, self.sharding, self.layout, self.weak_type) ==\n-              (other.shape, other.dtype, other.sharding, other.layout, other.weak_type))\n+      return ((self.shape, self.dtype, self.sharding, self._dll, self.weak_type) ==\n+              (other.shape, other.dtype, other.sharding, other._dll, other.weak_type))\n \n   def __hash__(self):\n-    return hash((self.shape, self.dtype, self.sharding, self.layout,\n+    # TODO(frostig): avoid the conversion from dict by addressing\n+    # https://github.com/jax-ml/jax/issues/8182\n+    return hash((self.shape, self.dtype, self.sharding, self._dll,\n                  self.weak_type))\n \n   def __setattr__(self, name, value):\ndiff --git a/jax/_src/array.py b/jax/_src/array.py\nindex 422fa5086e62..29c7a17b07f1 100644\n--- a/jax/_src/array.py\n+++ b/jax/_src/array.py\n@@ -36,7 +36,7 @@\n from jax._src.interpreters import mlir\n from jax._src.interpreters import pxla\n from jax._src.interpreters import xla\n-from jax._src.layout import AutoLayout, DeviceLocalLayout, Layout\n+from jax._src.layout import AutoLayout, DeviceLocalLayout, Format\n from jax._src.lib import xla_client as xc\n from jax._src.lib import _jax\n from jax._src.sharding import Sharding\n@@ -550,14 +550,14 @@ def addressable_shards(self) -> Sequence[Shard]:\n   def layout(self):\n     # TODO(yashkatariya): Remove the deleted check from here.\n     if self.is_deleted():\n-      return Layout(None, self.sharding)\n+      return Format(None, self.sharding)\n     try:\n-      return Layout(DeviceLocalLayout.from_pjrt_layout(self._pjrt_layout),\n+      return Format(DeviceLocalLayout.from_pjrt_layout(self._pjrt_layout),\n                     self.sharding)\n     except _jax.XlaRuntimeError as e:\n       msg, *_ = e.args\n       if type(msg) is str and msg.startswith(\"UNIMPLEMENTED\"):\n-        return Layout(None, self.sharding)\n+        return Format(None, self.sharding)\n       else:\n         raise\n \n@@ -711,7 +711,7 @@ def _get_and_check_dtype(arrays: Sequence[basearray.Array | np.ndarray],\n # TODO(yashkatariya): Remove None from callback input type.\n \n def make_array_from_callback(\n-    shape: Shape, sharding: Sharding | Layout,\n+    shape: Shape, sharding: Sharding | Format,\n     data_callback: Callable[[Index | None], ArrayLike],\n     dtype: DTypeLike | None = None) -> ArrayImpl:\n   # pyformat: disable\n@@ -756,12 +756,12 @@ def make_array_from_callback(\n     (4, 2)\n   \"\"\"\n   # pyformat: enable\n-  dll = sharding.device_local_layout if isinstance(sharding, Layout) else None\n+  dll = sharding.device_local_layout if isinstance(sharding, Format) else None\n   if isinstance(dll, AutoLayout):\n     raise TypeError(\n         \"`DeviceLocalLayout.AUTO` cannot be used in place of a device-local\"\n         f\" layout when calling `jax.make_array_from_callback`. Got {sharding}\")\n-  sharding = sharding.sharding if isinstance(sharding, Layout) else sharding\n+  sharding = sharding.sharding if isinstance(sharding, Format) else sharding\n   if not isinstance(sharding, Sharding):\n     raise TypeError(\n         f\"sharding should be an instance of `jax.sharding`. Got {sharding} of\"\n@@ -823,7 +823,7 @@ def get_data(index: Index | None) -> ArrayImpl | np.ndarray:\n     )\n \n   if dll is not None:\n-    devices = [Layout(dll, SingleDeviceSharding(d)) for d in devices]\n+    devices = [Format(dll, SingleDeviceSharding(d)) for d in devices]\n     # pxla.batched_device_put doesn't support Layout... Take the slow route\n     arrays = api.device_put(per_device_values, devices)\n     return ArrayImpl(aval, sharding, arrays, committed=True)\n@@ -1218,7 +1218,7 @@ def _array_shard_arg(xs, shardings, layouts, copy_semantics):\n         batch_cs.append(cs)\n       # Resharding starts here:\n       elif not same_layout:\n-        results.append(api.device_put(x, Layout(layout, sharding)))\n+        results.append(api.device_put(x, Format(layout, sharding)))\n       elif dispatch.is_single_device_sharding(x.sharding):\n         results.append(shard_device_array(x, devices, indices, sharding))\n       else:\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex d1ea7439cb0c..a7c8d4ea7380 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -43,7 +43,7 @@\n from jax._src.interpreters import pxla\n from jax._src.interpreters import xla\n from jax._src.api_util import InternalFloatingPointError\n-from jax._src.layout import DeviceLocalLayout, Layout\n+from jax._src.layout import DeviceLocalLayout, Format\n from jax._src.lib import xla_client as xc\n from jax._src.mesh import AbstractMesh, Mesh\n from jax._src.monitoring import record_scalar, record_event_duration_secs, record_event_time_span\n@@ -479,8 +479,8 @@ def _device_put_sharding_impl(x, aval, device, copy):\n \n \n def _device_put_impl(\n-    x, *, device: Device | Sharding | Layout | None,\n-    src: Device | Sharding | Layout | None, copy: CopySemantics):\n+    x, *, device: Device | Sharding | Format | None,\n+    src: Device | Sharding | Format | None, copy: CopySemantics):\n   if (isinstance(device, TransferToMemoryKind) or\n       isinstance(src, TransferToMemoryKind)):\n     raise ValueError(\n@@ -494,7 +494,7 @@ def _device_put_impl(\n     raise TypeError(\n         f\"Argument '{x}' of type {type(x)} is not a valid JAX type\") from err\n \n-  if isinstance(device, Layout):\n+  if isinstance(device, Format):\n     l = device\n     dll = l.device_local_layout\n     x_dll = x.layout.device_local_layout if hasattr(x, 'layout') else None\n@@ -519,8 +519,8 @@ def _device_put_impl(\n \n def _batched_device_put_impl(\n     *xs,\n-    devices: Sequence[Device | Sharding | Layout | None],\n-    srcs: Sequence[Device | Sharding | Layout | None],\n+    devices: Sequence[Device | Sharding | Format | None],\n+    srcs: Sequence[Device | Sharding | Format | None],\n     copy_semantics: Sequence[CopySemantics]):\n   ys = []\n   dsa_indices, dsa_xs, dsa_shardings, dsa_copy_semantics = [], [], [], []\n@@ -536,7 +536,7 @@ def _batched_device_put_impl(\n   if dsa_xs:\n     # Batch shard_arg calls. Helps improve efficiency for backends that support\n     # efficient batch transfer.\n-    # device_put handles `Layout` via a different path, so just pass `None` as\n+    # device_put handles `Format` via a different path, so just pass `None` as\n     # the layout here.\n     shard_arg_results = pxla.shard_args(dsa_shardings, [None] * len(dsa_xs),\n                                         dsa_copy_semantics, dsa_xs)\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex d0a22cd784b4..8ebf4133a5fd 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -56,7 +56,7 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import mlir\n from jax._src.interpreters import xla\n-from jax._src.layout import DeviceLocalLayout, AutoLayout, Layout\n+from jax._src.layout import DeviceLocalLayout, AutoLayout, Format\n from jax._src.lib import xla_client as xc\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n@@ -208,7 +208,7 @@ def _shard_np_array(xs, shardings, layouts, copy_semantics):\n       x = np.zeros(x.shape, dtype=np.dtype(bool))\n     aval = core.shaped_abstractify(x)\n     if layout is not None:\n-      results.append(api.device_put(x, Layout(layout, sharding)))\n+      results.append(api.device_put(x, Format(layout, sharding)))\n     else:\n       if sharding.is_fully_replicated:\n         shards = [x] * len(devices)\ndiff --git a/jax/_src/layout.py b/jax/_src/layout.py\nindex 3675433c43d8..c50c1787b94e 100644\n--- a/jax/_src/layout.py\n+++ b/jax/_src/layout.py\n@@ -94,7 +94,7 @@ def check_compatible_aval(self, aval_shape: Shape):\n ShardingOptions = Union[Sharding, None, AutoSharding]\n \n \n-class Layout:\n+class Format:\n   __slots__ = ['device_local_layout', 'sharding']\n \n   def __init__(self, device_local_layout: LayoutOptions = None,\n@@ -139,7 +139,9 @@ def __hash__(self):\n     return hash((self.device_local_layout, self.sharding))\n \n   def __eq__(self, other):\n-    if not isinstance(other, Layout):\n+    if not isinstance(other, Format):\n       return False\n     return (self.device_local_layout == other.device_local_layout and\n             self.sharding == other.sharding)\n+\n+Layout = Format  # TODO(frostig, yashkatariya): remove this alias\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex d5286be8e0c9..94a754a1a597 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -69,7 +69,7 @@\n     SingleDeviceSharding, PmapSharding, AUTO, UNSPECIFIED, UnspecifiedValue,\n     prepare_axis_resources, parse_flatten_op_sharding, canonicalize_sharding,\n     flatten_spec, _internal_use_concrete_mesh)\n-from jax._src.layout import Layout, DeviceLocalLayout, AutoLayout\n+from jax._src.layout import Format, DeviceLocalLayout, AutoLayout\n from jax._src.state import discharge as state_discharge, RefEffect, AbstractRef\n from jax._src.traceback_util import api_boundary\n from jax._src.tree_util import (\n@@ -374,13 +374,13 @@ def _split_layout_and_sharding(entries):\n   layouts, shardings = [], []\n \n   for e in entries_flat:\n-    if isinstance(e, Layout):\n+    if isinstance(e, Format):\n       layouts.append(e.device_local_layout)\n       shardings.append(e.sharding)\n     elif isinstance(e, (DeviceLocalLayout, AutoLayout)):\n       raise ValueError(\n           '`jax.jit` does not accept device-local layouts directly. Create '\n-          'a `Layout` instance wrapping this device-local layout and pass '\n+          'a `Format` instance wrapping this device-local layout and pass '\n           f'that to `jit` instead. Got {e}')\n     else:\n       layouts.append(None)\n@@ -1645,7 +1645,7 @@ def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n     else:\n       # arg_layout can be None because some backends don't implement the\n       # required layout methods. Hence `arr.layout` can return\n-      # `Layout(None, sharding)`\n+      # `Format(None, sharding)`\n       if (committed\n           and not is_pmap_sharding\n           and arg_layout is not None\n@@ -2813,7 +2813,7 @@ def _sharding_constraint_impl(x, sharding, layout, context_mesh,\n     if (hasattr(x, 'layout') and x.layout.device_local_layout == layout and\n         x.sharding.is_equivalent_to(sharding, x.ndim)):\n       return x\n-    return api.jit(_identity_fn, out_shardings=Layout(layout, sharding))(x)\n+    return api.jit(_identity_fn, out_shardings=Format(layout, sharding))(x)\n \n \n sharding_constraint_p = core.Primitive(\"sharding_constraint\")\n@@ -3160,7 +3160,7 @@ def _layout_constraint_impl(x, *, layout):\n         f' jax.Arrays. Got {type(x)}')\n   if x.layout.device_local_layout == layout:  # type: ignore\n     return x\n-  return api.jit(_identity_fn, out_shardings=Layout(layout, x.sharding))(x)\n+  return api.jit(_identity_fn, out_shardings=Format(layout, x.sharding))(x)\n layout_constraint_p.def_impl(_layout_constraint_impl)\n \n def _layout_constraint_hlo_lowering(ctx, x_node, *, layout):\ndiff --git a/jax/_src/stages.py b/jax/_src/stages.py\nindex d92d1ccb2aa3..17649aae3081 100644\n--- a/jax/_src/stages.py\n+++ b/jax/_src/stages.py\n@@ -46,7 +46,7 @@\n from jax._src import typing\n from jax._src import util\n from jax._src.sharding_impls import UnspecifiedValue, AUTO\n-from jax._src.layout import Layout\n+from jax._src.layout import Format, DeviceLocalLayout\n from jax._src.interpreters import mlir\n from jax._src.lib.mlir import ir\n from jax._src.lib import _jax\n@@ -105,7 +105,7 @@ def input_layouts(self):\n \n   def output_layouts(self):\n     raise NotImplementedError(\n-        \"compiled executable carries no input layout information\")\n+        \"compiled executable carries no output layout information\")\n \n   def as_text(self) -> str:\n     \"\"\"A human-readable text representation of this executable.\n@@ -438,39 +438,58 @@ def runtime_executable(self) -> Any | None:\n     \"\"\"\n     return self._executable.runtime_executable()\n \n-  @property\n-  def input_shardings(self):  # PyTree[sharding.Sharding]\n+  def _input_shardings_flat(self):\n     shardings_flat = self._executable._in_shardings\n     # Some input shardings got DCE'd\n     if self.in_tree.num_leaves > len(shardings_flat):\n       iter_shardings_flat = iter(shardings_flat)\n       shardings_flat = [next(iter_shardings_flat) if i in self._executable._kept_var_idx\n                         else None for i in range(self.in_tree.num_leaves)]\n+    return shardings_flat\n+\n+  @property\n+  def input_shardings(self):  # -> PyTree[sharding.Sharding]\n+    shardings_flat = self._input_shardings_flat()\n     return tree_util.tree_unflatten(self.in_tree, shardings_flat)  # pytype: disable=attribute-error\n \n   @property\n-  def output_shardings(self):  # PyTree[sharding.Sharding]\n+  def output_shardings(self):  # -> PyTree[sharding.Sharding]\n     shardings_flat = self._executable._out_shardings\n     return tree_util.tree_unflatten(self.out_tree, shardings_flat)  # pytype: disable=attribute-error\n \n-  @property\n-  def input_layouts(self):\n-    dll_flat = self._executable._xla_in_layouts\n-    layouts_flat = [Layout(l, s)\n-                    for l, s in zip(dll_flat, self._executable._in_shardings)]\n+  def _input_layouts_flat(self):\n+    layouts_flat = self._executable._xla_in_layouts\n     # Some input layouts got DCE'd\n     if self.in_tree.num_leaves > len(layouts_flat):\n       iter_layouts_flat = iter(layouts_flat)\n       layouts_flat = [next(iter_layouts_flat) if i in self._executable._kept_var_idx\n-                      else Layout() for i in range(self.in_tree.num_leaves)]\n-    return tree_util.tree_unflatten(self.in_tree, layouts_flat)  # pytype: disable=attribute-error\n+                      else None for i in range(self.in_tree.num_leaves)]\n+    return layouts_flat\n+\n+  @property\n+  def input_formats(self):\n+    layouts_flat = self._input_layouts_flat()\n+    shardings_flat = self._input_shardings_flat()\n+    formats_flat = [Format(l, s) for l, s in zip(layouts_flat, shardings_flat)]\n+    return tree_util.tree_unflatten(self.in_tree, formats_flat)  # pytype: disable=attribute-error\n+\n+  @property\n+  def output_formats(self):\n+    layouts_flat = self._executable._xla_out_layouts\n+    shardings_flat = self._executable._out_shardings\n+    assert all(isinstance(l, DeviceLocalLayout) for l in layouts_flat)\n+    formats_flat = [Format(l, s) for l, s in zip(layouts_flat, shardings_flat)]\n+    return tree_util.tree_unflatten(self.out_tree, formats_flat)  # pytype: disable=attribute-error\n+\n+  # TODO(frostig, yashkatariya): remove\n+  @property\n+  def input_layouts(self):\n+    return self.input_formats\n \n+  # TODO(frostig, yashkatariya): remove\n   @property\n   def output_layouts(self):\n-    dll_flat = self._executable._xla_out_layouts\n-    layouts_flat = [Layout(l, s)\n-                    for l, s in zip(dll_flat, self._executable._out_shardings)]\n-    return tree_util.tree_unflatten(self.out_tree, layouts_flat)  # pytype: disable=attribute-error\n+    return self.output_formats\n \n   @staticmethod\n   def call(*args, **kwargs):\ndiff --git a/jax/experimental/array_serialization/serialization.py b/jax/experimental/array_serialization/serialization.py\nindex 82e9e3dc938b..44b2eb9ccd03 100644\n--- a/jax/experimental/array_serialization/serialization.py\n+++ b/jax/experimental/array_serialization/serialization.py\n@@ -32,7 +32,7 @@\n from jax._src import sharding\n from jax._src import typing\n from jax._src import util\n-from jax._src.layout import Layout\n+from jax._src.layout import Format\n from jax._src.lib import _jax\n from jax.experimental.array_serialization import tensorstore_impl as ts_impl\n # ruff: noqa: F401\n@@ -352,7 +352,7 @@ def serialize_with_paths(\n         transaction=transaction,\n     )\n \n-  def deserialize(self, shardings: Sequence[sharding.Sharding | Layout],\n+  def deserialize(self, shardings: Sequence[sharding.Sharding | Format],\n                   tensorstore_specs: Sequence[dict[str, Any]],\n                   global_shapes: Sequence[array.Shape] | None = None,\n                   dtypes: Sequence[typing.DTypeLike] | None = None,\ndiff --git a/jax/experimental/array_serialization/serialization_test.py b/jax/experimental/array_serialization/serialization_test.py\nindex 9a6b91d04c9a..3bee72967101 100644\n--- a/jax/experimental/array_serialization/serialization_test.py\n+++ b/jax/experimental/array_serialization/serialization_test.py\n@@ -27,7 +27,7 @@\n from jax._src import config\n from jax._src import test_util as jtu\n from jax._src.layout import DeviceLocalLayout as DLL\n-from jax._src.layout import Layout\n+from jax._src.layout import Format\n from jax.experimental.array_serialization import serialization\n from jax.experimental.array_serialization import tensorstore_impl as ts_impl\n import jax.numpy as jnp\n@@ -593,7 +593,7 @@ def test_load_with_layout(self):\n     s = NamedSharding(mesh, P('x', 'y'))\n     arr = jax.device_put(np_inp, s)\n \n-    out_layout = jax.jit(lambda x: x.T, out_shardings=Layout(DLL.AUTO)).lower(\n+    out_layout = jax.jit(lambda x: x.T, out_shardings=Format(DLL.AUTO)).lower(\n         arr).compile().output_layouts\n     self.assertEqual(arr.layout.device_local_layout.major_to_minor,\n                      out_layout.device_local_layout.major_to_minor[::-1])\ndiff --git a/jax/experimental/array_serialization/tensorstore_impl.py b/jax/experimental/array_serialization/tensorstore_impl.py\nindex 873cc82da95e..7578bbb831e0 100644\n--- a/jax/experimental/array_serialization/tensorstore_impl.py\n+++ b/jax/experimental/array_serialization/tensorstore_impl.py\n@@ -25,7 +25,7 @@\n import jax\n from jax import numpy as jnp\n from jax._src import array\n-from jax._src.layout import Layout\n+from jax._src.layout import Format\n from jax._src import typing\n import numpy as np\n import tensorstore as ts\n@@ -424,7 +424,7 @@ def estimate_read_memory_footprint(t: ts.TensorStore,\n \n \n async def async_deserialize(\n-    user_in_sharding: jax.sharding.Sharding | Layout,\n+    user_in_sharding: jax.sharding.Sharding | Format,\n     tensorstore_spec: ts.Spec | dict[str, Any],\n     global_shape: Sequence[int] | None = None,\n     dtype=None,\n@@ -435,13 +435,13 @@ async def async_deserialize(\n ):\n   \"\"\"Main performant deserialization routine for arrays using tensorstore.\"\"\"\n   in_sharding = (user_in_sharding.sharding\n-                 if isinstance(user_in_sharding, Layout) else user_in_sharding)\n+                 if isinstance(user_in_sharding, Format) else user_in_sharding)\n   if not isinstance(in_sharding, jax.sharding.Sharding):\n     raise ValueError(\n         'sharding passed to deserialization should be specified, concrete and'\n         f' an instance of `jax.sharding.Sharding`. Got {in_sharding}')\n   dll = (user_in_sharding.device_local_layout\n-         if isinstance(user_in_sharding, Layout) else None)\n+         if isinstance(user_in_sharding, Format) else None)\n   t = await ts.open(\n       tensorstore_spec,\n       open=True,\n@@ -476,7 +476,7 @@ async def cb(index: array.Index, device: jax.Device):\n     if out.dtype == jnp.int4:\n       out = jnp.asarray(out)  # type: ignore\n     result = jax.device_put(\n-        out, Layout(dll, jax.sharding.SingleDeviceSharding(device)))\n+        out, Format(dll, jax.sharding.SingleDeviceSharding(device)))\n     if byte_limiter is not None:\n       # NB: `out` actually might not be ready for garbage collection by the\n       # time we call release_bytes . Thus peak memory usage still might grow\n@@ -495,7 +495,7 @@ async def cb(index: array.Index, device: jax.Device):\n \n \n # TODO(rdyro): Remove this function.\n-def _run_deserialization(shardings: Sequence[jax.sharding.Sharding | Layout],\n+def _run_deserialization(shardings: Sequence[jax.sharding.Sharding | Format],\n                         tensorstore_specs: Sequence[dict[str, Any]],\n                         global_shapes: Sequence[array.Shape] | None = None,\n                         dtypes: Sequence[typing.DTypeLike] | None = None,\ndiff --git a/jax/experimental/layout.py b/jax/experimental/layout.py\nindex e98cfbc68104..1c243541d99b 100644\n--- a/jax/experimental/layout.py\n+++ b/jax/experimental/layout.py\n@@ -14,8 +14,10 @@\n \n from jax._src.layout import (\n     DeviceLocalLayout as DeviceLocalLayout,\n-    Layout as Layout,\n+    Format as Format,\n )\n from jax._src.pjit import (\n     with_layout_constraint as with_layout_constraint,\n )\n+\n+Layout = Format\ndiff --git a/tests/layout_test.py b/tests/layout_test.py\nindex c15816d7794a..cfec2253dfc8 100644\n--- a/tests/layout_test.py\n+++ b/tests/layout_test.py\n@@ -23,7 +23,7 @@\n from jax._src import config\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip\n-from jax.experimental.layout import (with_layout_constraint, Layout,\n+from jax.experimental.layout import (with_layout_constraint, Format,\n                                      DeviceLocalLayout as DLL)\n from jax.experimental.compute_on import compute_on\n \n@@ -51,8 +51,8 @@ def init(x, y):\n     sds1 = jax.ShapeDtypeStruct(np_inp1.shape, np_inp1.dtype, sharding=s1)\n     sds2 = jax.ShapeDtypeStruct(np_inp2.shape, np_inp2.dtype, sharding=s2)\n \n-    lowered_apply = jax.jit(apply, in_shardings=Layout(DLL.AUTO),\n-                            out_shardings=Layout(DLL.AUTO)).lower(sds1, sds2)\n+    lowered_apply = jax.jit(apply, in_shardings=Format(DLL.AUTO),\n+                            out_shardings=Format(DLL.AUTO)).lower(sds1, sds2)\n     compiled_apply = lowered_apply.compile()\n \n     arg_layouts, kw_layouts = compiled_apply.input_layouts\n@@ -122,8 +122,8 @@ def f(x):\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n \n-    compiled_auto = jax.jit(f, in_shardings=Layout(DLL.AUTO),\n-                            out_shardings=Layout(DLL.AUTO)).lower(sds).compile()\n+    compiled_auto = jax.jit(f, in_shardings=Format(DLL.AUTO),\n+                            out_shardings=Format(DLL.AUTO)).lower(sds).compile()\n     self.assertTupleEqual(\n         compiled_auto.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n         (2, 1, 0))\n@@ -146,8 +146,8 @@ def test_in_layouts_out_layouts(self):\n     def f(x):\n       return x.T\n \n-    compiled = jax.jit(f, in_shardings=Layout(),\n-                       out_shardings=Layout(DLL.AUTO)).lower(arr).compile()\n+    compiled = jax.jit(f, in_shardings=Format(),\n+                       out_shardings=Format(DLL.AUTO)).lower(arr).compile()\n     self.assertTupleEqual(\n         compiled.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n         (1, 0))\n@@ -166,8 +166,8 @@ def test_sharding_and_layouts(self):\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n \n-    compiled = jax.jit(lambda x: x.T, in_shardings=Layout(DLL.AUTO, s),\n-                       out_shardings=Layout(DLL.AUTO, s)).lower(np_inp).compile()\n+    compiled = jax.jit(lambda x: x.T, in_shardings=Format(DLL.AUTO, s),\n+                       out_shardings=Format(DLL.AUTO, s)).lower(np_inp).compile()\n     out = compiled(np_inp)\n     self.assertTupleEqual(\n         compiled.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n@@ -185,8 +185,8 @@ def f(x, y, z, a, b, c):\n \n     shape = (8, 2)\n     inps = [np.arange(math.prod(shape)).reshape(shape)] * 6\n-    compiled = jax.jit(f, in_shardings=Layout(DLL.AUTO),\n-                       out_shardings=Layout(DLL.AUTO)).lower(*inps).compile()\n+    compiled = jax.jit(f, in_shardings=Format(DLL.AUTO),\n+                       out_shardings=Format(DLL.AUTO)).lower(*inps).compile()\n     arg_layouts, _ = compiled.input_layouts\n     out1, out2 = compiled(*inps)\n \n@@ -216,8 +216,8 @@ def test_no_error_dced_args(self):\n     def f(x, y):\n       return x * 2\n \n-    jf = jax.jit(f, in_shardings=Layout(DLL.AUTO, s),\n-                 out_shardings=Layout(DLL.AUTO, s))\n+    jf = jax.jit(f, in_shardings=Format(DLL.AUTO, s),\n+                 out_shardings=Format(DLL.AUTO, s))\n     compiled = jf.lower(np_inp, np_inp).compile()\n     arg_layouts, _ = compiled.input_layouts\n     arrs = [jax.device_put(i, l) for i, l in zip(arrs, arg_layouts)]\n@@ -244,10 +244,10 @@ def f(x):\n     with self.assertRaisesRegex(\n         ValueError,\n         'Layout passed to jit does not match the layout on the respective arg'):\n-      jax.jit(f, in_shardings=Layout(DLL.AUTO)).lower(arr)\n+      jax.jit(f, in_shardings=Format(DLL.AUTO)).lower(arr)\n \n-    compiled = jax.jit(f, in_shardings=Layout(DLL.AUTO),\n-                       out_shardings=Layout(DLL.AUTO)).lower(sds).compile()\n+    compiled = jax.jit(f, in_shardings=Format(DLL.AUTO),\n+                       out_shardings=Format(DLL.AUTO)).lower(sds).compile()\n \n     with self.assertRaisesRegex(\n         ValueError,\n@@ -273,7 +273,7 @@ def test_device_put_concrete_layout(self):\n     arr = jax.device_put(np_inp, s)\n \n     compiled = jax.jit(\n-        lambda x: x * 2, out_shardings=Layout(DLL.AUTO)).lower(arr).compile()\n+        lambda x: x * 2, out_shardings=Format(DLL.AUTO)).lower(arr).compile()\n     col = compiled.output_layouts\n \n     out = jax.device_put(np_inp, col)\n@@ -286,17 +286,17 @@ def test_device_put_concrete_layout(self):\n   def test_device_put_non_concrete_layout_error(self):\n     np_inp = np.arange(16).reshape(8, 2)\n \n-    l1 = Layout(DLL.AUTO, SingleDeviceSharding(jax.devices()[0]))\n+    l1 = Format(DLL.AUTO, SingleDeviceSharding(jax.devices()[0]))\n     with self.assertRaisesRegex(\n         ValueError, 'sharding and device_local_layout.*should be concrete'):\n       jax.device_put(np_inp, l1)\n \n-    l2 = Layout(DLL.AUTO)\n+    l2 = Format(DLL.AUTO)\n     with self.assertRaisesRegex(\n         ValueError, 'sharding and device_local_layout.*should be concrete'):\n       jax.device_put(np_inp, l2)\n \n-    l3 = Layout(None, SingleDeviceSharding(jax.devices()[0]))\n+    l3 = Format(None, SingleDeviceSharding(jax.devices()[0]))\n     out = jax.device_put(np_inp, l3)\n     self.assertArraysEqual(out, np_inp)\n     self.assertTrue(out._committed)\n@@ -306,7 +306,7 @@ def invalid_layout_spec(self):\n     compiled = jax.jit(lambda x: x).lower(x).compile()\n     with self.assertRaisesRegex(\n         ValueError, 'Sharding has to be concrete when layout.*'):\n-      Layout(compiled.output_layouts[0], None)\n+      Format(compiled.output_layouts[0], None)\n \n   def test_layout_on_sds(self):\n     mesh = jtu.create_mesh((2, 1), ('x', 'y'))\n@@ -314,7 +314,7 @@ def test_layout_on_sds(self):\n     np_inp = np.arange(16).reshape(8, 2)\n     arr = jax.device_put(np_inp, s)\n \n-    out_layout = jax.jit(jnp.sin, out_shardings=Layout(DLL.AUTO)).lower(\n+    out_layout = jax.jit(jnp.sin, out_shardings=Format(DLL.AUTO)).lower(\n         arr).compile().output_layouts\n \n     sds = jax.ShapeDtypeStruct(arr.shape, arr.dtype, sharding=out_layout)\n@@ -325,7 +325,7 @@ def test_layout_on_sds(self):\n         TypeError,\n         'DeviceLocalLayout.AUTO` cannot be used in place of a device-local'\n         ' layout in a `ShapeDtypeStruct`'):\n-      jax.ShapeDtypeStruct(arr.shape, arr.dtype, sharding=Layout(DLL.AUTO))\n+      jax.ShapeDtypeStruct(arr.shape, arr.dtype, sharding=Format(DLL.AUTO))\n \n   def test_make_array_from_callback(self):\n     mesh = jtu.create_mesh((2, 1), ('x', 'y'))\n@@ -344,13 +344,13 @@ def test_make_array_from_callback(self):\n         TypeError,\n         '`DeviceLocalLayout.AUTO` cannot be used in place of a device-local'\n         ' layout'):\n-      jax.make_array_from_callback(np_inp.shape, Layout(DLL.AUTO, s),\n+      jax.make_array_from_callback(np_inp.shape, Format(DLL.AUTO, s),\n                                    lambda idx: np_inp[idx])\n \n     with self.assertRaisesRegex(\n         TypeError, 'sharding should be an instance of `jax.sharding`'):\n       jax.make_array_from_callback(\n-          np_inp.shape, Layout(None, None), lambda idx: np_inp[idx])\n+          np_inp.shape, Format(None, None), lambda idx: np_inp[idx])\n \n   def test_wsc_concrete_layout(self):\n     mesh = jtu.create_mesh((2, 2), ('x', 'y'))\n@@ -367,7 +367,7 @@ def f(x):\n       y = x.T\n       # Constrain `y` to the original layout of `arr` because without it,\n       # the layout of `y` would be the transpose of `arr`.\n-      return jax.lax.with_sharding_constraint(y, Layout(custom_dll, s))\n+      return jax.lax.with_sharding_constraint(y, Format(custom_dll, s))\n \n     out = f(arr)\n     self.assertEqual(out.layout.device_local_layout.major_to_minor,\n@@ -390,7 +390,7 @@ def f(x):\n       y = x.T\n       # Constrain `y` to the original layout of `arr` because without it,\n       # the layout of `y` would be the transpose of `arr`.\n-      return jax.lax.with_sharding_constraint(y, Layout(custom_dll, s))\n+      return jax.lax.with_sharding_constraint(y, Format(custom_dll, s))\n \n     out = f(arr)\n     self.assertEqual(out.layout.device_local_layout.major_to_minor,\n@@ -404,7 +404,7 @@ def test_device_put_user_concrete_layout(self):\n     dll = DLL(major_to_minor=(1, 0))\n     s = SingleDeviceSharding(jax.devices()[0])\n \n-    out = jax.device_put(np_inp, Layout(dll, s))\n+    out = jax.device_put(np_inp, Format(dll, s))\n     self.assertEqual(out.layout.device_local_layout.major_to_minor,\n                      dll.major_to_minor)\n     self.assertArraysEqual(out, np_inp)\n@@ -417,7 +417,7 @@ def test_device_put_user_concrete_layout_multi_device(self):\n     jnp_inp = jnp.arange(math.prod(shape)).reshape(shape)\n     arr = jax.device_put(np_inp, s)\n \n-    custom_layout = Layout(DLL(major_to_minor=(0, 1)), s)\n+    custom_layout = Format(DLL(major_to_minor=(0, 1)), s)\n     out1 = jax.device_put(arr, custom_layout)\n \n     with jax.sharding.use_mesh(mesh):\n@@ -441,7 +441,7 @@ def f(x):\n       return x.T\n \n     custom_dll = DLL(major_to_minor=(0, 1))\n-    f = jax.jit(f, out_shardings=Layout(custom_dll, s))\n+    f = jax.jit(f, out_shardings=Format(custom_dll, s))\n \n     out = f(arr)\n     self.assertArraysEqual(out, np_inp.T)\n@@ -450,7 +450,7 @@ def f(x):\n \n   def test_compatible_aval_error(self):\n     custom_dll = DLL(major_to_minor=(0, 1, 2))\n-    l = Layout(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n+    l = Format(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n     inp = np.arange(8)\n \n     @partial(jax.jit, in_shardings=l)\n@@ -464,7 +464,7 @@ def f(x):\n \n   def test_incompatible_aval_error_device_put(self):\n     custom_dll = DLL(major_to_minor=(0, 1, 2))\n-    l = Layout(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n+    l = Format(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n     inp = np.arange(8)\n \n     with self.assertRaisesRegex(\n@@ -482,8 +482,8 @@ def test_concrete_layout_in_shardings(self):\n     custom_dll = DLL(major_to_minor=(0, 1))\n \n     @partial(jax.jit,\n-             in_shardings=Layout(custom_dll, s),\n-             out_shardings=Layout(DLL.AUTO))\n+             in_shardings=Format(custom_dll, s),\n+             out_shardings=Format(DLL.AUTO))\n     def f(x):\n       return x.T\n \n@@ -494,7 +494,7 @@ def f(x):\n \n     custom_dll2 = DLL(major_to_minor=(1, 0))\n \n-    @partial(jax.jit, in_shardings=Layout(custom_dll2, s))\n+    @partial(jax.jit, in_shardings=Format(custom_dll2, s))\n     def g(x):\n       return x.T\n \n@@ -508,7 +508,7 @@ def test_in_layouts_jit_jnp_input(self):\n     sharding = jax.sharding.SingleDeviceSharding(jax.devices()[0])\n \n     f = jax.jit(lambda x: x + 1,\n-                in_shardings=Layout(major_last_layout, sharding))\n+                in_shardings=Format(major_last_layout, sharding))\n \n     arr = jnp.arange(8 * 128).reshape(8, 128)\n     out = f(arr)\n@@ -533,9 +533,9 @@ def test_layout_donation(self):\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n \n     custom_dll = DLL(major_to_minor=(0, 1))\n-    arr = jax.device_put(np_inp, Layout(custom_dll, s))\n+    arr = jax.device_put(np_inp, Format(custom_dll, s))\n \n-    @partial(jax.jit, in_shardings=Layout(custom_dll, s), donate_argnums=0)\n+    @partial(jax.jit, in_shardings=Format(custom_dll, s), donate_argnums=0)\n     def f(x):\n       return x\n \n@@ -550,7 +550,7 @@ def test_layout_donation_auto(self):\n \n     arr = jax.device_put(np_inp, s)\n \n-    @partial(jax.jit, out_shardings=Layout(DLL.AUTO), donate_argnums=0)\n+    @partial(jax.jit, out_shardings=Format(DLL.AUTO), donate_argnums=0)\n     def f(x):\n       return x * x\n \n@@ -564,7 +564,7 @@ def test_layout_donation_matching_in_and_out(self):\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n \n     custom_dll = DLL(major_to_minor=(0, 1))\n-    l = Layout(custom_dll, s)\n+    l = Format(custom_dll, s)\n     arr = jax.device_put(np_inp, l)\n \n     @partial(jax.jit, in_shardings=l, out_shardings=l, donate_argnums=0)\n@@ -582,7 +582,7 @@ def test_layout_donation_mismatching_in_and_out_fails(self):\n     np_inp = np.arange(math.prod(shape), dtype=jnp.bfloat16).reshape(shape)\n \n     custom_dll1 = DLL(major_to_minor=(1, 0), _tiling=((8,128), (2,1)))\n-    l1 = Layout(custom_dll1, s)\n+    l1 = Format(custom_dll1, s)\n     arr = jax.device_put(np_inp, s)\n \n     @partial(jax.jit, out_shardings=l1, donate_argnums=0)\n@@ -594,7 +594,7 @@ def f(x):\n     self.assertFalse(arr.is_deleted())\n \n   def test_donation_error_on_auto(self):\n-    @partial(jax.jit, donate_argnums=0, in_shardings=Layout(DLL.AUTO))\n+    @partial(jax.jit, donate_argnums=0, in_shardings=Format(DLL.AUTO))\n     def f(x):\n       return x * 2\n \n@@ -602,7 +602,7 @@ def f(x):\n         ValueError, \".*Did you mean to set the.*output layout.*AUTO.*\"):\n       f(jnp.arange(8))\n \n-    @partial(jax.jit, donate_argnums=0, out_shardings=Layout(DLL.AUTO))\n+    @partial(jax.jit, donate_argnums=0, out_shardings=Format(DLL.AUTO))\n     def g(x):\n       return x * 2\n \n@@ -619,9 +619,9 @@ def test_sparsecore_compute(self):\n \n     dll = DLL(major_to_minor=(0, 1), _tiling=((8,),))\n     s = SingleDeviceSharding(jax.devices()[0])\n-    sparse_layout = Layout(dll, s)\n+    sparse_layout = Format(dll, s)\n     sparecore_arr = jax.device_put(inp, sparse_layout)\n-    dense_layout = Layout(DLL(major_to_minor=(0, 1)), s)\n+    dense_layout = Format(DLL(major_to_minor=(0, 1)), s)\n \n     @compute_on('tpu_sparsecore')\n     @jax.jit\n@@ -645,7 +645,7 @@ def test_sparsecore_compute_twice(self):\n \n     dll = DLL(major_to_minor=(0, 1), _tiling=((8,),))\n     s = SingleDeviceSharding(jax.devices()[0])\n-    sparse_layout = Layout(dll, s)\n+    sparse_layout = Format(dll, s)\n     sparecore_arr = jax.device_put(inp, sparse_layout)\n \n     @compute_on('tpu_sparsecore')\n@@ -675,11 +675,11 @@ def test_sparsecore_and_host_compute(self):\n     s = SingleDeviceSharding(jax.devices()[0])\n \n     sparse_dll = DLL(major_to_minor=(0, 1), _tiling=((8,),))\n-    sparse_layout = Layout(sparse_dll, s)\n+    sparse_layout = Format(sparse_dll, s)\n     sparecore_arr = jax.device_put(inp, sparse_layout)\n \n     host_dll = DLL(major_to_minor=(0, 1), _tiling=((1,),))\n-    host_layout = Layout(host_dll, s)\n+    host_layout = Format(host_dll, s)\n     host_arr = jax.device_put(inp, host_layout)\n \n     @compute_on('tpu_sparsecore')\n@@ -710,7 +710,7 @@ def test_cpp_layout_cache_miss(self):\n     arr = jax.device_put(np_inp, s)\n \n     arr_m2m = arr.layout.device_local_layout.major_to_minor\n-    custom_layout = Layout(DLL(major_to_minor=arr_m2m[::-1]), s)\n+    custom_layout = Format(DLL(major_to_minor=arr_m2m[::-1]), s)\n     arr2 = jax.device_put(np_inp, custom_layout)\n \n     @jax.jit\n@@ -731,7 +731,7 @@ def test_layout_donation_with_default_layout(self):\n     shape = (16, 16)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     arr = jax.device_put(np_inp, s)\n-    out_layout = Layout(arr.layout.device_local_layout, s)\n+    out_layout = Format(arr.layout.device_local_layout, s)\n \n     @partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\n     def f(x):\ndiff --git a/tests/memories_test.py b/tests/memories_test.py\nindex e0a42d4a0146..fd40330f8db8 100644\n--- a/tests/memories_test.py\n+++ b/tests/memories_test.py\n@@ -25,7 +25,7 @@\n from jax import lax\n from jax._src import test_util as jtu\n from jax._src import xla_bridge as xb\n-from jax._src.layout import DeviceLocalLayout as DLL, Layout\n+from jax._src.layout import DeviceLocalLayout as DLL, Format\n from jax._src import config\n from jax.ad_checkpoint import checkpoint_name, checkpoint as new_checkpoint\n import jax.numpy as jnp\n@@ -1574,8 +1574,8 @@ def test_fn(x_in, y_in):\n     y = jnp.reshape(y, (16, 64))\n     custom_dll = DLL(major_to_minor=(0, 1), _tiling=((8, 128),))\n     custom_dll_linear = DLL(major_to_minor=(0, 1), _tiling=((1,),))\n-    x = jax.device_put(x, Layout(custom_dll, sharding))\n-    y = jax.device_put(y, Layout(custom_dll_linear, p_sharding))\n+    x = jax.device_put(x, Format(custom_dll, sharding))\n+    y = jax.device_put(y, Format(custom_dll_linear, p_sharding))\n \n     x1 = jnp.arange(0, 1024, dtype=jnp.float32)\n     x1 = jnp.reshape(x1, (16, 64))\n@@ -1585,8 +1585,8 @@ def test_fn(x_in, y_in):\n     jit_fn = jax.jit(\n         test_fn,\n         out_shardings=(\n-            Layout(custom_dll, sharding),\n-            Layout(custom_dll_linear, p_sharding),\n+            Format(custom_dll, sharding),\n+            Format(custom_dll_linear, p_sharding),\n         ),\n     )\n     x_out, y_out = jit_fn(x, y)\n@@ -1613,8 +1613,8 @@ def test_fn(x_in, y_in):\n     y = jnp.reshape(y, (32, 64))\n     custom_dll = DLL(major_to_minor=(0, 1), _tiling=((8, 128),))\n     custom_dll_linear = DLL(major_to_minor=(0, 1), _tiling=((1,),))\n-    x = jax.device_put(x, Layout(custom_dll, sharding))\n-    y = jax.device_put(y, Layout(custom_dll_linear, p_sharding))\n+    x = jax.device_put(x, Format(custom_dll, sharding))\n+    y = jax.device_put(y, Format(custom_dll_linear, p_sharding))\n \n     x1 = jnp.arange(0, 2048, dtype=jnp.float32)\n     x1 = jnp.reshape(x1, (32, 64))\n@@ -1624,8 +1624,8 @@ def test_fn(x_in, y_in):\n     jit_fn = jax.jit(\n         test_fn,\n         out_shardings=(\n-            Layout(custom_dll, sharding),\n-            Layout(custom_dll_linear, p_sharding),\n+            Format(custom_dll, sharding),\n+            Format(custom_dll_linear, p_sharding),\n         ),\n     )\n     x_out, y_out = jit_fn(x, y)\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex d37b21bd2460..751fd63823e3 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -55,7 +55,7 @@\n     SingleDeviceSharding, parse_flatten_op_sharding)\n from jax._src.pjit import (pjit, mesh_cast, auto_axes, explicit_axes,\n                            use_auto_axes, use_explicit_axes, reshard)\n-from jax._src.layout import Layout, DeviceLocalLayout as DLL\n+from jax._src.layout import Format, DeviceLocalLayout as DLL\n from jax._src.named_sharding import DuplicateSpecError\n from jax._src import mesh as mesh_lib\n from jax._src.mesh import AxisType\n@@ -4997,8 +4997,8 @@ def test_sds_update(self):\n \n     sh = NamedSharding(mesh, P())\n     s4 = jax.ShapeDtypeStruct((2, 2), jnp.int32,\n-                              sharding=Layout(DLL((0, 1)), sh))\n-    new_layout = Layout(DLL((1, 0)), NamedSharding(mesh, P('x')))\n+                              sharding=Format(DLL((0, 1)), sh))\n+    new_layout = Format(DLL((1, 0)), NamedSharding(mesh, P('x')))\n     s4_u = s4.update(sharding=new_layout)\n     self.assertEqual(s4_u.sharding, new_layout.sharding)\n     self.assertEqual(s4_u.layout, new_layout)\n\n```",
        "from_id": [
            "froystig",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[hijax] all pre-existing Box tests passing, still using typechange env\n\nCo-authored-by: Dougal Maclaurin <dougalm@google.com>",
        "output": "```diff\nCommit: 7eec8e1b6a6ed30fd8ce6a5d42134e0d2e8492aa\nDate: 2025-05-29T22:40:07Z\nURL: https://github.com/jax-ml/jax/commit/7eec8e1b6a6ed30fd8ce6a5d42134e0d2e8492aa\nFiles changed: 8\nAdditions: +665, Deletions: -200\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex b20b85a43b6e..24150aba6584 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,7 +88,8 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info', '_is_high', '_final_typechange_env']\n+               '_effects', '_debug_info', '_is_high',\n+               '_initial_typechange_env', '_final_typechange_env']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -97,6 +98,7 @@ class Jaxpr:\n   _effects: Effects\n   _debug_info: DebugInfo\n   _is_high: bool\n+  _initial_typechange_env: dict[Var, Any]\n   _final_typechange_env: dict[Var, Any]\n \n   @property\n@@ -127,6 +129,10 @@ def debug_info(self) -> DebugInfo:\n   def is_high(self) -> bool:\n     return self._is_high\n \n+  @property\n+  def initial_typechange_env(self) -> dict[Var, Any]:\n+    return self._initial_typechange_env\n+\n   @property\n   def final_typechange_env(self) -> dict[Var, Any]:\n     return self._final_typechange_env\n@@ -139,6 +145,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n                is_high: bool = False,\n+               initial_typechange_env: dict | None = None,\n                final_typechange_env: dict | None = None,\n                ):\n     \"\"\"\n@@ -165,6 +172,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n     self._is_high = is_high\n+    self._initial_typechange_env = initial_typechange_env or {}\n     self._final_typechange_env = final_typechange_env or {}\n \n   def __str__(self):\n@@ -193,6 +201,8 @@ def replace(self, **kwargs):\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n         is_high=kwargs.pop(\"is_high\", self.is_high),\n+        initial_typechange_env=kwargs.pop(\"initial_typechange_env\",\n+                                          self.initial_typechange_env),\n         final_typechange_env=kwargs.pop(\"final_typechange_env\",\n                                         self.final_typechange_env),\n     )\n@@ -222,6 +232,22 @@ def subjaxprs(jaxpr: Jaxpr) -> Iterator[Jaxpr]:\n     yield from jaxprs_in_params(eqn.params)\n \n \n+@dataclass(frozen=True)\n+class TypeChange:\n+  aval: AbstractValue\n+  initial_type_state: Any\n+  final_type_state: Any\n+\n+  def to_tangent_aval(self):\n+    return TypeChange(self.aval.to_tangent_aval(),\n+                      self.initial_type_state.to_tangent_aval(),\n+                      self.final_type_state.to_tangent_aval())\n+\n+  def normalize(self):\n+    return TypeChange(self.aval.normalize(),\n+                      self.initial_type_state.normalize(),\n+                      self.final_type_state.normalize())\n+\n class ClosedJaxpr:\n   __slots__ = ['__weakref__', '_jaxpr', '_consts']\n \n@@ -241,6 +267,13 @@ def __init__(self, jaxpr: Jaxpr, consts: Sequence):\n   def in_avals(self):\n     return [v.aval for v in self.jaxpr.invars]\n \n+  @property\n+  def in_avals_aug(self):\n+    ienv = self.jaxpr.initial_typechange_env\n+    fenv = self.jaxpr.final_typechange_env\n+    return [TypeChange(v.aval, ienv[v], fenv[v]) if v.aval.mutable else v.aval\n+            for v in self.jaxpr.invars]\n+\n   @property\n   def out_avals(self):\n     return [v.aval for v in self.jaxpr.outvars]\n@@ -542,10 +575,6 @@ def _true_bind(self, *args, **params):\n     # is called frequently and it's slightly faster to avoid using a context\n     # manager object.\n     prev_trace = trace_ctx.trace\n-\n-    if self.is_high(**params) and prev_trace.requires_low:\n-      return self.to_lojax(*args, **params)  # type: ignore\n-\n     trace_ctx.set_trace(eval_trace)\n     try:\n       return self.bind_with_trace(prev_trace, args, params)\n@@ -553,6 +582,11 @@ def _true_bind(self, *args, **params):\n       trace_ctx.set_trace(prev_trace)\n \n   def bind_with_trace(self, trace, args, params):\n+    # TODO(mattjj,dougalm): remove this block?\n+    if self.is_high(**params) and trace.requires_low:\n+      with set_current_trace(trace):\n+        return self.to_lojax(*args, **params)  # type: ignore\n+\n     return trace.process_primitive(self, args, params)\n \n   def def_impl(self, impl):\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 7cbdfff01462..0cd99a197f66 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -641,6 +641,9 @@ def to_concrete_value(self):\n   def get_referent(self):\n     return core.get_referent(self.primal)\n \n+  def type_state(self):\n+    return self.primal.type_state()\n+\n def _primal_tangent_shapes_match(primal, tangent):\n   if type(tangent) is not Zero:\n     primal_aval = get_aval(primal).strip_weak_type()\n@@ -1166,8 +1169,9 @@ def _jvp_jaxpr(jaxpr: core.ClosedJaxpr,\n                    debug_info=jaxpr.jaxpr.debug_info)\n   f_jvp, out_nonzeros = f_jvp_traceable(\n       jvp(f, instantiate=instantiate, transform_stack=False), nonzeros)\n-  tangent_avals = [aval.to_tangent_aval() for aval, nz in zip(jaxpr.in_avals, nonzeros) if nz]\n-  avals_in = list(it.chain(jaxpr.in_avals, tangent_avals))\n+  tangent_avals = [aval.to_tangent_aval()\n+                   for aval, nz in zip(jaxpr.in_avals_aug, nonzeros) if nz]\n+  avals_in = list(it.chain(jaxpr.in_avals_aug, tangent_avals))\n   jaxpr_out, avals_out, literals_out, () = pe.trace_to_jaxpr_dynamic(\n       f_jvp, avals_in)\n   return core.ClosedJaxpr(jaxpr_out, literals_out), out_nonzeros()\n@@ -1189,14 +1193,12 @@ def rearrange_binders(jaxpr: core.ClosedJaxpr, primals_in, tangents_in, primals_\n   new_invars = _perm(primals_in, tangents_in, jaxpr.jaxpr.invars)\n   new_outvars = _perm(primals_out, tangents_out, jaxpr.jaxpr.outvars)\n   new_debug_info = jaxpr.jaxpr.debug_info\n-  new_arg_names = tuple(_perm(primals_in, tangents_in,\n-                              jaxpr.jaxpr.debug_info.safe_arg_names(len(jaxpr.jaxpr.invars))))\n-  new_result_paths = tuple(_perm(primals_out, tangents_out,\n-                                  jaxpr.jaxpr.debug_info.safe_result_paths(len(jaxpr.jaxpr.outvars))))\n+  arg_names = jaxpr.jaxpr.debug_info.safe_arg_names(len(jaxpr.in_avals))\n+  result_paths = jaxpr.jaxpr.debug_info.safe_result_paths(len(jaxpr.out_avals))\n+  new_arg_names = tuple(_perm(primals_in, tangents_in, arg_names))\n+  new_result_paths = tuple(_perm(primals_out, tangents_out, result_paths))\n   new_debug_info = new_debug_info._replace(\n-      arg_names=new_arg_names,\n-      result_paths=new_result_paths,\n-  )\n+      arg_names=new_arg_names, result_paths=new_result_paths)\n   constvars = jaxpr.jaxpr.constvars\n   new_effects = pe._renumber_effects(\n       (*constvars, *new_invars), (*constvars, *jaxpr.jaxpr.invars),\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 3c499429a663..444b60f15fa5 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -896,10 +896,8 @@ def convert_constvars_jaxpr(jaxpr: Jaxpr) -> Jaxpr:\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   dbg = jaxpr.debug_info._replace(\n       arg_names=(\"\",) * len(jaxpr.constvars) + jaxpr.debug_info.arg_names)\n-  lifted_jaxpr = Jaxpr(constvars=(),\n-                       invars=jaxpr.constvars + jaxpr.invars,\n-                       outvars=jaxpr.outvars, eqns=jaxpr.eqns,\n-                       effects=jaxpr.effects, debug_info=dbg)\n+  lifted_jaxpr = jaxpr.replace(\n+      constvars=(), invars=jaxpr.constvars + jaxpr.invars, debug_info=dbg)\n   config.enable_checks.value and core.check_jaxpr(lifted_jaxpr)\n   return lifted_jaxpr\n \n@@ -1014,10 +1012,9 @@ def fun(*known_vals_in):\n     known_vals_out = [pval.get_known() for pval in out_pvals if pval.is_known()]\n     return [*known_vals_out, *residuals]\n \n-  known_avals = [a for a, uk in zip(jaxpr.in_avals, in_unknowns) if not uk]\n+  known_avals = [a for a, uk in zip(jaxpr.in_avals_aug, in_unknowns) if not uk]\n   jaxpr_known, _, consts_known, () = trace_to_jaxpr_dynamic(\n-      lu.wrap_init(fun, debug_info=f.debug_info),\n-      known_avals)\n+      lu.wrap_init(fun, debug_info=f.debug_info), known_avals)\n   (out_unknowns, jaxpr_unknown, res_avals), = cell  # pytype: disable=bad-unpacking\n \n   # check jaxpr_known and jaxpr_unknown in isolation\n@@ -1579,6 +1576,20 @@ def dce_jaxpr_closed_call_rule(used_outputs: list[bool], eqn: JaxprEqn\n def close_jaxpr(jaxpr: Jaxpr) -> ClosedJaxpr:\n   return ClosedJaxpr(jaxpr, ())\n \n+def move_invars_right(jaxpr: ClosedJaxpr, to_move: Sequence[bool]):\n+  return _move_invars_right(jaxpr, tuple(to_move))\n+\n+@weakref_lru_cache\n+def _move_invars_right(jaxpr: ClosedJaxpr, to_move: tuple[bool, ...]):\n+  invars, rest = split_list(jaxpr.jaxpr.invars, [len(to_move)])\n+  left_invars, right_invars = partition_list(to_move, invars)\n+  new_invars = [*left_invars, *right_invars, *rest]\n+  new_effs = _renumber_effects(\n+      (*jaxpr.jaxpr.constvars, *new_invars),\n+      (*jaxpr.jaxpr.constvars, *jaxpr.jaxpr.invars),\n+      jaxpr.jaxpr.effects)\n+  return jaxpr.replace(jaxpr=jaxpr.jaxpr.replace(invars=new_invars, effects=new_effs))\n+\n def move_binders_to_front(closed_jaxpr: ClosedJaxpr, to_move: Sequence[bool]\n                           ) -> ClosedJaxpr:\n   \"\"\"Reorder `invars` by moving those indicated in `to_move` to the front.\"\"\"\n@@ -1640,6 +1651,10 @@ def full_lower(self):\n     if val is None: return self\n     return core.full_lower(val)\n \n+  def type_state(self):\n+    var = self._trace.frame.tracer_to_var.get(id(self))\n+    return self._trace.frame.current_typechange_env[var]\n+\n   def _contents(self):\n     return ()\n \n@@ -1735,7 +1750,8 @@ class JaxprStackFrame:\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n   is_high: bool\n-  final_typechange_env: dict\n+  initial_typechange_env: dict\n+  current_typechange_env: dict\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1751,7 +1767,8 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_vars = []\n     self.debug_info = debug_info\n     self.is_high = False\n-    self.final_typechange_env = {}\n+    self.initial_typechange_env = {}\n+    self.current_typechange_env = {}\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1777,8 +1794,11 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n+    final_typechange_env = {v: s for v, s in self.current_typechange_env.items()\n+                            if v in self.initial_typechange_env}\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info, self.is_high, self.final_typechange_env)\n+                  debug_info, self.is_high, self.initial_typechange_env,\n+                  final_typechange_env)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1895,8 +1915,6 @@ def new_arg(self, aval, source_info: SourceInfo):\n     self.frame.tracers.append(tracer)\n     self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n     self.frame.invars.append(var)\n-    if aval.mutable:\n-      self.frame.final_typechange_env[var] = aval\n     return tracer\n \n   def new_const(self, c, source_info: SourceInfo):\n@@ -1921,6 +1939,8 @@ def _new_const(self, aval, c, source_info: SourceInfo) -> DynamicJaxprTracer:\n       self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n       self.frame.constid_to_tracer[id(c)] = tracer\n       self.frame.constvar_to_val[var] = c\n+      if aval.mutable:\n+        self.frame.initial_typechange_env[var] = c.type_state()\n     return tracer\n \n   def get_const(self, tracer) -> Any:\n@@ -2235,18 +2255,24 @@ def trace_to_jaxpr_dynamic(\n            list[tuple[PyTreeDef, PyTreeDef, tuple[Any, str, AttrKind]]]]:\n   keep_inputs = [True] * len(in_avals) if keep_inputs is None else keep_inputs\n   trace = DynamicJaxprTrace(fun.debug_info, lower=lower)\n+  in_avals_ = [a.aval if isinstance(a, core.TypeChange) else a for a in in_avals]\n   with core.ensure_no_leaks(trace), source_info_util.reset_name_stack():\n     source_info = source_info_util.current()\n     in_tracers = _input_type_to_tracers(\n-        partial(trace.new_arg, source_info=source_info), in_avals)\n+        partial(trace.new_arg, source_info=source_info), in_avals_)\n     in_tracers = [t for t, keep in zip(in_tracers, keep_inputs) if keep]\n+    trace.frame.initial_typechange_env = initial_typechange_env = {\n+        v: a.initial_type_state for v, a in zip(trace.frame.invars, in_avals)\n+        if isinstance(a, core.TypeChange)}\n+    trace.frame.current_typechange_env = dict(initial_typechange_env)\n+\n     try:\n       with core.set_current_trace(trace):\n         ans = fun.call_wrapped(*in_tracers)\n       _check_returned_jaxtypes(fun.debug_info, ans)\n       out_tracers = map(partial(trace.to_jaxpr_tracer, source_info=source_info), ans)\n       _check_no_returned_refs(fun.debug_info, out_tracers)\n-      jaxpr, consts, attrs_tracked = trace.to_jaxpr(out_tracers, fun.debug_info)\n+      jaxpr, consts, attrs_tracked = trace.frame.to_jaxpr(trace, out_tracers, fun.debug_info)\n       del fun, in_tracers, out_tracers, ans\n     finally:\n       trace.frame.reset_states(trace)\n@@ -2718,21 +2744,38 @@ def _linearize_of_pmap_hack(f: lu.WrappedFun, jaxpr, consts) -> tuple[Jaxpr, lis\n \n @weakref_lru_cache\n def lower_jaxpr(hi_jaxpr):\n-  in_avals = [lo_ty for t in hi_jaxpr.in_avals for lo_ty in t.lo_ty()]\n+  initial_env = hi_jaxpr.jaxpr.initial_typechange_env\n+  lo_avals = [lo_ty for v in hi_jaxpr.jaxpr.invars\n+              for lo_ty in (v.aval.lo_ty_(initial_env[v]) if v.aval.mutable\n+                            else v.aval.lo_ty())]\n   f = lu.wrap_init(partial(lower_traceable, hi_jaxpr),\n                    debug_info=hi_jaxpr.jaxpr.debug_info)\n-  lo_jaxpr, _, consts, () = trace_to_jaxpr_dynamic(f, in_avals, lower=True)\n-  return core.ClosedJaxpr(lo_jaxpr, consts)\n+  lo_jaxpr, _, lo_consts, () = trace_to_jaxpr_dynamic(f, lo_avals, lower=True)\n+  return core.ClosedJaxpr(lo_jaxpr, lo_consts)\n \n def lower_traceable(jaxpr, *lo_args):\n+  env = jaxpr.jaxpr.initial_typechange_env\n   lo_args_ = iter(lo_args)\n-  hi_args = [t.raise_val(*it.islice(lo_args_, len(t.lo_ty())))\n-             for t in jaxpr.in_avals]\n+  hi_args = [v.aval.raise_val(*it.islice(lo_args_, len(v.aval.lo_ty())))\n+             if not v.aval.mutable else\n+             v.aval.new_from_loval(env[v], *it.islice(lo_args_, len(v.aval.lo_ty_(env[v]))))\n+             for v in jaxpr.jaxpr.invars]\n   assert (problem := next(lo_args_, None)) is None\n   hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n   mut_outs = [lo_val for v, ty in jaxpr.jaxpr.final_typechange_env.items()\n-              for lo_val in ty.get(hi_args[in_idx[v]])]\n-  lo_outs = [lo_val for t, hi_val in zip(jaxpr.out_avals, hi_outs)\n-             for lo_val in t.lower_val(hi_val)]\n+              for lo_val in v.aval.read_loval(ty, hi_args[in_idx[v]])]\n+  lo_outs = [lo_val for v, hi_val in zip(jaxpr.jaxpr.outvars, hi_outs)\n+             for lo_val in v.aval.lower_val(hi_val)]\n   return mut_outs + lo_outs\n+\n+def convert_const_himutables(jaxpr):\n+  move = [core.typeof(c).mutable for c in jaxpr.consts]\n+  constvals, in_mutables = partition_list(move, jaxpr.consts)\n+  constvars, boxvars = partition_list(move, jaxpr.jaxpr.constvars)\n+  invars = *boxvars, *jaxpr.jaxpr.invars\n+  effects = make_jaxpr_effects(constvars, invars, jaxpr.jaxpr.outvars,\n+                               jaxpr.jaxpr.eqns)\n+  new_jaxpr = jaxpr.jaxpr.replace(constvars=constvars, invars=invars,\n+                                  effects=effects)\n+  return jaxpr.replace(jaxpr=new_jaxpr, consts=constvals), in_mutables\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex d0a22cd784b4..276e2b18cd40 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -1775,6 +1775,7 @@ def _move_mutable_consts(\n   constvars, mutvars = partition_list(hoist, jaxpr.constvars)\n   invars = (*jaxpr.invars, *mutvars)\n   effects = pe.make_jaxpr_effects(constvars, invars, jaxpr.outvars, jaxpr.eqns)\n+  # TODO(mattjj): debug_info must be updated...\n   jaxpr = core.Jaxpr(constvars, invars, jaxpr.outvars, jaxpr.eqns,\n                      effects, closed_jaxpr.jaxpr.debug_info)\n   return core.ClosedJaxpr(jaxpr, consts), in_mut\n@@ -2181,8 +2182,7 @@ def lower_sharding_computation(\n   The caller of this code can pass in a singleton UNSPECIFIED because the\n   number of out_avals might not be known at that time and\n   lower_sharding_computation calculates the number of out_avals so it can apply\n-  the singleton UNSPECIFIED to all out_avals.\n-  \"\"\"\n+  the singleton UNSPECIFIED to all out_avals.\"\"\"\n   auto_spmd_lowering = check_if_any_auto(\n       it.chain.from_iterable([in_shardings, out_shardings]))\n \ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex b9ce8ae09380..4df4c517090b 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -17,7 +17,7 @@\n from collections.abc import Callable, Sequence\n from functools import partial\n import inspect\n-import itertools\n+import itertools as it\n import operator\n from typing import Any, TypeVar\n import weakref\n@@ -438,11 +438,11 @@ def _merge_attrs_out(attrs_tracked, out_state, out_append):\n   out_attrs = []\n   for _, out_tree, (_, _, k) in attrs_tracked:\n     if k in (pe.ReadWrite, pe.BoxAttr):\n-      out_attrs.extend(itertools.islice(out_state_, out_tree.num_leaves))\n+      out_attrs.extend(it.islice(out_state_, out_tree.num_leaves))\n     elif k is pe.Append:\n       out_attrs.append(next(out_append_))\n     elif k is pe.ListAttr:\n-      out_attrs.extend(itertools.islice(out_append_, out_tree.num_leaves))\n+      out_attrs.extend(it.islice(out_append_, out_tree.num_leaves))\n     else:\n       assert False\n   assert next(out_state_, None) is next(out_append_, None) is None\n@@ -931,7 +931,7 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n   ys_avals = [core.unmapped_aval(length, 0, y_aval)\n               for y_aval in y_avals]\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(a), None)\n-                 for a in itertools.chain(carry_avals, ys_avals)]\n+                 for a in it.chain(carry_avals, ys_avals)]\n   del carry_avals, y_avals\n   # Create equation.\n   linear_unknown = tuple([False] * len(intensive_res) +\n@@ -1500,6 +1500,17 @@ def arrange_jaxpr_args_for_wrapped(args):\n   assert len(refs_out_matching_in_avals) == len(in_avals)\n   return refs_out_matching_in_avals, [*carry_out, *ys]\n \n+def _scan_staging(trace, *args, **params):\n+  outs = trace.default_process_primitive(scan_p, args, params)\n+  jaxpr = params['jaxpr']\n+  trace.frame.is_high = jaxpr.jaxpr.is_high\n+  invars = [trace.frame.tracer_to_var[id(t)] for t in args]\n+  var_map = dict(zip(jaxpr.jaxpr.invars, invars))\n+  final_env = {var_map[v]: ty for v, ty in\n+               jaxpr.jaxpr.final_typechange_env.items()}\n+  trace.frame.current_typechange_env.update(final_env)\n+  return outs\n+\n scan_p = core.Primitive(\"scan\")\n scan_p.multiple_results = True\n scan_p.skip_canonicalization = True\n@@ -1518,6 +1529,65 @@ def arrange_jaxpr_args_for_wrapped(args):\n pe.padding_rules[scan_p] = _scan_padding_rule\n pe.dce_rules[scan_p] = _scan_dce_rule\n state_discharge.register_partial_discharge_rule(scan_p)(_scan_state_partial_discharge_rule)\n+pe.custom_staging_rules[scan_p] = _scan_staging\n+\n+def _is_high(jaxpr, **_) -> bool:\n+  return jaxpr.jaxpr.is_high\n+scan_p.is_high = _is_high  # type: ignore\n+\n+def _to_lojax(*hi_args, jaxpr, num_carry, num_consts, linear, **params):\n+  ienv, fenv = jaxpr.jaxpr.initial_typechange_env, jaxpr.jaxpr.final_typechange_env\n+\n+  # move box binders and hi_args from consts slots to carry slots\n+  to_move = [t.mutable for t in jaxpr.in_avals[:num_consts]]\n+  jaxpr = pe.move_invars_right(jaxpr, to_move)\n+  hi_args = _move_right(hi_args, to_move)\n+  num_consts -= sum(to_move)\n+  num_carry += sum(to_move)\n+\n+  # expand num_consts, num_carry, linear according to lo types\n+  const_invars, carry_invars, _ = split_list(jaxpr.jaxpr.invars, [num_consts, num_carry])\n+  num_consts = sum(len(v.aval.lo_ty() if not v.aval.mutable\n+                       else v.aval.lo_ty_(ienv[v])) for v in const_invars)\n+  num_carry = sum(len(v.aval.lo_ty() if not v.aval.mutable\n+                      else v.aval.lo_ty_(ienv[v])) for v in carry_invars)\n+  linear = [l for v, l_ in zip(jaxpr.jaxpr.invars, linear)\n+            for l in (l_,) * len(v.aval.lo_ty() if not v.aval.mutable\n+                                 else v.aval.lo_ty_(ienv[v]))]\n+  lo_muts_out = sum(len(m.leaf_avals) for m in fenv.values())  # TODO hardcoded\n+\n+  # collect lo inputs values\n+  lo_args = [lo_val for v, x in zip(jaxpr.jaxpr.invars, hi_args)\n+             for lo_val in (v.aval.read_loval(ienv[v], x) if v.aval.mutable\n+                            else v.aval.lower_val(x))]\n+\n+  # lower the jaxpr and bind it using lo input values\n+  lo_jaxpr = pe.lower_jaxpr(jaxpr)\n+  all_outs = scan_p.bind(*lo_args, jaxpr=lo_jaxpr, num_consts=num_consts,\n+                         num_carry=num_carry, linear=tuple(linear), **params)\n+  out_mut, lo_outs = split_list(all_outs, [lo_muts_out])\n+\n+  # collect and apply mutations\n+  out_mut_ = iter(out_mut)\n+  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n+  for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n+    lo_vals = it.islice(out_mut_, len(var.aval.lo_ty_(ty)))\n+    var.aval.update_from_loval(ty, hi_args[in_idx[var]], *lo_vals)\n+  assert next(out_mut_, None) is None\n+\n+  # collect output values into hi types\n+  lo_outs_ = iter(lo_outs)\n+  hi_outs = [t.raise_val(*it.islice(lo_outs_, len(t.lo_ty())))\n+             for t in jaxpr.out_avals]\n+  assert next(lo_outs_, None) is None\n+\n+  return hi_outs\n+scan_p.to_lojax = _to_lojax\n+\n+def _move_right(lst, to_move):\n+  lst, rest = split_list(lst, [len(to_move)])\n+  left, right = partition_list(to_move, lst)\n+  return [*left, *right, *rest]\n \n def _propagate_mem_kind_scan(*xm, reverse, length, num_consts, num_carry, jaxpr,\n                              linear, unroll, _split_transpose):\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex d5286be8e0c9..c767647195b8 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -590,6 +590,8 @@ def _infer_params_impl(\n     in_type = in_avals = tuple(core.shaped_abstractify(x) for x in explicit_args)  # type: ignore\n   else:\n     in_type = in_avals  # type: ignore\n+    in_type = tuple(core.TypeChange(a, x.type_state(), None) if a.mutable  # type: ignore\n+                    else a for a, x in zip(in_type, explicit_args))\n   assert in_avals is not None\n \n   in_shardings_flat, in_layouts_flat = _process_in_axis_resources(\n@@ -705,7 +707,7 @@ def _infer_params_internal(\n   if entry.pjit_params is None:\n     p, args_flat = _infer_params_impl(\n         fun, ji, ctx_mesh, dbg, args, kwargs, in_avals=avals)\n-    if p.attrs_tracked or p.box_data:  # if attrs/boxes, don't populate cache\n+    if p.attrs_tracked or p.box_data or p.params['jaxpr'].jaxpr.is_high:\n       return p, p.consts + args_flat\n     entry.pjit_params = p\n   return entry.pjit_params, entry.pjit_params.consts + dynargs\n@@ -1407,16 +1409,14 @@ def _create_pjit_jaxpr(\n           lu.annotate(fun, cast(core.InputType, in_type)))\n       attrs_tracked = []\n     else:\n-      jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n-          fun, in_type)\n-      # assert attr_data is sentinel or attr_data matches attrs_tracked\n+      jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(fun, in_type)\n \n   if config.debug_key_reuse.value:\n     # Import here to avoid circular imports\n     from jax.experimental.key_reuse._core import check_key_reuse_jaxpr\n     check_key_reuse_jaxpr(jaxpr)\n \n-  if any(isinstance(c, core.Tracer) for c in consts):\n+  if any(isinstance(c, core.Tracer) or core.typeof(c).mutable for c in consts):\n     closed_jaxpr = pe.close_jaxpr(pe.convert_constvars_jaxpr(jaxpr))\n     final_consts = consts\n   else:\n@@ -1561,21 +1561,41 @@ def _is_high(jaxpr, **_) -> bool:\n   return jaxpr.jaxpr.is_high\n pjit_p.is_high = _is_high  # type: ignore\n \n-def _to_lojax( *hi_args, jaxpr, **params):\n-  params, num_mutants = _lojax_expand_params(jaxpr, **params)\n+def _to_lojax(*hi_args, jaxpr, **params):\n+  ienv, fenv = jaxpr.jaxpr.initial_typechange_env, jaxpr.jaxpr.final_typechange_env\n \n-  lo_args = [lo_val for t, hi_val in zip(jaxpr.in_avals, hi_args)\n-             for lo_val in t.lower_val(hi_val)]\n+  # convert closed-over boxes to explicit args\n+  jaxpr, closed_over_himutables = pe.convert_const_himutables(jaxpr)\n+  hi_args = [*closed_over_himutables, *hi_args]\n+  params = _converted_mutables_add_params(len(closed_over_himutables), **params)\n+\n+  # expand pjit params that must match number of lo inputs/outputs\n+  lo_nums_in = [len(v.aval.lo_ty() if not v.aval.mutable\n+                    else v.aval.lo_ty_(ienv[v]))\n+                for v in jaxpr.jaxpr.invars]\n+  lo_nums_out = [len(t.lo_ty()) for t in jaxpr.out_avals]\n+  lo_muts_out = sum(len(m.leaf_avals) for m in fenv.values())  # TODO hardcoded\n+  params = _lojax_expand_params(lo_nums_in, lo_nums_out, lo_muts_out, **params)\n+\n+  # collect lo input values\n+  lo_args = [lo_val for v, x in zip(jaxpr.jaxpr.invars, hi_args)\n+             for lo_val in (v.aval.read_loval(ienv[v], x) if v.aval.mutable\n+                            else v.aval.lower_val(x))]\n+\n+  # lower the jaxpr and bind it using lo input values\n   lo_jaxpr = pe.lower_jaxpr(jaxpr)\n   all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, **params)\n-  out_mut, lo_outs = split_list(all_outs, [num_mutants])\n+  out_mut, lo_outs = split_list(all_outs, [lo_muts_out])\n \n+  # collect and apply mutations\n   out_mut_ = iter(out_mut)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n   for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n-    ty.set(hi_args[in_idx[var]], *it.islice(out_mut_, len(ty.lo_ty())))\n+    lo_vals = it.islice(out_mut_, len(var.aval.lo_ty_(ty)))\n+    var.aval.update_from_loval(ty, hi_args[in_idx[var]], *lo_vals)\n   assert next(out_mut_, None) is None\n \n+  # collect output values into hi types\n   lo_outs_ = iter(lo_outs)\n   hi_outs = [t.raise_val(*it.islice(lo_outs_, len(t.lo_ty())))\n              for t in jaxpr.out_avals]\n@@ -1584,29 +1604,35 @@ def _to_lojax( *hi_args, jaxpr, **params):\n   return hi_outs\n pjit_p.to_lojax = _to_lojax\n \n+def _converted_mutables_add_params(\n+    n, *, donated_invars, in_shardings, in_layouts, **params):\n+  donated_invars = (False,) * n + donated_invars\n+  in_shardings = (UNSPECIFIED,) * n + in_shardings\n+  in_layouts = (None,) * n + in_layouts\n+  return dict(params, donated_invars=donated_invars, in_shardings=in_shardings,\n+              in_layouts=in_layouts)\n+\n def _lojax_expand_params(\n-    hi_jaxpr, *, donated_invars, in_shardings, in_layouts, out_shardings,\n-    out_layouts, **params):\n+    nums_in, nums_out, muts_out, *, donated_invars, in_shardings, in_layouts,\n+    out_shardings, out_layouts, **params):\n   # some pjit params match the length of hi_jaxpr.invars/outvars, so when\n   # lowering we must expand them to match their number of lojax types\n-  def expand(hi_tys, xs):\n-    return tuple(y for hi, x in zip(hi_tys, xs) for y in (x,) * len(hi.lo_ty()))\n-  donated_invars = expand(hi_jaxpr.in_avals , donated_invars)\n-  in_shardings   = expand(hi_jaxpr.in_avals , in_shardings  )\n-  in_layouts     = expand(hi_jaxpr.in_avals , in_layouts    )\n-  out_shardings  = expand(hi_jaxpr.out_avals, out_shardings )\n-  out_layouts    = expand(hi_jaxpr.out_avals, out_layouts   )\n+  def expand(ns, xs):\n+    return tuple(y for n, x in zip(ns, xs) for y in (x,) * n)\n+  donated_invars = expand(nums_in , donated_invars)\n+  in_shardings   = expand(nums_in , in_shardings  )\n+  in_layouts     = expand(nums_in , in_layouts    )\n+  out_shardings  = expand(nums_out, out_shardings )\n+  out_layouts    = expand(nums_out, out_layouts   )\n \n   # also, the lo_jaxpr has pure outputs corresponding to mutable hi_jaxpr types\n-  num_mutants = sum(len(hi_ty.lo_ty()) for hi_ty in\n-                    hi_jaxpr.jaxpr.final_typechange_env.values())\n-  out_shardings = (UNSPECIFIED,) * num_mutants + out_shardings\n-  out_layouts = (None,) * num_mutants + out_layouts\n+  out_shardings = (UNSPECIFIED,) * muts_out + out_shardings\n+  out_layouts = (None,) * muts_out + out_layouts\n \n   new_params = dict(params, donated_invars=donated_invars,\n                     in_shardings=in_shardings, in_layouts=in_layouts,\n                     out_shardings=out_shardings, out_layouts=out_layouts)\n-  return new_params, num_mutants\n+  return new_params\n \n \n def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n@@ -1948,6 +1974,7 @@ def pjit_staging_rule(trace, *args, **params):\n \n   jaxpr = params['jaxpr']\n   source_info = source_info_util.current()\n+  consts = []\n   if config.dynamic_shapes.value:\n     jaxpr, in_fwd, out_shardings, out_layouts = _pjit_forwarding(\n         jaxpr, params['out_shardings'], params['out_layouts'])\n@@ -1981,6 +2008,14 @@ def pjit_staging_rule(trace, *args, **params):\n         pjit_p, (*args, *consts), new_params)\n   else:\n     out_tracers = trace.default_process_primitive(pjit_p, args, params)\n+\n+  trace.frame.is_high = jaxpr.jaxpr.is_high\n+  invars = [trace.frame.tracer_to_var[id(t)] for t in it.chain(args, consts)]\n+  var_map = dict(zip(jaxpr.jaxpr.invars, invars))\n+  final_env = {var_map[v]: ty for v, ty in\n+               jaxpr.jaxpr.final_typechange_env.items()}\n+  trace.frame.current_typechange_env.update(final_env)\n+\n   return out_tracers\n pe.custom_staging_rules[pjit_p] = pjit_staging_rule\n \ndiff --git a/tests/attrs_test.py b/tests/attrs_test.py\nindex 60a3753a7ba5..90083626fb8e 100644\n--- a/tests/attrs_test.py\n+++ b/tests/attrs_test.py\n@@ -1056,7 +1056,7 @@ def f(x):\n     self.assertAllClose(box.get(), 2.0)\n \n   @parameterized.parameters([False, True])\n-  def test_grad_closrue_stop_gradient(self, jit):\n+  def test_grad_closure_stop_gradient(self, jit):\n     box = Box(0.0)\n \n     def f(x):\n@@ -1124,7 +1124,6 @@ def f(lst, x):\n       lst.append(2.0)\n       lst.append({'c': x + 3.0})\n \n-\n     tracing_ok = True\n     lst1 = List()\n     f(lst1, 0)\ndiff --git a/tests/hijax_test.py b/tests/hijax_test.py\nindex 21034d164d28..8b7d045c6c5a 100644\n--- a/tests/hijax_test.py\n+++ b/tests/hijax_test.py\n@@ -17,18 +17,19 @@\n from dataclasses import dataclass\n from functools import partial\n import itertools as it\n+from typing import Any\n import unittest\n \n-from absl.testing import absltest\n+from absl.testing import absltest, parameterized\n \n import jax\n import jax.numpy as jnp\n \n from jax._src import config\n from jax._src import core\n-from jax._src import dtypes\n from jax._src.interpreters import ad\n from jax._src.interpreters import partial_eval as pe\n+from jax._src import ad_util\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip, safe_map\n \n@@ -37,6 +38,8 @@\n map, unsafe_map = safe_map, map\n zip, unsafe_zip = safe_zip, zip\n \n+PyTreeDef = Any\n+\n \n # TODO(mattjj,dougalm): move HiPrimitive, Box, etc out of tests and into library\n class HiPrimitive(core.Primitive):\n@@ -65,124 +68,6 @@ def jvp(self, primals, tangents, **params):\n   def transpose(self, *args, **params):\n     assert False  # TODO\n \n-\n-class BoxTy(core.AbstractValue):\n-  mutable = True\n-\n-  def __init__(self, leaf_avals, treedef):\n-    self._leaf_avals = leaf_avals  # hijax avals\n-    self._treedef = treedef\n-\n-  # aval interface: hashability and str_short\n-  def __hash__(self):\n-    return hash((self._leaf_avals, self._treedef))\n-\n-  def __eq__(self, other):\n-    return (isinstance(other, BoxTy) and self._leaf_avals == other._leaf_avals\n-            and self._treedef == other._treedef)\n-\n-  def str_short(self, short_dtypes=False):\n-    return 'BoxTy'\n-\n-  # hijax interface: lower val, raise val, and low type\n-  def lo_ty(self):\n-    return [lo_aval for hi_aval in self._leaf_avals for lo_aval in hi_aval.lo_ty()]\n-\n-  def lower_val(self, box):\n-    leaf_vals, treedef = jax.tree.flatten(box._val)\n-    assert treedef == self._treedef\n-    return [lo_val for hi_aval, hi_val in zip(self._leaf_avals, leaf_vals)\n-            for lo_val in hi_aval.lower_val(hi_val)]\n-\n-  def raise_val(self, *lo_vals):\n-    lo_vals_ = iter(lo_vals)\n-    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n-               for hi_ty in self._leaf_avals]\n-    assert next(lo_vals_, None) is None\n-    return Box(jax.tree.unflatten(self._treedef, hi_vals))  # will be mutated\n-\n-  # mutable interface: get/set\n-  def get(self, box):\n-    leaf_vals, treedef = jax.tree.flatten(box._val)\n-    assert treedef == self._treedef\n-    return [lo_val for hi_ty, hi_val in zip(self._leaf_avals, leaf_vals)\n-            for lo_val in hi_ty.lower_val(hi_val)]\n-\n-  def set(self, box, *lo_vals):\n-    lo_vals_ = iter(lo_vals)\n-    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n-               for hi_ty in self._leaf_avals]\n-    assert next(lo_vals_, None) is None\n-    box._val = jax.tree.unflatten(self._treedef, hi_vals)\n-\n-  # TODO placeholder thing\n-  def to_tangent_aval(self):\n-    return core.ShapedArray((), dtypes.float0)  # TODO revise placeholder\n-\n-class Box:  # noqa: F811\n-  def __init__(self, val):\n-    self._val = val\n-\n-  @property\n-  def ty(self):\n-    leaves, treedef = jax.tree.flatten(self._val)\n-    leaf_avals = tuple(map(core.typeof, leaves))\n-    return BoxTy(leaf_avals, treedef)\n-core.pytype_aval_mappings[Box] = lambda b: b.ty\n-\n-\n-class BoxSet(HiPrimitive):\n-  multiple_results = True\n-\n-  def is_high(self, *, treedef) -> bool: return True\n-\n-  def staging(self, trace, box, *leaves, treedef):\n-    super().staging(trace, box, *leaves, treedef=treedef)\n-    avals = tuple(t.aval for t in leaves)\n-    trace.frame.final_typechange_env[trace.getvar(box)] = BoxTy(avals, treedef)\n-\n-  def abstract_eval(self, box_ty, *leaf_avals, treedef):\n-    return [], set()  # TODO better typechecking...\n-\n-  def to_lojax(_, box, *leaves, treedef):\n-    box._val = jax.tree.unflatten(treedef, leaves)\n-    return []\n-\n-  def jvp(_, primals, tangents, *, treedef):\n-    assert False  # TODO\n-\n-  def transpose(_, *args, treedef):\n-    assert False  # TODO\n-box_set_p = BoxSet('box_set')\n-\n-def box_set(box, val):\n-  leaves, treedef = jax.tree.flatten(val)\n-  box_set_p.bind(box, *leaves, treedef=treedef)\n-\n-\n-class BoxGet(HiPrimitive):\n-  multiple_results = True\n-\n-  def is_high(self) -> bool: return True\n-\n-  def abstract_eval(self, box_ty):\n-    return box_ty._leaf_avals, set()\n-\n-  def to_lojax(_, box):\n-    return jax.tree.leaves(box._val)\n-\n-  def jvp(_, primals, tangents):\n-    assert False  # TODO\n-\n-  def transpose(_, *args):\n-    assert False  # TODO\n-box_get_p = BoxGet('box_get')\n-\n-def box_get(box):\n-  leaf_vals = box_get_p.bind(box)\n-  return jax.tree.unflatten(core.typeof(box)._treedef, leaf_vals)\n-\n-\n class HijaxTest(jtu.JaxTestCase):\n \n   def test_custom_types_and_primitive(self):\n@@ -194,8 +79,6 @@ class MyArray:\n \n     @dataclass(frozen=True)\n     class MyTy(core.AbstractValue):\n-      mutable = False\n-\n       def to_tangent_aval(self):\n         return MyTy()\n       def str_short(self, short_dtypes=False):\n@@ -324,6 +207,392 @@ def f(x):\n     self.assertIsInstance(a_grad, MyArray)\n     self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n \n+\n+def new_box():\n+  (), treedef = jax.tree.flatten(None)\n+  return new_box_p.bind(treedef=treedef)\n+\n+def box_get(box):\n+  tys = box.type_state()\n+  leaf_vals = box_get_p.bind(box, avals=tys.leaf_avals)\n+  return jax.tree.unflatten(tys.treedef, leaf_vals)\n+\n+def box_set(box, val):\n+  leaves, treedef = jax.tree.flatten(val)\n+  box_set_p.bind(box, *leaves, treedef=treedef)\n+\n+@dataclass(frozen=True)\n+class BoxTypeState:\n+  leaf_avals: tuple[core.AbstractValue, ...]\n+  treedef: PyTreeDef\n+\n+  def to_tangent_aval(self):\n+    return BoxTypeState(tuple(a.to_tangent_aval() for a in self.leaf_avals),\n+                        self.treedef)\n+\n+  def normalize(self):\n+    return BoxTypeState(tuple(a.normalize() for a in self.leaf_avals),\n+                        self.treedef)\n+\n+class BoxTy(core.AbstractValue):\n+  mutable = True\n+\n+  # forwarded to value\n+  get = core.aval_method(box_get)\n+  set = core.aval_method(box_set)\n+\n+  # aval interface: hashability and str_short\n+  def __hash__(self): return hash(BoxTy)\n+  def __eq__(self, other): return isinstance(other, BoxTy)\n+\n+  def str_short(self, short_dtypes=False):\n+    return 'BoxTy'\n+\n+  # mutable interface\n+  def lo_ty_(self, box_state):\n+    return [lo_ty for t in box_state.leaf_avals for lo_ty in t.lo_ty()]\n+\n+  def new_from_loval(self, box_state: BoxTypeState, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in box_state.leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    return Box(jax.tree.unflatten(box_state.treedef, hi_vals))  # will be mutated\n+\n+  def read_loval(self, box_state: BoxTypeState, box):\n+    leaf_vals, treedef = jax.tree.flatten(box_get(box))\n+    assert treedef == box_state.treedef\n+    return [lo_val for hi_ty, hi_val in zip(box_state.leaf_avals, leaf_vals)\n+            for lo_val in hi_ty.lower_val(hi_val)]\n+\n+  def update_from_loval(self, box_state: BoxTypeState, box, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in box_state.leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    box_set(box, jax.tree.unflatten(box_state.treedef, hi_vals))\n+\n+  def to_tangent_aval(self):\n+    return BoxTy()\n+\n+class Box:  # noqa: F811\n+  def __init__(self, val):\n+    self._val = val\n+\n+  def get(self):\n+    return box_get(self)\n+\n+  def set(self, val):\n+    box_set(self, val)\n+\n+  @property\n+  def ty(self):\n+    return BoxTy()\n+\n+  def type_state(self):\n+    leaves, treedef = jax.tree.flatten(self._val)\n+    leaf_avals = tuple(map(core.typeof, leaves))\n+    return BoxTypeState(leaf_avals, treedef)\n+core.pytype_aval_mappings[Box] = lambda b: b.ty\n+\n+\n+class NewBox(HiPrimitive):\n+  def is_high(self, *, treedef) -> bool: return True\n+\n+  def staging(self, trace, *, treedef):\n+    tracer = super().staging(trace, treedef=treedef)\n+    var = trace.frame.tracer_to_var[id(tracer)]\n+    leaves, treedef = jax.tree.flatten(None)\n+    trace.frame.current_typechange_env[var] = BoxTypeState(leaves, treedef)\n+    return tracer\n+\n+  def abstract_eval(self, *, treedef):\n+    return BoxTy(), set()\n+\n+  def to_lojax(_, *, treedef):\n+    return Box(None)\n+\n+  def jvp(_, primals, tangents, *, treedef):\n+    assert False  # TODO\n+\n+  def transpose(_, *args, treedef):\n+    assert False  # TODO\n+new_box_p = NewBox('new_box')\n+\n+\n+class BoxSet(HiPrimitive):\n+  multiple_results = True\n+\n+  def is_high(self, *, treedef) -> bool: return True\n+\n+  def staging(self, trace, box_tracer, *leaves, treedef):\n+    super().staging(trace, box_tracer, *leaves, treedef=treedef)\n+    var = trace.getvar(box_tracer)\n+    avals = tuple(t.aval for t in leaves)\n+    trace.frame.current_typechange_env[var] = BoxTypeState(avals, treedef)\n+    return []\n+\n+  def abstract_eval(self, box_ty, *leaf_avals, treedef):\n+    return [], set()  # TODO better typechecking...\n+\n+  def to_lojax(_, box, *leaves, treedef):\n+    box._val = jax.tree.unflatten(treedef, leaves)\n+    return []\n+\n+  def jvp(_, primals, tangents, *, treedef):\n+    box, *vals = primals\n+    box_dot, *val_dots = tangents\n+    if type(box_dot) is ad_util.Zero:\n+      raise Exception(\"you're an idiot\")\n+    box_set_p.bind(box, *vals, treedef=treedef)\n+    box_set_p.bind(box_dot, *val_dots, treedef=treedef)\n+    return [], []\n+\n+  def transpose(_, *args, treedef):\n+    assert False  # TODO\n+box_set_p = BoxSet('box_set')\n+\n+\n+class BoxGet(HiPrimitive):\n+  multiple_results = True\n+\n+  def abstract_eval(self, box_ty, *, avals):\n+    return avals, set()\n+\n+  def to_lojax(_, box, *, avals):\n+    return jax.tree.leaves(box._val)\n+\n+  def jvp(_, primals, tangents, *, avals):\n+    (box,), (box_dot,) = primals, tangents\n+    return (box_get_p.bind(box, avals=avals),\n+            box_get_p.bind(box_dot, avals=[a.to_tangent_aval() for a in avals]))\n+\n+  def transpose(_, *args):\n+    assert False  # TODO\n+box_get_p = BoxGet('box_get')\n+\n+\n+\n+class BoxTest(jtu.JaxTestCase):\n+\n+  def test_jit_arg(self):\n+    @jax.jit\n+    def f(box, x):\n+      assert tracing_ok\n+      box.set(box.get() + x)\n+\n+    tracing_ok = True\n+    box1 = Box(1.0)\n+    f(box1, 1.)\n+    self.assertAllClose(box1.get(), 2.0)\n+\n+    tracing_ok = False\n+    box2 = Box(2.0)\n+    f(box2, 2.)\n+    self.assertAllClose(box2.get(), 4.0)\n+\n+  def test_jit_arg2(self):\n+    # set without get\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    box = Box(0.0)\n+    f(box, 1.)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+\n+  def test_jit_arg_in_pytree(self):\n+    @jax.jit\n+    def f(dct, x):\n+      assert tracing_ok\n+      box = dct['box']\n+      box.set(box.get() + x)\n+\n+    tracing_ok = True\n+    box1 = Box(1.0)\n+    f({'box': box1, 'a': 1.0}, 1.)\n+    self.assertAllClose(box1.get(), 2.0)\n+\n+    tracing_ok = False\n+    box2 = Box(2.0)\n+    f({'box': box2, 'a': 2.0}, 2.)\n+    self.assertAllClose(box2.get(), 4.0)\n+\n+    tracing_ok = True\n+    box3 = Box(3)  # int, dtype changed\n+    f({'box': box3, 'a': 2.0}, 2.)\n+    self.assertAllClose(box3.get(), 5.0)\n+\n+  def test_jit_closure(self):\n+    box = Box(1.0)\n+\n+    @jax.jit\n+    def f(x):\n+      assert tracing_ok\n+      box.set(box.get() + x)\n+\n+    tracing_ok = True\n+    f(2.0)\n+    self.assertAllClose(box.get(), 3.0)\n+    tracing_ok = False\n+    f(5.0)\n+    self.assertAllClose(box.get(), 8.0)\n+\n+  def test_jit_closure_nested(self):\n+    box = Box(5.0)\n+\n+    @jax.jit\n+    def f(x):\n+      box.set(box.get() + x)\n+\n+    @jax.jit\n+    def g(x):\n+      f(x)\n+\n+    g(3.0)\n+    self.assertAllClose(box.get(), 8.0)\n+\n+  def test_jit_closure_nested2(self):\n+    @jax.jit\n+    def h(x):\n+      box = new_box()\n+      box.set(x)\n+\n+      @jax.jit\n+      def k(x):\n+        box.set(box.get() + x)\n+\n+      k(1.0)\n+      k(1.0)\n+      return box.get()\n+\n+    ans = h(2.0)\n+    self.assertAllClose(ans, 4.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_jvp_closure_stop_gradient(self, jit):\n+    box = Box(1.0)\n+\n+    def f(x):\n+      y = 2 * x\n+      box.set(box.get() + jax.lax.stop_gradient(y))\n+      return y\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    y, y_dot = jax.jvp(f, (1.0,), (1.0,))\n+    self.assertAllClose(y, 2.0)\n+    self.assertAllClose(y_dot, 2.0)\n+    self.assertAllClose(box.get(), 3.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_jvp_arg(self, jit):\n+    def f(box, x):\n+      box.set(box.get() + x)\n+      return x\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    box = Box(5.0)\n+    box_dot = Box(1.0)\n+    y, y_dot = jax.jvp(f, (box, 2.), (box_dot, 1.))\n+    self.assertAllClose(y, 2.0)\n+    self.assertAllClose(y_dot, 1.0)\n+    self.assertAllClose(box.get(), 7.0)\n+    self.assertAllClose(box_dot.get(), 2.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_custom_vjp_plumbing(self, jit):\n+    box = Box(0.0)\n+\n+    @jax.custom_vjp\n+    def foo(x):\n+      return x\n+    def foo_fwd(x):\n+      return foo(x), None\n+    def foo_bwd(_, g):\n+      box.set(g)\n+      return g,\n+    foo.defvjp(foo_fwd, foo_bwd)\n+\n+    def f(x):\n+      x = 2 * x\n+      x = foo(x)\n+      x = 2 * x\n+      return x\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    jax.grad(f)(1.0)\n+    self.assertAllClose(box.get(), 2.0)\n+\n+  # TODO(mattjj,dougalm): make this work...\n+  # @parameterized.parameters([False, True])\n+  # def test_custom_vjp_plumbing_abstracted(self, jit):\n+  #   box = Box(0.0)\n+\n+  #   @jax.custom_vjp\n+  #   def foo(box, x):\n+  #     return x\n+  #   def foo_fwd(box, x):\n+  #     return x, box\n+  #   def foo_bwd(box, g):\n+  #     box.set(g)\n+  #     return None, g\n+  #   foo.defvjp(foo_fwd, foo_bwd)\n+\n+  #   def f(box, x):\n+  #     x = 2 * x\n+  #     x = foo(box, x)\n+  #     x = 2 * x\n+  #     return x\n+\n+  #   if jit:\n+  #     f = jax.jit(f)\n+\n+  #   jax.grad(partial(f, box))(1.0)\n+  #   self.assertAllClose(box.get(), 2.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_grad_closure_stop_gradient(self, jit):\n+    box = Box(0.0)\n+\n+    def f(x):\n+      y = x * 2\n+      box.set(box.get() + jax.lax.stop_gradient(y))\n+      return y\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    g = jax.grad(f)(1.0)\n+    self.assertAllClose(g, 2.0)\n+    self.assertAllClose(box.get(), 2.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_scan_basic(self, jit):\n+    box = Box(1.0)\n+\n+    def double_it_10():\n+      def body(_, __):\n+        box.set(box.get() * 2)\n+        return None, None\n+      _, _ = jax.lax.scan(body, None, None, length=10)\n+\n+    if jit:\n+      double_it_10 = jax.jit(double_it_10)\n+\n+    double_it_10()\n+    self.assertAllClose(box.get(), 1024., check_dtypes=False)\n+\n+  # TODO error-checking tests from attrs_test.py\n+\n+  ###\n+\n   def test_box_autodiff(self):\n     if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n \n@@ -336,7 +605,7 @@ def abstract_eval(_, box_aval, x_aval):\n         return x_aval, set()\n \n       def to_lojax(_, box, x):\n-        assert False  # TODO\n+        return x\n \n       def jvp(_, primals, tangents):\n         box, x = primals\n@@ -351,14 +620,6 @@ def transpose(self, *args):\n     def stash_tangents(box, x):\n       return stash_tangents_p.bind(box, x)\n \n-    @jax.jit\n-    def f(box, x):\n-      box_set(box, x)\n-\n-    box = Box(0.0)\n-    f(box, 1.)\n-    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n-\n     @jax.jit\n     def f(box, x):\n       x = stash_tangents(box, x)\n@@ -449,5 +710,26 @@ def f(box):\n     self.assertAllClose(b_.arr, 2, check_dtypes=False)\n \n \n+class ListTy(core.AbstractValue):\n+  mutable = True\n+\n+  # forwarded to value\n+  get = core.aval_method(box_get)\n+  set = core.aval_method(box_set)\n+\n+  # aval interface: hashability and str_short\n+  def __hash__(self): return hash(BoxTy)\n+  def __eq__(self, other): return isinstance(other, BoxTy)\n+\n+  def str_short(self, short_dtypes=False):\n+    return 'ListTy'\n+\n+  # TODO\n+\n+class ListTest(jtu.JaxTestCase):\n+  ...\n+\n+\n+\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add support for inout arguments\n\nThey are passed in between inputs and outputs to the kernel body and returned\nafter outputs from the kernel.\n\nPiperOrigin-RevId: 764898480",
        "output": "```diff\nCommit: 5b1372998c1c20f1e938ee4b715fd96d0cd54feb\nDate: 2025-05-29T22:00:43Z\nURL: https://github.com/jax-ml/jax/commit/5b1372998c1c20f1e938ee4b715fd96d0cd54feb\nFiles changed: 4\nAdditions: +59, Deletions: -14\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 84caff69a090..003fa0419f63 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -907,6 +907,7 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n           block=block,\n           in_shapes=(*in_shapes, *semaphores_shape),\n           out_shape=(*out_shapes, *semaphores_shape),\n+          inout_shape=(),\n           smem_scratch_shape=scratch_buffers,\n           lowering_semantics=lowering_semantics,\n           module_name=mlir.sanitize_name(debug_info.func_name),\ndiff --git a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\nindex a14ccbb7daa9..ccbe4d36edc9 100644\n--- a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n+++ b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n@@ -105,6 +105,7 @@ def zero_init_gmem_scratch():\n       *args, *scratch_args,\n       module=module,\n       out_types=lowering_result.new_out_shapes,\n+      inout_types=(),\n       input_output_aliases=input_output_aliases,\n       use_custom_barrier=False, # False until we add get_barrier_semaphore() feature\n   )\ndiff --git a/jax/experimental/mosaic/gpu/core.py b/jax/experimental/mosaic/gpu/core.py\nindex 193fd1bd3589..79a0cd56328b 100644\n--- a/jax/experimental/mosaic/gpu/core.py\n+++ b/jax/experimental/mosaic/gpu/core.py\n@@ -26,6 +26,7 @@\n from typing import Any, Callable, Generic, TypeVar\n import weakref\n \n+import itertools\n import jax\n from jax._src import sharding_impls\n from jax._src.interpreters import mlir\n@@ -124,9 +125,12 @@ def supports_cross_device_collectives():\n \n \n @mosaic_gpu_p.def_abstract_eval\n-def _mosaic_gpu_abstract_eval(*_, module, out_types):\n+def _mosaic_gpu_abstract_eval(*_, module, out_types, inout_types):\n   del module  # Unused.\n-  return [jax._src.core.ShapedArray(t.shape, t.dtype) for t in out_types]\n+  return [\n+      jax._src.core.ShapedArray(t.shape, t.dtype)\n+      for t in itertools.chain(out_types, inout_types)\n+  ]\n \n \n def _has_communication(module, **_):\n@@ -146,6 +150,7 @@ def _mosaic_gpu_lowering_rule(\n     *args,\n     module,\n     out_types,\n+    inout_types,\n     input_output_aliases: tuple[tuple[int, int], ...] = (),\n     use_custom_barrier: bool = False,\n ):\n@@ -170,8 +175,19 @@ def _mosaic_gpu_lowering_rule(\n     else:\n       raise NotImplementedError(f\"Unsupported sharding context: {axis_context}\")\n \n-  assert len(args) == len(ctx.avals_in)\n-  assert len(out_types) == len(ctx.avals_out)\n+  if inout_types:\n+    if input_output_aliases:\n+      raise ValueError(\n+          \"input_output_aliases and inout_types are mutually exclusive\"\n+      )\n+    num_inputs = len(ctx.avals_in)\n+    num_outputs = len(ctx.avals_out)\n+    input_output_aliases = tuple(\n+        (num_inputs - 1 - i, num_outputs - 1 - i)\n+        for i in range(len(inout_types))\n+    )\n+  assert len(ctx.avals_in) == len(args)\n+  assert len(ctx.avals_out) == len(out_types) + len(inout_types)\n   module = _run_serde_pass(\n       module,\n       serialize=True,\n@@ -562,6 +578,7 @@ def _lower_as_gpu_kernel(\n     block: tuple[int, int, int],\n     in_shapes: tuple[Any, ...],\n     out_shape,\n+    inout_shape,\n     smem_scratch_shape: ShapeTree | Union[ShapeTree],\n     lowering_semantics: LoweringSemantics,\n     module_name: str,\n@@ -576,13 +593,14 @@ def _shape_to_ref_ty(shape: jax.ShapeDtypeStruct) -> ir.MemRefType:\n     return ir.MemRefType.get(shape.shape, utils.dtype_to_ir_type(shape.dtype))\n \n   in_ref_tys = [_shape_to_ref_ty(t) for t in in_shapes]\n+  inout_ref_tys = [_shape_to_ref_ty(t) for t in inout_shape]\n \n   unwrap_output_tuple = False\n   if isinstance(out_shape, list):\n     out_shape = tuple(out_shape)\n   elif not isinstance(out_shape, tuple):\n     out_shape = (out_shape,)\n-    unwrap_output_tuple = True\n+    unwrap_output_tuple = not inout_shape\n   out_ref_tys = [_shape_to_ref_ty(t) for t in out_shape]\n   if prof_spec is not None:\n     out_shape = (*out_shape, prof_spec.jax_buffer_type(grid, block))\n@@ -610,19 +628,18 @@ def main(token_ptr, buffers):\n       nonlocal launch_ctx\n       token = builtin.unrealized_conversion_cast([token_ty], [token_ptr])\n       arg_refs = []\n-      for i, ref_ty in enumerate([*in_ref_tys, *out_ref_tys]):\n+      # XLA will pass in inout refs again as outputs, but we ignore them.\n+      for i, ref_ty in enumerate([*in_ref_tys, *inout_ref_tys, *out_ref_tys]):\n         ptr = llvm.LoadOp(ptr_ty, llvm.GEPOp(ptr_ty, buffers, [], [i], ptr_ty, llvm.GEPNoWrapFlags.none))\n         arg_refs.append(utils.ptr_as_memref(ptr, ir.MemRefType(ref_ty)))\n-      in_refs = arg_refs[:len(in_ref_tys)]\n-      out_refs = arg_refs[len(in_ref_tys):]\n-      prof_buffer = out_refs.pop() if prof_spec is not None else None\n+      prof_buffer = arg_refs.pop() if prof_spec is not None else None\n       with _launch(\n           token, grid, cluster, block, smem_scratch_shape,\n           lowering_semantics, module, prof_spec, prof_buffer\n       ) as (_launch_ctx, smem_refs):\n         nonlocal launch_ctx\n         launch_ctx = _launch_ctx\n-        body(launch_ctx, *in_refs, *out_refs, smem_refs)\n+        body(launch_ctx, *arg_refs, smem_refs)\n     main.func_op.attributes[\"llvm.emit_c_interface\"] = ir.UnitAttr.get()\n   sym_tab = ir.SymbolTable(module.operation)\n   sym_tab.insert(main.func_op)\n@@ -680,16 +697,22 @@ def as_gpu_kernel(\n     kernel_name: str | None = None,\n     ir_version: int | None = None,\n     thread_semantics: LoweringSemantics = LoweringSemantics.Lane,\n+    inout_shape = (),\n ):\n   if isinstance(in_shape, list):\n     in_shape = tuple(in_shape)\n   elif not isinstance(in_shape, tuple):\n     in_shape = (in_shape,)\n+  if isinstance(inout_shape, list):\n+    inout_shape = tuple(inout_shape)\n+  elif not isinstance(inout_shape, tuple):\n+    inout_shape = (inout_shape,)\n \n   module, out_shape, unwrap_output_tuple, launch_ctx = (\n       _lower_as_gpu_kernel(\n-          body, grid, cluster, block, in_shape, out_shape, smem_scratch_shape,\n-          thread_semantics, module_name, kernel_name, prof_spec\n+          body, grid, cluster, block, in_shape, out_shape, inout_shape,\n+          smem_scratch_shape, thread_semantics, module_name, kernel_name,\n+          prof_spec\n       )\n   )\n \n@@ -711,7 +734,7 @@ def as_gpu_kernel(\n   if launch_ctx.is_device_collective and not supports_cross_device_collectives():\n     raise RuntimeError(\"Kernel is a cross-device collective but no support is available.\")\n \n-  expected_arg_tys, expected_arg_treedef = jax.tree.flatten(in_shape)\n+  expected_arg_tys, expected_arg_treedef = jax.tree.flatten((*in_shape, *inout_shape))\n   def _check_args(*args):\n     arg_treedef = jax.tree.structure(args)\n     if arg_treedef != expected_arg_treedef:\n@@ -735,7 +758,7 @@ def _check_args(*args):\n         )\n \n   def bind(*args) -> Any:\n-    return mosaic_gpu_p.bind(*args, module=module, out_types=out_shape)\n+    return mosaic_gpu_p.bind(*args, module=module, out_types=out_shape, inout_types=inout_shape)\n \n   if prof_spec is not None:\n     @jax.jit\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 99b7d67cd691..a56aa04f6f60 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -3543,6 +3543,26 @@ def test_pass_is_registered(self):\n       pipeline.run(module.operation)\n \n \n+class ApiTest(TestCase):\n+\n+  def test_inout(self):\n+    def kernel(ctx, src, inout, dst, smem):\n+      val = memref.load(inout, [])\n+      gpu.barrier()\n+      new_val = arith.constant(ir.IntegerType.get_signless(32), 42)\n+      memref.store(new_val, inout, [])\n+      x = mgpu.FragmentedArray.load_strided(src, is_signed=True)\n+      (x + val).store_untiled(dst)\n+    x = jnp.arange(128, dtype=jnp.int32)\n+    y = jnp.asarray(2.0, dtype=jnp.int32)\n+    kernel = mgpu.as_gpu_kernel(\n+        kernel, (1, 1, 1), (128, 1, 1), x, x, (), inout_shape=y,\n+    )\n+    xo, yo = kernel(x, y)\n+    np.testing.assert_array_equal(xo, x + 2.0)\n+    np.testing.assert_array_equal(yo, jnp.asarray(42, dtype=jnp.int32))\n+\n+\n if hp is not None:\n   @hps.composite\n   def tiled_layouts(\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas] Add a base class for custom BufferedRef implementations.\n\nPiperOrigin-RevId: 764889905",
        "output": "```diff\nCommit: 81769209c08fe6be844ab703c35e7eb31bc1c54b\nDate: 2025-05-29T21:40:22Z\nURL: https://github.com/jax-ml/jax/commit/81769209c08fe6be844ab703c35e7eb31bc1c54b\nFiles changed: 2\nAdditions: +149, Deletions: -100\ndiff --git a/jax/_src/pallas/mosaic/pipeline.py b/jax/_src/pallas/mosaic/pipeline.py\nindex 4ef22179260b..f4dab313fb6f 100644\n--- a/jax/_src/pallas/mosaic/pipeline.py\n+++ b/jax/_src/pallas/mosaic/pipeline.py\n@@ -242,9 +242,134 @@ def _get_dim_size(bd):\n   block_shape_nones = tuple(_get_dim_size(x) for x in spec.block_shape)\n   return tuple(x for x in block_shape_nones if x is not None)\n \n+\n+class BufferedRefBase:\n+  \"\"\"Abstract interface for BufferedRefs.\"\"\"\n+\n+  @property\n+  def spec(self) -> pl.BlockSpec:\n+    raise NotImplementedError()\n+\n+  @property\n+  def buffer_type(self) -> BufferType:\n+    raise NotImplementedError()\n+\n+  @property\n+  def is_input(self):\n+    return self.buffer_type in [\n+        BufferType.INPUT,\n+        BufferType.ACCUMULATOR,\n+        BufferType.INPUT_OUTPUT,\n+    ]\n+\n+  @property\n+  def is_output(self):\n+    return self.buffer_type in [\n+        BufferType.OUTPUT,\n+        BufferType.ACCUMULATOR,\n+        BufferType.INPUT_OUTPUT,\n+    ]\n+\n+  @property\n+  def is_accumulator(self):\n+    return self.buffer_type == BufferType.ACCUMULATOR\n+\n+  @property\n+  def is_input_output(self):\n+    return self.buffer_type == BufferType.INPUT_OUTPUT\n+\n+  @property\n+  def is_manual(self):\n+    return self.buffer_type == BufferType.MANUAL\n+\n+  def init_slots(self):\n+    \"\"\"Initialize slot indices.\"\"\"\n+    raise NotImplementedError()\n+\n+  def swap_slots(self):\n+    \"\"\"Switch to the next slot.\"\"\"\n+    raise NotImplementedError()\n+\n+  @property\n+  def block_shape(self) -> Sequence[pl.BlockDim | int | None] | None:\n+    return self.spec.block_shape\n+\n+  @property\n+  def compute_index(self):\n+    return self.spec.index_map\n+\n+  def get_dma_slice(self, src_shape, src_dtype, grid_indices):\n+    # We need to handle blocks that might go OOB in the src array. An in bounds\n+    # block looks like this (for array shape (600, 600) and block shape\n+    # (256, 256)):\n+    #\n+    # +--------------+------------------|\n+    # | Block (0,0)  |                  |\n+    # | (256, 256)   |                  |\n+    # +--------------+                  |\n+    # |    A (600, 600)                 |\n+    # |                                 |\n+    # +---------------------------------+\n+    #\n+    # For in-bounds blocks, we don't need to do anything special.\n+    # An out-of-bounds block looks like this:\n+    #\n+    # +--------------+------------------|\n+    # |                                 |\n+    # |                                 |\n+    # +                                 |\n+    # |    A (600, 600)                 |\n+    # +--------------+                  |\n+    # | Block (2,0)  |                  |\n+    # + --------------------------------|\n+    # | XXXXXXXXXX   |\n+    # +--------------+\n+    # where the X's indicate where the block is out of bounds.\n+    #\n+    # When we have an out of bounds block like this, we need to truncate it to\n+    # a tile boundary (tiles are (8, 128) along the two minormost dimensions).\n+    # In this case, we'll have a block that is indexing the\n+    # 512:768 elements of A along the first dimension. We need to convert 768\n+    # into 600 (600 % 8 == 0), so our indexing will look like this:\n+\n+    # +--------------+------------------|\n+    # |                                 |\n+    # |                                 |\n+    # +                                 |\n+    # |    A (600, 600)                 |\n+    # +--------------+                  |\n+    # | Block (2,0)  |                  |\n+    # + --------------------------------|\n+    # where it is now a (88, 256) sized block.\n+    #\n+    # Suppose A is now (601, 600), instead of picking a (88, 256)-sized block\n+    # for the last iteration on that dimension, we will pick the next highest\n+    # tile multiple, i.e. (96, 256).\n+    if len(src_shape) < 2:\n+      raise NotImplementedError(\"Must use >1D values.\")\n+\n+    tiling = _make_tiling(src_shape, src_dtype)\n+    block_indices = self.compute_index(*grid_indices)\n+    return tuple(\n+        _make_block_slice(bi, bs, ss, t)\n+        for bi, bs, ss, t in zip(\n+            block_indices, self.block_shape, src_shape, tiling, strict=True\n+        )\n+    )\n+\n+  def bind_existing_ref(self, window_ref, indices):\n+    \"\"\"For handling VMEM references, the pipeline aliases the existing ref.\"\"\"\n+    del window_ref, indices\n+    return self\n+\n+  def with_spec(self, spec: pl.BlockSpec) -> 'BufferedRefBase':\n+    \"\"\"Returns a new BufferedRefBase with the given block spec.\"\"\"\n+    raise NotImplementedError()\n+\n+\n @tree_util.register_pytree_node_class\n @dataclasses.dataclass(frozen=True)\n-class BufferedRef:\n+class BufferedRef(BufferedRefBase):\n   \"\"\"A helper class to automate VMEM double buffering in pallas pipelines.\n \n   Attributes:\n@@ -257,7 +382,6 @@ class BufferedRef:\n       reference, this simply points to the existing ref.\n     accum_ref: accumulating buffer used by accumulator BufferedRefs.\n     current_slot: current slot index to the working buffer.\n-    next_slot: slot that will point to the working buffer in the next iteration.\n     sem_recvs: Double buffered semaphores for input DMAs.\n     sem_sends: Double buffered semaphores for output DMAs.\n     block_shape: passthrough property for the BlockSpec's block_shape.\n@@ -272,33 +396,37 @@ class BufferedRef:\n     swap: Tracks whether the BufferedRef slots need to be swapped before next\n       copy.\n   \"\"\"\n-  spec: pl.BlockSpec       # static metadata\n+  _spec: pl.BlockSpec       # static metadata\n   dtype: Any               # static metadata\n-  buffer_type: BufferType  # static metadata\n+  _buffer_type: BufferType  # static metadata\n   window_ref: ArrayRef | None\n   accum_ref: ArrayRef | None\n   current_slot: ArrayRef | None\n-  # TODO(ramiroleal): Unused by class. Remove argument from\n-  # BufferedRef instantiations.\n-  next_slot: ArrayRef | None\n   sem_recvs: SemaphoreTuple | None\n   sem_sends: SemaphoreTuple | None\n   # TODO(ramiroleal): Improve prefetch/postyeet interface to avoid\n   # using this ref.\n   swap: ArrayRef | None\n \n+  @property\n+  def spec(self):\n+    return self._spec\n+\n+  @property\n+  def buffer_type(self):\n+    return self._buffer_type\n+\n   def tree_flatten(self):\n     return (\n         (\n             self.window_ref,\n             self.accum_ref,\n             self.current_slot,\n-            self.next_slot,\n             self.sem_recvs,\n             self.sem_sends,\n             self.swap,\n         ),\n-        (self.spec, self.dtype, self.buffer_type),\n+        (self._spec, self.dtype, self._buffer_type),\n     )\n \n   @classmethod\n@@ -334,13 +462,12 @@ def create(cls, spec: pl.BlockSpec, dtype, buffer_type, needs_swap_ref=True\n       # reference is already in VMEM, we just need allocate the accumulation\n       # buffer and we will refer to the original reference slices directly.\n       return cls(\n-          spec=spec,\n+          _spec=spec,\n           dtype=dtype,\n-          buffer_type=buffer_type,\n+          _buffer_type=buffer_type,\n           window_ref=None,  # to be bound to existing ref by the pipeline routine\n           accum_ref=accum_ref,\n           current_slot=None,\n-          next_slot=None,\n           sem_recvs=None,\n           sem_sends=None,\n           swap=None,\n@@ -348,13 +475,12 @@ def create(cls, spec: pl.BlockSpec, dtype, buffer_type, needs_swap_ref=True\n     else:\n       memory_space = SMEM if spec.memory_space == SMEM else VMEM\n       return cls(\n-          spec=spec,\n+          _spec=spec,\n           dtype=dtype,\n-          buffer_type=buffer_type,\n+          _buffer_type=buffer_type,\n           window_ref=memory_space((2,) + block_shape, dtype),\n           accum_ref=accum_ref,\n           current_slot=SMEM((1,), jnp.int32),\n-          next_slot=None,\n           sem_recvs=(\n               None\n               if buffer_type is BufferType.OUTPUT\n@@ -396,6 +522,10 @@ def compute_index(self):\n   def memory_space(self):\n     return self.spec.memory_space\n \n+  def with_spec(self, spec: pl.BlockSpec) -> 'BufferedRef':\n+    \"\"\"Returns a new BufferedRef with the given block spec.\"\"\"\n+    return dataclasses.replace(self, _spec=spec)\n+\n   @property\n   def current_ref(self):\n     buffer_slice = tuple(\n@@ -409,30 +539,6 @@ def current_ref(self):\n     else:\n       return self.window_ref.at[(self.current_slot_index, *buffer_slice)]\n \n-  @property\n-  def is_input(self):\n-    return self.buffer_type in [\n-        BufferType.INPUT,\n-        BufferType.ACCUMULATOR,\n-        BufferType.INPUT_OUTPUT,\n-    ]\n-\n-  @property\n-  def is_output(self):\n-    return self.buffer_type in [\n-        BufferType.OUTPUT,\n-        BufferType.ACCUMULATOR,\n-        BufferType.INPUT_OUTPUT,\n-    ]\n-\n-  @property\n-  def is_accumulator(self):\n-    return self.buffer_type == BufferType.ACCUMULATOR\n-\n-  @property\n-  def is_input_output(self):\n-    return self.buffer_type == BufferType.INPUT_OUTPUT\n-\n   @property\n   def current_slot_index(self):\n     \"\"\"Index in double buffer corresponding to the current slot.\"\"\"\n@@ -491,65 +597,6 @@ def swap_slots(self):\n     if self.swap is not None:\n       self.swap[0] = False\n \n-  def get_dma_slice(self, src_shape, src_dtype, grid_indices):\n-    # We need to handle blocks that might go OOB in the src array. An in bounds\n-    # block looks like this (for array shape (600, 600) and block shape\n-    # (256, 256)):\n-    #\n-    # +--------------+------------------|\n-    # | Block (0,0)  |                  |\n-    # | (256, 256)   |                  |\n-    # +--------------+                  |\n-    # |    A (600, 600)                 |\n-    # |                                 |\n-    # +---------------------------------+\n-    #\n-    # For in-bounds blocks, we don't need to do anything special.\n-    # An out-of-bounds block looks like this:\n-    #\n-    # +--------------+------------------|\n-    # |                                 |\n-    # |                                 |\n-    # +                                 |\n-    # |    A (600, 600)                 |\n-    # +--------------+                  |\n-    # | Block (2,0)  |                  |\n-    # + --------------------------------|\n-    # | XXXXXXXXXX   |\n-    # +--------------+\n-    # where the X's indicate where the block is out of bounds.\n-    #\n-    # When we have an out of bounds block like this, we need to truncate it to\n-    # a tile boundary (tiles are (8, 128) along the two minormost dimensions).\n-    # In this case, we'll have a block that is indexing the\n-    # 512:768 elements of A along the first dimension. We need to convert 768\n-    # into 600 (600 % 8 == 0), so our indexing will look like this:\n-\n-    # +--------------+------------------|\n-    # |                                 |\n-    # |                                 |\n-    # +                                 |\n-    # |    A (600, 600)                 |\n-    # +--------------+                  |\n-    # | Block (2,0)  |                  |\n-    # + --------------------------------|\n-    # where it is now a (88, 256) sized block.\n-    #\n-    # Suppose A is now (601, 600), instead of picking a (88, 256)-sized block\n-    # for the last iteration on that dimension, we will pick the next highest\n-    # tile multiple, i.e. (96, 256).\n-    if len(src_shape) < 2:\n-      raise NotImplementedError(\"Must use >1D values.\")\n-\n-    tiling = _make_tiling(src_shape, src_dtype)\n-    block_indices = self.compute_index(*grid_indices)\n-    return tuple(\n-        _make_block_slice(bi, bs, ss, t)\n-        for bi, bs, ss, t in zip(\n-            block_indices, self.block_shape, src_shape, tiling, strict=True\n-        )\n-    )\n-\n   def copy_in(self, src_ref, grid_indices):\n     \"\"\"Starts copy of HBM dma slice into the current slot.\"\"\"\n     assert self.is_input\n@@ -674,7 +721,8 @@ def accumulate(self):\n # Helper to tree map over BufferedRefs as leaves.\n map_brefs = functools.partial(\n     jax.tree.map,\n-    is_leaf=lambda x: isinstance(x, BufferedRef))\n+    is_leaf=lambda x: isinstance(x, BufferedRefBase)\n+)\n \n \n def _filter_indices(\n@@ -922,7 +970,7 @@ def _end():\n         buffered_ref.wait_out(dst_ref, self.indices)\n \n   def swap_slots(self, buffered_ref, hbm_ref, schedule=None):\n-    if buffered_ref.swap is not None:\n+    if isinstance(buffered_ref, BufferedRef) and buffered_ref.swap is not None:\n       swap = buffered_ref.swap[0]\n     else:\n       # If we are not using an SMEM `swap` tensor to keep track of\ndiff --git a/jax/experimental/pallas/tpu.py b/jax/experimental/pallas/tpu.py\nindex 401b2fe66c45..e27fdaaadd8f 100644\n--- a/jax/experimental/pallas/tpu.py\n+++ b/jax/experimental/pallas/tpu.py\n@@ -30,6 +30,7 @@\n from jax._src.pallas.mosaic.helpers import run_on_first_core as run_on_first_core\n from jax._src.pallas.mosaic.lowering import LoweringException as LoweringException\n from jax._src.pallas.mosaic.pipeline import BufferedRef as BufferedRef\n+from jax._src.pallas.mosaic.pipeline import BufferedRefBase as BufferedRefBase\n from jax._src.pallas.mosaic.pipeline import emit_pipeline as emit_pipeline\n from jax._src.pallas.mosaic.pipeline import emit_pipeline_with_allocations as emit_pipeline_with_allocations\n from jax._src.pallas.mosaic.pipeline import get_pipeline_schedule as get_pipeline_schedule\n\n```",
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add non-collective blackwell matmul example\n\nPiperOrigin-RevId: 764889122",
        "output": "```diff\nCommit: a808fe89efcd4929c8c84bb81bd04156b9199e9e\nDate: 2025-05-29T21:38:12Z\nURL: https://github.com/jax-ml/jax/commit/a808fe89efcd4929c8c84bb81bd04156b9199e9e\nFiles changed: 4\nAdditions: +341, Deletions: -0\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex a56733a89f60..84caff69a090 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -1543,6 +1543,8 @@ def _slice_lowering_rule(\n \n \n @register_lowering_rule(lax.select_n_p, mgpu.LoweringSemantics.Lane)\n+@register_lowering_rule(lax.select_n_p, mgpu.LoweringSemantics.Lane,\n+                        gpu_core.PrimitiveSemantics.Warp)\n @register_lowering_rule(lax.select_n_p, mgpu.LoweringSemantics.Warpgroup)\n def _select_n_lowering_rule(ctx: LoweringRuleContext, pred, *cases):\n   if len(cases) != 2:\n@@ -1551,6 +1553,10 @@ def _select_n_lowering_rule(ctx: LoweringRuleContext, pred, *cases):\n         f\" {len(cases)}\"\n     )\n   pred_aval, *cases_avals = ctx.avals_in\n+  if ctx.module_ctx.primitive_semantics == gpu_core.PrimitiveSemantics.Warp:\n+    if not all(aval.shape == () for aval in ctx.avals_in):\n+      raise NotImplementedError(\n+          \"Can only select on scalars in warp-level lowering.\")\n   [out_aval] = ctx.avals_out\n   if ctx.module_ctx.lowering_semantics == mgpu.LoweringSemantics.Lane:\n     pred = _ensure_fa(pred, pred_aval.dtype)\ndiff --git a/jax/experimental/pallas/ops/gpu/blackwell_matmul_mgpu.py b/jax/experimental/pallas/ops/gpu/blackwell_matmul_mgpu.py\nnew file mode 100644\nindex 000000000000..df8365f843a8\n--- /dev/null\n+++ b/jax/experimental/pallas/ops/gpu/blackwell_matmul_mgpu.py\n@@ -0,0 +1,230 @@\n+# Copyright 2025 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Matrix Multiplication kernel for Blackwell GPUs.\"\"\"\n+import dataclasses\n+import functools\n+import itertools\n+import jax\n+from jax import lax\n+from jax._src import test_util as jtu  # noqa: F401\n+from jax.experimental.mosaic.gpu import profiler\n+import jax.experimental.pallas as pl\n+import jax.experimental.pallas.mosaic_gpu as plgpu\n+import jax.numpy as jnp\n+import numpy as np\n+\n+\n+@dataclasses.dataclass(frozen=True)\n+class TuningConfig:\n+  block_m: int\n+  block_n: int\n+  block_k: int\n+  max_concurrent_steps: int\n+  collective: bool\n+\n+\n+def _find_swizzle(dim_size_bits: int):\n+  \"\"\"Finds the largest swizzle that fits the dimension size.\"\"\"\n+  for swizzle_bytes in (128, 64, 32, 16):\n+    if dim_size_bits % (swizzle_bytes * 8) == 0:\n+      return swizzle_bytes\n+  raise ValueError(\n+      f\"Dimension size has {dim_size_bits} bits, which is not a multiple of 128\"\n+  )\n+\n+\n+def matmul_kernel(a, b, config: TuningConfig):\n+  dtype = a.dtype\n+  if a.dtype != b.dtype:\n+    raise ValueError(\n+        f\"Matmul LHS and RHS have incompatible dtypes {a.dtype} vs {b.dtype}\"\n+    )\n+  m, k = a.shape\n+  k2, n = b.shape\n+  if k != k2:\n+    raise ValueError(\n+        f\"Matmul LHS and RHS have incompatible shapes {a.shape} vs {b.shape}\"\n+    )\n+  collective = config.collective\n+  if collective:\n+    raise ValueError(\"Collective matmul is not supported yet.\")\n+  block_m, block_n, block_k = (config.block_m, config.block_n, config.block_k)\n+  swizzle = _find_swizzle(block_k * jnp.dtype(dtype).itemsize * 8)\n+  swizzle_elems = swizzle // jnp.dtype(dtype).itemsize\n+  transforms = (\n+      plgpu.TilingTransform((8, swizzle_elems)),\n+      plgpu.SwizzleTransform(swizzle),\n+  )\n+  block_lhs = (block_m, block_k)\n+  block_rhs = (block_k, block_n)\n+  block_out = (block_m, block_n)\n+  if m % block_m != 0:\n+    raise ValueError(f\"{m=} must be divisible by {block_m=}\")\n+  if n % block_n != 0:\n+    raise ValueError(f\"{n=} must be divisible by {block_n=}\")\n+  if k % block_k != 0:\n+    raise ValueError(f\"{k=} must be divisible by {block_k=}\")\n+  m_iters = m // block_m\n+  n_iters = n // block_n\n+  k_iters = k // block_k\n+  max_concurrent_steps = config.max_concurrent_steps\n+\n+  def kernel(a_gmem, b_gmem, out_gmem,\n+             a_smem, b_smem, acc_tmem, acc_smem,\n+             a_tma_barrier, b_tma_barrier, consumed_barrier):\n+    m_index = lax.axis_index(\"m\")\n+    n_index = lax.axis_index(\"n\")\n+    slice_m = pl.ds(m_index * block_m, block_m)\n+    slice_n = pl.ds(n_index * block_n, block_n)\n+    acc_slice_m = pl.ds(m_index * block_m, block_m)\n+    acc_slice_n = pl.ds(n_index * block_n, block_n)\n+\n+    @pl.core_map(plgpu.WarpMesh(axis_name=\"warp\"))\n+    def _per_warp():\n+      warp_id = lax.axis_index(\"warp\")\n+\n+      @pl.when(warp_id == 0)\n+      def _memory():\n+        def _loop_body(ki, _):\n+          slot = lax.rem(ki, max_concurrent_steps)\n+\n+          @pl.when(ki >= max_concurrent_steps)\n+          def _():\n+            plgpu.barrier_wait(consumed_barrier.at[slot])\n+\n+          slice_k = pl.ds(ki * block_k, block_k)\n+          plgpu.copy_gmem_to_smem(\n+              a_gmem.at[slice_m, slice_k],\n+              a_smem.at[slot],\n+              a_tma_barrier.at[slot],\n+          )\n+          plgpu.copy_gmem_to_smem(\n+              b_gmem.at[slice_k, slice_n],\n+              b_smem.at[slot],\n+              b_tma_barrier.at[slot],\n+          )\n+\n+        lax.fori_loop(0, k_iters, _loop_body, None)\n+\n+      @pl.when(warp_id == 1)\n+      def _compute():\n+        def _loop_body(ki, _):\n+          slot = lax.rem(ki, max_concurrent_steps)\n+          plgpu.barrier_wait(a_tma_barrier.at[slot])\n+          plgpu.barrier_wait(b_tma_barrier.at[slot])\n+          is_last_iter = ki >= k_iters - 1\n+          barrier_slot = lax.select_n(is_last_iter,\n+                                      slot, max_concurrent_steps)\n+          plgpu.tcgen05_mma(\n+              acc_tmem,\n+              a_smem.at[slot],\n+              b_smem.at[slot],\n+              consumed_barrier.at[barrier_slot],\n+              accumulate=(ki > 0),\n+          )\n+        lax.fori_loop(0, k_iters, _loop_body, None)\n+\n+    plgpu.barrier_wait(consumed_barrier.at[max_concurrent_steps])\n+    acc_smem[...] = acc_tmem[...].astype(dtype)\n+    plgpu.commit_smem()\n+    plgpu.copy_smem_to_gmem(\n+        acc_smem, out_gmem.at[acc_slice_m, acc_slice_n]\n+    )\n+    plgpu.wait_smem_to_gmem(0)\n+\n+  f = plgpu.kernel(\n+      kernel,\n+      out_shape=jax.ShapeDtypeStruct((m, n), dtype),\n+      grid=(m_iters, n_iters),\n+      grid_names=(\"m\", \"n\"),\n+      # TODO(justinfu): Add collective support.\n+      cluster_names=(),\n+      cluster=(),\n+      scratch_shapes=(   # type: ignore\n+        plgpu.SMEM(\n+            (max_concurrent_steps, *block_lhs), dtype, transforms=transforms\n+        ),\n+        plgpu.SMEM(\n+            (max_concurrent_steps, *block_rhs), dtype, transforms=transforms\n+        ),\n+        plgpu.TMEM(block_out, jnp.float32, collective=collective),\n+        plgpu.SMEM(block_out, dtype, transforms=transforms),\n+        plgpu.Barrier(\n+            num_arrivals=1, num_barriers=max_concurrent_steps\n+        ),\n+        plgpu.Barrier(\n+            num_arrivals=1, num_barriers=max_concurrent_steps\n+        ),\n+        plgpu.Barrier(\n+            num_arrivals=1,\n+            num_barriers=max_concurrent_steps + 1,\n+            for_tensor_core=True,\n+        ),\n+      )\n+  )\n+  return f(a, b)\n+\n+\n+def main(_) -> None:\n+  problem_it = itertools.product(\n+      (1024, 4096, 8192), (1024, 4096, 8192), (1024, 8192)\n+  )\n+  for M, N, K in problem_it:\n+    print(f\"==== {M=} {N=} {K=} ====\")\n+    matmul_flops = 2 * M * N * K\n+    peak_flops = 2.25e15  # f16 TensorCore peak = 2250 TFLOPS\n+    a = jax.random.uniform(jax.random.key(0), (M, K), jnp.bfloat16)\n+    b = jax.random.uniform(jax.random.key(1), (K, N), jnp.bfloat16)\n+    tuning_it = itertools.product(\n+        (128,), (128, 256), (64, 128), (2, 3, 4), (False,)\n+    )\n+    best_util = -float(\"inf\")\n+    for (block_m, block_n, block_k,\n+         max_concurrent_steps, collective) in tuning_it:\n+      config = TuningConfig(\n+          block_m=block_m,\n+          block_n=block_n,\n+          block_k=block_k,\n+          max_concurrent_steps=max_concurrent_steps,\n+          collective=collective,\n+      )\n+      try:\n+        out, runtime_ms = profiler.measure(\n+            functools.partial(matmul_kernel, config=config)\n+        )(a, b)\n+      except ValueError as e:\n+        if \"exceeds available shared memory\" in e.args[0]:\n+          continue\n+        raise\n+      if M * N * K <= 1024 * 1024 * 1024:\n+        expected = a @ b\n+        np.testing.assert_allclose(out, expected)\n+      runtime_us = runtime_ms * 1e3   # type: ignore\n+      optimal_time = matmul_flops / peak_flops * 1e6  # us\n+      achieved_tc_util = optimal_time / runtime_us * 100\n+      if achieved_tc_util > best_util:\n+        best_util = achieved_tc_util\n+      print(\n+          f\"{block_m=} {block_n=} {block_k=} {max_concurrent_steps=}:  \"\n+          f\"{runtime_us:<7.1f}us\"\n+          f\" = {achieved_tc_util:4.1f}% TC utilization\"\n+      )\n+    print(f\"\\tBest utilization: {best_util:4.1f}%\")\n+\n+\n+if __name__ == \"__main__\":\n+  from absl import app\n+\n+  jax.config.config_with_absl()\n+  app.run(main)\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 8be899123de0..48eddae69a60 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -805,6 +805,23 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"mgpu_matmul_test\",\n+    srcs = [\"mgpu_matmul_test.py\"],\n+    enable_backends = [],\n+    enable_configs = [],  # TODO(justinfu): Enable B200 when available.\n+    env = {\"XLA_FLAGS\": \"--xla_gpu_autotune_level=0\"},\n+    shard_count = 8,\n+    deps = [\n+        \"//jax:pallas\",\n+        \"//jax:pallas_experimental_gpu_ops\",\n+        \"//jax:pallas_mosaic_gpu\",\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n+)\n+\n jax_multiplatform_test(\n     name = \"mgpu_ragged_dot_run\",\n     srcs = [\"//jax/experimental/pallas/ops/gpu:ragged_dot_mgpu.py\"],\ndiff --git a/tests/pallas/mgpu_matmul_test.py b/tests/pallas/mgpu_matmul_test.py\nnew file mode 100644\nindex 000000000000..4013db78f6a2\n--- /dev/null\n+++ b/tests/pallas/mgpu_matmul_test.py\n@@ -0,0 +1,88 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Test different parameterizations of matrix multiplication.\"\"\"\n+\n+import contextlib\n+import os\n+\n+from absl.testing import absltest\n+from absl.testing import parameterized\n+from jax._src import config\n+from jax._src import test_util as jtu\n+from jax._src.pallas import pallas_call\n+import jax.numpy as jnp\n+import numpy as np\n+\n+\n+# pylint: disable=g-import-not-at-top\n+try:\n+  # We only import this to see if Mosaic is available.\n+  import jax.experimental.mosaic.gpu  # noqa: F401\n+except ImportError:\n+  blackwell_matmul_mgpu = None\n+else:\n+  from jax.experimental.pallas.ops.gpu import blackwell_matmul_mgpu\n+\n+\n+config.parse_flags_with_absl()\n+os.environ[\"XLA_FLAGS\"] = (\n+    os.environ.get(\"XLA_FLAGS\", \"\") + \" --xla_gpu_autotune_level=0\")\n+\n+\n+@jtu.with_config(jax_traceback_filtering=\"off\")\n+class MatrixMultiplicationSm100ATest(jtu.JaxTestCase):\n+\n+  def setUp(self):\n+    super().setUp()\n+    if blackwell_matmul_mgpu is None:\n+      self.skipTest(\"Mosaic GPU not available.\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_equal(\"10.0\")):\n+      self.skipTest(\"Only works on GPU with capability sm100a\")\n+    context_stack = contextlib.ExitStack()\n+    context_stack.enter_context(pallas_call._PALLAS_USE_MOSAIC_GPU(True))\n+    self.addCleanup(context_stack.close)\n+\n+  @parameterized.product(\n+      m=(1024, 4096),\n+      k=(1024, 4096),\n+      n=(1024, 4096),\n+      dtype=(jnp.float16,),\n+  )\n+  def test_matmul(\n+      self,\n+      m,\n+      n,\n+      k,\n+      dtype,\n+  ):\n+    k1, k2, = jax.random.split(jax.random.key(42), 2)\n+    a = jax.random.normal(k1, (m, k), dtype)\n+    b = jax.random.normal(k2, (k, n), dtype)\n+\n+    out = blackwell_matmul_mgpu.matmul_kernel(\n+        a,\n+        b,\n+        blackwell_matmul_mgpu.TuningConfig(\n+            block_m=128, block_n=128, block_k=128,\n+            max_concurrent_steps=2,\n+            collective=False,\n+        ),\n+    )\n+    out_ref = a @ b\n+    np.testing.assert_allclose(out, out_ref, atol=2e-3, rtol=1e-3)\n+\n+if __name__ == \"__main__\":\n+  absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] Added `pl.loop` -- a decorator for writing stateless loops\n\nPiperOrigin-RevId: 764871167",
        "output": "```diff\nCommit: 976aa7ac31f49957803547ebff80720e39aba04e\nDate: 2025-05-29T20:55:32Z\nURL: https://github.com/jax-ml/jax/commit/976aa7ac31f49957803547ebff80720e39aba04e\nFiles changed: 6\nAdditions: +28, Deletions: -14\ndiff --git a/jax/_src/pallas/helpers.py b/jax/_src/pallas/helpers.py\nindex 5c77d0a04f09..71004cd405a3 100644\n--- a/jax/_src/pallas/helpers.py\n+++ b/jax/_src/pallas/helpers.py\n@@ -13,6 +13,8 @@\n # limitations under the License.\n \"\"\"Pallas helper functions.\"\"\"\n \n+from collections.abc import Callable\n+\n import jax\n from jax._src import checkify\n from jax._src import config\n@@ -69,6 +71,20 @@ def _wrapped(f):\n   return _wrapped\n \n \n+def loop(\n+    lower: jax.typing.ArrayLike,\n+    upper: jax.typing.ArrayLike,\n+    *,\n+    unroll: int | bool | None = None,\n+) -> Callable[[Callable[[jax.Array], None]], None]:\n+  def decorator(body):\n+    jax.lax.fori_loop(\n+        lower, upper, lambda idx, _: body(idx), init_val=None, unroll=unroll\n+    )\n+\n+  return decorator\n+\n+\n _ENABLE_DEBUG_CHECKS = config.bool_state(\n     \"jax_pallas_enable_debug_checks\",\n     default=False,\ndiff --git a/jax/_src/pallas/mosaic_gpu/pipeline.py b/jax/_src/pallas/mosaic_gpu/pipeline.py\nindex f85b73b6b946..be9f663a42b7 100644\n--- a/jax/_src/pallas/mosaic_gpu/pipeline.py\n+++ b/jax/_src/pallas/mosaic_gpu/pipeline.py\n@@ -764,12 +764,10 @@ def memory_loop_body(step, carry):\n                     memory_loop_body, (indices,))\n       # Await all the arrivals to not leave barriers in a bad state.\n       # We only need to account for the prologue steps.\n-      def _epi_step(step, _):\n+      @pl.loop(0, prologue_steps, unroll=not has_dynamic_grid)\n+      def _epi_step(step):\n         for barrier in consumed_barrier_refs:\n           gpu_primitives.barrier_wait(barrier.at[step])\n-      jax.lax.fori_loop(\n-          0, prologue_steps, _epi_step, None, unroll=not has_dynamic_grid\n-      )\n \n     wg_idx = lax.axis_index(wg_axis)\n     lax.cond(\ndiff --git a/jax/experimental/pallas/__init__.py b/jax/experimental/pallas/__init__.py\nindex caf77a3c4fce..da2bc9119dd0 100644\n--- a/jax/experimental/pallas/__init__.py\n+++ b/jax/experimental/pallas/__init__.py\n@@ -38,6 +38,7 @@\n from jax._src.pallas.cost_estimate import estimate_cost as estimate_cost\n from jax._src.pallas.helpers import empty as empty\n from jax._src.pallas.helpers import empty_like as empty_like\n+from jax._src.pallas.helpers import loop as loop\n from jax._src.pallas.helpers import when as when\n from jax._src.pallas.helpers import debug_check as debug_check\n from jax._src.pallas.helpers import debug_checks_enabled as debug_checks_enabled\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex 447e3affd7c1..650668daf67a 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -245,7 +245,8 @@ def _memory_wg():\n         plgpu.copy_gmem_to_smem(k_ref.at[s], k_smem.at[i], k_barriers.at[i])\n         plgpu.copy_gmem_to_smem(v_ref.at[s], v_smem.at[i], v_barriers.at[i])\n \n-      def kv_loop(kv_step, _):\n+      @pl.loop(0, block_max_kv_steps - max_concurrent_steps)\n+      def _kv_loop(kv_step):\n         tma_step = kv_step + max_concurrent_steps\n         tma_slot = lax.rem(kv_step, jnp.array(max_concurrent_steps, kv_step.dtype))\n         s = (batch, pl.ds(tma_step * block_kv, block_kv), kv_head)\n@@ -253,7 +254,6 @@ def kv_loop(kv_step, _):\n         plgpu.copy_gmem_to_smem(k_ref.at[s], k_smem.at[tma_slot], k_barriers.at[tma_slot])\n         plgpu.barrier_wait(v_consumed_barriers.at[tma_slot])\n         plgpu.copy_gmem_to_smem(v_ref.at[s], v_smem.at[tma_slot], v_barriers.at[tma_slot])\n-      lax.fori_loop(0, block_max_kv_steps - max_concurrent_steps, kv_loop, None)\n \n   def entry(q_ref, k_ref, v_ref, out_ref, lse_ref):\n     compute_wgs = 2\ndiff --git a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\nindex 36d29cf082d8..5e4dda4494ba 100644\n--- a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n@@ -107,12 +107,13 @@ def m_loop(mi, _):\n \n       # For some reason ptxas spills if we unroll the loop over k\n       copy_block = 32\n-      def k_copy_loop(ki, _):\n+      @pl.loop(0, k // copy_block)\n+      def _k_copy_loop(ki):\n         k_slice = pl.ds(ki * copy_block, copy_block)\n         scratch_ref[0, :, k_slice] = lhs_ref[m_tile_slice, k_slice]\n-      jax.lax.fori_loop(0, k // copy_block, k_copy_loop, None)\n \n-      def device_loop(device_offset, _):\n+      @pl.loop(0, num_devices)\n+      def _device_loop(device_offset):\n         # Loop invariant: scratch_ref.at[scratch_slot] is ready to be used\n         # We're double buffering the scratch space. At each step, we read from\n         # scratch_ref.at[scratch_slot] and write to scratch_ref.at[next_scratch_slot]\n@@ -168,7 +169,7 @@ def k_loop(idxs, lhs_smem, rhs_smem):\n           )\n           # Wait for the next scratch to arrive --- see the loop invariant.\n           pl.semaphore_wait(received_sem)\n-      jax.lax.fori_loop(0, num_devices, device_loop, None)\n+\n     grid_size = m_shard // block_m\n     m_steps = grid_size // num_sms + jnp.int32(sm_id < grid_size % num_sms)\n     # TODO(apaszke): Use the ND-loop helper.\ndiff --git a/jax/experimental/pallas/ops/tpu/flash_attention.py b/jax/experimental/pallas/ops/tpu/flash_attention.py\nindex ef8dd61abacb..06746986a15e 100644\n--- a/jax/experimental/pallas/ops/tpu/flash_attention.py\n+++ b/jax/experimental/pallas/ops/tpu/flash_attention.py\n@@ -383,10 +383,8 @@ def start_new_sequence():\n \n   @pl.when(should_run)\n   def run():\n-    @functools.partial(\n-        lax.fori_loop, 0, block_k_major // block_k, init_val=None, unroll=True\n-    )\n-    def body(i, _):\n+    @pl.loop(0, block_k_major // block_k, unroll=True)\n+    def _body(i):\n       m_prev = m_scratch_ref[batch_idx]\n       l_prev = l_scratch_ref[batch_idx]\n       q = q_tile_ref[batch_idx]  # [block_q, head_dim]\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas Fuser] Use lu transformation to physicalize fwd/bwd functions in custom_vjp rule\n\nPiperOrigin-RevId: 764871024",
        "output": "```diff\nCommit: 3abdf560cc912320e0c0b69bae9851d7d1b93d6b\nDate: 2025-05-29T20:53:32Z\nURL: https://github.com/jax-ml/jax/commit/3abdf560cc912320e0c0b69bae9851d7d1b93d6b\nFiles changed: 2\nAdditions: +38, Deletions: -0\ndiff --git a/jax/_src/pallas/fuser/BUILD b/jax/_src/pallas/fuser/BUILD\nindex a4c3402f5309..d5b5d128241d 100644\n--- a/jax/_src/pallas/fuser/BUILD\n+++ b/jax/_src/pallas/fuser/BUILD\n@@ -115,6 +115,7 @@ pytype_strict_library(\n         \"//jax\",\n         \"//jax:api_util\",\n         \"//jax:core\",\n+        \"//jax:custom_derivatives\",\n         \"//jax:dtypes\",\n         \"//jax:partial_eval\",\n         \"//jax:source_info_util\",\ndiff --git a/jax/_src/pallas/fuser/fusible_dtype.py b/jax/_src/pallas/fuser/fusible_dtype.py\nindex 7d9c2ca67855..152b20ff66ea 100644\n--- a/jax/_src/pallas/fuser/fusible_dtype.py\n+++ b/jax/_src/pallas/fuser/fusible_dtype.py\n@@ -17,11 +17,13 @@\n import abc\n import dataclasses\n import functools\n+import itertools as it\n from typing import Any, Sequence, TypeVar\n \n import jax\n from jax._src import api_util\n from jax._src import core\n+from jax._src import custom_derivatives\n from jax._src import dtypes\n from jax._src import linear_util as lu\n from jax._src import source_info_util\n@@ -312,6 +314,41 @@ def _cond_physicalize_rule(ctx: Context, *args, branches, **kwargs):\n _physicalize_rules[conditionals.cond_p] = _cond_physicalize_rule\n \n \n+@lu.transformation2\n+def _physicalize_transform(f, *args):\n+  vals, zeros = args[::2], args[1::2]\n+  assert len(vals) == len(zeros)\n+  wrapper = lambda *inner_vals: f(\n+      *it.chain.from_iterable(zip(inner_vals, zeros))\n+  )\n+  return physicalize(wrapper)(*vals)\n+\n+\n+@lu.transformation2\n+def _physicalize_transform_bwd(f, const_avals, *args):\n+  return [custom_derivatives.Zero(a) for a in const_avals] + list(\n+      physicalize(f)(*args)\n+  )\n+\n+\n+def _custom_vjp_call_physicalize_rule(\n+    ctx: Context, *args, call_jaxpr, num_consts, fwd_jaxpr_thunk, bwd, **kwargs\n+):\n+  _assert_no_fusion_types(ctx.avals_out)\n+  new_jaxpr = physicalize_closed_jaxpr(call_jaxpr)\n+  fun = lu.wrap_init(core.jaxpr_as_fun(new_jaxpr),\n+                     debug_info=call_jaxpr.jaxpr.debug_info)\n+  fwd = custom_derivatives.lift_fwd(num_consts, fwd_jaxpr_thunk)\n+  fwd_physicalized = _physicalize_transform(fwd)\n+  const_avals, _ = util.split_list(new_jaxpr.in_avals, [num_consts])\n+  bwd_physicalized = _physicalize_transform_bwd(bwd, const_avals)\n+  return custom_derivatives.custom_vjp_call_p.bind(\n+      fun, fwd_physicalized, bwd_physicalized, *args, **kwargs\n+  )\n+\n+_physicalize_rules[custom_derivatives.custom_vjp_call_p] = _custom_vjp_call_physicalize_rule\n+\n+\n def _run_state_rule(ctx: Context, *args, jaxpr, which_linear, is_initialized):\n   _assert_no_fusion_types(ctx.avals_in)\n   _assert_no_fusion_types(ctx.avals_out)\n\n```",
        "from_id": [
            "sharadmv",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29076 from hanzlfs:zhonglin/mosaic/collective_matmul\n\nPiperOrigin-RevId: 764852609",
        "output": "```diff\nCommit: 4e5725bba22976a78a89a86d79bc2979a1bbd5ee\nDate: 2025-05-29T20:09:32Z\nURL: https://github.com/jax-ml/jax/commit/4e5725bba22976a78a89a86d79bc2979a1bbd5ee\nFiles changed: 2\nAdditions: +9, Deletions: -4\ndiff --git a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\nindex 854d75dbf6a3..36d29cf082d8 100644\n--- a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n@@ -42,6 +42,7 @@ def all_gather_lhs_matmul(\n     block_n: int,\n     block_k: int,\n     max_concurrent_steps: int,\n+    dtype: jnp.dtype = jnp.float16,\n ) -> jax.Array:\n   if (num_devices := jax.device_count()) != jax.process_count():\n     raise ValueError(\"The kernel only supports one device per process\")\n@@ -49,6 +50,8 @@ def all_gather_lhs_matmul(\n     raise ValueError(\"The kernel can only work over all devices in a Mesh.\")\n   if max_concurrent_steps < 2:\n     raise ValueError(\"max_concurrent_steps must be >= 2\")\n+  if jnp.dtype(dtype) not in map(jnp.dtype, [jnp.float16, jnp.bfloat16]):\n+    raise NotImplementedError(f\"Only f16 and bf16 are supported, got dtype: {dtype}\")\n \n   num_sms = 132  # There are 132 SMs on a H100 SXM GPU.\n \n@@ -121,7 +124,7 @@ def device_loop(device_offset, _):\n         @functools.partial(\n             pl.run_scoped,\n             acc_ref=plgpu.ACC((block_m, block_n)),\n-            out_smem=plgpu.SMEM((block_m, block_n), jnp.float16, transforms=transforms),\n+            out_smem=plgpu.SMEM((block_m, block_n), dtype, transforms=transforms),\n         )\n         def _(acc_ref, out_smem):\n           pl.semaphore_wait(capacity_sem)\n@@ -173,8 +176,8 @@ def k_loop(idxs, lhs_smem, rhs_smem):\n \n   result, _ = plgpu.kernel(\n       kernel_body,\n-      out_shape=[jax.ShapeDtypeStruct((axis_size * m_shard, n_shard), jnp.float16),\n-                  jax.ShapeDtypeStruct((num_sms, 2, block_m, k), jnp.float16)],\n+      out_shape=[jax.ShapeDtypeStruct((axis_size * m_shard, n_shard), dtype),\n+                  jax.ShapeDtypeStruct((num_sms, 2, block_m, k), dtype)],\n       scratch_shapes=[\n           plgpu.SemaphoreType.REGULAR, plgpu.SemaphoreType.REGULAR,\n       ],\ndiff --git a/tests/pallas/mgpu_collective_matmul_test.py b/tests/pallas/mgpu_collective_matmul_test.py\nindex 386162b1992c..3760c7ccddb7 100644\n--- a/tests/pallas/mgpu_collective_matmul_test.py\n+++ b/tests/pallas/mgpu_collective_matmul_test.py\n@@ -68,6 +68,7 @@ def setUp(self):\n       block_n=(64, 128, 192),\n       block_k=(64, 128),\n       max_concurrent_steps=(2, 4),\n+      dtype=(jnp.float16, jnp.bfloat16),\n   )\n   def test_all_gather_lhs_matmul(\n       self,\n@@ -78,9 +79,9 @@ def test_all_gather_lhs_matmul(\n       block_n,\n       block_k,\n       max_concurrent_steps,\n+      dtype,\n   ):\n     num_devices = jax.device_count()\n-    dtype = jnp.float16\n     lhs_smem_size = block_m * block_k * max_concurrent_steps * 2\n     rhs_smem_size = block_k * block_n * max_concurrent_steps * 2\n     # H100 SMEM limit is 228kB.\n@@ -118,6 +119,7 @@ def run(body):\n             block_n=block_n,\n             block_k=block_k,\n             max_concurrent_steps=max_concurrent_steps,\n+            dtype=dtype,\n         )\n     )\n     ref_out = run(lambda x, y: lax.all_gather(x, \"x\", axis=0, tiled=True) @ y)\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reverts 42977e51816b9eb42c7360abe05f56cad70e894a\n\nPiperOrigin-RevId: 764832745",
        "output": "```diff\nCommit: 448c07d006e5cbc0cdd95ee7e477b9bdc606e1e9\nDate: 2025-05-29T19:16:31Z\nURL: https://github.com/jax-ml/jax/commit/448c07d006e5cbc0cdd95ee7e477b9bdc606e1e9\nFiles changed: 2\nAdditions: +0, Deletions: -21\ndiff --git a/jax/_src/pallas/fuser/BUILD b/jax/_src/pallas/fuser/BUILD\nindex d5b5d128241d..a4c3402f5309 100644\n--- a/jax/_src/pallas/fuser/BUILD\n+++ b/jax/_src/pallas/fuser/BUILD\n@@ -115,7 +115,6 @@ pytype_strict_library(\n         \"//jax\",\n         \"//jax:api_util\",\n         \"//jax:core\",\n-        \"//jax:custom_derivatives\",\n         \"//jax:dtypes\",\n         \"//jax:partial_eval\",\n         \"//jax:source_info_util\",\ndiff --git a/jax/_src/pallas/fuser/fusible_dtype.py b/jax/_src/pallas/fuser/fusible_dtype.py\nindex 09cd8f57dbc1..7d9c2ca67855 100644\n--- a/jax/_src/pallas/fuser/fusible_dtype.py\n+++ b/jax/_src/pallas/fuser/fusible_dtype.py\n@@ -22,7 +22,6 @@\n import jax\n from jax._src import api_util\n from jax._src import core\n-from jax._src import custom_derivatives\n from jax._src import dtypes\n from jax._src import linear_util as lu\n from jax._src import source_info_util\n@@ -313,25 +312,6 @@ def _cond_physicalize_rule(ctx: Context, *args, branches, **kwargs):\n _physicalize_rules[conditionals.cond_p] = _cond_physicalize_rule\n \n \n-def _custom_vjp_call_physicalize_rule(\n-    ctx: Context, *args, call_jaxpr, num_consts, fwd_jaxpr_thunk, bwd, **kwargs\n-):\n-  _assert_no_fusion_types(ctx.avals_out)\n-  new_jaxpr = physicalize_closed_jaxpr(call_jaxpr)\n-  fun = lu.wrap_init(core.jaxpr_as_fun(new_jaxpr),\n-                     debug_info=call_jaxpr.jaxpr.debug_info)\n-  fwd = custom_derivatives.lift_fwd(num_consts, fwd_jaxpr_thunk)\n-  new_fwd = lu.wrap_init(physicalize(fwd.f_transformed), debug_info=fwd.debug_info)\n-  const_avals, _ = util.split_list(new_jaxpr.in_avals, [num_consts])\n-  bwd = custom_derivatives._handle_consts_in_bwd(bwd, const_avals)\n-  new_bwd = lu.wrap_init(physicalize(bwd.f_transformed), debug_info=bwd.debug_info)\n-  return custom_derivatives.custom_vjp_call_p.bind(\n-      fun, new_fwd, new_bwd, *args, **kwargs\n-  )\n-\n-_physicalize_rules[custom_derivatives.custom_vjp_call_p] = _custom_vjp_call_physicalize_rule\n-\n-\n def _run_state_rule(ctx: Context, *args, jaxpr, which_linear, is_initialized):\n   _assert_no_fusion_types(ctx.avals_in)\n   _assert_no_fusion_types(ctx.avals_out)\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Lock down more permissions and update default usage for some workflows",
        "output": "```diff\nCommit: da845deb30955cfb32d265457903ad90fc3e2eb7\nDate: 2025-05-29T18:53:29Z\nURL: https://github.com/jax-ml/jax/commit/da845deb30955cfb32d265457903ad90fc3e2eb7\nFiles changed: 6\nAdditions: +4, Deletions: -47\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex 09c9d173e0d0..cc3ae89d97f9 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -16,22 +16,18 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version to test?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n \n jobs:\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 458589199c53..d30e1b56dab8 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -17,33 +17,28 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version to test?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       jaxlib-version:\n         description: \"Which jaxlib version to test? (head/pypi_latest)\"\n         type: string\n-        required: true\n         default: \"head\"\n       gcs_download_uri:\n         description: \"GCS location URI from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 72d554aa5d1b..95ab90412494 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -12,7 +12,6 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: choice\n-        required: true\n         default: \"linux-x86-n2-16\"\n         options:\n         - \"linux-x86-n2-16\"\n@@ -21,7 +20,6 @@ on:\n       artifact:\n         description: \"Which JAX artifact to build?\"\n         type: choice\n-        required: true\n         default: \"jaxlib\"\n         options:\n         - \"jax\"\n@@ -31,7 +29,6 @@ on:\n       python:\n         description: \"Which python version should the artifact be built for?\"\n         type: choice\n-        required: false\n         default: \"3.12\"\n         options:\n         - \"3.10\"\n@@ -41,7 +38,6 @@ on:\n       clone_main_xla:\n         description: \"Should latest XLA be used?\"\n         type: choice\n-        required: false\n         default: \"0\"\n         options:\n         - \"1\"\n@@ -49,7 +45,6 @@ on:\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: choice\n-        required: false\n         default: 'no'\n         options:\n         - 'yes'\n@@ -59,31 +54,25 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       artifact:\n         description: \"Which JAX artifact to build?\"\n         type: string\n-        required: true\n         default: \"jaxlib\"\n       python:\n         description: \"Which python version should the artifact be built for?\"\n         type: string\n-        required: false\n         default: \"3.12\"\n       clone_main_xla:\n         description: \"Should latest XLA be used?\"\n         type: string\n-        required: false\n         default: \"0\"\n       upload_artifacts_to_gcs:\n         description: \"Should the artifacts be uploaded to a GCS bucket?\"\n-        required: true\n         default: true\n         type: boolean\n       gcs_upload_uri:\n         description: \"GCS location prefix to where the artifacts should be uploaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n     outputs:\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex a92f2d96dc89..95086257c62b 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -17,34 +17,28 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version should the artifact be built for?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       download-jax-only-from-gcs:\n         description: \"Whether to download only the jax wheel from GCS (e.g for testing a jax only release)\"\n-        required: false\n         default: '0'\n         type: string\n       gcs_download_uri:\n         description: \"GCS location prefix from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n-\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 6fa4e14f8b85..d576370bb772 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -17,44 +17,36 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version to test?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       cuda-version:\n         description: \"Which CUDA version to test?\"\n         type: string\n-        required: true\n         default: \"12.8\"\n       use-nvidia-pip-wheels:\n         description: \"Whether to download CUDA packages from PyPI?\"\n         type: boolean\n-        required: false\n         default: false\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       download-jax-only-from-gcs:\n         description: \"Whether to download only the jax wheel from GCS (e.g for testing a jax only release)\"\n-        required: false\n         default: '0'\n         type: string\n       gcs_download_uri:\n         description: \"GCS location prefix from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n-\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 5f56b165c295..313bbede52f5 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -22,32 +22,26 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-ct5lp-224-8tpu\"\n       cores:\n         description: \"How many TPU cores should the test use?\"\n         type: string\n-        required: true\n         default: \"8\"\n       tpu-type:\n         description: \"Which TPU type is used for testing?\"\n         type: string\n-        required: true\n         default: \"v5e-8\"\n       python:\n         description: \"Which Python version should be used for testing?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       run-full-tpu-test-suite:\n         description: \"Should the full TPU test suite be run?\"\n         type: string\n-        required: false\n         default: \"0\"\n       libtpu-version-type:\n         description: \"Which libtpu version should be used for testing?\"\n         type: string\n-        required: false\n         # Choices are:\n         # - \"nightly\": Use the nightly libtpu wheel.\n         # - \"pypi_latest\": Use the latest libtpu wheel from PyPI.\n@@ -55,20 +49,17 @@ on:\n         default: \"nightly\"\n       download-jax-only-from-gcs:\n         description: \"Whether to download only the jax wheel from GCS (e.g for testing a jax only release)\"\n-        required: false\n         default: '0'\n         type: string\n       gcs_download_uri:\n         description: \"GCS location prefix from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n-\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\n\n```",
        "from_id": [
            "MichaelHudgins"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Check that the device order in the mesh follows logical_ids\n\nAs the comment in the code explains, we expect that the mesh ordering follows\ndevice ids, which should always equal the NVSHMEM PE ids that Mosaic uses for\nits collective implementations. Any divergence would have to be resolved through\nan extra translation layer at runtime.\n\nPiperOrigin-RevId: 764819378",
        "output": "```diff\nCommit: 57fe3f2aa60239e792ff46607372b4826440033b\nDate: 2025-05-29T18:44:47Z\nURL: https://github.com/jax-ml/jax/commit/57fe3f2aa60239e792ff46607372b4826440033b\nFiles changed: 2\nAdditions: +57, Deletions: -0\ndiff --git a/jax/experimental/mosaic/gpu/core.py b/jax/experimental/mosaic/gpu/core.py\nindex 4ed551654a0e..193fd1bd3589 100644\n--- a/jax/experimental/mosaic/gpu/core.py\n+++ b/jax/experimental/mosaic/gpu/core.py\n@@ -27,6 +27,7 @@\n import weakref\n \n import jax\n+from jax._src import sharding_impls\n from jax._src.interpreters import mlir\n from jax._src.lib import mosaic_gpu_dialect as dialect\n from jaxlib.mlir import ir\n@@ -127,6 +128,15 @@ def _mosaic_gpu_abstract_eval(*_, module, out_types):\n   del module  # Unused.\n   return [jax._src.core.ShapedArray(t.shape, t.dtype) for t in out_types]\n \n+\n+def _has_communication(module, **_):\n+  empty_str_attr = ir.StringAttr.get(\"\")\n+  for op in module.body:\n+    if \"nvshmem\" in getattr(op, \"sym_name\", empty_str_attr).value:\n+      return True\n+  return False\n+\n+\n # TODO(apaszke): Implement a proper system for managing kernel lifetimes\n KNOWN_KERNELS = {}\n \n@@ -139,6 +149,27 @@ def _mosaic_gpu_lowering_rule(\n     input_output_aliases: tuple[tuple[int, int], ...] = (),\n     use_custom_barrier: bool = False,\n ):\n+  axis_context = ctx.module_context.axis_context\n+  if _has_communication(module):\n+    # Those checks are trying to ensure that the logical device ids are\n+    # consistent with the NVSHMEM PE ids that Mosaic will be using for\n+    # communication. Any divergence here would require us to implement a logical\n+    # to physical translation, which is currently not implemented.\n+    if isinstance(axis_context, sharding_impls.SPMDAxisContext):\n+      mesh = axis_context.mesh\n+      if not np.array_equal(mesh.device_ids.ravel(), np.arange(mesh.size)):\n+        raise NotImplementedError(\n+            \"Mosaic GPU only supports meshes with device ordering that follows\"\n+            \" row-major device ids.\"\n+        )\n+    elif isinstance(axis_context, sharding_impls.ShardingContext):\n+      if axis_context.num_devices != 1:\n+        raise NotImplementedError(\n+            \"Mosaic GPU only supports single-device meshes in ShardingContext.\"\n+        )\n+    else:\n+      raise NotImplementedError(f\"Unsupported sharding context: {axis_context}\")\n+\n   assert len(args) == len(ctx.avals_in)\n   assert len(out_types) == len(ctx.avals_out)\n   module = _run_serde_pass(\ndiff --git a/tests/pallas/gpu_pallas_distributed_test.py b/tests/pallas/gpu_pallas_distributed_test.py\nindex 3aeee352ff6d..163adc385b23 100644\n--- a/tests/pallas/gpu_pallas_distributed_test.py\n+++ b/tests/pallas/gpu_pallas_distributed_test.py\n@@ -116,6 +116,32 @@ def kernel(y_ref, sem):\n     )()\n     np.testing.assert_allclose(y, jnp.ones_like(y))\n \n+  def test_permuted_mesh(self):\n+    def kernel(y_ref, sem):\n+      other_dev_id = 1 - lax.axis_index('x')\n+      pl.semaphore_signal(sem, 1, device_id=other_dev_id,\n+                          device_id_type=pl.DeviceIdType.LOGICAL)\n+      pl.semaphore_wait(sem)\n+\n+    kernel_call = pl.pallas_call(\n+        kernel,\n+        out_specs=pl.BlockSpec(memory_space=plgpu.GMEM),\n+        out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32),\n+        scratch_shapes=[plgpu.SemaphoreType.REGULAR],\n+    )\n+    mesh = jax.sharding.Mesh(jax.devices()[::-1], ['x'])  # Reverse the devices.\n+    f = jax.jit(\n+        shard_map.shard_map(\n+            kernel_call, mesh, in_specs=(), out_specs=P(None), check_rep=False,\n+        )\n+    )\n+    msg = (\n+        'Mosaic GPU only supports meshes with device ordering that follows'\n+        ' row-major device ids.'\n+    )\n+    with self.assertRaisesRegex(NotImplementedError, msg):\n+      f()\n+\n \n if __name__ == '__main__':\n   # This test doesn't work with the platform allocator, so we override it\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add dtype arg collective_matmul_mgpu.py to support bfloat16",
        "output": "```diff\nCommit: 1e334cfdd27b82f4af98e0a744b5af0e2a3634ec\nDate: 2025-05-29T17:56:13Z\nURL: https://github.com/jax-ml/jax/commit/1e334cfdd27b82f4af98e0a744b5af0e2a3634ec\nFiles changed: 2\nAdditions: +9, Deletions: -4\ndiff --git a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\nindex 854d75dbf6a3..36d29cf082d8 100644\n--- a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n@@ -42,6 +42,7 @@ def all_gather_lhs_matmul(\n     block_n: int,\n     block_k: int,\n     max_concurrent_steps: int,\n+    dtype: jnp.dtype = jnp.float16,\n ) -> jax.Array:\n   if (num_devices := jax.device_count()) != jax.process_count():\n     raise ValueError(\"The kernel only supports one device per process\")\n@@ -49,6 +50,8 @@ def all_gather_lhs_matmul(\n     raise ValueError(\"The kernel can only work over all devices in a Mesh.\")\n   if max_concurrent_steps < 2:\n     raise ValueError(\"max_concurrent_steps must be >= 2\")\n+  if jnp.dtype(dtype) not in map(jnp.dtype, [jnp.float16, jnp.bfloat16]):\n+    raise NotImplementedError(f\"Only f16 and bf16 are supported, got dtype: {dtype}\")\n \n   num_sms = 132  # There are 132 SMs on a H100 SXM GPU.\n \n@@ -121,7 +124,7 @@ def device_loop(device_offset, _):\n         @functools.partial(\n             pl.run_scoped,\n             acc_ref=plgpu.ACC((block_m, block_n)),\n-            out_smem=plgpu.SMEM((block_m, block_n), jnp.float16, transforms=transforms),\n+            out_smem=plgpu.SMEM((block_m, block_n), dtype, transforms=transforms),\n         )\n         def _(acc_ref, out_smem):\n           pl.semaphore_wait(capacity_sem)\n@@ -173,8 +176,8 @@ def k_loop(idxs, lhs_smem, rhs_smem):\n \n   result, _ = plgpu.kernel(\n       kernel_body,\n-      out_shape=[jax.ShapeDtypeStruct((axis_size * m_shard, n_shard), jnp.float16),\n-                  jax.ShapeDtypeStruct((num_sms, 2, block_m, k), jnp.float16)],\n+      out_shape=[jax.ShapeDtypeStruct((axis_size * m_shard, n_shard), dtype),\n+                  jax.ShapeDtypeStruct((num_sms, 2, block_m, k), dtype)],\n       scratch_shapes=[\n           plgpu.SemaphoreType.REGULAR, plgpu.SemaphoreType.REGULAR,\n       ],\ndiff --git a/tests/pallas/mgpu_collective_matmul_test.py b/tests/pallas/mgpu_collective_matmul_test.py\nindex 386162b1992c..3760c7ccddb7 100644\n--- a/tests/pallas/mgpu_collective_matmul_test.py\n+++ b/tests/pallas/mgpu_collective_matmul_test.py\n@@ -68,6 +68,7 @@ def setUp(self):\n       block_n=(64, 128, 192),\n       block_k=(64, 128),\n       max_concurrent_steps=(2, 4),\n+      dtype=(jnp.float16, jnp.bfloat16),\n   )\n   def test_all_gather_lhs_matmul(\n       self,\n@@ -78,9 +79,9 @@ def test_all_gather_lhs_matmul(\n       block_n,\n       block_k,\n       max_concurrent_steps,\n+      dtype,\n   ):\n     num_devices = jax.device_count()\n-    dtype = jnp.float16\n     lhs_smem_size = block_m * block_k * max_concurrent_steps * 2\n     rhs_smem_size = block_k * block_n * max_concurrent_steps * 2\n     # H100 SMEM limit is 228kB.\n@@ -118,6 +119,7 @@ def run(body):\n             block_n=block_n,\n             block_k=block_k,\n             max_concurrent_steps=max_concurrent_steps,\n+            dtype=dtype,\n         )\n     )\n     ref_out = run(lambda x, y: lax.all_gather(x, \"x\", axis=0, tiled=True) @ y)\n\n```",
        "from_id": [
            "hanzlfs"
        ]
    },
    {
        "text_input": "[pallas:mosaic] Enabled more lowering rules for all kernel types\n\nPiperOrigin-RevId: 764795007",
        "output": "```diff\nCommit: 64ef37a6fe33ba4c264750bbbb0bdb086406818c\nDate: 2025-05-29T17:48:58Z\nURL: https://github.com/jax-ml/jax/commit/64ef37a6fe33ba4c264750bbbb0bdb086406818c\nFiles changed: 1\nAdditions: +7, Deletions: -4\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex c6aaf77199b5..e2dfe526ea14 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -3212,15 +3212,16 @@ def _debug_callback_lowering_rule(ctx: LoweringRuleContext, *args, **kwargs):\n   return []\n \n \n-@register_lowering_rule(primitives.program_id_p)\n+@register_lowering_rule(\n+    primitives.program_id_p, kernel_types=[*tpu_core.KernelType]\n+)\n def _program_id_lowering_rule(ctx: LoweringRuleContext, *, axis: int):\n-\n   if ctx.lowering_context.user_grid_indices is None:\n     raise ValueError(\n         f\"program id: {axis} was passed, but user did not provide a grid.\"\n     )\n   length = len(ctx.lowering_context.user_grid_indices)\n-  if not (0 <= axis < length):\n+  if axis not in range(length):\n     raise ValueError(\n         f\"user passed in program id with axis: {axis}, but grid only has\"\n         f\" length: {length}\"\n@@ -3228,7 +3229,9 @@ def _program_id_lowering_rule(ctx: LoweringRuleContext, *, axis: int):\n   return ctx.lowering_context.user_grid_indices[axis]\n \n \n-@register_lowering_rule(primitives.num_programs_p)\n+@register_lowering_rule(\n+    primitives.num_programs_p, kernel_types=[*tpu_core.KernelType]\n+)\n def _num_programs_lowering_rule(ctx: LoweringRuleContext, *, axis: int):\n   mapped_axes = set(ctx.lowering_context.mapped_dims)\n   seen_user_axes = 0\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28972 from vfdev-5:fix-tsan-314-jax-build-step\n\nPiperOrigin-RevId: 764792000",
        "output": "```diff\nCommit: 63c1b8a74a9e55ff1ceac4c02c6f2470046db732\nDate: 2025-05-29T17:41:39Z\nURL: https://github.com/jax-ml/jax/commit/63c1b8a74a9e55ff1ceac4c02c6f2470046db732\nFiles changed: 1\nAdditions: +7, Deletions: -3\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex 67ff8dd93e3d..c3ee37dd82f4 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -120,6 +120,7 @@ jobs:\n       - name: Build TSAN Numpy wheel\n         if: steps.cache-numpy-tsan-restore.outputs.cache-hit != 'true'\n         run: |\n+          set -eux\n           cd numpy\n \n           # If we restored cpython from cache, we need to get python interpreter from python-tsan.tgz\n@@ -135,7 +136,6 @@ jobs:\n           export PATH=${GITHUB_WORKSPACE}/cpython-tsan/bin/:$PATH\n \n           python3 -m pip install uv~=0.5.30\n-\n           python3 -m uv pip install -r requirements/build_requirements.txt\n \n           CC=clang-18 CXX=clang++-18 python3 -m pip wheel --wheel-dir dist -v . --no-build-isolation -Csetup-args=-Db_sanitize=thread -Csetup-args=-Dbuildtype=debugoptimized\n@@ -272,11 +272,15 @@ jobs:\n             --bazel_options=--copt=-g \\\n             --clang_path=/usr/bin/clang-18\n \n-\n           mkdir -p dist\n+          # Check whether we have numpy wheel or exit with error\n+          ls ${GITHUB_WORKSPACE}/wheelhouse/numpy/*.whl || exit 1\n           cp -v ${GITHUB_WORKSPACE}/wheelhouse/numpy/*.whl dist/\n-          cp -v ${GITHUB_WORKSPACE}/wheelhouse/scipy/*.whl dist/\n           if [ \"${{ matrix.python-version }}\" == \"3.14\" ]; then\n+            # Check whether we have scipy wheel or exit with error\n+            ls ${GITHUB_WORKSPACE}/wheelhouse/scipy/*.whl || exit 1\n+            cp -v ${GITHUB_WORKSPACE}/wheelhouse/scipy/*.whl dist/\n+\n             # Patch build/requirements_lock_3_14_ft.txt to use TSAN instrumented NumPy and Scipy\n             sed -i \"s|--extra-index-url.*|--extra-index-url file://${GITHUB_WORKSPACE}/wheelhouse/|\" build/${{ matrix.requirements_lock_name }}.txt\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Expose `GSPMDSharding` via `jex` as a temporary measure.\n\nPiperOrigin-RevId: 764791015",
        "output": "```diff\nCommit: 605b8c0cc4216032e1aa9644fb7da39f04bafed5\nDate: 2025-05-29T17:39:04Z\nURL: https://github.com/jax-ml/jax/commit/605b8c0cc4216032e1aa9644fb7da39f04bafed5\nFiles changed: 2\nAdditions: +23, Deletions: -0\ndiff --git a/jax/extend/BUILD b/jax/extend/BUILD\nindex 6dc5d7d76311..c2a5c48bd2b0 100644\n--- a/jax/extend/BUILD\n+++ b/jax/extend/BUILD\n@@ -71,6 +71,12 @@ pytype_strict_library(\n     deps = [\"//jax\"],\n )\n \n+pytype_strict_library(\n+    name = \"sharding\",\n+    srcs = [\"sharding.py\"],\n+    deps = [\"//jax:sharding_impls\"],\n+)\n+\n pytype_strict_library(\n     name = \"source_info_util\",\n     srcs = [\"source_info_util.py\"],\ndiff --git a/jax/extend/sharding.py b/jax/extend/sharding.py\nnew file mode 100644\nindex 000000000000..8af2bf397249\n--- /dev/null\n+++ b/jax/extend/sharding.py\n@@ -0,0 +1,17 @@\n+# Copyright 2025 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# TODO(yashkatariya): Remove this after NamedSharding supports more complicated\n+# shardings like sub-axes, strided shardings, etc.\n+from jax._src.sharding_impls import GSPMDSharding as GSPMDSharding\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28985 from dfm:scan-fwd-ext-traceable\n\nPiperOrigin-RevId: 764784115",
        "output": "```diff\nCommit: d1a1346597f1f32fd2257ca2abc8bf565f6f1f11\nDate: 2025-05-29T17:23:35Z\nURL: https://github.com/jax-ml/jax/commit/d1a1346597f1f32fd2257ca2abc8bf565f6f1f11\nFiles changed: 2\nAdditions: +82, Deletions: -1\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 83c31928d7cb..b9ce8ae09380 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -341,7 +341,7 @@ def _create_jaxpr(init):\n   # If the body forwards an input carry to an output carry, that input is\n   # read-only and can be moved to be a const. Doing so can lead to efficiency\n   # wins, e.g. if the scan is inside a cond with a batched predicate.\n-  carry_fwd, _ = split_list(pe._jaxpr_forwarding(jaxpr.jaxpr), [num_carry])\n+  carry_fwd, ext_fwd = split_list(pe._jaxpr_forwarding(jaxpr.jaxpr), [num_carry])\n   move_to_const = [len(consts) + i == f for i, f in enumerate(carry_fwd)]\n   if any(move_to_const):\n     jaxpr = pe.prune_closed_jaxpr_outputs(\n@@ -352,12 +352,32 @@ def _create_jaxpr(init):\n     consts = [*new_consts, *consts]\n     num_carry -= len(new_consts)\n \n+  # When an extensive output is forwarded from an extensive input, we can\n+  # avoid copying it by pruning it from the jaxpr and forwarding manually. We\n+  # don't need to update the indexing based on the optimization above since it\n+  # doesn't change the total number of consts and carries combined, and\n+  # `ext_fwd` already only includes the extensive outputs. But, we do remove\n+  # the number of consts from the index since we're going to use it to index\n+  # into `in_flat`, which doesn't include consts.\n+  ext_to_ext_fwd = [\n+      in_idx - len(consts) if in_idx is not None and\n+      in_idx >= num_carry + len(consts) else None for in_idx in ext_fwd]\n+  jaxpr = pe.prune_closed_jaxpr_outputs(\n+      jaxpr, [True] * num_carry + [i is None for i in ext_to_ext_fwd])\n+\n   out = scan_p.bind(*consts, *in_flat,\n                     reverse=reverse, length=length, jaxpr=jaxpr,\n                     num_consts=len(consts), num_carry=num_carry,\n                     linear=(False,) * (len(consts) + len(in_flat)),\n                     unroll=unroll, _split_transpose=_split_transpose)\n \n+  # Apply input to output forwarding that was computed above.\n+  carry_out, out = split_list(out, [num_carry])\n+  out_ = iter(out)\n+  out = [next(out_) if f is None else _maybe_put(in_flat[f]) for f in ext_to_ext_fwd]\n+  assert next(out_, None) is None\n+  out = [*carry_out, *out]\n+\n   if any(move_to_const):\n     out = pe.merge_lists(move_to_const + [False] * num_ys, out, new_consts)\n \ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex 54dff47fea32..2f1e154627f4 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -3299,6 +3299,59 @@ def body_fun(c, _):\n     outs_ref = body_fun(body_fun(init_vals, [x[0] for x in xs])[0], [x[1] for x in xs])[0]\n     self.assertAllClose(outs, outs_ref, check_dtypes=False)\n \n+  @parameterized.parameters(itertools.product(range(3), repeat=4))\n+  @jtu.run_on_devices(\"cpu\")\n+  def test_scan_forwarding_correctness(\n+      self,\n+      seed,\n+      num_body_consts,\n+      num_const_fwds,\n+      num_input_fwds):\n+\n+    num_carry = num_const_fwds + 4\n+    num_xs = num_input_fwds + 2\n+    num_ys = num_xs + 1\n+\n+    rng = np.random.RandomState(seed)\n+    carry_perm = rng.permutation(num_carry)\n+    carry_iperm = np.argsort(carry_perm)\n+\n+    xs_perm = rng.permutation(num_xs)\n+    ys_perm = rng.permutation(num_ys)\n+    f = np.arange(num_xs)\n+    f = [f[i] if idx < num_input_fwds else None for idx, i in enumerate(xs_perm)]\n+    f += [None]\n+    in_fwd = [f[i] for i in ys_perm]\n+\n+    body_consts = [rng.randn(3) for _ in range(num_body_consts)]\n+    init_vals = list(rng.uniform(size=num_carry))\n+\n+    def body_fun(c, x):\n+      c = [c[i] for i in carry_iperm]\n+      carry_fwds, carry_dont_fwd = split_list(c, [num_const_fwds])\n+      carry_dont_fwd = [jnp.sin(x) * sum(jnp.sum(c) for c in body_consts)\n+                        for x in carry_dont_fwd]\n+      new_c_perm = [*carry_fwds, *carry_dont_fwd]\n+      new_c = [new_c_perm[i] for i in carry_perm]\n+\n+      x = [x[i] for i in xs_perm]\n+      x_fwd, x_dont_fwd = split_list(x, [num_input_fwds])\n+      x_dont_fwd = [jnp.cos(x) * sum(jnp.sum(c) for c in body_consts)\n+                    for x in x_dont_fwd]\n+      y = [*x_fwd, *x_dont_fwd, 0]\n+      y = [y[i] for i in ys_perm]\n+\n+      return new_c, y\n+\n+    xs = list(rng.uniform(size=(num_xs, 2)))\n+    final, outs = jax.lax.scan(body_fun, init_vals, xs)\n+    for f, y in zip(in_fwd, outs):\n+      if f is not None:\n+        self.assertAllClose(y, xs[f])\n+\n+    final_ref = body_fun(body_fun(init_vals, [x[0] for x in xs])[0], [x[1] for x in xs])[0]\n+    self.assertAllClose(final, final_ref, check_dtypes=False)\n+\n   def test_scan_diff_of_print(self):\n     # ref: https://github.com/jax-ml/jax/issues/28738\n     def f(c, _):\n@@ -3311,6 +3364,14 @@ def g(x):\n     eqn_jaxpr = jaxpr.eqns[0].params[\"jaxpr\"]\n     self.assertIn(\"debug_callback\", [e.primitive.name for e in eqn_jaxpr.eqns])\n \n+  def test_scan_input_to_output_forwarding(self):\n+    def f(c, x):\n+      return c + 1, x\n+    def g(x):\n+      return jax.lax.scan(f, 0, x)\n+    jaxpr = jax.make_jaxpr(g)(jnp.arange(3.))\n+    self.assertLen(jaxpr.eqns[0].params[\"jaxpr\"].jaxpr.outvars, 1)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas/Fuser] Add custom_vjp_call rule for physicalize\n\nPiperOrigin-RevId: 764763254",
        "output": "```diff\nCommit: 42977e51816b9eb42c7360abe05f56cad70e894a\nDate: 2025-05-29T16:33:08Z\nURL: https://github.com/jax-ml/jax/commit/42977e51816b9eb42c7360abe05f56cad70e894a\nFiles changed: 2\nAdditions: +21, Deletions: -0\ndiff --git a/jax/_src/pallas/fuser/BUILD b/jax/_src/pallas/fuser/BUILD\nindex a4c3402f5309..d5b5d128241d 100644\n--- a/jax/_src/pallas/fuser/BUILD\n+++ b/jax/_src/pallas/fuser/BUILD\n@@ -115,6 +115,7 @@ pytype_strict_library(\n         \"//jax\",\n         \"//jax:api_util\",\n         \"//jax:core\",\n+        \"//jax:custom_derivatives\",\n         \"//jax:dtypes\",\n         \"//jax:partial_eval\",\n         \"//jax:source_info_util\",\ndiff --git a/jax/_src/pallas/fuser/fusible_dtype.py b/jax/_src/pallas/fuser/fusible_dtype.py\nindex 7d9c2ca67855..09cd8f57dbc1 100644\n--- a/jax/_src/pallas/fuser/fusible_dtype.py\n+++ b/jax/_src/pallas/fuser/fusible_dtype.py\n@@ -22,6 +22,7 @@\n import jax\n from jax._src import api_util\n from jax._src import core\n+from jax._src import custom_derivatives\n from jax._src import dtypes\n from jax._src import linear_util as lu\n from jax._src import source_info_util\n@@ -312,6 +313,25 @@ def _cond_physicalize_rule(ctx: Context, *args, branches, **kwargs):\n _physicalize_rules[conditionals.cond_p] = _cond_physicalize_rule\n \n \n+def _custom_vjp_call_physicalize_rule(\n+    ctx: Context, *args, call_jaxpr, num_consts, fwd_jaxpr_thunk, bwd, **kwargs\n+):\n+  _assert_no_fusion_types(ctx.avals_out)\n+  new_jaxpr = physicalize_closed_jaxpr(call_jaxpr)\n+  fun = lu.wrap_init(core.jaxpr_as_fun(new_jaxpr),\n+                     debug_info=call_jaxpr.jaxpr.debug_info)\n+  fwd = custom_derivatives.lift_fwd(num_consts, fwd_jaxpr_thunk)\n+  new_fwd = lu.wrap_init(physicalize(fwd.f_transformed), debug_info=fwd.debug_info)\n+  const_avals, _ = util.split_list(new_jaxpr.in_avals, [num_consts])\n+  bwd = custom_derivatives._handle_consts_in_bwd(bwd, const_avals)\n+  new_bwd = lu.wrap_init(physicalize(bwd.f_transformed), debug_info=bwd.debug_info)\n+  return custom_derivatives.custom_vjp_call_p.bind(\n+      fun, new_fwd, new_bwd, *args, **kwargs\n+  )\n+\n+_physicalize_rules[custom_derivatives.custom_vjp_call_p] = _custom_vjp_call_physicalize_rule\n+\n+\n def _run_state_rule(ctx: Context, *args, jaxpr, which_linear, is_initialized):\n   _assert_no_fusion_types(ctx.avals_in)\n   _assert_no_fusion_types(ctx.avals_out)\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[CI] Move k8s actions test files out of .github directory",
        "output": "```diff\nCommit: 38ecd13a6c8fbd61308741bc63d1f07ed019806f\nDate: 2025-05-29T15:24:44Z\nURL: https://github.com/jax-ml/jax/commit/38ecd13a6c8fbd61308741bc63d1f07ed019806f\nFiles changed: 4\nAdditions: +6, Deletions: -2\ndiff --git a/.github/workflows/k8s.yaml b/.github/workflows/k8s.yaml\nindex 81552f9bb43b..86bc5e6c168b 100644\n--- a/.github/workflows/k8s.yaml\n+++ b/.github/workflows/k8s.yaml\n@@ -4,6 +4,8 @@ on:\n     branches:\n       - main\n     paths:\n+      - '.github/workflows/k8s.yaml'\n+      - 'ci/k8s/**'\n       - 'jax/distributed.py'\n       - 'jax/_src/distributed.py'\n       - 'jax/_src/clusters/**'\n@@ -11,6 +13,8 @@ on:\n     branches:\n       - main\n     paths:\n+      - '.github/workflows/k8s.yaml'\n+      - 'ci/k8s/**'\n       - 'jax/distributed.py'\n       - 'jax/_src/distributed.py'\n       - 'jax/_src/clusters/**'\n@@ -61,7 +65,7 @@ jobs:\n         run: kubectl apply -f jax/examples/k8s/svc-acct.yaml\n \n       - name: Submit test job\n-        run: kubectl apply -f jax/.github/workflows/k8s/${{ matrix.controller }}.yaml\n+        run: kubectl apply -f jax/ci/k8s/${{ matrix.controller }}.yaml\n \n       - name: Check job status\n         shell: bash -e -o pipefail {0}\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex 71b3d51caaa7..8cc28c9fe4ac 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -18,7 +18,7 @@ repos:\n     exclude: |\n       (?x)^(\n           examples/k8s/svc-acct\\.yaml |\n-          \\.github/workflows/k8s/indexed-job\\.yaml\n+          ci/k8s/indexed-job\\.yaml\n       )$\n   - id: end-of-file-fixer\n     # only include python files\ndiff --git a/.github/workflows/k8s/indexed-job.yaml b/ci/k8s/indexed-job.yaml\nsimilarity index 100%\nrename from .github/workflows/k8s/indexed-job.yaml\nrename to ci/k8s/indexed-job.yaml\ndiff --git a/.github/workflows/k8s/jobset.yaml b/ci/k8s/jobset.yaml\nsimilarity index 100%\nrename from .github/workflows/k8s/jobset.yaml\nrename to ci/k8s/jobset.yaml\n\n```",
        "from_id": [
            "MichaelHudgins"
        ]
    },
    {
        "text_input": "Apply extensive input to extensive output forwarding in scan.",
        "output": "```diff\nCommit: 770eff03dfbc225011535cb32ab92ae40cff679b\nDate: 2025-05-29T14:59:39Z\nURL: https://github.com/jax-ml/jax/commit/770eff03dfbc225011535cb32ab92ae40cff679b\nFiles changed: 2\nAdditions: +82, Deletions: -1\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 83c31928d7cb..b9ce8ae09380 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -341,7 +341,7 @@ def _create_jaxpr(init):\n   # If the body forwards an input carry to an output carry, that input is\n   # read-only and can be moved to be a const. Doing so can lead to efficiency\n   # wins, e.g. if the scan is inside a cond with a batched predicate.\n-  carry_fwd, _ = split_list(pe._jaxpr_forwarding(jaxpr.jaxpr), [num_carry])\n+  carry_fwd, ext_fwd = split_list(pe._jaxpr_forwarding(jaxpr.jaxpr), [num_carry])\n   move_to_const = [len(consts) + i == f for i, f in enumerate(carry_fwd)]\n   if any(move_to_const):\n     jaxpr = pe.prune_closed_jaxpr_outputs(\n@@ -352,12 +352,32 @@ def _create_jaxpr(init):\n     consts = [*new_consts, *consts]\n     num_carry -= len(new_consts)\n \n+  # When an extensive output is forwarded from an extensive input, we can\n+  # avoid copying it by pruning it from the jaxpr and forwarding manually. We\n+  # don't need to update the indexing based on the optimization above since it\n+  # doesn't change the total number of consts and carries combined, and\n+  # `ext_fwd` already only includes the extensive outputs. But, we do remove\n+  # the number of consts from the index since we're going to use it to index\n+  # into `in_flat`, which doesn't include consts.\n+  ext_to_ext_fwd = [\n+      in_idx - len(consts) if in_idx is not None and\n+      in_idx >= num_carry + len(consts) else None for in_idx in ext_fwd]\n+  jaxpr = pe.prune_closed_jaxpr_outputs(\n+      jaxpr, [True] * num_carry + [i is None for i in ext_to_ext_fwd])\n+\n   out = scan_p.bind(*consts, *in_flat,\n                     reverse=reverse, length=length, jaxpr=jaxpr,\n                     num_consts=len(consts), num_carry=num_carry,\n                     linear=(False,) * (len(consts) + len(in_flat)),\n                     unroll=unroll, _split_transpose=_split_transpose)\n \n+  # Apply input to output forwarding that was computed above.\n+  carry_out, out = split_list(out, [num_carry])\n+  out_ = iter(out)\n+  out = [next(out_) if f is None else _maybe_put(in_flat[f]) for f in ext_to_ext_fwd]\n+  assert next(out_, None) is None\n+  out = [*carry_out, *out]\n+\n   if any(move_to_const):\n     out = pe.merge_lists(move_to_const + [False] * num_ys, out, new_consts)\n \ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex 54dff47fea32..2f1e154627f4 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -3299,6 +3299,59 @@ def body_fun(c, _):\n     outs_ref = body_fun(body_fun(init_vals, [x[0] for x in xs])[0], [x[1] for x in xs])[0]\n     self.assertAllClose(outs, outs_ref, check_dtypes=False)\n \n+  @parameterized.parameters(itertools.product(range(3), repeat=4))\n+  @jtu.run_on_devices(\"cpu\")\n+  def test_scan_forwarding_correctness(\n+      self,\n+      seed,\n+      num_body_consts,\n+      num_const_fwds,\n+      num_input_fwds):\n+\n+    num_carry = num_const_fwds + 4\n+    num_xs = num_input_fwds + 2\n+    num_ys = num_xs + 1\n+\n+    rng = np.random.RandomState(seed)\n+    carry_perm = rng.permutation(num_carry)\n+    carry_iperm = np.argsort(carry_perm)\n+\n+    xs_perm = rng.permutation(num_xs)\n+    ys_perm = rng.permutation(num_ys)\n+    f = np.arange(num_xs)\n+    f = [f[i] if idx < num_input_fwds else None for idx, i in enumerate(xs_perm)]\n+    f += [None]\n+    in_fwd = [f[i] for i in ys_perm]\n+\n+    body_consts = [rng.randn(3) for _ in range(num_body_consts)]\n+    init_vals = list(rng.uniform(size=num_carry))\n+\n+    def body_fun(c, x):\n+      c = [c[i] for i in carry_iperm]\n+      carry_fwds, carry_dont_fwd = split_list(c, [num_const_fwds])\n+      carry_dont_fwd = [jnp.sin(x) * sum(jnp.sum(c) for c in body_consts)\n+                        for x in carry_dont_fwd]\n+      new_c_perm = [*carry_fwds, *carry_dont_fwd]\n+      new_c = [new_c_perm[i] for i in carry_perm]\n+\n+      x = [x[i] for i in xs_perm]\n+      x_fwd, x_dont_fwd = split_list(x, [num_input_fwds])\n+      x_dont_fwd = [jnp.cos(x) * sum(jnp.sum(c) for c in body_consts)\n+                    for x in x_dont_fwd]\n+      y = [*x_fwd, *x_dont_fwd, 0]\n+      y = [y[i] for i in ys_perm]\n+\n+      return new_c, y\n+\n+    xs = list(rng.uniform(size=(num_xs, 2)))\n+    final, outs = jax.lax.scan(body_fun, init_vals, xs)\n+    for f, y in zip(in_fwd, outs):\n+      if f is not None:\n+        self.assertAllClose(y, xs[f])\n+\n+    final_ref = body_fun(body_fun(init_vals, [x[0] for x in xs])[0], [x[1] for x in xs])[0]\n+    self.assertAllClose(final, final_ref, check_dtypes=False)\n+\n   def test_scan_diff_of_print(self):\n     # ref: https://github.com/jax-ml/jax/issues/28738\n     def f(c, _):\n@@ -3311,6 +3364,14 @@ def g(x):\n     eqn_jaxpr = jaxpr.eqns[0].params[\"jaxpr\"]\n     self.assertIn(\"debug_callback\", [e.primitive.name for e in eqn_jaxpr.eqns])\n \n+  def test_scan_input_to_output_forwarding(self):\n+    def f(c, x):\n+      return c + 1, x\n+    def g(x):\n+      return jax.lax.scan(f, 0, x)\n+    jaxpr = jax.make_jaxpr(g)(jnp.arange(3.))\n+    self.assertLen(jaxpr.eqns[0].params[\"jaxpr\"].jaxpr.outvars, 1)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Merge pull request #29081 from DanisNone:main\n\nPiperOrigin-RevId: 764711492",
        "output": "```diff\nCommit: 5f1105432285b479eb7566cecdba4aade3f1030e\nDate: 2025-05-29T14:00:42Z\nURL: https://github.com/jax-ml/jax/commit/5f1105432285b479eb7566cecdba4aade3f1030e\nFiles changed: 2\nAdditions: +33, Deletions: -2\ndiff --git a/jax/_src/lax/other.py b/jax/_src/lax/other.py\nindex 00e15ef6a91d..6da39b0c2405 100644\n--- a/jax/_src/lax/other.py\n+++ b/jax/_src/lax/other.py\n@@ -287,3 +287,35 @@ def _logaddexp_jvp(primals, tangents):\n   tangent_out = lax.add(lax.mul(t1, lax.exp(lax.sub(_replace_inf(x1), _replace_inf(primal_out)))),\n                         lax.mul(t2, lax.exp(lax.sub(_replace_inf(x2), _replace_inf(primal_out)))))\n   return primal_out, tangent_out\n+\n+\n+@custom_jvp\n+def logaddexp2(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n+  \"\"\"Compute log2(exp2(x1) + exp2(x2)) avoiding overflow.\"\"\"\n+  x1_arr = lax.asarray(x1)\n+  x2_arr = lax.asarray(x2)\n+  assert x1_arr.dtype == x2_arr.dtype\n+\n+  amax = lax.max(x1_arr, x2_arr)\n+  invln2 = lax._const(amax, 1/np.log(2))\n+  if dtypes.isdtype(x1_arr.dtype, \"real floating\"):\n+    delta = lax.sub(x1_arr, x2_arr)\n+    return lax.select(lax._isnan(delta),\n+                      lax.add(x1_arr, x2_arr),  # NaNs or infinities of the same sign.\n+                      lax.add(amax, lax.mul(invln2, lax.log1p(lax.exp2(lax.neg(lax.abs(delta)))))))\n+  elif dtypes.isdtype(x1_arr.dtype, \"complex floating\"):\n+    delta = lax.sub(lax.add(x1_arr, x2_arr), lax.mul(amax, lax._const(amax, 2)))\n+    out = lax.add(amax, lax.mul(invln2, lax.log1p(lax.exp2(delta))))\n+    return lax.complex(lax.real(out), _wrap_between(lax.imag(out), np.pi / np.log(2)))\n+  else:\n+    raise ValueError(f\"logaddexp2 requires floating-point or complex inputs; got {x1_arr.dtype}\")\n+\n+\n+@logaddexp2.defjvp\n+def _logaddexp2_jvp(primals, tangents):\n+  x1, x2 = primals\n+  t1, t2 = tangents\n+  primal_out = logaddexp2(x1, x2)\n+  tangent_out = lax.add(lax.mul(t1, lax.exp2(lax.sub(_replace_inf(x1), _replace_inf(primal_out)))),\n+                        lax.mul(t2, lax.exp2(lax.sub(_replace_inf(x2), _replace_inf(primal_out)))))\n+  return primal_out, tangent_out\ndiff --git a/jax/_src/numpy/ufuncs.py b/jax/_src/numpy/ufuncs.py\nindex 77b1220214ed..d722534e3136 100644\n--- a/jax/_src/numpy/ufuncs.py\n+++ b/jax/_src/numpy/ufuncs.py\n@@ -2782,8 +2782,7 @@ def logaddexp2(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n     Array(True, dtype=bool)\n   \"\"\"\n   x1, x2 = promote_args_inexact(\"logaddexp2\", x1, x2)\n-  ln2 = float(np.log(2))\n-  return logaddexp(x1 * ln2, x2 * ln2) / ln2\n+  return lax_other.logaddexp2(x1, x2)\n \n \n @export\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] `emit_pipeline` now allows specifying a carry\n\nPiperOrigin-RevId: 764672556",
        "output": "```diff\nCommit: 50253f1acfe5434a7a50507fd940641984d9e7a7\nDate: 2025-05-29T11:35:25Z\nURL: https://github.com/jax-ml/jax/commit/50253f1acfe5434a7a50507fd940641984d9e7a7\nFiles changed: 2\nAdditions: +65, Deletions: -19\ndiff --git a/jax/_src/pallas/mosaic_gpu/pipeline.py b/jax/_src/pallas/mosaic_gpu/pipeline.py\nindex a7f8d32677b0..f85b73b6b946 100644\n--- a/jax/_src/pallas/mosaic_gpu/pipeline.py\n+++ b/jax/_src/pallas/mosaic_gpu/pipeline.py\n@@ -16,13 +16,12 @@\n \n from __future__ import annotations\n \n-from typing import Protocol, TypeVar\n from collections.abc import Callable, Sequence\n import dataclasses\n import functools\n import itertools as it\n import math\n-from typing import Any\n+from typing import Any, Protocol, TypeVar\n \n import jax\n from jax import api_util\n@@ -176,28 +175,44 @@ def __eq__(self, other: _Slice) -> jax.Array:  # type: ignore\n \n \n def emit_pipeline(\n-    body: Callable[..., None],\n+    body: Callable[..., T],\n     *,\n     grid: pallas_core.TupleGrid,\n     in_specs: Sequence[pallas_core.BlockSpec] = (),\n     out_specs: Sequence[pallas_core.BlockSpec] = (),\n     max_concurrent_steps: int = 1,\n     delay_release: int = 0,\n+    init_carry: T | None = None,\n ):\n-  \"\"\"Creates a function to emit a manual pipeline within a Pallas kernel.\n+  r\"\"\"Creates a function to emit a manual pipeline within a Pallas kernel.\n \n   Args:\n-    body: The pipeline body, called with the indices for the current step, the\n-      input refs, followed by the output refs.\n-    grid: The grid to use for the pipeline.\n-    in_specs: The block specs for the inputs.\n-    out_specs: The block specs for the outputs.\n-    max_concurrent_steps: The maximum number of sequential stages that are\n-      active concurrently. Defaults to 1.\n-    delay_release: The number of steps to wait before reusing the input/output\n-      references. Defaults to 0, and must be strictly smaller than\n-      ``max_concurrent_steps``. Generally, you'll want to set it to 1 if you\n-      don't await the WGMMA in the body.\n+    body: The pipeline body function, which is called with\n+\n+      - ``indices``: Tuple of current loop indices.\n+      - ``*input_refs``: SMEM refs for inputs.\n+      - ``*output_refs``: SMEM refs for outputs.\n+\n+      If ``init_carry`` is provided, ``body`` receives an additional argument\n+      ``carry`` -- the carry from the previous iteration. It must then return\n+      the next carry value.\n+    grid: The grid dimensions for the pipeline.\n+    in_specs: A sequence of :class:`~jax.experimental.pallas.BlockSpec`\\s\n+      for inputs.\n+    out_specs: A sequence of :class:`~jax.experimental.pallas.BlockSpec`\\s\n+      for outputs.\n+    max_concurrent_steps: Maximum concurrently active pipeline stages.\n+    delay_release: Number of steps to delay before reusing input/output\n+      references. Must be ``< max_concurrent_steps``. Useful for hiding WGMMA\n+      latency (typically set to 1).\n+    init_carry: Optional initial carry. If provided, ``body`` handles\n+      carry-over state between iterations, and the pipeline returns the\n+      final carry.\n+\n+  Returns:\n+    A function that, when called with GMEM input and output refs, executes the\n+    pipeline and returns the final carry value (if ``init_carry`` was used),\n+    otherwise it returns None.\n   \"\"\"\n   if max_concurrent_steps <= delay_release:\n     raise ValueError(\n@@ -278,7 +293,7 @@ def scoped_pipeline(\n \n     def loop_body(step, carry):\n       slot = lax.rem(step, max_concurrent_steps)\n-      indices, fetch_indices, last_store_slices = carry\n+      indices, fetch_indices, last_store_slices, prev_body_carry = carry\n \n       if barrier_ref is not None:\n         # Wait for the current GMEM->SMEM copy to complete, if any.\n@@ -289,12 +304,13 @@ def loop_body(step, carry):\n             max_concurrent_steps - (1 + delay_release), wait_read_only=True\n         )\n \n-      body(\n+      next_body_carry = body(\n           indices,\n           *(\n               bref.get_ref_for_slot(slot)\n               for bref in it.chain(in_brefs, out_brefs)\n           ),\n+          *(prev_body_carry,) if init_carry is not None else (),\n       )\n \n       if copies_out_in_loop:\n@@ -346,6 +362,7 @@ def do_fetch():\n           _inc_grid_by_1(indices, grid),\n           _inc_grid_by_1(fetch_indices, grid),\n           new_store_slices,\n+          next_body_carry if init_carry is not None else None,\n       )\n \n     # Invariant: ``indices`` and ``fetch_indices`` are always\n@@ -360,8 +377,11 @@ def do_fetch():\n         else (_Slice(-1, -1),) * len(bref.spec.block_shape)\n         for bref in out_brefs\n     ]\n-    last_indices, _, _ = lax.fori_loop(\n-        0, num_steps, loop_body, (indices, fetch_indices, last_store_slices)\n+    last_indices, _, _, final_carry = lax.fori_loop(\n+        0,\n+        num_steps,\n+        loop_body,\n+        (indices, fetch_indices, last_store_slices, init_carry),\n     )\n \n     # Outputs invariant to the sequential axis are never written from inside the\n@@ -378,6 +398,7 @@ def do_fetch():\n \n     # Finalize the pipeline.\n     gpu_primitives.wait_smem_to_gmem(0)\n+    return final_carry if init_carry is not None else None\n \n   return pipeline\n \ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 608cfcba2465..f9a23e7be9d0 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -2841,6 +2841,31 @@ def kernel_body(_, x_smem, o_smem):\n     )\n     np.testing.assert_array_equal(kernel_fn(x), x + 1.0)\n \n+  def test_emit_with_carry(self):\n+    num_steps = 4\n+\n+    def kernel(o_gmem):\n+      plgpu.emit_pipeline(\n+          kernel_body,\n+          out_specs=[pl.BlockSpec((64, 64), lambda i: (0, i))],\n+          grid=(num_steps,),\n+          max_concurrent_steps=2,\n+          init_carry=0,\n+      )(o_gmem)\n+\n+    def kernel_body(_, o_smem, carry):\n+      o_smem[...] = lax.broadcast(carry, o_smem.shape)\n+      return carry + 1\n+\n+    kernel_fn = self.pallas_call(\n+        kernel,\n+        out_specs=pl.BlockSpec(memory_space=plgpu.GMEM),\n+        out_shape=jax.ShapeDtypeStruct((64, num_steps * 64), jnp.int32),\n+    )\n+    np.testing.assert_array_equal(\n+        kernel_fn(), jnp.tile(jnp.repeat(jnp.arange(num_steps), 64), (64, 1))\n+    )\n+\n \n class PipelineWGTest(\n     PipelineTest, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[mosaic_gpu] Use `DIScopeForLLVMFuncOpPass` from MLIR instead of its Triton fork\n\nPiperOrigin-RevId: 764652343",
        "output": "```diff\nCommit: 89828819a383911db365661e8c7b6203299c35fb\nDate: 2025-05-29T10:23:14Z\nURL: https://github.com/jax-ml/jax/commit/89828819a383911db365661e8c7b6203299c35fb\nFiles changed: 2\nAdditions: +8, Deletions: -15\ndiff --git a/jaxlib/mosaic/gpu/BUILD b/jaxlib/mosaic/gpu/BUILD\nindex fc1abb9397d5..d2abea0048d6 100644\n--- a/jaxlib/mosaic/gpu/BUILD\n+++ b/jaxlib/mosaic/gpu/BUILD\n@@ -151,6 +151,8 @@ cc_library(\n         \":nvshmem\",\n         \":passes\",\n         \":target\",\n+        \"//jaxlib/cuda:cuda_vendor\",\n+        \"//jaxlib/mosaic/dialect/gpu:mosaic_gpu\",\n         \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/cleanup\",\n@@ -181,6 +183,7 @@ cc_library(\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:IndexToLLVM\",\n         \"@llvm-project//mlir:LLVMDialect\",\n+        \"@llvm-project//mlir:LLVMIRTransforms\",\n         \"@llvm-project//mlir:LLVMToLLVMIRTranslation\",\n         \"@llvm-project//mlir:MathDialect\",\n         \"@llvm-project//mlir:MathToLLVM\",\n@@ -200,16 +203,11 @@ cc_library(\n         \"@llvm-project//mlir:UBToLLVM\",\n         \"@llvm-project//mlir:VectorDialect\",\n         \"@llvm-project//mlir:VectorToLLVM\",\n-        \"//jaxlib/cuda:cuda_vendor\",\n-        \"//jaxlib/mosaic/dialect/gpu:mosaic_gpu\",\n+        \"@tsl//tsl/profiler/lib:traceme\",\n         \"@xla//xla/ffi\",\n         \"@xla//xla/ffi:ffi_api\",\n         \"@xla//xla/service:custom_call_status\",\n         \"@xla//xla/service:custom_call_target_registry\",\n-        \"@tsl//tsl/profiler/lib:traceme\",\n-        # TODO(slebedev): Remove once enable-line-info is merged into the upstream\n-        # ensure-debug-info-scope-on-llvm-func pass in MLIR.\n-        \"@triton//:TritonLLVMIR\",\n     ],\n     alwayslink = True,\n )\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 524d0ffe23ab..5253d4590658 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -68,6 +68,7 @@ limitations under the License.\n #include \"mlir/Dialect/GPU/Transforms/Passes.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/Transforms/Passes.h\"\n #include \"mlir/Dialect/Math/IR/Math.h\"\n #include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n #include \"mlir/Dialect/MemRef/Transforms/Passes.h\"\n@@ -102,7 +103,6 @@ limitations under the License.\n #include \"xla/service/custom_call_status.h\"\n #include \"xla/service/custom_call_target_registry.h\"\n #include \"tsl/profiler/lib/traceme.h\"\n-#include \"triton/Target/LLVMIR/Passes.h\"\n \n namespace {\n \n@@ -340,7 +340,7 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n     mosaic::gpu::registerConvertGpuToLLVMPass();\n     mosaic::gpu::registerByvalInsertionPass();\n     mlir::arith::registerArithExpandOpsPass();\n-    mlir::registerLLVMDIScopePass();\n+    mlir::LLVM::registerDIScopeForLLVMFuncOpPass();\n     return true;\n   });\n   bool emit_line_info = getenv(\"MOSAIC_GPU_LINE_INFO\") != nullptr;\n@@ -360,10 +360,7 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n         convert-scf-to-cf,\n         convert-nvvm-to-llvm,\n         expand-strided-metadata,\n-        nvvm-attach-target{)\",\n-      // TODO(slebedev): Always use O=3 once\n-      // https://github.com/llvm/llvm-project/pull/140146 is merged.\n-      emit_line_info ? \"O=0\" : \"O=3\", \" chip=\", sm, \" fast=false features=+\",\n+        nvvm-attach-target{O=3 chip=)\", sm, \" fast=false features=+\",\n       ptx_isa,\n       R\"( ftz=false  module= triple=nvptx64-nvidia-cuda},\n         lower-affine,\n@@ -381,9 +378,7 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n         gpu.module(reconcile-unrealized-casts),\n         mosaic-convert-gpu-to-llvm,\n         )\",\n-      // TODO(slebedev): Switch to the ensure-debug-info-scope-on-llvm-func\n-      // pass in MLIR once Triton upstreams its changes.\n-      emit_line_info ? \"enable-line-info,\" : \"\",\n+      emit_line_info ? \"ensure-debug-info-scope-on-llvm-func{emission-kind=DebugDirectivesOnly},\" : \"\",\n       \"gpu-module-to-binary{format=\",\n       mlir::gpu::stringifyCompilationTarget(target).str(),\n       (!nvshmem_path.empty() ? \" l=\" + nvshmem_path : \"\"),\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "A more numerically stable implementation of logaddexp2",
        "output": "```diff\nCommit: a4a31ecd8476a4d10dedcf226d5a116af2d056b3\nDate: 2025-05-29T06:02:14Z\nURL: https://github.com/jax-ml/jax/commit/a4a31ecd8476a4d10dedcf226d5a116af2d056b3\nFiles changed: 2\nAdditions: +33, Deletions: -2\ndiff --git a/jax/_src/lax/other.py b/jax/_src/lax/other.py\nindex 00e15ef6a91d..6da39b0c2405 100644\n--- a/jax/_src/lax/other.py\n+++ b/jax/_src/lax/other.py\n@@ -287,3 +287,35 @@ def _logaddexp_jvp(primals, tangents):\n   tangent_out = lax.add(lax.mul(t1, lax.exp(lax.sub(_replace_inf(x1), _replace_inf(primal_out)))),\n                         lax.mul(t2, lax.exp(lax.sub(_replace_inf(x2), _replace_inf(primal_out)))))\n   return primal_out, tangent_out\n+\n+\n+@custom_jvp\n+def logaddexp2(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n+  \"\"\"Compute log2(exp2(x1) + exp2(x2)) avoiding overflow.\"\"\"\n+  x1_arr = lax.asarray(x1)\n+  x2_arr = lax.asarray(x2)\n+  assert x1_arr.dtype == x2_arr.dtype\n+\n+  amax = lax.max(x1_arr, x2_arr)\n+  invln2 = lax._const(amax, 1/np.log(2))\n+  if dtypes.isdtype(x1_arr.dtype, \"real floating\"):\n+    delta = lax.sub(x1_arr, x2_arr)\n+    return lax.select(lax._isnan(delta),\n+                      lax.add(x1_arr, x2_arr),  # NaNs or infinities of the same sign.\n+                      lax.add(amax, lax.mul(invln2, lax.log1p(lax.exp2(lax.neg(lax.abs(delta)))))))\n+  elif dtypes.isdtype(x1_arr.dtype, \"complex floating\"):\n+    delta = lax.sub(lax.add(x1_arr, x2_arr), lax.mul(amax, lax._const(amax, 2)))\n+    out = lax.add(amax, lax.mul(invln2, lax.log1p(lax.exp2(delta))))\n+    return lax.complex(lax.real(out), _wrap_between(lax.imag(out), np.pi / np.log(2)))\n+  else:\n+    raise ValueError(f\"logaddexp2 requires floating-point or complex inputs; got {x1_arr.dtype}\")\n+\n+\n+@logaddexp2.defjvp\n+def _logaddexp2_jvp(primals, tangents):\n+  x1, x2 = primals\n+  t1, t2 = tangents\n+  primal_out = logaddexp2(x1, x2)\n+  tangent_out = lax.add(lax.mul(t1, lax.exp2(lax.sub(_replace_inf(x1), _replace_inf(primal_out)))),\n+                        lax.mul(t2, lax.exp2(lax.sub(_replace_inf(x2), _replace_inf(primal_out)))))\n+  return primal_out, tangent_out\ndiff --git a/jax/_src/numpy/ufuncs.py b/jax/_src/numpy/ufuncs.py\nindex 77b1220214ed..d722534e3136 100644\n--- a/jax/_src/numpy/ufuncs.py\n+++ b/jax/_src/numpy/ufuncs.py\n@@ -2782,8 +2782,7 @@ def logaddexp2(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n     Array(True, dtype=bool)\n   \"\"\"\n   x1, x2 = promote_args_inexact(\"logaddexp2\", x1, x2)\n-  ln2 = float(np.log(2))\n-  return logaddexp(x1 * ln2, x2 * ln2) / ln2\n+  return lax_other.logaddexp2(x1, x2)\n \n \n @export\n\n```",
        "from_id": [
            "DanisNone"
        ]
    },
    {
        "text_input": "[Pallas Fuser] Add support for basic PRNG op fusion\n\nPiperOrigin-RevId: 764490044",
        "output": "```diff\nCommit: 37a9ac23681d85e5e5663d21fac4b88224715ba3\nDate: 2025-05-29T00:46:45Z\nURL: https://github.com/jax-ml/jax/commit/37a9ac23681d85e5e5663d21fac4b88224715ba3\nFiles changed: 4\nAdditions: +186, Deletions: -46\ndiff --git a/jax/_src/pallas/core.py b/jax/_src/pallas/core.py\nindex 7950f90bc377..a05f97eb122f 100644\n--- a/jax/_src/pallas/core.py\n+++ b/jax/_src/pallas/core.py\n@@ -272,6 +272,7 @@ class MemorySpace(enum.Enum):\n   ANY = \"any\"  # Unrestricted memory space (usually HBM)\n   ERROR = \"error\"  # Memory space for checkify errors.\n   INDEX = \"index\"  # Memory space for scalar prefetch arguments.\n+  KEY = \"key\"  # Memory space for PRNG keys.\n \n   def __str__(self) -> str:\n     return self.value\ndiff --git a/jax/_src/pallas/fuser/block_spec.py b/jax/_src/pallas/fuser/block_spec.py\nindex 3e9ff497bf1e..4f9d1c344429 100644\n--- a/jax/_src/pallas/fuser/block_spec.py\n+++ b/jax/_src/pallas/fuser/block_spec.py\n@@ -29,6 +29,7 @@\n from jax._src import core\n from jax._src import custom_derivatives\n from jax._src import pjit\n+from jax._src import prng\n from jax._src import state\n from jax._src import tree_util\n from jax._src import util\n@@ -215,7 +216,7 @@ def _wrap_block_spec_scalar_prefetch(\n     block_spec: pallas_core.BlockSpec,\n     num_grid_args: int,\n ) -> pallas_core.BlockSpec:\n-  if block_spec is pallas_core.no_block_spec:\n+  if block_spec is pallas_core.no_block_spec or block_spec.index_map is None:\n     return block_spec\n \n   def new_index_map(*args_and_scalar_prefetch):\n@@ -272,11 +273,12 @@ def wrapped(*args, **kwargs):\n     )\n     assert all(used_invars)\n     assert all(used_consts)\n+    read_usage_env = compute_usage(jaxpr, jaxpr_out_usages)\n     in_block_specs, env, read_usage_env = _pull_block_spec(\n         jaxpr,\n         tuple(flat_block_specs),\n-        jaxpr_out_usages,\n         scalar_prefetch_handler=scalar_prefetch_handler,\n+        read_usage_env=read_usage_env,\n         grid=grid,\n     )\n     kernel_fn = make_kernel_function(\n@@ -307,8 +309,8 @@ def wrapped(*args, **kwargs):\n def _pull_block_spec(\n     jaxpr: core.Jaxpr,\n     out_block_specs: tuple[pallas_core.BlockSpec, ...],\n-    out_usages,\n     *,\n+    read_usage_env: Callable[[core.Var], set[Usage]],\n     scalar_prefetch_handler: Any | None = None,\n     grid: tuple[int | jax.Array, ...],\n ) -> tuple[\n@@ -316,7 +318,6 @@ def _pull_block_spec(\n     tuple[dict[core.Var, pallas_core.BlockSpec], dict[int, Any]],\n     Any,\n ]:\n-  read_usage_env = compute_usage(jaxpr, out_usages)\n   jaxpr_invar_usages = util.safe_map(read_usage_env, jaxpr.invars)\n   env: dict[core.Var, pallas_core.BlockSpec] = {}\n   scalar_prefetch_fn_env = {}\n@@ -456,6 +457,8 @@ def _get_block_aval(bs, aval):\n       return aval\n     if bs is pallas_core.no_block_spec or bs is None:\n       return _no_aval\n+    if bs.block_shape is None:\n+      return aval\n     return aval.update(shape=_remove_nones(bs.block_shape))  # pytype: disable=attribute-error\n \n   in_block_avals = [\n@@ -830,7 +833,10 @@ def register_binop_rule(prim: core.Primitive):\n register_binop_rule(lax.eq_p)\n register_binop_rule(lax.gt_p)\n register_binop_rule(lax.ge_p)\n+register_binop_rule(lax.or_p)\n+register_binop_rule(lax.xor_p)\n register_binop_rule(lax.and_p)\n+register_binop_rule(lax.shift_right_logical_p)\n register_binop_rule(ad_util.add_any_p)\n \n \n@@ -1473,6 +1479,68 @@ def _convert_element_type_pull_rule(\n   return [block_spec]\n \n \n+@register_eval_rule(lax.bitcast_convert_type_p)\n+def _bitcast_convert_type_eval_rule(eval_ctx: KernelEvalContext, x, new_dtype):\n+  return jax.lax.bitcast_convert_type(x, new_dtype)\n+\n+\n+@register_pull_block_spec_rule(lax.bitcast_convert_type_p)\n+def _bitcast_convert_type_pull_rule(\n+    ctx: PullRuleContext,\n+    block_spec: pallas_core.BlockSpec,\n+    *,\n+    new_dtype: jnp.dtype,\n+):\n+  old_dtype = ctx.avals_in[0].dtype  # pytype: disable=attribute-error\n+  if old_dtype.itemsize != new_dtype.itemsize:\n+    raise NotImplementedError(\n+        'bitcast_convert_type with different bitwidths not supported yet:'\n+        f' {old_dtype=}, {new_dtype=}'\n+    )\n+  return [block_spec]\n+\n+\n+@register_eval_rule(prng.random_bits_p)\n+def _random_bits_eval_rule(eval_ctx: KernelEvalContext, key, bit_width, shape):\n+  del shape\n+  block_spec = eval_ctx.out_block_specs[0]\n+  indices = eval_ctx.get_out_block_indices()[0]\n+  block_shape = block_spec.block_shape\n+  # This is the important part here: we fold in block indices into the key so\n+  # each block gets different random numbers.\n+  for idx in indices:\n+    key = jax.random.fold_in(key, idx)\n+  return prng.random_bits(key, bit_width=bit_width, shape=block_shape)\n+\n+\n+@register_pull_block_spec_rule(prng.random_bits_p)\n+def _random_bits_pull_rule(\n+    ctx: PullRuleContext,\n+    block_spec: pallas_core.BlockSpec,\n+    **_,\n+):\n+  del ctx, block_spec\n+  key_block_spec = pallas_core.BlockSpec(\n+      block_shape=None, memory_space=pallas_core.MemorySpace.KEY\n+  )\n+  return [key_block_spec]\n+\n+@register_eval_rule(prng.random_wrap_p)\n+def _random_wrap_eval_rule(eval_ctx: KernelEvalContext, arr, *, impl):\n+  del eval_ctx\n+  return jax.random.wrap_key_data(arr, impl=impl)\n+\n+@register_pull_block_spec_rule(prng.random_wrap_p)\n+def _random_wrap_pull_rule(\n+    ctx: PullRuleContext,\n+    block_spec: pallas_core.BlockSpec,\n+    *,\n+    impl\n+):\n+  del ctx, block_spec, impl\n+  return [pallas_core.BlockSpec(block_shape=None)]\n+\n+\n @register_eval_rule(lax.iota_p)\n def _iota_eval_rule(\n     eval_ctx: KernelEvalContext, *, dimension, shape, dtype, sharding\n@@ -1599,12 +1667,13 @@ def _jit_eval_rule(ctx: KernelEvalContext, *args, jaxpr, **kwargs):\n     raise NotImplementedError('pjit with consts not supported yet')\n   out_tree = tree_util.tree_structure(tuple(jaxpr.outvars))\n   in_tree = tree_util.tree_structure((tuple(jaxpr.invars), {}))\n-  read_usage_env = compute_usage(jaxpr, ctx.out_usages)\n+  def read_usage_env(_: core.Var):\n+    return {Usage.REGULAR}\n   _, env, _ = _pull_block_spec(\n       jaxpr,\n       ctx.out_block_specs,\n-      ctx.out_usages,\n       scalar_prefetch_handler=ctx.scalar_prefetch_handler,\n+      read_usage_env=read_usage_env,\n       grid=ctx.grid,\n   )\n   kernel_fn = make_kernel_function(\n@@ -1628,11 +1697,13 @@ def _jit_pull_block_spec_rule(\n   jaxpr, consts = jaxpr.jaxpr, jaxpr.consts\n   if consts:\n     raise NotImplementedError('pjit with consts not supported yet')\n+  def read_usage_env(_: core.Var):\n+    return {Usage.REGULAR}\n   in_block_specs, _, _ = _pull_block_spec(\n       jaxpr,\n       out_block_specs,\n-      ctx.out_usages,\n       scalar_prefetch_handler=ctx.scalar_prefetch_handler,\n+      read_usage_env=read_usage_env,\n       grid=ctx.grid,\n   )\n   return in_block_specs\n@@ -1657,13 +1728,14 @@ def _custom_jvp_call_eval_rule(\n     raise NotImplementedError('custom_jvp_call with consts not supported yet')\n   out_tree = tree_util.tree_structure(tuple(jaxpr.outvars))\n   in_tree = tree_util.tree_structure((tuple(jaxpr.invars), {}))\n-  read_usage_env = compute_usage(jaxpr, ctx.out_usages)\n+  def read_usage_env(_: core.Var):\n+    return {Usage.REGULAR}\n   _, env, _ = _pull_block_spec(\n       jaxpr,\n       ctx.out_block_specs,\n-      ctx.out_usages,\n       scalar_prefetch_handler=ctx.scalar_prefetch_handler,\n       grid=ctx.grid,\n+      read_usage_env=read_usage_env,\n   )\n   kernel_fn = make_kernel_function(\n       jaxpr,\n@@ -1686,12 +1758,14 @@ def _custom_jvp_call_pull_block_spec_rule(\n   jaxpr, consts = call_jaxpr.jaxpr, call_jaxpr.consts\n   if consts:\n     raise NotImplementedError('custom_jvp_call with consts not supported yet')\n+  def read_usage_env(_: core.Var):\n+    return {Usage.REGULAR}\n   in_block_specs, _, _ = _pull_block_spec(\n       jaxpr,\n       out_block_specs,\n-      ctx.out_usages,\n       scalar_prefetch_handler=ctx.scalar_prefetch_handler,\n       grid=ctx.grid,\n+      read_usage_env=read_usage_env,\n   )\n   return in_block_specs\n \ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 635c473620c3..c6aaf77199b5 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -222,7 +222,11 @@ def _memory_space_to_tpu_memory_space(memory_space: MemorySpace | None\n     case pallas_core.MemorySpace.ANY:\n       # Map the general ANY memory space to TPU ANY memory space\n       return TPUMemorySpace.ANY\n-    case pallas_core.MemorySpace.ERROR | pallas_core.MemorySpace.INDEX:\n+    case (\n+        pallas_core.MemorySpace.ERROR\n+        | pallas_core.MemorySpace.INDEX\n+        | pallas_core.MemorySpace.KEY\n+    ):\n       return TPUMemorySpace.SMEM\n     case TPUMemorySpace():\n       # Leave the memory space unchanged\n@@ -365,7 +369,7 @@ def _get_arg_type(\n ):\n   memory_space = None\n   if isinstance(aval, pallas_core.AbstractMemoryRef):\n-    memory_space = aval.memory_space\n+    memory_space = _memory_space_to_tpu_memory_space(aval.memory_space)\n     # We assume unannotated memory refs are in VMEM\n     if memory_space is None:\n       memory_space = TPUMemorySpace.VMEM\n@@ -595,10 +599,10 @@ def _check_block_mappings(\n     rank = len(bm.block_shape)\n     # TODO(necula): add tests for SMEM blocks with trivial windowing\n     # We support scalars too\n-    if (bm.block_aval.memory_space == tpu_core.TPUMemorySpace.SMEM and\n-        bm.has_trivial_window()):\n+    memory_space = _memory_space_to_tpu_memory_space(bm.block_aval.memory_space)\n+    if memory_space == tpu_core.TPUMemorySpace.SMEM and bm.has_trivial_window():\n       continue\n-    if bm.block_aval.memory_space == tpu_core.TPUMemorySpace.SEMAPHORE:\n+    if memory_space == tpu_core.TPUMemorySpace.SEMAPHORE:\n       continue\n \n     def err_details():\n@@ -614,8 +618,10 @@ def err_details():\n           \"The Pallas TPU lowering currently supports only blocks of \"\n           \"rank >= 1. \" + err_details())\n \n-    if (bm.block_aval.memory_space == tpu_core.TPUMemorySpace.ANY and\n-        not bm.has_trivial_window()):\n+    if (\n+        memory_space == tpu_core.TPUMemorySpace.ANY\n+        and not bm.has_trivial_window()\n+    ):\n       raise ValueError(\n           \"The Pallas TPU lowering currently supports in memory space ANY \"\n           \"only blocks having the same block shape as the array shape \"\n@@ -3723,10 +3729,16 @@ def new_lowering(key, bit_width, shape):\n \n @register_lowering_rule(prng.random_fold_in_p)\n def random_fold_in_lowering(ctx, keys, msgs):\n-  keys_aval, _ = ctx.avals_in\n+  keys_aval, msgs_aval = ctx.avals_in\n   impl = keys_aval.dtype._impl\n   fold_in_lowering = lower_fun(impl.fold_in, multiple_results=False)\n-  return fold_in_lowering(ctx, keys, msgs)\n+  if pl_random.is_pallas_impl(impl):\n+    return fold_in_lowering(ctx, keys, msgs)\n+  else:\n+    ctx = dataclasses.replace(ctx,\n+                        avals_in=[jax_core.physical_aval(keys_aval), msgs_aval],\n+                        avals_out=map(jax_core.physical_aval, ctx.avals_out))\n+    return fold_in_lowering(ctx, keys, msgs)\n \n \n @register_lowering_rule(prng.random_unwrap_p)\ndiff --git a/tests/pallas/fuser_block_spec_test.py b/tests/pallas/fuser_block_spec_test.py\nindex 5c0ef0352b1c..b348ba971c38 100644\n--- a/tests/pallas/fuser_block_spec_test.py\n+++ b/tests/pallas/fuser_block_spec_test.py\n@@ -755,7 +755,9 @@ def f():\n     )(new_values)\n     self.assertLen(value_block_specs, 1)\n     self.assertEmpty(scalar_prefetch_values)\n-    self.assertEqual(value_block_specs[0].block_shape, (pl.Element(128, (0, 16)), 128))\n+    self.assertEqual(\n+        value_block_specs[0].block_shape, (pl.Element(128, (0, 16)), 128)\n+    )\n     self.assertEqual(value_block_specs[0].index_map(0, 1, 2), (16, 1))\n     self.assertEqual(value_block_specs[0].index_map(1, 1, 2), (128 + 16, 1))\n \n@@ -801,10 +803,13 @@ def f(x):\n   def test_basic_swap(self):\n     value = jnp.arange((512 * 1024), dtype=jnp.int32).reshape((512, 1024)) * 2\n     x = jnp.zeros((256, 512), dtype=jnp.int32)\n+\n     def outer(refs):\n       ref, y_ref = refs\n+\n       def f(x):\n         return ref.swap(x)\n+\n       in_type = jax.ShapeDtypeStruct((512, 1024), jnp.int32)\n       f2, new_values, scalar_prefetch_values = block_spec_lib.get_fusion_values(\n           f, in_type\n@@ -826,73 +831,121 @@ def f(x):\n       self.assertEqual(x_block_spec.index_map(3, 2, 1), (3, 1))\n \n       y_ref[...] = kernel_fn((0, 1, 1), scalar_prefetch_values, (ref,), x)\n+\n     y = jnp.zeros((256, 512), jnp.int32)\n     _, y = pl.run_state(outer)((value, y))\n     np.testing.assert_array_equal(y, value[:256, 512:1024])\n \n   def test_basic_get(self):\n     value = jnp.arange((512 * 1024), dtype=jnp.int32).reshape((512, 1024)) * 2\n+\n     def outer(refs):\n       ref, y_ref = refs\n+\n       def f():\n         return ref.get()\n \n       block_spec = pl.BlockSpec((256, 512), lambda i, j, k: (i, k))\n-      kernel_fn, (), _ = (\n-          block_spec_lib.pull_block_spec(\n-              f,\n-              block_spec,\n-              grid=(2, 3, 4),\n-              scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n-          )()\n-      )\n+      kernel_fn, (), _ = block_spec_lib.pull_block_spec(\n+          f,\n+          block_spec,\n+          grid=(2, 3, 4),\n+          scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+      )()\n       y_ref[...] = kernel_fn((0, 1, 1), ())\n+\n     y = jnp.zeros((256, 512), jnp.int32)\n     _, y = pl.run_state(outer)((value, y))\n     np.testing.assert_array_equal(y, value[:256, 512:1024])\n \n   def test_get_with_squeezed_block_spec(self):\n-    value = jnp.arange((4 * 512 * 1024), dtype=jnp.int32).reshape((4, 512, 1024)) * 2\n+    value = (\n+        jnp.arange((4 * 512 * 1024), dtype=jnp.int32).reshape((4, 512, 1024))\n+        * 2\n+    )\n+\n     def outer(refs):\n       ref, y_ref = refs\n+\n       def f():\n         return ref.get()\n \n-      block_spec = pl.BlockSpec((pl.Squeezed(), 256, 512), lambda i, j, k: (j, i, k))\n-      kernel_fn, (), _ = (\n-          block_spec_lib.pull_block_spec(\n-              f,\n-              block_spec,\n-              grid=(2, 3, 4),\n-              scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n-          )()\n+      block_spec = pl.BlockSpec(\n+          (pl.Squeezed(), 256, 512), lambda i, j, k: (j, i, k)\n       )\n+      kernel_fn, (), _ = block_spec_lib.pull_block_spec(\n+          f,\n+          block_spec,\n+          grid=(2, 3, 4),\n+          scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+      )()\n       y_ref[...] = kernel_fn((0, 3, 1), ())\n+\n     y = jnp.zeros((256, 512), jnp.int32)\n     _, y = pl.run_state(outer)((value, y))\n     np.testing.assert_array_equal(y, value[3, :256, 512:1024])\n \n   def test_get_with_squeezed_indexer(self):\n-    value = jnp.arange((4 * 512 * 1024), dtype=jnp.int32).reshape((4, 512, 1024)) * 2\n+    value = (\n+        jnp.arange((4 * 512 * 1024), dtype=jnp.int32).reshape((4, 512, 1024))\n+        * 2\n+    )\n+\n     def outer(refs):\n       ref, y_ref = refs\n+\n       def f():\n         return ref[3]\n \n       block_spec = pl.BlockSpec((256, 512), lambda i, j, k: (i, k))\n-      kernel_fn, (), _ = (\n-          block_spec_lib.pull_block_spec(\n-              f,\n-              block_spec,\n-              grid=(2, 3, 4),\n-              scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n-          )()\n-      )\n+      kernel_fn, (), _ = block_spec_lib.pull_block_spec(\n+          f,\n+          block_spec,\n+          grid=(2, 3, 4),\n+          scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+      )()\n       y_ref[...] = kernel_fn((0, 2, 1), ())\n+\n     y = jnp.zeros((256, 512), jnp.int32)\n     _, y = pl.run_state(outer)((value, y))\n     np.testing.assert_array_equal(y, value[3, :256, 512:1024])\n \n+  def test_random_noise(self):\n+    key = jax.random.key(0, impl='threefry2x32')\n+\n+    def f(key):\n+      return jax.random.uniform(key, (512, 512), dtype=jnp.float32)\n+\n+    f2, new_values, scalar_prefetch_values = block_spec_lib.get_fusion_values(\n+        f, key\n+    )\n+    self.assertEmpty(new_values)\n+    self.assertEmpty(scalar_prefetch_values)\n+\n+    block_spec = pl.BlockSpec((128, 256), lambda i, j: (i, j))\n+    kernel_fn, (value_block_specs, key_block_spec), _ = (\n+        block_spec_lib.pull_block_spec(\n+            f2,\n+            block_spec,\n+            grid=(4, 2),\n+            scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+        )(new_values, key)\n+    )\n+    self.assertEmpty(value_block_specs)\n+    self.assertEqual(key_block_spec.memory_space, pl.MemorySpace.KEY)\n+    self.assertIsNone(key_block_spec.block_shape)\n+    @jax.jit\n+    def gen(idx):\n+      k = key\n+      for i in idx:\n+        k = jax.random.fold_in(k, i)\n+      return jax.random.uniform(k, (128, 256), dtype=jnp.float32)\n+    for i in range(4):\n+      for j in range(2):\n+        out = kernel_fn((i, j), scalar_prefetch_values, (), key)\n+        out_ref = gen((i, j))\n+        np.testing.assert_array_equal(out, out_ref)\n+\n \n class PullBlockSpecHOPTest(jtu.JaxTestCase):\n \n\n```",
        "from_id": [
            "sharadmv",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29047 from mattjj:returning-non-jaxtype\n\nPiperOrigin-RevId: 764449037",
        "output": "```diff\nCommit: 22b4f263b216a48e7db6c02cc8be8af258c18ef4\nDate: 2025-05-28T22:50:11Z\nURL: https://github.com/jax-ml/jax/commit/22b4f263b216a48e7db6c02cc8be8af258c18ef4\nFiles changed: 2\nAdditions: +38, Deletions: -1\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 6ea16ec8e8ba..3c499429a663 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -2243,7 +2243,7 @@ def trace_to_jaxpr_dynamic(\n     try:\n       with core.set_current_trace(trace):\n         ans = fun.call_wrapped(*in_tracers)\n-\n+      _check_returned_jaxtypes(fun.debug_info, ans)\n       out_tracers = map(partial(trace.to_jaxpr_tracer, source_info=source_info), ans)\n       _check_no_returned_refs(fun.debug_info, out_tracers)\n       jaxpr, consts, attrs_tracked = trace.to_jaxpr(out_tracers, fun.debug_info)\n@@ -2255,6 +2255,20 @@ def trace_to_jaxpr_dynamic(\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   return jaxpr, [v.aval for v in jaxpr.outvars], consts, attrs_tracked\n \n+def _check_returned_jaxtypes(dbg, out_tracers):\n+  for i, x in enumerate(out_tracers):\n+    try:\n+      core.typeof(x)\n+    except TypeError:\n+      if (dbg and len(paths := dbg.result_paths()) > i and\n+          (p := paths[i].removeprefix('result'))):\n+        extra = f' at output component {p}'\n+      else:\n+        extra = ''\n+      raise TypeError(\n+      f\"function {dbg.func_src_info} traced for {dbg.traced_for} returned a \"\n+      f\"value of type {type(x)}{extra}, which is not a valid JAX type\") from None\n+\n def _check_no_returned_refs(\n     dbg: core.DebugInfo,\n     out_tracers: Sequence[DynamicJaxprTracer]\ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex f5b74e1e10d6..584eb0eda496 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -5058,6 +5058,29 @@ def test_ensure_compile_time_eval_no_leaks(self):\n     with jax.ensure_compile_time_eval():\n       jnp.linalg.solve(jnp.eye(3), jnp.ones(3))  # doesn't crash\n \n+  def test_returned_non_jaxtype(self):\n+\n+    class TestEnum(enum.Enum):\n+      A = enum.auto()\n+\n+    @jax.tree_util.register_dataclass\n+    @dataclasses.dataclass\n+    class TestClass3:\n+      test_enum_field: TestEnum = dataclasses.field(metadata=dict(static=True))\n+      test_data_field: int\n+\n+    def test_jax_function(test_class: TestClass3) -> TestEnum:\n+      return test_class.test_enum_field\n+\n+    jitted_test_function = jax.jit(test_jax_function)\n+    with self.assertRaisesRegex(TypeError, \"returned a value of type\"):\n+        jitted_test_function(\n+            TestClass3(\n+                test_data_field=1,\n+                test_enum_field=TestEnum.A,\n+            )\n+        )\n+\n \n class RematTest(jtu.JaxTestCase):\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Integrate LLVM at llvm/llvm-project@2b8bff6f66fd\n\nUpdates LLVM usage to match\n[2b8bff6f66fd](https://github.com/llvm/llvm-project/commit/2b8bff6f66fd)\n\nPiperOrigin-RevId: 764439621",
        "output": "```diff\nCommit: 2dc69daec8ed513668e155bc3c9973f2d5d32b05\nDate: 2025-05-28T22:24:52Z\nURL: https://github.com/jax-ml/jax/commit/2dc69daec8ed513668e155bc3c9973f2d5d32b05\nFiles changed: 4\nAdditions: +15, Deletions: -11\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu.td b/jaxlib/mosaic/dialect/tpu/tpu.td\nindex 505478b9ad72..766900cd07e4 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu.td\n+++ b/jaxlib/mosaic/dialect/tpu/tpu.td\n@@ -513,8 +513,8 @@ def TPU_FPToSIOp : TPU_Op<\"fptosi\", [Pure, ElementwiseMappable]> {\n // Internal operation. All arith.sitofp operations that change the bitwidth\n // must be canonicalized to this operation.\n def TPU_SIToFPOp : TPU_Op<\"sitofp\", [Pure, ElementwiseMappable]> {\n-  let arguments = (ins AnyVectorOfAnyRank:$in, TPU_RoundingModeEnum:$rounding_mode);\n-  let results = (outs AnyVectorOfAnyRank:$output);\n+  let arguments = (ins AnyType:$in, TPU_RoundingModeEnum:$rounding_mode);\n+  let results = (outs AnyType:$output);\n   let assemblyFormat = [{ $in attr-dict `:` type($in) `->` type($output) }];\n }\n \ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 6502a9c6682e..53d8712d5274 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -1161,10 +1161,10 @@ LogicalResult tpu_sitofp_rule(RewriteContext &ctx, Operation &op,\n         FAILUREOR_ASSIGN_OR_RETURN(\n             xla::Array<Value> vregs,\n             ext_op_rule_impl(ctx, builder, sitofp_op, layout_in, layout_out));\n-        sitofp_op.replaceAllUsesWith(assemble(builder, sitofp_op.getType(),\n-                                              layout_out, std::move(vregs),\n-                                              ctx.target_shape)\n-                                         .getResult());\n+        sitofp_op.replaceAllUsesWith(\n+            assemble(builder, cast<VectorType>(sitofp_op.getType()), layout_out,\n+                     std::move(vregs), ctx.target_shape)\n+                .getResult());\n         sitofp_op.erase();\n         return success();\n       }\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\nindex 1d8ea1299f04..c963cff0be50 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n@@ -715,10 +715,12 @@ LogicalResult canonicalize_sitofp(const CanonicalizeContext &ctx,\n     }\n   }\n   if (is_vector) {\n-    x = builder.create<arith::SIToFPOp>(\n-        VectorType::get(src_vty.getShape(), builder.getF32Type()), x);\n+    x = builder.create<tpu::SIToFPOp>(\n+        VectorType::get(src_vty.getShape(), builder.getF32Type()), x,\n+        tpu::RoundingMode::kToNearestEven);\n   } else {\n-    x = builder.create<arith::SIToFPOp>(builder.getF32Type(), x);\n+    x = builder.create<tpu::SIToFPOp>(builder.getF32Type(), x,\n+                                      tpu::RoundingMode::kToNearestEven);\n   }\n   if (dst_bitwidth < 32) {\n     x = builder.create<arith::TruncFOp>(op.getType(), x);\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex 976e31cb55f4..14d1fb2104fa 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -155,8 +155,10 @@ class VectorLayoutInferer {\n           return failure();\n         }\n       } else if (auto op = dyn_cast<tpu::SIToFPOp>(any_op);\n-                 op && op.getIn().getType().getElementTypeBitWidth() <\n-                           op.getType().getElementTypeBitWidth()) {\n+                 op &&\n+                 cast<VectorType>(op.getIn().getType())\n+                         .getElementTypeBitWidth() <\n+                     cast<VectorType>(op.getType()).getElementTypeBitWidth()) {\n         if (inferExt(&any_op).failed()) {\n           return failure();\n         }\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Change all us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build container to us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build.\n\nThese containers are the same (same build script), but they are just in a different repositories.\n\nPiperOrigin-RevId: 764435895",
        "output": "```diff\nCommit: c5b908ceca710900e0e099b451b7c138ffcdf0ec\nDate: 2025-05-28T22:14:44Z\nURL: https://github.com/jax-ml/jax/commit/c5b908ceca710900e0e099b451b7c138ffcdf0ec\nFiles changed: 15\nAdditions: +18, Deletions: -18\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex d6173a809500..09c9d173e0d0 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -41,7 +41,7 @@ jobs:\n         # Explicitly set the shell to bash\n         shell: bash\n     runs-on: ${{ inputs.runner }}\n-    container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest') ||\n+    container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest') ||\n                    (contains(inputs.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') }}\n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: ${{ inputs.python }}\ndiff --git a/.github/workflows/bazel_cpu_rbe.yml b/.github/workflows/bazel_cpu_rbe.yml\nindex 2f8eb2c33cee..3eff0932adcb 100644\n--- a/.github/workflows/bazel_cpu_rbe.yml\n+++ b/.github/workflows/bazel_cpu_rbe.yml\n@@ -28,7 +28,7 @@ jobs:\n   run_tests:\n     if: github.event.repository.fork == false\n     runs-on: ${{ matrix.runner }}\n-    container: ${{ (contains(matrix.runner, 'linux-x86') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest') ||\n+    container: ${{ (contains(matrix.runner, 'linux-x86') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest') ||\n                    (contains(matrix.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') }}\n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: ${{ matrix.python }}\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 348d19763989..458589199c53 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -51,7 +51,7 @@ jobs:\n         # Explicitly set the shell to bash\n         shell: bash\n     runs-on: ${{ inputs.runner }}\n-    container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest\"\n+    container: \"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-cuda12.8-cudnn9.8:latest\"\n \n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: ${{ inputs.python }}\ndiff --git a/.github/workflows/bazel_cuda_rbe.yml b/.github/workflows/bazel_cuda_rbe.yml\nindex cd4e9a021cfc..2c57b35587fa 100644\n--- a/.github/workflows/bazel_cuda_rbe.yml\n+++ b/.github/workflows/bazel_cuda_rbe.yml\n@@ -28,7 +28,7 @@ jobs:\n   run_tests:\n     if: github.event.repository.fork == false\n     runs-on: ${{ matrix.runner }}\n-    container: 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest'\n+    container: 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest'\n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: ${{ matrix.python }}\n       JAXCI_ENABLE_X64: ${{ matrix.enable-x_64 }}\ndiff --git a/.github/workflows/bazel_optional_h100_b200.yml b/.github/workflows/bazel_optional_h100_b200.yml\nindex 68fea50857a5..16c7bb95c16b 100644\n--- a/.github/workflows/bazel_optional_h100_b200.yml\n+++ b/.github/workflows/bazel_optional_h100_b200.yml\n@@ -27,7 +27,7 @@ jobs:\n   run_tests:\n     if: ${{ github.event.repository.fork == false && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || contains(github.event.pull_request.labels.*.name, 'CI Optional GPU Presubmit')) }}\n     runs-on: linux-x86-a4-224-b200-1gpu\n-    container: 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest'\n+    container: 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-cuda12.8-cudnn9.8:latest'\n     name: \"Bazel single B200 CUDA tests\"\n # End Presubmit Naming Check github-cuda-presubmits\n     steps:\n@@ -72,7 +72,7 @@ jobs:\n   run_multiaccelerator_tests:\n     if: ${{ github.event.repository.fork == false && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || contains(github.event.pull_request.labels.*.name, 'CI Optional GPU Presubmit')) }}\n     runs-on: linux-x86-a3-8g-h100-8gpu\n-    container: 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest'\n+    container: 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-cuda12.8-cudnn9.8:latest'\n     name: \"Bazel multiple H100 CUDA tests\"\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 90c888471b73..72d554aa5d1b 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -100,7 +100,7 @@ jobs:\n \n     runs-on: ${{ inputs.runner }}\n \n-    container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest') ||\n+    container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest') ||\n                    (contains(inputs.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') ||\n                    (contains(inputs.runner, 'windows-x86') && null) }}\n \ndiff --git a/.github/workflows/cloud-tpu-ci-nightly.yml b/.github/workflows/cloud-tpu-ci-nightly.yml\nindex 3ed560b04a88..1f096ce48e2d 100644\n--- a/.github/workflows/cloud-tpu-ci-nightly.yml\n+++ b/.github/workflows/cloud-tpu-ci-nightly.yml\n@@ -47,7 +47,7 @@ jobs:\n       LIBTPU_OLDEST_VERSION_DATE: 20250228\n       PYTHON: python${{ matrix.python-version }}\n     runs-on: ${{ matrix.tpu.runner }}\n-    container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n+    container: \"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\"\n     timeout-minutes: 180\n     defaults:\n       run:\ndiff --git a/.github/workflows/jax-array-api.yml b/.github/workflows/jax-array-api.yml\nindex 16a72fe34714..41879a6f2e9f 100644\n--- a/.github/workflows/jax-array-api.yml\n+++ b/.github/workflows/jax-array-api.yml\n@@ -15,7 +15,7 @@ permissions: {}\n jobs:\n   build:\n     runs-on: linux-x86-n2-16\n-    container: us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\n+    container: us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\n     strategy:\n       matrix:\n         python-version: [3.11]\ndiff --git a/.github/workflows/numpy_nightly.yml b/.github/workflows/numpy_nightly.yml\nindex d9a858216857..c0036ccf8f7f 100644\n--- a/.github/workflows/numpy_nightly.yml\n+++ b/.github/workflows/numpy_nightly.yml\n@@ -33,7 +33,7 @@ jobs:\n     strategy:\n       matrix:\n             python: [\"3.13\",]\n-    container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n+    container: \"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\"\n     name: \"CI - jaxlib head with NumPy nightly\"\n \n     env:\ndiff --git a/.github/workflows/oldest_supported_numpy.yml b/.github/workflows/oldest_supported_numpy.yml\nindex c7f1f1e38a26..67fc9f10e5ce 100644\n--- a/.github/workflows/oldest_supported_numpy.yml\n+++ b/.github/workflows/oldest_supported_numpy.yml\n@@ -23,7 +23,7 @@ jobs:\n       run:\n         shell: bash\n     runs-on: \"linux-x86-n2-64\"\n-    container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n+    container: \"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\"\n # Begin Presubmit Naming Check - name modification requires internal check to be updated\n     name: \"CI - Oldest Supported NumPy (Python 3.10, x64=0)\"\n # End Presubmit Naming Check github-oldest-supported-numpy-presubmit\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex d08cb520eab4..a92f2d96dc89 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -52,7 +52,7 @@ jobs:\n         # Explicitly set the shell to bash to override Windows's default (cmd)\n         shell: bash\n     runs-on: ${{ inputs.runner }}\n-    container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest') ||\n+    container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest') ||\n                    (contains(inputs.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') ||\n                    (contains(inputs.runner, 'windows-x86') && null) }}\n \ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex d095dbfb9e80..6fa4e14f8b85 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -65,9 +65,9 @@ jobs:\n     # Test the oldest and newest supported CUDA versions.\n     # If testing the CUDA packages from PyPI, then use the ml-build image which does not have any\n     # CUDA pckages installed on the system.\n-    container:  ${{ !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.1') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.1-cudnn9.8:latest') ||\n-                !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.8') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest') ||\n-                inputs.use-nvidia-pip-wheels && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest'}}\n+    container:  ${{ !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.1') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-cuda12.1-cudnn9.8:latest') ||\n+                !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.8') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-cuda12.8-cudnn9.8:latest') ||\n+                inputs.use-nvidia-pip-wheels && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest'}}\n     name: \"${{ (contains(inputs.runner, 'h100') && 'h100') ||\n         (contains(inputs.runner, 'b200') && 'b200') ||\n         (contains(inputs.runner, 'l4') && 'l4') }}, CUDA ${{ inputs.cuda-version }}, py ${{ inputs.python }}, x64=${{ inputs.enable-x64 }}\"\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex ce5dcf0c9fc8..5f56b165c295 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -75,7 +75,7 @@ jobs:\n       run:\n         shell: bash\n     runs-on: ${{ inputs.runner }}\n-    container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n+    container: \"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\"\n     # Begin Presubmit Naming Check - name modification requires internal check to be updated\n     name: \"${{ inputs.tpu-type }}, py ${{ inputs.python }}, libtpu=${{ inputs.libtpu-version-type }}\"\n     # End Presubmit Naming Check github-tpu-presubmits\ndiff --git a/.github/workflows/wheel_tests_nightly_release.yml b/.github/workflows/wheel_tests_nightly_release.yml\nindex c536466c7dcb..6d25ee281c7b 100644\n--- a/.github/workflows/wheel_tests_nightly_release.yml\n+++ b/.github/workflows/wheel_tests_nightly_release.yml\n@@ -154,7 +154,7 @@ jobs:\n         fail-fast: false # don't cancel all jobs on failure\n         matrix:\n           python: [\"3.10\", \"3.13\", \"3.13-nogil\"]\n-    container:  \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n+    container:  \"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\"\n \n     # Verifies that JAX's release wheels can be installed\n     name: \"Verify release wheels install (Python ${{ matrix.python }})\"\ndiff --git a/ci/envs/docker.env b/ci/envs/docker.env\nindex a0f558520d45..5135b61ac45b 100644\n--- a/ci/envs/docker.env\n+++ b/ci/envs/docker.env\n@@ -29,7 +29,7 @@ export JAXCI_DOCKER_ARGS=\"\"\n # Linux x86 image for building JAX artifacts, running Pytests CPU/TPU tests, and\n # Bazel tests\n if [[ $os == \"linux\" ]] && [[ $arch == \"x86_64\" ]]; then\n-  export JAXCI_DOCKER_IMAGE=\"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n+  export JAXCI_DOCKER_IMAGE=\"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\"\n fi\n \n # Linux Aarch64 image for building JAX artifacts, running Pytests CPU tests, and\n\n```",
        "from_id": [
            "quoctruong",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Rework CUDA_ROOT logic a bit\n\nPiperOrigin-RevId: 764419062",
        "output": "```diff\nCommit: 5aa339561871ccf037f1216237cf4e5db937376c\nDate: 2025-05-28T21:32:19Z\nURL: https://github.com/jax-ml/jax/commit/5aa339561871ccf037f1216237cf4e5db937376c\nFiles changed: 1\nAdditions: +18, Deletions: -7\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex bf6a04783be7..524d0ffe23ab 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -45,6 +45,7 @@ limitations under the License.\n #include \"absl/strings/str_replace.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n+// Leave this comment here. Internal Google business.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/Support/CodeGen.h\"\n #include \"llvm/Support/TargetSelect.h\"\n@@ -130,12 +131,17 @@ class TemporaryDirectory {\n   std::string path;\n };\n \n+const char *GetCUDARoot() {\n+  return getenv(\"CUDA_ROOT\");\n+}\n+\n absl::StatusOr<std::string> RunCUDATool(const char* tool,\n                                         const std::vector<const char*>& args,\n                                         bool stderr_to_stdout = true) {\n   CHECK(!args.empty() && args.back() == nullptr);\n-  const char* cuda_path_ptr = getenv(\"CUDA_ROOT\");\n-  if (!cuda_path_ptr) return absl::InternalError(\"Failed to get CUDA_ROOT\");\n+  const char* cuda_path_ptr = GetCUDARoot();\n+  if (!cuda_path_ptr)\n+    return absl::InternalError(\"Failed to get the CUDA toolkit path\");\n   std::string tool_path(cuda_path_ptr);\n   tool_path += \"/bin/\";\n   tool_path += tool;\n@@ -338,6 +344,10 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n     return true;\n   });\n   bool emit_line_info = getenv(\"MOSAIC_GPU_LINE_INFO\") != nullptr;\n+  const char *cuda_root = GetCUDARoot();\n+  if (!cuda_root) {\n+    return mlir::failure();\n+  }\n   return mlir::parsePassPipeline(absl::StrCat(\n       R\"(\n       builtin.module(\n@@ -374,11 +384,12 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n       // TODO(slebedev): Switch to the ensure-debug-info-scope-on-llvm-func\n       // pass in MLIR once Triton upstreams its changes.\n       emit_line_info ? \"enable-line-info,\" : \"\",\n-      R\"(\n-        gpu-module-to-binary{format=)\" +\n-          mlir::gpu::stringifyCompilationTarget(target).str() +\n-          (!nvshmem_path.empty() ? R\"( l=)\" + nvshmem_path : \"\") +\n-          (emit_line_info ? \"  opts=-lineinfo\" : \"\") + R\"(},\n+      \"gpu-module-to-binary{format=\",\n+      mlir::gpu::stringifyCompilationTarget(target).str(),\n+      (!nvshmem_path.empty() ? \" l=\" + nvshmem_path : \"\"),\n+      (emit_line_info ? \"  opts=-lineinfo\" : \"\"),\n+      \" toolkit=\", cuda_root,\n+      R\"(},\n         convert-math-to-llvm{approximate-log1p=true},\n         canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true},\n         cse,\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29074 from MichaelHudgins:actions-fixes\n\nPiperOrigin-RevId: 764408327",
        "output": "```diff\nCommit: ea4049f224cdedf7454660133f619c754c23fa16\nDate: 2025-05-28T21:06:07Z\nURL: https://github.com/jax-ml/jax/commit/ea4049f224cdedf7454660133f619c754c23fa16\nFiles changed: 24\nAdditions: +102, Deletions: -53\ndiff --git a/.github/actionlint.yaml b/.github/actionlint.yaml\nnew file mode 100644\nindex 000000000000..e7ee1a086558\n--- /dev/null\n+++ b/.github/actionlint.yaml\n@@ -0,0 +1,20 @@\n+# Configuration related to self-hosted runner.\n+self-hosted-runner:\n+  labels:\n+    - \"linux-x86-n2-32\" # Linux X86 runner using the 32 vcpu n2-standard-32 machine.\n+    - \"linux-x86-n2-64\" # Linux X86 runner using the 64 vcpu n2-standard-64 machine.\n+    - \"linux-x86-g2-16-l4-1gpu\" # Linux X86 GPU runner using g2-standard-16 machine with 1 NVIDIA L4 GPU attached.\n+    - \"linux-x86-g2-48-l4-4gpu\" # Linux X86 GPU runner using g2-standard-48 machine with 4 NVIDIA L4 GPUs attached.\n+    - \"linux-x86-ct5lp-224-8tpu\" # Linux X86 TPU runner using ct5lp-hightpu-8t machine with 2x4 topology.\n+    - \"linux-arm64-c4a-16\" # Linux ARM64 CPU Runner using the 16 vcpu c4a-standard-16 machine.\n+    - \"linux-arm64-c4a-64\" # Linux ARM64 CPU Runner using the 64 vcpu c4a-standard-64 machine.\n+    - \"windows-x86-n2-16\" # Windows X86 runner using n2-standard-16 machine.\n+    - \"windows-x86-n2-64\" # Windows X86 runner using n2-standard-64 machine.\n+    - \"linux-x86-a4-224-b200-1gpu\" # Linux X86 GPU runner using 1 B200 GPU and 1/8 the resources of a a4-highgpu-8g machine\n+    - \"linux-x86-a3-8g-h100-8gpu\" # Linux X86 GPU runner using a3-highgpu-8g machine with 8 NVIDIA H100 GPUs attached.\n+    - \"linux-x86-ct6e-180-8tpu\" # Linux X86 TPU runner using ct6e-hightpu-8t machine with 2x4 topology.\n+    - \"linux-x86-ct6e-180-4tpu\" # Linux X86 TPU runner using ct6e-hightpu-4t machine with 2x2 topology.\n+    - \"linux-x86-ct4p-240-4tpu\" # Linux X86 TPU runner using ct4p-hightpu-4t machine with 2x2x1 topology.\n+    - \"linux-x86-n2-128\" # Linux X86 runner using the 128 vcpu n2-standard-128 machine.\n+    - \"linux-x86-n2-16\" # Linux X86 runner using the 16 vcpu n2-standard-16 machine.\n+    - \"linux-x86_64-cirrascale-64-8gpu-amd-mi250\" # AMD runner\ndiff --git a/.github/workflows/asan.yaml b/.github/workflows/asan.yaml\nindex ea69d92e552e..533d4381f474 100644\n--- a/.github/workflows/asan.yaml\n+++ b/.github/workflows/asan.yaml\n@@ -13,7 +13,7 @@ on:\n       - main\n     paths:\n       - '**/workflows/asan.yaml'\n-\n+permissions: {}\n jobs:\n   asan:\n     # Don't execute in fork due to runner type\n@@ -41,11 +41,13 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           path: jax\n+          persist-credentials: false\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           repository: python/cpython\n           path: cpython\n           ref: v3.13.0\n+          persist-credentials: false\n       - name: Build CPython with ASAN enabled\n         env:\n           ASAN_OPTIONS: detect_leaks=0\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex 7eb6d2ed27b4..d6173a809500 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -9,9 +9,7 @@\n #    - Executes the `run_bazel_test_cpu_py_import_rbe.sh` script, which performs the following actions:\n #      - Runs the Bazel CPU tests with py_import dependency.\n name: CI - Bazel CPU tests with py_import (RBE)\n-permissions:\n-  contents: read\n-\n+permissions: {}\n on:\n   workflow_call:\n     inputs:\n@@ -54,6 +52,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       # Halt for testing\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\ndiff --git a/.github/workflows/bazel_cpu_rbe.yml b/.github/workflows/bazel_cpu_rbe.yml\nindex a8b40c260260..2f8eb2c33cee 100644\n--- a/.github/workflows/bazel_cpu_rbe.yml\n+++ b/.github/workflows/bazel_cpu_rbe.yml\n@@ -18,7 +18,7 @@ on:\n     branches:\n       - main\n       - 'release/**'\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n@@ -53,6 +53,8 @@ jobs:\n # End Presubmit Naming Check github-cpu-presubmits\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 5a3ceaa8a4e8..348d19763989 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -44,7 +44,6 @@ on:\n         type: string\n         required: false\n         default: 'no'\n-\n jobs:\n   run-tests:\n     defaults:\n@@ -67,6 +66,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/bazel_cuda_rbe.yml b/.github/workflows/bazel_cuda_rbe.yml\nindex 83f651c0ef95..cd4e9a021cfc 100644\n--- a/.github/workflows/bazel_cuda_rbe.yml\n+++ b/.github/workflows/bazel_cuda_rbe.yml\n@@ -23,7 +23,7 @@ concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n   cancel-in-progress: ${{ !contains(github.ref, 'release/') && github.ref != 'main' }}\n-\n+permissions: {}\n jobs:\n   run_tests:\n     if: github.event.repository.fork == false\n@@ -49,6 +49,8 @@ jobs:\n # End Presubmit Naming Check github-cuda-presubmits\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\ndiff --git a/.github/workflows/bazel_optional_h100_b200.yml b/.github/workflows/bazel_optional_h100_b200.yml\nindex ec907280938e..68fea50857a5 100644\n--- a/.github/workflows/bazel_optional_h100_b200.yml\n+++ b/.github/workflows/bazel_optional_h100_b200.yml\n@@ -32,6 +32,8 @@ jobs:\n # End Presubmit Naming Check github-cuda-presubmits\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n@@ -74,6 +76,8 @@ jobs:\n     name: \"Bazel multiple H100 CUDA tests\"\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 7bca28c3190d..90c888471b73 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -90,10 +90,7 @@ on:\n       gcs_upload_uri:\n         description: \"GCS location prefix to where the artifacts were uploaded\"\n         value: ${{ jobs.build-artifacts.outputs.gcs_upload_uri }}\n-\n-permissions:\n-  contents: read\n-\n+permissions: {}\n jobs:\n   build-artifacts:\n     defaults:\n@@ -122,6 +119,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Enable RBE if building on Linux x86 or Windows x86\n         if: contains(inputs.runner, 'linux-x86') || contains(inputs.runner, 'windows-x86')\n         run: echo \"JAXCI_BUILD_ARTIFACT_WITH_RBE=1\" >> $GITHUB_ENV\ndiff --git a/.github/workflows/ci-build.yaml b/.github/workflows/ci-build.yaml\nindex 0769c698d5fe..ada470526ef8 100644\n--- a/.github/workflows/ci-build.yaml\n+++ b/.github/workflows/ci-build.yaml\n@@ -16,10 +16,7 @@ on:\n     branches:\n       - main\n \n-permissions:\n-  contents: read  # to fetch code\n-  actions: write  # to cancel previous workflows\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n@@ -30,6 +27,8 @@ jobs:\n     timeout-minutes: 5\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set up Python 3.11\n         uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n         with:\n@@ -65,6 +64,8 @@ jobs:\n             num_generated_cases: 1\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Image Setup\n       run: |\n         apt update\n@@ -106,6 +107,8 @@ jobs:\n         python-version: ['3.10']\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Set up Python ${{ matrix.python-version }}\n       uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n       with:\n@@ -136,6 +139,8 @@ jobs:\n         python-version: ['3.10']\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Image Setup\n       run: |\n         apt update\n@@ -166,6 +171,8 @@ jobs:\n             num_generated_cases: 10\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Set up Python ${{ matrix.python-version }}\n       uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n       with:\n@@ -198,6 +205,8 @@ jobs:\n     timeout-minutes: 30\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Set up Python\n       uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n       with:\ndiff --git a/.github/workflows/cloud-tpu-ci-nightly.yml b/.github/workflows/cloud-tpu-ci-nightly.yml\nindex c7394a498dd6..3ed560b04a88 100644\n--- a/.github/workflows/cloud-tpu-ci-nightly.yml\n+++ b/.github/workflows/cloud-tpu-ci-nightly.yml\n@@ -57,6 +57,8 @@ jobs:\n       # mandates using a specific commit for non-Google actions. We use\n       # https://github.com/sethvargo/ratchet to pin specific versions.\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       # Checkout XLA at head, if we're building jaxlib at head.\n       - name: Checkout XLA at head\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n@@ -64,6 +66,7 @@ jobs:\n         with:\n           repository: openxla/xla\n           path: xla\n+          persist-credentials: false\n       # We need to mark the GitHub workspace as safe as otherwise git commands will fail.\n       - name: Mark GitHub workspace as safe\n         run: |\ndiff --git a/.github/workflows/cloud-tpu-ci-presubmit.yml b/.github/workflows/cloud-tpu-ci-presubmit.yml\nindex 090259c0f849..fe1f2820b338 100644\n--- a/.github/workflows/cloud-tpu-ci-presubmit.yml\n+++ b/.github/workflows/cloud-tpu-ci-presubmit.yml\n@@ -25,9 +25,7 @@ on:\n \n # This should also be set to read-only in the project settings, but it's nice to\n # document and enforce the permissions here.\n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\ndiff --git a/.github/workflows/jax-array-api.yml b/.github/workflows/jax-array-api.yml\nindex 6419cb730b71..16a72fe34714 100644\n--- a/.github/workflows/jax-array-api.yml\n+++ b/.github/workflows/jax-array-api.yml\n@@ -11,22 +11,21 @@ on:\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n+permissions: {}\n jobs:\n   build:\n-\n     runs-on: linux-x86-n2-16\n     container: us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\n     strategy:\n       matrix:\n         python-version: [3.11]\n-\n     env:\n       PYTHON: \"python${{ matrix.python-version }}\"\n-\n     steps:\n     - name: Checkout jax\n       uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Checkout array-api-tests\n       uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       with:\n@@ -35,6 +34,7 @@ jobs:\n         ref: 'c847143beb8d769bde5dbcc063fe19ed7acc2f9b'  # Latest commit as of 2025-05-12\n         submodules: 'true'\n         path: 'array-api-tests'\n+        persist-credentials: false\n     - name: Install dependencies\n       run: |\n         $PYTHON -m uv pip install --system .[ci] pytest-xdist -r array-api-tests/requirements.txt\ndiff --git a/.github/workflows/k8s.yaml b/.github/workflows/k8s.yaml\nindex 5756b1afbbd2..81552f9bb43b 100644\n--- a/.github/workflows/k8s.yaml\n+++ b/.github/workflows/k8s.yaml\n@@ -1,5 +1,4 @@\n name: Multi-process run using K8s\n-\n on:\n   push:\n     branches:\n@@ -16,19 +15,14 @@ on:\n       - 'jax/_src/distributed.py'\n       - 'jax/_src/clusters/**'\n \n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n defaults:\n   run:\n     shell: bash -ex -o pipefail {0}\n-\n jobs:\n-\n   distributed-initialize:\n     runs-on: ubuntu-22.04\n     strategy:\n@@ -40,6 +34,7 @@ jobs:\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # ratchet:actions/checkout@v4\n         with:\n           path: jax\n+          persist-credentials: false\n           \n       - name: Start Minikube cluster\n         uses: medyagh/setup-minikube@cea33675329b799adccc9526aa5daccc26cd5052 # ratchet:medyagh/setup-minikube@v0.0.19\n@@ -105,7 +100,7 @@ jobs:\n           done\n \n       - name: Examine individual pod outputs\n-        if: \"!cancelled()\"\n+        if: ${{ !cancelled() }}\n         run: |\n           set +x\n           kubectl get pods --no-headers | awk '{print $1}' | while read -s pod; do\ndiff --git a/.github/workflows/metal_plugin_ci.yml b/.github/workflows/metal_plugin_ci.yml\nindex 2135e473d6be..c76153d48f10 100644\n--- a/.github/workflows/metal_plugin_ci.yml\n+++ b/.github/workflows/metal_plugin_ci.yml\n@@ -14,7 +14,7 @@ on:\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n+permissions: {}\n jobs:\n   jax-metal-plugin-test:\n \n@@ -30,6 +30,7 @@ jobs:\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n         with:\n           path: jax\n+          persist-credentials: false\n       - name: Setup build and test enviroment\n         run: |\n           rm -rf ${GITHUB_WORKSPACE}/jax-metal-venv\ndiff --git a/.github/workflows/numpy_nightly.yml b/.github/workflows/numpy_nightly.yml\nindex 17357e9f1dd8..d9a858216857 100644\n--- a/.github/workflows/numpy_nightly.yml\n+++ b/.github/workflows/numpy_nightly.yml\n@@ -18,9 +18,7 @@ on:\n   schedule:\n     - cron: \"0 */3 * * *\" # Run once every 3 hours\n \n-permissions:\n-      contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n@@ -46,12 +44,15 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Checkout ml_dtypes\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683\n         with:\n           repository: jax-ml/ml_dtypes\n           ref: main\n           path: ml_dtypes\n+          persist-credentials: false\n       # Halt for testing\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\ndiff --git a/.github/workflows/oldest_supported_numpy.yml b/.github/workflows/oldest_supported_numpy.yml\nindex a63cb0b1c614..c7f1f1e38a26 100644\n--- a/.github/workflows/oldest_supported_numpy.yml\n+++ b/.github/workflows/oldest_supported_numpy.yml\n@@ -10,12 +10,7 @@ on:\n   push:\n     branches:\n       - main\n-\n-# This should also be set to read-only in the project settings, but it's nice to\n-# document and enforce the permissions here.\n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n@@ -23,7 +18,7 @@ concurrency:\n \n jobs:\n   test-oldest-supported-numpy:\n-    if: \"github.event.repository.fork == false && !startsWith(github.head_ref, 'release/')\"\n+    if: ${{ github.event.repository.fork == false && !startsWith(github.head_ref, 'release/') }}\n     defaults:\n       run:\n         shell: bash\n@@ -40,6 +35,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Install Python dependencies\n         run: |\n           $JAXCI_PYTHON -m uv pip install -r build/test-requirements.txt\n@@ -52,8 +49,6 @@ jobs:\n       # Halt for testing\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n-        with:\n-          halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Pytest CPU tests\n         timeout-minutes: 30\n         run: ./ci/run_pytest_cpu.sh\n\\ No newline at end of file\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex 263bfd7ec9a9..d08cb520eab4 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -67,6 +67,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 5f8888526aad..d095dbfb9e80 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -79,6 +79,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 8c2457208e12..ce5dcf0c9fc8 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -11,7 +11,6 @@\n #      - Installs the downloaded jaxlib wheel.\n #      - Runs the TPU tests with Pytest.\n name: CI - Pytest TPU\n-\n on:\n   workflow_call:\n     inputs:\n@@ -90,6 +89,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/release-notification.yml b/.github/workflows/release-notification.yml\nindex a4a342ef6de7..6d68bf922655 100644\n--- a/.github/workflows/release-notification.yml\n+++ b/.github/workflows/release-notification.yml\n@@ -2,14 +2,21 @@ name: Google Chat Release Notification\n on:\n   release:\n     types: [published]\n+permissions: {}\n jobs:\n   build:\n+    env:\n+      WEBHOOK_URL: ${{ secrets.RELEASES_WEBHOOK }}\n+      RELEASE_NAME: ${{github.event.release.name}}\n+      PUBLISHED_AT: ${{github.event.release.published_at}}\n+      AUTHOR_LOGIN: ${{github.event.release.author.login}}\n+      RELEASE_URL: ${{github.event.release.url}}\n     runs-on: ubuntu-latest\n     steps:\n       - name: Google Chat Notification\n         run: |\n-          curl --location --request POST '${{ secrets.RELEASES_WEBHOOK }}' \\\n+          curl --location --request POST '${WEBHOOK_URL}' \\\n           --header 'Content-Type: application/json' \\\n           --data-raw '{\n-              \"text\": \"Release ${{github.event.release.name}} at ${{github.event.release.published_at}} by ${{github.event.release.author.login}}. <${{github.event.release.url}}|[github]>\"\n+              \"text\": \"Release $RELEASE_NAME at $PUBLISHED_AT by $AUTHOR_LOGIN. <$RELEASE_URL|[github]>\"\n           }'\ndiff --git a/.github/workflows/rocm-ci.yml b/.github/workflows/rocm-ci.yml\nindex 0ce20726ce63..4bfb8cb50a5e 100644\n--- a/.github/workflows/rocm-ci.yml\n+++ b/.github/workflows/rocm-ci.yml\n@@ -6,9 +6,7 @@ on:\n     branches:\n       - main\n \n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n \n@@ -36,6 +34,7 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n         with:\n           path: ${{ env.WORKSPACE_DIR }}\n+          persist-credentials: false\n       - name: Build JAX\n         run: |\n           pushd $WORKSPACE_DIR\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex ce4130c31a30..67ff8dd93e3d 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -3,7 +3,6 @@ name: CI - Free-threading and Thread Sanitizer (nightly)\n concurrency:\n   group: ${{ github.workflow }}-${{ github.ref }}\n   cancel-in-progress: true\n-\n on:\n   schedule:\n     - cron: \"0 5 * * *\" # Daily at 05:00 UTC == 00:00 EST == 21:00 PST\n@@ -14,7 +13,7 @@ on:\n     paths:\n       - '**/workflows/tsan.yaml'\n       - '**/workflows/tsan-suppressions*.txt'\n-\n+permissions: {}\n jobs:\n   tsan:\n     runs-on: linux-x86-n2-64\n@@ -50,17 +49,20 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           path: jax\n+          persist-credentials: false\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           repository: numpy/numpy\n           path: numpy\n           submodules: true\n+          persist-credentials: false\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         if: ${{ matrix.python-version == '3.14' }}\n         with:\n           repository: scipy/scipy\n           path: scipy\n           submodules: true\n+          persist-credentials: false\n \n       - name: Get year & week number\n         id: get-date\n@@ -81,6 +83,8 @@ jobs:\n           repository: python/cpython\n           path: cpython\n           ref: ${{ matrix.github_branch }}\n+          persist-credentials: false\n+\n \n       - name: Build TSAN CPython ${{ matrix.python-version }}\n         if: steps.cache-cpython-tsan-restore.outputs.cache-hit != 'true'\ndiff --git a/.github/workflows/upstream-nightly.yml b/.github/workflows/upstream-nightly.yml\nindex 349ddf0d96a3..23b8ac32d844 100644\n--- a/.github/workflows/upstream-nightly.yml\n+++ b/.github/workflows/upstream-nightly.yml\n@@ -19,7 +19,7 @@ on:\n       - main\n     paths:\n       - '**workflows/upstream-nightly.yml'\n-\n+permissions: {}\n jobs:\n   upstream-dev:\n     runs-on: linux-x86-n2-64\n@@ -33,6 +33,8 @@ jobs:\n         python-version: [\"3.13\"]\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set up Python ${{ matrix.python-version }}\n         uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n         with:\ndiff --git a/.github/workflows/wheel_tests_nightly_release.yml b/.github/workflows/wheel_tests_nightly_release.yml\nindex 7bad41647e6b..c536466c7dcb 100644\n--- a/.github/workflows/wheel_tests_nightly_release.yml\n+++ b/.github/workflows/wheel_tests_nightly_release.yml\n@@ -33,7 +33,7 @@ on:\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n+permissions: {}\n jobs:\n   run-pytest-cpu:\n     uses: ./.github/workflows/pytest_cpu.yml\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update actions to adhere to best practices",
        "output": "```diff\nCommit: 36eeceb5ef4263559ede1bac18b63306c2e2ddd6\nDate: 2025-05-28T20:50:03Z\nURL: https://github.com/jax-ml/jax/commit/36eeceb5ef4263559ede1bac18b63306c2e2ddd6\nFiles changed: 24\nAdditions: +102, Deletions: -53\ndiff --git a/.github/actionlint.yaml b/.github/actionlint.yaml\nnew file mode 100644\nindex 000000000000..e7ee1a086558\n--- /dev/null\n+++ b/.github/actionlint.yaml\n@@ -0,0 +1,20 @@\n+# Configuration related to self-hosted runner.\n+self-hosted-runner:\n+  labels:\n+    - \"linux-x86-n2-32\" # Linux X86 runner using the 32 vcpu n2-standard-32 machine.\n+    - \"linux-x86-n2-64\" # Linux X86 runner using the 64 vcpu n2-standard-64 machine.\n+    - \"linux-x86-g2-16-l4-1gpu\" # Linux X86 GPU runner using g2-standard-16 machine with 1 NVIDIA L4 GPU attached.\n+    - \"linux-x86-g2-48-l4-4gpu\" # Linux X86 GPU runner using g2-standard-48 machine with 4 NVIDIA L4 GPUs attached.\n+    - \"linux-x86-ct5lp-224-8tpu\" # Linux X86 TPU runner using ct5lp-hightpu-8t machine with 2x4 topology.\n+    - \"linux-arm64-c4a-16\" # Linux ARM64 CPU Runner using the 16 vcpu c4a-standard-16 machine.\n+    - \"linux-arm64-c4a-64\" # Linux ARM64 CPU Runner using the 64 vcpu c4a-standard-64 machine.\n+    - \"windows-x86-n2-16\" # Windows X86 runner using n2-standard-16 machine.\n+    - \"windows-x86-n2-64\" # Windows X86 runner using n2-standard-64 machine.\n+    - \"linux-x86-a4-224-b200-1gpu\" # Linux X86 GPU runner using 1 B200 GPU and 1/8 the resources of a a4-highgpu-8g machine\n+    - \"linux-x86-a3-8g-h100-8gpu\" # Linux X86 GPU runner using a3-highgpu-8g machine with 8 NVIDIA H100 GPUs attached.\n+    - \"linux-x86-ct6e-180-8tpu\" # Linux X86 TPU runner using ct6e-hightpu-8t machine with 2x4 topology.\n+    - \"linux-x86-ct6e-180-4tpu\" # Linux X86 TPU runner using ct6e-hightpu-4t machine with 2x2 topology.\n+    - \"linux-x86-ct4p-240-4tpu\" # Linux X86 TPU runner using ct4p-hightpu-4t machine with 2x2x1 topology.\n+    - \"linux-x86-n2-128\" # Linux X86 runner using the 128 vcpu n2-standard-128 machine.\n+    - \"linux-x86-n2-16\" # Linux X86 runner using the 16 vcpu n2-standard-16 machine.\n+    - \"linux-x86_64-cirrascale-64-8gpu-amd-mi250\" # AMD runner\ndiff --git a/.github/workflows/asan.yaml b/.github/workflows/asan.yaml\nindex ea69d92e552e..533d4381f474 100644\n--- a/.github/workflows/asan.yaml\n+++ b/.github/workflows/asan.yaml\n@@ -13,7 +13,7 @@ on:\n       - main\n     paths:\n       - '**/workflows/asan.yaml'\n-\n+permissions: {}\n jobs:\n   asan:\n     # Don't execute in fork due to runner type\n@@ -41,11 +41,13 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           path: jax\n+          persist-credentials: false\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           repository: python/cpython\n           path: cpython\n           ref: v3.13.0\n+          persist-credentials: false\n       - name: Build CPython with ASAN enabled\n         env:\n           ASAN_OPTIONS: detect_leaks=0\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex 7eb6d2ed27b4..d6173a809500 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -9,9 +9,7 @@\n #    - Executes the `run_bazel_test_cpu_py_import_rbe.sh` script, which performs the following actions:\n #      - Runs the Bazel CPU tests with py_import dependency.\n name: CI - Bazel CPU tests with py_import (RBE)\n-permissions:\n-  contents: read\n-\n+permissions: {}\n on:\n   workflow_call:\n     inputs:\n@@ -54,6 +52,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       # Halt for testing\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\ndiff --git a/.github/workflows/bazel_cpu_rbe.yml b/.github/workflows/bazel_cpu_rbe.yml\nindex a8b40c260260..2f8eb2c33cee 100644\n--- a/.github/workflows/bazel_cpu_rbe.yml\n+++ b/.github/workflows/bazel_cpu_rbe.yml\n@@ -18,7 +18,7 @@ on:\n     branches:\n       - main\n       - 'release/**'\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n@@ -53,6 +53,8 @@ jobs:\n # End Presubmit Naming Check github-cpu-presubmits\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 5a3ceaa8a4e8..348d19763989 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -44,7 +44,6 @@ on:\n         type: string\n         required: false\n         default: 'no'\n-\n jobs:\n   run-tests:\n     defaults:\n@@ -67,6 +66,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/bazel_cuda_rbe.yml b/.github/workflows/bazel_cuda_rbe.yml\nindex 83f651c0ef95..cd4e9a021cfc 100644\n--- a/.github/workflows/bazel_cuda_rbe.yml\n+++ b/.github/workflows/bazel_cuda_rbe.yml\n@@ -23,7 +23,7 @@ concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n   cancel-in-progress: ${{ !contains(github.ref, 'release/') && github.ref != 'main' }}\n-\n+permissions: {}\n jobs:\n   run_tests:\n     if: github.event.repository.fork == false\n@@ -49,6 +49,8 @@ jobs:\n # End Presubmit Naming Check github-cuda-presubmits\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\ndiff --git a/.github/workflows/bazel_optional_h100_b200.yml b/.github/workflows/bazel_optional_h100_b200.yml\nindex ec907280938e..68fea50857a5 100644\n--- a/.github/workflows/bazel_optional_h100_b200.yml\n+++ b/.github/workflows/bazel_optional_h100_b200.yml\n@@ -32,6 +32,8 @@ jobs:\n # End Presubmit Naming Check github-cuda-presubmits\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n@@ -74,6 +76,8 @@ jobs:\n     name: \"Bazel multiple H100 CUDA tests\"\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 7bca28c3190d..90c888471b73 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -90,10 +90,7 @@ on:\n       gcs_upload_uri:\n         description: \"GCS location prefix to where the artifacts were uploaded\"\n         value: ${{ jobs.build-artifacts.outputs.gcs_upload_uri }}\n-\n-permissions:\n-  contents: read\n-\n+permissions: {}\n jobs:\n   build-artifacts:\n     defaults:\n@@ -122,6 +119,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Enable RBE if building on Linux x86 or Windows x86\n         if: contains(inputs.runner, 'linux-x86') || contains(inputs.runner, 'windows-x86')\n         run: echo \"JAXCI_BUILD_ARTIFACT_WITH_RBE=1\" >> $GITHUB_ENV\ndiff --git a/.github/workflows/ci-build.yaml b/.github/workflows/ci-build.yaml\nindex 0769c698d5fe..ada470526ef8 100644\n--- a/.github/workflows/ci-build.yaml\n+++ b/.github/workflows/ci-build.yaml\n@@ -16,10 +16,7 @@ on:\n     branches:\n       - main\n \n-permissions:\n-  contents: read  # to fetch code\n-  actions: write  # to cancel previous workflows\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n@@ -30,6 +27,8 @@ jobs:\n     timeout-minutes: 5\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set up Python 3.11\n         uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n         with:\n@@ -65,6 +64,8 @@ jobs:\n             num_generated_cases: 1\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Image Setup\n       run: |\n         apt update\n@@ -106,6 +107,8 @@ jobs:\n         python-version: ['3.10']\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Set up Python ${{ matrix.python-version }}\n       uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n       with:\n@@ -136,6 +139,8 @@ jobs:\n         python-version: ['3.10']\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Image Setup\n       run: |\n         apt update\n@@ -166,6 +171,8 @@ jobs:\n             num_generated_cases: 10\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Set up Python ${{ matrix.python-version }}\n       uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n       with:\n@@ -198,6 +205,8 @@ jobs:\n     timeout-minutes: 30\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Set up Python\n       uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n       with:\ndiff --git a/.github/workflows/cloud-tpu-ci-nightly.yml b/.github/workflows/cloud-tpu-ci-nightly.yml\nindex c7394a498dd6..3ed560b04a88 100644\n--- a/.github/workflows/cloud-tpu-ci-nightly.yml\n+++ b/.github/workflows/cloud-tpu-ci-nightly.yml\n@@ -57,6 +57,8 @@ jobs:\n       # mandates using a specific commit for non-Google actions. We use\n       # https://github.com/sethvargo/ratchet to pin specific versions.\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       # Checkout XLA at head, if we're building jaxlib at head.\n       - name: Checkout XLA at head\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n@@ -64,6 +66,7 @@ jobs:\n         with:\n           repository: openxla/xla\n           path: xla\n+          persist-credentials: false\n       # We need to mark the GitHub workspace as safe as otherwise git commands will fail.\n       - name: Mark GitHub workspace as safe\n         run: |\ndiff --git a/.github/workflows/cloud-tpu-ci-presubmit.yml b/.github/workflows/cloud-tpu-ci-presubmit.yml\nindex 090259c0f849..fe1f2820b338 100644\n--- a/.github/workflows/cloud-tpu-ci-presubmit.yml\n+++ b/.github/workflows/cloud-tpu-ci-presubmit.yml\n@@ -25,9 +25,7 @@ on:\n \n # This should also be set to read-only in the project settings, but it's nice to\n # document and enforce the permissions here.\n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\ndiff --git a/.github/workflows/jax-array-api.yml b/.github/workflows/jax-array-api.yml\nindex 6419cb730b71..16a72fe34714 100644\n--- a/.github/workflows/jax-array-api.yml\n+++ b/.github/workflows/jax-array-api.yml\n@@ -11,22 +11,21 @@ on:\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n+permissions: {}\n jobs:\n   build:\n-\n     runs-on: linux-x86-n2-16\n     container: us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\n     strategy:\n       matrix:\n         python-version: [3.11]\n-\n     env:\n       PYTHON: \"python${{ matrix.python-version }}\"\n-\n     steps:\n     - name: Checkout jax\n       uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Checkout array-api-tests\n       uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       with:\n@@ -35,6 +34,7 @@ jobs:\n         ref: 'c847143beb8d769bde5dbcc063fe19ed7acc2f9b'  # Latest commit as of 2025-05-12\n         submodules: 'true'\n         path: 'array-api-tests'\n+        persist-credentials: false\n     - name: Install dependencies\n       run: |\n         $PYTHON -m uv pip install --system .[ci] pytest-xdist -r array-api-tests/requirements.txt\ndiff --git a/.github/workflows/k8s.yaml b/.github/workflows/k8s.yaml\nindex 5756b1afbbd2..81552f9bb43b 100644\n--- a/.github/workflows/k8s.yaml\n+++ b/.github/workflows/k8s.yaml\n@@ -1,5 +1,4 @@\n name: Multi-process run using K8s\n-\n on:\n   push:\n     branches:\n@@ -16,19 +15,14 @@ on:\n       - 'jax/_src/distributed.py'\n       - 'jax/_src/clusters/**'\n \n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n defaults:\n   run:\n     shell: bash -ex -o pipefail {0}\n-\n jobs:\n-\n   distributed-initialize:\n     runs-on: ubuntu-22.04\n     strategy:\n@@ -40,6 +34,7 @@ jobs:\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # ratchet:actions/checkout@v4\n         with:\n           path: jax\n+          persist-credentials: false\n           \n       - name: Start Minikube cluster\n         uses: medyagh/setup-minikube@cea33675329b799adccc9526aa5daccc26cd5052 # ratchet:medyagh/setup-minikube@v0.0.19\n@@ -105,7 +100,7 @@ jobs:\n           done\n \n       - name: Examine individual pod outputs\n-        if: \"!cancelled()\"\n+        if: ${{ !cancelled() }}\n         run: |\n           set +x\n           kubectl get pods --no-headers | awk '{print $1}' | while read -s pod; do\ndiff --git a/.github/workflows/metal_plugin_ci.yml b/.github/workflows/metal_plugin_ci.yml\nindex 2135e473d6be..c76153d48f10 100644\n--- a/.github/workflows/metal_plugin_ci.yml\n+++ b/.github/workflows/metal_plugin_ci.yml\n@@ -14,7 +14,7 @@ on:\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n+permissions: {}\n jobs:\n   jax-metal-plugin-test:\n \n@@ -30,6 +30,7 @@ jobs:\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n         with:\n           path: jax\n+          persist-credentials: false\n       - name: Setup build and test enviroment\n         run: |\n           rm -rf ${GITHUB_WORKSPACE}/jax-metal-venv\ndiff --git a/.github/workflows/numpy_nightly.yml b/.github/workflows/numpy_nightly.yml\nindex 17357e9f1dd8..d9a858216857 100644\n--- a/.github/workflows/numpy_nightly.yml\n+++ b/.github/workflows/numpy_nightly.yml\n@@ -18,9 +18,7 @@ on:\n   schedule:\n     - cron: \"0 */3 * * *\" # Run once every 3 hours\n \n-permissions:\n-      contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n@@ -46,12 +44,15 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Checkout ml_dtypes\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683\n         with:\n           repository: jax-ml/ml_dtypes\n           ref: main\n           path: ml_dtypes\n+          persist-credentials: false\n       # Halt for testing\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\ndiff --git a/.github/workflows/oldest_supported_numpy.yml b/.github/workflows/oldest_supported_numpy.yml\nindex a63cb0b1c614..c7f1f1e38a26 100644\n--- a/.github/workflows/oldest_supported_numpy.yml\n+++ b/.github/workflows/oldest_supported_numpy.yml\n@@ -10,12 +10,7 @@ on:\n   push:\n     branches:\n       - main\n-\n-# This should also be set to read-only in the project settings, but it's nice to\n-# document and enforce the permissions here.\n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n@@ -23,7 +18,7 @@ concurrency:\n \n jobs:\n   test-oldest-supported-numpy:\n-    if: \"github.event.repository.fork == false && !startsWith(github.head_ref, 'release/')\"\n+    if: ${{ github.event.repository.fork == false && !startsWith(github.head_ref, 'release/') }}\n     defaults:\n       run:\n         shell: bash\n@@ -40,6 +35,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Install Python dependencies\n         run: |\n           $JAXCI_PYTHON -m uv pip install -r build/test-requirements.txt\n@@ -52,8 +49,6 @@ jobs:\n       # Halt for testing\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n-        with:\n-          halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Pytest CPU tests\n         timeout-minutes: 30\n         run: ./ci/run_pytest_cpu.sh\n\\ No newline at end of file\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex 263bfd7ec9a9..d08cb520eab4 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -67,6 +67,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 5f8888526aad..d095dbfb9e80 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -79,6 +79,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 8c2457208e12..ce5dcf0c9fc8 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -11,7 +11,6 @@\n #      - Installs the downloaded jaxlib wheel.\n #      - Runs the TPU tests with Pytest.\n name: CI - Pytest TPU\n-\n on:\n   workflow_call:\n     inputs:\n@@ -90,6 +89,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/release-notification.yml b/.github/workflows/release-notification.yml\nindex a4a342ef6de7..6d68bf922655 100644\n--- a/.github/workflows/release-notification.yml\n+++ b/.github/workflows/release-notification.yml\n@@ -2,14 +2,21 @@ name: Google Chat Release Notification\n on:\n   release:\n     types: [published]\n+permissions: {}\n jobs:\n   build:\n+    env:\n+      WEBHOOK_URL: ${{ secrets.RELEASES_WEBHOOK }}\n+      RELEASE_NAME: ${{github.event.release.name}}\n+      PUBLISHED_AT: ${{github.event.release.published_at}}\n+      AUTHOR_LOGIN: ${{github.event.release.author.login}}\n+      RELEASE_URL: ${{github.event.release.url}}\n     runs-on: ubuntu-latest\n     steps:\n       - name: Google Chat Notification\n         run: |\n-          curl --location --request POST '${{ secrets.RELEASES_WEBHOOK }}' \\\n+          curl --location --request POST '${WEBHOOK_URL}' \\\n           --header 'Content-Type: application/json' \\\n           --data-raw '{\n-              \"text\": \"Release ${{github.event.release.name}} at ${{github.event.release.published_at}} by ${{github.event.release.author.login}}. <${{github.event.release.url}}|[github]>\"\n+              \"text\": \"Release $RELEASE_NAME at $PUBLISHED_AT by $AUTHOR_LOGIN. <$RELEASE_URL|[github]>\"\n           }'\ndiff --git a/.github/workflows/rocm-ci.yml b/.github/workflows/rocm-ci.yml\nindex 0ce20726ce63..4bfb8cb50a5e 100644\n--- a/.github/workflows/rocm-ci.yml\n+++ b/.github/workflows/rocm-ci.yml\n@@ -6,9 +6,7 @@ on:\n     branches:\n       - main\n \n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n \n@@ -36,6 +34,7 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n         with:\n           path: ${{ env.WORKSPACE_DIR }}\n+          persist-credentials: false\n       - name: Build JAX\n         run: |\n           pushd $WORKSPACE_DIR\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex ce4130c31a30..67ff8dd93e3d 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -3,7 +3,6 @@ name: CI - Free-threading and Thread Sanitizer (nightly)\n concurrency:\n   group: ${{ github.workflow }}-${{ github.ref }}\n   cancel-in-progress: true\n-\n on:\n   schedule:\n     - cron: \"0 5 * * *\" # Daily at 05:00 UTC == 00:00 EST == 21:00 PST\n@@ -14,7 +13,7 @@ on:\n     paths:\n       - '**/workflows/tsan.yaml'\n       - '**/workflows/tsan-suppressions*.txt'\n-\n+permissions: {}\n jobs:\n   tsan:\n     runs-on: linux-x86-n2-64\n@@ -50,17 +49,20 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           path: jax\n+          persist-credentials: false\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           repository: numpy/numpy\n           path: numpy\n           submodules: true\n+          persist-credentials: false\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         if: ${{ matrix.python-version == '3.14' }}\n         with:\n           repository: scipy/scipy\n           path: scipy\n           submodules: true\n+          persist-credentials: false\n \n       - name: Get year & week number\n         id: get-date\n@@ -81,6 +83,8 @@ jobs:\n           repository: python/cpython\n           path: cpython\n           ref: ${{ matrix.github_branch }}\n+          persist-credentials: false\n+\n \n       - name: Build TSAN CPython ${{ matrix.python-version }}\n         if: steps.cache-cpython-tsan-restore.outputs.cache-hit != 'true'\ndiff --git a/.github/workflows/upstream-nightly.yml b/.github/workflows/upstream-nightly.yml\nindex 349ddf0d96a3..23b8ac32d844 100644\n--- a/.github/workflows/upstream-nightly.yml\n+++ b/.github/workflows/upstream-nightly.yml\n@@ -19,7 +19,7 @@ on:\n       - main\n     paths:\n       - '**workflows/upstream-nightly.yml'\n-\n+permissions: {}\n jobs:\n   upstream-dev:\n     runs-on: linux-x86-n2-64\n@@ -33,6 +33,8 @@ jobs:\n         python-version: [\"3.13\"]\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set up Python ${{ matrix.python-version }}\n         uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n         with:\ndiff --git a/.github/workflows/wheel_tests_nightly_release.yml b/.github/workflows/wheel_tests_nightly_release.yml\nindex 7bad41647e6b..c536466c7dcb 100644\n--- a/.github/workflows/wheel_tests_nightly_release.yml\n+++ b/.github/workflows/wheel_tests_nightly_release.yml\n@@ -33,7 +33,7 @@ on:\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n+permissions: {}\n jobs:\n   run-pytest-cpu:\n     uses: ./.github/workflows/pytest_cpu.yml\n\n```",
        "from_id": [
            "MichaelHudgins"
        ]
    },
    {
        "text_input": "Make CI job names to be shorter\n\nThis strips away the redundant terms in job names to keep them shorter and easy to read. Actions displays job names that reuse workflows in the following format: `caller workflow name / called workflow name`. The changes here are done in the called workflow names as changing the caller workflow names seem to make the summary page hard to parse (see https://github.com/jax-ml/jax/actions/runs/15217612585).\n\nHere's how the continuous workflow's summary page looks like with this change: https://github.com/jax-ml/jax/actions/runs/15286609214/job/42998511666\n\nPiperOrigin-RevId: 764390866",
        "output": "```diff\nCommit: 1994074023349c6f6c144a474f5b9c0fea8f9e16\nDate: 2025-05-28T20:25:14Z\nURL: https://github.com/jax-ml/jax/commit/1994074023349c6f6c144a474f5b9c0fea8f9e16\nFiles changed: 7\nAdditions: +18, Deletions: -7\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex c98bcee980e8..7eb6d2ed27b4 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -49,7 +49,8 @@ jobs:\n       JAXCI_HERMETIC_PYTHON_VERSION: ${{ inputs.python }}\n       JAXCI_ENABLE_X64: ${{ inputs.enable-x64 }}\n \n-    name: \"Bazel CPU tests with py_import (${{ inputs.runner }}, Python ${{ inputs.python }}, x64=${{ inputs.enable-x64 }})\"\n+    name: \"${{ (contains(inputs.runner, 'linux-x86') && 'linux x86') ||\n+        (contains(inputs.runner, 'linux-arm64') && 'linux arm64') }}, py ${{ inputs.python }}, x64=${{ inputs.enable-x64 }}\"\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 3e68034dfbf4..5a3ceaa8a4e8 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -60,7 +60,10 @@ jobs:\n       # Enable writing to the Bazel remote cache bucket.\n       JAXCI_WRITE_TO_BAZEL_REMOTE_CACHE: \"1\"\n \n-    name: \"Bazel single accelerator and multi-accelerator CUDA tests (jaxlib version=${{ inputs.jaxlib-version }}, ${{ inputs.runner }}, Python ${{ inputs.python }}, x64=${{ inputs.enable-x64 }})\"\n+    name: \"jaxlib=${{ inputs.jaxlib-version }},\n+          ${{ (contains(inputs.runner, 'h100') && 'h100') ||\n+          (contains(inputs.runner, 'b200') && 'b200') ||\n+          (contains(inputs.runner, 'l4') && 'l4') }}, py ${{ inputs.python }}, x64=${{ inputs.enable-x64 }}\"\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex d5fc35a99cd5..7bca28c3190d 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -111,7 +111,10 @@ jobs:\n       JAXCI_HERMETIC_PYTHON_VERSION: \"${{ inputs.python }}\"\n       JAXCI_CLONE_MAIN_XLA: \"${{ inputs.clone_main_xla }}\"\n \n-    name: Build ${{ inputs.artifact }} (${{ inputs.runner }}, Python ${{ inputs.python }}, clone main XLA=${{ inputs.clone_main_xla }})\n+    name: \"${{ inputs.artifact }},\n+            ${{ (contains(inputs.runner, 'linux-x86') && 'linux x86') ||\n+            (contains(inputs.runner, 'linux-arm64') && 'linux arm64') ||\n+            (contains(inputs.runner, 'windows-x86') && 'windows x86') }}, py ${{ inputs.python }}, clone main XLA=${{ inputs.clone_main_xla }}\"\n \n     # Map the job outputs to step outputs\n     outputs:\ndiff --git a/.github/workflows/cloud-tpu-ci-presubmit.yml b/.github/workflows/cloud-tpu-ci-presubmit.yml\nindex 40c99735c2de..090259c0f849 100644\n--- a/.github/workflows/cloud-tpu-ci-presubmit.yml\n+++ b/.github/workflows/cloud-tpu-ci-presubmit.yml\n@@ -54,7 +54,7 @@ jobs:\n     needs: [build-jax-artifacts]\n     uses: ./.github/workflows/pytest_tpu.yml\n     # Begin Presubmit Naming Check - name modification requires internal check to be updated\n-    name: \"TPU test (jaxlib=head, v5e-8)\"\n+    name: \"TPU test (jaxlib=head)\"\n     with:\n       runner: \"linux-x86-ct5lp-224-8tpu\"\n       cores: \"8\"\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex 3af06fe8037e..263bfd7ec9a9 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -56,7 +56,9 @@ jobs:\n                    (contains(inputs.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') ||\n                    (contains(inputs.runner, 'windows-x86') && null) }}\n \n-    name: \"Pytest CPU (${{ inputs.runner }}, Python ${{ inputs.python }}, x64=${{ inputs.enable-x64 }})\"\n+    name: \"${{ (contains(inputs.runner, 'linux-x86') && 'linux x86') ||\n+        (contains(inputs.runner, 'linux-arm64') && 'linux arm64') ||\n+        (contains(inputs.runner, 'windows-x86') && 'windows x86') }}, py ${{ inputs.python }}, x64=${{ inputs.enable-x64 }}\"\n \n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: \"${{ inputs.python }}\"\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 78f32cda672d..5f8888526aad 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -68,7 +68,9 @@ jobs:\n     container:  ${{ !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.1') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.1-cudnn9.8:latest') ||\n                 !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.8') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest') ||\n                 inputs.use-nvidia-pip-wheels && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest'}}\n-    name: \"Pytest CUDA (${{ inputs.runner }}, CUDA ${{ inputs.cuda }}, Python ${{ inputs.python }}, x64=${{ inputs.enable-x64 }})\"\n+    name: \"${{ (contains(inputs.runner, 'h100') && 'h100') ||\n+        (contains(inputs.runner, 'b200') && 'b200') ||\n+        (contains(inputs.runner, 'l4') && 'l4') }}, CUDA ${{ inputs.cuda-version }}, py ${{ inputs.python }}, x64=${{ inputs.enable-x64 }}\"\n \n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: \"${{ inputs.python }}\"\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 22cd64977dc5..8c2457208e12 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -78,7 +78,7 @@ jobs:\n     runs-on: ${{ inputs.runner }}\n     container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n     # Begin Presubmit Naming Check - name modification requires internal check to be updated\n-    name: \"Pytest TPU (${{ inputs.tpu-type }}, Python ${{ inputs.python }}, libtpu=${{ inputs.libtpu-version-type }})\"\n+    name: \"${{ inputs.tpu-type }}, py ${{ inputs.python }}, libtpu=${{ inputs.libtpu-version-type }}\"\n     # End Presubmit Naming Check github-tpu-presubmits\n \n     env:\n\n```",
        "from_id": [
            "nitins17",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29069 from jenriver:skip_mds_for_cpu\n\nPiperOrigin-RevId: 764376257",
        "output": "```diff\nCommit: da4ea8d97394ea9a1eb6043de4eef51a233171d5\nDate: 2025-05-28T19:48:42Z\nURL: https://github.com/jax-ml/jax/commit/da4ea8d97394ea9a1eb6043de4eef51a233171d5\nFiles changed: 1\nAdditions: +6, Deletions: -0\ndiff --git a/jax/_src/cloud_tpu_init.py b/jax/_src/cloud_tpu_init.py\nindex 0539e4253063..f42794db7696 100644\n--- a/jax/_src/cloud_tpu_init.py\n+++ b/jax/_src/cloud_tpu_init.py\n@@ -15,11 +15,14 @@\n import datetime\n import os\n import re\n+import logging\n import warnings\n from jax import version\n from jax._src import config\n from jax._src import hardware_utils\n \n+logger = logging.getLogger(__name__)\n+\n running_in_cloud_tpu_vm: bool = False\n \n \n@@ -74,6 +77,9 @@ def cloud_tpu_init() -> None:\n   # Exit early if we're not running on a Cloud TPU VM or libtpu isn't installed.\n   libtpu_path = get_tpu_library_path()\n   num_tpu_chips, tpu_id = hardware_utils.num_available_tpu_chips_and_device_id()\n+  if num_tpu_chips == 0:\n+    logger.info('Using LibTPU with a device other than TPU. Skipping TPU metadata query.')\n+    os.environ['TPU_SKIP_MDS_QUERY'] = '1'\n   if (\n       tpu_id is not None\n       and tpu_id >= hardware_utils.TpuVersion.v5e\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Skip TPU metadata server query when not using TPU.\n\nResolve an issue where `jax.devices()` hangs due to unwanted TPU\nmetadata query when using LibTPU with a device other than TPU (ex:\nCPU's).\nThis feature can be useful in cross [AOT](https://docs.jax.dev/en/latest/aot.html).",
        "output": "```diff\nCommit: 68fcf154bdcce970ef8fae50b1876404726855bf\nDate: 2025-05-28T18:24:05Z\nURL: https://github.com/jax-ml/jax/commit/68fcf154bdcce970ef8fae50b1876404726855bf\nFiles changed: 1\nAdditions: +6, Deletions: -0\ndiff --git a/jax/_src/cloud_tpu_init.py b/jax/_src/cloud_tpu_init.py\nindex 0539e4253063..f42794db7696 100644\n--- a/jax/_src/cloud_tpu_init.py\n+++ b/jax/_src/cloud_tpu_init.py\n@@ -15,11 +15,14 @@\n import datetime\n import os\n import re\n+import logging\n import warnings\n from jax import version\n from jax._src import config\n from jax._src import hardware_utils\n \n+logger = logging.getLogger(__name__)\n+\n running_in_cloud_tpu_vm: bool = False\n \n \n@@ -74,6 +77,9 @@ def cloud_tpu_init() -> None:\n   # Exit early if we're not running on a Cloud TPU VM or libtpu isn't installed.\n   libtpu_path = get_tpu_library_path()\n   num_tpu_chips, tpu_id = hardware_utils.num_available_tpu_chips_and_device_id()\n+  if num_tpu_chips == 0:\n+    logger.info('Using LibTPU with a device other than TPU. Skipping TPU metadata query.')\n+    os.environ['TPU_SKIP_MDS_QUERY'] = '1'\n   if (\n       tpu_id is not None\n       and tpu_id >= hardware_utils.TpuVersion.v5e\n\n```",
        "from_id": [
            "jenriver"
        ]
    },
    {
        "text_input": "Merge pull request #28665 from mattjj:smap-systematic\n\nPiperOrigin-RevId: 764330041",
        "output": "```diff\nCommit: bfc20eb30bb117760615e678631de10224e98936\nDate: 2025-05-28T17:50:59Z\nURL: https://github.com/jax-ml/jax/commit/bfc20eb30bb117760615e678631de10224e98936\nFiles changed: 1\nAdditions: +88, Deletions: -0\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex df69db7c9462..f473a4dc0547 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -3853,6 +3853,94 @@ def f(x):\n     self.assertAllClose(f(jnp.arange(8.)), jnp.array([1.,  5.,  9., 13.]))\n \n \n+def smap_ref(f, in_axes, out_axes, axis_name, axis_size):\n+  del axis_name  # no collectives\n+  def smapped(*args):\n+    split_args = zip(*[split_arg(x, d, axis_size) for x, d in zip(args, in_axes)])\n+    split_result = [f(*xs) for xs in split_args]\n+    return concat_result(split_result, out_axes)\n+  return smapped\n+\n+def split_arg(x, d, axis_size):\n+  if d is None:\n+    x = np.tile(x, [axis_size] + [1] * (x.ndim - 1))\n+  return np.split(x, axis_size, d or 0)\n+\n+def concat_result(results, out_axes):\n+  if not isinstance(results[0], (list, tuple)):\n+    return results[0] if out_axes is None else np.concatenate(results, out_axes)\n+  return [res[0] if d is None else np.concatenate(res, d)\n+          for res, d in zip(zip(*results), out_axes)]\n+\n+def sample_smap() -> Chooser:\n+  spec = yield fun_specs\n+  mesh_shape = yield mesh_shapes\n+  axis_names = ('i', 'j', 'k', 'l')[:len(mesh_shape)]\n+  mesh = SimpleNamespace(shape=dict(zip(axis_names, mesh_shape)),\n+                         axis_names=axis_names)\n+  axis_name = yield axis_names\n+  body_in_types = yield (tys for tys in it.product(input_shapes, repeat=spec.num_inputs)\n+                         if not spec.valid_types or spec.valid_types(*tys))\n+  in_axes = yield from sample_in_axes(body_in_types)\n+  out_rep = spec.out_rep(*[ax is None for ax in in_axes])\n+  body_out_type = jax.eval_shape(spec.fun, *body_in_types)\n+  out_axes = yield from sample_out_axes(out_rep, body_out_type)\n+  in_str = '(' + ','.join(jax.core.ShapedArray(t.shape, t.dtype).str_short()\n+                          for t in body_in_types) + ')'\n+  name = f'{spec.name}_{mesh.shape}_{in_axes}_{out_axes}_{axis_name}_{in_str}'\n+  in_types = [ty.update(shape=dilate_axis(ty.shape, d, mesh.shape[axis_name]))\n+              for ty, d in zip(body_in_types, in_axes)]\n+  args = [np.arange(ty.size, dtype=ty.dtype).reshape(ty.shape) / ty.size\n+          for ty in in_types]\n+  return name, spec, mesh.shape, in_axes, out_axes, axis_name, args\n+\n+def sample_in_axes(body_in_types) -> Chooser:\n+  in_axes = []\n+  for ty in body_in_types:\n+    in_axes.append((yield [None, *range(ty.ndim)]))\n+  return tuple(in_axes)\n+\n+def sample_out_axes(out_rep, body_out_type) -> Chooser:\n+  if not isinstance(body_out_type, (list, tuple)):\n+    out_axes = yield [None] * out_rep + list(range(body_out_type.ndim))\n+  else:\n+    out_axes_ = []\n+    for ty, r in zip(body_out_type, out_rep):\n+      out_axes_.append((yield [None] * r + list(range(ty.ndim))))\n+    out_axes = tuple(out_axes_)\n+  return out_axes\n+\n+def dilate_axis(shape: tuple[int, ...], i: int | None, size: int) -> tuple[int, ...]:\n+  if i is None:\n+    return shape\n+  shp = list(shape)\n+  shp[i] *= size\n+  return tuple(shp)\n+\n+class SmapSystematicTest(jtu.JaxTestCase):\n+\n+  @staticmethod\n+  def make_mesh(mesh_shape):\n+    return jtu.create_mesh(tuple(mesh_shape.values()), tuple(mesh_shape))\n+\n+  @parameterized.parameters(\n+      sample(jtu.NUM_GENERATED_CASES.value, sample_smap))\n+  def test_against_ref(self, fun_spec, mesh_shape, in_axes, out_axes, axis_name, args):\n+    fun = fun_spec.fun\n+    mesh = self.make_mesh(mesh_shape)\n+    args = map(jnp.array, args)\n+\n+    with jax.sharding.use_mesh(mesh):\n+      fun_ = smap(fun, in_axes=in_axes, out_axes=out_axes, axis_name=axis_name)\n+      out = jax.jit(fun_)(*args)\n+\n+    fun_ref = smap_ref(fun, in_axes=in_axes, out_axes=out_axes, axis_name=axis_name,\n+                       axis_size=mesh_shape[axis_name])\n+    expected = fun_ref(*args)\n+\n+    self.assertAllClose(out, expected, check_dtypes=False)\n+\n+\n @jtu.with_config(jax_use_shardy_partitioner=True)\n # TODO(phawkins): enable this test unconditionally once shardy is the default.\n @unittest.skipIf(sdy is None, \"shardy is not enabled\")\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29030 from dfm:custom-transpose-nones\n\nPiperOrigin-RevId: 764328992",
        "output": "```diff\nCommit: f32ce04fd02ccfb5fbb79a0c7c84843acd7ff050\nDate: 2025-05-28T17:48:45Z\nURL: https://github.com/jax-ml/jax/commit/f32ce04fd02ccfb5fbb79a0c7c84843acd7ff050\nFiles changed: 2\nAdditions: +25, Deletions: -5\ndiff --git a/jax/_src/custom_transpose.py b/jax/_src/custom_transpose.py\nindex 21e607b5bff2..fb125e174122 100644\n--- a/jax/_src/custom_transpose.py\n+++ b/jax/_src/custom_transpose.py\n@@ -217,7 +217,6 @@ def custom_transpose_transpose_rule(\n   # Consider passing this information to the custom transpose rule?\n \n   res_arg, lin_arg = tree_unflatten(call_in_tree, args)\n-  del lin_arg\n   assert all(not ad.is_undefined_primal(x) for x in tree_leaves(res_arg))\n \n   cts = [ad_util.zeros_like_aval(ct.aval) if type(ct) is ad_util.Zero else ct\n@@ -225,10 +224,17 @@ def custom_transpose_transpose_rule(\n   ct_out = tree_unflatten(out_tree, cts)\n   ct_lin = transpose.call_wrapped(res_arg, ct_out)\n   check_transpose_rule_trees(transpose, lin_tree, tree_structure(ct_lin))\n-  ct_lin_flat, _ = tree_flatten(\n-      tree_broadcast(lin_tree, ct_lin, is_leaf=lambda x: x is None),\n-      is_leaf=lambda x: x is None)\n-  return [None] * len(tree_leaves(res_arg)) + ct_lin_flat\n+  ct_lin = tree_broadcast(lin_tree, ct_lin, is_leaf=lambda x: x is None)\n+\n+  # When the transpose returns None, we treat that as a Zero, except when the\n+  # input is also None. In that case, the cotangent corresponding to that input\n+  # should be dropped.\n+  zero = object()\n+  ct_lin = tree_map(lambda l, ct: zero if ct is None and l is not None else ct,\n+                    lin_arg, ct_lin, is_leaf=ad.is_undefined_primal)\n+\n+  ct_lin_flat, _ = tree_flatten(ct_lin)\n+  return [None] * res_tree.num_leaves + [None if ct is zero else ct for ct in ct_lin_flat]\n \n \n def custom_transpose_lowering(*args, call_jaxpr, **params):\ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 9d10b40c6030..bfe391797920 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3722,6 +3722,20 @@ def gt(x, t):\n     with config.use_direct_linearize(True):\n       self.assertAllClose(jax.grad(f)(0.5), jnp.cos(0.5))\n \n+  def test_input_none(self):\n+    # ref: https://github.com/jax-ml/jax/issues/29009\n+    @jax.custom_jvp\n+    def f(x, y): return y\n+    @f.defjvp\n+    def f_jvp(p, t): return f(*p), g(p, t)\n+\n+    @custom_transpose(jnp.float32(0))\n+    def g(r, x): return x[1]\n+    @g.def_transpose\n+    def gt(r, t): return None, jnp.zeros_like(r[1])\n+\n+    jax.grad(f, argnums=(1,))(None, jnp.float32(2))  # doesn't crash\n+\n \n class CustomDceTest(jtu.JaxTestCase):\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[better-errors] if a non-jaxtype is returned, say it's a return problem",
        "output": "```diff\nCommit: ba64c02fb3baa283c5475a2b74b7ceed584c25c7\nDate: 2025-05-28T17:45:32Z\nURL: https://github.com/jax-ml/jax/commit/ba64c02fb3baa283c5475a2b74b7ceed584c25c7\nFiles changed: 2\nAdditions: +38, Deletions: -1\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 6ea16ec8e8ba..3c499429a663 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -2243,7 +2243,7 @@ def trace_to_jaxpr_dynamic(\n     try:\n       with core.set_current_trace(trace):\n         ans = fun.call_wrapped(*in_tracers)\n-\n+      _check_returned_jaxtypes(fun.debug_info, ans)\n       out_tracers = map(partial(trace.to_jaxpr_tracer, source_info=source_info), ans)\n       _check_no_returned_refs(fun.debug_info, out_tracers)\n       jaxpr, consts, attrs_tracked = trace.to_jaxpr(out_tracers, fun.debug_info)\n@@ -2255,6 +2255,20 @@ def trace_to_jaxpr_dynamic(\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   return jaxpr, [v.aval for v in jaxpr.outvars], consts, attrs_tracked\n \n+def _check_returned_jaxtypes(dbg, out_tracers):\n+  for i, x in enumerate(out_tracers):\n+    try:\n+      core.typeof(x)\n+    except TypeError:\n+      if (dbg and len(paths := dbg.result_paths()) > i and\n+          (p := paths[i].removeprefix('result'))):\n+        extra = f' at output component {p}'\n+      else:\n+        extra = ''\n+      raise TypeError(\n+      f\"function {dbg.func_src_info} traced for {dbg.traced_for} returned a \"\n+      f\"value of type {type(x)}{extra}, which is not a valid JAX type\") from None\n+\n def _check_no_returned_refs(\n     dbg: core.DebugInfo,\n     out_tracers: Sequence[DynamicJaxprTracer]\ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex f5b74e1e10d6..584eb0eda496 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -5058,6 +5058,29 @@ def test_ensure_compile_time_eval_no_leaks(self):\n     with jax.ensure_compile_time_eval():\n       jnp.linalg.solve(jnp.eye(3), jnp.ones(3))  # doesn't crash\n \n+  def test_returned_non_jaxtype(self):\n+\n+    class TestEnum(enum.Enum):\n+      A = enum.auto()\n+\n+    @jax.tree_util.register_dataclass\n+    @dataclasses.dataclass\n+    class TestClass3:\n+      test_enum_field: TestEnum = dataclasses.field(metadata=dict(static=True))\n+      test_data_field: int\n+\n+    def test_jax_function(test_class: TestClass3) -> TestEnum:\n+      return test_class.test_enum_field\n+\n+    jitted_test_function = jax.jit(test_jax_function)\n+    with self.assertRaisesRegex(TypeError, \"returned a value of type\"):\n+        jitted_test_function(\n+            TestClass3(\n+                test_data_field=1,\n+                test_enum_field=TestEnum.A,\n+            )\n+        )\n+\n \n class RematTest(jtu.JaxTestCase):\n \n\n```",
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Added the missing resource estimation rule for `pl.run_state`\n\nPiperOrigin-RevId: 764276682",
        "output": "```diff\nCommit: de491b96c5df15464fb6410cf264c4d830bfe0f9\nDate: 2025-05-28T15:41:34Z\nURL: https://github.com/jax-ml/jax/commit/de491b96c5df15464fb6410cf264c4d830bfe0f9\nFiles changed: 1\nAdditions: +52, Deletions: -10\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 82a0a47c4a0d..a56733a89f60 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -176,11 +176,18 @@ def _estimate_resources(\n   rs = Resources(smem_scratch_bytes=0)\n   for eqn in jaxpr.eqns:\n     # TODO(slebedev): Add support for other primitives, notably control flow.\n-    rule = _resource_estimators.get(eqn.primitive)\n-    if rule is None:\n-      # Assume that unsupported primitives are neutral wrt resource usage.\n+    if rule := _resource_estimators.get(eqn.primitive):\n+      rs |= rule(ctx, *(invar.aval for invar in eqn.invars), **eqn.params)\n       continue\n-    rs |= rule(ctx, *(invar.aval for invar in eqn.invars), **eqn.params)\n+    # Assume that unsupported primitives are neutral wrt resource usage,\n+    # unless they have a jaxpr in their params.\n+    if any(\n+        isinstance(v, (jax_core.Jaxpr, jax_core.ClosedJaxpr))\n+        for v in eqn.params.values()\n+    ):\n+      raise NotImplementedError(\n+          f\"Resource estimation does not support {eqn.primitive}\"\n+      )\n \n   return rs\n \n@@ -188,7 +195,7 @@ def _estimate_resources(\n @_register_resource_estimator(lax.cond_p)\n def _cond_resource_estimator(\n     ctx: ResourceEstimatorContext, *args, branches\n-) -> int:\n+) -> Resources:\n   del args  # Unused.\n   return functools.reduce(\n       lambda a, b: a | b,\n@@ -199,7 +206,7 @@ def _cond_resource_estimator(\n @_register_resource_estimator(lax.scan_p)\n def _scan_resource_estimator(\n     ctx: ResourceEstimatorContext, *args, jaxpr: jax_core.ClosedJaxpr, **params\n-) -> int:\n+) -> Resources:\n   del args, params  # Unused.\n   return _estimate_resources(ctx, jaxpr)\n \n@@ -211,17 +218,52 @@ def _while_resource_estimator(\n     cond_jaxpr: jax_core.ClosedJaxpr,\n     body_jaxpr: jax_core.ClosedJaxpr,\n     **params,\n-) -> int:\n+) -> Resources:\n   del args, params  # Unused.\n   return _estimate_resources(ctx, cond_jaxpr) | _estimate_resources(\n       ctx, body_jaxpr\n   )\n \n \n+@_register_resource_estimator(pjit.pjit_p)\n+def _pjit_resource_estimator(\n+    ctx: ResourceEstimatorContext,\n+    *args,\n+    jaxpr: jax_core.ClosedJaxpr,\n+    **params,\n+) -> Resources:\n+  del args, params  # Unused.\n+  return _estimate_resources(ctx, jaxpr)\n+\n+\n+@_register_resource_estimator(pallas_core.core_map_p)\n+def _core_map_resource_estimator(\n+    ctx: ResourceEstimatorContext,\n+    *args,\n+    jaxpr: jax_core.ClosedJaxpr,\n+    **params,\n+) -> Resources:\n+  del args, params  # Unused.\n+  return _estimate_resources(ctx, jaxpr)\n+\n+\n+@_register_resource_estimator(discharge.run_state_p)\n+def _run_state_resource_estimator(\n+    ctx: ResourceEstimatorContext, *args, jaxpr: jax_core.Jaxpr, **params\n+) -> Resources:\n+  del args, params  # Unused.\n+  return _estimate_resources(ctx, jaxpr)\n+\n+\n @_register_resource_estimator(primitives.run_scoped_p)\n def _run_scoped_resource_estimator(\n-    ctx: ResourceEstimatorContext, *consts, jaxpr: jax_core.Jaxpr, collective_axes\n-) -> int:\n+    ctx: ResourceEstimatorContext,\n+    *consts,\n+    jaxpr: jax_core.Jaxpr,\n+    collective_axes,\n+) -> Resources:\n+  del collective_axes  # Unused.\n+\n   # NOTE: This rule assumes that the allocation happens collectively, although\n   # it can't be checked here due to limited context. We check this in the actual\n   # lowering rule.\n@@ -280,7 +322,7 @@ def _run_scoped_resource_estimator(\n @_register_resource_estimator(lax.reduce_sum_p)\n def _reduce_sum_resource_estimator(\n     ctx: ResourceEstimatorContext, x_aval: jax_core.ShapedArray, *, axes\n-) -> int:\n+) -> Resources:\n   del ctx, axes  # Unused.\n   # We don't need shmem for some reductons, but it depends on the layout, so we\n   # conservatively request some scratch space.\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:triton] Removed the `Triton` prefix from `TritonCompilerParams`\n\nAll Triton-specific APIs are always used qualified, e.g. `plgpu.TritonCompilerParams`,\nso the prefix is redundant.\n\nPiperOrigin-RevId: 764276165",
        "output": "```diff\nCommit: 30eecf68052c2ee485c40a04f07b3fe2097a7f8a\nDate: 2025-05-28T15:39:42Z\nURL: https://github.com/jax-ml/jax/commit/30eecf68052c2ee485c40a04f07b3fe2097a7f8a\nFiles changed: 13\nAdditions: +52, Deletions: -28\ndiff --git a/docs/jax.experimental.pallas.triton.rst b/docs/jax.experimental.pallas.triton.rst\nindex 76b0896ccf17..023a33bb0909 100644\n--- a/docs/jax.experimental.pallas.triton.rst\n+++ b/docs/jax.experimental.pallas.triton.rst\n@@ -9,7 +9,7 @@ Classes\n .. autosummary::\n    :toctree: _autosummary\n \n-   TritonCompilerParams\n+   CompilerParams\n \n Functions\n ---------\n@@ -19,4 +19,4 @@ Functions\n \n    approx_tanh\n    debug_barrier\n-   elementwise_inline_asm\n\\ No newline at end of file\n+   elementwise_inline_asm\ndiff --git a/docs/pallas/CHANGELOG.md b/docs/pallas/CHANGELOG.md\nindex 2d8a83c897f1..40a30057354d 100644\n--- a/docs/pallas/CHANGELOG.md\n+++ b/docs/pallas/CHANGELOG.md\n@@ -13,6 +13,14 @@ Remember to align the itemized text with the first line of an item within a list\n \n ## Unreleased\n \n+* Deprecations\n+\n+  * {class}`jax.experimental.pallas.triton.TritonCompilerParams` has been\n+    renamed to {class}`jax.experimental.pallas.triton.CompilerParams`. The\n+    old name is deprecated and will be removed in a future release.\n+\n+## Released with jax 0.6.1\n+\n * Removals\n \n   * Removed previously deprecated {mod}`jax.experimental.pallas.gpu`. To use\ndiff --git a/jax/_src/pallas/pallas_call.py b/jax/_src/pallas/pallas_call.py\nindex 964709b4c915..6f8c96a4591c 100644\n--- a/jax/_src/pallas/pallas_call.py\n+++ b/jax/_src/pallas/pallas_call.py\n@@ -1499,7 +1499,7 @@ def pallas_call(\n     interpret: Any = False,\n     name: str | None = None,\n     compiler_params: (\n-        Mapping[Backend, CompilerParams] | CompilerParams | None\n+        Mapping[Backend, \"CompilerParams\"] | \"CompilerParams\" | None\n     ) = None,\n     cost_estimate: CostEstimate | None = None,\n     backend: Backend | None = None,\n@@ -1550,7 +1550,7 @@ def pallas_call(\n     compiler_params: Optional compiler parameters. The value should either be a\n       backend-specific dataclass\n       (:class:`jax.experimental.pallas.tpu.TPUCompilerParams`,\n-      :class:`jax.experimental.pallas.triton.TritonCompilerParams`,\n+      :class:`jax.experimental.pallas.triton.CompilerParams`,\n       :class:`jax.experimental.pallas.mosaic_gpu.CompilerParams`) or a dict\n       mapping backend name to the corresponding platform-specific dataclass.\n     backend: Optional string literal one of  ``\"mosaic_tpu\"``, ``\"triton\"`` or\n@@ -1600,13 +1600,13 @@ def _normalize_compiler_params(\n ) -> Mapping[Backend, CompilerParams]:\n   if compiler_params is None:\n     return {}\n-  if isinstance(compiler_params, pallas_core.CompilerParams):\n+  if isinstance(compiler_params, CompilerParams):\n     compiler_params = {compiler_params.BACKEND: compiler_params}\n   assert isinstance(compiler_params, Mapping)\n   for backend, params in compiler_params.items():\n     if backend not in [\"mosaic_tpu\", \"mosaic_gpu\", \"triton\"]:\n       raise ValueError(f\"Unknown backend in compiler_params: {backend}\")\n-    if not isinstance(params, pallas_core.CompilerParams):\n+    if not isinstance(params, CompilerParams):\n       raise ValueError(\n           f\"Unexpected compiler_params for backend {backend}: {params}\"\n       )\ndiff --git a/jax/_src/pallas/triton/core.py b/jax/_src/pallas/triton/core.py\nindex 6b3e10f2b018..7b6e69dc8dd8 100644\n--- a/jax/_src/pallas/triton/core.py\n+++ b/jax/_src/pallas/triton/core.py\n@@ -21,7 +21,7 @@\n from jax._src.pallas import core as pallas_core\n \n @dataclasses.dataclass(frozen=True)\n-class TritonCompilerParams(pallas_core.CompilerParams):\n+class CompilerParams(pallas_core.CompilerParams):\n   \"\"\"Compiler parameters for Triton.\n \n   Attributes:\ndiff --git a/jax/_src/pallas/triton/pallas_call_registration.py b/jax/_src/pallas/triton/pallas_call_registration.py\nindex e111cef0f924..9bb5c8f21628 100644\n--- a/jax/_src/pallas/triton/pallas_call_registration.py\n+++ b/jax/_src/pallas/triton/pallas_call_registration.py\n@@ -72,9 +72,9 @@ def pallas_call_lowering(\n   [lowering_platform] = ctx.platforms or ctx.module_context.platforms\n \n   if \"triton\" in compiler_params:\n-    params = cast(triton_core.TritonCompilerParams, compiler_params[\"triton\"])\n+    params = cast(triton_core.CompilerParams, compiler_params[\"triton\"])\n   else:\n-    params = triton_core.TritonCompilerParams()\n+    params = triton_core.CompilerParams()\n   num_warps = 4 if params.num_warps is None else params.num_warps\n   num_stages = params.num_stages\n   if num_stages is None:\ndiff --git a/jax/experimental/pallas/ops/gpu/attention.py b/jax/experimental/pallas/ops/gpu/attention.py\nindex 2442ed14f351..ae429be5d73a 100644\n--- a/jax/experimental/pallas/ops/gpu/attention.py\n+++ b/jax/experimental/pallas/ops/gpu/attention.py\n@@ -288,7 +288,7 @@ def mha(\n       grid=grid_,\n       in_specs=in_specs,\n       out_specs=out_specs,\n-      compiler_params=plgpu.TritonCompilerParams(\n+      compiler_params=plgpu.CompilerParams(\n           num_warps=num_warps_, num_stages=num_stages),\n       out_shape=out_shape,\n       debug=debug,\n@@ -351,7 +351,7 @@ def _preprocess_backward(out, do, lse, block_q: int,\n                        lambda i, j, k: (j, i, k, 0)),\n       ],\n       out_specs=pl.BlockSpec((None, None, block_q), lambda i, j, k: (j, k, i)),\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=4, num_stages=3),\n+      compiler_params=plgpu.CompilerParams(num_warps=4, num_stages=3),\n       out_shape=out_shape,\n       debug=debug,\n       interpret=interpret,\n@@ -634,7 +634,7 @@ def _mha_backward(sm_scale: float, causal: bool, block_sizes: BlockSizes,\n         name=\"mha_backward\",\n         debug=debug,\n         interpret=interpret,\n-        compiler_params=plgpu.TritonCompilerParams(\n+        compiler_params=plgpu.CompilerParams(\n             num_warps=num_warps_, num_stages=2\n         ),\n     )(q, k, v, segment_ids, out, do, lse, delta)\ndiff --git a/jax/experimental/pallas/ops/gpu/decode_attention.py b/jax/experimental/pallas/ops/gpu/decode_attention.py\nindex e2c19b3eaf2d..ee8c22d1b3a4 100644\n--- a/jax/experimental/pallas/ops/gpu/decode_attention.py\n+++ b/jax/experimental/pallas/ops/gpu/decode_attention.py\n@@ -193,7 +193,7 @@ def decode_attn_unbatched(\n       pl.BlockSpec((None, block_h), lambda i, j: (j, i)),  # l\n       pl.BlockSpec((None, block_h), lambda i, j: (j, i)),  # m\n     ],\n-    compiler_params=plgpu.TritonCompilerParams(\n+    compiler_params=plgpu.CompilerParams(\n       num_warps=num_warps_, num_stages=num_stages\n     ),\n     out_shape=[\ndiff --git a/jax/experimental/pallas/ops/gpu/layer_norm.py b/jax/experimental/pallas/ops/gpu/layer_norm.py\nindex 187d74ee1fd9..b838885a9136 100644\n--- a/jax/experimental/pallas/ops/gpu/layer_norm.py\n+++ b/jax/experimental/pallas/ops/gpu/layer_norm.py\n@@ -94,7 +94,7 @@ def layer_norm_forward(\n   ]\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=num_warps),\n+      compiler_params=plgpu.CompilerParams(num_warps=num_warps),\n       grid=(),\n       out_shape=out_shape,\n       debug=False,\n@@ -215,7 +215,7 @@ def layer_norm_backward(\n   out_shape_dx = jax.ShapeDtypeStruct(shape=(n,), dtype=x.dtype)\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=num_warps),\n+      compiler_params=plgpu.CompilerParams(num_warps=num_warps),\n       grid=(),\n       out_shape=out_shape_dx,\n       debug=False,\n@@ -247,7 +247,7 @@ def layer_norm_backward(\n   grid_ = (pl.cdiv(reshaped_x.shape[1], block_n),)\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=num_warps),\n+      compiler_params=plgpu.CompilerParams(num_warps=num_warps),\n       grid=grid_,\n       out_shape=out_shape_dwbias,\n       debug=False,\n@@ -283,7 +283,7 @@ def layer_norm(\n   out_shape = jax.ShapeDtypeStruct(shape=(n,), dtype=x.dtype)\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(\n+      compiler_params=plgpu.CompilerParams(\n           num_warps=num_warps, num_stages=num_stages),\n       grid=(),\n       out_shape=out_shape,\ndiff --git a/jax/experimental/pallas/ops/gpu/paged_attention.py b/jax/experimental/pallas/ops/gpu/paged_attention.py\nindex b30ef554fe12..fbf861f92412 100644\n--- a/jax/experimental/pallas/ops/gpu/paged_attention.py\n+++ b/jax/experimental/pallas/ops/gpu/paged_attention.py\n@@ -222,7 +222,7 @@ def paged_attention_unbatched(\n       ],\n       debug=debug,\n       interpret=interpret,\n-      compiler_params=plgpu.TritonCompilerParams(\n+      compiler_params=plgpu.CompilerParams(\n           num_warps=num_warps, num_stages=num_stages\n       ),\n       name=f\"paged_attention_{block_h=}_{pages_per_compute_block=}\",\ndiff --git a/jax/experimental/pallas/ops/gpu/rms_norm.py b/jax/experimental/pallas/ops/gpu/rms_norm.py\nindex baeaeb8a57b3..a1b2b582f7bb 100644\n--- a/jax/experimental/pallas/ops/gpu/rms_norm.py\n+++ b/jax/experimental/pallas/ops/gpu/rms_norm.py\n@@ -82,7 +82,7 @@ def rms_norm_forward(\n   ]\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=num_warps),\n+      compiler_params=plgpu.CompilerParams(num_warps=num_warps),\n       grid=(),\n       out_shape=out_shape,\n       debug=False,\n@@ -196,7 +196,7 @@ def rms_norm_backward(\n   out_shape_dx = jax.ShapeDtypeStruct(shape=(n,), dtype=x.dtype)\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=num_warps),\n+      compiler_params=plgpu.CompilerParams(num_warps=num_warps),\n       grid=(),\n       out_shape=out_shape_dx,\n       debug=False,\n@@ -228,7 +228,7 @@ def rms_norm_backward(\n   grid_ = (pl.cdiv(reshaped_x.shape[1], block_n),)\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=num_warps),\n+      compiler_params=plgpu.CompilerParams(num_warps=num_warps),\n       grid=grid_,\n       out_shape=out_shape_dwbias,\n       debug=False,\n@@ -264,7 +264,7 @@ def rms_norm(\n   out_shape = jax.ShapeDtypeStruct(shape=(n,), dtype=x.dtype)\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(\n+      compiler_params=plgpu.CompilerParams(\n           num_warps=num_warps, num_stages=num_stages\n       ),\n       grid=(),\ndiff --git a/jax/experimental/pallas/ops/gpu/softmax.py b/jax/experimental/pallas/ops/gpu/softmax.py\nindex 7fc6a0f50cb4..68960081288e 100644\n--- a/jax/experimental/pallas/ops/gpu/softmax.py\n+++ b/jax/experimental/pallas/ops/gpu/softmax.py\n@@ -80,7 +80,7 @@ def softmax(\n   kernel = functools.partial(_vmappable_softmax_kernel, block_row=block_row)\n   f = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(\n+      compiler_params=plgpu.CompilerParams(\n           num_warps=num_warps, num_stages=1),\n       grid=(),\n       out_shape=out_shape,\ndiff --git a/jax/experimental/pallas/triton.py b/jax/experimental/pallas/triton.py\nindex 06adb9e6da7e..1c512540adf2 100644\n--- a/jax/experimental/pallas/triton.py\n+++ b/jax/experimental/pallas/triton.py\n@@ -14,7 +14,23 @@\n \n \"\"\"Triton-specific Pallas APIs.\"\"\"\n \n-from jax._src.pallas.triton.core import TritonCompilerParams as TritonCompilerParams\n+from jax._src.pallas.triton.core import CompilerParams as CompilerParams\n from jax._src.pallas.triton.primitives import approx_tanh as approx_tanh\n from jax._src.pallas.triton.primitives import debug_barrier as debug_barrier\n from jax._src.pallas.triton.primitives import elementwise_inline_asm as elementwise_inline_asm\n+\n+import typing as _typing  # pylint: disable=g-import-not-at-top\n+if _typing.TYPE_CHECKING:\n+  TritonCompilerParams = CompilerParams\n+else:\n+  from jax._src.deprecations import deprecation_getattr as _deprecation_getattr\n+  _deprecations = {\n+      # Deprecated on May 27th 2025.\n+      \"TritonCompilerParams\": (\n+          \"TritonCompilerParams is deprecated, use CompilerParams instead.\",\n+          CompilerParams,\n+      ),\n+  }\n+  __getattr__ = _deprecation_getattr(__name__, _deprecations)\n+  del _deprecation_getattr\n+del _typing\ndiff --git a/tests/pallas/ops_test.py b/tests/pallas/ops_test.py\nindex 3e777ac7ea2c..c819d050c8a5 100644\n--- a/tests/pallas/ops_test.py\n+++ b/tests/pallas/ops_test.py\n@@ -1646,7 +1646,7 @@ def kernel(x_ref, o_ref):\n \n   @unittest.skipIf(\n       sys.platform == \"win32\",\n-      \"plgpu_triton.TritonCompilerParams unavailable on Windows\",\n+      \"plgpu_triton.CompilerParams unavailable on Windows\",\n   )\n   def test_debug_print(self):\n     self.skip_if_mosaic_gpu()\n@@ -1661,7 +1661,7 @@ def test_debug_print(self):\n     @functools.partial(\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct((2,), jnp.float32),\n-        compiler_params=plgpu_triton.TritonCompilerParams(\n+        compiler_params=plgpu_triton.CompilerParams(\n             num_warps=1, num_stages=1\n         ),\n     )\n@@ -1677,7 +1677,7 @@ def kernel(x_ref, o_ref):\n \n   @unittest.skipIf(\n       sys.platform == \"win32\",\n-      \"plgpu_triton.TritonCompilerParams unavailable on Windows\",\n+      \"plgpu_triton.CompilerParams unavailable on Windows\",\n   )\n   def test_debug_print_with_values(self):\n     if jtu.test_device_matches([\"tpu\"]):\n@@ -1690,7 +1690,7 @@ def test_debug_print_with_values(self):\n     @functools.partial(\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct((2,), jnp.float32),\n-        compiler_params=plgpu_triton.TritonCompilerParams(\n+        compiler_params=plgpu_triton.CompilerParams(\n             num_warps=1, num_stages=1\n         ),\n     )\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix JAX PGLE test\n\nXLA dumps one more HLO file by default, which leads to one more PGLE profile\nfile.\n\nPiperOrigin-RevId: 764274080",
        "output": "```diff\nCommit: fd28b2f45953d5dfd01a5bc9e7795fe38ba22b72\nDate: 2025-05-28T15:34:04Z\nURL: https://github.com/jax-ml/jax/commit/fd28b2f45953d5dfd01a5bc9e7795fe38ba22b72\nFiles changed: 1\nAdditions: +6, Deletions: -4\ndiff --git a/tests/pgle_test.py b/tests/pgle_test.py\nindex fd55f0a392f0..e136c3ab8a5a 100644\n--- a/tests/pgle_test.py\n+++ b/tests/pgle_test.py\n@@ -167,8 +167,9 @@ def f(x):\n           self.assertArraysEqual(f(x), expected)\n         self.assertEqual(cache_miss_count(), 2)\n         fdo_profiles_before_pgle = self.get_fdo_profiles(dump_dir)\n-        # One for before and one for after optimization.\n-        self.assertLen(fdo_profiles_before_pgle, 2)\n+        # One for before optimizatiom, one after SPMD partitioning, and one\n+        # after optimization.\n+        self.assertLen(fdo_profiles_before_pgle, 3)\n         # The FDO profile file should be empty.\n         self.assertEqual(\n             os.path.getsize(os.path.join(dump_dir, fdo_profiles_before_pgle[0])), 0)\n@@ -178,8 +179,9 @@ def f(x):\n           self.assertArraysEqual(f(x), expected)\n         self.assertEqual(cache_miss_count(), 2)\n         fdo_profiles_after_pgle = self.get_fdo_profiles(dump_dir)\n-        # One for before and one for after optimization.\n-        self.assertLen(fdo_profiles_after_pgle, 4)\n+        # One more before optimizatiom, one more after SPMD partitioning, and\n+        # one more after optimization.\n+        self.assertLen(fdo_profiles_after_pgle, 6)\n \n         for fdo_profile in fdo_profiles_after_pgle:\n           if fdo_profile not in fdo_profiles_before_pgle:\n\n```",
        "from_id": [
            "frgossen",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Set the mesh in SPMDAxisContext to be a concrete mesh so that pallas/mosaic:GPU can get access to the device ids in the mesh\n\nPiperOrigin-RevId: 764263324",
        "output": "```diff\nCommit: 0d0393fd39b44c0627616752df71bc7b97904b80\nDate: 2025-05-28T15:05:07Z\nURL: https://github.com/jax-ml/jax/commit/0d0393fd39b44c0627616752df71bc7b97904b80\nFiles changed: 1\nAdditions: +11, Deletions: -2\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 1ed831d8577f..c38312868163 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -783,6 +783,13 @@ def _shardy_shard_map_token_sharding(\n   return ns._to_sdy_sharding(0)\n \n \n+def _get_spmdaxis_ctx_mesh(mesh):\n+  if isinstance(mesh, AbstractMesh):\n+    concrete_mesh = get_concrete_mesh()\n+    return concrete_mesh if concrete_mesh is not None else mesh\n+  return mesh\n+\n+\n def _shard_map_lowering_shardy(\n     ctx, in_nodes, jaxpr, mesh, in_specs, out_specs, manual_axes, check_vma):\n   axis_ctx = ctx.module_context.axis_context\n@@ -793,7 +800,8 @@ def _shard_map_lowering_shardy(\n     shardy_manual_axes = frozenset(mesh.axis_names) - axis_ctx.manual_axes\n   else:\n     shardy_manual_axes = manual_axes\n-  new_axis_context = sharding_impls.SPMDAxisContext(mesh, manual_axes)\n+  new_axis_context = sharding_impls.SPMDAxisContext(\n+      _get_spmdaxis_ctx_mesh(mesh), manual_axes)\n   sub_ctx = ctx.module_context.replace(axis_context=new_axis_context)\n \n   tokens = [ctx.tokens_in.get(eff) for eff in ctx.tokens_in.effects()]\n@@ -868,7 +876,8 @@ def _shard_map_lowering(ctx, *in_nodes, jaxpr, mesh, in_specs, out_specs,\n   out_avals_ = [x.aval for x in jaxpr.outvars]\n   in_nodes_ = map(partial(_xla_shard, ctx, mesh, manual_axes), in_specs,\n                   ctx.avals_in, in_avals_, in_nodes)\n-  new_axis_context = sharding_impls.SPMDAxisContext(mesh, manual_axes)\n+  new_axis_context = sharding_impls.SPMDAxisContext(\n+      _get_spmdaxis_ctx_mesh(mesh), manual_axes)\n   sub_ctx = ctx.module_context.replace(axis_context=new_axis_context)\n   with _extend_axis_env(mesh, manual_axes), config._check_vma(check_vma):\n     out_nodes_, tokens_out = mlir.call_lowering(\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Implement a new MMA/TMEM read pipelined matmul kernel\n\nThis replaces the old scheme that still included a bit of a bubble at the\nend of each tile with a new scheme that should be entirely bubble-free, for\nas long as the MMA loop is long enough to hide the store latency (i.e. for big\nenough K dimensions). This also removes the problems with spills we had in the\nprevious version since the register footprint is relatively small now.\n\nPiperOrigin-RevId: 764256446",
        "output": "```diff\nCommit: 98e6041a214af302e9945f066d1db56084584e6b\nDate: 2025-05-28T14:44:32Z\nURL: https://github.com/jax-ml/jax/commit/98e6041a214af302e9945f066d1db56084584e6b\nFiles changed: 6\nAdditions: +49, Deletions: -38\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 6d54e153a9a2..82a0a47c4a0d 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -258,8 +258,7 @@ def _run_scoped_resource_estimator(\n         packing = 4 // aval.dtype.itemsize\n       else:\n         packing = 1\n-      layout = tcgen05._infer_tmem_layout(\n-          aval.shape, collective=aval.collective, packing=packing)\n+      layout = tcgen05._infer_tmem_layout(aval.shape, packing=packing)\n       cols_used = layout.cols_in_shape(aval.shape)\n       cols_used = tcgen05._alloc_ncols(cols_used, exact=False)\n       rs += Resources(tmem_scratch_cols=cols_used)\n@@ -391,8 +390,7 @@ def alloc_tmem(\n     else:\n       packing = 1\n     if layout is None:\n-      layout = tcgen05._infer_tmem_layout(\n-          struct.shape, collective, packing=packing)\n+      layout = tcgen05._infer_tmem_layout(struct.shape, packing=packing)\n     unpadded_cols_used = layout.cols_in_shape(struct.shape)\n     cols_used = tcgen05._alloc_ncols(unpadded_cols_used, exact_cols)\n \ndiff --git a/jax/experimental/mosaic/gpu/core.py b/jax/experimental/mosaic/gpu/core.py\nindex 73403fccd595..4ed551654a0e 100644\n--- a/jax/experimental/mosaic/gpu/core.py\n+++ b/jax/experimental/mosaic/gpu/core.py\n@@ -351,7 +351,7 @@ def ref(member_thunks=member_thunks):\n         )\n         if layout is None:\n           layout = tcgen05._infer_tmem_layout(\n-              shape, collective, 1 if packing is None else packing\n+              shape, 1 if packing is None else packing\n           )\n         num_cols = layout.cols_in_shape(shape)\n         tmem_allocs.append(_TMEMAlloc(addr_ref, num_cols, collective))\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex 52909f6a6a2e..ac5a8985ebff 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -107,9 +107,8 @@ def compute_output(block_m_start, n_start, call_counter):\n       call_counter should be 0 the first time this function is called and\n       incremented by 1 before each subsequent call.\n       \"\"\"\n-      isnt_first_call = arith.cmpi(\n-          arith.CmpIPredicate.ne, call_counter, c(0, index)\n-      )\n+      acc_slot = arith.remui(call_counter, c(2, index))\n+      acc_slice = acc.slice(slice(None), mgpu.ds(arith.muli(acc_slot, c(tile_n, index)), tile_n))\n       # All blocks in the cluster share the same m_start -- align it!\n       m_start = arith.muli(arith.divui(block_m_start, c(tile_m, index)), c(tile_m, index))\n       with mgpu.when(is_leader_of(TMA_WARP)):\n@@ -119,6 +118,9 @@ def _tma_body(ki, _):\n           isnt_warmup = arith.cmpi(\n               arith.CmpIPredicate.uge, ki, c(max_concurrent_steps, index)\n           )\n+          isnt_first_call = arith.cmpi(\n+              arith.CmpIPredicate.ne, call_counter, c(0, index)\n+          )\n           with mgpu.when(arith.ori(isnt_first_call, isnt_warmup)):\n             ab_empty_barriers[slot].wait()\n           full_barrier = ab_full_barriers[slot]\n@@ -151,15 +153,16 @@ def _tma_body(ki, _):\n           )\n \n       # We wait in all blocks in the cluster to avoid double arrival errors.\n-      with mgpu.when(arith.andi(is_leader_of(MMA_WARP), isnt_first_call)):\n-        tmem_done_barrier.wait(for_tensor_core=True)\n+      reuses_tmem = arith.cmpi(arith.CmpIPredicate.uge, call_counter, c(2, index))\n+      with mgpu.when(arith.andi(is_leader_of(MMA_WARP), reuses_tmem)):\n+        tmem_done_barrier[acc_slot].wait(for_tensor_core=True)\n       with mgpu.when(arith.andi(is_leader_of(MMA_WARP), is_leader_block)):\n         @mgpu.fori(c(k_loop_iter, index), arith.constant(i1, 0))\n         def _mma_body(ki, accumulate):\n           slot = arith.remui(ki, c(max_concurrent_steps, index))\n           ab_full_barriers[slot].wait()\n           tcgen05.mma(\n-              acc,\n+              acc_slice,\n               mgpu.memref_slice(a_smem, slot),\n               mgpu.memref_transpose(mgpu.memref_slice(b_smem, slot), (1, 0, 3, 2)),\n               a_swizzle=swizzle,\n@@ -173,21 +176,17 @@ def _mma_body(ki, accumulate):\n               arith.CmpIPredicate.eq, ki, c(k_loop_iter - 1, index)\n           )\n           with mgpu.when(is_last_iter):\n-            tcgen05.commit_arrive(mma_done_barrier, collective=collective, ctx=ctx)\n+            tcgen05.commit_arrive(mma_done_barrier[acc_slot], collective=collective, ctx=ctx)\n           return accumulate\n \n       with mgpu.when(is_store_warpgroup):\n-        mma_done_barrier.wait(for_tensor_core=True)\n-        final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n+        mma_done_barrier[acc_slot].wait(for_tensor_core=True)\n+        final_acc = acc_slice.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n         assert tile_n % epilogue_tile_n == 0\n         for ni in range(tile_n // epilogue_tile_n):\n           n_slice = ds(ni * epilogue_tile_n, epilogue_tile_n)\n           final_acc[:, n_slice].store_tiled(d_smem, swizzle=128)\n           # We store the first tile before arriving to reduce register pressure.\n-          if ni == 0:\n-            # Make sure we've loaded all of TMEM before we arrive.\n-            tcgen05.wait_tmem_load()\n-            tmem_done_barrier.arrive(for_tensor_core=True)\n           mgpu.commit_shared()\n           store_n_start = arith.addi(n_start, c(ni * epilogue_tile_n, index))\n           ctx.async_copy(\n@@ -200,7 +199,8 @@ def _mma_body(ki, accumulate):\n               gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n               swizzle=128,\n           )\n-          ctx.await_async_copy(0)\n+          ctx.await_async_copy(0, await_read_only=True)\n+        tmem_done_barrier[acc_slot].arrive(for_tensor_core=True)\n \n     # We statically assign the tiles to SMs.\n     logical_grid_size = math.prod(logical_grid)\n@@ -246,9 +246,9 @@ def _mn_loop(local_mn_step, _):\n   smem = (\n       smem_buffers,\n       [mgpu.Barrier(arrival_count=1, num_barriers=max_concurrent_steps)] * 2,\n-      mgpu.Barrier(arrival_count=1),\n-      mgpu.ClusterBarrier(collective_dims=(gpu.Dimension.x,), num_barriers=1),\n-      mgpu.TMEM((128, tile_n), jnp.float32, collective=collective),\n+      mgpu.Barrier(arrival_count=1, num_barriers=2),\n+      mgpu.ClusterBarrier(collective_dims=(gpu.Dimension.x,), num_barriers=2),\n+      mgpu.TMEM((128, 2 * tile_n), jnp.float32, collective=collective),\n   )\n   num_sms = 148\n   return mgpu.as_gpu_kernel(\n@@ -273,7 +273,7 @@ def main(unused_argv):\n   b = jr.normal(key=kb, shape=(n, k), dtype=jnp.float16)\n \n   tile_m = (128,)\n-  tile_n = (128, 256, 512)\n+  tile_n = (128, 256)\n   max_concurrent_steps = (2, 4, 5, 6)\n   grid_tile_m = (1, 2, 4, 8, 16)\n   collective = (False, True)\n@@ -290,7 +290,7 @@ def main(unused_argv):\n       tile_n *= 2\n     if m < tile_m or n < tile_n:\n       continue\n-    if tile_n > 512:\n+    if 2 * tile_n > 512:\n       continue\n     if (m // tile_m) % kwargs[\"grid_tile_m\"]:\n       continue\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex 86fbd31ed56d..13d945249b69 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -173,9 +173,14 @@ def mma(\n     raise ValueError(\n         f\"Accumulator shape mismatch: expected {(m, n * num_cta)}, got {d.shape}\"\n     )\n-  if d.layout != (expected_layout := _infer_tmem_layout(d.shape, collective, packing=1)):\n+  expected_d_layout = (\n+      TMEM_COLLECTIVE_N512_LAYOUT\n+      if collective and n * num_cta == 512\n+      else TMEM_DEFAULT_LAYOUT\n+  )\n+  if d.layout != expected_d_layout:\n     raise ValueError(\n-        f\"Accumulator layout mismatch: expected {expected_layout}, got {d.layout}\"\n+        f\"Accumulator layout mismatch: expected {expected_d_layout}, got {d.layout}\"\n     )\n   f32 = ir.F32Type.get()\n   f16 = ir.F16Type.get()\n@@ -570,9 +575,7 @@ def cols_in_shape(self, shape: tuple[int, int]):\n     return num_tiles // tiles_in_row * cols_in_tile\n \n \n-def _infer_tmem_layout(\n-    shape: tuple[int, int], collective: bool, packing: int = 1\n-) -> TMEMLayout:\n+def _infer_tmem_layout(shape: tuple[int, int], packing: int = 1) -> TMEMLayout:\n   if shape[0] > TMEM_ROWS:\n     raise ValueError(\n         \"Can only infer TMEM layout for shapes with at most 128 rows, got:\"\n@@ -593,14 +596,14 @@ def _infer_tmem_layout(\n         \"Can only infer TMEM layout for shapes with column count that's a\"\n         f\" multiple of 8, got: {shape[1]}\"\n     )\n-  if collective and shape[1] == 512:\n-    return TMEMLayout(\n-        elements_in_tile=(shape[0], 128), column_tile_stride=2, packing=packing\n-    )\n-  else:\n-    return TMEMLayout(elements_in_tile=(shape[0], 8), packing=packing)\n+  return TMEMLayout(elements_in_tile=(shape[0], 8), packing=packing)\n \n \n+TMEM_DEFAULT_LAYOUT = TMEMLayout(elements_in_tile=(TMEM_ROWS, 8), packing=1)\n+TMEM_COLLECTIVE_N512_LAYOUT = TMEMLayout(\n+    elements_in_tile=(TMEM_ROWS, 128), column_tile_stride=2, packing=1\n+)\n+\n @dataclasses.dataclass(frozen=True)\n class TMEMRef:\n   address: ir.Value\n@@ -669,6 +672,8 @@ def slice(self, *idxs):\n     col_idx = base_idx[1]\n     if not isinstance(col_idx, ir.Value):\n       col_idx = arith.constant(i32, col_idx)\n+    if col_idx.type == ir.IndexType.get():\n+      col_idx = arith.index_cast(i32, col_idx)\n     if packing != 1:\n       col_idx = arith.divui(col_idx, arith.constant(i32, packing))\n     return TMEMRef(\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 232cb703d06a..99b7d67cd691 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -1398,11 +1398,12 @@ def quantize(x):\n     y_block_shape = (n_block_tile, k) if rhs_transpose else (k, n_block_tile)\n     y = quantize(self.prng.uniform(-1, 1, y_shape)).astype(in_jax_dtype)\n     out_shape = jax.ShapeDtypeStruct((m, n), out_jax_dtype)\n+    tmem_layout = tcgen05.TMEM_COLLECTIVE_N512_LAYOUT if n == 512 else None\n     scratch_shape = [\n         jax.ShapeDtypeStruct(tile_shape(x_block_shape, tiling), in_jax_dtype),\n         jax.ShapeDtypeStruct(tile_shape(y_block_shape, tiling), in_jax_dtype),\n         mgpu.TMABarrier(3),\n-        mgpu.TMEM((128, n), out_jax_dtype, collective=True),\n+        mgpu.TMEM((128, n), out_jax_dtype, collective=True, layout=tmem_layout),\n     ]\n     z = mgpu.as_gpu_kernel(\n         kernel, (2, 1, 1), (128, 1, 1), (x, y), out_shape, scratch_shape, cluster=(2, 1, 1)\ndiff --git a/tests/mosaic/matmul_test.py b/tests/mosaic/matmul_test.py\nindex 680e699c8972..13082885710e 100644\n--- a/tests/mosaic/matmul_test.py\n+++ b/tests/mosaic/matmul_test.py\n@@ -161,8 +161,15 @@ def test_matmul_sm100(self, data):\n     tile_m = data.draw(\n         hps.sampled_from([t for t in [128] if t * num_ctas <= m]), label=\"tile_m\"\n     )\n+    tmem_cols = 512\n     tile_n = data.draw(\n-        hps.sampled_from([t for t in [64, 128, 256] if t * num_ctas <= n]), label=\"tile_n\"\n+        hps.sampled_from([\n+            t\n+            for t in [64, 128, 256]\n+            # We're double buffering TMEM in the kernel, hence the 2x.\n+            if t * num_ctas <= n and 2 * t * num_ctas <= tmem_cols\n+        ]),\n+        label=\"tile_n\",\n     )\n     grid_m = m // (num_ctas * tile_m)\n     grid_tile_m = data.draw(hps.sampled_from([1, 2, 4, 8, 16]), label=\"grid_tile_m\")\n@@ -196,4 +203,4 @@ def test_matmul_sm100(self, data):\n \n \n if __name__ == \"__main__\":\n-  absltest.main(testLoader=jtu.JaxTestLoader())\n+  absltest.main(argv=[\"python\"], testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Reduce SMEM pressure of the GMEM store\n\nThis reworks the previous scheme by transferring all of TMEM to registers at once,\nand then doing RMEM->SMEM->GMEM in multiple phases, allowing us to use a smaller\nSMEM buffer. This, in turn, lets us bump max_concurrent_steps for the MMA pipeline\nwhich increases performance considerably.\n\nThe only downside of this scheme is that even though it should be technically feasible\nto perform the epilogue with 255 registers per thread, ptxas generates a number of spills\nthat might be lowering our performance. Either way, it's still better than the previous\nalternatives.\n\nPiperOrigin-RevId: 764249234",
        "output": "```diff\nCommit: 360799e6405004a9d9a59e044122168314dff970\nDate: 2025-05-28T14:20:00Z\nURL: https://github.com/jax-ml/jax/commit/360799e6405004a9d9a59e044122168314dff970\nFiles changed: 3\nAdditions: +43, Deletions: -14\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex bf50ca702063..52909f6a6a2e 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -179,17 +179,28 @@ def _mma_body(ki, accumulate):\n       with mgpu.when(is_store_warpgroup):\n         mma_done_barrier.wait(for_tensor_core=True)\n         final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n-        final_acc.store_tiled(d_smem, swizzle=128)\n-        mgpu.commit_shared()\n-        tmem_done_barrier.arrive()\n-        ctx.async_copy(\n-            src_ref=d_smem,\n-            dst_ref=d,\n-            gmem_slice=(ds(block_m_start, block_tile_m), ds(n_start, tile_n)),\n-            gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n-            swizzle=128,\n-        )\n-        ctx.await_async_copy(0)\n+        assert tile_n % epilogue_tile_n == 0\n+        for ni in range(tile_n // epilogue_tile_n):\n+          n_slice = ds(ni * epilogue_tile_n, epilogue_tile_n)\n+          final_acc[:, n_slice].store_tiled(d_smem, swizzle=128)\n+          # We store the first tile before arriving to reduce register pressure.\n+          if ni == 0:\n+            # Make sure we've loaded all of TMEM before we arrive.\n+            tcgen05.wait_tmem_load()\n+            tmem_done_barrier.arrive(for_tensor_core=True)\n+          mgpu.commit_shared()\n+          store_n_start = arith.addi(n_start, c(ni * epilogue_tile_n, index))\n+          ctx.async_copy(\n+              src_ref=d_smem,\n+              dst_ref=d,\n+              gmem_slice=(\n+                  ds(block_m_start, block_tile_m),\n+                  ds(store_n_start, epilogue_tile_n),\n+              ),\n+              gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n+              swizzle=128,\n+          )\n+          ctx.await_async_copy(0)\n \n     # We statically assign the tiles to SMs.\n     logical_grid_size = math.prod(logical_grid)\n@@ -227,8 +238,9 @@ def _mn_loop(local_mn_step, _):\n         mgpu.tile_shape((max_concurrent_steps, block_tile_n, tile_k), tiling),\n         dtype),\n   )\n+  epilogue_tile_n = 64\n   epilogue_buffer = jax.ShapeDtypeStruct(\n-      mgpu.tile_shape((block_tile_m, tile_n), (128, swizzle_elems)),\n+      mgpu.tile_shape((block_tile_m, epilogue_tile_n), (128, swizzle_elems)),\n       dtype)\n   smem_buffers = [compute_buffers, epilogue_buffer]\n   smem = (\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex c4f670527a0c..86fbd31ed56d 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -477,6 +477,17 @@ def tmem_load(tmem_addr, shape, num, pack: bool):\n   return [llvm.extractvalue(i32, regs, [i]) for i in range(num_out_regs)]\n \n \n+def wait_tmem_load():\n+  llvm.inline_asm(\n+      ir.Type.parse(\"!llvm.void\"),\n+      [],\n+      \"tcgen05.wait::ld.sync.aligned;\",\n+      \"\",\n+      has_side_effects=True,\n+  )\n+  utils.warpgroup_barrier()\n+\n+\n def tmem_store(tmem_addr, shape, num, regs, unpack: bool):\n   num_out_regs, regs_vector = _tmem_access_helper(shape, num)\n   pack_mod = \".unpack::16b\" if unpack else \"\"\n@@ -832,7 +843,7 @@ def _transfer_32xcols(\n   regs_per_instr = atom_shape[0] * atom_shape[1] // (utils.WARP_SIZE * reg_packing)\n   # We artificially lower the instr_num compared to its limits, because higher\n   # values can lead to register spills..\n-  instr_num = min(total_num, 64 // regs_per_instr)\n+  instr_num = min(total_num, 32 // regs_per_instr)\n   assert 32 % atom_rows == 0\n   num_row_steps = 32 // atom_rows\n   for lane_step in range(num_row_steps):\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex e55a21442db0..51b6ed4612ca 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -993,11 +993,17 @@ def __iter__(self):\n   def __getitem__(self, offset):\n     return CollectiveBarrierRef(self.barrier[offset], self.cluster_mask)\n \n-  def arrive(self):\n+  def arrive(self, for_tensor_core: bool = False):\n     \"\"\"Arrives on a barrier in all blocks that share at least one of the coordinates along the collective dimensions.\n \n     Note that unlike in arrive, each warpgroup arrives once.\n     \"\"\"\n+    if for_tensor_core:\n+      llvm.inline_asm(\n+          ir.Type.parse(\"!llvm.void\"),\n+          [], \"tcgen05.fence::before_thread_sync;\", \"\",\n+          has_side_effects=True,\n+      )\n     if self.barrier.num_barriers != 1:\n       raise ValueError(\"Can only arrive on a single barrier\")\n     if self.cluster_mask is None:\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Use a second warpgroup to store the MMA outputs\n\nThis allows us to prime the GMEM->SMEM pipeline for the next tile\nwhile storing the SMEM->GMEM tile for the current one. However, this implies\nthat we can no longer share the same SMEM region for the MMA pipeline\nand the epilogue, which pushes the SMEM pressure so high that we can't fetch\ntoo many steps into the future. Overall the performance is slightly worse than\nfor the baseline kernel, but it recovers and improves upon it in the follow up.\n\nPiperOrigin-RevId: 764220403",
        "output": "```diff\nCommit: 39f09066e35205335f4c9dd1013316200e77dc31\nDate: 2025-05-28T12:42:34Z\nURL: https://github.com/jax-ml/jax/commit/39f09066e35205335f4c9dd1013316200e77dc31\nFiles changed: 2\nAdditions: +39, Deletions: -21\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex 8bf8ca557496..bf50ca702063 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -86,7 +86,7 @@ def build_kernel(\n   logical_grid = (grid_tile_m, n // tile_n, m // (block_tile_m * grid_tile_m))\n \n   def kernel(ctx, a, b, d, smem):\n-    ((a_smem, b_smem), d_smem), barriers, mma_done_barrier, acc = smem\n+    ((a_smem, b_smem), d_smem), barriers, mma_done_barrier, tmem_done_barrier, acc = smem\n     (ab_full_barriers, ab_empty_barriers) = barriers\n \n     warp_idx = mgpu.warp_idx(sync=True)\n@@ -97,6 +97,9 @@ def kernel(ctx, a, b, d, smem):\n     is_leader_block = arith.cmpi(\n         arith.CmpIPredicate.eq, ctx.cluster_idx(gpu.Dimension.x), c(0, index)\n     )\n+    is_store_warpgroup = arith.cmpi(\n+        arith.CmpIPredicate.eq, mgpu.warpgroup_idx(sync=True), c(1, i32)\n+    )\n \n     def compute_output(block_m_start, n_start, call_counter):\n       \"\"\"Compute and store a single output tile.\n@@ -104,6 +107,9 @@ def compute_output(block_m_start, n_start, call_counter):\n       call_counter should be 0 the first time this function is called and\n       incremented by 1 before each subsequent call.\n       \"\"\"\n+      isnt_first_call = arith.cmpi(\n+          arith.CmpIPredicate.ne, call_counter, c(0, index)\n+      )\n       # All blocks in the cluster share the same m_start -- align it!\n       m_start = arith.muli(arith.divui(block_m_start, c(tile_m, index)), c(tile_m, index))\n       with mgpu.when(is_leader_of(TMA_WARP)):\n@@ -113,9 +119,6 @@ def _tma_body(ki, _):\n           isnt_warmup = arith.cmpi(\n               arith.CmpIPredicate.uge, ki, c(max_concurrent_steps, index)\n           )\n-          isnt_first_call = arith.cmpi(\n-              arith.CmpIPredicate.ne, call_counter, c(0, index)\n-          )\n           with mgpu.when(arith.ori(isnt_first_call, isnt_warmup)):\n             ab_empty_barriers[slot].wait()\n           full_barrier = ab_full_barriers[slot]\n@@ -147,6 +150,9 @@ def _tma_body(ki, _):\n               **common_args,\n           )\n \n+      # We wait in all blocks in the cluster to avoid double arrival errors.\n+      with mgpu.when(arith.andi(is_leader_of(MMA_WARP), isnt_first_call)):\n+        tmem_done_barrier.wait(for_tensor_core=True)\n       with mgpu.when(arith.andi(is_leader_of(MMA_WARP), is_leader_block)):\n         @mgpu.fori(c(k_loop_iter, index), arith.constant(i1, 0))\n         def _mma_body(ki, accumulate):\n@@ -170,20 +176,20 @@ def _mma_body(ki, accumulate):\n             tcgen05.commit_arrive(mma_done_barrier, collective=collective, ctx=ctx)\n           return accumulate\n \n-      gpu.barrier()\n-      mma_done_barrier.wait(for_tensor_core=True)\n-\n-      final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n-      final_acc.store_tiled(d_smem, swizzle=128)\n-      mgpu.commit_shared()\n-      ctx.async_copy(\n-          src_ref=d_smem,\n-          dst_ref=d,\n-          gmem_slice=(ds(block_m_start, block_tile_m), ds(n_start, tile_n)),\n-          gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n-          swizzle=swizzle,\n-      )\n-      ctx.await_async_copy(0)\n+      with mgpu.when(is_store_warpgroup):\n+        mma_done_barrier.wait(for_tensor_core=True)\n+        final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n+        final_acc.store_tiled(d_smem, swizzle=128)\n+        mgpu.commit_shared()\n+        tmem_done_barrier.arrive()\n+        ctx.async_copy(\n+            src_ref=d_smem,\n+            dst_ref=d,\n+            gmem_slice=(ds(block_m_start, block_tile_m), ds(n_start, tile_n)),\n+            gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n+            swizzle=128,\n+        )\n+        ctx.await_async_copy(0)\n \n     # We statically assign the tiles to SMs.\n     logical_grid_size = math.prod(logical_grid)\n@@ -224,18 +230,19 @@ def _mn_loop(local_mn_step, _):\n   epilogue_buffer = jax.ShapeDtypeStruct(\n       mgpu.tile_shape((block_tile_m, tile_n), (128, swizzle_elems)),\n       dtype)\n-  smem_buffers = mgpu.Union([compute_buffers, epilogue_buffer])\n+  smem_buffers = [compute_buffers, epilogue_buffer]\n   smem = (\n       smem_buffers,\n       [mgpu.Barrier(arrival_count=1, num_barriers=max_concurrent_steps)] * 2,\n       mgpu.Barrier(arrival_count=1),\n+      mgpu.ClusterBarrier(collective_dims=(gpu.Dimension.x,), num_barriers=1),\n       mgpu.TMEM((128, tile_n), jnp.float32, collective=collective),\n   )\n   num_sms = 148\n   return mgpu.as_gpu_kernel(\n       kernel,\n       (num_sms, 1, 1),  # This is a persistent kernel.\n-      (128, 1, 1),\n+      (2 * 128, 1, 1),\n       (\n           jax.ShapeDtypeStruct((m, k), dtype),\n           jax.ShapeDtypeStruct((n, k), dtype),\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex 224f5a09cfe5..e55a21442db0 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -817,8 +817,19 @@ def update_parities(self, parities: ir.Value) -> tuple[ir.Value, ir.Value]:\n     )\n     return parity, arith.xori(parities, bitmask)\n \n-  def arrive(self, arrival_count: int = 1, can_complete: bool = True):\n+  def arrive(\n+      self,\n+      arrival_count: int = 1,\n+      can_complete: bool = True,\n+      for_tensor_core: bool = False,\n+  ):\n     i64 = ir.IntegerType.get_signless(64)\n+    if for_tensor_core:\n+      llvm.inline_asm(\n+          ir.Type.parse(\"!llvm.void\"),\n+          [], \"tcgen05.fence::before_thread_sync;\", \"\",\n+          has_side_effects=True,\n+      )\n     if can_complete:\n       if arrival_count > 1:\n         count = c(arrival_count - 1, ir.IntegerType.get_signless(32))\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Implement FragmentedArray.__getitem__ for arbitrary tiled layouts\n\nAny tile-aligned slicing is easy to handle.\n\nPiperOrigin-RevId: 764189366",
        "output": "```diff\nCommit: 0b17f6ce59e893940b16cb8d2aa7b39661e587df\nDate: 2025-05-28T10:49:44Z\nURL: https://github.com/jax-ml/jax/commit/0b17f6ce59e893940b16cb8d2aa7b39661e587df\nFiles changed: 2\nAdditions: +108, Deletions: -27\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex 77584b5f0dd4..04dd30023293 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -1376,26 +1376,26 @@ def bitcast(self, elt: ir.Type, *, output_is_signed: bool | None = None):\n     )\n \n   def __getitem__(self, idx):\n-    if self.layout !=  WGMMA_LAYOUT:\n-      raise NotImplementedError(\"Only WGMMA layouts support slicing\")\n+    if not isinstance(self.layout, TiledLayout):\n+      raise NotImplementedError(\"Only arrays with tiled layouts can be sliced\")\n     base_idx, slice_shape, is_squeezed = utils.parse_indices(idx, self.shape)\n+    if any(isinstance(idx, ir.Value) for idx in base_idx):\n+      raise ValueError(\"Only static slicing allowed\")\n     if any(is_squeezed):\n       raise NotImplementedError(\"Only slicing implemented\")\n-    if (\n-        base_idx[0] % 64\n-        or slice_shape[0] % 64\n-        or base_idx[1] % 8\n-        or slice_shape[1] % 8\n+    base_tile_shape = self.layout.base_tile_shape\n+    if len(base_tile_shape) != len(self.shape):\n+      raise NotImplementedError(\"Tiling has different rank than array\")\n+    if any(\n+        b % t or l % t\n+        for b, l, t in zip(base_idx, slice_shape, base_tile_shape, strict=True)\n     ):\n       raise NotImplementedError(\"Only tile aligned slicing supported\")\n-    base_idx[0] //= 64\n-    slice_shape[0] //= 64\n-    base_idx[1] //= 8\n-    slice_shape[1] //= 8\n-    new_regs = self.registers[\n-        base_idx[0] : base_idx[0] + slice_shape[0],\n-        base_idx[1] : base_idx[1] + slice_shape[1],\n-    ]\n+    register_slices = tuple(\n+        slice(b // t, (b + l) // t)\n+        for b, l, t in zip(base_idx, slice_shape, base_tile_shape, strict=True)\n+    )\n+    new_regs = self.registers[register_slices]\n     return FragmentedArray(\n         _registers=new_regs, _layout=self.layout, _is_signed=self.is_signed\n     )\n@@ -1882,6 +1882,21 @@ def select(self, on_true, on_false):\n         lambda t, p, f: arith.select(p, t, f), self, on_false,\n     )\n \n+  @classmethod\n+  def build(\n+      cls,\n+      shape: tuple[int, ...],\n+      layout: FragmentedLayout,\n+      fn: Callable[..., ir.Value],  # ir.Value varargs, one for each dim\n+      *,\n+      is_signed: bool | None = None,\n+  ):\n+    undef = llvm.mlir_undef(ir.IntegerType.get_signless(32))\n+    dummy = cls.splat(undef, shape, layout, is_signed=False)\n+    return dummy.foreach(\n+        lambda _, idx: fn(*idx), create_array=True, is_signed=is_signed\n+    )\n+\n   def foreach(\n       self,\n       fn: Callable[[ir.Value, tuple[ir.Value, ...]], ir.Value | None],\n@@ -1892,8 +1907,19 @@ def foreach(\n     \"\"\"Call a function for each value and index.\"\"\"\n     index = ir.IndexType.get()\n     new_regs = None\n-    if create_array:\n-      new_regs = np.full_like(self.registers, llvm.mlir_undef(self.registers.flat[0].type))\n+    orig_fn = fn\n+    def fn(*args):\n+      nonlocal new_regs\n+      result = orig_fn(*args)\n+      old_reg_type = self.registers.flat[0].type\n+      # Lazily create new_regs once we know the desired output type.\n+      if create_array and new_regs is None:\n+        if ir.VectorType.isinstance(old_reg_type):\n+          new_reg_type = ir.VectorType.get(old_reg_type.shape, result.type)\n+        else:\n+          new_reg_type = result.type\n+        new_regs = np.full_like(self.registers, llvm.mlir_undef(new_reg_type))\n+      return result\n     for mlir_idx, reg_idx in zip(self.layout.thread_idxs(self.shape), np.ndindex(self.registers.shape), strict=True):\n       reg = self.registers[reg_idx]\n       assert len(mlir_idx) == len(self.shape), (mlir_idx, self.shape)\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 0c79d26782c7..232cb703d06a 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -3544,7 +3544,9 @@ def test_pass_is_registered(self):\n \n if hp is not None:\n   @hps.composite\n-  def tiled_layouts(draw, initial_tile, vector_transfer: bool = False):\n+  def tiled_layouts(\n+      draw, initial_tile, vector_transfer: bool = False\n+  ) -> fa.TiledLayout:\n     assert all(t.bit_count() == 1 for t in initial_tile)\n     assert math.prod(initial_tile) >= 128\n     tiles = [initial_tile]\n@@ -3605,20 +3607,28 @@ def tiled_layouts(draw, initial_tile, vector_transfer: bool = False):\n         vector_dim=vector_dim,\n     )\n \n+  @hps.composite\n+  def shape_and_tiled_layout(\n+      draw, vector_transfer: bool = False\n+  ) -> tuple[tuple[int, ...], fa.TiledLayout]:\n+    rank = draw(hps.integers(2, 3))\n+    initial_tile = tuple(\n+        draw(hps.sampled_from([1, 2, 4, 8, 16, 32, 64, 128]))\n+        for _ in range(rank)\n+    )\n+    hp.assume(128 <= math.prod(initial_tile) < 128 * 32)\n+    shape = tuple(t * draw(hps.integers(1, 5)) for t in initial_tile)\n+    hp.assume(math.prod(shape) <= 128 * 128)\n+    layout = draw(tiled_layouts(initial_tile, vector_transfer=vector_transfer))\n+    return shape, layout\n+\n   class HypothesisTest(TestCase):\n \n     def test_reduce(self):\n       @hps.composite\n       def strategy(draw):\n-        rank = draw(hps.integers(2, 3))\n-        initial_tile = tuple(\n-            draw(hps.sampled_from([1, 2, 4, 8, 16, 32, 64, 128]))\n-            for _ in range(rank)\n-        )\n-        hp.assume(128 <= math.prod(initial_tile) < 128 * 32)\n-        shape = tuple(t * draw(hps.integers(1, 5)) for t in initial_tile)\n-        hp.assume(math.prod(shape) <= 128 * 128)\n-        layout = draw(tiled_layouts(initial_tile, vector_transfer=True))\n+        shape, layout = draw(shape_and_tiled_layout(vector_transfer=True))\n+        rank = len(shape)\n         reduced_dims = draw(hps.sets(hps.integers(0, rank - 1), min_size=1))\n         return shape, layout, tuple(reduced_dims)\n \n@@ -3645,6 +3655,51 @@ def kernel(ctx, src, dst, scratch):\n         np.testing.assert_array_equal(result, x.max(reduced_dims))\n       run()\n \n+    def test_slice(self):\n+      i32 = ir.IntegerType.get_signless(32)\n+      index = ir.IndexType.get()\n+\n+      @hps.composite\n+      def strategy(draw):\n+        shape, layout = draw(shape_and_tiled_layout(vector_transfer=True))\n+        tiling = layout.base_tile_shape\n+        tiled_shape = mgpu.tile_shape(shape, tiling)[:len(shape)]\n+        def draw_slice(size, tile):\n+          start = draw(hps.integers(0, size - 1))\n+          length = draw(hps.integers(1, size - start))\n+          return slice(start * tile, (start + length) * tile)\n+        slices = tuple(map(draw_slice, tiled_shape, tiling))\n+        return shape, layout, slices\n+\n+      basic_slices = (slice(128, 256), slice(16, 16 + 32))\n+      @hp.given(strategy())\n+      @hp.example(((256, 256), fa.WGMMA_LAYOUT, basic_slices))\n+      @hp.example(((256, 256), tcgen05.LAYOUT, basic_slices))\n+      @hp.example(((256, 256), tcgen05.TMEM_NATIVE_LAYOUT, basic_slices))\n+      def run(args):\n+        shape, layout, slices = args\n+        def kernel(ctx, dst, _):\n+          def linear_index(*idxs):\n+            total = arith.constant(index, 0)\n+            stride = 1\n+            for i, size in zip(idxs[::-1], shape[::-1]):\n+              total = arith.addi(total, arith.muli(i, c(stride, index)))\n+              stride *= size\n+            return arith.index_cast(i32, total)\n+          x = mgpu.FragmentedArray.build(\n+              shape, layout, linear_index, is_signed=True\n+          )\n+          x[slices].store_untiled(dst, optimized=False)\n+\n+        slice_shape = tuple(len(range(size)[s]) for s, size in zip(slices, shape))\n+        out_shape = jax.ShapeDtypeStruct(shape=slice_shape, dtype=jnp.int32)\n+        result = mgpu.as_gpu_kernel(\n+            kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ()\n+        )()\n+        iota = np.arange(np.prod(shape), dtype=jnp.int32).reshape(*shape)\n+        np.testing.assert_array_equal(result, iota[slices])\n+      run()\n+\n \n if __name__ == \"__main__\":\n   absltest.main(argv=[\"python\"], testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Improve the error message when PTX version inference fails\n\nPiperOrigin-RevId: 764182705",
        "output": "```diff\nCommit: f7adde5227514094beb725766fc4648f115a0aa5\nDate: 2025-05-28T10:25:24Z\nURL: https://github.com/jax-ml/jax/commit/f7adde5227514094beb725766fc4648f115a0aa5\nFiles changed: 1\nAdditions: +3, Deletions: -2\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 54fef13a8521..bf6a04783be7 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -232,8 +232,9 @@ absl::StatusOr<int> GetLatestPtxasPtxIsaVersion() {\n   // Unsupported .version 99.99; current version is '8.8'\n   std::vector<std::string> chunks = absl::StrSplit(status.message(), '\\'');\n   if (chunks.size() != 3) {\n-    return absl::InternalError(\n-        \"Failed to locate PTX ISA version in ptxas error message\");\n+    return absl::InternalError(absl::StrCat(\n+        \"Failed to locate PTX ISA version in ptxas error message: \",\n+        status.message()));\n   }\n   std::vector<std::string> major_minor = absl::StrSplit(chunks[1], '.');\n   if (major_minor.size() != 2) {\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move jax/_src/attrs.py to its own BUILD rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nThis required moving a couple `jax.numpy` imports into local functions. These could probably be addressed by moving the registrations elsewhere.\n\nPiperOrigin-RevId: 764170653",
        "output": "```diff\nCommit: 27e4a7486246879a16946ed18a9ba964d72d97c2\nDate: 2025-05-28T09:41:51Z\nURL: https://github.com/jax-ml/jax/commit/27e4a7486246879a16946ed18a9ba964d72d97c2\nFiles changed: 2\nAdditions: +23, Deletions: -5\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 396e6fdf6ed4..2e2d7902577d 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -297,7 +297,6 @@ py_library_providing_imports_info(\n         \"_src/ad_checkpoint.py\",\n         \"_src/api.py\",\n         \"_src/array.py\",\n-        \"_src/attrs.py\",\n         \"_src/blocked_sampler.py\",\n         \"_src/buffer_callback.py\",\n         \"_src/callback.py\",\n@@ -377,6 +376,7 @@ py_library_providing_imports_info(\n         \":ad\",\n         \":ad_util\",\n         \":api_util\",\n+        \":attrs\",\n         \":basearray\",\n         \":batching\",\n         \":cloud_tpu_init\",\n@@ -465,6 +465,22 @@ pytype_strict_library(\n     ] + py_deps(\"numpy\"),\n )\n \n+pytype_strict_library(\n+    name = \"attrs\",\n+    srcs = [\"_src/attrs.py\"],\n+    deps = [\n+        \":ad\",\n+        \":ad_util\",\n+        \":api_util\",\n+        \":core\",\n+        \":dtypes\",\n+        \":partial_eval\",\n+        \":source_info_util\",\n+        \":tree_util\",\n+        \":util\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"basearray\",\n     srcs = [\"_src/basearray.py\"],\ndiff --git a/jax/_src/attrs.py b/jax/_src/attrs.py\nindex db738ee6368d..7ad6f0e52d32 100644\n--- a/jax/_src/attrs.py\n+++ b/jax/_src/attrs.py\n@@ -16,7 +16,6 @@\n \n from typing import Any, Callable\n \n-import jax\n from jax._src import core\n from jax._src import source_info_util\n from jax._src import api_util\n@@ -53,7 +52,8 @@ def jax_setattr(obj: Any, attr: str, val: PyTree) -> None:\n     return t.process_setattr(obj, attr, val)\n \n def jax_appendattr(obj: Any, attr: str, val: Array) -> None:\n-  return jax_extendattr(obj, attr, jax.numpy.expand_dims(val, 0))\n+  import jax.numpy as jnp  # pytype: disable=import-error\n+  return jax_extendattr(obj, attr, jnp.expand_dims(val, 0))\n \n def jax_extendattr(obj: Any, attr: str, val: Array) -> None:\n   with core.take_current_trace() as t:\n@@ -68,12 +68,13 @@ def _setattr_impl(_, obj, attr, val):\n core.EvalTrace.process_setattr = _setattr_impl\n \n def _extendattr_impl(_, obj, attr, val):\n+  import jax.numpy as jnp  # pytype: disable=import-error\n   cur = getattr(obj, attr, dne_sentinel)\n   if cur is dne_sentinel:\n     new = val\n   else:\n     _check_append_type_agreement(obj, attr, core.typeof(cur), core.typeof(val))\n-    new = jax.numpy.concatenate([cur, val])\n+    new = jnp.concatenate([cur, val])\n   setattr(obj, attr, new)\n core.EvalTrace.process_extendattr = _extendattr_impl\n \n@@ -122,6 +123,7 @@ def _setattr_staging(trace, obj, attr, val):\n pe.DynamicJaxprTrace.process_setattr = _setattr_staging\n \n def _extendattr_staging(trace, obj, attr, val):\n+  import jax.numpy as jnp  # pytype: disable=import-error\n   frame = trace.frame\n \n   if (obj, attr, ReadWrite) in frame.attrs_tracked:\n@@ -138,7 +140,7 @@ def _extendattr_staging(trace, obj, attr, val):\n   else:\n     assert init_val is not dne_sentinel\n     with core.set_current_trace(trace):\n-      tracer = jax.numpy.concatenate([init_val, val])\n+      tracer = jnp.concatenate([init_val, val])\n   setattr(obj, attr, tracer)\n pe.DynamicJaxprTrace.process_extendattr = _extendattr_staging\n \n\n```",
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Make the Blackwell matmul kernel persistent\n\nThis helps with performance a bit (we only allocate and deallocate TMEM once in each\nSM), and opens up the opportunity for better overlapping of the epilogue.\n\nPiperOrigin-RevId: 764168230",
        "output": "```diff\nCommit: 6004c7b6daf68650bec2595ab86bc1efc2eb8845\nDate: 2025-05-28T09:34:54Z\nURL: https://github.com/jax-ml/jax/commit/6004c7b6daf68650bec2595ab86bc1efc2eb8845\nFiles changed: 1\nAdditions: +48, Deletions: -20\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex 929c7c498986..8bf8ca557496 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -15,6 +15,7 @@\n \"\"\"Matmul kernel for Blackwell.\"\"\"\n \n import itertools\n+import math\n \n import jax\n from jax._src.interpreters import mlir\n@@ -81,6 +82,9 @@ def build_kernel(\n   if (m // block_tile_m) % grid_tile_m:\n     raise ValueError(f\"{m=} // {tile_m=} must be divisible by {grid_tile_m=}\")\n \n+  # We intend this to be iterated in column-major order.\n+  logical_grid = (grid_tile_m, n // tile_n, m // (block_tile_m * grid_tile_m))\n+\n   def kernel(ctx, a, b, d, smem):\n     ((a_smem, b_smem), d_smem), barriers, mma_done_barrier, acc = smem\n     (ab_full_barriers, ab_empty_barriers) = barriers\n@@ -94,17 +98,25 @@ def kernel(ctx, a, b, d, smem):\n         arith.CmpIPredicate.eq, ctx.cluster_idx(gpu.Dimension.x), c(0, index)\n     )\n \n-    # This function executes the kernel for a single output tile.\n-    def compute_output(block_m_start, n_start):\n-      \"\"\"Compute and store a single output tile.\"\"\"\n+    def compute_output(block_m_start, n_start, call_counter):\n+      \"\"\"Compute and store a single output tile.\n+\n+      call_counter should be 0 the first time this function is called and\n+      incremented by 1 before each subsequent call.\n+      \"\"\"\n       # All blocks in the cluster share the same m_start -- align it!\n       m_start = arith.muli(arith.divui(block_m_start, c(tile_m, index)), c(tile_m, index))\n       with mgpu.when(is_leader_of(TMA_WARP)):\n         @mgpu.fori(c(k_loop_iter, index), None)\n         def _tma_body(ki, _):\n           slot = arith.remui(ki, c(max_concurrent_steps, index))\n-          # TODO(apaszke): Use a predicate instead of a conditional.\n-          with mgpu.when(arith.cmpi(arith.CmpIPredicate.uge, ki, c(max_concurrent_steps, index))):\n+          isnt_warmup = arith.cmpi(\n+              arith.CmpIPredicate.uge, ki, c(max_concurrent_steps, index)\n+          )\n+          isnt_first_call = arith.cmpi(\n+              arith.CmpIPredicate.ne, call_counter, c(0, index)\n+          )\n+          with mgpu.when(arith.ori(isnt_first_call, isnt_warmup)):\n             ab_empty_barriers[slot].wait()\n           full_barrier = ab_full_barriers[slot]\n           with mgpu.when(is_leader_block):\n@@ -150,15 +162,12 @@ def _mma_body(ki, accumulate):\n               collective=collective,\n           )\n           accumulate = arith.constant(i1, 1)\n+          tcgen05.commit_arrive(ab_empty_barriers[slot], collective=collective, ctx=ctx)\n           is_last_iter = arith.cmpi(\n               arith.CmpIPredicate.eq, ki, c(k_loop_iter - 1, index)\n           )\n-          barrier_ptr = arith.select(\n-              is_last_iter,\n-              mma_done_barrier.get_ptr(),\n-              ab_empty_barriers[slot].get_ptr(),\n-          )\n-          tcgen05.commit_arrive(barrier_ptr, collective=collective, ctx=ctx)\n+          with mgpu.when(is_last_iter):\n+            tcgen05.commit_arrive(mma_done_barrier, collective=collective, ctx=ctx)\n           return accumulate\n \n       gpu.barrier()\n@@ -176,15 +185,33 @@ def _mma_body(ki, accumulate):\n       )\n       ctx.await_async_copy(0)\n \n-    m_idx = arith.addi(\n-        gpu.block_id(gpu.Dimension.x),\n-        arith.muli(gpu.block_id(gpu.Dimension.z), c(grid_tile_m, index)),\n+    # We statically assign the tiles to SMs.\n+    logical_grid_size = math.prod(logical_grid)\n+    sm_id = gpu.block_id(gpu.Dimension.x)\n+    extra_step = arith.cmpi(\n+        arith.CmpIPredicate.slt, sm_id, c(logical_grid_size % num_sms, index)\n+    )  # Some SMs do an extra step when grid size isn't divisible by SM count.\n+    mn_steps = arith.addi(\n+        mgpu.c(logical_grid_size // num_sms, index),\n+        arith.index_castui(index, extra_step),\n     )\n-    n_idx = gpu.block_id(gpu.Dimension.y)\n-    block_m_start = arith.muli(m_idx, c(block_tile_m, index))\n-    n_start = arith.muli(n_idx, c(tile_n,index))\n-    # This is not a persistent kernel, so we only process one tile.\n-    compute_output(block_m_start, n_start)\n+\n+    @mgpu.fori(mn_steps, None)\n+    def _mn_loop(local_mn_step, _):\n+      global_mn_step = arith.addi(\n+          sm_id, arith.muli(local_mn_step, mgpu.c(num_sms, index))\n+      )\n+      logical_idxs = []\n+      for dim_size in logical_grid:\n+        logical_idxs.append(arith.remui(global_mn_step, mgpu.c(dim_size, index)))\n+        global_mn_step = arith.divui(global_mn_step, mgpu.c(dim_size, index))\n+      lx, ly, lz = logical_idxs\n+      m_idx = arith.addi(lx, arith.muli(lz, c(grid_tile_m, index)))\n+      n_idx = ly\n+\n+      block_m_start = arith.muli(m_idx, c(block_tile_m, index))\n+      n_start = arith.muli(n_idx, c(tile_n,index))\n+      compute_output(block_m_start, n_start, local_mn_step)\n \n   compute_buffers = (\n     jax.ShapeDtypeStruct(\n@@ -204,9 +231,10 @@ def _mma_body(ki, accumulate):\n       mgpu.Barrier(arrival_count=1),\n       mgpu.TMEM((128, tile_n), jnp.float32, collective=collective),\n   )\n+  num_sms = 148\n   return mgpu.as_gpu_kernel(\n       kernel,\n-      (grid_tile_m, n // tile_n, m // (block_tile_m * grid_tile_m)),\n+      (num_sms, 1, 1),  # This is a persistent kernel.\n       (128, 1, 1),\n       (\n           jax.ShapeDtypeStruct((m, k), dtype),\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] Fix `broadcast_in_dim` fuser eval rule.\n\nPiperOrigin-RevId: 764019664",
        "output": "```diff\nCommit: 69c431759123b4db3de9578837d4dbe0b58db74b\nDate: 2025-05-28T00:53:15Z\nURL: https://github.com/jax-ml/jax/commit/69c431759123b4db3de9578837d4dbe0b58db74b\nFiles changed: 2\nAdditions: +77, Deletions: -34\ndiff --git a/jax/_src/pallas/fuser/block_spec.py b/jax/_src/pallas/fuser/block_spec.py\nindex 3d4df549949c..3e9ff497bf1e 100644\n--- a/jax/_src/pallas/fuser/block_spec.py\n+++ b/jax/_src/pallas/fuser/block_spec.py\n@@ -1364,14 +1364,15 @@ def _broadcast_in_dim_usage_rule(ctx, used_out: set[Usage], **params):\n def _broadcast_in_dim_eval_rule(\n     eval_ctx: KernelEvalContext, x, broadcast_dimensions, **params\n ):\n-  if not eval_ctx.avals_in[0].shape:  # pytype: disable=attribute-error\n-    # Scalar -> Array broadcast\n-    block_spec = eval_ctx.out_block_specs[0]\n-    shape = tuple(\n-        _block_size(s) for s in block_spec.block_shape if s is not None\n-    )\n-    return jax.lax.broadcast_in_dim(x, broadcast_dimensions=(), shape=shape)\n-  return x\n+  del params  # Unused.\n+  shape = tuple(map(_block_size, eval_ctx.out_block_specs[0].block_shape))\n+  dims = tuple(\n+      d - sum(s is None for s in shape[:d])\n+      for d in broadcast_dimensions\n+      if shape[d] is not None\n+  )\n+  shape = tuple(s for s in shape if s is not None)\n+  return jax.lax.broadcast_in_dim(x, broadcast_dimensions=dims, shape=shape)\n \n \n @register_pull_block_spec_rule(lax.broadcast_in_dim_p)\n@@ -1385,15 +1386,20 @@ def _broadcast_in_dim_pull_rule(\n ):\n   del shape, sharding\n \n-  if not ctx.avals_in[0].shape:  # pytype: disable=attribute-error\n+  shape = ctx.avals_in[0].shape  # pytype: disable=attribute-error\n+  if not shape:\n     return [pallas_core.no_block_spec]\n \n   def new_index_map(*args):\n     idx = block_spec.index_map(*args)\n-    return tuple(idx[i] for i in broadcast_dimensions)\n+    return tuple(\n+        0 if (d == 1) else idx[i]\n+        for i, d in zip(broadcast_dimensions, shape, strict=True)\n+    )\n \n   new_block_shape = tuple(\n-      block_spec.block_shape[i] for i in broadcast_dimensions\n+      b if ((b := block_spec.block_shape[i]) is None) or (d != 1) else 1\n+      for i, d in zip(broadcast_dimensions, shape, strict=True)\n   )\n   return [pallas_core.BlockSpec(new_block_shape, new_index_map)]\n \ndiff --git a/tests/pallas/fuser_block_spec_test.py b/tests/pallas/fuser_block_spec_test.py\nindex f7e70ec1d708..5c0ef0352b1c 100644\n--- a/tests/pallas/fuser_block_spec_test.py\n+++ b/tests/pallas/fuser_block_spec_test.py\n@@ -653,9 +653,12 @@ def f():\n         kernel_fn((0, 0, 3, 0), scalar_prefetch_values, ()), x\n     )\n \n-  def test_broadcast_array(self):\n+  @parameterized.parameters(\n+      (False, False), (False, True), (True, False), (True, True)\n+  )\n+  def test_broadcast_array(self, bcast0, bcast1):\n \n-    x = jnp.ones((512, 512))\n+    x = jnp.ones((1 if bcast0 else 512, 1 if bcast1 else 512))\n \n     def f():\n       return jax.lax.broadcast_in_dim(x, (2, 2, 512, 512), (2, 3))\n@@ -664,9 +667,8 @@ def f():\n     self.assertLen(new_values, 1)\n     self.assertEmpty(scalar_prefetch_values)\n \n-    block_spec = pl.BlockSpec(\n-        (None, 1, 128, 128), lambda i, j, k, l: (i, j, k, l)\n-    )\n+    block_shape = (None, 1, 128, 128)\n+    block_spec = pl.BlockSpec(block_shape, lambda i, j, k, l: (i, j, k, l))\n     kernel_fn, (value_block_specs,), _ = block_spec_lib.pull_block_spec(\n         f2,\n         block_spec,\n@@ -674,27 +676,62 @@ def f():\n         scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n     )(new_values)\n     self.assertLen(value_block_specs, 1)\n-    x_block_spec = value_block_specs[0]\n-    self.assertEqual(x_block_spec.index_map(0, 0, 1, 2), (1, 2))\n-    self.assertEqual(x_block_spec.index_map(1, 2, 3, 3), (3, 3))\n-\n-    x = jnp.full((128, 128), fill_value=1.2345, dtype=jnp.float32)\n-    np.testing.assert_array_equal(\n-        kernel_fn((0, 0, 0, 0), scalar_prefetch_values, (x,)), x\n-    )\n-    np.testing.assert_array_equal(\n-        kernel_fn((1, 1, 0, 0), scalar_prefetch_values, (x,)), x\n-    )\n-    np.testing.assert_array_equal(\n-        kernel_fn((0, 0, 0, 1), scalar_prefetch_values, (x,)), x\n-    )\n-    np.testing.assert_array_equal(\n-        kernel_fn((0, 0, 1, 0), scalar_prefetch_values, (x,)), x\n+    x_index_map = value_block_specs[0].index_map\n+    self.assertEqual(\n+        x_index_map(0, 0, 1, 2), (0 if bcast0 else 1, 0 if bcast1 else 2)\n     )\n-    np.testing.assert_array_equal(\n-        kernel_fn((0, 0, 3, 0), scalar_prefetch_values, (x,)), x\n+    self.assertEqual(\n+        x_index_map(1, 2, 3, 3), (0 if bcast0 else 3, 0 if bcast1 else 3)\n     )\n \n+    block_shape = (1 if bcast0 else 128, 1 if bcast1 else 128)\n+    self.assertEqual(block_shape, value_block_specs[0].block_shape)\n+    x = jnp.full(block_shape, fill_value=1.2345, dtype=jnp.float32)\n+    y = jax.lax.broadcast_in_dim(x, (1, 128, 128), (1, 2))\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 0, 0), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((1, 1, 0, 0), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 0, 1), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 1, 0), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 3, 0), (), (x,)), y)\n+\n+  @parameterized.parameters(0, 1, 2, 3)\n+  def test_broadcast_1d_array(self, bcast_dim):\n+    full_shape = (2, 2, 512, 512)\n+    x = jnp.ones((full_shape[bcast_dim],))\n+\n+    def f():\n+      return jax.lax.broadcast_in_dim(x, full_shape, (bcast_dim,))\n+\n+    f2, new_values, scalar_prefetch_values = block_spec_lib.get_fusion_values(f)\n+    self.assertLen(new_values, 1)\n+    self.assertEmpty(scalar_prefetch_values)\n+\n+    block_shape = (None, 1, 128, 128)\n+    block_spec = pl.BlockSpec(block_shape, lambda i, j, k, l: (i, j, k, l))\n+    kernel_fn, (value_block_specs,), _ = block_spec_lib.pull_block_spec(\n+        f2,\n+        block_spec,\n+        grid=(2, 2, 4, 4),\n+        scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+    )(new_values)\n+    self.assertLen(value_block_specs, 1)\n+    x_index_map = value_block_specs[0].index_map\n+    self.assertEqual(x_index_map(0, 0, 1, 2), ((0, 0, 1, 2)[bcast_dim],))\n+    self.assertEqual(x_index_map(1, 2, 3, 3), ((1, 2, 3, 3)[bcast_dim],))\n+\n+    if block_shape[bcast_dim] is None:\n+      x = jnp.ones(())\n+      y = jax.lax.broadcast_in_dim(x, (1, 128, 128), ())\n+    else:\n+      x = jnp.arange(block_shape[bcast_dim] or 1, dtype=jnp.float32)\n+      y = jax.lax.broadcast_in_dim(x, (1, 128, 128), (bcast_dim - 1,))\n+\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 0, 0), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((1, 1, 0, 0), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 0, 1), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 1, 0), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 3, 0), (), (x,)), y)\n+\n   def test_element_indexing(self):\n \n     x = np.zeros((512, 512), dtype=np.float32)\n\n```",
        "from_id": [
            "chr1sj0nes",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reshape ragged_all_to_all to correct shape before concatenating\n\nPreviously the result of vmapped RA2A was concatenating a flattened result.\n\nPiperOrigin-RevId: 763958632",
        "output": "```diff\nCommit: 669f08a8276bda81fb851a2242158802fcbc5f47\nDate: 2025-05-27T21:54:40Z\nURL: https://github.com/jax-ml/jax/commit/669f08a8276bda81fb851a2242158802fcbc5f47\nFiles changed: 2\nAdditions: +64, Deletions: -4\ndiff --git a/jax/_src/lax/parallel.py b/jax/_src/lax/parallel.py\nindex c5f8d3988144..a5bb7222143d 100644\n--- a/jax/_src/lax/parallel.py\n+++ b/jax/_src/lax/parallel.py\n@@ -1448,13 +1448,15 @@ def _ragged_all_to_all_batched_collective(axis_data, vals_in, dims_in,\n   sliced_results = []\n   for i in range(operand.shape[operand_dim]):\n     sliced_operand = slicing.slice_in_dim(operand, start_index=i, limit_index=i+1, axis=operand_dim).flatten()\n-    sliced_output = slicing.slice_in_dim(output, start_index=i, limit_index=i+1, axis=output_dim).flatten()\n+    sliced_output = slicing.slice_in_dim(output, start_index=i, limit_index=i+1, axis=output_dim)\n+    sliced_output_shape = sliced_output.shape\n+    sliced_output = sliced_output.flatten()\n     sliced_input_offsets = slicing.slice_in_dim(input_offsets, start_index=i, limit_index=i+1, axis=input_offsets_dim).flatten()\n     sliced_send_sizes = slicing.slice_in_dim(send_sizes, start_index=i, limit_index=i+1, axis=send_sizes_dim).flatten()\n     sliced_output_offsets = slicing.slice_in_dim(output_offsets, start_index=i, limit_index=i+1, axis=output_offsets_dim).flatten()\n     sliced_recv_sizes = slicing.slice_in_dim(recv_sizes, start_index=i, limit_index=i+1, axis=recv_sizes_dim).flatten()\n     sliced_result = ragged_all_to_all(sliced_operand, sliced_output, sliced_input_offsets, sliced_send_sizes, sliced_output_offsets, sliced_recv_sizes, axis_name=axis_name, axis_index_groups=axis_index_groups)\n-    sliced_result = lax.expand_dims(sliced_result, dimensions=(output_dim,))\n+    sliced_result = lax.expand_dims(sliced_result.reshape(sliced_output_shape), dimensions=(output_dim,))\n     sliced_results.append(sliced_result)\n \n   concat_result = lax.concatenate(sliced_results, dimension=output_dim)\ndiff --git a/tests/ragged_collective_test.py b/tests/ragged_collective_test.py\nindex 1734f67ff063..8b94b862419c 100644\n--- a/tests/ragged_collective_test.py\n+++ b/tests/ragged_collective_test.py\n@@ -382,6 +382,66 @@ def fwd(\n         c, jnp.array([[0, 0, 1, 0], [0, 2, 3, 4]], dtype=jnp.int32)\n     )\n \n+  def test_ragged_all_to_all_vmap_multi_dim_operand(self):\n+    device_type = jax.devices()[0].platform\n+    if device_type == 'tpu' and jtu.get_tpu_version() < 4:\n+      raise unittest.SkipTest(\n+          'UNSUPPORTED: HLO opcode `ragged-all-to-all` is not supported by TPU'\n+          f' v{jtu.get_tpu_version()}'\n+      )\n+\n+    axis_name = 'x'\n+    mesh_axes = dict(x=2)\n+    mesh = jtu.create_mesh(tuple(mesh_axes.values()), tuple(mesh_axes.keys()))\n+    data_sharding = P(axis_name, None, None)\n+    operand_data = jnp.zeros((2, 2, 3), dtype=jnp.int32)\n+    output_data = jnp.zeros((2, 2, 4), dtype=jnp.int32)\n+    input_offsets_data = jnp.zeros((2, 2, 2), dtype=jnp.int32)\n+    send_sizes_data = jnp.zeros((2, 2, 2), dtype=jnp.int32)\n+    output_offsets_data = jnp.zeros((2, 2, 2), dtype=jnp.int32)\n+    recv_sizes_data = jnp.zeros((2, 2, 2), dtype=jnp.int32)\n+\n+    operand = jax.device_put(operand_data, jax.sharding.NamedSharding(mesh, data_sharding))\n+    output = jax.device_put(output_data, jax.sharding.NamedSharding(mesh, data_sharding))\n+    input_offsets = jax.device_put(input_offsets_data, jax.sharding.NamedSharding(mesh, data_sharding))\n+    send_sizes = jax.device_put(send_sizes_data, jax.sharding.NamedSharding(mesh, data_sharding))\n+    output_offsets = jax.device_put(output_offsets_data, jax.sharding.NamedSharding(mesh, data_sharding))\n+    recv_sizes = jax.device_put(recv_sizes_data, jax.sharding.NamedSharding(mesh, data_sharding))\n+\n+    @partial(\n+        shard_map,\n+        mesh=mesh,\n+        in_specs=(\n+            P(axis_name, None),\n+            P(axis_name, None),\n+            P(axis_name, None),\n+            P(axis_name, None),\n+            P(axis_name, None),\n+            P(axis_name, None),\n+        ),\n+        out_specs=P(axis_name),\n+        check_vma=False,\n+    )\n+    def fwd(\n+        operand, output, input_offsets, send_sizes, output_offsets, recv_sizes\n+    ):\n+      return lax.ragged_all_to_all(\n+          operand=operand.reshape(operand.shape[1:]),\n+          output=output.reshape(output.shape[1:]),\n+          input_offsets=input_offsets.reshape(input_offsets.shape[1:]),\n+          send_sizes=send_sizes.reshape(send_sizes.shape[1:]),\n+          output_offsets=output_offsets.reshape(output_offsets.shape[1:]),\n+          recv_sizes=recv_sizes.reshape(recv_sizes.shape[1:]),\n+          axis_name=axis_name,\n+      )\n+\n+    res = vmap(\n+        fwd, in_axes=0, out_axes=0, axis_name='x'\n+    )(\n+        operand, output, input_offsets, send_sizes, output_offsets, recv_sizes\n+    )\n+    self.assertEqual(res.shape, (2, 2, 4))\n+\n   @parameterized.named_parameters(\n     dict(\n         testcase_name='_batch_0_data_shard_axis_0_input_0',\n@@ -510,8 +570,6 @@ def fwd(\n         fwd, in_axes=vmap_batch_axis, out_axes=0, axis_name=vmap_axis_name\n     )(\n         operand, output, input_offsets, send_sizes, output_offsets, recv_sizes\n-    ).reshape(\n-        (2, 2, 4)\n     )\n     expected_res = jnp.array([[[1, 4, 0, 0], [2, 3, 5, 0]],\n                               [[1, 4, 0, 0], [2, 3, 5, 0]]], dtype=jnp.int32)\n\n```",
        "from_id": [
            "ghpvnist",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29043 from hawkinsp:locks\n\nPiperOrigin-RevId: 763950695",
        "output": "```diff\nCommit: 0caeb982a4dec6b98096a21cdf218dece9f86bd6\nDate: 2025-05-27T21:33:54Z\nURL: https://github.com/jax-ml/jax/commit/0caeb982a4dec6b98096a21cdf218dece9f86bd6\nFiles changed: 6\nAdditions: +183, Deletions: -173\ndiff --git a/build/requirements.in b/build/requirements.in\nindex c5ce2ea279bd..c1be7a250bff 100644\n--- a/build/requirements.in\n+++ b/build/requirements.in\n@@ -16,11 +16,11 @@ wheel\n # JAX's own libraries. We include these in the requirements so you can\n # bazel test without building jaxlib and without manually updating the\n # the requirements files.\n-jaxlib\n+jaxlib==0.6.1\n \n # The with-cuda extra also includes NVIDIA's pip packages.\n-jax-cuda12-plugin[with-cuda] ; sys_platform == \"linux\"\n-jax-cuda12-pjrt ; sys_platform == \"linux\"\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\"\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\"\n \n # TPU dependencies\n libtpu ; sys_platform == \"linux\" and platform_machine == \"x86_64\"\ndiff --git a/build/requirements_lock_3_10.txt b/build/requirements_lock_3_10.txt\nindex a4c6b1bf2b77..832c801ced63 100644\n--- a/build/requirements_lock_3_10.txt\n+++ b/build/requirements_lock_3_10.txt\n@@ -160,43 +160,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.5 \\\n     --hash=sha256:00bd361b903dc4bbf4eb165f24d1acbee754fce22ded24c3d56eec268658a5cf \\\n@@ -494,7 +494,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549\ndiff --git a/build/requirements_lock_3_11.txt b/build/requirements_lock_3_11.txt\nindex 0633e733414b..de3c35ed3c02 100644\n--- a/build/requirements_lock_3_11.txt\n+++ b/build/requirements_lock_3_11.txt\n@@ -154,43 +154,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.5 \\\n     --hash=sha256:00bd361b903dc4bbf4eb165f24d1acbee754fce22ded24c3d56eec268658a5cf \\\n@@ -489,7 +489,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549\ndiff --git a/build/requirements_lock_3_12.txt b/build/requirements_lock_3_12.txt\nindex 1ab77a6ec36e..04c6990da696 100644\n--- a/build/requirements_lock_3_12.txt\n+++ b/build/requirements_lock_3_12.txt\n@@ -154,43 +154,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.5 \\\n     --hash=sha256:00bd361b903dc4bbf4eb165f24d1acbee754fce22ded24c3d56eec268658a5cf \\\n@@ -489,7 +489,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549\ndiff --git a/build/requirements_lock_3_13.txt b/build/requirements_lock_3_13.txt\nindex c20068b732e6..965cb3bc9672 100644\n--- a/build/requirements_lock_3_13.txt\n+++ b/build/requirements_lock_3_13.txt\n@@ -181,43 +181,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.7 \\\n     --hash=sha256:073a36c8273647592ea332e816e75ef8da5c303236ec0167196793eb1e34657a \\\n@@ -544,7 +544,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.4.0 \\\n     --hash=sha256:69bb92469f86a1565195ece4ac0323943e83477171b91d24c35afe028a90d7cd \\\n     --hash=sha256:96ca72f1b886d148241348783498194c577fa30a8faac108586b14f1ba4473ac\ndiff --git a/build/requirements_lock_3_13_ft.txt b/build/requirements_lock_3_13_ft.txt\nindex 3795343df0cb..e7d111c3b3e9 100644\n--- a/build/requirements_lock_3_13_ft.txt\n+++ b/build/requirements_lock_3_13_ft.txt\n@@ -172,43 +172,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.8 \\\n     --hash=sha256:01c3d31902c7db5fb6182832713d3b4122ad9317c2c5877d0539227d96bb2e50 \\\n@@ -495,7 +495,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.4.0 \\\n     --hash=sha256:69bb92469f86a1565195ece4ac0323943e83477171b91d24c35afe028a90d7cd \\\n     --hash=sha256:96ca72f1b886d148241348783498194c577fa30a8faac108586b14f1ba4473ac\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update lock files for jaxlib 0.6.1",
        "output": "```diff\nCommit: c09b1bb763d846a694f919e5a5adda9575ce66d6\nDate: 2025-05-27T20:44:50Z\nURL: https://github.com/jax-ml/jax/commit/c09b1bb763d846a694f919e5a5adda9575ce66d6\nFiles changed: 6\nAdditions: +183, Deletions: -173\ndiff --git a/build/requirements.in b/build/requirements.in\nindex c5ce2ea279bd..c1be7a250bff 100644\n--- a/build/requirements.in\n+++ b/build/requirements.in\n@@ -16,11 +16,11 @@ wheel\n # JAX's own libraries. We include these in the requirements so you can\n # bazel test without building jaxlib and without manually updating the\n # the requirements files.\n-jaxlib\n+jaxlib==0.6.1\n \n # The with-cuda extra also includes NVIDIA's pip packages.\n-jax-cuda12-plugin[with-cuda] ; sys_platform == \"linux\"\n-jax-cuda12-pjrt ; sys_platform == \"linux\"\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\"\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\"\n \n # TPU dependencies\n libtpu ; sys_platform == \"linux\" and platform_machine == \"x86_64\"\ndiff --git a/build/requirements_lock_3_10.txt b/build/requirements_lock_3_10.txt\nindex a4c6b1bf2b77..832c801ced63 100644\n--- a/build/requirements_lock_3_10.txt\n+++ b/build/requirements_lock_3_10.txt\n@@ -160,43 +160,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.5 \\\n     --hash=sha256:00bd361b903dc4bbf4eb165f24d1acbee754fce22ded24c3d56eec268658a5cf \\\n@@ -494,7 +494,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549\ndiff --git a/build/requirements_lock_3_11.txt b/build/requirements_lock_3_11.txt\nindex 0633e733414b..de3c35ed3c02 100644\n--- a/build/requirements_lock_3_11.txt\n+++ b/build/requirements_lock_3_11.txt\n@@ -154,43 +154,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.5 \\\n     --hash=sha256:00bd361b903dc4bbf4eb165f24d1acbee754fce22ded24c3d56eec268658a5cf \\\n@@ -489,7 +489,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549\ndiff --git a/build/requirements_lock_3_12.txt b/build/requirements_lock_3_12.txt\nindex 1ab77a6ec36e..04c6990da696 100644\n--- a/build/requirements_lock_3_12.txt\n+++ b/build/requirements_lock_3_12.txt\n@@ -154,43 +154,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.5 \\\n     --hash=sha256:00bd361b903dc4bbf4eb165f24d1acbee754fce22ded24c3d56eec268658a5cf \\\n@@ -489,7 +489,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549\ndiff --git a/build/requirements_lock_3_13.txt b/build/requirements_lock_3_13.txt\nindex c20068b732e6..965cb3bc9672 100644\n--- a/build/requirements_lock_3_13.txt\n+++ b/build/requirements_lock_3_13.txt\n@@ -181,43 +181,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.7 \\\n     --hash=sha256:073a36c8273647592ea332e816e75ef8da5c303236ec0167196793eb1e34657a \\\n@@ -544,7 +544,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.4.0 \\\n     --hash=sha256:69bb92469f86a1565195ece4ac0323943e83477171b91d24c35afe028a90d7cd \\\n     --hash=sha256:96ca72f1b886d148241348783498194c577fa30a8faac108586b14f1ba4473ac\ndiff --git a/build/requirements_lock_3_13_ft.txt b/build/requirements_lock_3_13_ft.txt\nindex 3795343df0cb..e7d111c3b3e9 100644\n--- a/build/requirements_lock_3_13_ft.txt\n+++ b/build/requirements_lock_3_13_ft.txt\n@@ -172,43 +172,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.8 \\\n     --hash=sha256:01c3d31902c7db5fb6182832713d3b4122ad9317c2c5877d0539227d96bb2e50 \\\n@@ -495,7 +495,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.4.0 \\\n     --hash=sha256:69bb92469f86a1565195ece4ac0323943e83477171b91d24c35afe028a90d7cd \\\n     --hash=sha256:96ca72f1b886d148241348783498194c577fa30a8faac108586b14f1ba4473ac\n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Merge pull request #28955 from jax-ml:prevent-partial-eval-dce-effects\n\nPiperOrigin-RevId: 763886950",
        "output": "```diff\nCommit: 0f4da0c921964fd977b7c2bf96e386e84a7613dd\nDate: 2025-05-27T18:52:32Z\nURL: https://github.com/jax-ml/jax/commit/0f4da0c921964fd977b7c2bf96e386e84a7613dd\nFiles changed: 12\nAdditions: +52, Deletions: -36\ndiff --git a/jax/_src/ad_checkpoint.py b/jax/_src/ad_checkpoint.py\nindex 2d743bf06c6b..2a056d5c94f0 100644\n--- a/jax/_src/ad_checkpoint.py\n+++ b/jax/_src/ad_checkpoint.py\n@@ -578,7 +578,7 @@ def remat_partial_eval(trace: pe.JaxprTrace, *tracers: core.Tracer,\n   out_jaxpr_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(x.aval), None)\n                        for x in jaxpr_unknown.outvars]\n   new_params = dict(params, jaxpr=jaxpr_unknown, differentiated=True)\n-  recipe = pe.new_eqn_recipe(in_jaxpr_tracers, out_jaxpr_tracers, remat_p,\n+  recipe = pe.new_eqn_recipe(trace, in_jaxpr_tracers, out_jaxpr_tracers, remat_p,\n                              new_params, jaxpr_unknown.effects,\n                              source_info_util.current())\n \ndiff --git a/jax/_src/debugging.py b/jax/_src/debugging.py\nindex e931a6edb9b3..3490de5118e1 100644\n--- a/jax/_src/debugging.py\n+++ b/jax/_src/debugging.py\n@@ -127,7 +127,7 @@ def debug_callback_jvp_rule(primals, tangents, **params):\n ad.primitive_jvps[debug_callback_p] = debug_callback_jvp_rule\n \n def debug_callback_transpose_rule(*flat_args, callback: Callable[..., Any],\n-    effect: DebugEffect):\n+                                  effect: DebugEffect, partitioned):\n   del flat_args, callback, effect\n   raise ValueError(\"Transpose doesn't support debugging callbacks.\")\n ad.primitive_transposes[debug_callback_p] = debug_callback_transpose_rule\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 9366b91f8022..7cbdfff01462 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -885,7 +885,7 @@ def make_zero(aval):\n     out_nz_tracers = [trace.to_jaxpr_tracer(r)\n                       for (r, nz) in zip(out_tangents, out_nzs) if nz]\n     in_tracers = [t for t, nz in zip(tangent_args, nonzeros) if nz]\n-    jaxpr, out_consts, _ = pe.tracers_to_jaxpr(in_tracers, out_nz_tracers, jvp.debug_info)\n+    jaxpr, out_consts, _ = pe.tracers_to_jaxpr(in_tracers, out_nz_tracers, [], jvp.debug_info)\n     jaxpr, used_consts, _ = pe.dce_jaxpr_consts(\n         jaxpr, [True] * len(jaxpr.outvars),\n         [False] * len(jaxpr.constvars) + [True] * len(jaxpr.invars))\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex f77db5443a86..6ea16ec8e8ba 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -16,6 +16,7 @@\n from collections import namedtuple\n from collections.abc import Callable, Sequence, Hashable\n import contextlib\n+from dataclasses import dataclass\n from functools import partial\n import itertools as it\n import operator as op\n@@ -42,7 +43,7 @@\n                            mapped_aval, unmapped_aval, DBIdx, InDBIdx, OutDBIdx,\n                            InputType, OutputType, get_referent, JaxprEqnContext)\n from jax._src.source_info_util import SourceInfo\n-from jax._src.state.types import AbstractRef, ReadEffect\n+from jax._src.state.types import AbstractRef, ReadEffect, RefEffect\n from jax._src.tree_util import (PyTreeDef, treedef_tuple, tree_flatten,\n                                 tree_structure, register_static)\n from jax._src.util import (unzip2, safe_zip, safe_map, toposort, split_list,\n@@ -147,6 +148,10 @@ def get_aval(self) -> AbstractValue:\n     else:\n       return self[0]\n \n+@dataclass(frozen=True)\n+class EffectHandle:\n+  parents : list[Tracer]\n+  recipe : JaxprEqnRecipe\n \n class JaxprTrace(Trace['JaxprTracer']):\n \n@@ -156,6 +161,8 @@ def __init__(self, parent_trace:Trace, name_stack: source_info_util.NameStack, t\n     self.tag = tag\n     self.parent_trace = parent_trace\n     self.requires_low = False\n+    self.effect_handles : list[EffectHandle] = []\n+    self.counter = it.count()\n \n   def to_jaxpr_tracer(self, x):\n     if isinstance(x, JaxprTracer) and x._trace.tag is self.tag:\n@@ -239,14 +246,19 @@ def default_process_primitive(self, primitive, tracers, params):\n     if primitive.multiple_results:\n       out_tracers = [JaxprTracer(self, PartialVal.unknown(aval), None)\n                      for aval in out_aval]\n-      eqn = new_eqn_recipe(tracers, out_tracers, primitive, params, effects,\n+      eqn = new_eqn_recipe(self, tracers, out_tracers, primitive, params, effects,\n                            source)\n+      if any(isinstance(e, RefEffect) for e in effects):\n+        self.effect_handles.append(EffectHandle(tracers, eqn))\n       for t in out_tracers: t.recipe = eqn\n       return out_tracers\n     else:\n       out_tracer = JaxprTracer(self, PartialVal.unknown(out_aval), None)\n-      out_tracer.recipe = new_eqn_recipe(tracers, [out_tracer], primitive,\n-                                         params, effects, source)\n+      eqn = new_eqn_recipe(self, tracers, [out_tracer], primitive,\n+                           params, effects, source)\n+      if any(isinstance(e, RefEffect) for e in effects):\n+        self.effect_handles.append(EffectHandle(tracers, eqn))\n+      out_tracer.recipe = eqn\n       return out_tracer\n \n   def process_call(self, primitive, f: lu.WrappedFun, tracers, params):\n@@ -321,7 +333,7 @@ def process_call(self, primitive, f: lu.WrappedFun, tracers, params):\n                      for a in out_type]\n     name_stack = self._current_truncated_name_stack()\n     source = source_info_util.current().replace(name_stack=name_stack)\n-    eqn = new_eqn_recipe((*res_tracers, *env_tracers, *unknown_arg_tracers),\n+    eqn = new_eqn_recipe(self, (*res_tracers, *env_tracers, *unknown_arg_tracers),\n                          out_tracers, primitive, staged_params, jaxpr.effects,\n                          source)\n     for t in out_tracers: t.recipe = eqn\n@@ -390,7 +402,7 @@ def const_out_axes_thunk():\n                    for a in out_avals]\n     effs = core.filter_named_axis_effects(jaxpr.effects, {params['axis_name']})\n     src_info = source_info_util.current()\n-    eqn = new_eqn_recipe((*const_tracers, *env_tracers, *unknown_arg_tracers),\n+    eqn = new_eqn_recipe(self, (*const_tracers, *env_tracers, *unknown_arg_tracers),\n                          out_tracers, primitive, staged_params, effs, src_info)\n     for t in out_tracers: t.recipe = eqn\n \n@@ -425,7 +437,7 @@ def process_custom_transpose(self, prim, call, tracers, **params):\n                      for aval in params['out_types']]\n       in_tracers = map(self.instantiate_const, tracers)\n       new_params = dict(params, call=call)\n-      eqn = new_eqn_recipe(in_tracers, out_tracers, prim, new_params,\n+      eqn = new_eqn_recipe(self, in_tracers, out_tracers, prim, new_params,\n           core.no_effects, source_info_util.current())\n       for t in out_tracers: t.recipe = eqn\n       return out_tracers\n@@ -470,7 +482,7 @@ def fwd_jaxpr_thunk(*zeros):\n         out_trees=out_trees,\n         symbolic_zeros=symbolic_zeros\n     )\n-    eqn = new_eqn_recipe((*res_tracers, *env_tracers, *tracers),\n+    eqn = new_eqn_recipe(self, (*res_tracers, *env_tracers, *tracers),\n                          out_tracers, prim, params, jaxpr.effects, source)\n     for t in out_tracers: t.recipe = eqn\n     return out_tracers\n@@ -657,7 +669,7 @@ def _trace_to_subjaxpr_nounits(f: Callable, trace: JaxprTrace,\n   out_tracers = [trace.instantiate_const(t) if inst else t\n                  for inst, t in zip(instantiate, out_tracers)]\n   out_tracers_ = [t for t in out_tracers if not t.is_known()]\n-  jaxpr, out_consts, env = tracers_to_jaxpr(in_tracers, out_tracers_, debug_info)\n+  jaxpr, out_consts, env = tracers_to_jaxpr(in_tracers, out_tracers_, trace.effect_handles, debug_info)\n   return out_tracers, jaxpr, out_consts, env\n \n # The below variant implements an optimization where residuals which are also\n@@ -739,7 +751,8 @@ class JaxprEqnRecipe(NamedTuple):\n   source_info: source_info_util.SourceInfo\n   ctx: JaxprEqnContext\n \n-def new_eqn_recipe(in_tracers: Sequence[JaxprTracer],\n+def new_eqn_recipe(trace: JaxprTrace,\n+                   in_tracers: Sequence[JaxprTracer],\n                    out_tracers: Sequence[JaxprTracer],\n                    primitive: Primitive,\n                    params: dict[str, Any],\n@@ -762,7 +775,7 @@ def new_eqn_recipe(in_tracers: Sequence[JaxprTracer],\n       config.threefry_partitionable.value,\n       xla_metadata_lib.current_xla_metadata(),\n   )\n-  return JaxprEqnRecipe(object(), tuple(in_tracers), map(ref, out_tracers),\n+  return JaxprEqnRecipe(next(trace.counter), tuple(in_tracers), map(ref, out_tracers),\n                         out_avals, primitive, params, effects, source_info,\n                         ctx)\n \n@@ -780,6 +793,7 @@ def recipe_to_eqn(getvar: Callable[[JaxprTracer], Atom],\n def tracers_to_jaxpr(\n   in_tracers: Sequence[JaxprTracer],\n   out_tracers: Sequence[JaxprTracer],\n+  effect_handles: Sequence[Any],\n   debug_info: core.DebugInfo,\n   ) -> tuple[Jaxpr, tuple[Any, ...], tuple[Any, ...]]:\n   \"\"\"Constructs Jaxpr given tracers for inputs and outputs.\n@@ -821,7 +835,15 @@ def type_substitute(aval: AbstractValue) -> AbstractValue:\n \n   processed_eqn_ids = set()\n   eqns: list[core.JaxprEqn] = []\n-  for t in toposort((*in_tracers, *out_tracers)):\n+\n+  reachable = toposort\n+  tracers = reachable((*in_tracers, *out_tracers, *effect_handles))\n+  def sort_key(t):\n+    r = t.recipe\n+    return r.eqn_id if isinstance(r, JaxprEqnRecipe) else -1\n+  tracers = sorted(tracers, key=sort_key)\n+\n+  for t in tracers:\n     r = t.recipe\n     if isinstance(r, JaxprEqnRecipe):\n       # TODO broadcast_in_dim can create a new tracer, not present in parents\ndiff --git a/jax/_src/lax/control_flow/conditionals.py b/jax/_src/lax/control_flow/conditionals.py\nindex 741636c47e31..4e8368341d9f 100644\n--- a/jax/_src/lax/control_flow/conditionals.py\n+++ b/jax/_src/lax/control_flow/conditionals.py\n@@ -617,7 +617,7 @@ def _cond_partial_eval(trace, *tracers, branches, **params):\n   name_stack = source_info_util.current_name_stack()[len(trace.name_stack):]\n   source = source_info_util.current().replace(name_stack=name_stack)\n   eqn = pe.new_eqn_recipe(\n-      [index_tracer] + res_tracers + ops_tracers, out_tracers, cond_p, params,\n+      trace, [index_tracer] + res_tracers + ops_tracers, out_tracers, cond_p, params,\n       core.join_effects(*(j.effects for j in branches_unknown)), source)\n   for t in out_tracers: t.recipe = eqn\n   return util.merge_lists(out_uks, out_consts, out_tracers)\ndiff --git a/jax/_src/lax/control_flow/for_loop.py b/jax/_src/lax/control_flow/for_loop.py\nindex fc7ebde4cbea..90b81ae367aa 100644\n--- a/jax/_src/lax/control_flow/for_loop.py\n+++ b/jax/_src/lax/control_flow/for_loop.py\n@@ -498,7 +498,7 @@ def _for_partial_eval(trace: pe.JaxprTrace, *tracers: pe.JaxprTracer,\n \n   assert len(unknown_inputs) == len(res_ref_unknown_outputs)\n   assert len(unknown_inputs) == len(jaxpr_unknown.invars) - 1\n-  eqn = pe.new_eqn_recipe(unknown_inputs, res_ref_unknown_outputs,\n+  eqn = pe.new_eqn_recipe(trace, unknown_inputs, res_ref_unknown_outputs,\n                           for_p, dict(jaxpr=jaxpr_unknown, nsteps=nsteps,\n                                       reverse=reverse,\n                                       which_linear=which_linear_unknown,\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 7efe3294fdca..83c31928d7cb 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -920,7 +920,7 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n   name_stack = source_info_util.current_name_stack()[len(trace.name_stack):]\n   source = source_info_util.current().replace(name_stack=name_stack)\n   assert len(out_tracers) == len(jaxpr_unknown.out_avals)\n-  eqn = pe.new_eqn_recipe([*intensive_res, *unknown_inputs, *extensive_res],\n+  eqn = pe.new_eqn_recipe(trace, [*intensive_res, *unknown_inputs, *extensive_res],\n                           out_tracers, scan_p,\n                           dict(reverse=reverse, length=length, unroll=unroll,\n                                jaxpr=jaxpr_unknown, linear=linear_unknown,\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex a9d81c684297..68363d10bc04 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -6550,7 +6550,7 @@ def _broadcast_in_dim_partial_eval(\n   out_aval = core.DShapedArray(tuple(shape_), operand.dtype, operand.weak_type)\n   out_tracer = pe.JaxprTracer(trace, pe.PartialVal.unknown(out_aval), None)\n   eqn = pe.new_eqn_recipe(\n-      [operand_tracer, *dyn_shape_tracers], [out_tracer], broadcast_in_dim_p,\n+      trace, [operand_tracer, *dyn_shape_tracers], [out_tracer], broadcast_in_dim_p,\n       dict(shape=shape, broadcast_dimensions=broadcast_dimensions,\n            sharding=None),\n       core.no_effects, source_info_util.current())\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 0c55f3fe30ab..d5286be8e0c9 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -2324,18 +2324,8 @@ def _pjit_partial_eval(trace: pe.JaxprTrace,\n \n   known_ins = tuple(pv.is_known() for pv in in_pvals)\n   unknown_ins = tuple(not k for k in known_ins)\n-  if any(isinstance(e, (RefEffect, core.InternalMutableArrayEffect))\n-         for e in jaxpr.effects):\n-    known_jaxpr_, unknown_jaxpr_, unknown_outs, _, num_res_val, num_res_ref = \\\n-        pe.partial_eval_jaxpr_stateful(jaxpr.jaxpr, unknown_ins, unknown_ins,\n-                                       False, False, None)\n-    if num_res_ref: raise NotImplementedError\n-    known_jaxpr = pe.ClosedJaxpr(known_jaxpr_, jaxpr.consts)\n-    unknown_jaxpr = pe.ClosedJaxpr(unknown_jaxpr_, jaxpr.consts)\n-    res_avals = unknown_jaxpr.in_avals[:num_res_val]\n-  else:\n-    known_jaxpr, unknown_jaxpr, unknown_outs, res_avals = \\\n-        pe.partial_eval_jaxpr_nounits(jaxpr, unknown_ins, instantiate=False)\n+  known_jaxpr, unknown_jaxpr, unknown_outs, res_avals = \\\n+      pe.partial_eval_jaxpr_nounits(jaxpr, unknown_ins, instantiate=False)\n   unknown_outs = tuple(unknown_outs)  # type: ignore[assignment]\n   known_outs = tuple(not uk for uk in unknown_outs)\n   num_residuals = len(res_avals)\n@@ -2431,7 +2421,7 @@ def keep_where(l, should_keep):\n       pe.JaxprTracer(trace, pe.PartialVal.unknown(aval), None)\n       for aval in unknown_out_avals\n   ]\n-  eqn = pe.new_eqn_recipe((*unknown_tracers_in, *residual_tracers),\n+  eqn = pe.new_eqn_recipe(trace, (*unknown_tracers_in, *residual_tracers),\n                           unknown_tracers_out,\n                           pjit_p,\n                           unknown_params,\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex bc6bda9c16de..1ed831d8577f 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -1392,7 +1392,7 @@ def known_out_specs():\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(a), None)\n                  for a in out_avals]\n   effs = core.filter_named_axis_effects(jaxpr.effects, mesh.axis_names)\n-  eqn = pe.new_eqn_recipe((*const_tracers, *env_tracers, *unk_arg_tracers),\n+  eqn = pe.new_eqn_recipe(trace, (*const_tracers, *env_tracers, *unk_arg_tracers),\n                           out_tracers, shard_map_p, unk_params,\n                           effs, source_info_util.current())\n   for t in out_tracers: t.recipe = eqn\ndiff --git a/jax/_src/state/discharge.py b/jax/_src/state/discharge.py\nindex bc6a20a0a76e..100447f12d18 100644\n--- a/jax/_src/state/discharge.py\n+++ b/jax/_src/state/discharge.py\n@@ -828,7 +828,7 @@ def _run_state_partial_eval(trace: pe.JaxprTrace, *tracers: pe.JaxprTracer,\n                    is_initialized=(True,) * len(jaxpr_unknown.invars))\n   _, eqn_effects = run_state_p.abstract_eval(*[v.aval for v in unknown_inputs],\n                                              **uk_params)\n-  eqn = pe.new_eqn_recipe(unknown_inputs, res_ref_unknown_outputs,\n+  eqn = pe.new_eqn_recipe(trace, unknown_inputs, res_ref_unknown_outputs,\n                           run_state_p, uk_params,\n                           eqn_effects, source)\n   for t in res_ref_unknown_outputs: t.recipe = eqn\ndiff --git a/tests/mutable_array_test.py b/tests/mutable_array_test.py\nindex 865d4f8520f1..0da335e2fac5 100644\n--- a/tests/mutable_array_test.py\n+++ b/tests/mutable_array_test.py\n@@ -192,14 +192,18 @@ def f():\n     x = f()\n     self.assertArraysEqual(x, jnp.zeros(8))\n \n-  def test_grad_mutable_array(self):\n-    @jax.jit\n+  @parameterized.parameters([False, True])\n+  def test_grad_mutable_array(self, jit):\n+\n     def f(x):\n       x_ = core.mutable_array(x)\n       x_[()] = x_[()] + x_[()]\n       y = core.freeze(x_)\n       return y\n \n+    if jit:\n+      f = jax.jit(f)\n+\n     ans = jax.grad(f)(1.)\n     expected = 2.0\n     self.assertAllClose(ans, expected, check_dtypes=False)\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29001 from johannahaffner:test-clip\n\nPiperOrigin-RevId: 763865376",
        "output": "```diff\nCommit: 1d10a488d524b3ad1562f3ef0c0c74030913a5f9\nDate: 2025-05-27T18:00:42Z\nURL: https://github.com/jax-ml/jax/commit/1d10a488d524b3ad1562f3ef0c0c74030913a5f9\nFiles changed: 2\nAdditions: +9, Deletions: -0\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex 0bd287dadd51..ad2b3ad6aa75 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -3410,6 +3410,7 @@ def clip(\n   Returns:\n     An array containing values from ``arr``, with values smaller than ``min`` set\n     to ``min``, and values larger than ``max`` set to ``max``.\n+    Wherever ``min`` is larger than ``max``, the value of ``max`` is returned.\n \n   See also:\n     - :func:`jax.numpy.minimum`: Compute the element-wise minimum value of two arrays.\ndiff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py\nindex 875024617b5f..29e6586ffa18 100644\n--- a/tests/lax_numpy_test.py\n+++ b/tests/lax_numpy_test.py\n@@ -1065,6 +1065,14 @@ def testClipDeprecatedArgs(self):\n                                              \"Passing arguments 'a', 'a_min' or 'a_max' to jax.numpy.clip is deprecated\"):\n       jnp.clip(jnp.arange(4), a_min=2, a_max=3)\n \n+  def testClipUpperPrecedence(self):\n+    a_min = 3 * np.ones(1)\n+    a_max = 2 * np.ones(1)\n+    x = 4 * np.ones(1)\n+    y = jnp.clip(x, min=a_min, max=a_max)\n+    assert y == a_max, f\"Expected {y} to equal {a_max} when a_min > a_max.\"\n+    assert y == jnp.asarray(np.clip(x, a_min=a_min, a_max=a_max))\n+\n   def testHypotComplexInputError(self):\n     rng = jtu.rand_default(self.rng())\n     x = rng((5,), dtype=jnp.complex64)\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix sempahore typo in JAX\n\nPiperOrigin-RevId: 763862020",
        "output": "```diff\nCommit: e258708fc74da6b5757b0996f0fe4bdab07bc526\nDate: 2025-05-27T17:53:33Z\nURL: https://github.com/jax-ml/jax/commit/e258708fc74da6b5757b0996f0fe4bdab07bc526\nFiles changed: 3\nAdditions: +5, Deletions: -5\ndiff --git a/docs/pallas/tpu/distributed.ipynb b/docs/pallas/tpu/distributed.ipynb\nindex f2b9562c0db2..75aeeb92ca43 100644\n--- a/docs/pallas/tpu/distributed.ipynb\n+++ b/docs/pallas/tpu/distributed.ipynb\n@@ -178,7 +178,7 @@\n     \"\\n\",\n     \"`send_sem` and `recv_sem` are instances of a special type of semaphore reserved exclusively for use with DMAs. They must be allocated with the `tpu.SemaphoreType.DMA` type when specifying input specs to `pallas_call`.\\n\",\n     \"\\n\",\n-    \"Internally, DMA semaphores can be thought of as integer-valued progress trackers. On DMA start, the local device will begin to increment the value of `send_sem` and the receiver's `recv_sem` asynchronously. Waiting on a semaphore will block until the value of the semaphore reaches the total bytes of data sent/received; when the value is reached, waiting threads are released and the sempahore's value is decremented by the same amount. This means that either all data has been sent (for `send_sem`) or all data has been received (for `dst_sem`). The value of the semaphore can be read with `pl.semaphore_read`, but note that the underlying semantics of the value could change between hardware generations (e.g. the value may not represent exactly the number of bytes sent, although this is a useful mental model to have when reasoning about the behavior of the semaphore).\\n\",\n+    \"Internally, DMA semaphores can be thought of as integer-valued progress trackers. On DMA start, the local device will begin to increment the value of `send_sem` and the receiver's `recv_sem` asynchronously. Waiting on a semaphore will block until the value of the semaphore reaches the total bytes of data sent/received; when the value is reached, waiting threads are released and the semaphore's value is decremented by the same amount. This means that either all data has been sent (for `send_sem`) or all data has been received (for `dst_sem`). The value of the semaphore can be read with `pl.semaphore_read`, but note that the underlying semantics of the value could change between hardware generations (e.g. the value may not represent exactly the number of bytes sent, although this is a useful mental model to have when reasoning about the behavior of the semaphore).\\n\",\n     \"\\n\",\n     \"### Routing\\n\",\n     \"\\n\",\n@@ -531,7 +531,7 @@\n     \"\\n\",\n     \"Semaphores must be zero at the end of a Pallas program to complete succesfully. There are two error cases where this may happen:\\n\",\n     \" - If a semaphore is over-signaled, the program will end with non-zero (>0) semaphores. In this case, the program will crash upon completion. This is useful for debugging as non-zero semaphores typically means there is a bug somewhere inside of the program.\\n\",\n-    \" - If a semaphore is over-waited, the program will hang on the blocking `semaphore_wait` call while it waits for the sempahore to be incremented. In this case the device or program will need to be restarted.\\n\",\n+    \" - If a semaphore is over-waited, the program will hang on the blocking `semaphore_wait` call while it waits for the semaphore to be incremented. In this case the device or program will need to be restarted.\\n\",\n     \"\\n\",\n     \"#### Barrier Semaphores\\n\",\n     \"\\n\",\ndiff --git a/docs/pallas/tpu/distributed.md b/docs/pallas/tpu/distributed.md\nindex 36528bfbddec..7b1f26bccf89 100644\n--- a/docs/pallas/tpu/distributed.md\n+++ b/docs/pallas/tpu/distributed.md\n@@ -163,7 +163,7 @@ def example_kernel(input_ref, output_ref, send_sem, recv_sem):\n \n `send_sem` and `recv_sem` are instances of a special type of semaphore reserved exclusively for use with DMAs. They must be allocated with the `tpu.SemaphoreType.DMA` type when specifying input specs to `pallas_call`.\n \n-Internally, DMA semaphores can be thought of as integer-valued progress trackers. On DMA start, the local device will begin to increment the value of `send_sem` and the receiver's `recv_sem` asynchronously. Waiting on a semaphore will block until the value of the semaphore reaches the total bytes of data sent/received; when the value is reached, waiting threads are released and the sempahore's value is decremented by the same amount. This means that either all data has been sent (for `send_sem`) or all data has been received (for `dst_sem`). The value of the semaphore can be read with `pl.semaphore_read`, but note that the underlying semantics of the value could change between hardware generations (e.g. the value may not represent exactly the number of bytes sent, although this is a useful mental model to have when reasoning about the behavior of the semaphore).\n+Internally, DMA semaphores can be thought of as integer-valued progress trackers. On DMA start, the local device will begin to increment the value of `send_sem` and the receiver's `recv_sem` asynchronously. Waiting on a semaphore will block until the value of the semaphore reaches the total bytes of data sent/received; when the value is reached, waiting threads are released and the semaphore's value is decremented by the same amount. This means that either all data has been sent (for `send_sem`) or all data has been received (for `dst_sem`). The value of the semaphore can be read with `pl.semaphore_read`, but note that the underlying semantics of the value could change between hardware generations (e.g. the value may not represent exactly the number of bytes sent, although this is a useful mental model to have when reasoning about the behavior of the semaphore).\n \n ### Routing\n \n@@ -453,7 +453,7 @@ In order to use regular semaphores, they can be allocated in the same way as a D\n \n Semaphores must be zero at the end of a Pallas program to complete succesfully. There are two error cases where this may happen:\n  - If a semaphore is over-signaled, the program will end with non-zero (>0) semaphores. In this case, the program will crash upon completion. This is useful for debugging as non-zero semaphores typically means there is a bug somewhere inside of the program.\n- - If a semaphore is over-waited, the program will hang on the blocking `semaphore_wait` call while it waits for the sempahore to be incremented. In this case the device or program will need to be restarted.\n+ - If a semaphore is over-waited, the program will hang on the blocking `semaphore_wait` call while it waits for the semaphore to be incremented. In this case the device or program will need to be restarted.\n \n #### Barrier Semaphores\n \ndiff --git a/jax/_src/pallas/mosaic/interpret.py b/jax/_src/pallas/mosaic/interpret.py\nindex f3a165105640..401ed02288bc 100644\n--- a/jax/_src/pallas/mosaic/interpret.py\n+++ b/jax/_src/pallas/mosaic/interpret.py\n@@ -746,7 +746,7 @@ def _allocate_semaphores(\n ):\n   \"\"\"Allocates semaphores on the device with id `device_id` and core with id `local_core_id`.\n \n-  The number of sempahores allocated is given by the product of the entries in\n+  The number of semaphores allocated is given by the product of the entries in\n   `shape`.\n \n   Since for each semaphore id there is really only one global `Semaphore`\n\n```",
        "from_id": [
            "apivovarov",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Add an unsafe flag that disables automatic WG-barrier insertion\n\nEnabling this flag can introduce races into certain kernels, which is why it's\nFalse by default. Still, there's plenty of kernels where it's unnecessary and\na few of those suffer performance regressions when it is on. So it makes sense\nto at least allow users to opt out.\n\nPiperOrigin-RevId: 763853668",
        "output": "```diff\nCommit: 6f0b99356d4b95d5547c11c8ab12aa4f224e4181\nDate: 2025-05-27T17:32:55Z\nURL: https://github.com/jax-ml/jax/commit/6f0b99356d4b95d5547c11c8ab12aa4f224e4181\nFiles changed: 3\nAdditions: +29, Deletions: -7\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 2fca1464ee0b..7fb933f5623d 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -87,6 +87,16 @@ class CompilerParams(pallas_core.CompilerParams):\n       references. Defaults to 0, and must be strictly smaller than\n       max_concurrent_steps. Generally, you'll want to set it to 1 if you don't\n       await the WGMMA in the body.\n+    unsafe_no_auto_barriers: If True, Pallas will never automatically insert\n+      barrier instructions that ensure synchronous semantics of loads and stores.\n+      At the moment, the insertion is done conservatively and might regress\n+      performance. There are (at least) two conditions that must be satisfied\n+      for the use of this flag to be safe. First, no memory region is ever read\n+      *and* written to by the same thread (async copies are performed by\n+      background threads and do not count towards this rule). Secondly, no\n+      thread ever calls commit_smem(), reads from the committed SMEM and then\n+      issues an async copy overwriting that region (this is a very artificial\n+      and highly unlikely scenario).\n     profile_space: The number of profiler events that can be collected in a\n       single invocation. It is undefined behavior if a thread collects more\n       events than this.\n@@ -97,6 +107,7 @@ class CompilerParams(pallas_core.CompilerParams):\n   dimension_semantics: Sequence[DimensionSemantics] | None = None\n   max_concurrent_steps: int = 1\n   delay_release: int = 0\n+  unsafe_no_auto_barriers: bool = False\n   profile_space: int = 0\n   profile_dir: str = \"\"\n   lowering_semantics: mgpu.core.LoweringSemantics = mgpu.core.LoweringSemantics.Lane\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex cf867e55f4c9..6d54e153a9a2 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -327,6 +327,8 @@ class ModuleContext:\n   lowering_semantics: mgpu.LoweringSemantics\n   primitive_semantics: gpu_core.PrimitiveSemantics\n   mesh: mesh_lib.Mesh | None\n+  # See the documentation of unsafe_no_auto_barriers in CompilerParams.\n+  auto_barriers: bool\n   warp_axis_name: str | None = None\n \n   @property\n@@ -822,6 +824,7 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n         lowering_semantics=lowering_semantics,\n         primitive_semantics=gpu_core.PrimitiveSemantics.Warpgroup,\n         mesh=jax_mesh,\n+        auto_barriers=not params.unsafe_no_auto_barriers,\n     )\n     del runtime_smem, grouped_barriers, runtime_barriers\n     _ = lower_jaxpr_to_mosaic_gpu(\n@@ -1389,7 +1392,8 @@ def _swap_lowering_rule(\n       ctx, x_ref, transforms, handle_transposes=not transposed_value,\n       allow_peer_refs=True\n   )\n-  mgpu.warpgroup_barrier()  # Make sure reads have completed before we write.\n+  if ctx.module_ctx.auto_barriers:\n+    mgpu.warpgroup_barrier()  # Make sure reads have completed before we write.\n   match transforms:\n     case (\n         gpu_core.UnswizzleRef(swizzle),\n@@ -1443,7 +1447,8 @@ def _swap_lowering_rule(\n           value.store_untiled(x_smem)\n     case _:\n       raise NotImplementedError(f\"Unsupported transforms: {transforms}\")\n-  mgpu.warpgroup_barrier()  # Make sure the writes have completed.\n+  if ctx.module_ctx.auto_barriers:\n+    mgpu.warpgroup_barrier()  # Make sure the writes have completed.\n   return old_value\n \n \n@@ -2796,7 +2801,8 @@ def _core_map_lowering_rule(\n     # We allow the warps to schedule async copies without synchronizing with\n     # other warps, so we need to add a barrier here to make sure all reads and\n     # writes have completed.\n-    mgpu.warpgroup_barrier()\n+    if ctx.module_ctx.auto_barriers:\n+      mgpu.warpgroup_barrier()\n     _ = lower_jaxpr_to_mosaic_gpu(\n         module_ctx,\n         ctx.launch_ctx,\n@@ -2804,8 +2810,9 @@ def _core_map_lowering_rule(\n         args=(),\n         consts=args,\n     )\n-    # TODO(apaszke,justinfu): Do we really need this barrier?\n-    mgpu.warpgroup_barrier()\n+    if ctx.module_ctx.auto_barriers:\n+      # TODO(apaszke,justinfu): Do we really need this barrier?\n+      mgpu.warpgroup_barrier()\n     return []\n   raise ValueError(f\"Unsupported mesh: {mesh}\")\n \n@@ -3052,7 +3059,8 @@ def _semaphore_signal_lowering_rule(\n   # anything about the state of the other three warps in the warpgroup (they\n   # might still be e.g. reading memory that someone will overwrite once they\n   # receive a signal).\n-  mgpu.utils.warpgroup_barrier()\n+  if ctx.module_ctx.auto_barriers:\n+    mgpu.utils.warpgroup_barrier()\n   pred = ctx.module_ctx.single_wg_lane_predicate\n   llvm_dialect.inline_asm(\n     i32,\n@@ -3098,6 +3106,8 @@ def _semaphore_wait_lowering_rule(ctx: LoweringRuleContext, *args, args_tree):\n     after_block = while_op.after.blocks.append(i32_ty)\n     with ir.InsertionPoint.at_block_begin(after_block):\n       scf_dialect.yield_(after_block.arguments)\n+  # NOTE: This barrier is necessary for a correct lowering of this op and can't\n+  # be removed even if auto_barriers is False.\n   mgpu_utils.warpgroup_barrier()\n   return ()\n \ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 61be6e35cc55..9e40d046af13 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -530,7 +530,8 @@ def _copy_gmem_to_smem_lowering(\n       # arrive with the whole transfer size, while everyone else arrives with 0.\n       # But we should continue using this scheme as it's likely to be faster.\n       bytes //= WARPGROUP_SIZE\n-      mgpu.warpgroup_barrier()  # Make sure all reads have completed.\n+      if ctx.module_ctx.auto_barriers:\n+        mgpu.warpgroup_barrier()  # Make sure all reads have completed.\n       barrier.arrive_expect_tx(bytes)\n     else:\n       # In Warp-level lowering, we arrive on each CUDA thread in a warp, but\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "#sdy Remove redundant sdy export since it's now done as part of `MlirToXlaComputation`.\n\nPiperOrigin-RevId: 763837933",
        "output": "```diff\nCommit: 3b3c3385e8c51c81fea111707ff107a5a40edca3\nDate: 2025-05-27T16:58:14Z\nURL: https://github.com/jax-ml/jax/commit/3b3c3385e8c51c81fea111707ff107a5a40edca3\nFiles changed: 3\nAdditions: +0, Deletions: -17\ndiff --git a/jaxlib/mlir.cc b/jaxlib/mlir.cc\nindex a632cac71d10..4c8188b04a7f 100644\n--- a/jaxlib/mlir.cc\n+++ b/jaxlib/mlir.cc\n@@ -106,8 +106,6 @@ absl::StatusOr<XlaComputation> PyMlirModuleToXlaComputation(\n   TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n                       ParseMlirModuleString(mlir_module, context));\n   XlaComputation computation;\n-  // SDY dialect may be part of the module which XLA doesn't know about.\n-  TF_RETURN_IF_ERROR(ExportShardyForHloRoundTrip(*module));\n   TF_RETURN_IF_ERROR(MlirToXlaComputation(*module, computation, use_tuple_args,\n                                           return_tuple,\n                                           /*use_shardy=*/false));\ndiff --git a/jaxlib/py_client.cc b/jaxlib/py_client.cc\nindex 842bdfecad3d..98bde8c27396 100644\n--- a/jaxlib/py_client.cc\n+++ b/jaxlib/py_client.cc\n@@ -459,11 +459,6 @@ PyClient::CompileAndLoad(nb_class_ptr<PyClient> client, std::string mlir_module,\n   mlir::MLIRContext context;\n   TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n                       ParseMlirModuleString(mlir_module, context));\n-  if (options.executable_build_options.use_shardy_partitioner()) {\n-    // Since Shardy is located in the middle of the XLA pipeline, we need to\n-    // export it before going to HLO while preserving Shardy ops and attrs.\n-    TF_RETURN_IF_ERROR(ExportShardyForHloRoundTrip(*module));\n-  }\n   return CompileAndLoadIfrtProgram(\n       client, std::make_unique<xla::ifrt::HloProgram>(module.get()),\n       MakeIfrtCompileOptions(std::move(options), std::move(executable_devices),\n@@ -478,11 +473,6 @@ PyClient::CompileAndLoad(nb_class_ptr<PyClient> client, std::string mlir_module,\n   mlir::MLIRContext context;\n   TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n                       ParseMlirModuleString(mlir_module, context));\n-  if (options.executable_build_options.use_shardy_partitioner()) {\n-    // Since Shardy is located in the middle of the XLA pipeline, we need to\n-    // export it before going to HLO while preserving Shardy ops and attrs.\n-    TF_RETURN_IF_ERROR(ExportShardyForHloRoundTrip(*module));\n-  }\n \n   std::vector<tsl::RCReference<ifrt::LoadedHostCallback>>\n       ifrt_loaded_host_callbacks;\ndiff --git a/jaxlib/py_compile_only_client.cc b/jaxlib/py_compile_only_client.cc\nindex 2de896d80bef..274f57acba00 100644\n--- a/jaxlib/py_compile_only_client.cc\n+++ b/jaxlib/py_compile_only_client.cc\n@@ -82,11 +82,6 @@ class CompileOnlyPyClient : public PyClient {\n     mlir::MLIRContext context;\n     TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n                         ParseMlirModuleString(mlir_module, context));\n-    if (options.executable_build_options.use_shardy_partitioner()) {\n-      // Since Shardy is located in the middle of the XLA pipeline, we need to\n-      // export it before going to HLO while preserving Shardy ops and attrs.\n-      TF_RETURN_IF_ERROR(ExportShardyForHloRoundTrip(*module));\n-    }\n     auto* ifrt_client =\n         llvm::dyn_cast_or_null<CompileOnlyIfRtClient>(this->ifrt_client());\n     CHECK(ifrt_client) << \"CompileOnlyPyClient requires ifrt_client be a \"\n\n```",
        "from_id": [
            "tomnatan30",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix handling of input None in custom_transpose.",
        "output": "```diff\nCommit: fce93d2f829ae20b4be49451fb3511df91627679\nDate: 2025-05-27T14:58:45Z\nURL: https://github.com/jax-ml/jax/commit/fce93d2f829ae20b4be49451fb3511df91627679\nFiles changed: 2\nAdditions: +25, Deletions: -5\ndiff --git a/jax/_src/custom_transpose.py b/jax/_src/custom_transpose.py\nindex 21e607b5bff2..fb125e174122 100644\n--- a/jax/_src/custom_transpose.py\n+++ b/jax/_src/custom_transpose.py\n@@ -217,7 +217,6 @@ def custom_transpose_transpose_rule(\n   # Consider passing this information to the custom transpose rule?\n \n   res_arg, lin_arg = tree_unflatten(call_in_tree, args)\n-  del lin_arg\n   assert all(not ad.is_undefined_primal(x) for x in tree_leaves(res_arg))\n \n   cts = [ad_util.zeros_like_aval(ct.aval) if type(ct) is ad_util.Zero else ct\n@@ -225,10 +224,17 @@ def custom_transpose_transpose_rule(\n   ct_out = tree_unflatten(out_tree, cts)\n   ct_lin = transpose.call_wrapped(res_arg, ct_out)\n   check_transpose_rule_trees(transpose, lin_tree, tree_structure(ct_lin))\n-  ct_lin_flat, _ = tree_flatten(\n-      tree_broadcast(lin_tree, ct_lin, is_leaf=lambda x: x is None),\n-      is_leaf=lambda x: x is None)\n-  return [None] * len(tree_leaves(res_arg)) + ct_lin_flat\n+  ct_lin = tree_broadcast(lin_tree, ct_lin, is_leaf=lambda x: x is None)\n+\n+  # When the transpose returns None, we treat that as a Zero, except when the\n+  # input is also None. In that case, the cotangent corresponding to that input\n+  # should be dropped.\n+  zero = object()\n+  ct_lin = tree_map(lambda l, ct: zero if ct is None and l is not None else ct,\n+                    lin_arg, ct_lin, is_leaf=ad.is_undefined_primal)\n+\n+  ct_lin_flat, _ = tree_flatten(ct_lin)\n+  return [None] * res_tree.num_leaves + [None if ct is zero else ct for ct in ct_lin_flat]\n \n \n def custom_transpose_lowering(*args, call_jaxpr, **params):\ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 9d10b40c6030..bfe391797920 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3722,6 +3722,20 @@ def gt(x, t):\n     with config.use_direct_linearize(True):\n       self.assertAllClose(jax.grad(f)(0.5), jnp.cos(0.5))\n \n+  def test_input_none(self):\n+    # ref: https://github.com/jax-ml/jax/issues/29009\n+    @jax.custom_jvp\n+    def f(x, y): return y\n+    @f.defjvp\n+    def f_jvp(p, t): return f(*p), g(p, t)\n+\n+    @custom_transpose(jnp.float32(0))\n+    def g(r, x): return x[1]\n+    @g.def_transpose\n+    def gt(r, t): return None, jnp.zeros_like(r[1])\n+\n+    jax.grad(f, argnums=(1,))(None, jnp.float32(2))  # doesn't crash\n+\n \n class CustomDceTest(jtu.JaxTestCase):\n \n\n```",
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "[Mosaic GPU][NFC] Refactor the body of the matmul kernel\n\nThis will make it much simpler to make the kernel persistent.\n\nPiperOrigin-RevId: 763782577",
        "output": "```diff\nCommit: ee727f98746e77a0cd7f8ad5b63debfd0ba00053\nDate: 2025-05-27T14:26:58Z\nURL: https://github.com/jax-ml/jax/commit/ee727f98746e77a0cd7f8ad5b63debfd0ba00053\nFiles changed: 1\nAdditions: +84, Deletions: -79\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex 3653d9be8d8d..929c7c498986 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -94,92 +94,97 @@ def kernel(ctx, a, b, d, smem):\n         arith.CmpIPredicate.eq, ctx.cluster_idx(gpu.Dimension.x), c(0, index)\n     )\n \n+    # This function executes the kernel for a single output tile.\n+    def compute_output(block_m_start, n_start):\n+      \"\"\"Compute and store a single output tile.\"\"\"\n+      # All blocks in the cluster share the same m_start -- align it!\n+      m_start = arith.muli(arith.divui(block_m_start, c(tile_m, index)), c(tile_m, index))\n+      with mgpu.when(is_leader_of(TMA_WARP)):\n+        @mgpu.fori(c(k_loop_iter, index), None)\n+        def _tma_body(ki, _):\n+          slot = arith.remui(ki, c(max_concurrent_steps, index))\n+          # TODO(apaszke): Use a predicate instead of a conditional.\n+          with mgpu.when(arith.cmpi(arith.CmpIPredicate.uge, ki, c(max_concurrent_steps, index))):\n+            ab_empty_barriers[slot].wait()\n+          full_barrier = ab_full_barriers[slot]\n+          with mgpu.when(is_leader_block):\n+            full_barrier.arrive_expect_tx(\n+                bytecount((tile_m, tile_k), dtype) + bytecount((tile_n, tile_k), dtype)\n+            )\n+          k_start = arith.muli(ki, c(tile_k, index))\n+          common_args = dict(\n+              swizzle=swizzle,\n+              barrier=full_barrier,\n+              arrive=False,\n+              predicate=None,\n+              collective=gpu.Dimension.x,\n+              partitioned=0,  # Non-contracting dim is always 0.\n+          )\n+          ctx.async_copy(\n+              src_ref=a,\n+              dst_ref=mgpu.memref_slice(a_smem, slot),\n+              gmem_slice=(ds(m_start, tile_m), ds(k_start, tile_k)),\n+              gmem_transform=mgpu.TileTransform(tiling),\n+              **common_args,\n+          )\n+          ctx.async_copy(\n+              src_ref=b,\n+              dst_ref=mgpu.memref_slice(b_smem, slot),\n+              gmem_slice=(ds(n_start, tile_n), ds(k_start, tile_k)),\n+              gmem_transform=mgpu.TileTransform(tiling),\n+              **common_args,\n+          )\n+\n+      with mgpu.when(arith.andi(is_leader_of(MMA_WARP), is_leader_block)):\n+        @mgpu.fori(c(k_loop_iter, index), arith.constant(i1, 0))\n+        def _mma_body(ki, accumulate):\n+          slot = arith.remui(ki, c(max_concurrent_steps, index))\n+          ab_full_barriers[slot].wait()\n+          tcgen05.mma(\n+              acc,\n+              mgpu.memref_slice(a_smem, slot),\n+              mgpu.memref_transpose(mgpu.memref_slice(b_smem, slot), (1, 0, 3, 2)),\n+              a_swizzle=swizzle,\n+              b_swizzle=swizzle,\n+              accumulate=accumulate,\n+              collective=collective,\n+          )\n+          accumulate = arith.constant(i1, 1)\n+          is_last_iter = arith.cmpi(\n+              arith.CmpIPredicate.eq, ki, c(k_loop_iter - 1, index)\n+          )\n+          barrier_ptr = arith.select(\n+              is_last_iter,\n+              mma_done_barrier.get_ptr(),\n+              ab_empty_barriers[slot].get_ptr(),\n+          )\n+          tcgen05.commit_arrive(barrier_ptr, collective=collective, ctx=ctx)\n+          return accumulate\n+\n+      gpu.barrier()\n+      mma_done_barrier.wait(for_tensor_core=True)\n+\n+      final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n+      final_acc.store_tiled(d_smem, swizzle=128)\n+      mgpu.commit_shared()\n+      ctx.async_copy(\n+          src_ref=d_smem,\n+          dst_ref=d,\n+          gmem_slice=(ds(block_m_start, block_tile_m), ds(n_start, tile_n)),\n+          gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n+          swizzle=swizzle,\n+      )\n+      ctx.await_async_copy(0)\n+\n     m_idx = arith.addi(\n         gpu.block_id(gpu.Dimension.x),\n         arith.muli(gpu.block_id(gpu.Dimension.z), c(grid_tile_m, index)),\n     )\n     n_idx = gpu.block_id(gpu.Dimension.y)\n     block_m_start = arith.muli(m_idx, c(block_tile_m, index))\n-    # All blocks in the cluster share the same m_start -- align it!\n-    m_start = arith.muli(arith.divui(block_m_start, c(tile_m, index)), c(tile_m, index))\n     n_start = arith.muli(n_idx, c(tile_n,index))\n-\n-    with mgpu.when(is_leader_of(TMA_WARP)):\n-      @mgpu.fori(c(k_loop_iter, index), None)\n-      def _tma_body(ki, _):\n-        slot = arith.remui(ki, c(max_concurrent_steps, index))\n-        # TODO(apaszke): Use a predicate instead of a conditional.\n-        with mgpu.when(arith.cmpi(arith.CmpIPredicate.uge, ki, c(max_concurrent_steps, index))):\n-          ab_empty_barriers[slot].wait()\n-        full_barrier = ab_full_barriers[slot]\n-        with mgpu.when(is_leader_block):\n-          full_barrier.arrive_expect_tx(\n-              bytecount((tile_m, tile_k), dtype) + bytecount((tile_n, tile_k), dtype)\n-          )\n-        k_start = arith.muli(ki, c(tile_k, index))\n-        common_args = dict(\n-            swizzle=swizzle,\n-            barrier=full_barrier,\n-            arrive=False,\n-            predicate=None,\n-            collective=gpu.Dimension.x,\n-            partitioned=0,  # Non-contracting dim is always 0.\n-        )\n-        ctx.async_copy(\n-            src_ref=a,\n-            dst_ref=mgpu.memref_slice(a_smem, slot),\n-            gmem_slice=(ds(m_start, tile_m), ds(k_start, tile_k)),\n-            gmem_transform=mgpu.TileTransform(tiling),\n-            **common_args,\n-        )\n-        ctx.async_copy(\n-            src_ref=b,\n-            dst_ref=mgpu.memref_slice(b_smem, slot),\n-            gmem_slice=(ds(n_start, tile_n), ds(k_start, tile_k)),\n-            gmem_transform=mgpu.TileTransform(tiling),\n-            **common_args,\n-        )\n-\n-    with mgpu.when(arith.andi(is_leader_of(MMA_WARP), is_leader_block)):\n-      @mgpu.fori(c(k_loop_iter, index), arith.constant(i1, 0))\n-      def _mma_body(ki, accumulate):\n-        slot = arith.remui(ki, c(max_concurrent_steps, index))\n-        ab_full_barriers[slot].wait()\n-        tcgen05.mma(\n-            acc,\n-            mgpu.memref_slice(a_smem, slot),\n-            mgpu.memref_transpose(mgpu.memref_slice(b_smem, slot), (1, 0, 3, 2)),\n-            a_swizzle=swizzle,\n-            b_swizzle=swizzle,\n-            accumulate=accumulate,\n-            collective=collective,\n-        )\n-        accumulate = arith.constant(i1, 1)\n-        is_last_iter = arith.cmpi(\n-            arith.CmpIPredicate.eq, ki, c(k_loop_iter - 1, index)\n-        )\n-        barrier_ptr = arith.select(\n-            is_last_iter,\n-            mma_done_barrier.get_ptr(),\n-            ab_empty_barriers[slot].get_ptr(),\n-        )\n-        tcgen05.commit_arrive(barrier_ptr, collective=collective, ctx=ctx)\n-        return accumulate\n-\n-    gpu.barrier()\n-    mma_done_barrier.wait(for_tensor_core=True)\n-\n-    final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n-    final_acc.store_tiled(d_smem, swizzle=128)\n-    mgpu.commit_shared()\n-    ctx.async_copy(\n-        src_ref=d_smem,\n-        dst_ref=d,\n-        gmem_slice=(ds(block_m_start, block_tile_m), ds(n_start, tile_n)),\n-        gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n-        swizzle=swizzle,\n-    )\n-    ctx.await_async_copy(0)\n+    # This is not a persistent kernel, so we only process one tile.\n+    compute_output(block_m_start, n_start)\n \n   compute_buffers = (\n     jax.ShapeDtypeStruct(\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Fix missing symbol errors in OSS collective kernels\n\nWe sometimes access NVSHMEM functions from the host code too, which means\nwe should include the NVSHMEM host library in the context of the ExecutionEngine.\n\nPiperOrigin-RevId: 763777731",
        "output": "```diff\nCommit: f5ffd7fc7c717b9e1aa91d3887bfb1c93189c73e\nDate: 2025-05-27T14:10:34Z\nURL: https://github.com/jax-ml/jax/commit/f5ffd7fc7c717b9e1aa91d3887bfb1c93189c73e\nFiles changed: 1\nAdditions: +7, Deletions: -4\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 7df185cffaf1..54fef13a8521 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -555,9 +555,12 @@ absl::StatusOr<std::pair<std::unique_ptr<mlir::ExecutionEngine>, bool>> Compile(\n     return absl::InternalError(\"Pass pipeline failed\");\n   }\n \n-  llvm::SmallVector<llvm::StringRef> runtime_lib;\n-  if (const char* lib_path = getenv(\"MOSAIC_GPU_RUNTIME_LIB_PATH\")) {\n-    runtime_lib.emplace_back(lib_path);\n+  llvm::SmallVector<llvm::StringRef> runtime_libs;\n+  if (const char* runtime_lib_path = getenv(\"MOSAIC_GPU_RUNTIME_LIB_PATH\")) {\n+    runtime_libs.emplace_back(runtime_lib_path);\n+  }\n+  if (const char* nvshmem_path = getenv(\"MOSAIC_GPU_NVSHMEM_SO_PATH\")) {\n+    runtime_libs.emplace_back(nvshmem_path);\n   }\n   // Create a transformer to run all LLVM optimization passes at the\n   // specified optimization level.\n@@ -566,7 +569,7 @@ absl::StatusOr<std::pair<std::unique_ptr<mlir::ExecutionEngine>, bool>> Compile(\n   mlir::ExecutionEngineOptions options;\n   options.transformer = transformer;\n   options.jitCodeGenOptLevel = llvm::CodeGenOptLevel::Aggressive;\n-  options.sharedLibPaths = runtime_lib;\n+  options.sharedLibPaths = runtime_libs;\n   auto maybe_execution_engine = mlir::ExecutionEngine::create(module, options);\n   if (!maybe_execution_engine) {\n     return absl::InternalError(\"Failed to compile kernel\");\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add tests for the Blackwell matmul kernel\n\nJust to give us extra confidence while we make changes.\n\nPiperOrigin-RevId: 763767275",
        "output": "```diff\nCommit: 487eeb4c0fa518b4055f2d154ffd49153809e845\nDate: 2025-05-27T13:36:12Z\nURL: https://github.com/jax-ml/jax/commit/487eeb4c0fa518b4055f2d154ffd49153809e845\nFiles changed: 5\nAdditions: +113, Deletions: -24\ndiff --git a/jax/experimental/mosaic/gpu/examples/BUILD b/jax/experimental/mosaic/gpu/examples/BUILD\nindex fe1a7e9180ac..b24c38b34235 100644\n--- a/jax/experimental/mosaic/gpu/examples/BUILD\n+++ b/jax/experimental/mosaic/gpu/examples/BUILD\n@@ -39,6 +39,15 @@ py_library(\n     ],\n )\n \n+py_library(\n+    name = \"matmul_blackwell\",\n+    srcs = [\"matmul_blackwell.py\"],\n+    deps = [\n+        \"//jax\",\n+        \"//jax:mosaic_gpu\",\n+    ],\n+)\n+\n py_library(\n     name = \"flash_attention\",\n     srcs = [\"flash_attention.py\"],\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex f771c8bc1ef1..3653d9be8d8d 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -41,7 +41,8 @@ def bytecount(shape, dtype):\n \n \n def build_kernel(\n-    m, n, k,\n+    m, k, n,\n+    dtype: jnp.dtype,\n     tile_m: int = 128,\n     tile_n: int = 128,\n     grid_tile_m: int = 1,\n@@ -51,12 +52,15 @@ def build_kernel(\n   i1 = ir.IntegerType.get_signless(1)\n   i32 = ir.IntegerType.get_signless(32)\n   index = ir.IndexType.get()\n+  if jnp.dtype(dtype).itemsize != 2:\n+    raise NotImplementedError(f\"Only tested with 16-bit dtypes, but got {dtype}\")\n+  if tile_m != 128:\n+    raise NotImplementedError(f\"Only tile_m=128 supported, but got {tile_m}\")\n \n   swizzle = 128\n-  swizzle_elems = tile_k = swizzle // 2\n+  swizzle_elems = tile_k = 8 * swizzle // jnp.finfo(dtype).bits\n   tiling = (8, swizzle_elems)\n \n-  in_dtype = jnp.float16\n   k_loop_iter = k // tile_k\n   max_concurrent_steps = min(max_concurrent_steps, k_loop_iter)\n \n@@ -74,7 +78,7 @@ def build_kernel(\n     raise ValueError(f\"{n=} must be divisible by {tile_n=}\")\n   if k % tile_k != 0:\n     raise ValueError(f\"{k=} must be divisible by {tile_k=}\")\n-  if (m // tile_m) % grid_tile_m:\n+  if (m // block_tile_m) % grid_tile_m:\n     raise ValueError(f\"{m=} // {tile_m=} must be divisible by {grid_tile_m=}\")\n \n   def kernel(ctx, a, b, d, smem):\n@@ -83,8 +87,12 @@ def kernel(ctx, a, b, d, smem):\n \n     warp_idx = mgpu.warp_idx(sync=True)\n     is_warp_leader = nvvm.elect_sync(i1)\n-    is_leader_of = lambda i: arith.andi(arith.cmpi(arith.CmpIPredicate.eq, warp_idx, c(i, i32)), is_warp_leader)\n-    is_leader_block = arith.cmpi(arith.CmpIPredicate.eq, ctx.cluster_idx(gpu.Dimension.x), c(0, index))\n+    is_leader_of = lambda i: arith.andi(\n+        arith.cmpi(arith.CmpIPredicate.eq, warp_idx, c(i, i32)), is_warp_leader\n+    )\n+    is_leader_block = arith.cmpi(\n+        arith.CmpIPredicate.eq, ctx.cluster_idx(gpu.Dimension.x), c(0, index)\n+    )\n \n     m_idx = arith.addi(\n         gpu.block_id(gpu.Dimension.x),\n@@ -96,7 +104,6 @@ def kernel(ctx, a, b, d, smem):\n     m_start = arith.muli(arith.divui(block_m_start, c(tile_m, index)), c(tile_m, index))\n     n_start = arith.muli(n_idx, c(tile_n,index))\n \n-\n     with mgpu.when(is_leader_of(TMA_WARP)):\n       @mgpu.fori(c(k_loop_iter, index), None)\n       def _tma_body(ki, _):\n@@ -107,7 +114,7 @@ def _tma_body(ki, _):\n         full_barrier = ab_full_barriers[slot]\n         with mgpu.when(is_leader_block):\n           full_barrier.arrive_expect_tx(\n-              bytecount((tile_m, tile_k), in_dtype) + bytecount((tile_n, tile_k), in_dtype)\n+              bytecount((tile_m, tile_k), dtype) + bytecount((tile_n, tile_k), dtype)\n           )\n         k_start = arith.muli(ki, c(tile_k, index))\n         common_args = dict(\n@@ -162,7 +169,8 @@ def _mma_body(ki, accumulate):\n     gpu.barrier()\n     mma_done_barrier.wait(for_tensor_core=True)\n \n-    acc.load().astype(ir.F16Type.get()).store_tiled(d_smem, swizzle=128)\n+    final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n+    final_acc.store_tiled(d_smem, swizzle=128)\n     mgpu.commit_shared()\n     ctx.async_copy(\n         src_ref=d_smem,\n@@ -176,14 +184,14 @@ def _mma_body(ki, accumulate):\n   compute_buffers = (\n     jax.ShapeDtypeStruct(\n         mgpu.tile_shape((max_concurrent_steps, block_tile_m, tile_k), tiling),\n-        jnp.float16),\n+        dtype),\n     jax.ShapeDtypeStruct(\n-         mgpu.tile_shape((max_concurrent_steps, block_tile_n, tile_k), tiling),\n-         jnp.float16),\n+        mgpu.tile_shape((max_concurrent_steps, block_tile_n, tile_k), tiling),\n+        dtype),\n   )\n   epilogue_buffer = jax.ShapeDtypeStruct(\n       mgpu.tile_shape((block_tile_m, tile_n), (128, swizzle_elems)),\n-      jnp.float16)\n+      dtype)\n   smem_buffers = mgpu.Union([compute_buffers, epilogue_buffer])\n   smem = (\n       smem_buffers,\n@@ -196,10 +204,10 @@ def _mma_body(ki, accumulate):\n       (grid_tile_m, n // tile_n, m // (block_tile_m * grid_tile_m)),\n       (128, 1, 1),\n       (\n-          jax.ShapeDtypeStruct((m, k), jnp.float16),\n-          jax.ShapeDtypeStruct((n, k), jnp.float16),\n+          jax.ShapeDtypeStruct((m, k), dtype),\n+          jax.ShapeDtypeStruct((n, k), dtype),\n       ),\n-      jax.ShapeDtypeStruct((m, n), jnp.float16),\n+      jax.ShapeDtypeStruct((m, n), dtype),\n       smem,\n       cluster=(2 if collective else 1, 1, 1),\n   )\n@@ -236,7 +244,7 @@ def main(unused_argv):\n       continue\n     try:\n       with mlir.make_ir_context(), ir.Location.unknown():\n-        f = build_kernel(m, n, k, **kwargs)\n+        f = build_kernel(m, k, n, jnp.float16, **kwargs)\n         _, runtime = profiler.measure(f)(a, b)\n     except ValueError as e:\n       if \"Mosaic GPU kernel exceeds available shared memory\" not in str(e):\n@@ -251,7 +259,7 @@ def main(unused_argv):\n     raise ValueError(\"No valid configuration found\")\n \n   with mlir.make_ir_context(), ir.Location.unknown():\n-    d, runtime = profiler.measure(build_kernel(m, n, k, **best_kwargs))(a, b)\n+    d, runtime = profiler.measure(build_kernel(m, k, n, jnp.float16, **best_kwargs))(a, b)\n   d_ref, ref_runtime = profiler.measure(jax.jit(lambda a, b: a @ b.T))(a, b)\n \n   tflops = float(2 * k * m * n) / (runtime / 1e3) / 1e12\ndiff --git a/tests/mosaic/BUILD b/tests/mosaic/BUILD\nindex 9b6a7f79d099..ffaa0c3c843f 100644\n--- a/tests/mosaic/BUILD\n+++ b/tests/mosaic/BUILD\n@@ -126,6 +126,7 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:mosaic_gpu\",\n         \"//jax/experimental/mosaic/gpu/examples:matmul\",\n+        \"//jax/experimental/mosaic/gpu/examples:matmul_blackwell\",\n     ] + py_deps([\n         \"absl/testing\",\n         \"numpy\",\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 42a3f0fc83c1..0c79d26782c7 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -228,8 +228,7 @@ def setUp(self):\n     super().setUp()\n     self.prng = np.random.default_rng(1234)\n     self.context = mlir.make_ir_context()\n-    if mgpu_dialect is not None:\n-      mgpu_dialect.register_dialect(self.context)\n+    mgpu_dialect.register_dialect(self.context)\n     self.enter_context(config.traceback_filtering(\"off\"))\n     self.enter_context(self.context)\n     self.enter_context(ir.Location.unknown())\ndiff --git a/tests/mosaic/matmul_test.py b/tests/mosaic/matmul_test.py\nindex 9634718d2d44..680e699c8972 100644\n--- a/tests/mosaic/matmul_test.py\n+++ b/tests/mosaic/matmul_test.py\n@@ -19,7 +19,11 @@\n from absl.testing import absltest, parameterized\n from jax._src import config\n from jax._src import test_util as jtu\n+from jax._src.interpreters import mlir\n+from jax._src.lib.mlir import ir\n+from jax.experimental.mosaic.gpu import dialect as mgpu_dialect  # pylint: disable=g-importing-member\n import jax.numpy as jnp\n+import numpy as np\n \n import hypothesis as hp\n import hypothesis.strategies as hps\n@@ -31,6 +35,7 @@\n   matmul = None\n else:\n   from jax.experimental.mosaic.gpu.examples import matmul\n+  from jax.experimental.mosaic.gpu.examples import matmul_blackwell\n \n \n config.parse_flags_with_absl()\n@@ -53,9 +58,13 @@ def setUp(self):\n     super().setUp()\n     if matmul is None:\n       self.skipTest(\"Mosaic GPU not available.\")\n-    if (not jtu.test_device_matches([\"cuda\"]) or\n-        not jtu.is_cuda_compute_capability_equal(\"9.0\")):\n-      self.skipTest(\"Only works on GPU with capability sm90a\")\n+    if not jtu.test_device_matches([\"cuda\"]):\n+      self.skipTest(\"Test needs a GPU device\")\n+    self.context = mlir.make_ir_context()\n+    mgpu_dialect.register_dialect(self.context)\n+    self.enter_context(config.traceback_filtering(\"off\"))\n+    self.enter_context(self.context)\n+    self.enter_context(ir.Location.unknown())\n \n   @parameterized.named_parameters(\n       (f\"_shard{i}\", i) for i in range(5)\n@@ -63,7 +72,10 @@ def setUp(self):\n   @seed_hypothesis\n   @hp.settings(max_examples=100)  # Add verbosity=hp.Verbosity.verbose to debug\n   @hp.given(hps.data())\n-  def test_matmul(self, data):\n+  def test_matmul_sm90(self, data):\n+    if not jtu.is_cuda_compute_capability_equal(\"9.0\"):\n+      self.skipTest(\"Only works on GPU with capability sm90a\")\n+\n     in_dtype = data.draw(\n         hps.sampled_from([jnp.float16, jnp.bfloat16, jnp.float32]),\n         label=\"in_dtype\",\n@@ -122,6 +134,66 @@ def test_matmul(self, data):\n         hp.assume(False)\n       raise e\n \n+  @parameterized.named_parameters(\n+      # TODO(apaszke): Increase shard count once we have more B200s in CI.\n+      (f\"_shard{i}\", i) for i in range(1)\n+  )\n+  @seed_hypothesis\n+  @hp.settings(max_examples=100)  # Add verbosity=hp.Verbosity.verbose to debug\n+  @hp.given(hps.data())\n+  def test_matmul_sm100(self, data):\n+    if not jtu.is_cuda_compute_capability_equal(\"10.0\"):\n+      self.skipTest(\"Only works on GPU with capability sm100a\")\n+\n+    dtype = data.draw(\n+        hps.sampled_from([jnp.float16, jnp.bfloat16]),\n+        label=\"dtype\",\n+    )\n+    m, n, k = (\n+        data.draw(hps.sampled_from([128, 256, 512, 2048, 8192]), label=d) for d in \"mnk\"\n+    )\n+    max_concurrent_steps = data.draw(\n+        hps.integers(2, 5), label=\"max_concurrent_steps\"\n+    )\n+    collective = data.draw(hps.booleans(), label=\"collective\")\n+    num_ctas = 2 if collective else 1\n+    hp.assume(not (m == 128 and collective))  # Too small for collective MMA.\n+    tile_m = data.draw(\n+        hps.sampled_from([t for t in [128] if t * num_ctas <= m]), label=\"tile_m\"\n+    )\n+    tile_n = data.draw(\n+        hps.sampled_from([t for t in [64, 128, 256] if t * num_ctas <= n]), label=\"tile_n\"\n+    )\n+    grid_m = m // (num_ctas * tile_m)\n+    grid_tile_m = data.draw(hps.sampled_from([1, 2, 4, 8, 16]), label=\"grid_tile_m\")\n+    hp.assume(grid_m % grid_tile_m == 0)\n+\n+    try:\n+      kernel = matmul_blackwell.build_kernel(\n+          m,\n+          k,\n+          n,\n+          dtype=dtype,\n+          tile_m=tile_m,\n+          tile_n=tile_n,\n+          grid_tile_m=grid_tile_m,\n+          max_concurrent_steps=max_concurrent_steps,\n+          collective=collective,\n+      )\n+    except ValueError as e:\n+      if \"Mosaic GPU kernel exceeds available shared memory\" in str(e):\n+        hp.assume(False)\n+      raise\n+\n+    ka, kb = jax.random.split(jax.random.key(0), 2)\n+    a = jax.random.normal(key=ka, shape=(m, k), dtype=dtype)\n+    b = jax.random.normal(key=kb, shape=(n, k), dtype=dtype)\n+    out = kernel(a, b)\n+    out_ref = jnp.dot(a, b.T)\n+    np.testing.assert_allclose(\n+        out, out_ref, atol=1e-3, rtol=1e-3 if k < 512 else 1e-2\n+    )\n+\n \n if __name__ == \"__main__\":\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "#sdy remove redundant call to sdy-round-trip-export in JAX export.\n\nWe already call `xla::sdy::addSdyRoundTripExportPipeline` in `xla::SerializeUsingVersionedStablehlo` so no need for this anymore.\n\nPiperOrigin-RevId: 763762358",
        "output": "```diff\nCommit: a57b4a1583e9b67f2520b76f7e5c466f3c5ff99b\nDate: 2025-05-27T13:18:26Z\nURL: https://github.com/jax-ml/jax/commit/a57b4a1583e9b67f2520b76f7e5c466f3c5ff99b\nFiles changed: 1\nAdditions: +3, Deletions: -7\ndiff --git a/jax/_src/export/_export.py b/jax/_src/export/_export.py\nindex 189818541a2c..b390574c0a79 100644\n--- a/jax/_src/export/_export.py\n+++ b/jax/_src/export/_export.py\n@@ -694,7 +694,7 @@ def _export_lowered(\n   shardy_enabled = _jax.sdy.lowered_with_shardy(\n       mlir.module_to_bytecode(mlir_module))\n \n-  mlir_module_serialized = _module_to_bytecode(mlir_module, shardy_enabled)\n+  mlir_module_serialized = _module_to_bytecode(mlir_module)\n \n   # Figure out the result types and shapes\n   if \"global_out_avals\" in lowering.compile_args:\n@@ -808,12 +808,8 @@ def _get_exported_vjp(exp_primal: Exported) -> Exported:\n       calling_convention_version=version,\n       _get_vjp=_get_exported_vjp)\n \n-def _module_to_bytecode(module: ir.Module, shardy_enabled: bool) -> bytes:\n-  if shardy_enabled:\n-    mlir_str = _jax.sdy.sdy_round_trip_export_pipeline(\n-        mlir.module_to_bytecode(module))\n-  else:\n-    mlir_str = mlir.module_to_bytecode(module)\n+def _module_to_bytecode(module: ir.Module) -> bytes:\n+  mlir_str = mlir.module_to_bytecode(module)\n   # `target_version` is used to manage situations when a StableHLO producer\n   # and a StableHLO consumer were built using different versions of StableHLO.\n   #\n\n```",
        "from_id": [
            "bartchr808",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas] Require parallel dimensions to form a prefix of the grid in TPU interpret mode.\n\nSince dimensions with parallel semantics must now appear as the leading dimensions of the grid, this CL also makes the sequential iteration over cores in the simulation never re-visit a core after the simulation has moved on to the next core. This enables the simulation to correctly omit loads and stores of kernel buffers if the same (slice of a) buffer is processed by multiple kernel invocations on the same core.\n\nPiperOrigin-RevId: 763737647",
        "output": "```diff\nCommit: 8124cb64dc7329fa348da82a0e92f0731dccae0a\nDate: 2025-05-27T11:56:59Z\nURL: https://github.com/jax-ml/jax/commit/8124cb64dc7329fa348da82a0e92f0731dccae0a\nFiles changed: 2\nAdditions: +385, Deletions: -109\ndiff --git a/jax/_src/pallas/mosaic/interpret.py b/jax/_src/pallas/mosaic/interpret.py\nindex 4258e52e6541..f3a165105640 100644\n--- a/jax/_src/pallas/mosaic/interpret.py\n+++ b/jax/_src/pallas/mosaic/interpret.py\n@@ -86,18 +86,23 @@ class TPUInterpretParams:\n       replaced with arrays all of `jnp.inf`. Additionaly any floating point\n       operands to any operation will be replaced with (arrays of) `jnp.inf`.\n       Default: False.\n-    uninitialized_memory: If \"nan\", allocated buffers are initialized to\n-      contain all NaNs (or to their maximum possible value for integers). If\n-      \"zero\", allocated buffers are initialized to all zeros.\n+    uninitialized_memory: If \"nan\", allocated buffers are initialized to contain\n+      all NaNs (or to their maximum possible value for integers). If \"zero\",\n+      allocated buffers are initialized to all zeros.\n       Default: \"nan\".\n     random_seed: Seed for random number generator used during interpretation.\n       Currently random numbers are used to randomize the grid coordinates along\n       dimensions with 'parallel' semantics.\n       Default: None.\n     grid_point_recorder: Callback that is invoked by the interpreter for each\n-      grid point in the order in which the grid points are traversed. This is\n-      intended for inspecting the randomization of coordinates along grid\n-      dimensions with 'parallel' semantics.\n+      grid point in the order in which the grid points are traversed. The\n+      callback is invoked with two arguments:\n+        - A tuple of grid coordinates.\n+        - The local core ID of the core that is processing the grid point.\n+      This callback is intended for inspecting\n+        - the randomization of coordinates along grid dimensions with 'parallel'\n+          semantics and\n+        - the mapping of grid points to local (i.e. per-device) cores.\n       Default: None.\n     num_cores_per_device: The number of cores per device.\n       Default: 1.\n@@ -107,7 +112,9 @@ class TPUInterpretParams:\n   skip_floating_point_ops: bool = False\n   uninitialized_memory: Literal[\"nan\", \"zero\"] = \"nan\"\n   random_seed: int | None = None\n-  grid_point_recorder: Callable[[tuple[jnp.int32, ...]], None] | None = None\n+  grid_point_recorder: (\n+      Callable[[tuple[np.int32, ...], np.int32], None] | None\n+  ) = None\n   num_cores_per_device: int = 1\n \n \n@@ -1752,11 +1759,45 @@ def _get_mosaic_params(compiler_params: dict[str, pallas_core.CompilerParams]) -\n def _get_parallel_dim_semantics(\n     compiler_params: dict[str, Any], num_dimensions_in_grid: int,\n ) -> tuple[bool, ...]:\n-  \"\"\"Returns a tuple of booleans indicating whether the corresponding dimension in the grid is parallel.\"\"\"\n+  \"\"\"Returns a tuple indicating which grid dimensions have parallel semantics.\n+\n+  Args:\n+    compiler_params: Representation of a `mosaic_core.TPUCompilerParams` object\n+      as a dictionary.\n+    num_dimensions_in_grid: The number of dimensions in the grid.\n+\n+  Returns:\n+    A tuple of booleans where the entry at index `i` is `True` precisely if the\n+    `i`-th dimension in the grid has parallel semantics.\n+\n+  Raises:\n+    ValueError: If the dimensions with parallel semantics do not form a prefix\n+      of the grid.\n+  \"\"\"\n   mosaic_params = _get_mosaic_params(compiler_params)\n   if mosaic_params.dimension_semantics is None:\n     return (False,) * num_dimensions_in_grid\n-  return tuple(ds == 'parallel' for ds in mosaic_params.dimension_semantics)\n+  result = tuple(ds == 'parallel' for ds in mosaic_params.dimension_semantics)\n+  for ds0, ds1 in zip(result[:-1], result[1:]):\n+    if ds1 and not ds0:\n+      raise ValueError(\n+          'Dimensions with parallel semantics must form a prefix of the grid.'\n+      )\n+  return result\n+\n+\n+def _get_parallel_subgrid_size(\n+    parallel_semantics_per_dim: tuple[bool, ...], grid: tuple[int, ...]\n+) -> int:\n+  \"\"\"Returns the size of the subgrid along the parallel dimensions.\"\"\"\n+  return functools.reduce(\n+      lambda x, y: x * y,\n+      (\n+          dim_size if parallel_dim else 1\n+          for dim_size, parallel_dim in zip(grid, parallel_semantics_per_dim)\n+      ),\n+      1,\n+  )\n \n _GridPointCoordinatesPerDim = tuple[Array, ...]\n \n@@ -1836,24 +1877,6 @@ def _get_grid_point(\n     grid_point.append(li if jnp.size(coords) == 0 else coords[li])\n   return jnp.array(grid_point, dtype=np.int32)\n \n-\n-def _get_next_local_core_id(\n-    local_core_id: int,\n-    parallel_semantics_per_dim: tuple[bool, ...],\n-    grid_point: Array,\n-    next_grid_point: Array,\n-    interpret_params: TPUInterpretParams,\n-) -> int:\n-  delta = next_grid_point - grid_point\n-  assert delta.shape == (len(parallel_semantics_per_dim),)\n-  parallel_semantics_per_dim = jnp.array(parallel_semantics_per_dim)\n-  deltas_along_parallel_dims = jnp.where(parallel_semantics_per_dim, delta, 0)\n-  return jax.lax.cond(\n-      jnp.any(deltas_along_parallel_dims),\n-      lambda: (local_core_id + 1) % interpret_params.num_cores_per_device,\n-      lambda: local_core_id,\n-  )\n-\n def _uninitialized_value(shape, dtype, interpret_params):\n   if interpret_params.uninitialized_memory == 'nan':\n     if jnp.issubdtype(dtype, jnp.floating):\n@@ -2078,13 +2101,28 @@ def interpret_pallas_call(\n     # Base case is always one iteration when grid is ()\n     num_iterations = 1\n \n-  parallel_semantics_per_dim = _get_parallel_dim_semantics(\n-      compiler_params, len(grid)\n-  )\n   randomized_grid_coordinates = _get_randomized_grid_coordinates(\n       grid, compiler_params, interpret_params.random_seed  # type: ignore[arg-type]\n   )\n \n+  parallel_dim_semantics = _get_parallel_dim_semantics(\n+      compiler_params, len(grid)\n+  )\n+  parallel_subgrid_size = _get_parallel_subgrid_size(\n+      parallel_dim_semantics, grid  # type: ignore[arg-type]\n+  )\n+  num_points_in_parallel_subgrid_per_core = (\n+      parallel_subgrid_size + interpret_params.num_cores_per_device - 1\n+  ) // interpret_params.num_cores_per_device  # We round up here.\n+  num_iterations_per_point_in_parallel_subgrid = (\n+      # This is evenly divisible.\n+      num_iterations // parallel_subgrid_size  # type: ignore[operator]\n+  )\n+  num_iterations_per_core = (\n+      num_points_in_parallel_subgrid_per_core\n+      * num_iterations_per_point_in_parallel_subgrid\n+  )\n+\n   def _get_local_grid_env(loop_idx):\n     if grid_mapping.local_grid_env is not None:\n       return grid_mapping.local_grid_env(loop_idx, grid)\n@@ -2153,20 +2191,20 @@ def body(\n         cur_start_indices,\n     ) = carry\n     if interpret_params.grid_point_recorder is not None:\n-      callback.io_callback(interpret_params.grid_point_recorder, (), grid_point)\n+      callback.io_callback(\n+          interpret_params.grid_point_recorder,\n+          (),\n+          grid_point,\n+          cur_local_core_id,\n+      )\n+\n+    next_local_core_id = (iteration_idx + 1) // num_iterations_per_core\n \n     with pallas_core.grid_env(_get_local_grid_env(loop_idx)):\n       next_loop_idx = _get_next_indices(grid, loop_idx)\n       next_grid_point = _get_grid_point(\n           next_loop_idx, randomized_grid_coordinates\n       )\n-      next_local_core_id = _get_next_local_core_id(\n-          cur_local_core_id,\n-          parallel_semantics_per_dim,\n-          grid_point,\n-          next_grid_point,\n-          interpret_params,\n-      )\n       next_start_indices = [\n           _compute_start_indices(\n               bm,\n@@ -2178,8 +2216,8 @@ def body(\n           )\n           for bm in grid_mapping.block_mappings\n       ]\n-      # Copy slices of the input to the kernel buffers.\n \n+      # Copy slices of the input to the kernel buffers.\n       def _store_slice_to_kernel_input(index, input_var):\n         # Copy from the HBM buffer for the pallas_call input to the kernel\n         # input buffer.\ndiff --git a/tests/pallas/tpu_pallas_interpret_test.py b/tests/pallas/tpu_pallas_interpret_test.py\nindex 014667d5f948..47d4ba3e1acf 100644\n--- a/tests/pallas/tpu_pallas_interpret_test.py\n+++ b/tests/pallas/tpu_pallas_interpret_test.py\n@@ -18,6 +18,8 @@\n contains only tests that do not use shard_map.\n \"\"\"\n \n+from collections.abc import Callable\n+import dataclasses\n import functools\n \n from absl.testing import absltest\n@@ -59,11 +61,18 @@ def num_stores(self):\n     return self._num_stores\n \n \n+@dataclasses.dataclass(frozen=True)\n+class ProcessedGridPoint():\n+  \"\"\"Represents a grid point and the ID of the core that has processed it.\"\"\"\n+  grid_point: tuple[int, ...]\n+  core_id: int\n+\n+\n class GridPointRecorderContext(object):\n-  \"\"\"Records grid points in the order in which they are traversed.\"\"\"\n+  \"\"\"Records grid points in the order in which they are procsessed.\"\"\"\n \n   def __init__(self):\n-    self._grid_points = []\n+    self._grid_points: list[ProcessedGridPoint] = []\n \n   def __enter__(self):\n     return self\n@@ -71,14 +80,17 @@ def __enter__(self):\n   def __exit__(self, ty, value, traceback):\n     ...\n \n-  def get_recorder(self):\n-    def _recorder(grid_point):\n-      self._grid_points.append(grid_point)\n+  def get_recorder(self) -> Callable[[tuple[np.int32, ...], np.int32], None]:\n+    def _recorder(grid_point, core_id):\n+      processed_grid_point = ProcessedGridPoint(\n+          tuple(int(coord) for coord in grid_point), int(core_id)\n+      )\n+      self._grid_points.append(processed_grid_point)\n \n     return _recorder\n \n   @property\n-  def grid_points(self):\n+  def grid_points(self) -> list[ProcessedGridPoint]:\n     return self._grid_points\n \n \n@@ -359,7 +371,7 @@ def kernel(s_ref, o_ref):\n       s_ref[0] = s + 1\n       o_ref[:] = jax.lax.full_like(o_ref, s)\n \n-    def kernel_call_dimensions_arbitrary_parallel(s, grid_point_recorder):\n+    def kernel_call_dimensions_parallel_arbitrary(s, grid_point_recorder):\n       return pl.pallas_call(\n           kernel,\n           out_shape=jax.ShapeDtypeStruct((32, 512), jnp.float32),\n@@ -370,13 +382,13 @@ def kernel_call_dimensions_arbitrary_parallel(s, grid_point_recorder):\n               random_seed=12345, grid_point_recorder=grid_point_recorder\n           ),\n           compiler_params=pltpu.TPUCompilerParams(\n-              dimension_semantics=('arbitrary', 'parallel')\n+              dimension_semantics=('parallel', 'arbitrary')\n           ),\n       )(s)\n \n     with GridPointRecorderContext() as grid_point_recorder:\n       result = jax.jit(\n-          kernel_call_dimensions_arbitrary_parallel, static_argnums=1\n+          kernel_call_dimensions_parallel_arbitrary, static_argnums=1\n       )(\n           jnp.zeros((1,), jnp.int32),\n           grid_point_recorder.get_recorder(),\n@@ -384,85 +396,55 @@ def kernel_call_dimensions_arbitrary_parallel(s, grid_point_recorder):\n       np.testing.assert_allclose(\n           result[::8, ::128],\n           [\n-              [ 2.0,  3.0,  0.0,  1.0],\n-              [ 6.0,  7.0,  4.0,  5.0],\n-              [10.0, 11.0,  8.0,  9.0],\n-              [14.0, 15.0, 12.0, 13.0],\n+              [ 8.0,  9.0, 10.0, 11.0],\n+              [12.0, 13.0, 14.0, 15.0],\n+              [ 0.0,  1.0,  2.0,  3.0],\n+              [ 4.0,  5.0,  6.0,  7.0],\n           ],\n       )\n-      np.testing.assert_array_equal(\n+      self.assertListEqual(\n           grid_point_recorder.grid_points,\n           [\n-              [0, 2],\n-              [0, 3],\n-              [0, 0],\n-              [0, 1],\n-              [1, 2],\n-              [1, 3],\n-              [1, 0],\n-              [1, 1],\n-              [2, 2],\n-              [2, 3],\n-              [2, 0],\n-              [2, 1],\n-              [3, 2],\n-              [3, 3],\n-              [3, 0],\n-              [3, 1],\n+              ProcessedGridPoint((2, 0), 0),\n+              ProcessedGridPoint((2, 1), 0),\n+              ProcessedGridPoint((2, 2), 0),\n+              ProcessedGridPoint((2, 3), 0),\n+              ProcessedGridPoint((3, 0), 0),\n+              ProcessedGridPoint((3, 1), 0),\n+              ProcessedGridPoint((3, 2), 0),\n+              ProcessedGridPoint((3, 3), 0),\n+              ProcessedGridPoint((0, 0), 0),\n+              ProcessedGridPoint((0, 1), 0),\n+              ProcessedGridPoint((0, 2), 0),\n+              ProcessedGridPoint((0, 3), 0),\n+              ProcessedGridPoint((1, 0), 0),\n+              ProcessedGridPoint((1, 1), 0),\n+              ProcessedGridPoint((1, 2), 0),\n+              ProcessedGridPoint((1, 3), 0),\n           ],\n       )\n \n-    def kernel_call_dimensions_parallel_arbitrary(s, grid_point_recorder):\n+  def test_dimensions_arbitrary_parallel_raises(self):\n+    def kernel_call(s):\n+      def kernel(s_ref, o_ref):\n+        s = s_ref[0]\n+        o_ref[0] = s\n+\n       return pl.pallas_call(\n           kernel,\n           out_shape=jax.ShapeDtypeStruct((32, 512), jnp.float32),\n           grid=(4, 4),\n           in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n           out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n-          interpret=mosaic_interpret.TPUInterpretParams(\n-              random_seed=12345, grid_point_recorder=grid_point_recorder\n-          ),\n+          interpret=mosaic_interpret.TPUInterpretParams(random_seed=12345),\n           compiler_params=pltpu.TPUCompilerParams(\n-              dimension_semantics=('parallel', 'arbitrary')\n+              dimension_semantics=('arbitrary', 'parallel')\n           ),\n       )(s)\n \n-    with GridPointRecorderContext() as grid_point_recorder:\n-      result = jax.jit(\n-          kernel_call_dimensions_parallel_arbitrary, static_argnums=1\n-      )(\n+    with self.assertRaises(ValueError):\n+      jax.jit(kernel_call)(\n           jnp.zeros((1,), jnp.int32),\n-          grid_point_recorder.get_recorder(),\n-      )\n-      np.testing.assert_allclose(\n-          result[::8, ::128],\n-          [\n-              [ 8.0,  9.0, 10.0, 11.0],\n-              [12.0, 13.0, 14.0, 15.0],\n-              [ 0.0,  1.0,  2.0,  3.0],\n-              [ 4.0,  5.0,  6.0,  7.0],\n-          ],\n-      )\n-      np.testing.assert_array_equal(\n-          grid_point_recorder.grid_points,\n-          [\n-              [2, 0],\n-              [2, 1],\n-              [2, 2],\n-              [2, 3],\n-              [3, 0],\n-              [3, 1],\n-              [3, 2],\n-              [3, 3],\n-              [0, 0],\n-              [0, 1],\n-              [0, 2],\n-              [0, 3],\n-              [1, 0],\n-              [1, 1],\n-              [1, 2],\n-              [1, 3],\n-          ],\n       )\n \n   def test_dynamic_parallel_dimension_raises(self):\n@@ -583,6 +565,262 @@ def kernel(x_ref, o_ref, vmem_ref):\n     self.assertFalse(mosaic_interpret.races.races_found)\n     np.testing.assert_allclose(y, 2.0 * x)\n \n+  def test_parallel_dimension_and_multiple_cores(self):\n+    def kernel(s_ref, o_ref):\n+      s = s_ref[0]\n+      s_ref[0] = s + 1\n+      o_ref[:] = jax.lax.full_like(o_ref, s)\n+\n+    def kernel_call(s, num_cores_per_device, grid_point_recorder):\n+      return pl.pallas_call(\n+          kernel,\n+          out_shape=jax.ShapeDtypeStruct((32, 512), jnp.float32),\n+          grid=(4, 4),\n+          in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n+          out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n+          interpret=mosaic_interpret.TPUInterpretParams(\n+              random_seed=12345,\n+              num_cores_per_device=num_cores_per_device,\n+              grid_point_recorder=grid_point_recorder,\n+          ),\n+          compiler_params=pltpu.TPUCompilerParams(\n+              dimension_semantics=('parallel', 'arbitrary')\n+          ),\n+      )(s)\n+\n+    with self.subTest('num_cores_per_device=1'):\n+      with GridPointRecorderContext() as grid_point_recorder:\n+        result = jax.jit(kernel_call, static_argnums=(1, 2))(\n+            jnp.zeros((1,), jnp.int32), 1, grid_point_recorder.get_recorder()\n+        )\n+        np.testing.assert_allclose(\n+            result[::8, ::128],\n+            [\n+                [8.0, 9.0, 10.0, 11.0],\n+                [12.0, 13.0, 14.0, 15.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [4.0, 5.0, 6.0, 7.0],\n+            ],\n+        )\n+        self.assertListEqual(\n+            grid_point_recorder.grid_points,\n+            # parallel_subgrid_size = 4\n+            # num_parallel_points_per_core = (4 + 1 - 1) // 1 = 4\n+            # num_iterations_per_core = 4 * (16 // 4) = 16\n+            [\n+                ProcessedGridPoint((2, 0), 0),\n+                ProcessedGridPoint((2, 1), 0),\n+                ProcessedGridPoint((2, 2), 0),\n+                ProcessedGridPoint((2, 3), 0),\n+                ProcessedGridPoint((3, 0), 0),\n+                ProcessedGridPoint((3, 1), 0),\n+                ProcessedGridPoint((3, 2), 0),\n+                ProcessedGridPoint((3, 3), 0),\n+                ProcessedGridPoint((0, 0), 0),\n+                ProcessedGridPoint((0, 1), 0),\n+                ProcessedGridPoint((0, 2), 0),\n+                ProcessedGridPoint((0, 3), 0),\n+                ProcessedGridPoint((1, 0), 0),\n+                ProcessedGridPoint((1, 1), 0),\n+                ProcessedGridPoint((1, 2), 0),\n+                ProcessedGridPoint((1, 3), 0),\n+            ],\n+        )\n+\n+    with self.subTest('num_cores_per_device=2'):\n+      with GridPointRecorderContext() as grid_point_recorder:\n+        result = jax.jit(kernel_call, static_argnums=(1, 2))(\n+            jnp.zeros((1,), jnp.int32), 2, grid_point_recorder.get_recorder()\n+        )\n+        np.testing.assert_allclose(\n+            result[::8, ::128],\n+            [\n+                [0.0, 1.0, 2.0, 3.0],\n+                [4.0, 5.0, 6.0, 7.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [4.0, 5.0, 6.0, 7.0],\n+            ],\n+        )\n+        self.assertListEqual(\n+            grid_point_recorder.grid_points,\n+            # parallel_subgrid_size = 4\n+            # num_parallel_points_per_core = (4 + 2 - 1) // 2 = 2\n+            # num_iterations_per_core = 2 * (16 // 4) = 8\n+            [\n+                ProcessedGridPoint((2, 0), 0),\n+                ProcessedGridPoint((2, 1), 0),\n+                ProcessedGridPoint((2, 2), 0),\n+                ProcessedGridPoint((2, 3), 0),\n+                ProcessedGridPoint((3, 0), 0),\n+                ProcessedGridPoint((3, 1), 0),\n+                ProcessedGridPoint((3, 2), 0),\n+                ProcessedGridPoint((3, 3), 0),\n+                ProcessedGridPoint((0, 0), 1),\n+                ProcessedGridPoint((0, 1), 1),\n+                ProcessedGridPoint((0, 2), 1),\n+                ProcessedGridPoint((0, 3), 1),\n+                ProcessedGridPoint((1, 0), 1),\n+                ProcessedGridPoint((1, 1), 1),\n+                ProcessedGridPoint((1, 2), 1),\n+                ProcessedGridPoint((1, 3), 1),\n+            ],\n+        )\n+\n+    with self.subTest('num_cores_per_device=3'):\n+      with GridPointRecorderContext() as grid_point_recorder:\n+        result = jax.jit(kernel_call, static_argnums=(1, 2))(\n+            jnp.zeros((1,), jnp.int32), 3, grid_point_recorder.get_recorder()\n+        )\n+        np.testing.assert_allclose(\n+            result[::8, ::128],\n+            [\n+                [0.0, 1.0, 2.0, 3.0],\n+                [4.0, 5.0, 6.0, 7.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [4.0, 5.0, 6.0, 7.0],\n+            ],\n+        )\n+        self.assertListEqual(\n+            grid_point_recorder.grid_points,\n+            # parallel_subgrid_size = 4\n+            # num_parallel_points_per_core = (4 + 3 - 1) // 3 = 2\n+            # num_iterations_per_core = 2 * (16 // 4) = 8\n+            [\n+                ProcessedGridPoint((2, 0), 0),\n+                ProcessedGridPoint((2, 1), 0),\n+                ProcessedGridPoint((2, 2), 0),\n+                ProcessedGridPoint((2, 3), 0),\n+                ProcessedGridPoint((3, 0), 0),\n+                ProcessedGridPoint((3, 1), 0),\n+                ProcessedGridPoint((3, 2), 0),\n+                ProcessedGridPoint((3, 3), 0),\n+                ProcessedGridPoint((0, 0), 1),\n+                ProcessedGridPoint((0, 1), 1),\n+                ProcessedGridPoint((0, 2), 1),\n+                ProcessedGridPoint((0, 3), 1),\n+                ProcessedGridPoint((1, 0), 1),\n+                ProcessedGridPoint((1, 1), 1),\n+                ProcessedGridPoint((1, 2), 1),\n+                ProcessedGridPoint((1, 3), 1),\n+            ],\n+        )\n+\n+    with self.subTest('num_cores_per_device=4'):\n+      with GridPointRecorderContext() as grid_point_recorder:\n+        result = jax.jit(kernel_call, static_argnums=(1, 2))(\n+            jnp.zeros((1,), jnp.int32), 4, grid_point_recorder.get_recorder()\n+        )\n+        np.testing.assert_allclose(\n+            result[::8, ::128],\n+            [\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+            ],\n+        )\n+        self.assertListEqual(\n+            grid_point_recorder.grid_points,\n+            # parallel_subgrid_size = 4\n+            # num_parallel_points_per_core = (4 + 4 - 1) // 4 = 1\n+            # num_iterations_per_core = 1 * (16 // 4) = 4\n+            [\n+                ProcessedGridPoint((2, 0), 0),\n+                ProcessedGridPoint((2, 1), 0),\n+                ProcessedGridPoint((2, 2), 0),\n+                ProcessedGridPoint((2, 3), 0),\n+                ProcessedGridPoint((3, 0), 1),\n+                ProcessedGridPoint((3, 1), 1),\n+                ProcessedGridPoint((3, 2), 1),\n+                ProcessedGridPoint((3, 3), 1),\n+                ProcessedGridPoint((0, 0), 2),\n+                ProcessedGridPoint((0, 1), 2),\n+                ProcessedGridPoint((0, 2), 2),\n+                ProcessedGridPoint((0, 3), 2),\n+                ProcessedGridPoint((1, 0), 3),\n+                ProcessedGridPoint((1, 1), 3),\n+                ProcessedGridPoint((1, 2), 3),\n+                ProcessedGridPoint((1, 3), 3),\n+            ],\n+        )\n+\n+    with self.subTest('num_cores_per_device=5'):\n+      with GridPointRecorderContext() as grid_point_recorder:\n+        result = jax.jit(kernel_call, static_argnums=(1, 2))(\n+            jnp.zeros((1,), jnp.int32), 5, grid_point_recorder.get_recorder()\n+        )\n+        np.testing.assert_allclose(\n+            result[::8, ::128],\n+            [\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+            ],\n+        )\n+        self.assertListEqual(\n+            grid_point_recorder.grid_points,\n+            # parallel_subgrid_size = 4\n+            # num_parallel_points_per_core = (4 + 5 - 1) // 5 = 1\n+            # num_iterations_per_core = 1 * (16 // 4) = 4\n+            [\n+                ProcessedGridPoint((2, 0), 0),\n+                ProcessedGridPoint((2, 1), 0),\n+                ProcessedGridPoint((2, 2), 0),\n+                ProcessedGridPoint((2, 3), 0),\n+                ProcessedGridPoint((3, 0), 1),\n+                ProcessedGridPoint((3, 1), 1),\n+                ProcessedGridPoint((3, 2), 1),\n+                ProcessedGridPoint((3, 3), 1),\n+                ProcessedGridPoint((0, 0), 2),\n+                ProcessedGridPoint((0, 1), 2),\n+                ProcessedGridPoint((0, 2), 2),\n+                ProcessedGridPoint((0, 3), 2),\n+                ProcessedGridPoint((1, 0), 3),\n+                ProcessedGridPoint((1, 1), 3),\n+                ProcessedGridPoint((1, 2), 3),\n+                ProcessedGridPoint((1, 3), 3),\n+            ],\n+        )\n+\n+    with self.subTest('num_cores_per_device=6'):\n+      with GridPointRecorderContext() as grid_point_recorder:\n+        result = jax.jit(kernel_call, static_argnums=(1, 2))(\n+            jnp.zeros((1,), jnp.int32), 6, grid_point_recorder.get_recorder()\n+        )\n+        np.testing.assert_allclose(\n+            result[::8, ::128],\n+            [\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+            ],\n+        )\n+        self.assertListEqual(\n+            grid_point_recorder.grid_points,\n+            # parallel_subgrid_size = 4\n+            # num_parallel_points_per_core = (4 + 6 - 1) // 6 = 1\n+            # num_iterations_per_core = 1 * (16 // 4) = 4\n+            [\n+                ProcessedGridPoint((2, 0), 0),\n+                ProcessedGridPoint((2, 1), 0),\n+                ProcessedGridPoint((2, 2), 0),\n+                ProcessedGridPoint((2, 3), 0),\n+                ProcessedGridPoint((3, 0), 1),\n+                ProcessedGridPoint((3, 1), 1),\n+                ProcessedGridPoint((3, 2), 1),\n+                ProcessedGridPoint((3, 3), 1),\n+                ProcessedGridPoint((0, 0), 2),\n+                ProcessedGridPoint((0, 1), 2),\n+                ProcessedGridPoint((0, 2), 2),\n+                ProcessedGridPoint((0, 3), 2),\n+                ProcessedGridPoint((1, 0), 3),\n+                ProcessedGridPoint((1, 1), 3),\n+                ProcessedGridPoint((1, 2), 3),\n+                ProcessedGridPoint((1, 3), 3),\n+            ],\n+        )\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move jax/_src/custom_derivatives.py to its own BUILD rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nThis was unblocked by moving ad, batching, and custom_transpose to their own rules in prior changes. It required one small code refactoring: moving an effects registration to the location where the effect is defined.\n\nPiperOrigin-RevId: 763736189",
        "output": "```diff\nCommit: c13de5cb83c33c7c4d9a3bbbdfd35478e5bc91eb\nDate: 2025-05-27T11:50:50Z\nURL: https://github.com/jax-ml/jax/commit/c13de5cb83c33c7c4d9a3bbbdfd35478e5bc91eb\nFiles changed: 8\nAdditions: +31, Deletions: -4\ndiff --git a/jax/BUILD b/jax/BUILD\nindex a236cd206a55..396e6fdf6ed4 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -303,7 +303,6 @@ py_library_providing_imports_info(\n         \"_src/callback.py\",\n         \"_src/checkify.py\",\n         \"_src/custom_batching.py\",\n-        \"_src/custom_derivatives.py\",\n         \"_src/custom_partitioning.py\",\n         \"_src/custom_partitioning_sharding_rule.py\",\n         \"_src/debugging.py\",\n@@ -388,6 +387,7 @@ py_library_providing_imports_info(\n         \":core\",\n         \":custom_api_util\",\n         \":custom_dce\",\n+        \":custom_derivatives\",\n         \":custom_transpose\",\n         \":deprecations\",\n         \":dtypes\",\n@@ -613,6 +613,30 @@ pytype_strict_library(\n     ],\n )\n \n+pytype_strict_library(\n+    name = \"custom_derivatives\",\n+    srcs = [\"_src/custom_derivatives.py\"],\n+    deps = [\n+        \":ad\",\n+        \":ad_util\",\n+        \":api_util\",\n+        \":batching\",\n+        \":config\",\n+        \":core\",\n+        \":custom_api_util\",\n+        \":custom_transpose\",\n+        \":dtypes\",\n+        \":effects\",\n+        \":mlir\",\n+        \":partial_eval\",\n+        \":state_types\",\n+        \":traceback_util\",\n+        \":tree_util\",\n+        \":util\",\n+        \":xla\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"custom_transpose\",\n     srcs = [\"_src/custom_transpose.py\"],\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex d76d145fd0a6..2a09665f6285 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -40,7 +40,6 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import xla\n from jax._src.interpreters.batching import not_mapped\n-from jax._src.lax import lax\n from jax._src.tree_util import (\n     tree_flatten, tree_unflatten, tree_map, treedef_is_leaf, treedef_tuple,\n     register_pytree_node_class, tree_leaves, tree_flatten_with_path,\n@@ -410,8 +409,6 @@ def jvp(*xs):\n     return [*out_primals, *out_tangents]\n   return lu.wrap_init(jvp, debug_info=jvp_jaxpr_fun.debug_info)\n \n-effects.custom_derivatives_allowed_effects.add_type(lax.InOutFeedEffect)\n-\n custom_jvp_call_p = CustomJVPCallPrimitive('custom_jvp_call')\n \n def _custom_jvp_call_typecheck(_, *in_avals, call_jaxpr, jvp_jaxpr_fun,\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex a49c27d06eee..a9d81c684297 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -8188,6 +8188,7 @@ class InOutFeedEffect(effects.Effect):\n infeed_effect = InOutFeedEffect()\n outfeed_effect = InOutFeedEffect()\n \n+effects.custom_derivatives_allowed_effects.add_type(InOutFeedEffect)\n \n def infeed(token, shape=None, partitions=None):\n   \"\"\"Consumes an infeed value of `shape` from the host. Experimental.\ndiff --git a/jax/_src/pallas/fuser/BUILD b/jax/_src/pallas/fuser/BUILD\nindex a62a9937d91d..a4c3402f5309 100644\n--- a/jax/_src/pallas/fuser/BUILD\n+++ b/jax/_src/pallas/fuser/BUILD\n@@ -50,6 +50,7 @@ pytype_strict_library(\n         \"//jax:ad_util\",\n         \"//jax:api_util\",\n         \"//jax:core\",\n+        \"//jax:custom_derivatives\",\n         \"//jax:partial_eval\",\n         \"//jax:tree_util\",\n         \"//jax:util\",\ndiff --git a/jax/_src/pallas/mosaic/BUILD b/jax/_src/pallas/mosaic/BUILD\nindex fdd3a56ac7c8..83525f11d3cf 100644\n--- a/jax/_src/pallas/mosaic/BUILD\n+++ b/jax/_src/pallas/mosaic/BUILD\n@@ -103,6 +103,7 @@ py_library(\n         \"//jax\",\n         \"//jax:ad_util\",\n         \"//jax:core\",\n+        \"//jax:custom_derivatives\",\n         \"//jax:dtypes\",\n         \"//jax:mesh\",\n         \"//jax:mlir\",\ndiff --git a/jax/_src/pallas/triton/BUILD b/jax/_src/pallas/triton/BUILD\nindex 2b8ee4eaa8f2..acbc11a60039 100644\n--- a/jax/_src/pallas/triton/BUILD\n+++ b/jax/_src/pallas/triton/BUILD\n@@ -63,6 +63,7 @@ pytype_strict_library(\n         \"//jax:api_util\",\n         \"//jax:config\",\n         \"//jax:core\",\n+        \"//jax:custom_derivatives\",\n         \"//jax:mlir\",\n         \"//jax:partial_eval\",\n         \"//jax:source_info_util\",\ndiff --git a/jax/extend/BUILD b/jax/extend/BUILD\nindex 06fb8e671120..6dc5d7d76311 100644\n--- a/jax/extend/BUILD\n+++ b/jax/extend/BUILD\n@@ -46,6 +46,7 @@ py_library_providing_imports_info(\n         \"//jax:ad\",\n         \"//jax:ad_util\",\n         \"//jax:core\",\n+        \"//jax:custom_derivatives\",\n     ],\n )\n \ndiff --git a/tests/BUILD b/tests/BUILD\nindex 6fac61933d59..e3672eb73f48 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -49,6 +49,7 @@ jax_multiplatform_test(\n     srcs = [\"custom_api_test.py\"],\n     shard_count = 10,\n     deps = [\n+        \"//jax:custom_derivatives\",\n         \"//jax:experimental\",\n     ] + py_deps([\n         \"absl/testing\",\n\n```",
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] `Barrier` and `ClusterBarrier` are now `kw_only=True`\n\nPiperOrigin-RevId: 763730217",
        "output": "```diff\nCommit: 4f717d31c5b94bcfb742d1a9aaf0024cca7ccd6c\nDate: 2025-05-27T11:30:22Z\nURL: https://github.com/jax-ml/jax/commit/4f717d31c5b94bcfb742d1a9aaf0024cca7ccd6c\nFiles changed: 4\nAdditions: +35, Deletions: -35\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 08e47cec4b2e..2fca1464ee0b 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -850,7 +850,7 @@ def __str__(self):\n     return self.name\n \n \n-@dataclasses.dataclass(frozen=True)\n+@dataclasses.dataclass(frozen=True, kw_only=True)\n class Barrier:\n   \"\"\"Describes a barrier Ref.\n \n@@ -862,9 +862,9 @@ class Barrier:\n       the tensor core. This should be set to True when waiting on Blackwell\n       (TC Gen 5) asynchoronous matmul instructions.\n   \"\"\"\n-  num_arrivals: int\n+  num_arrivals: int = 1\n   num_barriers: int = 1\n-  for_tensor_core: bool = dataclasses.field(default=False, kw_only=True)\n+  for_tensor_core: bool = False\n \n   def get_ref_aval(self) -> AbstractMemoryRef:\n     aval = jax_core.ShapedArray(\n@@ -879,7 +879,7 @@ def __post_init__(self):\n           f\"Num arrivals must be at least 1, but got {self.num_arrivals}\"\n       )\n \n-@dataclasses.dataclass(frozen=True)\n+@dataclasses.dataclass(frozen=True, kw_only=True)\n class ClusterBarrier:\n   collective_axes: tuple[str | tuple[str, ...], ...]\n   num_barriers: int = 1\ndiff --git a/jax/_src/pallas/mosaic_gpu/pipeline.py b/jax/_src/pallas/mosaic_gpu/pipeline.py\nindex 4966eec7fa6d..a7f8d32677b0 100644\n--- a/jax/_src/pallas/mosaic_gpu/pipeline.py\n+++ b/jax/_src/pallas/mosaic_gpu/pipeline.py\n@@ -230,7 +230,7 @@ def pipeline(*gmem_refs: pallas_core.AbstractMemoryRef):\n         ],\n         [len(in_specs)],\n     )\n-    arrival_count = sum(map(_in_smem, in_specs))\n+    num_arrivals = sum(map(_in_smem, in_specs))\n     return pl.run_scoped(\n         functools.partial(\n             scoped_pipeline,\n@@ -240,10 +240,10 @@ def pipeline(*gmem_refs: pallas_core.AbstractMemoryRef):\n         in_smem_refs=in_smem_refs,\n         out_smem_refs=out_smem_refs,\n         barrier_ref=None\n-        if arrival_count == 0\n+        if num_arrivals == 0\n         else gpu_core.Barrier(\n             # TODO(slebedev): Change this to arrive only once.\n-            arrival_count,\n+            num_arrivals=num_arrivals,\n             num_barriers=max_concurrent_steps,\n         ),\n     )\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex c7e9f95e3f99..447e3affd7c1 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -278,9 +278,9 @@ def entry(q_ref, k_ref, v_ref, out_ref, lse_ref):\n         lambda *args: kernel(q_ref, k_ref, v_ref, out_ref, lse_ref, args),\n         scratch,\n         (\n-            plgpu.Barrier(1, num_barriers=max_concurrent_steps),\n-            plgpu.Barrier(1, num_barriers=max_concurrent_steps),\n-            plgpu.Barrier(1, num_barriers=compute_wgs),\n+            plgpu.Barrier(num_barriers=max_concurrent_steps),\n+            plgpu.Barrier(num_barriers=max_concurrent_steps),\n+            plgpu.Barrier(num_barriers=compute_wgs),\n         ),\n         (plgpu.Barrier(num_arrivals=compute_wgs, num_barriers=max_concurrent_steps),) * 2,\n         plgpu.Barrier(num_arrivals=compute_wgs),\n@@ -587,7 +587,7 @@ def compute_dk(acc_ref):\n       out_shape=q,\n       scratch_shapes=[\n           (q_scratch, do_scratch, lse_scratch, delta_scratch),  # type: ignore\n-          (plgpu.Barrier(1, num_barriers=compute_wgs),) * 4  # type: ignore\n+          (plgpu.Barrier(num_barriers=compute_wgs),) * 4  # type: ignore\n       ],\n       compiler_params=plgpu.CompilerParams(approx_math=True),\n       grid=(batch_size, num_q_tiles, num_q_heads),\n@@ -608,7 +608,7 @@ def compute_dk(acc_ref):\n     out_shape=[out_shape_kv, out_shape_kv],\n     scratch_shapes=[\n         (k_scratch, v_scratch),  # type: ignore\n-        (plgpu.Barrier(1, num_barriers=compute_wgs),) * 2  # type: ignore\n+        (plgpu.Barrier(num_barriers=compute_wgs),) * 2  # type: ignore\n   ],\n     compiler_params=plgpu.CompilerParams(approx_math=True),\n     grid=(batch_size, num_kv_tiles, num_q_heads),\n@@ -776,7 +776,7 @@ def compute_pv(acc_ref):\n             out_shape=out_shape,\n       scratch_shapes=(\n           tuple(smem_scratch),  # type: ignore\n-          plgpu.Barrier(1, num_barriers=compute_wgs),  # type: ignore\n+          plgpu.Barrier(num_barriers=compute_wgs),  # type: ignore\n           plgpu.Barrier(num_arrivals=compute_wgs),),  # type: ignore\n       compiler_params=plgpu.CompilerParams(\n           approx_math=True, lowering_semantics=plgpu.LoweringSemantics.Warpgroup,\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 7430fcc1ca53..608cfcba2465 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -408,7 +408,7 @@ def test_inline_mgpu(self):\n                 dtype,\n                 transforms=transforms,\n             ),\n-            plgpu.Barrier(num_arrivals=1),\n+            plgpu.Barrier(),\n         ],\n         out_specs=pl.BlockSpec(memory_space=plgpu.GMEM),\n     )\n@@ -540,7 +540,7 @@ def test_copy_gmem_to_smem(self, indexer):\n         in_specs=(pl.BlockSpec(memory_space=plgpu.GMEM),),\n         scratch_shapes=[\n             plgpu.SMEM((256,), jnp.float32),\n-            plgpu.Barrier(num_arrivals=1),\n+            plgpu.Barrier(),\n         ],\n     )\n     def kernel(x_ref_gmem, o_ref, scratch_ref, barrier_ref):\n@@ -586,7 +586,7 @@ def test_copy_gmem_to_smem_with_multiple_gmem_indexers(self, shape, indexers):\n         in_specs=(pl.BlockSpec(memory_space=plgpu.GMEM),),\n         scratch_shapes=[\n             plgpu.SMEM(shape, jnp.float32),\n-            plgpu.Barrier(num_arrivals=1),\n+            plgpu.Barrier(),\n         ],\n         grid=(1,),\n     )\n@@ -617,7 +617,7 @@ def test_gmem_to_smem_with_multiple_smem_indexers(self):\n         in_specs=(pl.BlockSpec(memory_space=plgpu.GMEM),),\n         scratch_shapes=[\n             plgpu.SMEM(x.shape, jnp.float32),\n-            plgpu.Barrier(num_arrivals=1),\n+            plgpu.Barrier(),\n         ],\n     )\n     def extract_x0(x_ref_gmem, o_ref, scratch_ref, barrier_ref):\n@@ -672,7 +672,7 @@ def test_copy_gmem_to_smem_with_indexed_barrier(self, indexer):\n         in_specs=(pl.BlockSpec(memory_space=plgpu.GMEM),),\n         scratch_shapes=[\n             plgpu.SMEM((128,), jnp.float32),\n-            plgpu.Barrier(num_arrivals=1, num_barriers=4),\n+            plgpu.Barrier(num_barriers=4),\n         ],\n     )\n     def kernel(x_ref_gmem, o_ref, scratch_ref, barrier_ref):\n@@ -713,7 +713,7 @@ def kernel(x_ref, o_ref, barrier_ref):\n         out_shape=jax.ShapeDtypeStruct([128, 128], jnp.float32),\n         in_specs=(in_spec,),\n         out_specs=out_spec,\n-        scratch_shapes=[plgpu.Barrier(num_arrivals=1)],\n+        scratch_shapes=[plgpu.Barrier()],\n     )\n     x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n     np.testing.assert_array_equal(f(x), x)\n@@ -736,7 +736,7 @@ def body(tmp_ref):\n         out_shape=jax.ShapeDtypeStruct([128, 128], jnp.float32),\n         in_specs=(in_spec,),\n         out_specs=out_spec,\n-        scratch_shapes=[plgpu.Barrier(num_arrivals=1)],\n+        scratch_shapes=[plgpu.Barrier()],\n     )\n     x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n     np.testing.assert_array_equal(f(x), x * 2)\n@@ -756,7 +756,7 @@ def body(tmp_ref):\n         kernel,\n         out_shape=jax.ShapeDtypeStruct([128, 128], jnp.float32),\n         in_specs=(in_spec,),\n-        scratch_shapes=[plgpu.Barrier(num_arrivals=1)],\n+        scratch_shapes=[plgpu.Barrier()],\n     )\n     x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n     np.testing.assert_array_equal(f(x), x * 2)\n@@ -783,7 +783,7 @@ def kernel(x_ref, o_ref, barrier_ref):\n         out_shape=jax.ShapeDtypeStruct([2, 128, 128], jnp.float32),\n         in_specs=(in_spec,),\n         out_specs=out_spec,\n-        scratch_shapes=[plgpu.Barrier(num_arrivals=1)],\n+        scratch_shapes=[plgpu.Barrier()],\n     )\n     x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n     np.testing.assert_array_equal(f(x), np.stack([x, x], axis=0))\n@@ -827,7 +827,7 @@ def kernel(x_ref, o_ref, barrier_ref):\n         out_shape=jax.ShapeDtypeStruct([2, 64, 2, 128], jnp.float32),\n         in_specs=(in_spec,),\n         out_specs=out_spec,\n-        scratch_shapes=[plgpu.Barrier(num_arrivals=1)],\n+        scratch_shapes=[plgpu.Barrier()],\n     )\n     x = jnp.arange(2 * 64 * 128, dtype=jnp.float32).reshape(2, 64, 128)\n     xt = x.transpose((1, 0, 2))\n@@ -847,7 +847,7 @@ def inner_body(scratch_ref):\n           plgpu.barrier_wait(barrier_ref)\n           o_ref[...] = scratch_ref[...] + 1\n         pl.run_scoped(inner_body, plgpu.SMEM((256,), jnp.float32))\n-      pl.run_scoped(body, plgpu.Barrier(num_arrivals=1))\n+      pl.run_scoped(body, plgpu.Barrier())\n \n     x = jnp.arange(256).astype(jnp.float32)\n     np.testing.assert_array_equal(kernel(x), x + 1.0)\n@@ -1016,7 +1016,7 @@ def test_get_swap_with_transforms(self, *transforms):\n         out_shape=jax.ShapeDtypeStruct(shape, jnp.int32),\n         scratch_shapes=[\n             plgpu.SMEM(shape, jnp.int32, transforms=tuple(transforms)),\n-            plgpu.Barrier(num_arrivals=1),\n+            plgpu.Barrier(),\n         ]\n     )\n     def kernel(x_ref, o_ref, scratch_ref, barrier_ref):\n@@ -1089,7 +1089,7 @@ def scoped_kernel(barrier_ref):\n         plgpu.barrier_wait(barrier_ref)\n \n       def branch():\n-        pl.run_scoped(scoped_kernel, plgpu.Barrier(num_arrivals=1))\n+        pl.run_scoped(scoped_kernel, plgpu.Barrier())\n \n       jax.lax.cond(x_ref_gmem[0] % 2 == 0, branch, branch)\n \n@@ -1698,7 +1698,7 @@ def kernel(x_gmem, o_gmem):\n               plgpu.SMEM(shape, large_ty, transforms=(tiling, large_swizzle)),\n               plgpu.SMEM(shape, small_ty, transforms=(tiling, small_swizzle))\n           ),\n-          plgpu.Barrier(1, num_barriers=1),\n+          plgpu.Barrier(num_barriers=1),\n       )\n \n     def scoped_kernel(x_gmem, o_gmem, aliased_ref, barrier):\n@@ -1780,7 +1780,7 @@ def body(x_ref, y_ref, barrier):\n           in_specs=[pl.BlockSpec(memory_space=plgpu.GMEM)],\n           out_specs=pl.BlockSpec(memory_space=plgpu.SMEM),\n           out_shape=x,\n-          scratch_shapes=[plgpu.Barrier(1)],\n+          scratch_shapes=[plgpu.Barrier()],\n       )(x)\n     except:\n       # assertRaisesRegex raises does not let us match the traceback.\n@@ -1921,7 +1921,7 @@ def _():\n         plgpu.wait_smem_to_gmem(0)\n       pl.run_scoped(scope,\n                     smem_ref=plgpu.SMEM((32, 32), jnp.float32),\n-                    tma_barrier=plgpu.Barrier(num_arrivals=1))\n+                    tma_barrier=plgpu.Barrier())\n     x = jax.random.uniform(jax.random.key(42), (64, 32), jnp.float32)\n     result = kernel(x)\n     np.testing.assert_array_equal(result, x[32:64])\n@@ -2345,7 +2345,7 @@ def test_tmem(self):\n             plgpu.TMEM((128, 128), jnp.float32),\n             plgpu.TMEM((128, 128), jnp.float32),\n             plgpu.SMEM((128, 128), jnp.float32, transforms=transforms),\n-            plgpu.Barrier(num_arrivals=1),\n+            plgpu.Barrier(),\n         ],\n         num_threads=1,\n         thread_name=\"x\",\n@@ -2416,7 +2416,7 @@ def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref,\n     scratch_shapes = [\n         plgpu.TMEM(shape, jnp.float32, packed=False),\n         plgpu.SMEM(shape, dtype, transforms=transforms),\n-        plgpu.Barrier(num_arrivals=1, for_tensor_core=True),\n+        plgpu.Barrier(for_tensor_core=True),\n     ]\n     if lhs_tmem:\n       scratch_shapes.append(plgpu.TMEM(shape, dtype, packed=True))\n@@ -2478,8 +2478,8 @@ def kernel(a_gmem, b_gmem, out_gmem):\n         b_smem=plgpu.SMEM(_rhs_shape, dtype, transforms=transforms),\n         acc_tmem=plgpu.TMEM(_acc_shape, jnp.float32, collective=True),\n         scratch_smem=plgpu.SMEM(_acc_shape, dtype, transforms=transforms),\n-        tma_barrier=plgpu.Barrier(num_arrivals=1),\n-        mma_barrier=plgpu.Barrier(num_arrivals=1, for_tensor_core=True),\n+        tma_barrier=plgpu.Barrier(),\n+        mma_barrier=plgpu.Barrier(for_tensor_core=True),\n         cluster_barrier=plgpu.ClusterBarrier(collective_axes=(\"x\",)),\n       )\n       def _scoped(a_smem, b_smem,\n@@ -2546,7 +2546,7 @@ def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref):\n     scratch_shapes = [\n         plgpu.TMEM(shape, jnp.float32, packed=False),\n         plgpu.SMEM(shape, dtype, transforms=transforms),\n-        plgpu.Barrier(num_arrivals=1, num_barriers=2, for_tensor_core=True),\n+        plgpu.Barrier(num_barriers=2, for_tensor_core=True),\n     ]\n     f = self.pallas_call(\n         kernel,\n@@ -2612,7 +2612,7 @@ def kernel(x_gmem, o_gmem):\n           functools.partial(scoped_kernel, x_gmem, o_gmem),\n           plgpu.SMEM((max_concurrent_steps, 32, 16), jnp.float32),\n           plgpu.SMEM((max_concurrent_steps, 32, 16), jnp.float32),\n-          plgpu.Barrier(1, num_barriers=max_concurrent_steps),\n+          plgpu.Barrier(num_barriers=max_concurrent_steps),\n       )\n \n     def scoped_kernel(x_gmem, o_gmem, x_smem, o_smem, barrier):\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Add a missing warpgroup barrier before warp core_map\n\nIf we don't synchronize the warps, some of them can go on and schedule\ne.g. async copies without waiting for the memory transactions of other\nwarps in the warpgroup to complete.\n\nPiperOrigin-RevId: 763721411",
        "output": "```diff\nCommit: 9a7f9f13efdb5129a861b44263ab3024fa95ceb6\nDate: 2025-05-27T11:00:55Z\nURL: https://github.com/jax-ml/jax/commit/9a7f9f13efdb5129a861b44263ab3024fa95ceb6\nFiles changed: 1\nAdditions: +5, Deletions: -0\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex b9a3aa17c39d..cf867e55f4c9 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -2793,6 +2793,10 @@ def _core_map_lowering_rule(\n           \"Can only close over scalars and Refs when using core_map with \"\n           f\"WarpMesh. Found array of shape {aval_in}.\"\n         )\n+    # We allow the warps to schedule async copies without synchronizing with\n+    # other warps, so we need to add a barrier here to make sure all reads and\n+    # writes have completed.\n+    mgpu.warpgroup_barrier()\n     _ = lower_jaxpr_to_mosaic_gpu(\n         module_ctx,\n         ctx.launch_ctx,\n@@ -2800,6 +2804,7 @@ def _core_map_lowering_rule(\n         args=(),\n         consts=args,\n     )\n+    # TODO(apaszke,justinfu): Do we really need this barrier?\n     mgpu.warpgroup_barrier()\n     return []\n   raise ValueError(f\"Unsupported mesh: {mesh}\")\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Make sure that lowering errors mention the offending line\n\nI thought this doesn't work, but it does! Still, adding a test to make sure\nwe don't regress it.\n\nPiperOrigin-RevId: 763717665",
        "output": "```diff\nCommit: b44b9634ec4c1613b87dcf1278b9437ce9ab8004\nDate: 2025-05-27T10:46:15Z\nURL: https://github.com/jax-ml/jax/commit/b44b9634ec4c1613b87dcf1278b9437ce9ab8004\nFiles changed: 1\nAdditions: +22, Deletions: -0\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 3c0b463ba1c7..7430fcc1ca53 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -21,6 +21,7 @@\n import re\n import sys\n import tempfile\n+import traceback\n from typing import ClassVar\n \n from absl.testing import absltest\n@@ -1766,6 +1767,27 @@ def body(idx, _):\n           jnp.tile((132 * sm_step + jnp.arange(132))[:, None], 128),\n       )\n \n+  def test_lowering_error_context(self):\n+    def body(x_ref, y_ref, barrier):\n+      plgpu.copy_gmem_to_smem(x_ref, y_ref, barrier)\n+      plgpu.barrier_wait(barrier)\n+\n+    x = jnp.arange(127, dtype=jnp.int4)  # Size is not a multiple of bytes\n+    offending_line = \"plgpu.copy_gmem_to_smem(x_ref, y_ref, barrier)\"\n+    try:\n+      pl.pallas_call(\n+          body,\n+          in_specs=[pl.BlockSpec(memory_space=plgpu.GMEM)],\n+          out_specs=pl.BlockSpec(memory_space=plgpu.SMEM),\n+          out_shape=x,\n+          scratch_shapes=[plgpu.Barrier(1)],\n+      )(x)\n+    except:\n+      # assertRaisesRegex raises does not let us match the traceback.\n+      self.assertIn(offending_line, traceback.format_exc())\n+    else:\n+      self.fail(\"Should have raised an exception\")\n+\n \n class PallasCallWarpPrimitiveSemanticsTest(PallasTest):\n   def setUp(self):\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28595 from andportnoy:mosaic-gpu-ptx-isa-from-ptxas-and-llvm\n\nPiperOrigin-RevId: 763701410",
        "output": "```diff\nCommit: 2cbec58cf7d330f04356fd31f1a7c31f8f4a70fd\nDate: 2025-05-27T09:46:03Z\nURL: https://github.com/jax-ml/jax/commit/2cbec58cf7d330f04356fd31f1a7c31f8f4a70fd\nFiles changed: 3\nAdditions: +206, Deletions: -86\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 27175c3773e6..7df185cffaf1 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -42,6 +42,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/strings/str_replace.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"llvm/ADT/SmallVector.h\"\n@@ -109,6 +110,106 @@ namespace ffi = xla::ffi;\n using MosaicInitFunc = void(void****);\n using MosaicHostFunc = void(void**);\n \n+class TemporaryDirectory {\n+ private:\n+  TemporaryDirectory(std::string path) : path(std::move(path)) {}\n+  // TODO(apaszke): Unlink in destructor.\n+\n+ public:\n+  static absl::StatusOr<TemporaryDirectory> Create() {\n+    std::string pattern = \"/tmp/mosaic-gpu-XXXXXX\";\n+    if (mkdtemp(pattern.data()) == NULL) {\n+      return absl::InternalError(\"Failed to create temporary directory\");\n+    }\n+    return TemporaryDirectory(std::move(pattern));\n+  }\n+\n+  std::string_view GetPath() { return path; }\n+\n+ private:\n+  std::string path;\n+};\n+\n+absl::StatusOr<std::string> RunCUDATool(const char* tool,\n+                                        const std::vector<const char*>& args,\n+                                        bool stderr_to_stdout = true) {\n+  CHECK(!args.empty() && args.back() == nullptr);\n+  const char* cuda_path_ptr = getenv(\"CUDA_ROOT\");\n+  if (!cuda_path_ptr) return absl::InternalError(\"Failed to get CUDA_ROOT\");\n+  std::string tool_path(cuda_path_ptr);\n+  tool_path += \"/bin/\";\n+  tool_path += tool;\n+  int stdout_pipe[2] = {-1, -1};\n+  pid_t child_pid;\n+  posix_spawn_file_actions_t file_actions;\n+  if (posix_spawn_file_actions_init(&file_actions)) {\n+    return absl::InternalError(\"Failed to initialize spawn file actions\");\n+  }\n+  absl::Cleanup file_actions_destroyer = [&file_actions] {\n+    posix_spawn_file_actions_destroy(&file_actions);\n+  };\n+  if (pipe(stdout_pipe) == -1) {\n+    return absl::InternalError(\"Failed to set up pipe\");\n+  }\n+  absl::Cleanup pipe_closer = [&stdout_pipe] {\n+    if (stdout_pipe[0] != -1) close(stdout_pipe[0]);\n+    if (stdout_pipe[1] != -1) close(stdout_pipe[1]);\n+  };\n+  // close read end in child\n+  if (posix_spawn_file_actions_addclose(&file_actions, stdout_pipe[0])) {\n+    return absl::InternalError(\"Failed to close read end of the pipe in child\");\n+  }\n+  if (posix_spawn_file_actions_adddup2(&file_actions, stdout_pipe[1],\n+                                       STDOUT_FILENO)) {\n+    return absl::InternalError(\"Failed to redirect stdout to pipe\");\n+  }\n+  if (stderr_to_stdout && posix_spawn_file_actions_adddup2(\n+                              &file_actions, STDOUT_FILENO, STDERR_FILENO)) {\n+    return absl::InternalError(\"Failed to redirect stderr to stdout\");\n+  }\n+  // execv is guaranteed by POSIX to not modify the args (other than\n+  // replacing the whole process image), so the const_cast is valid.\n+  if (int status =\n+          posix_spawn(&child_pid, tool_path.c_str(), &file_actions, nullptr,\n+                      const_cast<char* const*>(args.data()), environ)) {\n+    return absl::InternalError(\n+        absl::StrCat(\"Process spawn failed: \", strerror(status)));\n+  }\n+  // Proactively close write end in parent. If we don't do this, read\n+  // will block since the pipe will have an open write end in the\n+  // parent process.\n+  if (close(stdout_pipe[1]) == -1) {\n+    return absl::InternalError(\n+        absl::StrCat(\"Failed to close write end of pipe in parent process: \",\n+                     strerror(errno)));\n+  }\n+  // Mark the write end as successfully closed, so it doesn't get\n+  // closed a second time by the deferred pipe_closer.\n+  stdout_pipe[1] = -1;\n+  std::string stdout;\n+  char buf[1024];\n+  while (int bytes_read = read(stdout_pipe[0], buf, sizeof buf)) {\n+    if (bytes_read == -1) {\n+      return absl::InternalError(\n+          absl::StrCat(\"Failed to read from pipe: \", strerror(errno)));\n+    }\n+    stdout.append(buf, bytes_read);\n+  }\n+  int status;\n+  if (waitpid(child_pid, &status, 0) == -1) {\n+    return absl::InternalError(\"Failed to wait for CUDA tool invocation\");\n+  }\n+  if (status != 0) {\n+    std::string error_message = \"CUDA tool failed\";\n+    if (!stdout.empty()) {\n+      error_message += \": \";\n+      error_message += stdout;\n+    }\n+    return absl::InternalError(error_message);\n+  }\n+  return stdout;\n+}\n+\n void EnsureLLVMNVPTXTargetIsRegistered() {\n   static absl::once_flag register_nvptx_target_flag;\n   absl::call_once(register_nvptx_target_flag, []() {\n@@ -119,7 +220,65 @@ void EnsureLLVMNVPTXTargetIsRegistered() {\n   });\n }\n \n-absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion() {\n+absl::StatusOr<int> GetLatestPtxasPtxIsaVersion() {\n+  std::vector<const char*> ptxas_args = {\"ptxas\", \"--input-as-string\",\n+                                         \".version 99.99\", nullptr};\n+  auto status = RunCUDATool(\"ptxas\", ptxas_args).status();\n+  if (status.ok()) {\n+    return absl::InternalError(\"ptxas succeeded where it was expected to fail\");\n+  }\n+  // Output message is of the form:\n+  // ptxas application ptx input, line 1; fatal   :\n+  // Unsupported .version 99.99; current version is '8.8'\n+  std::vector<std::string> chunks = absl::StrSplit(status.message(), '\\'');\n+  if (chunks.size() != 3) {\n+    return absl::InternalError(\n+        \"Failed to locate PTX ISA version in ptxas error message\");\n+  }\n+  std::vector<std::string> major_minor = absl::StrSplit(chunks[1], '.');\n+  if (major_minor.size() != 2) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"Expected PTX ISA version to be formatted as \"\n+                        \"MAJOR.MINOR, instead got: %s\",\n+                        chunks[1]));\n+  }\n+  int major;\n+  if (!absl::SimpleAtoi(major_minor[0], &major)) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"Failed to parse PTX ISA major version, expected a \"\n+                        \"parsable integer, instead got: %s\",\n+                        major_minor[0]));\n+  }\n+  int minor;\n+  if (!absl::SimpleAtoi(major_minor[1], &minor)) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"Failed to parse PTX ISA minor version, expected a \"\n+                        \"parsable integer, instead got: %s\",\n+                        major_minor[1]));\n+  }\n+  if (minor >= 10) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"PTX ISA minor version %d is not less than or equal to \"\n+                        \"9, which is assumed for version comparison\",\n+                        minor));\n+  }\n+  return major * 10 + minor;\n+}\n+\n+absl::StatusOr<std::string> GetPtxIsaVersion() {\n+  TF_ASSIGN_OR_RETURN(int ptxas_latest_version, GetLatestPtxasPtxIsaVersion());\n+  // We'd like to target the latest PTX ISA version supported by\n+  // ptxas. However, it doesn't make sense to ask LLVM to target a PTX\n+  // ISA that it isn't aware of yet. Find the latest version supported\n+  // by LLVM and return the minimum of the two versions, one from\n+  // ptxas and the other from LLVM.\n+  TF_ASSIGN_OR_RETURN(int llvm_latest_version,\n+                      mosaic::gpu::GetLatestLlvmPtxIsaVersion());\n+  int final_version = std::min(ptxas_latest_version, llvm_latest_version);\n+  return absl::StrFormat(\"ptx%d\", final_version);\n+}\n+\n+absl::StatusOr<std::string> GetSmVersion() {\n   // Assumes driver has been initialized and a context exists. XLA already has\n   // some utilities to query this, but we try to stay runtime-agnostic, so we\n   // build our own here.\n@@ -138,10 +297,9 @@ absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion() {\n     return absl::InternalError(\"Failed to get minor compute capability\");\n   }\n   EnsureLLVMNVPTXTargetIsRegistered();\n-  return mosaic::gpu::GetSmAndPtxIsaVersion(major, minor);\n+  return mosaic::gpu::GetSmVersion(major, minor);\n }\n \n-\n mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n     mlir::MLIRContext* ctx, mlir::gpu::CompilationTarget target,\n     const std::string& sm, const std::string& ptx_isa, const std::string& nvshmem_path) {\n@@ -272,61 +430,6 @@ void InitContext(mlir::MLIRContext* context) {\n   context->loadAllAvailableDialects();\n }\n \n-absl::Status RunCUDATool(const char* tool,\n-                         const std::vector<const char*>& args,\n-                         bool stderr_to_stdout = false) {\n-  CHECK(!args.empty() && args.back() == nullptr);\n-  const char * cuda_path_ptr = getenv(\"CUDA_ROOT\");\n-  if (!cuda_path_ptr) return absl::InternalError(\"Failed to get CUDA_ROOT\");\n-  std::string tool_path(cuda_path_ptr);\n-  tool_path += \"/bin/\";\n-  tool_path += tool;\n-  pid_t child_pid;\n-  posix_spawn_file_actions_t file_actions;\n-  if (posix_spawn_file_actions_init(&file_actions)) {\n-    return absl::InternalError(\"Failed to initialize spawn file actions\");\n-  }\n-  if (posix_spawn_file_actions_adddup2(&file_actions, STDOUT_FILENO,\n-                                       STDERR_FILENO)) {\n-    return absl::InternalError(\"Failed to set up spawn file actions\");\n-  }\n-  // execv is guaranteed by POSIX to not modify the args (other than\n-  // replacing the whole process image), so the const_cast is valid.\n-  if (posix_spawn(&child_pid, tool_path.c_str(), &file_actions, nullptr,\n-                  const_cast<char* const*>(args.data()), environ)) {\n-    return absl::InternalError(\"Process spawn failed\");\n-  }\n-  int status;\n-  if (waitpid(child_pid, &status, 0) == -1) {\n-    return absl::InternalError(\"Failed to wait for CUDA tool invocation\");\n-  }\n-  if (status != 0) return absl::InternalError(\"CUDA tool failed\");\n-  if (posix_spawn_file_actions_destroy(&file_actions) != 0) {\n-    return absl::InternalError(\"Failed to clean up after posix_spawn\");\n-  }\n-  return absl::OkStatus();\n-}\n-\n-class TemporaryDirectory {\n- private:\n-  TemporaryDirectory(std::string path) : path(std::move(path)) {}\n-  // TODO(apaszke): Unlink in destructor.\n-\n- public:\n-  static absl::StatusOr<TemporaryDirectory> Create() {\n-    std::string pattern = \"/tmp/mosaic-gpu-XXXXXX\";\n-    if (mkdtemp(pattern.data()) == NULL) {\n-      return absl::InternalError(\"Failed to create temporary directory\");\n-    }\n-    return TemporaryDirectory(std::move(pattern));\n-  }\n-\n-  std::string_view GetPath() { return path; }\n-\n- private:\n-  std::string path;\n-};\n-\n void DumpCompilationOutput(mlir::ModuleOp module, const std::string& sm,\n                            const std::string& ptx_isa, const std::string& nvshmem_path) {\n   bool dump_ptx = getenv(\"MOSAIC_GPU_DUMP_PTX\") != nullptr;\n@@ -382,19 +485,23 @@ void DumpCompilationOutput(mlir::ModuleOp module, const std::string& sm,\n       ptxas_args.push_back(\"-v\");\n     }\n     ptxas_args.push_back(nullptr);\n-    if (auto status = RunCUDATool(\"ptxas\", ptxas_args); !status.ok()) {\n-      std::cerr << \"ptxas invocation failed: \" << status.message() << std::endl;\n+    if (auto result = RunCUDATool(\"ptxas\", ptxas_args); !result.ok()) {\n+      std::cerr << \"ptxas invocation failed: \" << result.status() << std::endl;\n       continue;\n+    } else if (dump_ptxas) {\n+      std::cout << *result << std::endl;\n     }\n     if (!dump_sass) { continue; }  // We're done.\n     // Call nvdisasm to pretty-print SASS.\n-    if (auto status = RunCUDATool(\n-            \"nvdisasm\", {\"nvdisasm\", \"-ndf\", \"-c\", elf_path.c_str(), nullptr});\n-        !status.ok()) {\n-      std::cerr << \"nvdisasm invocation failed: \" << status.message()\n+    auto result = RunCUDATool(\n+        \"nvdisasm\", {\"nvdisasm\", \"-ndf\", \"-c\", elf_path.c_str(), nullptr});\n+    if (!result.ok()) {\n+      std::cerr << \"nvdisasm invocation failed: \" << result.status()\n                 << std::endl;\n       continue;\n     }\n+    // Dump SASS.\n+    std::cout << *result << std::endl;\n   }\n }\n \n@@ -424,12 +531,8 @@ absl::StatusOr<std::string> get_nvshmem_llvm_lib_path() {\n absl::StatusOr<std::pair<std::unique_ptr<mlir::ExecutionEngine>, bool>> Compile(\n     mlir::ModuleOp module) {\n   tsl::profiler::TraceMe trace(\"Compile\");\n-  auto sm_and_ptx_isa = GetSmAndPtxIsaVersion();\n-  if (!sm_and_ptx_isa.ok()) {\n-    return sm_and_ptx_isa.status();\n-  }\n-  const std::string sm = sm_and_ptx_isa.value().first;\n-  const std::string ptx_isa = sm_and_ptx_isa.value().second;\n+  TF_ASSIGN_OR_RETURN(std::string sm, GetSmVersion());\n+  TF_ASSIGN_OR_RETURN(std::string ptx_isa, GetPtxIsaVersion());\n   bool is_comm_used = is_nvshmem_used(module);\n   std::string nvshmem_path = \"\";\n   if (is_comm_used) {\ndiff --git a/jaxlib/mosaic/gpu/target.cc b/jaxlib/mosaic/gpu/target.cc\nindex a259b3dead7b..dfb119b410af 100644\n--- a/jaxlib/mosaic/gpu/target.cc\n+++ b/jaxlib/mosaic/gpu/target.cc\n@@ -21,15 +21,16 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/match.h\"\n+#include \"absl/strings/numbers.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/strings/strip.h\"\n #include \"llvm/MC/MCSubtargetInfo.h\"\n #include \"llvm/MC/TargetRegistry.h\"\n \n namespace mosaic::gpu {\n \n-absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion(\n-    int major, int minor) {\n+absl::StatusOr<std::string> GetSmVersion(int major, int minor) {\n   // \"base\" compute capability as reported by the driver.\n   // For example for a Hopper H200 GPU this would return sm_90, and never\n   // sm_90a.\n@@ -64,25 +65,41 @@ absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion(\n       }\n     }\n   }\n+  return sm_arch_specific ? sm_arch_specific : sm_base;\n+}\n \n-  const std::string sm = sm_arch_specific ? sm_arch_specific : sm_base;\n-\n+absl::StatusOr<int> GetLatestLlvmPtxIsaVersion() {\n+  const std::string triple = \"nvptx64-nvidia-cuda\";\n+  std::string error;\n+  const llvm::Target* target =\n+      llvm::TargetRegistry::lookupTarget(triple, error);\n+  if (target == nullptr) {\n+    return absl::InternalError(absl::StrFormat(\n+        \"Failed to lookup LLVM target based on triple %s: %s\", triple, error));\n+  }\n+  // generic subtarget\n   std::unique_ptr<const llvm::MCSubtargetInfo> subtarget_info{\n-      target->createMCSubtargetInfo(triple, sm, \"\")};\n+      target->createMCSubtargetInfo(triple, \"\", \"\")};\n   if (subtarget_info == nullptr) {\n-    return absl::InternalError(\n-        absl::StrFormat(\"Failed to get LLVM subtarget info for sm %s\", sm));\n+    return absl::InternalError(absl::StrFormat(\n+        \"Failed to get generic LLVM subtarget info for triple %s\", triple));\n   }\n-\n+  int llvm_latest_version = 0;\n   for (const llvm::SubtargetFeatureKV& feature :\n-       subtarget_info->getEnabledProcessorFeatures()) {\n-    if (absl::StartsWith(feature.Key, \"ptx\")) {\n-      std::string ptx_isa = feature.Key;\n-      return std::make_pair(sm, ptx_isa);\n+       subtarget_info->getAllProcessorFeatures()) {\n+    absl::string_view version_string = feature.Key;\n+    if (absl::ConsumePrefix(&version_string, \"ptx\")) {\n+      int version;\n+      if (!absl::SimpleAtoi(version_string, &version)) {\n+        return absl::InternalError(\n+            absl::StrFormat(\"Failed to convert PTX ISA version to integer: %s\",\n+                            version_string));\n+      }\n+      llvm_latest_version =\n+          version > llvm_latest_version ? version : llvm_latest_version;\n     }\n   }\n-  return absl::InternalError(absl::StrFormat(\n-      \"Failed to find a PTX ISA LLVM subtarget feature for %s\", sm));\n+  return llvm_latest_version;\n }\n \n }  // namespace mosaic::gpu\ndiff --git a/jaxlib/mosaic/gpu/target.h b/jaxlib/mosaic/gpu/target.h\nindex 070ecedebd01..5a2a240d8db1 100644\n--- a/jaxlib/mosaic/gpu/target.h\n+++ b/jaxlib/mosaic/gpu/target.h\n@@ -22,8 +22,8 @@ limitations under the License.\n \n namespace mosaic::gpu {\n \n-absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion(\n-    int major, int minor);\n+absl::StatusOr<std::string> GetSmVersion(int major, int minor);\n+absl::StatusOr<int> GetLatestLlvmPtxIsaVersion();\n \n }  // namespace mosaic::gpu\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] The `cf` dialect is now always available\n\nPiperOrigin-RevId: 763697379",
        "output": "```diff\nCommit: f35d708503c7c0b401fe55e3716ad2f3ce8396cc\nDate: 2025-05-27T09:33:34Z\nURL: https://github.com/jax-ml/jax/commit/f35d708503c7c0b401fe55e3716ad2f3ce8396cc\nFiles changed: 3\nAdditions: +2, Deletions: -13\ndiff --git a/jax/_src/lib/mlir/dialects/__init__.py b/jax/_src/lib/mlir/dialects/__init__.py\nindex b49154e7936a..eccd40104dc1 100644\n--- a/jax/_src/lib/mlir/dialects/__init__.py\n+++ b/jax/_src/lib/mlir/dialects/__init__.py\n@@ -19,6 +19,7 @@\n if TYPE_CHECKING:\n   from jaxlib.mlir.dialects import arith as arith\n   from jaxlib.mlir.dialects import builtin as builtin\n+  from jaxlib.mlir.dialects import cf as cf\n   from jaxlib.mlir.dialects import chlo as chlo\n   from jaxlib.mlir.dialects import func as func\n   from jaxlib.mlir.dialects import gpu as gpu\n@@ -36,6 +37,7 @@\n   __getattr__, __dir__, __all__ = _lazy.attach(\"jaxlib.mlir.dialects\", [\n       \"arith\",\n       \"builtin\",\n+      \"cf\",\n       \"chlo\",\n       \"func\",\n       \"gpu\",\n@@ -57,4 +59,3 @@\n from jaxlib.mlir.dialects import stablehlo as hlo\n \n from jax._src import lib\n-from jaxlib.mlir.dialects import cf\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 4e8827401e0c..635c473620c3 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -3780,12 +3780,6 @@ def _check_lowering_rule(\n   if not pallas_helpers.debug_checks_enabled():\n     return []\n \n-  if cf is None:\n-    # TODO(slebedev): Remove once the minimal jaxlib version is 0.6.1.\n-    raise ValueError(\n-        \"cf dialect is not available. Make sure you have jaxlib 0.6.1 or later.\"\n-    )\n-\n   error = jax.tree.unflatten(err_tree, err_args)\n   [pred] = error._pred.values()\n   [exception_tree] = error._metadata.values()\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 9e396167f610..b9a3aa17c39d 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -3109,12 +3109,6 @@ def _check_lowering_rule(ctx: LoweringRuleContext, *err_args, err_tree, debug):\n   if not pallas_helpers.debug_checks_enabled():\n     return []\n \n-  if cf_dialect is None:\n-    # TODO(slebedev): Remove once the minimal jaxlib version is 0.6.1.\n-    raise ValueError(\n-        \"cf dialect is not available. Make sure you have jaxlib 0.6.1 or later.\"\n-    )\n-\n   error = jax.tree.unflatten(err_tree, err_args)\n   [pred] = error._pred.values()\n   [exception_tree] = error._metadata.values()\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Use PTX ISA version = min(ptxas, LLVM)",
        "output": "```diff\nCommit: c1e8f250b53e2c9636fd095ead21b64f13c1d422\nDate: 2025-05-26T15:17:49Z\nURL: https://github.com/jax-ml/jax/commit/c1e8f250b53e2c9636fd095ead21b64f13c1d422\nFiles changed: 3\nAdditions: +206, Deletions: -86\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 27175c3773e6..d109520582c9 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -42,6 +42,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/strings/str_replace.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"llvm/ADT/SmallVector.h\"\n@@ -109,6 +110,106 @@ namespace ffi = xla::ffi;\n using MosaicInitFunc = void(void****);\n using MosaicHostFunc = void(void**);\n \n+class TemporaryDirectory {\n+ private:\n+  TemporaryDirectory(std::string path) : path(std::move(path)) {}\n+  // TODO(apaszke): Unlink in destructor.\n+\n+ public:\n+  static absl::StatusOr<TemporaryDirectory> Create() {\n+    std::string pattern = \"/tmp/mosaic-gpu-XXXXXX\";\n+    if (mkdtemp(pattern.data()) == NULL) {\n+      return absl::InternalError(\"Failed to create temporary directory\");\n+    }\n+    return TemporaryDirectory(std::move(pattern));\n+  }\n+\n+  std::string_view GetPath() { return path; }\n+\n+ private:\n+  std::string path;\n+};\n+\n+absl::StatusOr<std::string> RunCUDATool(const char* tool,\n+                                        const std::vector<const char*>& args,\n+                                        bool stderr_to_stdout = true) {\n+  CHECK(!args.empty() && args.back() == nullptr);\n+  const char * cuda_path_ptr = getenv(\"CUDA_ROOT\");\n+  if (!cuda_path_ptr) return absl::InternalError(\"Failed to get CUDA_ROOT\");\n+  std::string tool_path(cuda_path_ptr);\n+  tool_path += \"/bin/\";\n+  tool_path += tool;\n+  int stdout_pipe[2] = {-1, -1};\n+  pid_t child_pid;\n+  posix_spawn_file_actions_t file_actions;\n+  if (posix_spawn_file_actions_init(&file_actions)) {\n+    return absl::InternalError(\"Failed to initialize spawn file actions\");\n+  }\n+  absl::Cleanup file_actions_destroyer = [&file_actions] {\n+    posix_spawn_file_actions_destroy(&file_actions);\n+  };\n+  if (pipe(stdout_pipe) == -1) {\n+    return absl::InternalError(\"Failed to set up pipe\");\n+  }\n+  absl::Cleanup pipe_closer = [&stdout_pipe] {\n+    if (stdout_pipe[0] != -1) close(stdout_pipe[0]);\n+    if (stdout_pipe[1] != -1) close(stdout_pipe[1]);\n+  };\n+  // close read end in child\n+  if (posix_spawn_file_actions_addclose(&file_actions, stdout_pipe[0])) {\n+    return absl::InternalError(\"Failed to close read end of the pipe in child\");\n+  }\n+  if (posix_spawn_file_actions_adddup2(&file_actions, stdout_pipe[1],\n+                                       STDOUT_FILENO)) {\n+    return absl::InternalError(\"Failed to redirect stdout to pipe\");\n+  }\n+  if (stderr_to_stdout && posix_spawn_file_actions_adddup2(\n+                              &file_actions, STDOUT_FILENO, STDERR_FILENO)) {\n+    return absl::InternalError(\"Failed to redirect stderr to stdout\");\n+  }\n+  // execv is guaranteed by POSIX to not modify the args (other than\n+  // replacing the whole process image), so the const_cast is valid.\n+  if (int status =\n+          posix_spawn(&child_pid, tool_path.c_str(), &file_actions, nullptr,\n+                      const_cast<char *const *>(args.data()), environ)) {\n+    return absl::InternalError(\n+        absl::StrCat(\"Process spawn failed: \", strerror(status)));\n+  }\n+  // Proactively close write end in parent. If we don't do this, read\n+  // will block since the pipe will have an open write end in the\n+  // parent process.\n+  if (close(stdout_pipe[1]) == -1) {\n+    return absl::InternalError(\n+        absl::StrCat(\"Failed to close write end of pipe in parent process: \",\n+                     strerror(errno)));\n+  }\n+  // Mark the write end as successfully closed, so it doesn't get\n+  // closed a second time by the deferred pipe_closer.\n+  stdout_pipe[1] = -1;\n+  std::string stdout;\n+  char buf[1024];\n+  while (int bytes_read = read(stdout_pipe[0], buf, sizeof buf)) {\n+    if (bytes_read == -1) {\n+      return absl::InternalError(\n+          absl::StrCat(\"Failed to read from pipe: \", strerror(errno)));\n+    }\n+    stdout.append(buf, bytes_read);\n+  }\n+  int status;\n+  if (waitpid(child_pid, &status, 0) == -1) {\n+    return absl::InternalError(\"Failed to wait for CUDA tool invocation\");\n+  }\n+  if (status != 0) {\n+    std::string error_message = \"CUDA tool failed\";\n+    if (!stdout.empty()) {\n+      error_message += \": \";\n+      error_message += stdout;\n+    }\n+    return absl::InternalError(error_message);\n+  }\n+  return stdout;\n+}\n+\n void EnsureLLVMNVPTXTargetIsRegistered() {\n   static absl::once_flag register_nvptx_target_flag;\n   absl::call_once(register_nvptx_target_flag, []() {\n@@ -119,7 +220,65 @@ void EnsureLLVMNVPTXTargetIsRegistered() {\n   });\n }\n \n-absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion() {\n+absl::StatusOr<int> GetLatestPtxasPtxIsaVersion() {\n+  std::vector<const char*> ptxas_args = {\"ptxas\", \"--input-as-string\",\n+                                         \".version 99.99\", nullptr};\n+  auto status = RunCUDATool(\"ptxas\", ptxas_args).status();\n+  if (status.ok()) {\n+    return absl::InternalError(\"ptxas succeeded where it was expected to fail\");\n+  }\n+  // Output message is of the form:\n+  // ptxas application ptx input, line 1; fatal   : Unsupported .version 99.99; current version is '8.8'\n+  std::vector<std::string> chunks =\n+      absl::StrSplit(status.message(), '\\'');\n+  if (chunks.size() != 3) {\n+    return absl::InternalError(\n+        \"Failed to locate PTX ISA version in ptxas error message\");\n+  }\n+  std::vector<std::string> major_minor = absl::StrSplit(chunks[1], '.');\n+  if (major_minor.size() != 2) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"Expected PTX ISA version to be formatted as \"\n+                        \"MAJOR.MINOR, instead got: %s\",\n+                        chunks[1]));\n+  }\n+  int major;\n+  if (!absl::SimpleAtoi(major_minor[0], &major)) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"Failed to parse PTX ISA major version, expected a \"\n+                        \"parsable integer, instead got: %s\",\n+                        major_minor[0]));\n+  }\n+  int minor;\n+  if (!absl::SimpleAtoi(major_minor[1], &minor)) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"Failed to parse PTX ISA minor version, expected a \"\n+                        \"parsable integer, instead got: %s\",\n+                        major_minor[1]));\n+  }\n+  if (minor >= 10) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"PTX ISA minor version %d is not less than or equal to \"\n+                        \"9, which is assumed for version comparison\",\n+                        minor));\n+  }\n+  return major * 10 + minor;\n+}\n+\n+absl::StatusOr<std::string> GetPtxIsaVersion() {\n+  TF_ASSIGN_OR_RETURN(int ptxas_latest_version, GetLatestPtxasPtxIsaVersion());\n+  // We'd like to target the latest PTX ISA version supported by\n+  // ptxas. However, it doesn't make sense to ask LLVM to target a PTX\n+  // ISA that it isn't aware of yet. Find the latest version supported\n+  // by LLVM and return the minimum of the two versions, one from\n+  // ptxas and the other from LLVM.\n+  TF_ASSIGN_OR_RETURN(int llvm_latest_version,\n+                      mosaic::gpu::GetLatestLlvmPtxIsaVersion());\n+  int final_version = std::min(ptxas_latest_version, llvm_latest_version);\n+  return absl::StrFormat(\"ptx%d\", final_version);\n+}\n+\n+absl::StatusOr<std::string> GetSmVersion() {\n   // Assumes driver has been initialized and a context exists. XLA already has\n   // some utilities to query this, but we try to stay runtime-agnostic, so we\n   // build our own here.\n@@ -138,10 +297,9 @@ absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion() {\n     return absl::InternalError(\"Failed to get minor compute capability\");\n   }\n   EnsureLLVMNVPTXTargetIsRegistered();\n-  return mosaic::gpu::GetSmAndPtxIsaVersion(major, minor);\n+  return mosaic::gpu::GetSmVersion(major, minor);\n }\n \n-\n mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n     mlir::MLIRContext* ctx, mlir::gpu::CompilationTarget target,\n     const std::string& sm, const std::string& ptx_isa, const std::string& nvshmem_path) {\n@@ -272,61 +430,6 @@ void InitContext(mlir::MLIRContext* context) {\n   context->loadAllAvailableDialects();\n }\n \n-absl::Status RunCUDATool(const char* tool,\n-                         const std::vector<const char*>& args,\n-                         bool stderr_to_stdout = false) {\n-  CHECK(!args.empty() && args.back() == nullptr);\n-  const char * cuda_path_ptr = getenv(\"CUDA_ROOT\");\n-  if (!cuda_path_ptr) return absl::InternalError(\"Failed to get CUDA_ROOT\");\n-  std::string tool_path(cuda_path_ptr);\n-  tool_path += \"/bin/\";\n-  tool_path += tool;\n-  pid_t child_pid;\n-  posix_spawn_file_actions_t file_actions;\n-  if (posix_spawn_file_actions_init(&file_actions)) {\n-    return absl::InternalError(\"Failed to initialize spawn file actions\");\n-  }\n-  if (posix_spawn_file_actions_adddup2(&file_actions, STDOUT_FILENO,\n-                                       STDERR_FILENO)) {\n-    return absl::InternalError(\"Failed to set up spawn file actions\");\n-  }\n-  // execv is guaranteed by POSIX to not modify the args (other than\n-  // replacing the whole process image), so the const_cast is valid.\n-  if (posix_spawn(&child_pid, tool_path.c_str(), &file_actions, nullptr,\n-                  const_cast<char* const*>(args.data()), environ)) {\n-    return absl::InternalError(\"Process spawn failed\");\n-  }\n-  int status;\n-  if (waitpid(child_pid, &status, 0) == -1) {\n-    return absl::InternalError(\"Failed to wait for CUDA tool invocation\");\n-  }\n-  if (status != 0) return absl::InternalError(\"CUDA tool failed\");\n-  if (posix_spawn_file_actions_destroy(&file_actions) != 0) {\n-    return absl::InternalError(\"Failed to clean up after posix_spawn\");\n-  }\n-  return absl::OkStatus();\n-}\n-\n-class TemporaryDirectory {\n- private:\n-  TemporaryDirectory(std::string path) : path(std::move(path)) {}\n-  // TODO(apaszke): Unlink in destructor.\n-\n- public:\n-  static absl::StatusOr<TemporaryDirectory> Create() {\n-    std::string pattern = \"/tmp/mosaic-gpu-XXXXXX\";\n-    if (mkdtemp(pattern.data()) == NULL) {\n-      return absl::InternalError(\"Failed to create temporary directory\");\n-    }\n-    return TemporaryDirectory(std::move(pattern));\n-  }\n-\n-  std::string_view GetPath() { return path; }\n-\n- private:\n-  std::string path;\n-};\n-\n void DumpCompilationOutput(mlir::ModuleOp module, const std::string& sm,\n                            const std::string& ptx_isa, const std::string& nvshmem_path) {\n   bool dump_ptx = getenv(\"MOSAIC_GPU_DUMP_PTX\") != nullptr;\n@@ -382,19 +485,23 @@ void DumpCompilationOutput(mlir::ModuleOp module, const std::string& sm,\n       ptxas_args.push_back(\"-v\");\n     }\n     ptxas_args.push_back(nullptr);\n-    if (auto status = RunCUDATool(\"ptxas\", ptxas_args); !status.ok()) {\n-      std::cerr << \"ptxas invocation failed: \" << status.message() << std::endl;\n+    if (auto result = RunCUDATool(\"ptxas\", ptxas_args); !result.ok()) {\n+      std::cerr << \"ptxas invocation failed: \" << result.status() << std::endl;\n       continue;\n+    } else if (dump_ptxas) {\n+      std::cout << *result << std::endl;\n     }\n     if (!dump_sass) { continue; }  // We're done.\n     // Call nvdisasm to pretty-print SASS.\n-    if (auto status = RunCUDATool(\n-            \"nvdisasm\", {\"nvdisasm\", \"-ndf\", \"-c\", elf_path.c_str(), nullptr});\n-        !status.ok()) {\n-      std::cerr << \"nvdisasm invocation failed: \" << status.message()\n+    auto result = RunCUDATool(\n+        \"nvdisasm\", {\"nvdisasm\", \"-ndf\", \"-c\", elf_path.c_str(), nullptr});\n+    if (!result.ok()) {\n+      std::cerr << \"nvdisasm invocation failed: \" << result.status()\n                 << std::endl;\n       continue;\n     }\n+    // Dump SASS.\n+    std::cout << *result << std::endl;\n   }\n }\n \n@@ -424,12 +531,8 @@ absl::StatusOr<std::string> get_nvshmem_llvm_lib_path() {\n absl::StatusOr<std::pair<std::unique_ptr<mlir::ExecutionEngine>, bool>> Compile(\n     mlir::ModuleOp module) {\n   tsl::profiler::TraceMe trace(\"Compile\");\n-  auto sm_and_ptx_isa = GetSmAndPtxIsaVersion();\n-  if (!sm_and_ptx_isa.ok()) {\n-    return sm_and_ptx_isa.status();\n-  }\n-  const std::string sm = sm_and_ptx_isa.value().first;\n-  const std::string ptx_isa = sm_and_ptx_isa.value().second;\n+  TF_ASSIGN_OR_RETURN(std::string sm, GetSmVersion());\n+  TF_ASSIGN_OR_RETURN(std::string ptx_isa, GetPtxIsaVersion());\n   bool is_comm_used = is_nvshmem_used(module);\n   std::string nvshmem_path = \"\";\n   if (is_comm_used) {\ndiff --git a/jaxlib/mosaic/gpu/target.cc b/jaxlib/mosaic/gpu/target.cc\nindex a259b3dead7b..4c1866fdaea9 100644\n--- a/jaxlib/mosaic/gpu/target.cc\n+++ b/jaxlib/mosaic/gpu/target.cc\n@@ -21,6 +21,8 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/match.h\"\n+#include \"absl/strings/numbers.h\"\n+#include \"absl/strings/strip.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"llvm/MC/MCSubtargetInfo.h\"\n@@ -28,8 +30,7 @@ limitations under the License.\n \n namespace mosaic::gpu {\n \n-absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion(\n-    int major, int minor) {\n+absl::StatusOr<std::string> GetSmVersion(int major, int minor) {\n   // \"base\" compute capability as reported by the driver.\n   // For example for a Hopper H200 GPU this would return sm_90, and never\n   // sm_90a.\n@@ -64,25 +65,41 @@ absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion(\n       }\n     }\n   }\n+  return sm_arch_specific ? sm_arch_specific : sm_base;\n+}\n \n-  const std::string sm = sm_arch_specific ? sm_arch_specific : sm_base;\n-\n+absl::StatusOr<int> GetLatestLlvmPtxIsaVersion() {\n+  const std::string triple = \"nvptx64-nvidia-cuda\";\n+  std::string error;\n+  const llvm::Target* target =\n+      llvm::TargetRegistry::lookupTarget(triple, error);\n+  if (target == nullptr) {\n+    return absl::InternalError(absl::StrFormat(\n+        \"Failed to lookup LLVM target based on triple %s: %s\", triple, error));\n+  }\n+  // generic subtarget\n   std::unique_ptr<const llvm::MCSubtargetInfo> subtarget_info{\n-      target->createMCSubtargetInfo(triple, sm, \"\")};\n+      target->createMCSubtargetInfo(triple, \"\", \"\")};\n   if (subtarget_info == nullptr) {\n-    return absl::InternalError(\n-        absl::StrFormat(\"Failed to get LLVM subtarget info for sm %s\", sm));\n+    return absl::InternalError(absl::StrFormat(\n+        \"Failed to get generic LLVM subtarget info for triple %s\", triple));\n   }\n-\n+  int llvm_latest_version = 0;\n   for (const llvm::SubtargetFeatureKV& feature :\n-       subtarget_info->getEnabledProcessorFeatures()) {\n-    if (absl::StartsWith(feature.Key, \"ptx\")) {\n-      std::string ptx_isa = feature.Key;\n-      return std::make_pair(sm, ptx_isa);\n+       subtarget_info->getAllProcessorFeatures()) {\n+    absl::string_view version_string = feature.Key;\n+    if (absl::ConsumePrefix(&version_string, \"ptx\")) {\n+      int version;\n+      if (!absl::SimpleAtoi(version_string, &version)) {\n+        return absl::InternalError(\n+            absl::StrFormat(\"Failed to convert PTX ISA version to integer: %s\",\n+                            version_string));\n+      }\n+      llvm_latest_version =\n+          version > llvm_latest_version ? version : llvm_latest_version;\n     }\n   }\n-  return absl::InternalError(absl::StrFormat(\n-      \"Failed to find a PTX ISA LLVM subtarget feature for %s\", sm));\n+  return llvm_latest_version;\n }\n \n }  // namespace mosaic::gpu\ndiff --git a/jaxlib/mosaic/gpu/target.h b/jaxlib/mosaic/gpu/target.h\nindex 070ecedebd01..5a2a240d8db1 100644\n--- a/jaxlib/mosaic/gpu/target.h\n+++ b/jaxlib/mosaic/gpu/target.h\n@@ -22,8 +22,8 @@ limitations under the License.\n \n namespace mosaic::gpu {\n \n-absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion(\n-    int major, int minor);\n+absl::StatusOr<std::string> GetSmVersion(int major, int minor);\n+absl::StatusOr<int> GetLatestLlvmPtxIsaVersion();\n \n }  // namespace mosaic::gpu\n \n\n```",
        "from_id": [
            "andportnoy"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Support remote async copies and use them in the collective matmul\n\nPiperOrigin-RevId: 763353415",
        "output": "```diff\nCommit: fae05bd8592b3508143179602b25dcd609a630b9\nDate: 2025-05-26T10:07:57Z\nURL: https://github.com/jax-ml/jax/commit/fae05bd8592b3508143179602b25dcd609a630b9\nFiles changed: 3\nAdditions: +35, Deletions: -5\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 1ec22bff3f6d..61be6e35cc55 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -37,6 +37,7 @@\n from jax._src.lib.mlir.dialects import gpu as gpu_dialect\n from jax._src.lib.mlir.dialects import nvvm as nvvm_dialect\n from jax._src.pallas import core as pallas_core\n+from jax._src.pallas import primitives as pallas_primitives\n from jax._src.pallas.mosaic_gpu import core as gpu_core\n from jax._src.pallas.mosaic_gpu import lowering\n from jax._src.pallas.mosaic_gpu.core import state_types\n@@ -282,6 +283,10 @@ def _copy_smem_to_gmem_lowering(\n   else:\n     indices, slice_lengths = _split_gmem_slice(copy_params[\"gmem_slice\"])\n   assert copy_params.get(\"swizzle\") is None\n+  if copy_params.get(\"gmem_peer_id\", None) is not None:\n+    raise NotImplementedError(\n+        \"GMEM refs with peer ids are not supported in warpgroup lowering.\"\n+    )\n   assert not copy_params.get(\"gmem_transform\")\n   mgpu.dialect.async_store(\n       src,\n@@ -317,13 +322,25 @@ def _split_gmem_slice(gmem_slice):\n def _extract_gmem_copy_params(transforms):\n   if not transforms:\n     return {}\n+  peer_id = None\n+  indexers = []\n   for transform in transforms:\n-    if not isinstance(transform, indexing.NDIndexer):\n+    if isinstance(transform, gpu_core.PeerMemRef):\n+      if transform.device_id_type != pallas_primitives.DeviceIdType.LOGICAL:\n+        raise NotImplementedError(\n+            \"Only logical device ids are supported for GMEM refs.\"\n+        )\n+      peer_id = lowering._ensure_ir_value(transform.device_id, jnp.int32)\n+      continue\n+    elif isinstance(transform, indexing.NDIndexer):\n+      indexers.append(transform)\n+    else:\n       raise NotImplementedError(\n           \"Non-indexing transforms on GMEM refs are not implemented.\")\n-  indexer = lowering.merge_indexers(transforms)\n+  indexer = lowering.merge_indexers(indexers)\n   return dict(\n       gmem_slice=lowering._ndindexer_indices(indexer),\n+      gmem_peer_id=peer_id,\n   )\n \n \n@@ -542,6 +559,10 @@ def _copy_gmem_to_smem_lowering(\n     indices, slice_lengths = _split_gmem_slice(copy_params[\"gmem_slice\"])\n   assert copy_params.get(\"swizzle\") is None\n   assert not copy_params.get(\"gmem_transform\")\n+  if copy_params.get(\"gmem_peer_id\", None) is not None:\n+    raise NotImplementedError(\n+        \"GMEM refs with peer ids are not supported in warpgroup lowering.\"\n+    )\n   barrier_ref = barrier.as_barrier_memref()\n   mgpu.dialect.arrive_expect_tx(barrier_ref, bytes)\n   mgpu.dialect.async_load(\ndiff --git a/jax/experimental/mosaic/gpu/launch_context.py b/jax/experimental/mosaic/gpu/launch_context.py\nindex e4f0c4efa22c..2a5bb96f4708 100644\n--- a/jax/experimental/mosaic/gpu/launch_context.py\n+++ b/jax/experimental/mosaic/gpu/launch_context.py\n@@ -407,7 +407,10 @@ def _get_tma_desc(\n         \"add\",\"min\",\"max\",\"inc\",\"dec\",\"and\",\"or\",\"xor\"\n       ] | None,\n   ):\n-    tma_desc_key = (gmem_ref, transformed_slice_shape, swizzle, gmem_transform)\n+    # Using ir.Values in cache keys is a little sketchy, but I think it should\n+    # be fine. Having it in the key will keep it alive, and if comparison and\n+    # hashing is by identity then it should work out.\n+    tma_desc_key = (gmem_ref, transformed_slice_shape, swizzle, gmem_transform, gmem_peer_id)\n     if (tma_desc := self.tma_descriptors.get(tma_desc_key, None)) is None:\n       i32 = ir.IntegerType.get_signless(32)\n       i64 = ir.IntegerType.get_signless(64)\ndiff --git a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\nindex a6c372f2cee7..854d75dbf6a3 100644\n--- a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n@@ -140,9 +140,15 @@ def k_loop(idxs, lhs_smem, rhs_smem):\n             plgpu.wgmma(acc_ref, lhs_smem, rhs_smem)\n             k_slice = pl.ds(ki * block_k, block_k)\n             # TODO(apaszke): No need to send on the last step\n-            # TODO(apaszke): Use an async copy. This is uncoalesced.\n-            send_scratch_ref[next_scratch_slot, :, k_slice] = lhs_smem[...]\n+            plgpu.copy_smem_to_gmem(\n+                lhs_smem, send_scratch_ref.at[next_scratch_slot, :, k_slice]\n+            )\n+            # We only delay release by 1 step, so we need to wait for the\n+            # previous copies.\n+            plgpu.wait_smem_to_gmem(1, wait_read_only=True)\n           k_loop(scratch_ref.at[scratch_slot], rhs_ref)\n+          # Make sure the copy is fully done.\n+          plgpu.wait_smem_to_gmem(0, wait_read_only=False)\n           # TODO(apaszke): Both of those semaphores perform a .sys release.\n           # This is very expensive and we should only do a single .sys fence.\n           pl.semaphore_signal(capacity_sem, device_id=recv_dev_id, device_id_type=pl.DeviceIdType.LOGICAL)\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add missing allocator config and skips in one of our distributed tests\n\nI added them in all other files, but forgot about this one.\n\nPiperOrigin-RevId: 763352483",
        "output": "```diff\nCommit: f9c7a1421571ceb441011775af41ea75410ceeeb\nDate: 2025-05-26T10:05:04Z\nURL: https://github.com/jax-ml/jax/commit/f9c7a1421571ceb441011775af41ea75410ceeeb\nFiles changed: 1\nAdditions: +9, Deletions: -0\ndiff --git a/tests/mosaic/gpu_test_distributed.py b/tests/mosaic/gpu_test_distributed.py\nindex fee2ce5b03a6..cf3913771983 100644\n--- a/tests/mosaic/gpu_test_distributed.py\n+++ b/tests/mosaic/gpu_test_distributed.py\n@@ -13,6 +13,8 @@\n # limitations under the License.\n # ==============================================================================\n \n+import os\n+\n from absl.testing import parameterized\n import jax\n from jax._src import config\n@@ -50,6 +52,8 @@ def setUp(self):\n       self.skipTest(\"Only works on GPU with capability >= sm90\")\n     if not mgpu.supports_cross_device_collectives():\n       self.skipTest(\"NVSHMEM library unavailable.\")\n+    if os.environ.get(\"XLA_PYTHON_CLIENT_ALLOCATOR\", \"\") == \"platform\":\n+      self.skipTest(\"NVSHMEM doesn't work with the platform allocator.\")\n     if jax.process_count() == 1:\n       self.skipTest(\"Test requires multiple processes.\")\n     if jax.device_count() != jax.process_count():\n@@ -97,4 +101,9 @@ def kernel(ctx, src, dst, scratch):\n \n \n if __name__ == \"__main__\":\n+  # This test doesn't work with the platform allocator, so we override it\n+  # if it's ran alone. If it's part of a larger test suite and the platform\n+  # allocator is used, setUp will skip the test.\n+  os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.01'\n+  os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'default'\n   jt_multiprocess.main()\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Clarify that upper bound takes precedence in jnp.clip where bounds are incongruent",
        "output": "```diff\nCommit: c22bba2f9c237feb8743caf5378ee3b7966209bd\nDate: 2025-05-25T17:07:35Z\nURL: https://github.com/jax-ml/jax/commit/c22bba2f9c237feb8743caf5378ee3b7966209bd\nFiles changed: 2\nAdditions: +9, Deletions: -0\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex 0bd287dadd51..ad2b3ad6aa75 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -3410,6 +3410,7 @@ def clip(\n   Returns:\n     An array containing values from ``arr``, with values smaller than ``min`` set\n     to ``min``, and values larger than ``max`` set to ``max``.\n+    Wherever ``min`` is larger than ``max``, the value of ``max`` is returned.\n \n   See also:\n     - :func:`jax.numpy.minimum`: Compute the element-wise minimum value of two arrays.\ndiff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py\nindex 875024617b5f..29e6586ffa18 100644\n--- a/tests/lax_numpy_test.py\n+++ b/tests/lax_numpy_test.py\n@@ -1065,6 +1065,14 @@ def testClipDeprecatedArgs(self):\n                                              \"Passing arguments 'a', 'a_min' or 'a_max' to jax.numpy.clip is deprecated\"):\n       jnp.clip(jnp.arange(4), a_min=2, a_max=3)\n \n+  def testClipUpperPrecedence(self):\n+    a_min = 3 * np.ones(1)\n+    a_max = 2 * np.ones(1)\n+    x = 4 * np.ones(1)\n+    y = jnp.clip(x, min=a_min, max=a_max)\n+    assert y == a_max, f\"Expected {y} to equal {a_max} when a_min > a_max.\"\n+    assert y == jnp.asarray(np.clip(x, a_min=a_min, a_max=a_max))\n+\n   def testHypotComplexInputError(self):\n     rng = jtu.rand_default(self.rng())\n     x = rng((5,), dtype=jnp.complex64)\n\n```",
        "from_id": [
            "johanna.haffner@bsse.ethz.ch"
        ]
    },
    {
        "text_input": "Move jax/_src/sourcemap to its own build rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, discourages private imports downstream, and leads to improved build and iteration times.\n\nPiperOrigin-RevId: 762621491",
        "output": "```diff\nCommit: d0195f2240fdf081e011e193585593df2e060b26\nDate: 2025-05-24T00:07:02Z\nURL: https://github.com/jax-ml/jax/commit/d0195f2240fdf081e011e193585593df2e060b26\nFiles changed: 2\nAdditions: +8, Deletions: -1\ndiff --git a/jax/BUILD b/jax/BUILD\nindex de187b4ce597..a236cd206a55 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -321,7 +321,6 @@ py_library_providing_imports_info(\n         \"_src/random.py\",\n         \"_src/shard_alike.py\",\n         \"_src/shard_map.py\",\n-        \"_src/sourcemap.py\",\n     ] + glob(\n         [\n             \"*.py\",\n@@ -413,6 +412,7 @@ py_library_providing_imports_info(\n         \":sharding_impls\",\n         \":sharding_specs\",\n         \":source_info_util\",\n+        \":sourcemap\",\n         \":stages\",\n         \":traceback_util\",\n         \":tree\",\n@@ -795,6 +795,11 @@ pytype_strict_library(\n     ],\n )\n \n+pytype_strict_library(\n+    name = \"sourcemap\",\n+    srcs = [\"_src/sourcemap.py\"],\n+)\n+\n pytype_strict_library(\n     name = \"source_mapper\",\n     srcs = glob(include = [\"experimental/source_mapper/**/*.py\"]),\n@@ -806,6 +811,7 @@ pytype_strict_library(\n         \":core\",\n         \":jax\",\n         \":source_info_util\",\n+        \":sourcemap\",\n     ] + py_deps(\"absl/flags\"),\n )\n \ndiff --git a/tests/BUILD b/tests/BUILD\nindex 6c9f3f74b56a..6fac61933d59 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -2109,6 +2109,7 @@ jax_py_test(\n     srcs = [\"sourcemap_test.py\"],\n     deps = [\n         \"//jax\",\n+        \"//jax:sourcemap\",\n         \"//jax:test_util\",\n     ] + py_deps([\n         \"absl/testing\",\n\n```",
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move jax/_src/tree.py to its own build rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nPiperOrigin-RevId: 762589488",
        "output": "```diff\nCommit: 2b9d7c80a84401c39b2ae7b9083b1db9d0bbc22a\nDate: 2025-05-23T22:29:02Z\nURL: https://github.com/jax-ml/jax/commit/2b9d7c80a84401c39b2ae7b9083b1db9d0bbc22a\nFiles changed: 1\nAdditions: +9, Deletions: -1\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 5fb96d34d91e..de187b4ce597 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -322,7 +322,6 @@ py_library_providing_imports_info(\n         \"_src/shard_alike.py\",\n         \"_src/shard_map.py\",\n         \"_src/sourcemap.py\",\n-        \"_src/tree.py\",\n     ] + glob(\n         [\n             \"*.py\",\n@@ -416,6 +415,7 @@ py_library_providing_imports_info(\n         \":source_info_util\",\n         \":stages\",\n         \":traceback_util\",\n+        \":tree\",\n         \":tree_util\",\n         \":typing\",\n         \":util\",\n@@ -1207,6 +1207,14 @@ pytype_strict_library(\n     ] + py_deps(\"numpy\"),\n )\n \n+pytype_strict_library(\n+    name = \"tree\",\n+    srcs = [\"_src/tree.py\"],\n+    deps = [\n+        \":tree_util\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"tree_util\",\n     srcs = [\"_src/tree_util.py\"],\n\n```",
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[ragged-paged-attn] Implement static kv cache quantization. (The scale of kv cache is a scalar float value)\n\nPiperOrigin-RevId: 762576286",
        "output": "```diff\nCommit: 966bcb932ea962133d77ea2ad29d65a85127b402\nDate: 2025-05-23T21:49:28Z\nURL: https://github.com/jax-ml/jax/commit/966bcb932ea962133d77ea2ad29d65a85127b402\nFiles changed: 3\nAdditions: +171, Deletions: -49\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 586e9c2dd6e6..5fb96d34d91e 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -908,6 +908,7 @@ pytype_strict_library(\n         \":pallas_tpu_users\",\n     ],\n     deps = [\n+        \":dtypes\",\n         \":jax\",\n         \":pallas\",\n         \":pallas_tpu\",\ndiff --git a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\nindex d9d952d5a378..67c0b376ecc6 100644\n--- a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n+++ b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n@@ -22,11 +22,13 @@\n import functools\n import jax\n from jax import lax\n+from jax._src import dtypes\n from jax.experimental import pallas as pl\n from jax.experimental.pallas import tpu as pltpu\n from jax.experimental.pallas.ops.tpu.ragged_paged_attention.tuned_block_sizes import get_tuned_block_sizes\n import jax.numpy as jnp\n \n+\n DEFAULT_MASK_VALUE = -0.7 * float(jnp.finfo(jnp.dtype(\"float32\")).max)\n \n \n@@ -80,6 +82,8 @@ def ref_ragged_paged_attention(\n     sliding_window: int | None = None,\n     soft_cap: float | None = None,\n     mask_value: float | None = DEFAULT_MASK_VALUE,\n+    k_scale: float | None = None,\n+    v_scale: float | None = None,\n ):\n   static_validate_inputs(\n       queries,\n@@ -89,6 +93,8 @@ def ref_ragged_paged_attention(\n       cu_q_lens,\n       num_seqs,\n       sm_scale=sm_scale,\n+      k_scale=k_scale,\n+      v_scale=v_scale,\n       sliding_window=sliding_window,\n       soft_cap=soft_cap,\n       mask_value=mask_value,\n@@ -115,6 +121,12 @@ def ref_ragged_paged_attention(\n     v = kv_pages[indices, :, 1::2, :].reshape(-1, num_kv_heads, head_dim)[\n         :kv_len\n     ]\n+    if k_scale is not None:\n+      k = k.astype(jnp.float32) * k_scale\n+      k = k.astype(q.dtype)\n+    if v_scale is not None:\n+      v = v.astype(jnp.float32) * v_scale\n+      v = v.astype(q.dtype)\n     k = jnp.repeat(k, num_query_per_kv, axis=1)\n     v = jnp.repeat(v, num_query_per_kv, axis=1)\n     attn = jnp.einsum(\"qhd,khd->hqk\", q, k, preferred_element_type=jnp.float32)\n@@ -150,7 +162,9 @@ def dynamic_validate_inputs(\n     sliding_window: int | None = None,\n     soft_cap: float | None = None,\n     mask_value: float | None = None,\n-    # Kernel specific params.\n+    k_scale: float | None = None,\n+    v_scale: float | None = None,\n+    # Kernel tuning params.\n     num_kv_pages_per_block: int | None = None,\n     num_queries_per_block: int | None = None,\n     vmem_limit_bytes: int | None = None,\n@@ -166,6 +180,8 @@ def dynamic_validate_inputs(\n       sliding_window=sliding_window,\n       soft_cap=soft_cap,\n       mask_value=mask_value,\n+      k_scale=k_scale,\n+      v_scale=v_scale,\n       num_kv_pages_per_block=num_kv_pages_per_block,\n       num_queries_per_block=num_queries_per_block,\n       vmem_limit_bytes=vmem_limit_bytes,\n@@ -210,7 +226,9 @@ def static_validate_inputs(\n     sliding_window: int | None = None,\n     soft_cap: float | None = None,\n     mask_value: float | None = None,\n-    # Kernel specific params.\n+    k_scale: float | None = None,\n+    v_scale: float | None = None,\n+    # Kernel tuning params.\n     num_kv_pages_per_block: int | None = None,\n     num_queries_per_block: int | None = None,\n     vmem_limit_bytes: int | None = None,\n@@ -218,6 +236,8 @@ def static_validate_inputs(\n   _, num_q_heads, head_dim = q.shape\n   _, _, num_combined_kv_heads, head_dim_k = kv_pages.shape\n   assert num_combined_kv_heads % 2 == 0\n+  assert isinstance(k_scale, float) or k_scale is None\n+  assert isinstance(v_scale, float) or v_scale is None\n   num_kv_heads = num_combined_kv_heads // 2\n   max_num_seqs, pages_per_seq = page_indices.shape\n   if num_seqs.shape != (1,):\n@@ -291,6 +311,8 @@ def ragged_paged_attention_kernel(\n     sliding_window: int | None = None,\n     soft_cap: float | None = None,\n     mask_value: float | None = DEFAULT_MASK_VALUE,\n+    k_scale: float | None = None,\n+    v_scale: float | None = None,\n ):\n   if mask_value is None:\n     mask_value = DEFAULT_MASK_VALUE\n@@ -334,23 +356,41 @@ def create_kv_async_copy_descriptors(\n     return async_copy_kv\n \n   # TODO(jevinjiang): Add these to Mosaic:\n-  # 1. Support arbitrary strided load/store for any dtype.\n+  # 1. Support arbitrary strided load/store for int4 and int8 dtype.\n   # 2. Support arbitrary strided load/store for any last dimension.\n   def strided_load_kv(ref, start, step):\n-    if ref.dtype == jnp.float32:\n-      return ref[start::step, :], ref[start + 1 :: step, :]\n     packing = get_dtype_packing(ref.dtype)\n-    assert ref.dtype == jnp.bfloat16\n+    if packing == 1:\n+      return [ref[start::step, :]], [ref[start + 1 :: step, :]]\n+    assert packing in (2, 4, 8)\n     assert step % packing == 0\n+    k_list, v_list = [], []\n     b_start = start // packing\n     b_step = step // packing\n     b_ref = ref.bitcast(jnp.uint32)\n     b = b_ref[b_start::b_step, :]\n-    bk = b << 16\n-    bv = b & jnp.uint32(0xffff0000)\n-    k = pltpu.bitcast(bk, jnp.float32).astype(jnp.bfloat16)\n-    v = pltpu.bitcast(bv, jnp.float32).astype(jnp.bfloat16)\n-    return k, v\n+\n+    # TODO(chengjiyao): use the general strided loading logic for bf16 after\n+    # fixing the issue in mosaic's infer vector layout pass\n+    if ref.dtype == jnp.bfloat16:\n+      bk = b << 16\n+      bv = b & jnp.uint32(0xFFFF0000)\n+      k = pltpu.bitcast(bk, jnp.float32).astype(jnp.bfloat16)\n+      v = pltpu.bitcast(bv, jnp.float32).astype(jnp.bfloat16)\n+      k_list.append(k)\n+      v_list.append(v)\n+    else:\n+      bitwidth = 32 // packing\n+      bitcast_dst_dtype = jnp.dtype(f\"uint{bitwidth}\")\n+      for i in range(0, packing, 2):\n+        bk = b >> (i * bitwidth)\n+        k = pltpu.bitcast(bk.astype(bitcast_dst_dtype), ref.dtype)\n+        k_list.append(k)\n+        bv = b >> ((i + 1) * bitwidth)\n+        v = pltpu.bitcast(bv.astype(bitcast_dst_dtype), ref.dtype)\n+        v_list.append(v)\n+\n+    return k_list, v_list\n \n   def fold_on_2nd_minor(vec):\n     assert vec.dtype == jnp.bfloat16 or vec.dtype == jnp.float32\n@@ -578,25 +618,42 @@ def prefetch_next_kv_blk():\n           num_kv_pages_per_blk * page_size * num_combined_kv_heads_per_blk,\n           head_dim,\n       )\n-      for kv_head_idx in range(num_kv_heads_per_blk):\n-        q_head_idx = kv_head_idx * num_q_heads_per_kv_head\n-        # TODO(jevinjiang): extra handlig for packed type that can start at\n-        # unaligned position!\n-        q = fold_on_2nd_minor(\n-            q_ref[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :]\n-        )\n-        k, v = strided_load_kv(\n-            kv_ref, kv_head_idx * 2, num_combined_kv_heads_per_blk\n-        )\n-        flash_attention(\n-            q,\n-            k,\n-            v,\n-            l_ref.at[kv_head_idx],\n-            m_ref.at[kv_head_idx],\n-            acc_ref.at[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :],\n-            kv_blk_idx=kv_blk_idx,\n+      kv_packing = get_dtype_packing(kv_ref.dtype)\n+      # NOTE: kv_packing is divided by 2 because k and v are packed together.\n+      kv_load_step = max(1, kv_packing // 2)\n+      for kv_head_chunk_idx in range(0, num_kv_heads_per_blk, kv_load_step):\n+        k_list, v_list = strided_load_kv(\n+            kv_ref, kv_head_chunk_idx * 2, num_combined_kv_heads_per_blk\n         )\n+        for step_idx in range(kv_load_step):\n+          k = k_list[step_idx]\n+          v = v_list[step_idx]\n+          if k_scale is not None:\n+            # NOTE: Conversion between arbitrary data types is not supported.\n+            # That's why it is converted to float32 first.\n+            k = k.astype(jnp.float32) * k_scale\n+            k = k.astype(q_ref.dtype)\n+          if v_scale is not None:\n+            v = v.astype(jnp.float32) * v_scale\n+            v = v.astype(q_ref.dtype)\n+          kv_head_idx = kv_head_chunk_idx + step_idx\n+          q_head_idx = kv_head_idx * num_q_heads_per_kv_head\n+          # TODO(jevinjiang): extra handlig for packed type that can start at\n+          # unaligned position!\n+          q = fold_on_2nd_minor(\n+              q_ref[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :]\n+          )\n+          flash_attention(\n+              q,\n+              k,\n+              v,\n+              l_ref.at[kv_head_idx],\n+              m_ref.at[kv_head_idx],\n+              acc_ref.at[\n+                  :, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :\n+              ],\n+              kv_blk_idx=kv_blk_idx,\n+          )\n       return kv_blk_idx + 1, next_buf_idx\n \n     _, next_buf_idx = lax.while_loop(\n@@ -625,15 +682,8 @@ def cdiv(a, b):\n \n \n def get_dtype_packing(dtype):\n-  if dtype == jnp.float32:\n-    return 1\n-  if dtype == jnp.bfloat16:\n-    return 2\n-  if dtype == jnp.int8:\n-    return 4\n-  if dtype == jnp.int4:\n-    return 8\n-  raise ValueError(f\"Not implemented: unsupported {dtype=}\")\n+  bits = dtypes.bit_width(dtype)\n+  return 32 // bits\n \n \n def get_min_heads_per_blk(\n@@ -681,6 +731,8 @@ def can_be_xla_fully_tiled(x, packing):\n         \"vmem_limit_bytes\",\n         \"sliding_window\",\n         \"soft_cap\",\n+        \"k_scale\",\n+        \"v_scale\",\n     ],\n )\n def ragged_paged_attention(\n@@ -696,6 +748,8 @@ def ragged_paged_attention(\n     sliding_window: int | None = None,\n     soft_cap: float | None = None,\n     mask_value: float | None = DEFAULT_MASK_VALUE,\n+    k_scale: float | None = None,\n+    v_scale: float | None = None,\n     num_kv_pages_per_block: int | None = None,\n     num_queries_per_block: int | None = None,\n     vmem_limit_bytes: int | None = None,\n@@ -715,6 +769,8 @@ def ragged_paged_attention(\n     sliding_window: the sliding window size for the attention.\n     soft_cap: the logit soft cap for the attention.\n     mask_value: mask value for causal mask.\n+    k_scale: the scale for the key cache.\n+    v_scale: the scale for the value cache.\n     num_kv_pages_per_block: number of kv pages to be processed in one flash\n       attention block in the pallas kernel.\n     num_queries_per_block: number of kv pages to be processed in one flash\n@@ -735,6 +791,8 @@ def ragged_paged_attention(\n       sliding_window=sliding_window,\n       soft_cap=soft_cap,\n       mask_value=mask_value,\n+      k_scale=k_scale,\n+      v_scale=v_scale,\n       num_kv_pages_per_block=num_kv_pages_per_block,\n       num_queries_per_block=num_queries_per_block,\n       vmem_limit_bytes=vmem_limit_bytes,\n@@ -823,6 +881,8 @@ def q_index_map(heads_blk_idx, q_blk_idx, *_):\n           sliding_window=sliding_window,\n           soft_cap=soft_cap,\n           mask_value=mask_value,\n+          k_scale=k_scale,\n+          v_scale=v_scale,\n       ),\n       grid_spec=pltpu.PrefetchScalarGridSpec(\n           num_scalar_prefetch=len(scalar_prefetches),\ndiff --git a/tests/pallas/tpu_ragged_paged_attention_test.py b/tests/pallas/tpu_ragged_paged_attention_test.py\nindex 4265445c69c7..eebc292ce3ab 100644\n--- a/tests/pallas/tpu_ragged_paged_attention_test.py\n+++ b/tests/pallas/tpu_ragged_paged_attention_test.py\n@@ -17,6 +17,7 @@\n from absl.testing import absltest\n from absl.testing import parameterized\n import jax\n+from jax._src import dtypes\n from jax._src import test_util as jtu\n from jax.experimental.pallas.ops.tpu.ragged_paged_attention import (\n     cdiv,\n@@ -39,7 +40,8 @@ def _test_ragged_paged_attention(\n       num_heads,  # [num_q_heads, num_kv_heads]\n       head_dim,\n       page_size,\n-      dtype,\n+      q_dtype,\n+      kv_dtype,\n       num_pages,\n       *,\n       num_kv_pages_per_block=8,\n@@ -49,6 +51,8 @@ def _test_ragged_paged_attention(\n       max_num_seq=8,\n       sliding_window: int | None = None,\n       soft_cap: float | None = None,\n+      k_scale: float | None = None,\n+      v_scale: float | None = None,\n   ):\n     if not jtu.is_device_tpu_at_least(version=4):\n       self.skipTest(\"Expect TPUv4+\")\n@@ -70,17 +74,27 @@ def _test_ragged_paged_attention(\n     q = jax.random.normal(\n         k0,\n         (max_num_batched_tokens, num_q_heads, head_dim),\n-        dtype=dtype,\n+        dtype=q_dtype,\n     )\n     page_cnt = 0\n     page_indices_list = []\n     kv_pages_list = []\n     for kv_len in kv_lens:\n-      kv = jax.random.normal(\n-          k1,\n-          (kv_len, num_kv_heads * 2, head_dim),\n-          dtype=dtype,\n-      )\n+      if jnp.issubdtype(kv_dtype, jnp.integer):\n+        # random.randint doesn't support int4, so we use jnp.int32 here and then\n+        # convert to the desired dtype.\n+        kv = jax.random.normal(\n+            k1,\n+            (kv_len, num_kv_heads * 2, head_dim),\n+            dtype=jnp.int32,\n+        )\n+        kv = kv.astype(kv_dtype)\n+      else:\n+        kv = jax.random.normal(\n+            k1,\n+            (kv_len, num_kv_heads * 2, head_dim),\n+            dtype=kv_dtype,\n+        )\n       kv = jnp.pad(\n           kv,\n           ((0, cdiv(kv_len, page_size) * page_size - kv_len), (0, 0), (0, 0)),\n@@ -138,7 +152,9 @@ def _test_ragged_paged_attention(\n         vmem_limit_bytes=vmem_limit_bytes,\n         sliding_window=sliding_window,\n         soft_cap=soft_cap,\n-    )[: actual_num_q_tokens]\n+        k_scale=k_scale,\n+        v_scale=v_scale,\n+    )[:actual_num_q_tokens]\n \n     expected = ref_ragged_paged_attention(\n         q,\n@@ -149,12 +165,17 @@ def _test_ragged_paged_attention(\n         num_seqs=num_seqs,\n         sliding_window=sliding_window,\n         soft_cap=soft_cap,\n+        k_scale=k_scale,\n+        v_scale=v_scale,\n     )\n+    dtype_bits = dtypes.bit_width(jnp.dtype(kv_dtype))\n     tols = {\n-        \"float32\": 0.15,\n-        \"bfloat16\": 0.2,\n+        32: 0.15,\n+        16: 0.2,\n+        8: 0.2,\n+        4: 0.2,\n     }\n-    tol = tols[jnp.dtype(dtype).name]\n+    tol = tols[dtype_bits]\n     self.assertAllClose(output, expected, atol=tol, rtol=tol)\n \n   @parameterized.product(\n@@ -173,9 +194,40 @@ def test_ragged_paged_attention_basic(self, dtype):\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n     )\n \n+  # TODO: support int4 and int8\n+  @parameterized.product(\n+      q_dtype=[jnp.bfloat16],\n+      kv_dtype=[jnp.float8_e5m2, jnp.float8_e4m3fn],\n+      kv_scales=[(0.5, 0.5), (None, None)],\n+  )\n+  def test_ragged_paged_attention_quantized_kv_cache(\n+      self, q_dtype, kv_dtype, kv_scales\n+  ):\n+    if not jtu.is_device_tpu_at_least(version=5):\n+      self.skipTest(\"Expect TPUv5+\")\n+    seq_lens = [(192, 328), (128, 180), (64, 255)]\n+    num_heads = (32, 8)\n+    head_dim = 128\n+    page_size = 16\n+    num_pages = 1000\n+    k_scale, v_scale = kv_scales\n+\n+    self._test_ragged_paged_attention(\n+        seq_lens,\n+        num_heads,\n+        head_dim,\n+        page_size,\n+        q_dtype,\n+        kv_dtype,\n+        num_pages,\n+        k_scale=k_scale,\n+        v_scale=v_scale,\n+    )\n+\n   @parameterized.product(\n       dtype=[jnp.float32, jnp.bfloat16],\n   )\n@@ -209,6 +261,7 @@ def test_ragged_paged_attention_decode_only(self, dtype):\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n     )\n \n@@ -245,6 +298,7 @@ def test_ragged_paged_attention_prefill_only(self, dtype):\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n     )\n \n@@ -281,6 +335,7 @@ def test_ragged_paged_attention_mixed(self, dtype):\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n     )\n \n@@ -316,6 +371,7 @@ def test_ragged_paged_attention_complex(\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n         num_kv_pages_per_block=num_kv_pages_per_block,\n         num_queries_per_block=num_queries_per_block,\n@@ -351,6 +407,7 @@ def test_ragged_paged_attention_sliding_window(\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n         num_kv_pages_per_block=num_kv_pages_per_block,\n         num_queries_per_block=num_queries_per_block,\n@@ -386,6 +443,7 @@ def test_ragged_paged_attention_logit_soft_capping(\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n         num_kv_pages_per_block=num_kv_pages_per_block,\n         num_queries_per_block=num_queries_per_block,\n@@ -407,6 +465,7 @@ def test_ragged_paged_attention_sliding_window_should_be_positive(self):\n           head_dim,\n           page_size,\n           dtype,\n+          dtype,\n           num_pages,\n           sliding_window=0,\n       )\n@@ -418,6 +477,7 @@ def test_ragged_paged_attention_sliding_window_should_be_positive(self):\n           head_dim,\n           page_size,\n           dtype,\n+          dtype,\n           num_pages,\n           sliding_window=-1,\n       )\n@@ -437,6 +497,7 @@ def test_ragged_paged_attention_soft_cap_cannot_be_zero(self):\n           head_dim,\n           page_size,\n           dtype,\n+          dtype,\n           num_pages,\n           soft_cap=0.0,\n       )\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Simplify attention VJP definition\n\nPiperOrigin-RevId: 762567722",
        "output": "```diff\nCommit: 292dea67fa3f550b8add78ea1840b566d95069b1\nDate: 2025-05-23T21:25:49Z\nURL: https://github.com/jax-ml/jax/commit/292dea67fa3f550b8add78ea1840b566d95069b1\nFiles changed: 2\nAdditions: +54, Deletions: -73\ndiff --git a/jax/experimental/pallas/ops/gpu/attention.py b/jax/experimental/pallas/ops/gpu/attention.py\nindex ccb3ae8fd3b7..2442ed14f351 100644\n--- a/jax/experimental/pallas/ops/gpu/attention.py\n+++ b/jax/experimental/pallas/ops/gpu/attention.py\n@@ -152,7 +152,7 @@ def body(start_k, carry):\n       # Apply mask to qk.\n       qk = jnp.where(mask, qk, DEFAULT_MASK_VALUE)\n \n-    m_curr = qk.max(axis=-1)\n+    m_curr = jnp.max(qk, axis=-1)\n     m_next = jnp.maximum(m_prev, m_curr)\n     correction = jnp.exp2(m_prev - m_next)\n     l_prev_corr = correction * l_prev\n@@ -201,7 +201,7 @@ def segment_mask(\n \n \n @functools.partial(\n-    jax.custom_vjp, nondiff_argnums=[4, 5, 6, 7, 8, 9, 10, 11, 12]\n+    jax.custom_vjp, nondiff_argnums=[4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n )\n @functools.partial(\n     jax.jit,\n@@ -215,6 +215,7 @@ def segment_mask(\n         \"grid\",\n         \"interpret\",\n         \"debug\",\n+        \"return_residuals\",\n     ],\n )\n def mha(\n@@ -231,6 +232,7 @@ def mha(\n     grid: tuple[int, ...] | None = None,\n     interpret: bool = False,\n     debug: bool = False,\n+    return_residuals: bool = False,\n ):\n   del backward_pass_impl\n   batch_size, q_seq_len, num_heads, head_dim = q.shape\n@@ -273,14 +275,19 @@ def mha(\n       if segment_ids is None\n       else pl.BlockSpec((None, kv_seq_len), lambda _, j, k: (j, 0))\n   )\n-  out_shape = jax.ShapeDtypeStruct(shape=q.shape, dtype=q.dtype)\n-  return pl.pallas_call(\n+  out_shape = [q]\n+  out_specs = [pl.BlockSpec((None, block_q, None, head_dim_padded),\n+                            lambda i, j, k: (j, i, k, 0))]\n+  if return_residuals:\n+    out_shape.append(jax.ShapeDtypeStruct(\n+        shape=(batch_size, num_heads, q_seq_len), dtype=jnp.float32))  # lse\n+    out_specs.append(\n+        pl.BlockSpec((None, None, block_q), lambda i, j, k: (j, k, i)))  # lse\n+  out = pl.pallas_call(\n       kernel,\n       grid=grid_,\n       in_specs=in_specs,\n-      out_specs=pl.BlockSpec(\n-          (None, block_q, None, head_dim_padded), lambda i, j, k: (j, i, k, 0)\n-      ),\n+      out_specs=out_specs,\n       compiler_params=plgpu.TritonCompilerParams(\n           num_warps=num_warps_, num_stages=num_stages),\n       out_shape=out_shape,\n@@ -288,6 +295,7 @@ def mha(\n       interpret=interpret,\n       name=\"mha_forward\",\n   )(q, k, v, segment_ids)\n+  return out if return_residuals else out[0]\n \n \n def _mha_forward(\n@@ -304,71 +312,17 @@ def _mha_forward(\n     grid: Any,\n     interpret: bool,\n     debug: bool,\n+    return_residuals: bool,\n ):\n-  del backward_pass_impl\n-  batch_size, q_seq_len, num_heads, head_dim = q.shape\n-  kv_seq_len = k.shape[1]\n-  block_q = min(block_sizes.block_q, q_seq_len)\n-  block_k = min(block_sizes.block_k, kv_seq_len)\n-  if (q.shape[-1] != k.shape[-1]) or (q.shape[-1] != v.shape[-1]):\n-    raise ValueError(\n-        f\"This kernel expects q, k, and v to have the same head dimension, but\"\n-        f\" found {q.shape=}, {k.shape=}, {v.shape=}.\"\n-    )\n-  if q_seq_len % block_q != 0:\n-    raise ValueError(f\"{q_seq_len=} must be a multiple of {block_q=}\")\n-  if kv_seq_len % block_k != 0:\n-    raise ValueError(f\"{kv_seq_len=} must be a multiple of {block_k=}\")\n-  head_dim_padded = pl.next_power_of_2(head_dim)\n-\n-  # Heuristics.\n-  grid_ = grid\n-  if grid_ is None:\n-    grid_ = (pl.cdiv(q_seq_len, block_q), batch_size, num_heads)\n-\n-  num_warps_ = num_warps\n-  if num_warps_ is None:\n-    num_warps_ = 4 if head_dim <= 64 else 8\n-  kernel = functools.partial(mha_forward_kernel, sm_scale=sm_scale,\n-                             causal=causal, block_q=block_q, block_k=block_k,\n-                             head_dim=head_dim)\n-  out_shape = [\n-      jax.ShapeDtypeStruct(shape=q.shape, dtype=q.dtype),  # out\n-      jax.ShapeDtypeStruct(\n-          shape=(batch_size, num_heads, q_seq_len), dtype=jnp.float32  # lse\n-      ),\n-  ]\n-  in_specs = [\n-      pl.BlockSpec((None, block_q, None, head_dim_padded),\n-                   lambda i, j, k: (j, i, k, 0)),\n-      pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n-                   lambda _, j, k: (j, 0, k, 0)),\n-      pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n-                   lambda _, j, k: (j, 0, k, 0)),\n-  ]\n-  in_specs.append(\n-      None  # type: ignore[arg-type]\n-      if segment_ids is None\n-      else pl.BlockSpec((None, kv_seq_len), lambda _, j, k: (j, 0))\n-  )\n-  out, lse = pl.pallas_call(\n-      kernel,\n-      grid=grid_,\n-      in_specs=in_specs,\n-      out_specs=[\n-          pl.BlockSpec((None, block_q, None, head_dim_padded),\n-                       lambda i, j, k: (j, i, k, 0)),\n-          pl.BlockSpec((None, None, block_q), lambda i, j, k: (j, k, i)),\n-      ],\n-      compiler_params=plgpu.TritonCompilerParams(\n-          num_warps=num_warps_, num_stages=num_stages\n-      ),\n-      out_shape=out_shape,\n-      debug=debug,\n-      interpret=interpret,\n-      name=\"mha_forward\",\n-  )(q, k, v, segment_ids)\n-  return out, (q, k, v, segment_ids, out, lse)\n+  out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n+                 causal=causal, block_sizes=block_sizes,\n+                 backward_pass_impl=backward_pass_impl,\n+                 num_warps=num_warps, num_stages=num_stages,\n+                 grid=grid, interpret=interpret, debug=debug,\n+                 return_residuals=True)\n+  residuals = (q, k, v, segment_ids, out, lse)\n+  ret = (out, lse) if return_residuals else out\n+  return ret, residuals\n \n \n def _preprocess_backward_kernel(out_ref, dout_ref, delta_ref, head_dim: int):\n@@ -576,9 +530,12 @@ def inner_loop_dq(start_k, dq):\n def _mha_backward(sm_scale: float, causal: bool, block_sizes: BlockSizes,\n                   backward_pass_impl: str, num_warps: int | None,\n                   num_stages: int, grid: Any, interpret: bool,\n-                  debug: bool, res, do):\n-  del num_stages, grid\n+                  debug: bool, return_residuals: bool, res, do):\n+  if return_residuals:\n+    raise ValueError(\n+        \"Kernel differentiation is not supported if return_residuals is True.\")\n   q, k, v, segment_ids, out, lse = res\n+  del num_stages, grid, return_residuals\n \n   if backward_pass_impl == \"xla\":\n     return jax.vjp(\ndiff --git a/tests/pallas/gpu_ops_test.py b/tests/pallas/gpu_ops_test.py\nindex 1637686365e1..cc2d15a8fdee 100644\n--- a/tests/pallas/gpu_ops_test.py\n+++ b/tests/pallas/gpu_ops_test.py\n@@ -313,6 +313,30 @@ def f_ref(q, k, v):\n     self.assertAllClose(dk, dk_ref, atol=5e-2)\n     self.assertAllClose(dv, dv_ref, atol=5e-2)\n \n+  def test_return_residuals_not_differentiable(self):\n+    batch_size, seq_len, num_heads, head_dim = 2, 128, 2, 128\n+    causal = False\n+    k1, k2, k3 = random.split(random.key(0), 3)\n+    q = random.normal(\n+        k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n+    )\n+    k = random.normal(\n+        k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n+    )\n+    v = random.normal(\n+        k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n+    )\n+    segment_ids = None\n+\n+    def f(q, k, v):\n+      return attention.mha(q, k, v, causal=causal, segment_ids=segment_ids,\n+                           interpret=self.INTERPRET,\n+                           return_residuals=True)[0].sum()\n+\n+    with self.assertRaisesRegex(ValueError, \"Kernel differentiation is not\"\n+                                \" supported if return_residuals is True.\"):\n+      _ = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n+\n \n class FusedAttentionInterpretTest(FusedAttentionTest):\n   INTERPRET = True\n\n```",
        "from_id": [
            "rdyro",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28979 from jakevdp:pytree-err\n\nPiperOrigin-RevId: 762562570",
        "output": "```diff\nCommit: 704c3c625979362e984ff9dd1c415867f8910db2\nDate: 2025-05-23T21:10:25Z\nURL: https://github.com/jax-ml/jax/commit/704c3c625979362e984ff9dd1c415867f8910db2\nFiles changed: 3\nAdditions: +32, Deletions: -3\ndiff --git a/jaxlib/pytree.cc b/jaxlib/pytree.cc\nindex 2700ac9e6c9a..bd845c47ec1e 100644\n--- a/jaxlib/pytree.cc\n+++ b/jaxlib/pytree.cc\n@@ -281,8 +281,16 @@ bool PyTreeDef::operator==(const PyTreeDef& other) const {\n         a.custom != b.custom) {\n       return false;\n     }\n-    if (a.node_data && a.node_data.not_equal(b.node_data)) {\n-      return false;\n+    try {\n+      if (a.node_data && a.node_data.not_equal(b.node_data)) {\n+        return false;\n+      }\n+    } catch (nb::python_error& e) {\n+      nb::raise_from(e, PyExc_ValueError,\n+                     \"Exception raised while checking equality of metadata \"\n+                     \"fields of pytree. Make sure that metadata fields are \"\n+                     \"hashable and have simple equality semantics. (Note: \"\n+                     \"arrays cannot be passed as metadata fields!)\");\n     }\n     if (!IsSortedPyDictKeysEqual(a.sorted_dict_keys, b.sorted_dict_keys)) {\n       return false;\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 24861bad81de..b9497b71dcb1 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 345\n+_version = 346\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\ndiff --git a/tests/tree_util_test.py b/tests/tree_util_test.py\nindex 8d4cd5854e7d..0d92156b2530 100644\n--- a/tests/tree_util_test.py\n+++ b/tests/tree_util_test.py\n@@ -1050,6 +1050,27 @@ def testPickle(self):\n       unpickled = pickle.loads(pickle.dumps(key))\n       self.assertEqual(key, unpickled)\n \n+  def testEqualityErrorWithArrayAsStaticArg(self):\n+    # Regression test for https://github.com/jax-ml/jax/issues/28659\n+    @tree_util.register_dataclass\n+    @dataclasses.dataclass\n+    class Tree:\n+      x : jnp.ndarray = dataclasses.field(metadata={'static': True})\n+\n+    f = jax.jit(lambda x: x)\n+\n+    if jax._src.lib.jaxlib_extension_version < 346:\n+      msg = \"The truth value of an array with more than one element is ambiguous.\"\n+    else:\n+      msg = \"Exception raised while checking equality of metadata fields of pytree.\"\n+\n+    # First call succeeds, because there is no equality check.\n+    f(Tree(jnp.arange(4)))\n+\n+    # Second fall fails, because arrays are marked static and compared for equality.\n+    with self.assertRaisesRegex(ValueError, msg):\n+      f(Tree(jnp.arange(4)))\n+\n \n class StaticTest(parameterized.TestCase):\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "TSAN CI, make jax buid/test step fail if missing deps wheels",
        "output": "```diff\nCommit: ae2f943b54bd353ad3731fc07b6ef3d723f83446\nDate: 2025-05-23T20:43:44Z\nURL: https://github.com/jax-ml/jax/commit/ae2f943b54bd353ad3731fc07b6ef3d723f83446\nFiles changed: 1\nAdditions: +7, Deletions: -3\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex ce4130c31a30..6cd502050344 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -116,6 +116,7 @@ jobs:\n       - name: Build TSAN Numpy wheel\n         if: steps.cache-numpy-tsan-restore.outputs.cache-hit != 'true'\n         run: |\n+          set -eux\n           cd numpy\n \n           # If we restored cpython from cache, we need to get python interpreter from python-tsan.tgz\n@@ -131,7 +132,6 @@ jobs:\n           export PATH=${GITHUB_WORKSPACE}/cpython-tsan/bin/:$PATH\n \n           python3 -m pip install uv~=0.5.30\n-\n           python3 -m uv pip install -r requirements/build_requirements.txt\n \n           CC=clang-18 CXX=clang++-18 python3 -m pip wheel --wheel-dir dist -v . --no-build-isolation -Csetup-args=-Db_sanitize=thread -Csetup-args=-Dbuildtype=debugoptimized\n@@ -268,11 +268,15 @@ jobs:\n             --bazel_options=--copt=-g \\\n             --clang_path=/usr/bin/clang-18\n \n-\n           mkdir -p dist\n+          # Check whether we have numpy wheel or exit with error\n+          ls ${GITHUB_WORKSPACE}/wheelhouse/numpy/*.whl || exit 1\n           cp -v ${GITHUB_WORKSPACE}/wheelhouse/numpy/*.whl dist/\n-          cp -v ${GITHUB_WORKSPACE}/wheelhouse/scipy/*.whl dist/\n           if [ \"${{ matrix.python-version }}\" == \"3.14\" ]; then\n+            # Check whether we have scipy wheel or exit with error\n+            ls ${GITHUB_WORKSPACE}/wheelhouse/scipy/*.whl || exit 1\n+            cp -v ${GITHUB_WORKSPACE}/wheelhouse/scipy/*.whl dist/\n+\n             # Patch build/requirements_lock_3_14_ft.txt to use TSAN instrumented NumPy and Scipy\n             sed -i \"s|--extra-index-url.*|--extra-index-url file://${GITHUB_WORKSPACE}/wheelhouse/|\" build/${{ matrix.requirements_lock_name }}.txt\n \n\n```",
        "from_id": [
            "vfdev-5"
        ]
    },
    {
        "text_input": "This is a change to patch some internal Google builds while we complete a refactor.\n\nPiperOrigin-RevId: 762547026",
        "output": "```diff\nCommit: 57d07e195b4643fd134abc175f46e108ea667875\nDate: 2025-05-23T20:32:14Z\nURL: https://github.com/jax-ml/jax/commit/57d07e195b4643fd134abc175f46e108ea667875\nFiles changed: 1\nAdditions: +7, Deletions: -0\ndiff --git a/jax/profiler.py b/jax/profiler.py\nindex 31f3ea186d79..d776791e9200 100644\n--- a/jax/profiler.py\n+++ b/jax/profiler.py\n@@ -14,6 +14,7 @@\n \n # Note: import <name> as <name> is required for names to be exported.\n # See PEP 484 & https://github.com/jax-ml/jax/issues/7570\n+from typing import Any\n \n from jax._src.profiler import (\n     ProfileOptions as ProfileOptions,\n@@ -28,3 +29,9 @@\n     stop_trace as stop_trace,\n     trace as trace,\n )\n+\n+# this is a temporary shim to please pytype in the meantime before the migration\n+# is complete for cl/760646494\n+ProfileData: Any = None\n+ProfileEvent: Any = None\n+ProfilePlane: Any = None\n\n```",
        "from_id": [
            "ZacCranko",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Transfer library: poison outstanding buffer fetches upon connection failure.\n\nPiperOrigin-RevId: 762546985",
        "output": "```diff\nCommit: 9153ab760bc146945f572a79d86fc345286d5f46\nDate: 2025-05-23T20:31:51Z\nURL: https://github.com/jax-ml/jax/commit/9153ab760bc146945f572a79d86fc345286d5f46\nFiles changed: 1\nAdditions: +7, Deletions: -0\ndiff --git a/jaxlib/py_socket_transfer.cc b/jaxlib/py_socket_transfer.cc\nindex 8086196b9df8..c7cc7c496b0b 100644\n--- a/jaxlib/py_socket_transfer.cc\n+++ b/jaxlib/py_socket_transfer.cc\n@@ -163,6 +163,8 @@ class PyTransferServerConnection {\n     }\n   }\n \n+  SocketServer::Connection& conn() { return *conn_; }\n+\n  private:\n   tsl::RCReference<SocketServer::Connection> conn_;\n };\n@@ -257,6 +259,11 @@ struct CopyDests {\n \n void RegisterTransferServerTypes(nanobind::module_& m) {\n   nb::class_<PyTransferServerConnection>(m, \"TransferConnection\")\n+#if JAX_IFRT_VERSION_NUMBER > 9\n+      .def(\n+          \"_testonly_inject_failure\",\n+          [](PyTransferServerConnection& self) { self.conn().InjectFailure(); })\n+#endif\n       .def(\"_pull_flat\", [](PyTransferServerConnection& self, uint64_t uuid,\n                             xla::nb_class_ptr<xla::PyClient> py_client,\n                             std::vector<nb::object> py_avals) {\n\n```",
        "from_id": [
            "pschuh",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move jax/_src/custom_dce.py to its own BUILD rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nThis was unblocked by moving batching & ad to their own rules in prior changes.\n\nPiperOrigin-RevId: 762527517",
        "output": "```diff\nCommit: f5a9d460723b417ca2032057f8b2ef5ad9e6fdf9\nDate: 2025-05-23T19:32:40Z\nURL: https://github.com/jax-ml/jax/commit/f5a9d460723b417ca2032057f8b2ef5ad9e6fdf9\nFiles changed: 1\nAdditions: +19, Deletions: -1\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 7c8847e2e94b..586e9c2dd6e6 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -303,7 +303,6 @@ py_library_providing_imports_info(\n         \"_src/callback.py\",\n         \"_src/checkify.py\",\n         \"_src/custom_batching.py\",\n-        \"_src/custom_dce.py\",\n         \"_src/custom_derivatives.py\",\n         \"_src/custom_partitioning.py\",\n         \"_src/custom_partitioning_sharding_rule.py\",\n@@ -390,6 +389,7 @@ py_library_providing_imports_info(\n         \":config\",\n         \":core\",\n         \":custom_api_util\",\n+        \":custom_dce\",\n         \":custom_transpose\",\n         \":deprecations\",\n         \":dtypes\",\n@@ -595,6 +595,24 @@ pytype_strict_library(\n     srcs = [\"_src/custom_api_util.py\"],\n )\n \n+pytype_strict_library(\n+    name = \"custom_dce\",\n+    srcs = [\"_src/custom_dce.py\"],\n+    deps = [\n+        \":ad\",\n+        \":api_util\",\n+        \":batching\",\n+        \":core\",\n+        \":custom_api_util\",\n+        \":mlir\",\n+        \":partial_eval\",\n+        \":source_info_util\",\n+        \":traceback_util\",\n+        \":tree_util\",\n+        \":util\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"custom_transpose\",\n     srcs = [\"_src/custom_transpose.py\"],\n\n```",
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add barrier transformation support to tcgen05_mma.\n\nAlso fix accumulator argument when it's dynamic.\n\nPiperOrigin-RevId: 762509416",
        "output": "```diff\nCommit: c4a90c193473a686c08a31650f23f4dc436c801e\nDate: 2025-05-23T18:43:10Z\nURL: https://github.com/jax-ml/jax/commit/c4a90c193473a686c08a31650f23f4dc436c801e\nFiles changed: 2\nAdditions: +106, Deletions: -16\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 379a972be9b0..1ec22bff3f6d 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -554,6 +554,7 @@ def _copy_gmem_to_smem_lowering(\n   )\n   return ()\n \n+\n lowering.register_lowering_rule(\n     copy_gmem_to_smem_p,\n     mgpu.LoweringSemantics.Lane,\n@@ -722,9 +723,14 @@ def _barrier_wait_pp_eqn(\n \n \n @lowering.register_lowering_rule(barrier_wait_p, mgpu.LoweringSemantics.Lane)\n-@lowering.register_lowering_rule(barrier_wait_p, mgpu.LoweringSemantics.Lane,\n-                                 gpu_core.PrimitiveSemantics.Warp)\n-@lowering.register_lowering_rule(barrier_wait_p, mgpu.LoweringSemantics.Warpgroup)\n+@lowering.register_lowering_rule(\n+    barrier_wait_p,\n+    mgpu.LoweringSemantics.Lane,\n+    gpu_core.PrimitiveSemantics.Warp,\n+)\n+@lowering.register_lowering_rule(\n+    barrier_wait_p, mgpu.LoweringSemantics.Warpgroup\n+)\n def _barrier_wait_lowering(\n     ctx: lowering.LoweringRuleContext,\n     barrier,\n@@ -1198,18 +1204,31 @@ def tcgen05_mma(acc: _Ref,\n   else:\n     b_transforms_leaves, b_transforms_tree = [], None\n \n+  if isinstance(barrier, pallas_core.TransformedRef):\n+    barrier_transforms_leaves, barrier_transforms_tree = jax.tree.flatten(\n+        barrier.transforms\n+    )\n+    barrier = barrier.ref\n+  else:\n+    barrier_transforms_leaves, barrier_transforms_tree = [], None\n+\n   tcgen05_mma_p.bind(acc, a, b, barrier, accumulate,\n                       *a_transforms_leaves, *b_transforms_leaves,\n+                      *barrier_transforms_leaves,\n                       a_transforms_tree=a_transforms_tree,\n                       b_transforms_tree=b_transforms_tree,\n+                      barrier_transforms_tree=barrier_transforms_tree,\n                       collective_axis=collective_axis)\n \n+\n @tcgen05_mma_p.def_abstract_eval\n def _tcgen05_mma_abstract_eval(acc, a, b, barrier, accumulate,\n                                *transforms_leaves,\n                                a_transforms_tree, b_transforms_tree,\n+                               barrier_transforms_tree,\n                                collective_axis):\n-  del (accumulate, transforms_leaves, a_transforms_tree, b_transforms_tree)\n+  del (accumulate, transforms_leaves, a_transforms_tree, b_transforms_tree,\n+       barrier_transforms_tree)\n \n   if acc.memory_space != gpu_core.TMEM:\n     raise ValueError(\"Accumulator must be a TMEM Ref.\")\n@@ -1233,6 +1252,7 @@ def _tcgen05_mma_abstract_eval(acc, a, b, barrier, accumulate,\n \n   return []\n \n+\n @lowering.register_lowering_rule(tcgen05_mma_p, *gpu_core.LANExWG_SEMANTICS)\n @lowering.register_lowering_rule(tcgen05_mma_p, *gpu_core.LANExWARP_SEMANTICS)\n def _tcgen05_mma_lowering(\n@@ -1245,16 +1265,26 @@ def _tcgen05_mma_lowering(\n     *transforms_leaves,\n     a_transforms_tree,\n     b_transforms_tree,\n+    barrier_transforms_tree,\n     collective_axis,\n ):\n   _, a_aval, b_aval, *_ = ctx.avals_in\n   lhs_swizzle: int | None = None\n   lhs_transpose: bool = False\n-  if a_transforms_tree is not None:\n-    a_transforms_leaves, b_transforms_leaves = util.split_list(\n-        transforms_leaves, [a_transforms_tree.num_leaves]\n-    )\n \n+  transforms_trees = (\n+      a_transforms_tree,\n+      b_transforms_tree,\n+      barrier_transforms_tree,\n+  )\n+  (a_transforms_leaves, b_transforms_leaves, barrier_transforms_leaves, _) = (\n+      util.split_list(\n+          transforms_leaves,\n+          [getattr(tree, \"num_leaves\", 0) for tree in transforms_trees],\n+      )\n+  )\n+\n+  if a_transforms_tree is not None:\n     a_transforms = a_transforms_tree.unflatten(a_transforms_leaves)\n     a_ref, a_transforms = lowering._handle_transforms(\n         ctx, a_ref, a_transforms, handle_transposes=False, handle_reshapes=True\n@@ -1276,9 +1306,8 @@ def _tcgen05_mma_lowering(\n     if lhs_tiling != (8, swizzle_elems):\n       raise ValueError(\"MMA lhs tiling does not fit swizzle. \"\n                        f\"{lhs_tiling=} expected={(8, swizzle_elems)}\")\n-  else:\n-    b_transforms_leaves = transforms_leaves  # type: ignore\n \n+  assert b_transforms_tree is not None\n   b_transforms = b_transforms_tree.unflatten(b_transforms_leaves)\n   b_ref, b_transforms = lowering._handle_transforms(\n       ctx, b_ref, b_transforms, handle_transposes=False, handle_reshapes=True\n@@ -1296,16 +1325,28 @@ def _tcgen05_mma_lowering(\n       raise NotImplementedError(\n           f\"Unsupported transforms: {b_transforms}.\"\n       )\n-\n   swizzle_elems = rhs_swizzle // b_aval.dtype.itemsize\n+  if rhs_tiling != (8, swizzle_elems):\n+    raise ValueError(\n+        \"MMA rhs tiling does not fit swizzle\"\n+        f\" {rhs_tiling=} expected={(8, swizzle_elems)}\"\n+    )\n+\n+  if barrier_transforms_tree is not None:\n+    barrier_transforms = barrier_transforms_tree.unflatten(\n+        barrier_transforms_leaves\n+    )\n+    indexer = _extract_barrier_indexer(barrier_transforms)\n+    if indexer is not None:\n+      barrier_ref = barrier_ref.__getitem__(\n+          *map(lowering._as_index, indexer.indices)\n+      )\n+\n   if lhs_swizzle is None:\n     lhs_swizzle = rhs_swizzle\n   elif rhs_swizzle != lhs_swizzle:\n     raise ValueError(\"MMA rhs swizzle must match lhs swizzle.\"\n                       f\" {lhs_swizzle=} {rhs_swizzle=}\")\n-  if rhs_tiling != (8, swizzle_elems):\n-    raise ValueError(\"MMA rhs tiling does not fit swizzle\"\n-                      f\" {rhs_tiling=} expected={(8, swizzle_elems)}\")\n   if lhs_transpose:\n     if isinstance(a_ref, tcgen05.TMEMRef):\n       raise ValueError(\"TMEM transpose not allowed.\")\n@@ -1314,6 +1355,9 @@ def _tcgen05_mma_lowering(\n     b_ref = mgpu.memref_transpose(b_ref, (1, 0, 3, 2))\n   if isinstance(accumulate, bool):\n     accumulate = mgpu.c(accumulate, ir.IntegerType.get_signless(1))\n+  elif isinstance(accumulate, mgpu.FragmentedArray):\n+    accumulate = accumulate.registers.item()\n+    assert isinstance(accumulate, ir.Value)\n \n   predicate = ctx.module_ctx.single_lane_predicate\n   collective = False\n@@ -1341,8 +1385,8 @@ def _tcgen05_mma_lowering(\n               acc,\n               a_ref,\n               b_ref,\n-              a_swizzle=lhs_swizzle,\n-              b_swizzle=rhs_swizzle,\n+              a_swizzle=int(lhs_swizzle),\n+              b_swizzle=int(rhs_swizzle),\n               accumulate=accumulate,\n               collective=collective,\n           )\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 029445143a0f..3c0b463ba1c7 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -2496,6 +2496,52 @@ def _scoped(a_smem, b_smem,\n     expected = x @ y\n     np.testing.assert_allclose(result, expected, rtol=1e-3)\n \n+  @parameterized.parameters((0,), (1,))\n+  def test_mma_barrier_indexing(\n+      self, barrier_index, shape=(128, 128), swizzle=128, dtype=jnp.float16\n+  ):\n+    self.skip_if_wg_semantics()\n+    swizzle_elems = swizzle // jnp.dtype(dtype).itemsize\n+    transforms = (\n+        plgpu.TilingTransform((8, swizzle_elems)),\n+        plgpu.SwizzleTransform(swizzle),\n+    )\n+\n+    def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref):\n+      plgpu.tcgen05_mma(\n+          acc_tmem,\n+          a_smem,\n+          b_smem,\n+          barrier_ref.at[barrier_index],\n+          accumulate=False,\n+      )\n+      plgpu.barrier_wait(barrier_ref.at[barrier_index])\n+      scratch_smem[...] = acc_tmem[...].astype(dtype)\n+      plgpu.commit_smem()\n+      plgpu.copy_smem_to_gmem(scratch_smem, out_ref)\n+      plgpu.wait_smem_to_gmem(0)\n+\n+    scratch_shapes = [\n+        plgpu.TMEM(shape, jnp.float32, packed=False),\n+        plgpu.SMEM(shape, dtype, transforms=transforms),\n+        plgpu.Barrier(num_arrivals=1, num_barriers=2, for_tensor_core=True),\n+    ]\n+    f = self.pallas_call(\n+        kernel,\n+        in_specs=(\n+            plgpu.BlockSpec(transforms=transforms, memory_space=plgpu.SMEM),\n+            plgpu.BlockSpec(transforms=transforms, memory_space=plgpu.SMEM),\n+        ),\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.GMEM),\n+        out_shape=jax.ShapeDtypeStruct(shape, dtype),\n+        scratch_shapes=scratch_shapes,\n+    )\n+    x = jax.random.uniform(jax.random.key(0), shape=shape, dtype=dtype)\n+    y = jax.random.uniform(jax.random.key(1), shape=shape, dtype=dtype)\n+    result = f(x, y)\n+    expected = x @ y\n+    np.testing.assert_allclose(result, expected, rtol=1e-3)\n+\n \n class PallasCallSm100AWGTest(\n     PallasCallSm100ATest, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n\n```",
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add support for copy_gmem_to_smem in Warp semantics.\n\nPiperOrigin-RevId: 762475094",
        "output": "```diff\nCommit: d4ab82637a9200752d30726d46281e97e5987427\nDate: 2025-05-23T17:17:09Z\nURL: https://github.com/jax-ml/jax/commit/d4ab82637a9200752d30726d46281e97e5987427\nFiles changed: 3\nAdditions: +56, Deletions: -12\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 53c890932e38..379a972be9b0 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -49,6 +49,7 @@\n import jax.numpy as jnp\n \n \n+WARP_SIZE = 32\n WARPGROUP_SIZE = 128\n \n \n@@ -464,7 +465,7 @@ def _copy_gmem_to_smem_lowering(\n     dst_transforms_treedef,\n     barrier_transforms_treedef,\n     collective_axes,\n-    warpgroup_sync: bool = True,\n+    for_warpgroup: bool = True,\n ):\n   flat_src_transforms, flat_dst_transforms, flat_barrier_transforms = (\n       util.split_list(\n@@ -505,15 +506,23 @@ def _copy_gmem_to_smem_lowering(\n   if ctx.module_ctx.lowering_semantics == mgpu.LoweringSemantics.Lane:\n     if bytes % WARPGROUP_SIZE:\n       raise NotImplementedError(\"Only aligned copies are supported\")\n-    # We arrive uniformly from each thread in the WG, so we need to divide the\n-    # number of bytes by the number of threads in the WG.\n-    # TODO: apaszke - Relax this. We can just select the WG leader and have it\n-    # arrive with the whole transfer size, while everyone else arrives with 0.\n-    # But we should continue using this scheme as it's likely to be faster.\n-    bytes //= WARPGROUP_SIZE\n-    if warpgroup_sync:\n+    if for_warpgroup:\n+      # We arrive uniformly from each thread in the WG, so we need to divide the\n+      # number of bytes by the number of threads in the WG.\n+      # TODO: apaszke - Relax this. We can just select the WG leader and have it\n+      # arrive with the whole transfer size, while everyone else arrives with 0.\n+      # But we should continue using this scheme as it's likely to be faster.\n+      bytes //= WARPGROUP_SIZE\n       mgpu.warpgroup_barrier()  # Make sure all reads have completed.\n-    barrier.arrive_expect_tx(bytes)\n+      barrier.arrive_expect_tx(bytes)\n+    else:\n+      # In Warp-level lowering, we arrive on each CUDA thread in a warp, but\n+      # the barrier still expects a full 128 arrivals so we arrive 4 times\n+      # on each CUDA thread instead.\n+      bytes //= WARP_SIZE\n+      barrier.arrive(arrival_count=3, can_complete=False)\n+      barrier.arrive_expect_tx(bytes)\n+\n     ctx.launch_ctx.async_copy(\n         src_ref=src,\n         dst_ref=dst,\n@@ -549,7 +558,7 @@ def _copy_gmem_to_smem_lowering(\n     copy_gmem_to_smem_p,\n     mgpu.LoweringSemantics.Lane,\n     primitive_semantics=gpu_core.PrimitiveSemantics.Warp,\n-)(functools.partial(_copy_gmem_to_smem_lowering, warpgroup_sync=False))\n+)(functools.partial(_copy_gmem_to_smem_lowering, for_warpgroup=False))\n \n \n def copy_gmem_to_smem(\n@@ -713,6 +722,8 @@ def _barrier_wait_pp_eqn(\n \n \n @lowering.register_lowering_rule(barrier_wait_p, mgpu.LoweringSemantics.Lane)\n+@lowering.register_lowering_rule(barrier_wait_p, mgpu.LoweringSemantics.Lane,\n+                                 gpu_core.PrimitiveSemantics.Warp)\n @lowering.register_lowering_rule(barrier_wait_p, mgpu.LoweringSemantics.Warpgroup)\n def _barrier_wait_lowering(\n     ctx: lowering.LoweringRuleContext,\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex 4aeb3358b97a..1915b0b45f11 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -816,9 +816,16 @@ def update_parities(self, parities: ir.Value) -> tuple[ir.Value, ir.Value]:\n     )\n     return parity, arith.xori(parities, bitmask)\n \n-  def arrive(self):\n+  def arrive(self, arrival_count: int = 1, can_complete: bool = True):\n     i64 = ir.IntegerType.get_signless(64)\n-    nvvm.mbarrier_arrive_shared(i64, self.get_ptr())\n+    if can_complete:\n+      if arrival_count > 1:\n+        count = c(arrival_count - 1, ir.IntegerType.get_signless(32))\n+        nvvm.mbarrier_arrive_nocomplete_shared(i64, self.get_ptr(), count)\n+      nvvm.mbarrier_arrive_shared(i64, self.get_ptr())\n+    else:\n+      count = c(arrival_count, ir.IntegerType.get_signless(32))\n+      nvvm.mbarrier_arrive_nocomplete_shared(i64, self.get_ptr(), count)\n \n   def arrive_expect_tx(\n       self, bytes: int | ir.Value, predicate: ir.Value | None = None\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex b3c2f11ee43f..029445143a0f 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1878,6 +1878,32 @@ def _():\n         },\n     )\n \n+  def test_copy_gmem_to_smem_from_different_warps(self):\n+    # In this test, we issue a copy from from warp 0 and await it in warp 1.\n+    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n+    @functools.partial(plgpu.kernel,\n+                       out_shape=jax.ShapeDtypeStruct((32, 32), jnp.float32))\n+    def kernel(x_ref, y_ref):\n+      def scope(smem_ref, tma_barrier):\n+        @pl.core_map(warp_mesh)\n+        def _():\n+          warp_id = lax.axis_index(\"warp\")\n+          @pl.when(warp_id == 0)\n+          def _():\n+            plgpu.copy_gmem_to_smem(x_ref.at[32:64], smem_ref, tma_barrier)\n+\n+          @pl.when(warp_id == 1)\n+          def _():\n+            plgpu.barrier_wait(tma_barrier)\n+            plgpu.copy_smem_to_gmem(smem_ref, y_ref)\n+        plgpu.wait_smem_to_gmem(0)\n+      pl.run_scoped(scope,\n+                    smem_ref=plgpu.SMEM((32, 32), jnp.float32),\n+                    tma_barrier=plgpu.Barrier(num_arrivals=1))\n+    x = jax.random.uniform(jax.random.key(42), (64, 32), jnp.float32)\n+    result = kernel(x)\n+    np.testing.assert_array_equal(result, x[32:64])\n+\n \n class PallasCallWGTest(\n     PallasCallTest, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n\n```",
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[tree_util] raise more informative error when pytree equality check fails",
        "output": "```diff\nCommit: aa63a159e5a2ef6145f8b87538e16c6bc42970b8\nDate: 2025-05-23T17:11:39Z\nURL: https://github.com/jax-ml/jax/commit/aa63a159e5a2ef6145f8b87538e16c6bc42970b8\nFiles changed: 3\nAdditions: +32, Deletions: -3\ndiff --git a/jaxlib/pytree.cc b/jaxlib/pytree.cc\nindex 2700ac9e6c9a..bd845c47ec1e 100644\n--- a/jaxlib/pytree.cc\n+++ b/jaxlib/pytree.cc\n@@ -281,8 +281,16 @@ bool PyTreeDef::operator==(const PyTreeDef& other) const {\n         a.custom != b.custom) {\n       return false;\n     }\n-    if (a.node_data && a.node_data.not_equal(b.node_data)) {\n-      return false;\n+    try {\n+      if (a.node_data && a.node_data.not_equal(b.node_data)) {\n+        return false;\n+      }\n+    } catch (nb::python_error& e) {\n+      nb::raise_from(e, PyExc_ValueError,\n+                     \"Exception raised while checking equality of metadata \"\n+                     \"fields of pytree. Make sure that metadata fields are \"\n+                     \"hashable and have simple equality semantics. (Note: \"\n+                     \"arrays cannot be passed as metadata fields!)\");\n     }\n     if (!IsSortedPyDictKeysEqual(a.sorted_dict_keys, b.sorted_dict_keys)) {\n       return false;\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 24861bad81de..b9497b71dcb1 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 345\n+_version = 346\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\ndiff --git a/tests/tree_util_test.py b/tests/tree_util_test.py\nindex 8d4cd5854e7d..0d92156b2530 100644\n--- a/tests/tree_util_test.py\n+++ b/tests/tree_util_test.py\n@@ -1050,6 +1050,27 @@ def testPickle(self):\n       unpickled = pickle.loads(pickle.dumps(key))\n       self.assertEqual(key, unpickled)\n \n+  def testEqualityErrorWithArrayAsStaticArg(self):\n+    # Regression test for https://github.com/jax-ml/jax/issues/28659\n+    @tree_util.register_dataclass\n+    @dataclasses.dataclass\n+    class Tree:\n+      x : jnp.ndarray = dataclasses.field(metadata={'static': True})\n+\n+    f = jax.jit(lambda x: x)\n+\n+    if jax._src.lib.jaxlib_extension_version < 346:\n+      msg = \"The truth value of an array with more than one element is ambiguous.\"\n+    else:\n+      msg = \"Exception raised while checking equality of metadata fields of pytree.\"\n+\n+    # First call succeeds, because there is no equality check.\n+    f(Tree(jnp.arange(4)))\n+\n+    # Second fall fails, because arrays are marked static and compared for equality.\n+    with self.assertRaisesRegex(ValueError, msg):\n+      f(Tree(jnp.arange(4)))\n+\n \n class StaticTest(parameterized.TestCase):\n \n\n```",
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Rename backend.compile to backend.compile_and_load.\n\nPart of a larger refactor. Today, `compile` returns a loaded executable i.e., fuses the compile and load functions. Eventually, `compile` should return an unloaded executable and `load` should return a loaded exectuable; the default jit path will still return a loaded executable.\n\nPiperOrigin-RevId: 762457830",
        "output": "```diff\nCommit: 9928409798fbdf4b9a0b811e78a7bb1698caeda3\nDate: 2025-05-23T16:37:44Z\nURL: https://github.com/jax-ml/jax/commit/9928409798fbdf4b9a0b811e78a7bb1698caeda3\nFiles changed: 1\nAdditions: +48, Deletions: -11\ndiff --git a/jax/_src/compiler.py b/jax/_src/compiler.py\nindex 343f747efbd7..4f805034e99c 100644\n--- a/jax/_src/compiler.py\n+++ b/jax/_src/compiler.py\n@@ -34,7 +34,9 @@\n from jax._src import profiler\n from jax._src import traceback_util\n from jax._src.interpreters import mlir\n+from jax._src.lib import jaxlib_extension_version\n from jax._src.lib import xla_client as xc\n+from jax._src.lib import _jax\n from jax._src.lib.mlir import ir\n import numpy as np\n \n@@ -291,6 +293,19 @@ def backend_compile(\n     executable_devices: xc.DeviceList,\n     options: xc.CompileOptions,\n     host_callbacks: Sequence[Any],\n+) -> xc.LoadedExecutable:\n+  return backend_compile_and_load(\n+      backend, module, executable_devices, options, host_callbacks\n+  )\n+\n+\n+@profiler.annotate_function\n+def backend_compile_and_load(\n+    backend: xc.Client,\n+    module: ir.Module,\n+    executable_devices: xc.DeviceList,\n+    options: xc.CompileOptions,\n+    host_callbacks: Sequence[Any],\n ) -> xc.LoadedExecutable:\n   sym_name = module.operation.attributes['sym_name']\n   module_name = ir.StringAttr(sym_name).value\n@@ -315,18 +330,40 @@ def backend_compile(\n   try:\n     # we use a separate function call to ensure that XLA compilation appears\n     # separately in Python profiling results\n-    if host_callbacks:\n+    # TODO(dsuo): Simplify this logic once backend_compile actually returns an\n+    # unloaded executable.\n+    if jaxlib_extension_version < 345 or (\n+        jaxlib_extension_version >= 345\n+        and isinstance(backend, _jax.CompileOnlyPyClient)\n+    ):\n+      if host_callbacks:\n+        return backend.compile(\n+            built_c,\n+            executable_devices=executable_devices,  # type: ignore\n+            compile_options=options,\n+            host_callbacks=host_callbacks,\n+        )\n+      # Some backends don't have `host_callbacks` option yet\n+      # TODO(sharadmv): remove this fallback when all backends allow `compile`\n+      # to take in `host_callbacks`\n       return backend.compile(\n+          built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n+    else:\n+      if host_callbacks:\n+        return backend.compile_and_load(\n+            built_c,\n+            executable_devices=executable_devices,\n+            compile_options=options,\n+            host_callbacks=host_callbacks,\n+        )\n+      # Some backends don't have `host_callbacks` option yet\n+      # TODO(sharadmv): remove this fallback when all backends allow `compile`\n+      # to take in `host_callbacks`\n+      return backend.compile_and_load(\n           built_c,\n-          executable_devices=executable_devices,  # type: ignore\n+          executable_devices=executable_devices,\n           compile_options=options,\n-          host_callbacks=host_callbacks,\n       )\n-    # Some backends don't have `host_callbacks` option yet\n-    # TODO(sharadmv): remove this fallback when all backends allow `compile`\n-    # to take in `host_callbacks`\n-    return backend.compile(\n-        built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n   except xc.XlaRuntimeError as e:\n     for error_handler in _XLA_RUNTIME_ERROR_HANDLERS:\n       handler_result = error_handler(e)\n@@ -391,7 +428,7 @@ def compile_or_get_cached(\n   )\n \n   if cache_key is None:\n-    return backend_compile(\n+    return backend_compile_and_load(\n         backend, computation, executable_devices, compile_options,\n         host_callbacks)\n \n@@ -419,7 +456,7 @@ def compile_or_get_cached(\n       config.share_binary_between_hosts.value\n       and is_multi_process\n       and distributed.global_state.client is not None\n-      # Host callbacks are currently baked into the HLO module so we cant share\n+      # Host callbacks are currently baked into the HLO module so we can't share\n       # them.\n       and len(host_callbacks) == 0\n   ):\n@@ -705,7 +742,7 @@ def _compile_and_write_cache(\n     cache_key: str,\n ) -> xc.LoadedExecutable:\n   start_time = time.monotonic()\n-  executable = backend_compile(\n+  executable = backend_compile_and_load(\n       backend, computation, executable_devices, compile_options, host_callbacks\n   )\n   compile_time = time.monotonic() - start_time\n\n```",
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move jax/_src/interpreters/batching.py into its own BUILD rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nUnfortunately this is not a clean build refactor, because batching depends on jax.lax, which in turn depends on batching. However, the problematic functions are only called within contexts where jax.lax is available for import.\n\nWe have a few options here:\n\n1. Continue to bundle the batching.py source with the main build.\n2. Build separately, but do the local import workaround in this CL (a pattern we use elsewhere).\n3. Build this separately, but move some batching definitions into jax.lax for a more strict dependency graph. Or pass the `lax` namespace explicitly to the function at the call site.\n\nI opted for (2) here because I judged the benefits of a refactored build to be worth the cost of localized impure dependencies, and the kind of refactoring in (3) would affect some downstream users.\n\nPiperOrigin-RevId: 762447323",
        "output": "```diff\nCommit: c2c55aef522abcff80fbcf9b11dd8faaa28924be\nDate: 2025-05-23T16:09:27Z\nURL: https://github.com/jax-ml/jax/commit/c2c55aef522abcff80fbcf9b11dd8faaa28924be\nFiles changed: 2\nAdditions: +38, Deletions: -10\ndiff --git a/jax/BUILD b/jax/BUILD\nindex e7f1fad3121d..7c8847e2e94b 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -315,7 +315,6 @@ py_library_providing_imports_info(\n         \"_src/ffi.py\",\n         \"_src/flatten_util.py\",\n         \"_src/interpreters/__init__.py\",\n-        \"_src/interpreters/batching.py\",\n         \"_src/interpreters/pxla.py\",\n         \"_src/pjit.py\",\n         \"_src/prng.py\",\n@@ -383,6 +382,7 @@ py_library_providing_imports_info(\n         \":ad_util\",\n         \":api_util\",\n         \":basearray\",\n+        \":batching\",\n         \":cloud_tpu_init\",\n         \":compilation_cache_internal\",\n         \":compiler\",\n@@ -707,6 +707,24 @@ pytype_strict_library(\n     ],\n )\n \n+pytype_strict_library(\n+    name = \"batching\",\n+    srcs = [\"_src/interpreters/batching.py\"],\n+    deps = [\n+        \":ad_util\",\n+        \":config\",\n+        \":core\",\n+        \":mesh\",\n+        \":partial_eval\",\n+        \":partition_spec\",\n+        \":sharding_impls\",\n+        \":source_info_util\",\n+        \":tree_util\",\n+        \":typing\",\n+        \":util\",\n+    ] + py_deps(\"numpy\"),\n+)\n+\n pytype_strict_library(\n     name = \"mlir\",\n     srcs = [\"_src/interpreters/mlir.py\"],\ndiff --git a/jax/_src/interpreters/batching.py b/jax/_src/interpreters/batching.py\nindex 0fbe54a30672..55769aa307fc 100644\n--- a/jax/_src/interpreters/batching.py\n+++ b/jax/_src/interpreters/batching.py\n@@ -21,7 +21,6 @@\n \n import numpy as np\n \n-import jax\n from jax._src import config\n from jax._src import core\n from jax._src import source_info_util\n@@ -301,11 +300,14 @@ def _cont(axis_size, elt, axis):\n from_elt_handlers: dict[type, FromEltHandler] = {}\n \n def make_iota(axis_size: AxisSize) -> Array:\n+  # Callers of this utility, via batch() or vtile(), must be in a context\n+  # where lax is importable.\n+  from jax import lax  # pytype: disable=import-error\n   handler = make_iota_handlers.get(type(axis_size))\n   if handler:\n     return handler(axis_size)\n   else:\n-    return jax.lax.iota('int32', int(axis_size))\n+    return lax.iota('int32', int(axis_size))\n make_iota_handlers: dict[type, MakeIotaHandler] = {}\n \n def register_vmappable(data_type: type, spec_type: type, axis_size_type: type,\n@@ -1019,10 +1021,13 @@ def broadcast_batcher(prim, args, dims, **params):\n     return (out, (0,) * len(out)) if prim.multiple_results else (out, 0)\n \n def _handle_scalar_broadcasting(nd, x, d):\n+  # Callers of this utility, via broadcast_batcher() or defbroadcasting(),\n+  # must be in a context where lax is importable.\n+  from jax import lax  # pytype: disable=import-error\n   if d is not_mapped or nd == np.ndim(x):\n     return x\n   else:\n-    return jax.lax.expand_dims(x, tuple(range(np.ndim(x), nd)))\n+    return lax.expand_dims(x, tuple(range(np.ndim(x), nd)))\n \n def defreducer(prim, ident):\n   primitive_batchers[prim] = partial(reducer_batcher, prim, ident)\n@@ -1078,17 +1083,20 @@ def mask_ragged_axes(operand: Array, ident, axis_spec: RaggedAxis) -> Array:\n \n def _mask_one_ragged_axis(\n     operand: Array, ident, axis_spec: RaggedAxis) -> Array:\n+  # Callers of this utility, via reducer_batcher() or defreducer(),\n+  # must be in a context where lax is importable.\n+  from jax import lax  # pytype: disable=import-error\n   assert len(axis_spec.ragged_axes) == 1, \"Mask just one ragged axis at a time\"\n   ragged_axis, segment_lengths = axis_spec.ragged_axes[0]\n   value = ident(operand.dtype)\n-  positions = jax.lax.broadcasted_iota('int32', operand.shape, ragged_axis)\n+  positions = lax.broadcasted_iota('int32', operand.shape, ragged_axis)\n   # TODO(mattjj, axch) can't get ._data, need to convert it\n-  # lengths = jax.lax.convert_element_type(segment_lengths._data, 'int32')\n-  lengths = jax.lax.convert_element_type(segment_lengths, 'int32')\n-  limits = jax.lax.broadcast_in_dim(\n+  # lengths = lax.convert_element_type(segment_lengths._data, 'int32')\n+  lengths = lax.convert_element_type(segment_lengths, 'int32')\n+  limits = lax.broadcast_in_dim(\n       lengths, operand.shape, [axis_spec.stacked_axis])\n   mask = positions < limits\n-  return jax.lax.select(mask, operand, jax.lax.broadcast(value, operand.shape))\n+  return lax.select(mask, operand, lax.broadcast(value, operand.shape))\n \n def move_stacked_axis(operand, bdim, dst):\n   dst = canonicalize_axis(dst, operand.ndim)\n@@ -1103,6 +1111,8 @@ def move_stacked_axis(operand, bdim, dst):\n ### general utilities for manipulating axes on jaxpr types (not vmappables)\n \n def broadcast(x, sz, axis, mesh_axis=None):\n+  # Callers of this utility must be in a context where lax is importable.\n+  from jax import lax  # pytype: disable=import-error\n   shape = list(np.shape(x))\n   shape.insert(axis, sz)\n   broadcast_dims = tuple(np.delete(np.arange(len(shape)), axis))\n@@ -1114,7 +1124,7 @@ def broadcast(x, sz, axis, mesh_axis=None):\n   # TODO(dougalm, yashkatariya): Delete this context manager once we figure\n   # out how to ensure jaxpr arguments always have the context mesh.\n   with mesh_lib.use_abstract_mesh(sharding.mesh):\n-    x = jax.lax.broadcast_in_dim(x, shape, broadcast_dims, out_sharding=sharding)\n+    x = lax.broadcast_in_dim(x, shape, broadcast_dims, out_sharding=sharding)\n     if config._check_vma.value:\n       # TODO(yashkatariya,parkers): don't do this, fix during fixit week 2026\n       spmd_names = core.get_axis_env().spmd_axis_names\n\n```",
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[CI] Pin the ML Connect action to a specific sha\n\nPiperOrigin-RevId: 762441171",
        "output": "```diff\nCommit: 9d6553815f12d6903d4ccfd810519e31e2a97810\nDate: 2025-05-23T15:51:15Z\nURL: https://github.com/jax-ml/jax/commit/9d6553815f12d6903d4ccfd810519e31e2a97810\nFiles changed: 11\nAdditions: +12, Deletions: -12\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex 14d6b95b4347..c98bcee980e8 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -55,7 +55,7 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Bazel CPU tests with py_import (RBE)\ndiff --git a/.github/workflows/bazel_cpu_rbe.yml b/.github/workflows/bazel_cpu_rbe.yml\nindex ef5084960b30..a8b40c260260 100644\n--- a/.github/workflows/bazel_cpu_rbe.yml\n+++ b/.github/workflows/bazel_cpu_rbe.yml\n@@ -54,7 +54,7 @@ jobs:\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       # Since we do not have a Linux Arm64 RBE pool, we do not run the tests on Arm64. Instead, we\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 677d8d869a22..3e68034dfbf4 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -106,7 +106,7 @@ jobs:\n           exit 1\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Bazel CUDA tests (Non-RBE)\ndiff --git a/.github/workflows/bazel_cuda_rbe.yml b/.github/workflows/bazel_cuda_rbe.yml\nindex 5a2c94c4db47..83f651c0ef95 100644\n--- a/.github/workflows/bazel_cuda_rbe.yml\n+++ b/.github/workflows/bazel_cuda_rbe.yml\n@@ -50,7 +50,7 @@ jobs:\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Bazel CUDA Tests with RBE\ndiff --git a/.github/workflows/bazel_optional_h100_b200.yml b/.github/workflows/bazel_optional_h100_b200.yml\nindex 7381ce6d80bf..ec907280938e 100644\n--- a/.github/workflows/bazel_optional_h100_b200.yml\n+++ b/.github/workflows/bazel_optional_h100_b200.yml\n@@ -33,7 +33,7 @@ jobs:\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Bazel single B200 CUDA Tests\n@@ -75,7 +75,7 @@ jobs:\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Bazel multiple H100 CUDA Tests\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 1b534ee3b6fc..d5fc35a99cd5 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -127,7 +127,7 @@ jobs:\n         run: echo \"JAXCI_WRITE_TO_BAZEL_REMOTE_CACHE=1\" >> $GITHUB_ENV\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Build ${{ inputs.artifact }}\ndiff --git a/.github/workflows/numpy_nightly.yml b/.github/workflows/numpy_nightly.yml\nindex 51876a7eb71d..17357e9f1dd8 100644\n--- a/.github/workflows/numpy_nightly.yml\n+++ b/.github/workflows/numpy_nightly.yml\n@@ -54,7 +54,7 @@ jobs:\n           path: ml_dtypes\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Install numpy & scipy development versions\ndiff --git a/.github/workflows/oldest_supported_numpy.yml b/.github/workflows/oldest_supported_numpy.yml\nindex 06e7cf6230df..a63cb0b1c614 100644\n--- a/.github/workflows/oldest_supported_numpy.yml\n+++ b/.github/workflows/oldest_supported_numpy.yml\n@@ -51,7 +51,7 @@ jobs:\n           $JAXCI_PYTHON -m uv pip install -e .[minimum-jaxlib]\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Pytest CPU tests\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex fc4633110667..3af06fe8037e 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -140,7 +140,7 @@ jobs:\n           fi\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Pytest CPU tests\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 2f22901e661a..78f32cda672d 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -138,7 +138,7 @@ jobs:\n         run: $JAXCI_PYTHON -m uv pip install -r build/test-requirements.txt\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Pytest CUDA tests\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex ae0250884831..22cd64977dc5 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -152,7 +152,7 @@ jobs:\n           fi\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Pytest TPU tests\n\n```",
        "from_id": [
            "MichaelHudgins",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[jaxlib] Add CompileOnlyPyClient to xla_client.\n\nWe have users of CompileOnlyPyClient that use `backend.compile` as we eventually intend it (i.e., return `ExecutableRef`, possibly `PyExecutable` eventually, instead of `PyLoadedExectuable`).\n\nPiperOrigin-RevId: 762440439",
        "output": "```diff\nCommit: 7d13c56570072565ce244bf6ff77c2a55f4d5e66\nDate: 2025-05-23T15:49:07Z\nURL: https://github.com/jax-ml/jax/commit/7d13c56570072565ce244bf6ff77c2a55f4d5e66\nFiles changed: 2\nAdditions: +12, Deletions: -1\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex ed0089a3dd88..67dc9ffc6001 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -553,6 +553,17 @@ class Client:\n   ) -> PjRtLayout: ...\n   def __getattr__(self, name: str) -> Any: ...\n \n+\n+class CompileOnlyPyClient(Client):\n+  def compile(\n+      self,\n+      computation: str | bytes,\n+      executable_devices: DeviceList | Sequence[Device],\n+      compile_options: CompileOptions = ...,\n+      host_callbacks: Sequence[Any] = ...,\n+  ) -> LoadedExecutable: ...\n+\n+\n class CpuCollectives: ...\n \n def make_gloo_tcp_collectives(\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex ac816e72bebe..24861bad81de 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 344\n+_version = 345\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\n\n```",
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] Slightly revamped how checkify is exposed in Pallas\n\n* We now re-export a restricted version of `debug_check` under `pl`.\n  Unlike the original, the `pl` version only allows a static message, i.e.\n  string interpolation is not supported.\n* Only debug checks are supported, which means that by default no checking\n  is done -- `debug_check` is lowered to a noop.\n* The context manager enabling debug checks is called `enable_debug_checks`.\n  I would very much like to drop the `enable_` prefix, but without it the\n  context manager reads too similar to `debug_check`.\n\nPiperOrigin-RevId: 762433258",
        "output": "```diff\nCommit: e989e23d6bb5895215864371e25724910a05fb0b\nDate: 2025-05-23T15:28:14Z\nURL: https://github.com/jax-ml/jax/commit/e989e23d6bb5895215864371e25724910a05fb0b\nFiles changed: 10\nAdditions: +72, Deletions: -61\ndiff --git a/docs/pallas/CHANGELOG.md b/docs/pallas/CHANGELOG.md\nindex 476cc54673a1..2d8a83c897f1 100644\n--- a/docs/pallas/CHANGELOG.md\n+++ b/docs/pallas/CHANGELOG.md\n@@ -26,6 +26,11 @@ Remember to align the itemized text with the first line of an item within a list\n     `block_shape` for each entry that needs unblocked indexing.\n   * {func}`jax.experimental.pallas.pallas_call` now requires `compiler_params`\n     to be a backend-specific dataclass instead of a param to value mapping.\n+  * {func}`jax.experimental.pallas.debug_check` is now supported both on\n+    TPU and Mosaic GPU. Previously, this functionality was only supported\n+    on TPU and required using the APIs from {mod}`jax.experimental.checkify`.\n+    Note that debug checks are not executed unless\n+    {data}`jax.experimental.pallas.enable_debug_checks` is set.\n \n ## Released with jax 0.5.0\n \ndiff --git a/jax/_src/pallas/core.py b/jax/_src/pallas/core.py\nindex 13c634eb395f..7950f90bc377 100644\n--- a/jax/_src/pallas/core.py\n+++ b/jax/_src/pallas/core.py\n@@ -44,22 +44,6 @@\n from jax._src.state.types import TransformedRef\n import jax.numpy as jnp\n \n-# TODO(slebedev): Rename to --jax_pallas_debug_assertions.\n-_ENABLE_RUNTIME_ASSERT = config.bool_state(\n-    \"jax_pallas_enable_runtime_assert\",\n-    default=False,\n-    help=(\n-        \"If set, enables runtime assertions in the kernel via checkify.check.\"\n-        \" Otherwise, runtime asserts will be ignored unless functionalized\"\n-        \" using checkify.checkify.\"\n-    ),\n-)\n-\n-\n-def runtime_assert_enabled() -> bool:\n-  \"\"\"Returns whether runtime asserts are enabled.\"\"\"\n-  return _ENABLE_RUNTIME_ASSERT.value\n-\n \n class DynamicGridDim:\n   def __repr__(self):\ndiff --git a/jax/_src/pallas/helpers.py b/jax/_src/pallas/helpers.py\nindex 6b274c0b6cce..5c77d0a04f09 100644\n--- a/jax/_src/pallas/helpers.py\n+++ b/jax/_src/pallas/helpers.py\n@@ -14,8 +14,10 @@\n \"\"\"Pallas helper functions.\"\"\"\n \n import jax\n-from jax._src.pallas import pallas_call\n+from jax._src import checkify\n+from jax._src import config\n from jax._src.pallas import core as pl_core\n+from jax._src.pallas import pallas_call\n \n \n @jax.named_call\n@@ -65,3 +67,29 @@ def _wrapped(f):\n     else:\n       jax.lax.cond(condition, f, lambda: None)\n   return _wrapped\n+\n+\n+_ENABLE_DEBUG_CHECKS = config.bool_state(\n+    \"jax_pallas_enable_debug_checks\",\n+    default=False,\n+    help=(\n+        \"If set, ``pl.debug_check`` calls are checked at runtime. Otherwise,\"\n+        \" they are a noop.\"\n+    ),\n+)\n+\n+\n+enable_debug_checks = _ENABLE_DEBUG_CHECKS\n+\n+\n+def debug_checks_enabled() -> bool:\n+  \"\"\"Returns runtime checks are enabled.\"\"\"\n+  return _ENABLE_DEBUG_CHECKS.value\n+\n+\n+def debug_check(condition, message):\n+  \"\"\"Check the condition if\n+  :func:`~jax.experimental.pallas.enable_debug_checks` is set, otherwise\n+  do nothing.\n+  \"\"\"\n+  return checkify.debug_check(condition, message)\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex d1222c16ac96..4e8827401e0c 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -61,6 +61,7 @@\n from jax._src.pallas import pallas_call\n from jax._src.pallas import primitives\n from jax._src.pallas import utils as pallas_utils\n+from jax._src.pallas import helpers as pallas_helpers\n from jax._src.pallas.mosaic import core as tpu_core\n from jax._src.pallas.mosaic import error_handling\n from jax._src.pallas.mosaic import primitives as tpu_primitives\n@@ -3766,17 +3767,18 @@ def _join_key_lowering_rule(ctx: LoweringRuleContext, *scalars, impl):\n \n \n @register_lowering_rule(checkify.check_p)\n-def _checkify_lowering_rule(\n-    ctx: LoweringRuleContext, *err_args, err_tree, debug):\n-  if not pallas_core.runtime_assert_enabled():\n-    if debug:\n-      return []\n-    else:\n-      raise LoweringException(\n-          \"Non-debug check must be functionalized. Enable runtime asserts via\"\n-          \" ``pl.enable_runtime_assert`` or --jax_pallas_enable_runtime_assert\"\n-          \" or, alternatively, functionalize with ``checkify.check``.\"\n-      )\n+def _check_lowering_rule(\n+    ctx: LoweringRuleContext, *err_args, err_tree, debug\n+):\n+  del ctx  # Unused.\n+\n+  if not debug:\n+    raise NotImplementedError(\n+        \"Non-debug checks are not supported by the Mosaic backend.\"\n+        \" Functionalize them via `jax.experimental.checkify`.\"\n+    )\n+  if not pallas_helpers.debug_checks_enabled():\n+    return []\n \n   if cf is None:\n     # TODO(slebedev): Remove once the minimal jaxlib version is 0.6.1.\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 00716bb1c675..9e396167f610 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -53,6 +53,7 @@\n from jax._src.pallas import pallas_call\n from jax._src.pallas import primitives\n from jax._src.pallas import utils as pallas_utils\n+from jax._src.pallas import helpers as pallas_helpers\n from jax._src.pallas.mosaic_gpu import core as gpu_core\n from jax._src.state import discharge\n from jax._src.state import indexing\n@@ -3097,18 +3098,16 @@ def _semaphore_wait_lowering_rule(ctx: LoweringRuleContext, *args, args_tree):\n \n \n @register_lowering_rule(checkify.check_p, mgpu.LoweringSemantics.Lane)\n-def _checkify_lowering_rule(\n-    ctx: LoweringRuleContext, *err_args, err_tree, debug\n-):\n-  if not pallas_core.runtime_assert_enabled():\n-    if debug:\n-      return []\n-    else:\n-      raise LoweringError(\n-          \"Non-debug check must be functionalized. Enable runtime asserts via\"\n-          \" ``pl.enable_runtime_assert`` or --jax_pallas_enable_runtime_assert\"\n-          \" or, alternatively, functionalize with ``checkify.check``.\"\n-      )\n+def _check_lowering_rule(ctx: LoweringRuleContext, *err_args, err_tree, debug):\n+  del ctx  # Unused.\n+\n+  if not debug:\n+    raise NotImplementedError(\n+        \"Non-debug checks are not supported by the Mosaic GPU backend.\"\n+        \" Functionalize them via `jax.experimental.checkify`.\"\n+    )\n+  if not pallas_helpers.debug_checks_enabled():\n+    return []\n \n   if cf_dialect is None:\n     # TODO(slebedev): Remove once the minimal jaxlib version is 0.6.1.\ndiff --git a/jax/experimental/pallas/__init__.py b/jax/experimental/pallas/__init__.py\nindex 406d6e965322..caf77a3c4fce 100644\n--- a/jax/experimental/pallas/__init__.py\n+++ b/jax/experimental/pallas/__init__.py\n@@ -18,7 +18,6 @@\n https://docs.jax.dev/en/latest/pallas.html.\n \"\"\"\n \n-from jax._src.pallas.core import _ENABLE_RUNTIME_ASSERT as enable_runtime_assert  # noqa: F401\n from jax._src.pallas.core import BlockDim as BlockDim\n from jax._src.pallas.core import Blocked as Blocked\n from jax._src.pallas.core import BlockSpec as BlockSpec\n@@ -33,7 +32,6 @@\n from jax._src.pallas.core import MemoryRef as MemoryRef\n from jax._src.pallas.core import MemorySpace as MemorySpace\n from jax._src.pallas.core import no_block_spec as no_block_spec\n-from jax._src.pallas.core import runtime_assert_enabled as runtime_assert_enabled\n from jax._src.pallas.core import semaphore as semaphore\n from jax._src.pallas.core import Squeezed as Squeezed\n from jax._src.pallas.core import squeezed as squeezed\n@@ -41,6 +39,9 @@\n from jax._src.pallas.helpers import empty as empty\n from jax._src.pallas.helpers import empty_like as empty_like\n from jax._src.pallas.helpers import when as when\n+from jax._src.pallas.helpers import debug_check as debug_check\n+from jax._src.pallas.helpers import debug_checks_enabled as debug_checks_enabled\n+from jax._src.pallas.helpers import enable_debug_checks as enable_debug_checks\n from jax._src.pallas.pallas_call import pallas_call as pallas_call\n from jax._src.pallas.pallas_call import pallas_call_p as pallas_call_p\n from jax._src.pallas.primitives import atomic_add as atomic_add\ndiff --git a/jax/experimental/pallas/g3doc/debugging.md b/jax/experimental/pallas/g3doc/debugging.md\nindex 6dfa95eb16fa..791705d00d30 100644\n--- a/jax/experimental/pallas/g3doc/debugging.md\n+++ b/jax/experimental/pallas/g3doc/debugging.md\n@@ -3,7 +3,7 @@\n <!--internal:0-->\n \n <!--*\n-freshness: { owner: 'justinfu' reviewed: '2024-11-19' }\n+freshness: { owner: 'slebedev' reviewed: '2025-05-22' }\n *-->\n \n [TOC]\n@@ -45,16 +45,14 @@ as a Python error after the kernel has successfully executed.\n \n #### Hard assertion\n \n-Hard assertions can be inserted with `checkify.check`\n-and running your program with the `--jax_pallas_enable_runtime_assert` flag.\n+Hard assertions can be inserted with `pl.debug_check`\n+and running your program with the `--jax_pallas_enable_debug_checks` flag.\n \n Your code will look like the following:\n \n ```python\n-from jax.experimental import checkify\n-\n def kernel(...):\n-  checkify.check(x > y, \"Check x > y failed\")  # Will halt if x <= y\n+  pl.debug_check(x > y, \"Check x > y failed\")  # Will halt if x <= y\n ```\n \n This will print a relatively lengthy dump which resembles the following:\n@@ -76,11 +74,10 @@ Functionalized asserts can be performed by checkify-ing the `pl.pallas_call` op\n from jax.experimental import checkify\n \n def kernel(...):\n-  checkify.check(x > y, \"Check x > y failed\")  # Will throw an error if x <= y\n+  pl.debug_check(x > y, \"Check x > y failed\")  # Will throw an error if x <= y\n \n kernel = pl.pallas_call(...)\n-checkified_kernel = checkify.checkify(kernel,\n-  errors=checkify.all_checks)\n+checkified_kernel = checkify.checkify(kernel, errors=checkify.all_checks)\n error, result = checkified_kernel(x)\n error.throw()\n ```\n@@ -203,5 +200,3 @@ In most cases the error message should hint at what is wrong.\n For specific errors:\n \n * `Mixed dtype operands in cmp` when using `jnp.mod`: Use lax.rem instead of jnp.mod\n-\n-\ndiff --git a/jax/experimental/pallas/tpu.py b/jax/experimental/pallas/tpu.py\nindex c8e2ba131a9b..401b2fe66c45 100644\n--- a/jax/experimental/pallas/tpu.py\n+++ b/jax/experimental/pallas/tpu.py\n@@ -51,8 +51,6 @@\n # Those primitives got moved to Pallas core. Keeping the updated imports\n # here for backward compatibility.\n from jax._src.pallas.core import semaphore as semaphore\n-from jax._src.pallas.core import runtime_assert_enabled as runtime_assert_enabled\n-from jax._src.pallas.core import _ENABLE_RUNTIME_ASSERT as enable_runtime_assert  # noqa: F401\n from jax._src.pallas.primitives import DeviceIdType as DeviceIdType\n from jax._src.pallas.primitives import semaphore_read as semaphore_read\n from jax._src.pallas.primitives import semaphore_signal as semaphore_signal\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 4ef2fa8096ee..b3c2f11ee43f 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1031,14 +1031,14 @@ def kernel(x_ref, o_ref, scratch_ref, barrier_ref):\n   def test_check(self):\n     self.skip_if_wg_semantics()\n \n-    self.enter_context(pallas_core._ENABLE_RUNTIME_ASSERT(True))\n+    self.enter_context(pl.enable_debug_checks(True))\n \n     @functools.partial(\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct([256], jnp.int32),\n     )\n     def kernel(x_ref, o_ref):\n-      checkify.check(_sum_same_dtype(x_ref[...]) > 0, \"x.sum() is negative\")\n+      pl.debug_check(_sum_same_dtype(x_ref[...]) > 0, \"x.sum() is negative\")\n       o_ref[...] = x_ref[...]\n \n     x = jnp.arange(256, dtype=jnp.int32)\ndiff --git a/tests/pallas/pallas_test.py b/tests/pallas/pallas_test.py\nindex 1114153b16c2..03399e12b609 100644\n--- a/tests/pallas/pallas_test.py\n+++ b/tests/pallas/pallas_test.py\n@@ -2276,7 +2276,7 @@ def kernel(x_ref, y_ref):\n       checkify.check(False, \"second check failed\")\n     input_ = jnp.arange(4, dtype=jnp.int32)\n     out_shape = jax.ShapeDtypeStruct(input_.shape, input_.dtype)\n-    with pltpu.enable_runtime_assert(True):\n+    with pl.enable_debug_checks(True):\n       pallas_call = pl.pallas_call(kernel, out_shape=out_shape)\n       pallas_call(input_)  # This should log \"second check failed\"\n \n@@ -2286,11 +2286,10 @@ def test_runtime_assert_is_noop_when_not_enabled(self):\n       self.skipTest(\"Runtime check only implemented on TPU.\")\n     def kernel(x_ref, y_ref):\n       y_ref[...] = x_ref[...]\n-      checkify.check(False, \"failed check\",\n-                     debug=True)  # This check always fails.\n+      pl.debug_check(False, \"failed check\")  # This check always fails.\n     input_ = jnp.arange(4, dtype=jnp.int32)\n     out_shape = jax.ShapeDtypeStruct(input_.shape, input_.dtype)\n-    with pltpu.enable_runtime_assert(False):\n+    with pl.enable_debug_checks(False):\n       pallas_call = pl.pallas_call(kernel, out_shape=out_shape)\n       result = pallas_call(input_)\n     np.testing.assert_allclose(result, input_)\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "pytest: use importlib mode by default.\n\nThis is an attempt to re-land https://github.com/jax-ml/jax/pull/28650, fixing build failures.\n\n**Motivation**\n\nhttps://github.com/jax-ml/jax/pull/28650 was motivated by recent changes in setuptools, which caused test failures with editable installs, but it also exposes a potentially larger issue with our approach to testing with pytest. As far as I understand it, the default pytest behavior is to prepend the working directory to `sys.path`, meaning that `jax` is imported from the source directory, rather than the installed version. Switching to the `importlib` import mode means that we correctly test against the installed version of `jax`, which seems like what we typically want to do.\n\nThe catch is that then we need to explicitly package any test utilities into the distribution. We don't currently package test-specific utilities like `internal_test_util` with JAX, but these utilities were still available to tests since they live within the `jax` source tree. This breaks when using `importlib` import mode, and a non-editable install of JAX.\n\n**Solutions**\n\nThe approach that I've taken here is to explicitly package everything needed by the tests into the `jax` distribution. This means that we can correctly test against the _installed_ version of JAX when using pytest.\n\nThis solution isn't ideal because it means that we're distributing `jax` submodules that aren't actually required except when running the test suite, but this seems like a small price to pay to me.\n\n**Alternatives**\n\nOne different approach that we could take would be to only support using pytest with _editable_ installs of JAX. This would work because the required files would still be discoverable in an editable install because they live within the source tree. In fact, before this change, most of our CI jobs actually did install an editable distribution (which is why the failures in https://github.com/jax-ml/jax/pull/28650 weren't caught in pre-submit!). The problem with this approach is that we're not actually testing JAX as it is used when installed from a distribution, and it wouldn't catch things like missing submodules. I think it's much better to test against the installed distribution!\n\nA more extreme approach would be to switch JAX to a `src/jax` and `src/jaxlib` layout (i.e. moving `jax` and `jaxlib` out of the root directory) as recommended by the Python packaging docs. Unfortunately this would be complicated with the way JAX is distributed internally at Google, so I think that's probably a non-starter.\n\nPiperOrigin-RevId: 762419160",
        "output": "```diff\nCommit: 9cbf4934936043d472bb7b81cf02c49af77a97b1\nDate: 2025-05-23T14:45:29Z\nURL: https://github.com/jax-ml/jax/commit/9cbf4934936043d472bb7b81cf02c49af77a97b1\nFiles changed: 7\nAdditions: +51, Deletions: -4\ndiff --git a/.github/workflows/ci-build.yaml b/.github/workflows/ci-build.yaml\nindex 09f169548796..0769c698d5fe 100644\n--- a/.github/workflows/ci-build.yaml\n+++ b/.github/workflows/ci-build.yaml\n@@ -88,7 +88,6 @@ jobs:\n         JAX_SKIP_SLOW_TESTS: true\n         PY_COLORS: 1\n       run: |\n-        uv pip install --system -e .\n         echo \"JAX_NUM_GENERATED_CASES=$JAX_NUM_GENERATED_CASES\"\n         echo \"JAX_ENABLE_X64=$JAX_ENABLE_X64\"\n         echo \"JAX_ENABLE_CUSTOM_PRNG=$JAX_ENABLE_CUSTOM_PRNG\"\n@@ -185,7 +184,6 @@ jobs:\n         JAX_SKIP_SLOW_TESTS: true\n         PY_COLORS: 1\n       run: |\n-        uv pip install --system -e .\n         echo \"JAX_NUM_GENERATED_CASES=$JAX_NUM_GENERATED_CASES\"\n         echo \"JAX_ENABLE_X64=$JAX_ENABLE_X64\"\n         echo \"JAX_ENABLE_CHECKS=$JAX_ENABLE_CHECKS\"\ndiff --git a/BUILD.bazel b/BUILD.bazel\nindex 887f28d4583e..44885124797f 100644\n--- a/BUILD.bazel\n+++ b/BUILD.bazel\n@@ -42,12 +42,19 @@ wheel_sources(\n         \"//jax:pallas_triton\",\n         \"//jax:source_mapper\",\n         \"//jax:sparse_test_util\",\n+        \"//jax:test_multiprocess\",\n         \"//jax:test_util\",\n+        \"//jax:internal_export_back_compat_test_util\",\n+        \"//jax:internal_export_back_compat_test_data\",\n+        \"//jax:internal_test_harnesses\",\n+        \"//jax:internal_test_util\",\n         \"//jax/_src/lib\",\n         \"//jax/_src/pallas/fuser\",\n         \"//jax/_src/pallas/mosaic_gpu\",\n         \"//jax/experimental/array_serialization:serialization\",\n         \"//jax/experimental/jax2tf\",\n+        \"//jax/experimental/mosaic/gpu/examples:flash_attention\",\n+        \"//jax/experimental/mosaic/gpu/examples:matmul\",\n         \"//jax/extend\",\n         \"//jax/extend:ifrt_programs\",\n         \"//jax/extend/mlir\",\ndiff --git a/jax/_src/internal_test_util/export_back_compat_test_data/__init__.py b/jax/_src/internal_test_util/export_back_compat_test_data/__init__.py\nnew file mode 100644\nindex 000000000000..3da0dd1fa3ca\n--- /dev/null\n+++ b/jax/_src/internal_test_util/export_back_compat_test_data/__init__.py\n@@ -0,0 +1,14 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\ndiff --git a/jax/_src/internal_test_util/export_back_compat_test_data/pallas/__init__.py b/jax/_src/internal_test_util/export_back_compat_test_data/pallas/__init__.py\nnew file mode 100644\nindex 000000000000..3da0dd1fa3ca\n--- /dev/null\n+++ b/jax/_src/internal_test_util/export_back_compat_test_data/pallas/__init__.py\n@@ -0,0 +1,14 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\ndiff --git a/jax/experimental/mosaic/gpu/examples/__init__.py b/jax/experimental/mosaic/gpu/examples/__init__.py\nnew file mode 100644\nindex 000000000000..3da0dd1fa3ca\n--- /dev/null\n+++ b/jax/experimental/mosaic/gpu/examples/__init__.py\n@@ -0,0 +1,14 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 83b85b0271f5..d48351197b54 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -88,7 +88,7 @@ doctest_optionflags = [\n     \"NUMBER\",\n     \"NORMALIZE_WHITESPACE\"\n ]\n-addopts = \"--doctest-glob='*.rst' --ignore='examples/ffi'\"\n+addopts = \"--doctest-glob='*.rst' --ignore='examples/ffi' --import-mode=importlib\"\n \n [tool.ruff]\n preview = true\ndiff --git a/setup.py b/setup.py\nindex ef78b8f6e7ff..2b50b041008d 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -57,7 +57,7 @@ def load_version_module(pkg_path):\n     long_description_content_type='text/markdown',\n     author='JAX team',\n     author_email='jax-dev@google.com',\n-    packages=find_packages(exclude=[\"*examples*\", \"*internal_test_util*\"]),\n+    packages=find_packages(exclude=[\"examples\"]),\n     package_data={'jax': ['py.typed', \"*.pyi\", \"**/*.pyi\"]},\n     python_requires='>=3.10',\n     install_requires=[\n\n```",
        "from_id": [
            "dfm",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move _src/custom_transpose.py into its own BUILD rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nPiperOrigin-RevId: 762414305",
        "output": "```diff\nCommit: 0a970e43eceeb5aa076d1b981c49dfcf796b041c\nDate: 2025-05-23T14:32:08Z\nURL: https://github.com/jax-ml/jax/commit/0a970e43eceeb5aa076d1b981c49dfcf796b041c\nFiles changed: 2\nAdditions: +21, Deletions: -1\ndiff --git a/jax/BUILD b/jax/BUILD\nindex e820bd06fe89..e7f1fad3121d 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -307,7 +307,6 @@ py_library_providing_imports_info(\n         \"_src/custom_derivatives.py\",\n         \"_src/custom_partitioning.py\",\n         \"_src/custom_partitioning_sharding_rule.py\",\n-        \"_src/custom_transpose.py\",\n         \"_src/debugging.py\",\n         \"_src/dispatch.py\",\n         \"_src/dlpack.py\",\n@@ -391,6 +390,7 @@ py_library_providing_imports_info(\n         \":config\",\n         \":core\",\n         \":custom_api_util\",\n+        \":custom_transpose\",\n         \":deprecations\",\n         \":dtypes\",\n         \":effects\",\n@@ -595,6 +595,25 @@ pytype_strict_library(\n     srcs = [\"_src/custom_api_util.py\"],\n )\n \n+pytype_strict_library(\n+    name = \"custom_transpose\",\n+    srcs = [\"_src/custom_transpose.py\"],\n+    deps = [\n+        \":ad\",\n+        \":ad_util\",\n+        \":api_util\",\n+        \":core\",\n+        \":custom_api_util\",\n+        \":mlir\",\n+        \":partial_eval\",\n+        \":source_info_util\",\n+        \":traceback_util\",\n+        \":tree_util\",\n+        \":util\",\n+        \":xla\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"deprecations\",\n     srcs = [\"_src/deprecations.py\"],\ndiff --git a/tests/BUILD b/tests/BUILD\nindex 2418c8224869..6c9f3f74b56a 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -61,6 +61,7 @@ jax_multiplatform_test(\n     srcs = [\"debug_info_test.py\"],\n     enable_configs = [\"tpu_v3_x4\"],\n     deps = [\n+        \"//jax:custom_transpose\",\n         \"//jax:experimental\",\n         \"//jax:pallas\",\n         \"//jax:pallas_gpu\",\n\n```",
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Pass the right number of immediate values in the wgmma inline asm.\n\nBefore this change, in the int32 case, we pass two extra immediate args compared to the number of parameters in the ASM string. Running tests with `-UNDEBUG` detects the error.\n\nI think this likely broke with the special-casing of `int32` in cl/761489756.\n\nI've added an assert that should prevent mismatches in the future.\n\nPiperOrigin-RevId: 762394530",
        "output": "```diff\nCommit: c1c0c0fa27f52de19ddd5112580c8da2f711e1e8\nDate: 2025-05-23T13:21:44Z\nURL: https://github.com/jax-ml/jax/commit/c1c0c0fa27f52de19ddd5112580c8da2f711e1e8\nFiles changed: 1\nAdditions: +8, Deletions: -1\ndiff --git a/jax/experimental/mosaic/gpu/wgmma.py b/jax/experimental/mosaic/gpu/wgmma.py\nindex abbd517fb37d..2fe826e173e5 100644\n--- a/jax/experimental/mosaic/gpu/wgmma.py\n+++ b/jax/experimental/mosaic/gpu/wgmma.py\n@@ -226,11 +226,18 @@ def lc(x):\n     return llvm.ConstantOp(i32, ir.IntegerAttr.get(i32, x)).result\n \n   use_out = scale_a = scale_b = lc(1)\n-  imms = [use_out, scale_a, scale_b]\n+  if out_ty == i32:\n+    imms = [use_out]\n+  else:\n+    imms = [use_out, scale_a, scale_b]\n+\n   if supports_transpose and a_transpose is not None:\n     imms += [lc(int(a_transpose)), lc(int(b_transpose))]\n   elif supports_transpose:\n     imms += [lc(int(b_transpose))]\n+\n+  assert len(imms) == num_imm_regs + 1  # +1 for the use_out_reg in setp.ne.b32\n+\n   if acc.ndim != 10 or acc.shape[0] != 1 or math.prod(acc.shape[2:]) != 2:\n     raise ValueError(acc.shape)\n   acc_struct_type = ir.Type.parse(\n\n```",
        "from_id": [
            "dimitar-asenov",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add checks for argument shapes and types\n\nApparently we never checked it and it's been quite easy to\nget this wrong.\n\nPiperOrigin-RevId: 762394139",
        "output": "```diff\nCommit: 6cc627a4dc194d91e2ea3920ddd51e310b88920e\nDate: 2025-05-23T13:19:53Z\nURL: https://github.com/jax-ml/jax/commit/6cc627a4dc194d91e2ea3920ddd51e310b88920e\nFiles changed: 2\nAdditions: +16, Deletions: -2\ndiff --git a/jax/experimental/mosaic/gpu/core.py b/jax/experimental/mosaic/gpu/core.py\nindex 5e1ed6b88412..9464bb587c71 100644\n--- a/jax/experimental/mosaic/gpu/core.py\n+++ b/jax/experimental/mosaic/gpu/core.py\n@@ -677,7 +677,7 @@ def as_gpu_kernel(\n   if launch_ctx.is_device_collective and not supports_cross_device_collectives():\n     raise RuntimeError(\"Kernel is a cross-device collective but no support is available.\")\n \n-  expected_arg_treedef = jax.tree.structure(in_shape)\n+  expected_arg_tys, expected_arg_treedef = jax.tree.flatten(in_shape)\n   def _check_args(*args):\n     arg_treedef = jax.tree.structure(args)\n     if arg_treedef != expected_arg_treedef:\n@@ -685,6 +685,20 @@ def _check_args(*args):\n           f\"Invalid argument structure: expected {expected_arg_treedef}, got\"\n           f\" {arg_treedef}, ({args=})\"\n       )\n+    for arg, expected_ty in zip(args, expected_arg_tys):\n+      if arg.shape != expected_ty.shape:\n+        raise ValueError(\n+            f\"Argument shape mismatch: expected {expected_ty.shape}, got\"\n+            f\" {arg.shape}\"\n+        )\n+      if arg.dtype != expected_ty.dtype:\n+        hint = \"\"\n+        if not arg.shape:\n+          hint = f\". Hint: cast the scalar to {expected_ty.dtype} explicitly.\"\n+        raise ValueError(\n+            f\"Argument dtype mismatch: expected {expected_ty.dtype}, got\"\n+            f\" {arg.dtype}{hint}\"\n+        )\n \n   def bind(*args) -> Any:\n     return mosaic_gpu_p.bind(*args, module=module, out_types=out_shape)\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 6ea27eb42878..42a3f0fc83c1 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -460,7 +460,7 @@ def test_scalar_argument(self, dtype):\n         \" values read from the 32-bit input buffer to sometimes\"\n         \" (nondeterministically) contain garbage.\")\n \n-    scalar = 42\n+    scalar = dtype(42)\n     expected = np.full((128, 128), scalar, dtype=dtype)\n \n     def kernel(ctx, inp, out, _):\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add support for async copies to peer devices\n\nPiperOrigin-RevId: 762370447",
        "output": "```diff\nCommit: 5a448b867cf9c91d99472b51f78c66749ce98e62\nDate: 2025-05-23T11:47:34Z\nURL: https://github.com/jax-ml/jax/commit/5a448b867cf9c91d99472b51f78c66749ce98e62\nFiles changed: 4\nAdditions: +177, Deletions: -1\ndiff --git a/jax/experimental/mosaic/gpu/launch_context.py b/jax/experimental/mosaic/gpu/launch_context.py\nindex aaae007a67f0..e4f0c4efa22c 100644\n--- a/jax/experimental/mosaic/gpu/launch_context.py\n+++ b/jax/experimental/mosaic/gpu/launch_context.py\n@@ -400,6 +400,7 @@ def _get_tma_desc(\n       self,\n       gmem_ref,\n       gmem_transform: tuple[MemRefTransform, ...],\n+      gmem_peer_id: int | ir.Value | None,\n       transformed_slice_shape: tuple[int, ...],\n       swizzle: int | None,\n       reduction_op: Literal[\n@@ -408,6 +409,7 @@ def _get_tma_desc(\n   ):\n     tma_desc_key = (gmem_ref, transformed_slice_shape, swizzle, gmem_transform)\n     if (tma_desc := self.tma_descriptors.get(tma_desc_key, None)) is None:\n+      i32 = ir.IntegerType.get_signless(32)\n       i64 = ir.IntegerType.get_signless(64)\n       ptr_ty = ir.Type.parse(\"!llvm.ptr\")\n       def init_tma_desc(host_ptr):\n@@ -432,6 +434,25 @@ def init_tma_desc(host_ptr):\n         base_ptr = llvm.getelementptr(\n             ptr_ty, alloc_ptr, [as_i64(offset)], [llvm_dyn], ref_ty.element_type, llvm.GEPNoWrapFlags.none,\n         )\n+        if gmem_peer_id is not None:\n+          if not isinstance(gmem_peer_id, ir.Value):\n+            peer_id = c(gmem_peer_id, i32)\n+          else:\n+            try:\n+              # We try to reproduce the gmem_peer_id computation on the host.\n+              peer_id = _recompute_peer_id(gmem_peer_id)\n+            except ReplicationError as e:\n+              raise ValueError(\n+                  \"Failed to recompute the async_copy peer id on the host\"\n+              ) from e\n+          self._ensure_nvshmem_decls()\n+          base_ptr = llvm.call(\n+              base_ptr.type,\n+              [base_ptr, peer_id],\n+              [],\n+              [],\n+              callee=\"nvshmem_ptr\",\n+          )\n         rank = ref_ty.rank\n         assert rank * 2 == len(sizes_and_strides)\n         swizzle_arg = (\n@@ -507,6 +528,7 @@ def async_copy(\n       dst_ref,\n       gmem_slice: Any = (),\n       gmem_transform: MemRefTransform | tuple[MemRefTransform, ...] = (),\n+      gmem_peer_id: int | ir.Value | None = None,\n       barrier: utils.BarrierRef | None = None,\n       swizzle: int | None = None,\n       arrive: bool | None = None,\n@@ -750,7 +772,8 @@ def partition_dim(dim: int, idx: ir.Value, num_chunks: int):\n       multicast_mask = None\n \n     tma_desc = self._get_tma_desc(\n-        gmem_ref, gmem_transform, tuple(slice_shape), swizzle, reduction_op,\n+        gmem_ref, gmem_transform, gmem_peer_id,\n+        tuple(slice_shape), swizzle, reduction_op,\n     )\n \n     # We constuct TMA descriptors in column-major order.\n@@ -893,3 +916,33 @@ def device_id(self) -> ir.Value:\n     self._ensure_nvshmem_decls()\n     i32 = ir.IntegerType.get_signless(32)\n     return llvm.call(i32, [], [], [], callee=\"nvshmem_my_pe\")\n+\n+\n+class ReplicationError(Exception):\n+  pass\n+\n+def _recompute_peer_id(peer_id: ir.Value, fuel=8) -> ir.Value:\n+  if fuel == 0:\n+    raise ReplicationError(\n+        \"gmem_peer_id computation is too complicated to recompute on the host\"\n+    )\n+  if isinstance(peer_id, ir.BlockArgument):\n+    raise ReplicationError(\"Can't recompute a value that's a block argument\")\n+  op = peer_id.owner.opview\n+  # We accept all arith ops\n+  if op.OPERATION_NAME.startswith(\"arith.\"):\n+    new_operands = [_recompute_peer_id(x, fuel - 1) for x in op.operands]\n+    result_types = [r.type for r in op.results]\n+    new_attributes = {na.name: na.attr for na in op.attributes}\n+    new_op = ir.Operation.create(\n+        op.OPERATION_NAME, result_types, new_operands, new_attributes\n+    )\n+    return new_op.results if len(new_op.results) > 1 else new_op.result\n+  # nvshmem_my_pe queries the device id of the current process and works on both\n+  # the host and the device.\n+  if isinstance(op, llvm.CallOp) and op.callee.value == \"nvshmem_my_pe\":\n+    i32 = ir.IntegerType.get_signless(32)\n+    return llvm.call(i32, [], [], [], callee=\"nvshmem_my_pe\")\n+  raise ReplicationError(\n+      f\"Unrecognized op can't be recomputed on the host: {op}\"\n+  )\ndiff --git a/jaxlib/mosaic/gpu/BUILD b/jaxlib/mosaic/gpu/BUILD\nindex 66f13bdac7f5..fc1abb9397d5 100644\n--- a/jaxlib/mosaic/gpu/BUILD\n+++ b/jaxlib/mosaic/gpu/BUILD\n@@ -122,6 +122,8 @@ cc_library(\n     # Linker may prune these symbols if they are not explicitly exported.\n     linkopts = [\n         \"-Wl,--export-dynamic-symbol='mosaic_gpu_*'\",\n+        \"-Wl,--export-dynamic-symbol='nvshmem_my_pe'\",\n+        \"-Wl,--export-dynamic-symbol='nvshmem_ptr'\",\n         \"-Wl,--export-dynamic-symbol='nvshmemx_barrier_all_on_stream'\",\n         \"-Wl,--export-dynamic-symbol='nvshmemx_cumodule_init'\",\n         \"-Wl,--export-dynamic-symbol='nvshmemx_init_status'\",\ndiff --git a/tests/mosaic/BUILD b/tests/mosaic/BUILD\nindex 75e1df335f6f..9b6a7f79d099 100644\n--- a/tests/mosaic/BUILD\n+++ b/tests/mosaic/BUILD\n@@ -63,6 +63,27 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"gpu_test_distributed\",\n+    srcs = [\"gpu_test_distributed.py\"],\n+    args = [\n+        \"--num_processes=2\",\n+        \"--gpus_per_process=1\",\n+    ],\n+    enable_backends = [],\n+    enable_configs = [\"gpu_h100x2\"],\n+    env = {\"XLA_FLAGS\": \"--xla_gpu_autotune_level=0 --xla_gpu_experimental_enable_nvshmem=true\"},\n+    tags = [\"multiaccelerator\"],\n+    deps = [\n+        \"//jax:experimental\",\n+        \"//jax:mosaic_gpu\",\n+        \"//jax:test_multiprocess\",\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n+)\n+\n jax_py_test(\n     name = \"gpu_dialect_test\",\n     srcs = [\"gpu_dialect_test.py\"],\ndiff --git a/tests/mosaic/gpu_test_distributed.py b/tests/mosaic/gpu_test_distributed.py\nnew file mode 100644\nindex 000000000000..fee2ce5b03a6\n--- /dev/null\n+++ b/tests/mosaic/gpu_test_distributed.py\n@@ -0,0 +1,100 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+from absl.testing import parameterized\n+import jax\n+from jax._src import config\n+from jax._src import test_util as jtu\n+from jax._src import test_multiprocess as jt_multiprocess\n+from jax._src.interpreters import mlir\n+from jax._src.lib.mlir import ir\n+from jax._src.lib.mlir.dialects import arith\n+from jax.experimental.mosaic.gpu import dialect as mgpu_dialect  # pylint: disable=g-importing-member\n+from jax.experimental import shard\n+from jax.experimental import multihost_utils\n+import jax.numpy as jnp\n+import numpy as np\n+try:\n+  import jax._src.lib.mosaic_gpu  # noqa: F401\n+  HAS_MOSAIC_GPU = True\n+except ImportError:\n+  HAS_MOSAIC_GPU = False\n+else:\n+  import jax.experimental.mosaic.gpu as mgpu\n+\n+\n+# ruff: noqa: F405\n+# pylint: disable=g-complex-comprehension\n+P = jax.sharding.PartitionSpec\n+\n+\n+class TestCase(parameterized.TestCase):\n+\n+  def setUp(self):\n+    if not HAS_MOSAIC_GPU:\n+      self.skipTest(\"jaxlib built without Mosaic GPU\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability >= sm90\")\n+    if not mgpu.supports_cross_device_collectives():\n+      self.skipTest(\"NVSHMEM library unavailable.\")\n+    if jax.process_count() == 1:\n+      self.skipTest(\"Test requires multiple processes.\")\n+    if jax.device_count() != jax.process_count():\n+      self.skipTest(\"Need 1 device per process\")\n+    super().setUp()\n+    self.prng = np.random.default_rng(1234)\n+    self.context = mlir.make_ir_context()\n+    if mgpu_dialect is not None:\n+      mgpu_dialect.register_dialect(self.context)\n+    self.enter_context(config.traceback_filtering(\"off\"))\n+    self.enter_context(self.context)\n+    self.enter_context(ir.Location.unknown())\n+\n+\n+class ProfilerTest(TestCase):\n+\n+  def test_remote_async_copy(self):\n+    i32 = ir.IntegerType.get_signless(32)\n+    def kernel(ctx, src, dst, scratch):\n+      tmp, barrier = scratch\n+      other_device = arith.subi(arith.constant(i32, 1), ctx.device_id())\n+      ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier)\n+      barrier.wait()\n+      ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_peer_id=other_device)\n+      ctx.await_async_copy(0)\n+    mesh = jax.make_mesh(\n+        (2,), (\"x\",), axis_types=(jax.sharding.AxisType.Explicit,)\n+    )\n+    with jax.sharding.use_mesh(mesh):\n+      x_np = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n+      x = shard.reshard(x_np, P(\"x\"))\n+      y = jax.jit(\n+          jax.shard_map(\n+              lambda x: mgpu.as_gpu_kernel(\n+                  kernel, (1, 1, 1), (128, 1, 1), x, x, (x, mgpu.TMABarrier())\n+              )(x),\n+              out_specs=P(\"x\"),\n+              check_vma=False,\n+          )\n+      )(x)\n+      y_np = multihost_utils.process_allgather(y, tiled=True)\n+      np.testing.assert_array_equal(\n+          y_np, np.concatenate(np.split(x_np, 2)[::-1], axis=0)\n+      )\n+\n+\n+if __name__ == \"__main__\":\n+  jt_multiprocess.main()\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Properly handle single-element outputs in inline assembly.\n\nThis issue was discovered when enabling the ragged dot example kernel to run using warpgroup semantics.\n\nThe new test requires `-UNDEBUG`.\n\nPiperOrigin-RevId: 762365352",
        "output": "```diff\nCommit: dc0cdf720bd3e3747d702a29cb1e63ef529eba80\nDate: 2025-05-23T11:28:14Z\nURL: https://github.com/jax-ml/jax/commit/dc0cdf720bd3e3747d702a29cb1e63ef529eba80\nFiles changed: 2\nAdditions: +37, Deletions: -11\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex f69d3f33fe7c..77584b5f0dd4 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -2591,17 +2591,28 @@ def _repack(regs_it, reg_ty):\n   all_reg_constraints = \",\".join(\n       [*(\"=\" + c for c in reg_constraints), *reg_constraints]\n   )\n-  struct_ty = ir.Type.parse(\n-      f\"!llvm.struct<({','.join(map(str, reg_dtypes))})>\"\n-  )\n-  result_struct = llvm.inline_asm(\n-      struct_ty, regs, ptx, all_reg_constraints,\n-      asm_dialect=0, has_side_effects=True,\n-  )\n-  regs = [\n-      llvm.extractvalue(dtype, result_struct, [i])\n-      for i, dtype in enumerate(reg_dtypes)\n-  ]\n+\n+  if len(reg_dtypes) == 1:\n+    # The InlineAsm::verify() function doesn't allow a struct output when there\n+    # is only one element (even though that seems to work for the case below).\n+    result_elem = llvm.inline_asm(\n+        reg_dtypes[0], regs, ptx, all_reg_constraints,\n+        asm_dialect=0, has_side_effects=True,\n+    )\n+    regs = [result_elem]\n+  else:\n+    struct_ty = ir.Type.parse(\n+        f\"!llvm.struct<({','.join(map(str, reg_dtypes))})>\"\n+    )\n+    result_struct = llvm.inline_asm(\n+        struct_ty, regs, ptx, all_reg_constraints,\n+        asm_dialect=0, has_side_effects=True,\n+    )\n+    regs = [\n+        llvm.extractvalue(dtype, result_struct, [i])\n+        for i, dtype in enumerate(reg_dtypes)\n+    ]\n+\n   i32 = ir.IntegerType.get_signless(32)\n   results = []\n   regs_it = iter(regs)\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex e7fc9723347b..6ea27eb42878 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -2451,6 +2451,21 @@ def kernel(ctx, inp, out, smem):\n     f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n     np.testing.assert_array_equal(f(x), x * 3)\n \n+  def test_optimization_barrier_with_single_value(self):\n+    shape = (64, 64)\n+    value = 5.0\n+    dtype = jnp.float32\n+    def kernel(ctx, out, smem):\n+      del ctx, smem\n+      mlir_type = utils.dtype_to_ir_type(dtype)\n+      arr = mgpu.FragmentedArray.splat(c(value, mlir_type), shape)\n+      arr = mgpu.optimization_barrier(arr)\n+      arr.store_untiled(out)\n+\n+    out_shape = jax.ShapeDtypeStruct(shape, dtype)\n+    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())\n+    np.testing.assert_array_equal(f(), jnp.full(shape, value, dtype=dtype))\n+\n   def test_convert_bool_to_u8(self):\n     m, n = 128, 128\n     def kernel(ctx, dst, _):\n\n```",
        "from_id": [
            "dimitar-asenov",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix psum_invariant transpose rule where we were binding `pvary` with `ad_util.Zero` which lead to errors like this: `TypeError: Argument 'Zero(float32[1,1,512])' of type '<class 'jax._src.ad_util.Zero'>' is not a valid JAX type`\n\nPiperOrigin-RevId: 762264425",
        "output": "```diff\nCommit: 199d9f73cba51e940a69c3e90f666e869c13f413\nDate: 2025-05-23T05:15:21Z\nURL: https://github.com/jax-ml/jax/commit/199d9f73cba51e940a69c3e90f666e869c13f413\nFiles changed: 2\nAdditions: +33, Deletions: -5\ndiff --git a/jax/_src/lax/parallel.py b/jax/_src/lax/parallel.py\nindex bf27261a2c8e..c5f8d3988144 100644\n--- a/jax/_src/lax/parallel.py\n+++ b/jax/_src/lax/parallel.py\n@@ -2056,13 +2056,25 @@ def _pgather_collective_batcher(axis_size, frame_name, _, vals_in, dims_in, *, a\n batching.skippable_batchers[psum_invariant_p] = partial(_names_in_param, 'axes')\n \n def _psum_invariant_transpose_rule(cts, *args, axes, axis_index_groups):\n-  del args\n-  return core.pvary_p.bind(*cts, axes=axes, axis_index_groups=axis_index_groups)\n+  def f(ct, arg):\n+    assert ad.is_undefined_primal(arg)\n+    return ad.Zero(arg.aval) if type(ct) is ad.Zero else ct\n+  cts = map(f, cts, args)\n+  nonzero_out_cts, treedef = tree_util.tree_flatten(cts)\n+  nonzero_in_cts = core.pvary_p.bind(*nonzero_out_cts, axes=axes,\n+                                     axis_index_groups=axis_index_groups)\n+  return tree_util.tree_unflatten(treedef, nonzero_in_cts)\n ad.deflinear2(psum_invariant_p, _psum_invariant_transpose_rule)\n \n ########################### pvary ##################################\n \n-def _pvary_transpose_rule(cts, *_, axes, axis_index_groups):\n-  return psum_invariant_p.bind(\n-      *cts, axes=axes, axis_index_groups=axis_index_groups)\n+def _pvary_transpose_rule(cts, *args, axes, axis_index_groups):\n+  def f(ct, arg):\n+    assert ad.is_undefined_primal(arg)\n+    return ad.Zero(arg.aval) if type(ct) is ad.Zero else ct\n+  cts = map(f, cts, args)\n+  nonzero_out_cts, treedef = tree_util.tree_flatten(cts)\n+  nonzero_in_cts = psum_invariant_p.bind(*nonzero_out_cts, axes=axes,\n+                                         axis_index_groups=axis_index_groups)\n+  return tree_util.tree_unflatten(treedef, nonzero_in_cts)\n ad.deflinear2(core.pvary_p, _pvary_transpose_rule)\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 90360989f13f..df69db7c9462 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -989,6 +989,22 @@ def f(x):\n     for i in range(len(jax.devices())):\n       self.assertIn(f'instance {i} has value', output())\n \n+  def test_psum_transpose_non_zero_cts(self):\n+    mesh = jtu.create_mesh((8,), 'x')\n+    @shard_map(mesh=mesh, in_specs=P('x'), out_specs=(P('x'), P()))\n+    def f1(x_block):\n+      return x_block, jax.lax.psum(x_block, axis_name='x')\n+\n+    x1 = jnp.arange(16.)\n+    f1(x1)  # doesn't crash\n+\n+    def f2(x_block):\n+      y, _ = f1(x_block)\n+      return y.sum()\n+\n+    jax.jit(jax.grad(f2))(x1)  # doesn't crash\n+    jax.grad(f2)(x1)  # doesn't crash\n+\n   @jtu.run_on_devices('cpu', 'gpu', 'tpu')\n   @jtu.thread_unsafe_test()\n   def test_debug_print_jit_partial_auto(self):\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Implement `_to_sdy_sharding` for GSPMDSharding.\n\nPiperOrigin-RevId: 762220186",
        "output": "```diff\nCommit: 82d76099b448a1ea9da1ae475d426abed2d57f90\nDate: 2025-05-23T02:22:41Z\nURL: https://github.com/jax-ml/jax/commit/82d76099b448a1ea9da1ae475d426abed2d57f90\nFiles changed: 5\nAdditions: +70, Deletions: -5\ndiff --git a/jax/_src/sharding_impls.py b/jax/_src/sharding_impls.py\nindex 982af82c5c4d..4703e6403079 100644\n--- a/jax/_src/sharding_impls.py\n+++ b/jax/_src/sharding_impls.py\n@@ -638,8 +638,25 @@ def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n     return self._hlo_sharding\n \n   def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n-    raise NotImplementedError(\n-        \"GSPMDSharding can't be converted to SdyArray.\")\n+    if self._hlo_sharding.tuple_elements():\n+      raise TypeError(\n+          f'Cannot convert GSPMDSharding {self._hlo_sharding} into SdyArray.')\n+    elif self._hlo_sharding.is_replicated():\n+      empty_mesh = mesh_lib.AbstractMesh((), ())\n+      return NamedSharding(empty_mesh, PartitionSpec())._to_sdy_sharding(\n+          num_dimensions)\n+    elif self._hlo_sharding.is_tiled():\n+      if not self._hlo_sharding.is_tile_assignment_iota():\n+        raise TypeError(\n+            f'Cannot convert GSPMDSharding {self._hlo_sharding} into SdyArray.')\n+      axis_sizes = tuple(self._hlo_sharding.get_axis_sizes())\n+      axis_names = tuple(f'_axis_{i}' for i in range(len(axis_sizes)))\n+      mesh = mesh_lib.AbstractMesh(axis_sizes, axis_names)\n+      return _gspmd_to_named_sharding_via_mesh(self, mesh)._to_sdy_sharding(\n+          num_dimensions)\n+    else:\n+      raise TypeError(\n+          f'Cannot convert GSPMDSharding {self._hlo_sharding} into SdyArray.')\n \n   @functools.cached_property\n   def is_fully_replicated(self) -> bool:\n@@ -1241,11 +1258,13 @@ def create_mesh_pspec_sharding(\n \n \n def _gspmd_to_named_sharding_via_mesh(\n-    out_s: GSPMDSharding, mesh: mesh_lib.Mesh) -> NamedSharding:\n+    out_s: GSPMDSharding, mesh: mesh_lib.Mesh | mesh_lib.AbstractMesh\n+) -> NamedSharding:\n   spec = parse_flatten_op_sharding(out_s._hlo_sharding, mesh)[0]\n   return create_mesh_pspec_sharding(\n       mesh, spec, memory_kind=out_s.memory_kind)\n \n+\n def flatten_spec(spec):\n   out = []\n   for s in spec:\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 898a4d5f2d22..ed0089a3dd88 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -417,6 +417,7 @@ class HloSharding:\n   def tuple_elements(self) -> list[HloSharding]: ...\n   def num_devices(self) -> int: ...\n   def num_dimensions(self) -> int: ...\n+  def is_tile_assignment_iota(self) -> bool: ...\n   def tile_assignment_dimensions(self) -> Sequence[int]: ...\n   def tile_assignment_devices(self) -> Sequence[int]: ...\n   def subgroup_types(self) -> Sequence[OpSharding_Type]: ...\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex c97ebf9d7c0a..ac816e72bebe 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 343\n+_version = 344\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\ndiff --git a/jaxlib/xla_compiler.cc b/jaxlib/xla_compiler.cc\nindex 1066c8137d32..f9ec134793ed 100644\n--- a/jaxlib/xla_compiler.cc\n+++ b/jaxlib/xla_compiler.cc\n@@ -1425,6 +1425,10 @@ void BuildXlaCompilerSubmodule(nb::module_& m) {\n             return self.tile_assignment().num_dimensions();\n           },\n           nb::lock_self())\n+      .def(\"is_tile_assignment_iota\",\n+           [](const xla::HloSharding& self) {\n+             return self.tile_assignment().iota().has_value();\n+           })\n       .def(\n           \"tile_assignment_dimensions\",\n           [](const xla::HloSharding& self) {\ndiff --git a/tests/array_test.py b/tests/array_test.py\nindex 0a04e7bd4bc9..10a17b557dea 100644\n--- a/tests/array_test.py\n+++ b/tests/array_test.py\n@@ -28,6 +28,7 @@\n from jax._src import op_shardings\n from jax._src import test_util as jtu\n from jax._src import xla_bridge as xb\n+from jax._src.lib import jaxlib_extension_version\n from jax._src.lib import xla_client as xc\n from jax._src.lib.mlir import dialects, ir\n from jax._src.util import safe_zip\n@@ -1467,7 +1468,7 @@ def test_named_sharding_unreduced_error(self):\n       NamedSharding(mesh, P('x', unreduced=('y', None)))\n \n   def test_hlo_sharding_get_axis_sizes(self):\n-    if jax._src.lib.jaxlib_extension_version < 343:\n+    if jaxlib_extension_version < 343:\n       self.skipTest('Requires jaxlib_extension_version >= 343')\n \n     op = xc.OpSharding()\n@@ -1479,6 +1480,46 @@ def test_hlo_sharding_get_axis_sizes(self):\n     self.assertIn('{devices=[6,35]<=[7,10,3]T(2,1,0)}', repr(s))\n     self.assertEqual(s._to_xla_hlo_sharding(2).get_axis_sizes(), [7, 2, 5, 3])\n \n+  @parameterized.named_parameters(\n+      ('2d_mesh_x_y', (4, 2), P('x', 'y')),\n+      ('2d_mesh_x', (4, 2), P('x')),\n+      ('2d_mesh_y', (4, 2), P('y')),\n+      ('2d_mesh_none_y', (4, 2), P(None, 'y')),\n+      ('2d_mesh_none_x', (4, 2), P(None, 'x')),\n+      ('2d_mesh_xy', (4, 2), P(('x', 'y'))),\n+      ('2d_mesh_none_xy', (4, 2), P(None, ('x', 'y'))),\n+      ('2d_mesh_fully_replicated', (4, 2), P()),\n+      ('2d_mesh_x_none', (2, 1), P(('x',), None)),\n+      ('3d_mesh_none_none_z', (2, 2, 2), P(None, None, 'z')),\n+      ('3d_mesh_none_y_none', (2, 2, 2), P(None, 'y', None)),\n+      ('3d_mesh_x_y_none', (2, 2, 2), P('x', 'y', None)),\n+      ('3d_mesh_none_yz', (2, 2, 2), P(None, ('y', 'z'))),\n+      ('3d_mesh_x_none_yz', (2, 2, 2), P('x', None, ('y', 'z'))),\n+      ('3d_mesh_none_x_yz', (2, 2, 2), P(None, 'x', ('y', 'z'))),\n+      ('3d_mesh_xy_z', (2, 2, 2), P(('x', 'y'), 'z')),\n+      ('3d_mesh_xy_none_z', (2, 2, 2), P(('x', 'y'), None, 'z')),\n+      ('3d_mesh_x_y_z', (2, 2, 2), P('x', 'y', 'z')),\n+      ('3d_mesh_xz_y', (2, 2, 2), P(('x', 'z'), 'y')),\n+      ('3d_mesh_xz_none_y', (2, 2, 2), P(('x', 'z'), None, 'y')),\n+      ('3d_mesh_y_none_xz', (2, 2, 2), P('y', None, ('x', 'z'))),\n+      ('3d_mesh_none_y_xz', (2, 2, 2), P(None, 'y', ('x', 'z'))),\n+      ('3d_mesh2_none_none_z', (1, 2, 4), P(None, None, 'z')),\n+      ('3d_mesh2_x_none_none', (1, 2, 4), P('x', None, None)),\n+      ('3d_mesh_x_none_none', (2, 1, 1), P('x', None, None)),\n+  )\n+  def test_gspmd_sharding_shardy_lowering(self, mesh_shape, pspec):\n+    if jaxlib_extension_version < 344:\n+      self.skipTest('Requires jaxlib_extension_version >= 344')\n+\n+    ndim = len(mesh_shape)\n+    mesh = jtu.create_mesh(\n+        mesh_shape, ('x', 'y') if ndim == 2 else ('x', 'y', 'z')\n+    )\n+    ns = jax.sharding.NamedSharding(mesh, pspec)\n+    gs = GSPMDSharding(ns._device_assignment, ns._to_xla_hlo_sharding(ndim))\n+    out_sdy_sharding = gs._to_sdy_sharding(ndim)\n+    self.assertTrue(out_sdy_sharding, ns._to_sdy_sharding(ndim))\n+\n \n @jtu.with_config(jax_use_shardy_partitioner=True)\n class ShardyShardingTest(jtu.JaxTestCase):\n\n```",
        "from_id": [
            "ZixuanJiang",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Introduce `get_axis_sizes` API for `HloSharding` in JAX.\n\nPiperOrigin-RevId: 762213931",
        "output": "```diff\nCommit: 326361828d20d9cc295edf2546c97696f446f1df\nDate: 2025-05-23T01:55:42Z\nURL: https://github.com/jax-ml/jax/commit/326361828d20d9cc295edf2546c97696f446f1df\nFiles changed: 5\nAdditions: +27, Deletions: -2\ndiff --git a/jaxlib/BUILD b/jaxlib/BUILD\nindex 363218f4a9f3..dd96b7d23a8e 100644\n--- a/jaxlib/BUILD\n+++ b/jaxlib/BUILD\n@@ -1212,6 +1212,7 @@ cc_library(\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//mlir:Support\",\n         \"@nanobind\",\n         \"@xla//xla:array\",\n         \"@xla//xla:debug_options_flags\",\n@@ -1242,6 +1243,7 @@ cc_library(\n         \"@xla//xla/service:hlo_module_config\",\n         \"@xla//xla/service:hlo_proto_cc\",\n         \"@xla//xla/service:name_uniquer\",\n+        \"@xla//xla/service/spmd/shardy/stablehlo_round_trip:stablehlo_import\",\n         \"@xla//xla/tsl/lib/strings:proto_serialization\",\n         \"@xla//xla/tsl/platform:env\",\n         \"@xla//xla/tsl/platform:errors\",\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 1d7f3042e8a3..898a4d5f2d22 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -422,6 +422,7 @@ class HloSharding:\n   def subgroup_types(self) -> Sequence[OpSharding_Type]: ...\n   def replicate_on_last_tile_dim(self) -> bool: ...\n   def to_proto(self) -> OpSharding: ...\n+  def get_axis_sizes(self) -> list[int]: ...\n \n # === END xla_compiler.cc\n \ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 8f8c829ee6c7..c97ebf9d7c0a 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 342\n+_version = 343\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\ndiff --git a/jaxlib/xla_compiler.cc b/jaxlib/xla_compiler.cc\nindex 73007530c27b..1066c8137d32 100644\n--- a/jaxlib/xla_compiler.cc\n+++ b/jaxlib/xla_compiler.cc\n@@ -32,6 +32,7 @@ limitations under the License.\n #include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"mlir/Support/LLVM.h\"\n #include \"nanobind/nanobind.h\"\n #include \"nanobind/ndarray.h\"\n #include \"nanobind/stl/optional.h\"  // IWYU pragma: keep\n@@ -71,6 +72,7 @@ limitations under the License.\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/service/hlo_graph_dumper.h\"\n #include \"xla/service/hlo_module_config.h\"\n+#include \"xla/service/spmd/shardy/stablehlo_round_trip/stablehlo_import.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/lib/strings/proto_serialization.h\"\n@@ -1447,6 +1449,13 @@ void BuildXlaCompilerSubmodule(nb::module_& m) {\n       .def(\"subgroup_types\", &xla::HloSharding::subgroup_types)\n       .def(\"__repr__\",\n            [](const xla::HloSharding& self) { return self.ToString(); })\n-      .def(\"to_proto\", &xla::HloSharding::ToProto);\n+      .def(\"to_proto\", &xla::HloSharding::ToProto)\n+      .def(\"get_axis_sizes\", [](const xla::HloSharding& self) {\n+        // If returning the SmallVector, we encounter the error \"unable to\n+        // convert function return value to a Python type!\".\n+        mlir::SmallVector<int64_t> mesh_shape =\n+            xla::sdy::getAxisSizes(self.tile_assignment());\n+        return std::vector<int64_t>(mesh_shape.begin(), mesh_shape.end());\n+      });\n }  // NOLINT(readability/fn_size)\n }  // namespace xla\ndiff --git a/tests/array_test.py b/tests/array_test.py\nindex 1691c3acc749..0a04e7bd4bc9 100644\n--- a/tests/array_test.py\n+++ b/tests/array_test.py\n@@ -1466,6 +1466,19 @@ def test_named_sharding_unreduced_error(self):\n         ValueError, \"unreduced cannot contain None.*\"):\n       NamedSharding(mesh, P('x', unreduced=('y', None)))\n \n+  def test_hlo_sharding_get_axis_sizes(self):\n+    if jax._src.lib.jaxlib_extension_version < 343:\n+      self.skipTest('Requires jaxlib_extension_version >= 343')\n+\n+    op = xc.OpSharding()\n+    op.type = xc.OpSharding.Type.OTHER\n+    op.tile_assignment_dimensions = [6, 35]\n+    op.iota_reshape_dims = [7, 10, 3]\n+    op.iota_transpose_perm = [2, 1, 0]\n+    s = GSPMDSharding(jax.devices(), op)\n+    self.assertIn('{devices=[6,35]<=[7,10,3]T(2,1,0)}', repr(s))\n+    self.assertEqual(s._to_xla_hlo_sharding(2).get_axis_sizes(), [7, 2, 5, 3])\n+\n \n @jtu.with_config(jax_use_shardy_partitioner=True)\n class ShardyShardingTest(jtu.JaxTestCase):\n\n```",
        "from_id": [
            "ZixuanJiang",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Adds explicit-axis handling to shard_map batching rule.\n\nWithout this handling, in explicit sharding mode vmap of a function with an internal shmap can introduce unnecessary replication.\n\nPiperOrigin-RevId: 762175189",
        "output": "```diff\nCommit: 7cc9053f9d09aff6df2917a0804d861a04be0260\nDate: 2025-05-22T23:39:47Z\nURL: https://github.com/jax-ml/jax/commit/7cc9053f9d09aff6df2917a0804d861a04be0260\nFiles changed: 2\nAdditions: +70, Deletions: -7\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 66df2505100c..0bae2da15272 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -1228,6 +1228,15 @@ def _device_put_eager_rule(mesh, *xs, srcs, devices, copy_semantics):\n \n # Batching\n \n+def _modify_specs_axis_data(trace, name, mesh, in_specs, in_dims):\n+  new_in_specs = [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, name)\n+                  for sp, d in zip(in_specs, in_dims)]\n+  new_size = trace.axis_data.size // prod(mesh.shape[n] for n in name)\n+  new_axis_data = batching.AxisData(\n+      trace.axis_data.name, new_size, trace.axis_data.spmd_name,\n+      trace.axis_data.explicit_mesh_axis)\n+  return new_in_specs, new_axis_data\n+\n def _shard_map_batch(\n     trace: batching.BatchTrace, prim: core.Primitive, fun: lu.WrappedFun,\n     in_tracers: Sequence[batching.BatchTracer], mesh: Mesh,\n@@ -1237,15 +1246,20 @@ def _shard_map_batch(\n   if any(isinstance(d, batching.RaggedAxis) for d in in_dims):\n     raise NotImplementedError\n   spmd_axis_name = trace.axis_data.spmd_name\n+  explicit_mesh_axis = trace.axis_data.explicit_mesh_axis\n   if spmd_axis_name is not None:\n     used = {n for spec in in_specs for n in _spec_to_vma(spec)}\n     if not config.disable_vmap_shmap_error.value and set(spmd_axis_name) & used:\n       raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map in_specs\")\n-    new_in_specs = [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, spmd_axis_name)\n-                    for sp, d in zip(in_specs, in_dims)]\n-    new_size = trace.axis_data.size // prod(mesh.shape[n] for n in spmd_axis_name)\n-    new_axis_data = batching.AxisData(trace.axis_data.name, new_size,\n-                                      trace.axis_data.spmd_name, None)\n+    new_in_specs, new_axis_data = _modify_specs_axis_data(\n+        trace, spmd_axis_name, mesh, in_specs, in_dims)\n+  elif explicit_mesh_axis is not None:\n+    used = {n for spec in in_specs for n in _spec_to_vma(spec)}\n+    if set(explicit_mesh_axis) & used:\n+      raise ValueError(\"vmapped away explicit mesh axis cannot appear in \"\n+                       \"shard_map in_specs\")\n+    new_in_specs, new_axis_data = _modify_specs_axis_data(\n+        trace, explicit_mesh_axis, mesh, in_specs, in_dims)\n   else:\n     new_in_specs = [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, None)\n                     for sp, d in zip(in_specs, in_dims)]\n@@ -1254,7 +1268,8 @@ def _shard_map_batch(\n \n   @as_hashable_function(closure=out_specs_thunk)\n   def new_out_specs_thunk():\n-    return _batch_out_specs(spmd_axis_name, out_dims(), out_specs_thunk())\n+    return _batch_out_specs(spmd_axis_name, explicit_mesh_axis, out_dims(),\n+                            out_specs_thunk())\n \n   new_params = dict(mesh=mesh, in_specs=new_in_specs,\n                     out_specs_thunk=new_out_specs_thunk, check_vma=check_vma,\n@@ -1266,13 +1281,21 @@ def new_out_specs_thunk():\n   return map(make_tracer, out_vals, out_dims())\n batching.BatchTrace.process_shard_map = _shard_map_batch\n \n-def _batch_out_specs(spmd_name, dims, out_specs):\n+def _batch_out_specs(spmd_name, explicit_mesh_axis, dims, out_specs):\n   if spmd_name is not None:\n     used = {n for spec in out_specs for n in _spec_to_vma(spec)}\n     if not config.disable_vmap_shmap_error.value and set(spmd_name) & used:\n       raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map out_specs\")\n     return [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, spmd_name)\n             for sp, d in zip(out_specs, dims)]\n+  elif explicit_mesh_axis is not None:\n+    used = {n for spec in out_specs for n in _spec_to_vma(spec)}\n+    if set(explicit_mesh_axis) & used:\n+      raise ValueError(\"vmapped away explicit mesh axis cannot appear in \"\n+                       \"shard_map out_specs\")\n+    return [sp if d is batching.not_mapped else\n+            pxla.batch_spec(sp, d, explicit_mesh_axis)\n+            for sp, d in zip(out_specs, dims)]\n   else:\n     return [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, None)\n             for sp, d in zip(out_specs, dims)]\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 5fbace3c98e1..90360989f13f 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -767,6 +767,46 @@ def f(x):\n     self.assertIn('out_specs', e.params)\n     self.assertEqual(e.params['out_specs'], (P('y', 'x'),))\n \n+  def test_vmap_explicit_mesh_axis(self):\n+    mesh = jtu.create_mesh(\n+        (1, 2, 2), ('z', 'x', 'y'), axis_types=(AxisType.Explicit,) * 3)\n+\n+    @shard_map(mesh=mesh, in_specs=P('y'), out_specs=P('y'))\n+    def f(x):\n+      return x\n+\n+    x = jnp.arange(4 * 4).reshape(4, 4)\n+    s = NamedSharding(mesh, P(('z', 'x'), 'y'))\n+    x = jax.device_put(x, s)\n+\n+    f = jax.jit(jax.vmap(f))\n+    out = f(x)\n+    self.assertEqual(out.sharding, s)\n+\n+  def test_vmap_explicit_mesh_axis_error(self):\n+    mesh = jtu.create_mesh((2, 2), ('x', 'y'),\n+                           axis_types=(AxisType.Explicit,) * 2)\n+\n+    @shard_map(mesh=mesh, in_specs=P('x'), out_specs=P('x'))\n+    def f(x):\n+      return x\n+\n+    x = jnp.arange(4 * 4).reshape(4, 4)\n+    s = NamedSharding(mesh, P('x', 'y'))\n+    x = jax.device_put(x, s)\n+\n+    f = jax.jit(jax.vmap(f))\n+    with self.assertRaisesRegex(\n+        ValueError, \"vmapped away explicit mesh axis cannot appear\"):\n+      f(x)\n+\n+    f = jax.jit(jax.vmap(f, spmd_axis_name='y'))\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'Only one of spmd_axis_name or arrays sharded on `Explicit` mesh axis'\n+        ' type is allowed'):\n+      f(x)\n+\n   def test_vmap_of_grad_spmd_axis_name(self):\n     mesh = jtu.create_mesh((2, 2), ('x', 'y'))\n \n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Avoid doing DCE of effectful ops and reordering in partial eval.",
        "output": "```diff\nCommit: 859e120fdd79575da26da4ea561c4bb135492b3c\nDate: 2025-05-22T23:23:26Z\nURL: https://github.com/jax-ml/jax/commit/859e120fdd79575da26da4ea561c4bb135492b3c\nFiles changed: 12\nAdditions: +52, Deletions: -36\ndiff --git a/jax/_src/ad_checkpoint.py b/jax/_src/ad_checkpoint.py\nindex 2d743bf06c6b..2a056d5c94f0 100644\n--- a/jax/_src/ad_checkpoint.py\n+++ b/jax/_src/ad_checkpoint.py\n@@ -578,7 +578,7 @@ def remat_partial_eval(trace: pe.JaxprTrace, *tracers: core.Tracer,\n   out_jaxpr_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(x.aval), None)\n                        for x in jaxpr_unknown.outvars]\n   new_params = dict(params, jaxpr=jaxpr_unknown, differentiated=True)\n-  recipe = pe.new_eqn_recipe(in_jaxpr_tracers, out_jaxpr_tracers, remat_p,\n+  recipe = pe.new_eqn_recipe(trace, in_jaxpr_tracers, out_jaxpr_tracers, remat_p,\n                              new_params, jaxpr_unknown.effects,\n                              source_info_util.current())\n \ndiff --git a/jax/_src/debugging.py b/jax/_src/debugging.py\nindex e931a6edb9b3..3490de5118e1 100644\n--- a/jax/_src/debugging.py\n+++ b/jax/_src/debugging.py\n@@ -127,7 +127,7 @@ def debug_callback_jvp_rule(primals, tangents, **params):\n ad.primitive_jvps[debug_callback_p] = debug_callback_jvp_rule\n \n def debug_callback_transpose_rule(*flat_args, callback: Callable[..., Any],\n-    effect: DebugEffect):\n+                                  effect: DebugEffect, partitioned):\n   del flat_args, callback, effect\n   raise ValueError(\"Transpose doesn't support debugging callbacks.\")\n ad.primitive_transposes[debug_callback_p] = debug_callback_transpose_rule\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 9366b91f8022..7cbdfff01462 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -885,7 +885,7 @@ def make_zero(aval):\n     out_nz_tracers = [trace.to_jaxpr_tracer(r)\n                       for (r, nz) in zip(out_tangents, out_nzs) if nz]\n     in_tracers = [t for t, nz in zip(tangent_args, nonzeros) if nz]\n-    jaxpr, out_consts, _ = pe.tracers_to_jaxpr(in_tracers, out_nz_tracers, jvp.debug_info)\n+    jaxpr, out_consts, _ = pe.tracers_to_jaxpr(in_tracers, out_nz_tracers, [], jvp.debug_info)\n     jaxpr, used_consts, _ = pe.dce_jaxpr_consts(\n         jaxpr, [True] * len(jaxpr.outvars),\n         [False] * len(jaxpr.constvars) + [True] * len(jaxpr.invars))\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex f77db5443a86..6ea16ec8e8ba 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -16,6 +16,7 @@\n from collections import namedtuple\n from collections.abc import Callable, Sequence, Hashable\n import contextlib\n+from dataclasses import dataclass\n from functools import partial\n import itertools as it\n import operator as op\n@@ -42,7 +43,7 @@\n                            mapped_aval, unmapped_aval, DBIdx, InDBIdx, OutDBIdx,\n                            InputType, OutputType, get_referent, JaxprEqnContext)\n from jax._src.source_info_util import SourceInfo\n-from jax._src.state.types import AbstractRef, ReadEffect\n+from jax._src.state.types import AbstractRef, ReadEffect, RefEffect\n from jax._src.tree_util import (PyTreeDef, treedef_tuple, tree_flatten,\n                                 tree_structure, register_static)\n from jax._src.util import (unzip2, safe_zip, safe_map, toposort, split_list,\n@@ -147,6 +148,10 @@ def get_aval(self) -> AbstractValue:\n     else:\n       return self[0]\n \n+@dataclass(frozen=True)\n+class EffectHandle:\n+  parents : list[Tracer]\n+  recipe : JaxprEqnRecipe\n \n class JaxprTrace(Trace['JaxprTracer']):\n \n@@ -156,6 +161,8 @@ def __init__(self, parent_trace:Trace, name_stack: source_info_util.NameStack, t\n     self.tag = tag\n     self.parent_trace = parent_trace\n     self.requires_low = False\n+    self.effect_handles : list[EffectHandle] = []\n+    self.counter = it.count()\n \n   def to_jaxpr_tracer(self, x):\n     if isinstance(x, JaxprTracer) and x._trace.tag is self.tag:\n@@ -239,14 +246,19 @@ def default_process_primitive(self, primitive, tracers, params):\n     if primitive.multiple_results:\n       out_tracers = [JaxprTracer(self, PartialVal.unknown(aval), None)\n                      for aval in out_aval]\n-      eqn = new_eqn_recipe(tracers, out_tracers, primitive, params, effects,\n+      eqn = new_eqn_recipe(self, tracers, out_tracers, primitive, params, effects,\n                            source)\n+      if any(isinstance(e, RefEffect) for e in effects):\n+        self.effect_handles.append(EffectHandle(tracers, eqn))\n       for t in out_tracers: t.recipe = eqn\n       return out_tracers\n     else:\n       out_tracer = JaxprTracer(self, PartialVal.unknown(out_aval), None)\n-      out_tracer.recipe = new_eqn_recipe(tracers, [out_tracer], primitive,\n-                                         params, effects, source)\n+      eqn = new_eqn_recipe(self, tracers, [out_tracer], primitive,\n+                           params, effects, source)\n+      if any(isinstance(e, RefEffect) for e in effects):\n+        self.effect_handles.append(EffectHandle(tracers, eqn))\n+      out_tracer.recipe = eqn\n       return out_tracer\n \n   def process_call(self, primitive, f: lu.WrappedFun, tracers, params):\n@@ -321,7 +333,7 @@ def process_call(self, primitive, f: lu.WrappedFun, tracers, params):\n                      for a in out_type]\n     name_stack = self._current_truncated_name_stack()\n     source = source_info_util.current().replace(name_stack=name_stack)\n-    eqn = new_eqn_recipe((*res_tracers, *env_tracers, *unknown_arg_tracers),\n+    eqn = new_eqn_recipe(self, (*res_tracers, *env_tracers, *unknown_arg_tracers),\n                          out_tracers, primitive, staged_params, jaxpr.effects,\n                          source)\n     for t in out_tracers: t.recipe = eqn\n@@ -390,7 +402,7 @@ def const_out_axes_thunk():\n                    for a in out_avals]\n     effs = core.filter_named_axis_effects(jaxpr.effects, {params['axis_name']})\n     src_info = source_info_util.current()\n-    eqn = new_eqn_recipe((*const_tracers, *env_tracers, *unknown_arg_tracers),\n+    eqn = new_eqn_recipe(self, (*const_tracers, *env_tracers, *unknown_arg_tracers),\n                          out_tracers, primitive, staged_params, effs, src_info)\n     for t in out_tracers: t.recipe = eqn\n \n@@ -425,7 +437,7 @@ def process_custom_transpose(self, prim, call, tracers, **params):\n                      for aval in params['out_types']]\n       in_tracers = map(self.instantiate_const, tracers)\n       new_params = dict(params, call=call)\n-      eqn = new_eqn_recipe(in_tracers, out_tracers, prim, new_params,\n+      eqn = new_eqn_recipe(self, in_tracers, out_tracers, prim, new_params,\n           core.no_effects, source_info_util.current())\n       for t in out_tracers: t.recipe = eqn\n       return out_tracers\n@@ -470,7 +482,7 @@ def fwd_jaxpr_thunk(*zeros):\n         out_trees=out_trees,\n         symbolic_zeros=symbolic_zeros\n     )\n-    eqn = new_eqn_recipe((*res_tracers, *env_tracers, *tracers),\n+    eqn = new_eqn_recipe(self, (*res_tracers, *env_tracers, *tracers),\n                          out_tracers, prim, params, jaxpr.effects, source)\n     for t in out_tracers: t.recipe = eqn\n     return out_tracers\n@@ -657,7 +669,7 @@ def _trace_to_subjaxpr_nounits(f: Callable, trace: JaxprTrace,\n   out_tracers = [trace.instantiate_const(t) if inst else t\n                  for inst, t in zip(instantiate, out_tracers)]\n   out_tracers_ = [t for t in out_tracers if not t.is_known()]\n-  jaxpr, out_consts, env = tracers_to_jaxpr(in_tracers, out_tracers_, debug_info)\n+  jaxpr, out_consts, env = tracers_to_jaxpr(in_tracers, out_tracers_, trace.effect_handles, debug_info)\n   return out_tracers, jaxpr, out_consts, env\n \n # The below variant implements an optimization where residuals which are also\n@@ -739,7 +751,8 @@ class JaxprEqnRecipe(NamedTuple):\n   source_info: source_info_util.SourceInfo\n   ctx: JaxprEqnContext\n \n-def new_eqn_recipe(in_tracers: Sequence[JaxprTracer],\n+def new_eqn_recipe(trace: JaxprTrace,\n+                   in_tracers: Sequence[JaxprTracer],\n                    out_tracers: Sequence[JaxprTracer],\n                    primitive: Primitive,\n                    params: dict[str, Any],\n@@ -762,7 +775,7 @@ def new_eqn_recipe(in_tracers: Sequence[JaxprTracer],\n       config.threefry_partitionable.value,\n       xla_metadata_lib.current_xla_metadata(),\n   )\n-  return JaxprEqnRecipe(object(), tuple(in_tracers), map(ref, out_tracers),\n+  return JaxprEqnRecipe(next(trace.counter), tuple(in_tracers), map(ref, out_tracers),\n                         out_avals, primitive, params, effects, source_info,\n                         ctx)\n \n@@ -780,6 +793,7 @@ def recipe_to_eqn(getvar: Callable[[JaxprTracer], Atom],\n def tracers_to_jaxpr(\n   in_tracers: Sequence[JaxprTracer],\n   out_tracers: Sequence[JaxprTracer],\n+  effect_handles: Sequence[Any],\n   debug_info: core.DebugInfo,\n   ) -> tuple[Jaxpr, tuple[Any, ...], tuple[Any, ...]]:\n   \"\"\"Constructs Jaxpr given tracers for inputs and outputs.\n@@ -821,7 +835,15 @@ def type_substitute(aval: AbstractValue) -> AbstractValue:\n \n   processed_eqn_ids = set()\n   eqns: list[core.JaxprEqn] = []\n-  for t in toposort((*in_tracers, *out_tracers)):\n+\n+  reachable = toposort\n+  tracers = reachable((*in_tracers, *out_tracers, *effect_handles))\n+  def sort_key(t):\n+    r = t.recipe\n+    return r.eqn_id if isinstance(r, JaxprEqnRecipe) else -1\n+  tracers = sorted(tracers, key=sort_key)\n+\n+  for t in tracers:\n     r = t.recipe\n     if isinstance(r, JaxprEqnRecipe):\n       # TODO broadcast_in_dim can create a new tracer, not present in parents\ndiff --git a/jax/_src/lax/control_flow/conditionals.py b/jax/_src/lax/control_flow/conditionals.py\nindex 741636c47e31..4e8368341d9f 100644\n--- a/jax/_src/lax/control_flow/conditionals.py\n+++ b/jax/_src/lax/control_flow/conditionals.py\n@@ -617,7 +617,7 @@ def _cond_partial_eval(trace, *tracers, branches, **params):\n   name_stack = source_info_util.current_name_stack()[len(trace.name_stack):]\n   source = source_info_util.current().replace(name_stack=name_stack)\n   eqn = pe.new_eqn_recipe(\n-      [index_tracer] + res_tracers + ops_tracers, out_tracers, cond_p, params,\n+      trace, [index_tracer] + res_tracers + ops_tracers, out_tracers, cond_p, params,\n       core.join_effects(*(j.effects for j in branches_unknown)), source)\n   for t in out_tracers: t.recipe = eqn\n   return util.merge_lists(out_uks, out_consts, out_tracers)\ndiff --git a/jax/_src/lax/control_flow/for_loop.py b/jax/_src/lax/control_flow/for_loop.py\nindex fc7ebde4cbea..90b81ae367aa 100644\n--- a/jax/_src/lax/control_flow/for_loop.py\n+++ b/jax/_src/lax/control_flow/for_loop.py\n@@ -498,7 +498,7 @@ def _for_partial_eval(trace: pe.JaxprTrace, *tracers: pe.JaxprTracer,\n \n   assert len(unknown_inputs) == len(res_ref_unknown_outputs)\n   assert len(unknown_inputs) == len(jaxpr_unknown.invars) - 1\n-  eqn = pe.new_eqn_recipe(unknown_inputs, res_ref_unknown_outputs,\n+  eqn = pe.new_eqn_recipe(trace, unknown_inputs, res_ref_unknown_outputs,\n                           for_p, dict(jaxpr=jaxpr_unknown, nsteps=nsteps,\n                                       reverse=reverse,\n                                       which_linear=which_linear_unknown,\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 7efe3294fdca..83c31928d7cb 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -920,7 +920,7 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n   name_stack = source_info_util.current_name_stack()[len(trace.name_stack):]\n   source = source_info_util.current().replace(name_stack=name_stack)\n   assert len(out_tracers) == len(jaxpr_unknown.out_avals)\n-  eqn = pe.new_eqn_recipe([*intensive_res, *unknown_inputs, *extensive_res],\n+  eqn = pe.new_eqn_recipe(trace, [*intensive_res, *unknown_inputs, *extensive_res],\n                           out_tracers, scan_p,\n                           dict(reverse=reverse, length=length, unroll=unroll,\n                                jaxpr=jaxpr_unknown, linear=linear_unknown,\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex a49c27d06eee..0e3695ba4506 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -6550,7 +6550,7 @@ def _broadcast_in_dim_partial_eval(\n   out_aval = core.DShapedArray(tuple(shape_), operand.dtype, operand.weak_type)\n   out_tracer = pe.JaxprTracer(trace, pe.PartialVal.unknown(out_aval), None)\n   eqn = pe.new_eqn_recipe(\n-      [operand_tracer, *dyn_shape_tracers], [out_tracer], broadcast_in_dim_p,\n+      trace, [operand_tracer, *dyn_shape_tracers], [out_tracer], broadcast_in_dim_p,\n       dict(shape=shape, broadcast_dimensions=broadcast_dimensions,\n            sharding=None),\n       core.no_effects, source_info_util.current())\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 0c55f3fe30ab..d5286be8e0c9 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -2324,18 +2324,8 @@ def _pjit_partial_eval(trace: pe.JaxprTrace,\n \n   known_ins = tuple(pv.is_known() for pv in in_pvals)\n   unknown_ins = tuple(not k for k in known_ins)\n-  if any(isinstance(e, (RefEffect, core.InternalMutableArrayEffect))\n-         for e in jaxpr.effects):\n-    known_jaxpr_, unknown_jaxpr_, unknown_outs, _, num_res_val, num_res_ref = \\\n-        pe.partial_eval_jaxpr_stateful(jaxpr.jaxpr, unknown_ins, unknown_ins,\n-                                       False, False, None)\n-    if num_res_ref: raise NotImplementedError\n-    known_jaxpr = pe.ClosedJaxpr(known_jaxpr_, jaxpr.consts)\n-    unknown_jaxpr = pe.ClosedJaxpr(unknown_jaxpr_, jaxpr.consts)\n-    res_avals = unknown_jaxpr.in_avals[:num_res_val]\n-  else:\n-    known_jaxpr, unknown_jaxpr, unknown_outs, res_avals = \\\n-        pe.partial_eval_jaxpr_nounits(jaxpr, unknown_ins, instantiate=False)\n+  known_jaxpr, unknown_jaxpr, unknown_outs, res_avals = \\\n+      pe.partial_eval_jaxpr_nounits(jaxpr, unknown_ins, instantiate=False)\n   unknown_outs = tuple(unknown_outs)  # type: ignore[assignment]\n   known_outs = tuple(not uk for uk in unknown_outs)\n   num_residuals = len(res_avals)\n@@ -2431,7 +2421,7 @@ def keep_where(l, should_keep):\n       pe.JaxprTracer(trace, pe.PartialVal.unknown(aval), None)\n       for aval in unknown_out_avals\n   ]\n-  eqn = pe.new_eqn_recipe((*unknown_tracers_in, *residual_tracers),\n+  eqn = pe.new_eqn_recipe(trace, (*unknown_tracers_in, *residual_tracers),\n                           unknown_tracers_out,\n                           pjit_p,\n                           unknown_params,\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 66df2505100c..4f60a833429a 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -1369,7 +1369,7 @@ def known_out_specs():\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(a), None)\n                  for a in out_avals]\n   effs = core.filter_named_axis_effects(jaxpr.effects, mesh.axis_names)\n-  eqn = pe.new_eqn_recipe((*const_tracers, *env_tracers, *unk_arg_tracers),\n+  eqn = pe.new_eqn_recipe(trace, (*const_tracers, *env_tracers, *unk_arg_tracers),\n                           out_tracers, shard_map_p, unk_params,\n                           effs, source_info_util.current())\n   for t in out_tracers: t.recipe = eqn\ndiff --git a/jax/_src/state/discharge.py b/jax/_src/state/discharge.py\nindex bc6a20a0a76e..100447f12d18 100644\n--- a/jax/_src/state/discharge.py\n+++ b/jax/_src/state/discharge.py\n@@ -828,7 +828,7 @@ def _run_state_partial_eval(trace: pe.JaxprTrace, *tracers: pe.JaxprTracer,\n                    is_initialized=(True,) * len(jaxpr_unknown.invars))\n   _, eqn_effects = run_state_p.abstract_eval(*[v.aval for v in unknown_inputs],\n                                              **uk_params)\n-  eqn = pe.new_eqn_recipe(unknown_inputs, res_ref_unknown_outputs,\n+  eqn = pe.new_eqn_recipe(trace, unknown_inputs, res_ref_unknown_outputs,\n                           run_state_p, uk_params,\n                           eqn_effects, source)\n   for t in res_ref_unknown_outputs: t.recipe = eqn\ndiff --git a/tests/mutable_array_test.py b/tests/mutable_array_test.py\nindex 865d4f8520f1..0da335e2fac5 100644\n--- a/tests/mutable_array_test.py\n+++ b/tests/mutable_array_test.py\n@@ -192,14 +192,18 @@ def f():\n     x = f()\n     self.assertArraysEqual(x, jnp.zeros(8))\n \n-  def test_grad_mutable_array(self):\n-    @jax.jit\n+  @parameterized.parameters([False, True])\n+  def test_grad_mutable_array(self, jit):\n+\n     def f(x):\n       x_ = core.mutable_array(x)\n       x_[()] = x_[()] + x_[()]\n       y = core.freeze(x_)\n       return y\n \n+    if jit:\n+      f = jax.jit(f)\n+\n     ans = jax.grad(f)(1.)\n     expected = 2.0\n     self.assertAllClose(ans, expected, check_dtypes=False)\n\n```",
        "from_id": [
            "dougalm"
        ]
    },
    {
        "text_input": "Use additional semaphores to avoid data races in TPU paged_attention_kernel.\n\nAlso prevents an out-of-bounds read of SMEM.  And re-enables tests for the TPU paged_attention_kernel.\n\n@apaszke confirmed the presence of data races using the race detector in the new TPU interpret mode.  With the additional semaphores, the race detector no longer detects any races in the this kernel and I no longer see any test failures in 20+ test runs on a TPU.\n\nDetails on the data races:\n\n - In each iteration, the kernel:\n   (a) Starts copying data for `k` and `v` for the next iteration.\n   (b) Waits for the copy of `k` for the current iteration to finish.\n   (c) Waits for the copy of `v` for the current iteration to finish.\n\n - It is possible for these copies to happen out of order -- that is:\n   (a) The copies for the next iteration can finish before the copies\n       for the current iteration.\n   (b) And the copies for `v` for the current iteration can finish\n       before the copies for `k` for the current iteration.\n\n - If the same DMA semaphore is used for everything, then out-of-order\n   copies can lead to:\n   (a) `k = async_copy_k.wait_and_get_loaded()` returns but the data\n       isn't all available because the underlying semaphore was\n       signaled by the completion of copies of `v` for the current\n       iteration or copies of `k` or `v` for the next iteration.\n   (a) `v = async_copy_v.wait_and_get_loaded()` returns but the data\n       isn't all available because the underlying semaphore was\n       signaled by the completion of copies of `k` or `v` for the\n       next iteration.\n\nPiperOrigin-RevId: 762136079",
        "output": "```diff\nCommit: 0e690c1a88049787f8371d462d674a94f9b85c73\nDate: 2025-05-22T21:53:42Z\nURL: https://github.com/jax-ml/jax/commit/0e690c1a88049787f8371d462d674a94f9b85c73\nFiles changed: 2\nAdditions: +15, Deletions: -10\ndiff --git a/jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py b/jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py\nindex 6280064f29d3..9c02679c45ea 100644\n--- a/jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py\n+++ b/jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py\n@@ -127,7 +127,8 @@ def paged_flash_attention_kernel(\n     k_scales_vmem_buffer,\n     v_vmem_buffer,\n     v_scales_vmem_buffer,\n-    sem,\n+    k_sems,\n+    v_sems,\n     *,\n     batch_size: int,\n     pages_per_compute_block: int,\n@@ -176,7 +177,9 @@ def advance_to_next_non_zero_length():\n \n       return (\n           lax.cond(\n-              jnp.logical_and(next_b < batch_size, lengths_ref[next_b] == 0),\n+              jnp.logical_and(\n+                  next_b < batch_size,\n+                  lengths_ref[lax.clamp(0, next_b, batch_size - 1)] == 0),\n               advance_to_next_non_zero_length,\n               lambda: next_b,\n           ),\n@@ -200,7 +203,7 @@ def create_kv_async_copy_descriptors(b, h, i, buffer_index):\n         k_scales_vmem_buffer.at[buffer_index]\n         if k_scales_vmem_buffer is not None\n         else None,\n-        sem,\n+        k_sems.at[buffer_index],\n         page_indices_ref,\n         page_offset,\n         pages_to_load,\n@@ -213,7 +216,7 @@ def create_kv_async_copy_descriptors(b, h, i, buffer_index):\n         v_scales_vmem_buffer.at[buffer_index]\n         if v_scales_vmem_buffer is not None\n         else None,\n-        sem,\n+        v_sems.at[buffer_index],\n         page_indices_ref,\n         page_offset,\n         pages_to_load,\n@@ -301,7 +304,8 @@ def paged_flash_attention_kernel_inline_seq_dim(\n     k_scales_vmem_buffer,\n     v_vmem_buffer,\n     v_scales_vmem_buffer,\n-    sem,\n+    k_sems,\n+    v_sems,\n     *,\n     batch_size: int,\n     pages_per_compute_block: int,\n@@ -336,7 +340,8 @@ def body(i, _):\n         k_scales_vmem_buffer,\n         v_vmem_buffer,\n         v_scales_vmem_buffer,\n-        sem,\n+        k_sems,\n+        v_sems,\n         batch_size=batch_size,\n         pages_per_compute_block=pages_per_compute_block,\n         pages_per_sequence=pages_per_sequence,\n@@ -584,7 +589,8 @@ def paged_attention(\n             ),\n             v_scales_pages.dtype,  # pytype: disable=attribute-error\n         ),  # v_scales_pages buffer\n-        pltpu.SemaphoreType.DMA,\n+        pltpu.SemaphoreType.DMA((2,)),\n+        pltpu.SemaphoreType.DMA((2,)),\n     )\n   else:\n     in_specs = [\n@@ -615,7 +621,8 @@ def paged_attention(\n             v_pages.dtype,\n         ),  # v_pages buffer\n         None,\n-        pltpu.SemaphoreType.DMA,\n+        pltpu.SemaphoreType.DMA((2,)),\n+        pltpu.SemaphoreType.DMA((2,)),\n     )\n \n   out, _, _ = pl.pallas_call(\ndiff --git a/tests/pallas/tpu_paged_attention_kernel_test.py b/tests/pallas/tpu_paged_attention_kernel_test.py\nindex 9886e7943f6f..ac24fea1b45a 100644\n--- a/tests/pallas/tpu_paged_attention_kernel_test.py\n+++ b/tests/pallas/tpu_paged_attention_kernel_test.py\n@@ -265,8 +265,6 @@ def test_paged_attention(\n       attn_logits_soft_cap,\n       are_kv_quantized,\n   ):\n-    # TODO(mvoz, skyewm): Re-enable this test once the data race is fixed.\n-    self.skipTest(\"This kernel has data races that need to be fixed.\")\n     if not jtu.is_device_tpu_at_least(4):\n       self.skipTest(\"Only supports TPU generation 4 or above\")\n     if jtu.is_device_tpu(version=4) and are_kv_quantized:\n\n```",
        "from_id": [
            "jburnim",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Raises an explicit error in reshard.\n\nHit this while working with sharding in types -- passing a sharding that had an empty mesh. (I think this was in a test). This failed trying to acces with `with_spec` attribute on None -- so just catching this case early.\n\nPiperOrigin-RevId: 762135310",
        "output": "```diff\nCommit: fc683368fa457bbffd3a693fc80ce880c1796e18\nDate: 2025-05-22T21:51:14Z\nURL: https://github.com/jax-ml/jax/commit/fc683368fa457bbffd3a693fc80ce880c1796e18\nFiles changed: 2\nAdditions: +15, Deletions: -0\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex ecdcf3e17332..0c55f3fe30ab 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -2987,6 +2987,11 @@ def reshard(xs, out_shardings):\n   out_flat = []\n   for x, x_aval, s in safe_zip(x_flat, x_avals_flat, shardings_flat):\n     ds = canonicalize_sharding(s, 'reshard')\n+    if ds is None:\n+      raise ValueError(\n+          'Reshard should only be used with out_shardings which are non-None '\n+          'and have a nonempty mesh. Got sharding {s}.'\n+      )\n     ds = ds.with_spec(ds.spec._normalized_spec_for_aval(x_aval.ndim))  # pytype: disable=attribute-error\n     out_flat.append(reshard_p.bind(x, dst_sharding=ds))\n   return tree_unflatten(treedef, out_flat)\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 92190dc6bf3c..d37b21bd2460 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -8781,6 +8781,16 @@ def f(x):\n                                 \"to Shardy\"):\n       jax.jit(f)(x)\n \n+  def test_reshard_empty_mesh_error(self):\n+    arr = jax.device_put(np.arange(8), jax.devices()[0])\n+    with self.assertRaisesRegex(ValueError, \"nonempty mesh\"):\n+      reshard(arr, NamedSharding(mesh_lib.empty_abstract_mesh, P(None)))\n+\n+  def test_reshard_none_sharding_error(self):\n+    arr = jax.device_put(np.arange(8), jax.devices()[0])\n+    with self.assertRaisesRegex(ValueError, \"non-None\"):\n+      reshard(arr, None)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "jkr26",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add CUDA pytest job strategy to test CUDA packages downloaded from PyPI\n\nContinuous and Nightly/Release workflows will now run the CUDA 12.8 test runs by using the Nvidia CUDA packages from PyPI instead of those on the system.\n\nPiperOrigin-RevId: 762125356",
        "output": "```diff\nCommit: 0e24e98032fef09e31a8e9219967aa92cf370b86\nDate: 2025-05-22T21:26:09Z\nURL: https://github.com/jax-ml/jax/commit/0e24e98032fef09e31a8e9219967aa92cf370b86\nFiles changed: 7\nAdditions: +56, Deletions: -31\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex a20be5b1dbcf..2f22901e661a 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -24,11 +24,16 @@ on:\n         type: string\n         required: true\n         default: \"3.12\"\n-      cuda:\n+      cuda-version:\n         description: \"Which CUDA version to test?\"\n         type: string\n         required: true\n-        default: \"12.3\"\n+        default: \"12.8\"\n+      use-nvidia-pip-wheels:\n+        description: \"Whether to download CUDA packages from PyPI?\"\n+        type: boolean\n+        required: false\n+        default: false\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n@@ -58,8 +63,11 @@ jobs:\n         shell: bash\n     runs-on: ${{ inputs.runner }}\n     # Test the oldest and newest supported CUDA versions.\n-    container:  ${{ (contains(inputs.cuda, '12.1') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.1-cudnn9.8:latest') ||\n-                (contains(inputs.cuda, '12.8') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest') }}\n+    # If testing the CUDA packages from PyPI, then use the ml-build image which does not have any\n+    # CUDA pckages installed on the system.\n+    container:  ${{ !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.1') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.1-cudnn9.8:latest') ||\n+                !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.8') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest') ||\n+                inputs.use-nvidia-pip-wheels && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest'}}\n     name: \"Pytest CUDA (${{ inputs.runner }}, CUDA ${{ inputs.cuda }}, Python ${{ inputs.python }}, x64=${{ inputs.enable-x64 }})\"\n \n     env:\n@@ -100,13 +108,24 @@ jobs:\n           if [[ \"${{ inputs.download-jax-only-from-gcs }}\" == \"1\" ]]; then\n             echo \"JAX only release. Only downloading the jax wheel from the release bucket.\"\n \n-            # Set the env var to install the CUDA plugin and PJRT packages from PyPI. jaxlib is\n-            # required dependency of jax so that gets installed automatically.\n-            echo \"JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI=jax_cuda_pypi\">> $GITHUB_ENV\n+            if [[ \"${{ inputs.use-nvidia-pip-wheels }}\" == false ]]; then\n+              # Install only the PJRT and JAX CUDA Plugin packages from PyPI. Nvidia CUDA packages\n+              # are used from the system.\n+              echo \"JAXCI_JAX_PYPI_EXTRAS=cuda12-local\">> $GITHUB_ENV\n+            else\n+             # Install the PJRT, JAX CUDA Plugin, and Nvidia CUDA packages from PyPI.\n+              echo \"JAXCI_JAX_PYPI_EXTRAS=cuda12\">> $GITHUB_ENV\n+            fi\n           else\n             gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n             gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*plugin*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n             gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*pjrt*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n+\n+             if [[ \"${{ inputs.use-nvidia-pip-wheels }}\" == true ]]; then\n+              # Install the Nvidia CUDA packages from PyPI. The wheels downloaded in the previous\n+              # step will be used for the PJRT and JAX CUDA Plugin packages.\n+              echo \"JAXCI_JAX_PYPI_EXTRAS=cuda12\">> $GITHUB_ENV\n+             fi\n           fi\n       - name: Skip the test run if the wheel artifacts were not downloaded successfully\n         if: steps.download-wheel-artifacts.outcome == 'failure'\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex d1af90283001..ae0250884831 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -137,9 +137,9 @@ jobs:\n             $JAXCI_PYTHON -m uv pip install --pre libtpu -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n           elif [[ \"${{ inputs.libtpu-version-type }}\" == \"pypi_latest\" ]]; then\n             echo \"Using latest libtpu from PyPI\"\n-            # Set JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI to \"tpu_pypi\". The `run_pytest_tpu.sh`\n-            # script will install the latest libtpu wheel from PyPI.\n-            echo \"JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI=tpu_pypi\" >> $GITHUB_ENV\n+            # Set JAXCI_JAX_PYPI_EXTRAS to \"tpu\". The `run_pytest_tpu.sh` script will install the\n+            # latest libtpu wheel from PyPI.\n+            echo \"JAXCI_JAX_PYPI_EXTRAS=tpu\" >> $GITHUB_ENV\n           elif [[ \"${{ inputs.libtpu-version-type }}\" == \"oldest_supported_libtpu\" ]]; then\n             echo \"Using oldest supported libtpu\"\n             $JAXCI_PYTHON -m uv pip install --pre libtpu-nightly==0.1.dev${{ env.LIBTPU_OLDEST_VERSION_DATE }} \\\ndiff --git a/.github/workflows/wheel_tests_continuous.yml b/.github/workflows/wheel_tests_continuous.yml\nindex 207075fd0340..91662ff51f3e 100644\n--- a/.github/workflows/wheel_tests_continuous.yml\n+++ b/.github/workflows/wheel_tests_continuous.yml\n@@ -117,25 +117,31 @@ jobs:\n           # See exlusions for what is fully tested\n           runner: [\"linux-x86-g2-48-l4-4gpu\", \"linux-x86-a3-8g-h100-8gpu\", \"linux-x86-a4-224-b200-1gpu\"]\n           python: [\"3.10\",]\n-          cuda: [\"12.1\", \"12.8\"]\n+          cuda: [\n+            {version: \"12.1\", use-nvidia-pip-wheels: false},\n+            {version: \"12.8\", use-nvidia-pip-wheels: true},\n+            ]\n           enable-x64: [1, 0]\n           exclude:\n             # H100 runs only a single config, CUDA 12.8 Enable x64 1\n             - runner: \"linux-x86-a3-8g-h100-8gpu\"\n-              cuda: \"12.1\"\n+              cuda:\n+                version: \"12.1\"\n             - runner: \"linux-x86-a3-8g-h100-8gpu\"\n               enable-x64: \"0\"\n             # B200 runs only a single config, CUDA 12.8 Enable x64 1\n             - runner: \"linux-x86-a4-224-b200-1gpu\"\n-              cuda: \"12.1\"\n+              cuda:\n+                version: \"12.1\"\n             - runner: \"linux-x86-a4-224-b200-1gpu\"\n               enable-x64: \"0\"\n \n-    name: \"Pytest CUDA (JAX artifacts version =  ${{ format('{0}', 'head') }})\"\n+    name: \"Pytest CUDA (JAX artifacts version =  ${{ format('{0}', 'head') }}, CUDA Pip packages = ${{ matrix.cuda.use-nvidia-pip-wheels }})\"\n     with:\n       runner: ${{ matrix.runner }}\n       python: ${{ matrix.python }}\n-      cuda:  ${{ matrix.cuda }}\n+      cuda-version:  ${{ matrix.cuda.version }}\n+      use-nvidia-pip-wheels: ${{ matrix.cuda.use-nvidia-pip-wheels }}\n       enable-x64:  ${{ matrix.enable-x64 }}\n       # GCS upload URI is the same for both artifact build jobs\n       gcs_download_uri: ${{ needs.build-jaxlib-artifact.outputs.gcs_upload_uri }}\ndiff --git a/.github/workflows/wheel_tests_nightly_release.yml b/.github/workflows/wheel_tests_nightly_release.yml\nindex 3e616a894d13..7bad41647e6b 100644\n--- a/.github/workflows/wheel_tests_nightly_release.yml\n+++ b/.github/workflows/wheel_tests_nightly_release.yml\n@@ -66,13 +66,17 @@ jobs:\n           # that build the wheels.\n           runner: [\"linux-x86-g2-48-l4-4gpu\"]\n           python: [\"3.10\",\"3.11\", \"3.12\", \"3.13\", \"3.13-nogil\"]\n-          cuda: [\"12.1\", \"12.8\"]\n+          cuda: [\n+            {cuda-version: \"12.1\", use-nvidia-pip-wheels: false},\n+            {cuda-version: \"12.8\", use-nvidia-pip-wheels: true}\n+          ]\n           enable-x64: [0]\n-    name: \"Pytest CUDA (JAX artifacts version = ${{ startsWith(github.ref_name, 'release/') && 'latest release' || 'nightly' }})\"\n+    name: \"Pytest CUDA (JAX artifacts version = ${{ startsWith(github.ref_name, 'release/') && 'latest release' || 'nightly' }}, CUDA Pip packages = ${{ matrix.cuda.use-nvidia-pip-wheels }})\"\n     with:\n       runner: ${{ matrix.runner }}\n       python: ${{ matrix.python }}\n-      cuda:  ${{ matrix.cuda }}\n+      cuda-version:  ${{ matrix.cuda.cuda-version }}\n+      use-nvidia-pip-wheels: ${{ matrix.cuda.use-nvidia-pip-wheels }}\n       enable-x64:  ${{ matrix.enable-x64 }}\n       download-jax-only-from-gcs: ${{inputs.download-jax-only-from-gcs}}\n       gcs_download_uri: ${{inputs.gcs_download_uri}}\ndiff --git a/ci/envs/README.md b/ci/envs/README.md\nindex 6b5dc554d824..cf7a0c12fc9f 100644\n--- a/ci/envs/README.md\n+++ b/ci/envs/README.md\n@@ -21,7 +21,7 @@ Name                                        | Default Value\n `JAXCI_ENABLE_X64`                          | 0                                        | By default, JAX enforces single-precision numbers to mitigate the Numpy APIs tendency to aggressively promote operands to `double`. When set to 1, the tests will use double-precision numbers.                                                                                                                                                             | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_ENABLE_X64&type=code)\n `JAXCI_TPU_CORES`                           | Unset                                    | Sets the number of TPU cores for the TPU machine type. Values are set in the workflow files.                                                                                                                                                                                                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_TPU_CORES&type=code)\n `JAXCI_RUN_FULL_TPU_TEST_SUITE`             | 0                                        | When set to 1, the full TPU test suite is run. Otherwise, a subset of tests is run.                                                                                                                                                                                                                                                                          | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_RUN_FULL_TPU_TEST_SUITE&type=code)\n-`JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI` | Unset                                    | Used to control the installation of JAX [extras](https://github.com/jax-ml/jax/blob/7e42539653d33ec995487b683794c0bc86f7199b/setup.py#L64) from PyPI. See [ci/utilities/install_wheels_locally.sh](https://github.com/jax-ml/jax/blob/main/ci/utilities/install_wheels_locally.sh) for the list of valid values and their behavior.                                                                                                                    | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI&type=code)\n+`JAXCI_JAX_PYPI_EXTRAS` | Unset                                    | Used to control the installation of JAX extras from PyPI. See JAX's [setup.py](https://github.com/jax-ml/jax/blob/c9934912885bb7c4b72c5a9271598235a6789a81/setup.py#L71) for the list of valid values.                                                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_JAX_PYPI_EXTRAS&type=code)\n \n ## Docker Specific Environment Variables\n \ndiff --git a/ci/envs/default.env b/ci/envs/default.env\nindex 774464724646..09594af89cbe 100644\n--- a/ci/envs/default.env\n+++ b/ci/envs/default.env\n@@ -58,7 +58,7 @@ export JAXCI_ENABLE_X64=${JAXCI_ENABLE_X64:-0}\n # Sets the number of TPU cores for the TPU machine type.\n export JAXCI_TPU_CORES=${JAXCI_TPU_CORES:-}\n \n-# JAXCI_PYTHON points to the Python binary on the system that should be used \n+# JAXCI_PYTHON points to the Python binary on the system that should be used\n # for installing the JAX wheels on the system and running Pytest scripts.\n export JAXCI_PYTHON=${JAXCI_PYTHON:-python${JAXCI_HERMETIC_PYTHON_VERSION}}\n \n@@ -66,5 +66,5 @@ export JAXCI_PYTHON=${JAXCI_PYTHON:-python${JAXCI_HERMETIC_PYTHON_VERSION}}\n # is run.\n export JAXCI_RUN_FULL_TPU_TEST_SUITE=${JAXCI_RUN_FULL_TPU_TEST_SUITE:-0}\n \n-# Controls which additional extras to install from PyPI.\n-export JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI=${JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI:-\"\"}\n\\ No newline at end of file\n+# Controls which additional extras for JAX to install from PyPI.\n+export JAXCI_JAX_PYPI_EXTRAS=${JAXCI_JAX_PYPI_EXTRAS:-\"\"}\n\\ No newline at end of file\ndiff --git a/ci/utilities/install_wheels_locally.sh b/ci/utilities/install_wheels_locally.sh\nindex 53f070d1e0e6..b1472d765c08 100644\n--- a/ci/utilities/install_wheels_locally.sh\n+++ b/ci/utilities/install_wheels_locally.sh\n@@ -22,15 +22,11 @@ WHEELS=( $(/usr/bin/find \"$JAXCI_OUTPUT_DIR/\" -type f \\(  -name \"*jax*py3*\" -o -\n \n for i in \"${!WHEELS[@]}\"; do\n   if [[ \"${WHEELS[$i]}\" == *jax*py3*none*any.whl ]]; then\n-    if [[ \"$JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI\" == \"tpu_pypi\" ]]; then\n-      # Append [tpu] to the jax wheel name to download the latest libtpu wheel\n-      # from PyPI.\n-      WHEELS[$i]=\"${WHEELS[$i]}[tpu]\"\n-    elif [[ \"$JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI\" == \"jax_cuda_pypi\" ]]; then\n-      # Append [cuda12-local] to the jax wheel name to download the latest\n-      # release of JAX's CUDA plugin and PJRT packages from PyPI. This is used\n-      # when running CUDA tests for a \"jax\" only release.\n-      WHEELS[$i]=\"${WHEELS[$i]}[cuda12-local]\"\n+    # Apppend an extra to the end of the JAX wheel path to install those\n+    # packages as well from PyPI. E.g. jax[tpu] will install the libtpu package\n+    # from PyPI. See ci/envs/README.md for more details.\n+    if [[ -n \"$JAXCI_JAX_PYPI_EXTRAS\" ]]; then\n+      WHEELS[$i]=\"${WHEELS[$i]}[$JAXCI_JAX_PYPI_EXTRAS]\"\n     fi\n   fi\n done\n\n```",
        "from_id": [
            "nitins17",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28899 from jenriver:absl_logging_fix\n\nPiperOrigin-RevId: 762099728",
        "output": "```diff\nCommit: e71d5d5051e511bc09e1d5a08d30c298420aa169\nDate: 2025-05-22T20:23:06Z\nURL: https://github.com/jax-ml/jax/commit/e71d5d5051e511bc09e1d5a08d30c298420aa169\nFiles changed: 3\nAdditions: +16, Deletions: -18\ndiff --git a/docs/export/export.md b/docs/export/export.md\nindex 63c0db14f905..95e47385997c 100644\n--- a/docs/export/export.md\n+++ b/docs/export/export.md\n@@ -710,10 +710,7 @@ total 32\n -rw-rw-r--@ 1 necula  wheel  2333 Jun 19 11:04 jax_ir3_jit_my_fun_export.mlir\n ```\n \n-Inside Google, you can turn on logging by using the `--vmodule` argument to\n-specify the logging levels for different modules,\n-e.g., `--vmodule=_export=3`.\n-\n+Set [`JAX_DEBUG_LOG_MODULES=jax._src.export`](https://docs.jax.dev/en/latest/config_options.html#jax_debug_log_modules) to enable extra debugging logging.\n \n (export_ensuring_compat)=\n ### Ensuring forward and backward compatibility\ndiff --git a/jax/_src/compiler.py b/jax/_src/compiler.py\nindex e1b8e7c35697..343f747efbd7 100644\n--- a/jax/_src/compiler.py\n+++ b/jax/_src/compiler.py\n@@ -241,7 +241,7 @@ def get_compile_options(\n   else:\n     compile_options.profile_version = _NO_PROFILE_DONT_RETRIEVE\n     if backend is None:\n-      logging.info(\"get_compile_options: no backend supplied; \"\n+      logger.info(\"get_compile_options: no backend supplied; \"\n                    \"disabling XLA-AutoFDO profile\")\n     else:\n       fdo_profile_version = get_latest_profile_version(backend)\n@@ -369,7 +369,7 @@ def compile_or_get_cached(\n   module_name = ir.StringAttr(sym_name).value\n \n   if dumped_to := mlir.dump_module_to_file(computation, \"compile\"):\n-    logging.info(\"Dumped the module to %s.\", dumped_to)\n+    logger.info(\"Dumped the module to %s.\", dumped_to)\n \n   is_multi_process = (\n       len({device.process_index for device in devices.flatten()}) > 1\n@@ -514,7 +514,7 @@ def _resolve_compilation_strategy(\n     # The compilation cache is enabled and AutoPGLE is enabled/expected\n     if _is_executable_in_cache(backend, pgle_optimized_cache_key):\n       if config.compilation_cache_expect_pgle.value:\n-        logging.info(f\"PGLE-optimized {module_name} loaded from compilation cache\")\n+        logger.info(f\"PGLE-optimized {module_name} loaded from compilation cache\")\n       # No need to record N profiles in this case\n       if pgle_profiler is not None:\n         pgle_profiler.disable()\ndiff --git a/jax/_src/export/_export.py b/jax/_src/export/_export.py\nindex c0ca1e108590..189818541a2c 100644\n--- a/jax/_src/export/_export.py\n+++ b/jax/_src/export/_export.py\n@@ -25,7 +25,7 @@\n import re\n from typing import Any, Protocol, TypeVar, Union, cast\n \n-from absl import logging\n+import logging\n import numpy as np\n \n import jax\n@@ -55,6 +55,8 @@\n \n from jax._src.export import shape_poly\n \n+logger = logging.getLogger(__name__)\n+\n map = util.safe_map\n zip = util.safe_zip\n \n@@ -704,16 +706,15 @@ def _export_lowered(\n     out_avals_flat = lowered.compile_args[\"out_avals\"]  # type: ignore\n \n   # Log and then check the module.\n-  if logging.vlog_is_on(3):\n-    logmsg = (f\"fun_name={fun_name} version={version} \"\n-              f\"lowering_platforms={lowering._platforms} \"  # type: ignore[unused-ignore,attribute-error]\n-              f\"disabled_checks={disabled_checks}\")\n-    logging.info(\"Exported JAX function: %s\\n\", logmsg)\n-    logging.info(mlir.dump_module_message(mlir_module, \"export\"))\n-    logging.info(\n-        \"Size of mlir_module_serialized: %d byte\",\n-        len(mlir_module_serialized),\n-    )\n+  logmsg = (f\"fun_name={fun_name} version={version} \"\n+            f\"lowering_platforms={lowering._platforms} \"  # type: ignore[unused-ignore,attribute-error]\n+            f\"disabled_checks={disabled_checks}\")\n+  logger.debug(\"Exported JAX function: %s\\n\", logmsg)\n+  logger.debug(mlir.dump_module_message(mlir_module, \"export\"))\n+  logger.debug(\n+      \"Size of mlir_module_serialized: %d byte\",\n+      len(mlir_module_serialized),\n+  )\n \n   _check_module(mlir_module,\n                 disabled_checks=disabled_checks,\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28946 from jakevdp:random-mode\n\nPiperOrigin-RevId: 762092464",
        "output": "```diff\nCommit: fdf6e1fac02824109994cdb455c1037979d062e5\nDate: 2025-05-22T20:05:44Z\nURL: https://github.com/jax-ml/jax/commit/fdf6e1fac02824109994cdb455c1037979d062e5\nFiles changed: 2\nAdditions: +32, Deletions: -10\ndiff --git a/jax/_src/random.py b/jax/_src/random.py\nindex 6f139dd9665c..60dad3a82021 100644\n--- a/jax/_src/random.py\n+++ b/jax/_src/random.py\n@@ -633,7 +633,8 @@ def choice(key: ArrayLike,\n            shape: Shape = (),\n            replace: bool = True,\n            p: RealArray | None = None,\n-           axis: int = 0) -> Array:\n+           axis: int = 0,\n+           mode: str | None = None) -> Array:\n   \"\"\"Generates a random sample from a given array.\n \n   .. warning::\n@@ -656,6 +657,12 @@ def choice(key: ArrayLike,\n       entries in a.\n     axis: int, optional. The axis along which the selection is performed.\n       The default, 0, selects by row.\n+    mode: optional, \"high\" or \"low\" for how many bits to use in the gumbel sampler\n+      when `p is None` and `replace = False`. The default is determined by the\n+      ``use_high_dynamic_range_gumbel`` config, which defaults to \"low\". With mode=\"low\",\n+      in float32 sampling will be biased for choices with probability less than about\n+      1E-7; with mode=\"high\" this limit is pushed down to about 1E-14. mode=\"high\"\n+      approximately doubles the cost of sampling.\n \n   Returns:\n     An array of shape `shape` containing samples from `a`.\n@@ -701,7 +708,7 @@ def choice(key: ArrayLike,\n       ind = jnp.searchsorted(p_cuml, r).astype(int)\n     else:\n       # Gumbel top-k trick: https://timvieira.github.io/blog/post/2019/09/16/algorithms-for-sampling-without-replacement/\n-      g = gumbel(key, (n_inputs,), dtype=p_arr.dtype) + jnp.log(p_arr)\n+      g = gumbel(key, (n_inputs,), dtype=p_arr.dtype, mode=mode) + jnp.log(p_arr)\n       ind = lax.top_k(g, k=n_draws)[1].astype(int)\n     result = ind if arr.ndim == 0 else jnp.take(arr, ind, axis)\n \n@@ -940,7 +947,8 @@ def bernoulli(key: ArrayLike,\n     mode: optional, \"high\" or \"low\" for how many bits to use when sampling.\n       default='low'. Set to \"high\" for correct sampling at small values of\n       `p`. When sampling in float32, bernoulli samples with mode='low' produce\n-      incorrect results for p < ~1E-7.\n+      incorrect results for p < ~1E-7. mode=\"high\" approximately doubles the\n+      cost of sampling.\n \n   Returns:\n     A random array with boolean dtype and shape given by ``shape`` if ``shape``\n@@ -1544,7 +1552,7 @@ def poisson(key: ArrayLike,\n def gumbel(key: ArrayLike,\n            shape: Shape = (),\n            dtype: DTypeLikeFloat = float,\n-           mode: str | None =None) -> Array:\n+           mode: str | None = None) -> Array:\n   \"\"\"Sample Gumbel random values with given shape and float dtype.\n \n   The values are distributed according to the probability density function:\n@@ -1559,6 +1567,11 @@ def gumbel(key: ArrayLike,\n     dtype: optional, a float dtype for the returned values (default float64 if\n       jax_enable_x64 is true, otherwise float32).\n     mode: optional, \"high\" or \"low\" for how many bits to use when sampling.\n+      The default is determined by the ``use_high_dynamic_range_gumbel`` config,\n+      which defaults to \"low\". When drawing float32 samples, with mode=\"low\" the\n+      uniform resolution is such that the largest possible gumbel logit is ~16;\n+      with mode=\"high\" this is increased to ~32, at approximately double the\n+      computational cost.\n \n   Returns:\n     A random array with the specified shape and dtype.\n@@ -1599,6 +1612,7 @@ def categorical(\n   axis: int = -1,\n   shape: Shape | None = None,\n   replace: bool = True,\n+  mode: str | None = None,\n ) -> Array:\n   \"\"\"Sample random values from categorical distributions.\n \n@@ -1615,6 +1629,12 @@ def categorical(\n       The default (None) produces a result shape equal to ``np.delete(logits.shape, axis)``.\n     replace: If True (default), perform sampling with replacement. If False, perform\n       sampling without replacement.\n+    mode: optional, \"high\" or \"low\" for how many bits to use in the gumbel sampler.\n+      The default is determined by the ``use_high_dynamic_range_gumbel`` config,\n+      which defaults to \"low\". With mode=\"low\", in float32 sampling will be biased\n+      for events with probability less than about 1E-7; with mode=\"high\" this limit\n+      is pushed down to about 1E-14. mode=\"high\" approximately doubles the cost of\n+      sampling.\n \n   Returns:\n     A random array with int dtype and shape given by ``shape`` if ``shape``\n@@ -1644,11 +1664,11 @@ def categorical(\n     logits_shape = list(shape[len(shape) - len(batch_shape):])\n     logits_shape.insert(axis % len(logits_arr.shape), logits_arr.shape[axis])\n     return jnp.argmax(\n-        gumbel(key, (*shape_prefix, *logits_shape), logits_arr.dtype) +\n+        gumbel(key, (*shape_prefix, *logits_shape), logits_arr.dtype, mode=mode) +\n         lax.expand_dims(logits_arr, tuple(range(len(shape_prefix)))),\n         axis=axis)\n   else:\n-    logits_arr += gumbel(key, logits_arr.shape, logits_arr.dtype)\n+    logits_arr += gumbel(key, logits_arr.shape, logits_arr.dtype, mode=mode)\n     k = math.prod(shape_prefix)\n     if k > logits_arr.shape[axis]:\n       raise ValueError(\ndiff --git a/tests/random_lax_test.py b/tests/random_lax_test.py\nindex f87b079b759c..9fe4d2ecbda3 100644\n--- a/tests/random_lax_test.py\n+++ b/tests/random_lax_test.py\n@@ -286,8 +286,9 @@ def testTruncatedNormal(self, dtype):\n     ],\n     dtype=jtu.dtypes.floating + jtu.dtypes.integer,\n     weighted=[True, False],\n+    mode=[None, 'low', 'high']\n   )\n-  def testChoice(self, dtype, input_range_or_shape, shape, replace, weighted, axis):\n+  def testChoice(self, dtype, input_range_or_shape, shape, replace, weighted, axis, mode):\n     # This is the function API that we test against (note that self.rng().choice differs)\n     np_choice = np.random.default_rng(0).choice\n     p_dtype = dtypes.to_inexact_dtype(dtype)\n@@ -303,7 +304,7 @@ def testChoice(self, dtype, input_range_or_shape, shape, replace, weighted, axis\n       p /= p.sum()\n     else:\n       p = None\n-    rand = lambda key, x: random.choice(key, x, shape, replace, p, axis)\n+    rand = lambda key, x: random.choice(key, x, shape, replace, p, axis, mode=mode)\n     sample = rand(key(), x)\n     if not is_range:\n       self.assertEqual(dtype, sample.dtype)\n@@ -397,15 +398,16 @@ def testBernoulli(self, p, dtype, mode):\n       ]\n     ],\n     sample_shape=[(10000,), (5000, 2)],\n+    mode=[None, 'low', 'high'],\n     dtype=jtu.dtypes.floating,\n   )\n-  def testCategorical(self, p, axis, dtype, sample_shape):\n+  def testCategorical(self, p, axis, dtype, sample_shape, mode):\n     key = lambda: self.make_key(0)\n     p = np.array(p, dtype=dtype)\n     logits = np.log(p) - 42 # test unnormalized\n     out_shape = tuple(np.delete(logits.shape, axis))\n     shape = sample_shape + out_shape\n-    rand = partial(random.categorical, shape=shape, axis=axis)\n+    rand = partial(random.categorical, shape=shape, axis=axis, mode=mode)\n     crand = jax.jit(rand)\n \n     uncompiled_samples = rand(key(), logits)\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Use the default python logging instead of absl log.\n\nUpdate use cases in export / compiler as well as the documentation.",
        "output": "```diff\nCommit: 7cf4f35442743b01f4685ca906d282586428b3d0\nDate: 2025-05-22T19:06:57Z\nURL: https://github.com/jax-ml/jax/commit/7cf4f35442743b01f4685ca906d282586428b3d0\nFiles changed: 3\nAdditions: +16, Deletions: -18\ndiff --git a/docs/export/export.md b/docs/export/export.md\nindex 63c0db14f905..95e47385997c 100644\n--- a/docs/export/export.md\n+++ b/docs/export/export.md\n@@ -710,10 +710,7 @@ total 32\n -rw-rw-r--@ 1 necula  wheel  2333 Jun 19 11:04 jax_ir3_jit_my_fun_export.mlir\n ```\n \n-Inside Google, you can turn on logging by using the `--vmodule` argument to\n-specify the logging levels for different modules,\n-e.g., `--vmodule=_export=3`.\n-\n+Set [`JAX_DEBUG_LOG_MODULES=jax._src.export`](https://docs.jax.dev/en/latest/config_options.html#jax_debug_log_modules) to enable extra debugging logging.\n \n (export_ensuring_compat)=\n ### Ensuring forward and backward compatibility\ndiff --git a/jax/_src/compiler.py b/jax/_src/compiler.py\nindex 04f993fed799..cbde6fdb3366 100644\n--- a/jax/_src/compiler.py\n+++ b/jax/_src/compiler.py\n@@ -242,7 +242,7 @@ def get_compile_options(\n   else:\n     compile_options.profile_version = _NO_PROFILE_DONT_RETRIEVE\n     if backend is None:\n-      logging.info(\"get_compile_options: no backend supplied; \"\n+      logger.info(\"get_compile_options: no backend supplied; \"\n                    \"disabling XLA-AutoFDO profile\")\n     else:\n       fdo_profile_version = get_latest_profile_version(backend)\n@@ -376,7 +376,7 @@ def compile_or_get_cached(\n   module_name = ir.StringAttr(sym_name).value\n \n   if dumped_to := mlir.dump_module_to_file(computation, \"compile\"):\n-    logging.info(\"Dumped the module to %s.\", dumped_to)\n+    logger.info(\"Dumped the module to %s.\", dumped_to)\n \n   is_multi_process = (\n       len({device.process_index for device in devices.flatten()}) > 1\n@@ -521,7 +521,7 @@ def _resolve_compilation_strategy(\n     # The compilation cache is enabled and AutoPGLE is enabled/expected\n     if _is_executable_in_cache(backend, pgle_optimized_cache_key):\n       if config.compilation_cache_expect_pgle.value:\n-        logging.info(f\"PGLE-optimized {module_name} loaded from compilation cache\")\n+        logger.info(f\"PGLE-optimized {module_name} loaded from compilation cache\")\n       # No need to record N profiles in this case\n       if pgle_profiler is not None:\n         pgle_profiler.disable()\ndiff --git a/jax/_src/export/_export.py b/jax/_src/export/_export.py\nindex c0ca1e108590..189818541a2c 100644\n--- a/jax/_src/export/_export.py\n+++ b/jax/_src/export/_export.py\n@@ -25,7 +25,7 @@\n import re\n from typing import Any, Protocol, TypeVar, Union, cast\n \n-from absl import logging\n+import logging\n import numpy as np\n \n import jax\n@@ -55,6 +55,8 @@\n \n from jax._src.export import shape_poly\n \n+logger = logging.getLogger(__name__)\n+\n map = util.safe_map\n zip = util.safe_zip\n \n@@ -704,16 +706,15 @@ def _export_lowered(\n     out_avals_flat = lowered.compile_args[\"out_avals\"]  # type: ignore\n \n   # Log and then check the module.\n-  if logging.vlog_is_on(3):\n-    logmsg = (f\"fun_name={fun_name} version={version} \"\n-              f\"lowering_platforms={lowering._platforms} \"  # type: ignore[unused-ignore,attribute-error]\n-              f\"disabled_checks={disabled_checks}\")\n-    logging.info(\"Exported JAX function: %s\\n\", logmsg)\n-    logging.info(mlir.dump_module_message(mlir_module, \"export\"))\n-    logging.info(\n-        \"Size of mlir_module_serialized: %d byte\",\n-        len(mlir_module_serialized),\n-    )\n+  logmsg = (f\"fun_name={fun_name} version={version} \"\n+            f\"lowering_platforms={lowering._platforms} \"  # type: ignore[unused-ignore,attribute-error]\n+            f\"disabled_checks={disabled_checks}\")\n+  logger.debug(\"Exported JAX function: %s\\n\", logmsg)\n+  logger.debug(mlir.dump_module_message(mlir_module, \"export\"))\n+  logger.debug(\n+      \"Size of mlir_module_serialized: %d byte\",\n+      len(mlir_module_serialized),\n+  )\n \n   _check_module(mlir_module,\n                 disabled_checks=disabled_checks,\n\n```",
        "from_id": [
            "jenriver"
        ]
    },
    {
        "text_input": "Add support for non-power-of-2 head size in flash attention\nIntroduce checks on sequences being divisible by block sizes to address https://github.com/jax-ml/jax/issues/27224\n\nPiperOrigin-RevId: 762051831",
        "output": "```diff\nCommit: 1aaec81f22a0dde3f7da56bc54bdd71f212076c9\nDate: 2025-05-22T18:36:42Z\nURL: https://github.com/jax-ml/jax/commit/1aaec81f22a0dde3f7da56bc54bdd71f212076c9\nFiles changed: 2\nAdditions: +104, Deletions: -84\ndiff --git a/jax/experimental/pallas/ops/gpu/attention.py b/jax/experimental/pallas/ops/gpu/attention.py\nindex 8b83d24ea199..ccb3ae8fd3b7 100644\n--- a/jax/experimental/pallas/ops/gpu/attention.py\n+++ b/jax/experimental/pallas/ops/gpu/attention.py\n@@ -86,28 +86,29 @@ def mha_forward_kernel(\n     segment_ids_ref: jax.Array | None,  # segment_id arrays\n     o_ref: Any,  # Output\n     *residual_refs: Any,  # Residual outputs\n-    num_heads: int,\n     sm_scale: float,\n     causal: bool,\n     block_q: int,\n-    block_d: int,\n     block_k: int,\n+    head_dim: int,\n ):\n   seq_len = k_ref.shape[0]\n   start_q = pl.program_id(0)\n+  head_dim_padded = q_ref.shape[-1]\n \n   # o is the buffer where we accumulate the output on sram.\n   # m_i and l_i (see FlashAttention paper) are updated during the k,v loop.\n   m_i = jnp.zeros(block_q, dtype=jnp.float32) - float('inf')\n   l_i = jnp.zeros(block_q, dtype=jnp.float32)\n   # acc is the buffer where we accumulate the output on sram.\n-  o = jnp.zeros((block_q, block_d), dtype=jnp.float32)\n+  o = jnp.zeros((block_q, head_dim_padded), dtype=jnp.float32)\n \n   # Load q: it will stay in L1 throughout. Indices form a matrix because we\n   # read, compute, and write all in 2d chunks. 1 element ~= 1 CUDA thread index.\n-  # q tile has shape [block_q, block_d], block_d == head_dim.\n+  # q tile has shape [block_q, head_dim_padded], head_dim_padded >= head_dim.\n   curr_q_slice = pl.dslice(start_q * block_q, block_q)\n-  q = q_ref[...]\n+  head_mask = (jnp.arange(head_dim_padded) < head_dim)[None, :]\n+  q = pl.load(q_ref, (slice(None), slice(None)), mask=head_mask, other=0.0)\n   q_segment_ids = (\n       None\n       if segment_ids_ref is None\n@@ -121,7 +122,7 @@ def body(start_k, carry):\n     o_prev, m_prev, l_prev = carry\n     curr_k_slice = pl.dslice(start_k * block_k, block_k)\n \n-    k = pl.load(k_ref, (curr_k_slice, slice(None)))\n+    k = pl.load(k_ref, (curr_k_slice, slice(None)), mask=head_mask, other=0.0)\n     qk = pl.dot(q, k.T)   # [block_q, block_k]\n \n     # Scale logits to convert from base-2 to the natural log domain.\n@@ -161,7 +162,7 @@ def body(start_k, carry):\n     l_curr = s_curr.sum(axis=-1)\n     l_next = l_prev_corr + l_curr\n     o_prev_corr = correction[:, None] * o_prev\n-    v = pl.load(v_ref, (curr_k_slice, pl.dslice(block_d)))\n+    v = pl.load(v_ref, (curr_k_slice, slice(None)), mask=head_mask)\n     o_curr = pl.dot(s_curr.astype(v.dtype), v)\n \n     o_next = o_prev_corr + o_curr\n@@ -182,7 +183,8 @@ def body(start_k, carry):\n     lse_ref = residual_refs[0]\n     lse_ref[...] = m_i + jnp.log2(l_i)\n   # Write output to dram.\n-  o_ref[...] = o.astype(o_ref.dtype)\n+  pl.store(o_ref, (slice(None), slice(o.shape[-1])), o.astype(o_ref.dtype),\n+           mask=head_mask)\n \n def segment_mask(\n     q_segment_ids: jax.Array,\n@@ -235,6 +237,17 @@ def mha(\n   kv_seq_len = k.shape[1]\n   block_q = min(block_sizes.block_q, q_seq_len)\n   block_k = min(block_sizes.block_k, kv_seq_len)\n+  head_dim_padded = pl.next_power_of_2(head_dim)\n+  if (q.shape[-1] != k.shape[-1]) or (q.shape[-1] != v.shape[-1]):\n+    raise ValueError(\n+        f\"This kernel expects q, k, and v to have the same head dimension, but\"\n+        f\" found {q.shape=}, {k.shape=}, {v.shape=}.\"\n+    )\n+  if q_seq_len % block_q != 0:\n+    raise ValueError(f\"{q_seq_len=} must be a multiple of {block_q=}\")\n+  if kv_seq_len % block_k != 0:\n+    raise ValueError(f\"{kv_seq_len=} must be a multiple of {block_k=}\")\n+\n   # Heuristics.\n   grid_ = grid\n   if grid_ is None:\n@@ -243,21 +256,17 @@ def mha(\n   num_warps_ = num_warps\n   if num_warps_ is None:\n     num_warps_ = 4 if head_dim <= 64 else 8\n-  kernel = functools.partial(mha_forward_kernel, num_heads=num_heads,\n-                             sm_scale=sm_scale, block_q=block_q,\n-                             block_k=block_k, block_d=head_dim,\n-                             causal=causal)\n+  kernel = functools.partial(mha_forward_kernel, sm_scale=sm_scale,\n+                             block_q=block_q, block_k=block_k,\n+                             head_dim=head_dim, causal=causal)\n \n   in_specs = [\n-      pl.BlockSpec(\n-          (None, block_q, None, head_dim), lambda i, j, k: (j, i, k, 0)\n-      ),\n-      pl.BlockSpec(\n-          (None, kv_seq_len, None, head_dim), lambda _, j, k: (j, 0, k, 0)\n-      ),\n-      pl.BlockSpec(\n-          (None, kv_seq_len, None, head_dim), lambda _, j, k: (j, 0, k, 0)\n-      ),\n+      pl.BlockSpec((None, block_q, None, head_dim_padded),\n+                   lambda i, j, k: (j, i, k, 0)),\n+      pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n+                   lambda _, j, k: (j, 0, k, 0)),\n+      pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n+                   lambda _, j, k: (j, 0, k, 0)),\n   ]\n   in_specs.append(\n       None  # type: ignore[arg-type]\n@@ -270,7 +279,7 @@ def mha(\n       grid=grid_,\n       in_specs=in_specs,\n       out_specs=pl.BlockSpec(\n-          (None, block_q, None, head_dim), lambda i, j, k: (j, i, k, 0)\n+          (None, block_q, None, head_dim_padded), lambda i, j, k: (j, i, k, 0)\n       ),\n       compiler_params=plgpu.TritonCompilerParams(\n           num_warps=num_warps_, num_stages=num_stages),\n@@ -301,6 +310,17 @@ def _mha_forward(\n   kv_seq_len = k.shape[1]\n   block_q = min(block_sizes.block_q, q_seq_len)\n   block_k = min(block_sizes.block_k, kv_seq_len)\n+  if (q.shape[-1] != k.shape[-1]) or (q.shape[-1] != v.shape[-1]):\n+    raise ValueError(\n+        f\"This kernel expects q, k, and v to have the same head dimension, but\"\n+        f\" found {q.shape=}, {k.shape=}, {v.shape=}.\"\n+    )\n+  if q_seq_len % block_q != 0:\n+    raise ValueError(f\"{q_seq_len=} must be a multiple of {block_q=}\")\n+  if kv_seq_len % block_k != 0:\n+    raise ValueError(f\"{kv_seq_len=} must be a multiple of {block_k=}\")\n+  head_dim_padded = pl.next_power_of_2(head_dim)\n+\n   # Heuristics.\n   grid_ = grid\n   if grid_ is None:\n@@ -309,9 +329,9 @@ def _mha_forward(\n   num_warps_ = num_warps\n   if num_warps_ is None:\n     num_warps_ = 4 if head_dim <= 64 else 8\n-  kernel = functools.partial(mha_forward_kernel, num_heads=num_heads,\n-                             sm_scale=sm_scale, causal=causal, block_q=block_q,\n-                             block_k=block_k, block_d=head_dim)\n+  kernel = functools.partial(mha_forward_kernel, sm_scale=sm_scale,\n+                             causal=causal, block_q=block_q, block_k=block_k,\n+                             head_dim=head_dim)\n   out_shape = [\n       jax.ShapeDtypeStruct(shape=q.shape, dtype=q.dtype),  # out\n       jax.ShapeDtypeStruct(\n@@ -319,15 +339,12 @@ def _mha_forward(\n       ),\n   ]\n   in_specs = [\n-      pl.BlockSpec(\n-          (None, block_q, None, head_dim), lambda i, j, k: (j, i, k, 0)\n-      ),\n-      pl.BlockSpec(\n-          (None, kv_seq_len, None, head_dim), lambda _, j, k: (j, 0, k, 0)\n-      ),\n-      pl.BlockSpec(\n-          (None, kv_seq_len, None, head_dim), lambda _, j, k: (j, 0, k, 0)\n-      ),\n+      pl.BlockSpec((None, block_q, None, head_dim_padded),\n+                   lambda i, j, k: (j, i, k, 0)),\n+      pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n+                   lambda _, j, k: (j, 0, k, 0)),\n+      pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n+                   lambda _, j, k: (j, 0, k, 0)),\n   ]\n   in_specs.append(\n       None  # type: ignore[arg-type]\n@@ -339,9 +356,8 @@ def _mha_forward(\n       grid=grid_,\n       in_specs=in_specs,\n       out_specs=[\n-          pl.BlockSpec(\n-              (None, block_q, None, head_dim), lambda i, j, k: (j, i, k, 0)\n-          ),\n+          pl.BlockSpec((None, block_q, None, head_dim_padded),\n+                       lambda i, j, k: (j, i, k, 0)),\n           pl.BlockSpec((None, None, block_q), lambda i, j, k: (j, k, i)),\n       ],\n       compiler_params=plgpu.TritonCompilerParams(\n@@ -355,10 +371,11 @@ def _mha_forward(\n   return out, (q, k, v, segment_ids, out, lse)\n \n \n-def _preprocess_backward_kernel(out_ref, dout_ref, delta_ref):\n+def _preprocess_backward_kernel(out_ref, dout_ref, delta_ref, head_dim: int):\n   # load\n-  o = out_ref[...].astype(jnp.float32)\n-  do = dout_ref[...].astype(jnp.float32)\n+  head_mask = (jnp.arange(out_ref.shape[-1]) < head_dim)[None, :]\n+  o = pl.load(out_ref, (slice(None), slice(None)), mask=head_mask, other=0.0)\n+  do = pl.load(dout_ref, (slice(None), slice(None)), mask=head_mask, other=0.0)\n   # compute\n   delta = jnp.sum(o * do, axis=1)\n   # write-back\n@@ -368,17 +385,16 @@ def _preprocess_backward_kernel(out_ref, dout_ref, delta_ref):\n def _preprocess_backward(out, do, lse, block_q: int,\n                          debug: bool, interpret: bool):\n   batch_size, seq_len, num_heads, head_dim = out.shape\n+  head_dim_padded = pl.next_power_of_2(head_dim)\n   out_shape = jax.ShapeDtypeStruct(lse.shape, lse.dtype)\n   delta = pl.pallas_call(\n-      _preprocess_backward_kernel,\n+      functools.partial(_preprocess_backward_kernel, head_dim=head_dim),\n       grid=(pl.cdiv(seq_len, block_q), batch_size, num_heads),\n       in_specs=[\n-          pl.BlockSpec(\n-              (None, block_q, None, head_dim), lambda i, j, k: (j, i, k, 0)\n-          ),\n-          pl.BlockSpec(\n-              (None, block_q, None, head_dim), lambda i, j, k: (j, i, k, 0)\n-          ),\n+          pl.BlockSpec((None, block_q, None, head_dim_padded),\n+                       lambda i, j, k: (j, i, k, 0)),\n+          pl.BlockSpec((None, block_q, None, head_dim_padded),\n+                       lambda i, j, k: (j, i, k, 0)),\n       ],\n       out_specs=pl.BlockSpec((None, None, block_q), lambda i, j, k: (j, k, i)),\n       compiler_params=plgpu.TritonCompilerParams(num_warps=4, num_stages=3),\n@@ -414,7 +430,7 @@ def mha_backward_kernel(\n     block_kv_dkv: int,\n     block_q_dq: int,\n     block_kv_dq: int,\n-    block_d: int,\n+    head_dim: int,\n ):\n   del out_ref  # Not needed\n   q_seq_len = q_ref.shape[0]\n@@ -427,11 +443,13 @@ def mha_backward_kernel(\n   start_k = pl.program_id(2)\n   curr_k_slice = pl.dslice(start_k * block_kv_dkv, block_kv_dkv)\n \n-  dv = jnp.zeros([block_kv_dkv, block_d], dtype=jnp.float32)\n-  dk = jnp.zeros([block_kv_dkv, block_d], dtype=jnp.float32)\n+  head_dim_padded = q_ref.shape[-1]\n+  dv = jnp.zeros([block_kv_dkv, head_dim_padded], dtype=jnp.float32)\n+  dk = jnp.zeros([block_kv_dkv, head_dim_padded], dtype=jnp.float32)\n \n-  v = pl.load(v_ref, (curr_k_slice, slice(None)))\n-  k = pl.load(k_ref, (curr_k_slice, slice(None)))\n+  head_mask = (jnp.arange(head_dim_padded) < head_dim)[None, :]\n+  v = pl.load(v_ref, (curr_k_slice, slice(None)), mask=head_mask, other=0.0)\n+  k = pl.load(k_ref, (curr_k_slice, slice(None)), mask=head_mask, other=0.0)\n   span_k = start_k * block_kv_dkv + jnp.arange(block_kv_dkv)\n   kv_segment_ids = (\n       None\n@@ -443,7 +461,7 @@ def inner_loop_dkdv(start_q, carry):\n     dv, dk = carry\n     curr_q_slice = pl.dslice(start_q * block_q_dkv, block_q_dkv)\n \n-    q = pl.load(q_ref, (curr_q_slice, slice(None)))\n+    q = pl.load(q_ref, (curr_q_slice, slice(None)), mask=head_mask, other=0.0)\n     qk = pl.dot(q, k.T)\n     qk_scale = math.log2(math.e)\n     if sm_scale != 1.:\n@@ -466,7 +484,8 @@ def inner_loop_dkdv(start_q, carry):\n \n     lse = pl.load(lse_ref, (curr_q_slice,))\n     di = pl.load(delta_ref, (curr_q_slice,))\n-    do = pl.load(do_scaled_ref, (curr_q_slice, slice(None)))\n+    do = pl.load(do_scaled_ref, (curr_q_slice, slice(None)), mask=head_mask,\n+                 other=0.0)\n \n     p = jnp.exp2(qk - lse[:, None])\n     dv = dv + pl.dot(p.astype(do.dtype).T, do)\n@@ -483,8 +502,10 @@ def inner_loop_dkdv(start_q, carry):\n   dv, dk = lax.fori_loop(\n       lower_bound, pl.cdiv(q_seq_len, block_q_dkv), inner_loop_dkdv, (dv, dk)\n   )\n-  dv_ref[...] = dv.astype(dv_ref.dtype)\n-  dk_ref[...] = dk.astype(dk_ref.dtype)\n+  pl.store(dv_ref, (slice(None), slice(dv.shape[-1])), dv.astype(dv_ref.dtype),\n+           mask=head_mask)\n+  pl.store(dk_ref, (slice(None), slice(dk.shape[-1])), dk.astype(dk_ref.dtype),\n+           mask=head_mask)\n \n   # Scan #2: dQ\n   #   1. Load a block of Q of size (block_q_dq, head_dim) in SMEM.\n@@ -493,22 +514,23 @@ def inner_loop_dkdv(start_q, carry):\n   start_q = pl.program_id(2)\n   curr_q_slice = pl.ds(start_q * block_q_dq, block_q_dq)\n   span_q = start_q * block_q_dq + jnp.arange(block_q_dq)\n-  dq = jnp.zeros([block_q_dq, block_d], dtype=jnp.float32)\n+  dq = jnp.zeros([block_q_dq, head_dim_padded], dtype=jnp.float32)\n \n-  q = pl.load(q_ref, (curr_q_slice, slice(None)))\n+  q = pl.load(q_ref, (curr_q_slice, slice(None)), mask=head_mask, other=0.0)\n   q_segment_ids = (\n       None\n       if segment_ids_ref is None\n       else pl.load(segment_ids_ref, (curr_q_slice,))\n   )\n   lse = pl.load(lse_ref, (curr_q_slice,))\n-  do = pl.load(do_scaled_ref, (curr_q_slice, slice(None)))\n+  do = pl.load(do_scaled_ref, (curr_q_slice, slice(None)), mask=head_mask,\n+               other=0.0)\n   di = pl.load(delta_ref, (curr_q_slice,))\n \n   def inner_loop_dq(start_k, dq):\n     curr_k_slice = pl.dslice(start_k * block_kv_dq, block_kv_dq)\n-    k = pl.load(k_ref, (curr_k_slice, slice(None)))\n-    v = pl.load(v_ref, (curr_k_slice, slice(None)))\n+    k = pl.load(k_ref, (curr_k_slice, slice(None)), mask=head_mask, other=0.0)\n+    v = pl.load(v_ref, (curr_k_slice, slice(None)), mask=head_mask, other=0.0)\n \n     qk = pl.dot(q, k.T)\n     qk_scale = math.log2(math.e)\n@@ -547,7 +569,8 @@ def inner_loop_dq(start_k, dq):\n     upper_bound = pl.cdiv(kv_seq_len, block_kv_dq)\n \n   dq = lax.fori_loop(0, upper_bound, inner_loop_dq, (dq))\n-  dq_ref[...] = dq.astype(dq_ref.dtype)\n+  pl.store(dq_ref, (slice(None), slice(dq.shape[-1])), dq.astype(dq_ref.dtype),\n+           mask=head_mask)\n \n \n def _mha_backward(sm_scale: float, causal: bool, block_sizes: BlockSizes,\n@@ -576,6 +599,7 @@ def _mha_backward(sm_scale: float, causal: bool, block_sizes: BlockSizes,\n     block_kv_dkv = min(block_sizes.block_kv_dkv, kv_seq_len)\n     block_q_dq = min(block_sizes.block_q_dq, q_seq_len)\n     block_kv_dq = min(block_sizes.block_kv_dq, kv_seq_len)\n+    head_dim_padded = pl.next_power_of_2(head_dim)\n \n     if q_seq_len // block_q_dq != kv_seq_len // block_kv_dkv:\n       raise ValueError(\n@@ -591,28 +615,24 @@ def _mha_backward(sm_scale: float, causal: bool, block_sizes: BlockSizes,\n     ]\n \n     in_specs = [\n-        pl.BlockSpec(\n-            (None, q_seq_len, None, head_dim), lambda i, j, _: (i, 0, j, 0)\n-        ),\n-        pl.BlockSpec(\n-            (None, kv_seq_len, None, head_dim), lambda i, j, _: (i, 0, j, 0)\n-        ),\n-        pl.BlockSpec(\n-            (None, kv_seq_len, None, head_dim), lambda i, j, _: (i, 0, j, 0)\n-        ),\n-        pl.BlockSpec(\n-            (None, q_seq_len, None, head_dim), lambda i, j, _: (i, 0, j, 0)\n-        ),\n-        pl.BlockSpec(\n-            (None, q_seq_len, None, head_dim), lambda i, j, _: (i, 0, j, 0)\n-        ),\n+        pl.BlockSpec((None, q_seq_len, None, head_dim_padded),\n+                     lambda i, j, _: (i, 0, j, 0)),\n+        pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n+                     lambda i, j, _: (i, 0, j, 0)),\n+        pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n+                     lambda i, j, _: (i, 0, j, 0)),\n+        pl.BlockSpec((None, q_seq_len, None, head_dim_padded),\n+                     lambda i, j, _: (i, 0, j, 0)),\n+        pl.BlockSpec((None, q_seq_len, None, head_dim_padded),\n+                     lambda i, j, _: (i, 0, j, 0)),\n         pl.BlockSpec((None, None, q_seq_len), lambda i, j, _: (i, j, 0)),\n         pl.BlockSpec((None, None, q_seq_len), lambda i, j, _: (i, j, 0)),\n     ]\n     if segment_ids is None:\n       in_specs.insert(3, None)  # type: ignore[arg-type]\n     else:\n-      in_specs.insert(3, pl.BlockSpec((None, kv_seq_len), lambda i, j, _: (i, 0)))\n+      in_specs.insert(3, pl.BlockSpec((None, kv_seq_len),\n+                                      lambda i, j, _: (i, 0)))\n \n     grid = (batch_size, num_heads, pl.cdiv(kv_seq_len, block_kv_dkv))\n     num_warps_ = num_warps\n@@ -635,22 +655,22 @@ def _mha_backward(sm_scale: float, causal: bool, block_sizes: BlockSizes,\n             block_kv_dkv=block_kv_dkv,\n             block_q_dq=block_q_dq,\n             block_kv_dq=block_kv_dq,\n-            block_d=head_dim,\n+            head_dim=head_dim,\n         ),\n         out_shape=out_shapes,\n         in_specs=in_specs,\n         grid=grid,\n         out_specs=[\n             pl.BlockSpec(\n-                (None, block_q_dq, None, head_dim),\n+                (None, block_q_dq, None, head_dim_padded),\n                 lambda i, j, k: (i, k, j, 0),  # dq\n             ),\n             pl.BlockSpec(\n-                (None, block_kv_dkv, None, head_dim),\n+                (None, block_kv_dkv, None, head_dim_padded),\n                 lambda i, j, k: (i, k, j, 0),  # dk\n             ),\n             pl.BlockSpec(\n-                (None, block_kv_dkv, None, head_dim),\n+                (None, block_kv_dkv, None, head_dim_padded),\n                 lambda i, j, k: (i, k, j, 0),  # dv\n             ),\n         ],\ndiff --git a/tests/pallas/gpu_ops_test.py b/tests/pallas/gpu_ops_test.py\nindex 1b758cdd0a58..1637686365e1 100644\n--- a/tests/pallas/gpu_ops_test.py\n+++ b/tests/pallas/gpu_ops_test.py\n@@ -153,7 +153,7 @@ def setUp(self):\n       batch_size=(1, 2),\n       seq_len=(128, 384),\n       num_heads=(1, 2, 8),\n-      head_dim=(32, 64, 128),\n+      head_dim=(32, 64, 72, 128),\n       block_sizes=(\n         ((\"block_q\", 128), (\"block_k\", 128)),\n         ((\"block_q\", 64), (\"block_k\", 64)),\n@@ -226,7 +226,7 @@ def impl(q, k, v):\n       batch_size=(1, 2),\n       seq_len=(128, 384),\n       num_heads=(1, 2),\n-      head_dim=(32, 64, 128,),\n+      head_dim=(32, 64, 72, 128,),\n       block_sizes=(\n           (\n               (\"block_q\", 128),\n\n```",
        "from_id": [
            "rdyro",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add support for loops, debug_print, and unary ops to Warp semantics.\n\nPiperOrigin-RevId: 762036132",
        "output": "```diff\nCommit: a827a274baf18ece651d37b7933bee3cb2f8760e\nDate: 2025-05-22T17:57:53Z\nURL: https://github.com/jax-ml/jax/commit/a827a274baf18ece651d37b7933bee3cb2f8760e\nFiles changed: 3\nAdditions: +139, Deletions: -60\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 2f81ecb969ac..00716bb1c675 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -1698,6 +1698,19 @@ def convert(ty, x):\n     lax.not_p: lambda ctx, x: ~x,\n })\n \n+def _unary_warp_lowering_rule(impl):\n+  def _lowering_rule(ctx: LoweringRuleContext, x):\n+    if not all(aval_in.shape == () for aval_in in ctx.avals_in):\n+      raise NotImplementedError(\n+          \"Non-scalar arithmetic is not supported in warp-level lowering.\")\n+    return impl(x)\n+  return _lowering_rule\n+\n+mosaic_lowering_rules[gpu_core.LANExWARP_SEMANTICS].update({\n+    lax.neg_p: _unary_warp_lowering_rule(lambda x: -x),\n+    lax.not_p: _unary_warp_lowering_rule(lambda x: ~x)\n+})\n+\n mosaic_lowering_rules[gpu_core.WGxWG_SEMANTICS].update({\n     lax.neg_p: _lower_fun(lambda x: jnp.subtract(0, x), multiple_results=False),\n     lax.not_p: _lower_fun(\n@@ -2163,6 +2176,8 @@ def _axis_index_warp_rule(ctx: LoweringRuleContext, *, axis_name: Hashable):\n \n \n @register_lowering_rule(primitives.debug_print_p, mgpu.LoweringSemantics.Lane)\n+@register_lowering_rule(primitives.debug_print_p, mgpu.LoweringSemantics.Lane,\n+                        gpu_core.PrimitiveSemantics.Warp)\n def _debug_print_lowering_rule(\n     ctx: LoweringRuleContext,\n     *args,\n@@ -2171,6 +2186,9 @@ def _debug_print_lowering_rule(\n ):\n   del has_placeholders  # Unused.\n   primitives.check_debug_print_format(fmt, *args)\n+  scope = mgpu.ThreadSubset.WARPGROUP\n+  if ctx.module_ctx.primitive_semantics == gpu_core.PrimitiveSemantics.Warp:\n+    scope = mgpu.ThreadSubset.WARP\n   if not any(aval.shape for aval in ctx.avals_in):\n     mgpu.debug_print(\n         fmt,\n@@ -2178,6 +2196,7 @@ def _debug_print_lowering_rule(\n             _ensure_ir_value(arg, aval.dtype)\n             for arg, aval in zip(args, ctx.avals_in)\n         ),\n+        scope=scope\n     )\n   elif len(ctx.avals_in) == 1:\n     [arg] = args\n@@ -2461,6 +2480,8 @@ def loop(loop_index, body_args):\n \n @register_lowering_rule(lax.scan_p, mgpu.LoweringSemantics.Lane)\n @register_lowering_rule(lax.scan_p, mgpu.LoweringSemantics.Warpgroup)\n+@register_lowering_rule(lax.scan_p, mgpu.LoweringSemantics.Lane,\n+                        gpu_core.PrimitiveSemantics.Warp)\n def _scan_lowering_rule(\n     ctx: LoweringRuleContext,\n     *args,\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex 1e20675f7909..4aeb3358b97a 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -144,7 +144,11 @@ def _debug_scalar_ty_format(arg):\n     return \"%f\", arg\n   raise NotImplementedError(f\"Can't print the type {arg.type}\")\n \n-def debug_print(fmt, *args, uniform=True):\n+def debug_print(fmt, *args, uniform=True, scope=None):\n+  if not uniform and scope is not None:\n+    raise ValueError(\"Cannot specify scope to a non-uniform debug_print.\")\n+  if scope is None:\n+    scope = ThreadSubset.WARPGROUP\n   type_formats = []\n   new_args = []\n   for arg in args:\n@@ -168,7 +172,7 @@ def debug_print(fmt, *args, uniform=True):\n       raise NotImplementedError(arg.type)\n     type_formats.append(ty_format)\n   ctx = (\n-      functools.partial(single_thread, scope=ThreadSubset.WARPGROUP)\n+      functools.partial(single_thread, scope=scope)\n       if uniform\n       else contextlib.nullcontext\n   )\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 7173639b879f..4ef2fa8096ee 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1569,64 +1569,6 @@ def kernel(x_ref, y_ref, o_ref):\n     y = jax.lax.iota(jnp.float32, 128) * 3\n     np.testing.assert_array_equal(kernel(x, y), x + y)\n \n-  def test_warp_specialization_axis_index(self):\n-    if self.LOWERING_SEMANTICS != plgpu.LoweringSemantics.Lane:\n-      self.skipTest(\"Test only works on Lane semantics\")\n-    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n-    @functools.partial(plgpu.kernel,\n-                       out_shape=jax.ShapeDtypeStruct((2, 128), jnp.int32))\n-    def kernel(y_ref):\n-      def scope(ones_smem_ref, threes_smem_ref):\n-        # Prepare data to copy.\n-        ones_smem_ref[:] = jnp.ones((1, 128), jnp.int32)\n-        threes_smem_ref[:] = jnp.ones((1, 128), jnp.int32) * 3\n-        plgpu.commit_smem()\n-        @pl.core_map(warp_mesh)\n-        def _():\n-          warp_id = lax.axis_index(\"warp\")\n-          # We cannot load/store inside of core_map, so we issue async\n-          # copies instead to produce a testable result.\n-          @pl.when(warp_id == 1)\n-          def _():\n-            plgpu.copy_smem_to_gmem(ones_smem_ref, y_ref.at[0:1])\n-          @pl.when(warp_id == 3)\n-          def _():\n-            plgpu.copy_smem_to_gmem(threes_smem_ref, y_ref.at[1:2])\n-        plgpu.wait_smem_to_gmem(0)\n-      pl.run_scoped(scope,\n-                    plgpu.SMEM((1, 128), jnp.int32),\n-                    plgpu.SMEM((1, 128), jnp.int32)\n-                    )\n-    result = kernel()\n-    expected = jnp.stack((jnp.ones((128,), jnp.int32),\n-                          jnp.ones((128,), jnp.int32) * 3), axis=0)\n-    np.testing.assert_array_equal(result, expected)\n-\n-  def test_warp_mesh_errors_when_closing_over_array(self):\n-    if self.LOWERING_SEMANTICS != plgpu.LoweringSemantics.Lane:\n-      self.skipTest(\"Test only works on Lane semantics\")\n-    # We currently do not allow closing over arrays when mapping over\n-    # a mesh, since we would need to present a view of the array local\n-    # to each warp.\n-    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n-    @functools.partial(plgpu.kernel,\n-                       out_shape=jax.ShapeDtypeStruct((32, 32), jnp.float32),\n-                       scratch_shapes=[plgpu.SMEM((32, 32), jnp.float32)])\n-    def kernel(out_ref, smem_ref):\n-      arr = jnp.ones((32, 32), dtype=jnp.float32)\n-      @pl.core_map(warp_mesh)\n-      def _():\n-        smem_ref[...] = arr + 1\n-      plgpu.commit_smem()\n-      plgpu.copy_smem_to_gmem(smem_ref, out_ref)\n-      plgpu.wait_smem_to_gmem(0)\n-    with self.assertRaisesRegex(\n-        mgpu_lowering.LoweringError,\n-        \"Can only close over scalars and Refs when using core_map with \"\n-        \"WarpMesh\",\n-    ):\n-      kernel()\n-\n   def test_smem_aliasing_works(self):\n     self.skip_if_wg_semantics()\n \n@@ -1825,6 +1767,118 @@ def body(idx, _):\n       )\n \n \n+class PallasCallWarpPrimitiveSemanticsTest(PallasTest):\n+  def setUp(self):\n+    super().setUp()\n+    if self.LOWERING_SEMANTICS != plgpu.LoweringSemantics.Lane:\n+      self.skipTest(\"Test only works on Lane semantics\")\n+\n+  def test_axis_index(self):\n+    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n+    @functools.partial(plgpu.kernel,\n+                       out_shape=jax.ShapeDtypeStruct((2, 128), jnp.int32))\n+    def kernel(y_ref):\n+      def scope(ones_smem_ref, threes_smem_ref):\n+        # Prepare data to copy.\n+        ones_smem_ref[:] = jnp.ones((1, 128), jnp.int32)\n+        threes_smem_ref[:] = jnp.ones((1, 128), jnp.int32) * 3\n+        plgpu.commit_smem()\n+        @pl.core_map(warp_mesh)\n+        def _():\n+          warp_id = lax.axis_index(\"warp\")\n+          # We cannot load/store inside of core_map, so we issue async\n+          # copies instead to produce a testable result.\n+          @pl.when(warp_id == 1)\n+          def _():\n+            plgpu.copy_smem_to_gmem(ones_smem_ref, y_ref.at[0:1])\n+          @pl.when(warp_id == 3)\n+          def _():\n+            plgpu.copy_smem_to_gmem(threes_smem_ref, y_ref.at[1:2])\n+        plgpu.wait_smem_to_gmem(0)\n+      pl.run_scoped(scope,\n+                    plgpu.SMEM((1, 128), jnp.int32),\n+                    plgpu.SMEM((1, 128), jnp.int32)\n+                    )\n+    result = kernel()\n+    expected = jnp.stack((jnp.ones((128,), jnp.int32),\n+                          jnp.ones((128,), jnp.int32) * 3), axis=0)\n+    np.testing.assert_array_equal(result, expected)\n+\n+  def test_errors_when_closing_over_array(self):\n+    # We currently do not allow closing over arrays when mapping over\n+    # a mesh, since we would need to present a view of the array local\n+    # to each warp.\n+    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n+    @functools.partial(plgpu.kernel,\n+                       out_shape=jax.ShapeDtypeStruct((32, 32), jnp.float32),\n+                       scratch_shapes=[plgpu.SMEM((32, 32), jnp.float32)])\n+    def kernel(out_ref, smem_ref):\n+      arr = jnp.ones((32, 32), dtype=jnp.float32)\n+      @pl.core_map(warp_mesh)\n+      def _():\n+        smem_ref[...] = arr + 1\n+      plgpu.commit_smem()\n+      plgpu.copy_smem_to_gmem(smem_ref, out_ref)\n+      plgpu.wait_smem_to_gmem(0)\n+    with self.assertRaisesRegex(\n+        mgpu_lowering.LoweringError,\n+        \"Can only close over scalars and Refs when using core_map with \"\n+        \"WarpMesh\",\n+    ):\n+      kernel()\n+\n+  def test_single_warp_scan(self):\n+    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n+    @functools.partial(plgpu.kernel,\n+                       out_shape=jax.ShapeDtypeStruct((10, 128), jnp.int32))\n+    def kernel(y_ref):\n+      def scope(smem_ref):\n+        # Prepare data to copy.\n+        for i in range(10):\n+          smem_ref[i, :] = jnp.ones_like(smem_ref.at[i]) * i\n+        plgpu.commit_smem()\n+        @pl.core_map(warp_mesh)\n+        def _():\n+          warp_id = lax.axis_index(\"warp\")\n+          @pl.when(warp_id == 0)\n+          def _():\n+            def loop_body(i, _):\n+              _slice = pl.ds(i, 1)\n+              plgpu.copy_smem_to_gmem(smem_ref.at[_slice], y_ref.at[_slice])\n+            lax.fori_loop(0, 10, loop_body, None)\n+        plgpu.wait_smem_to_gmem(0)\n+      pl.run_scoped(scope, plgpu.SMEM((10, 128), jnp.int32))\n+    result = kernel()\n+    expected = jnp.stack(\n+        [jnp.ones((128,), jnp.int32) * i for i in range(10)], axis=0)\n+    np.testing.assert_array_equal(result, expected)\n+\n+  def test_debug_print(self):\n+    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n+    @functools.partial(\n+        plgpu.kernel,\n+        out_shape=jnp.zeros(128, np.int32),\n+    )\n+    def kernel(ref):\n+      ref[...] = ref[...]  # Prevent kernel from being DCE'd\n+      @pl.core_map(warp_mesh)\n+      def _():\n+        warp_id = lax.axis_index(\"warp\")\n+        pl.debug_print(\"warp: {}\", warp_id)\n+\n+    with self.capture_stdout() as output:\n+      jax.block_until_ready(kernel())\n+    self.assertEqual(\n+        set(output().splitlines()),\n+        {\n+            \"warp: 0\",\n+            \"warp: 1\",\n+            \"warp: 2\",\n+            \"warp: 3\",\n+        },\n+    )\n+\n+\n class PallasCallWGTest(\n     PallasCallTest, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n ):\n\n```",
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Error out if wsc(x, P()) is called in a Explicit mesh context\n\nPiperOrigin-RevId: 762021159",
        "output": "```diff\nCommit: 210b5fc8674e4993254c804720144c570992984e\nDate: 2025-05-22T17:24:17Z\nURL: https://github.com/jax-ml/jax/commit/210b5fc8674e4993254c804720144c570992984e\nFiles changed: 2\nAdditions: +12, Deletions: -3\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 0624dad88a2b..ecdcf3e17332 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -2703,7 +2703,13 @@ def check_shardings_are_auto(shardings_flat):\n       raise ValueError(\n           'The spec of NamedSharding passed to with_sharding_constraint can'\n           f' only refer to Auto axes of the mesh. Got spec={s.spec} and'\n-          f' mesh={mesh}')\n+          f' mesh={mesh}. You probably meant to use `reshard` API?')\n+\n+  cur_mesh = mesh_lib.get_abstract_mesh()\n+  if cur_mesh._are_all_axes_explicit:\n+    raise ValueError(\n+        'with_sharding_constraint cannot be used when all axes of the mesh are'\n+        ' of type `Explicit`. Please use the `reshard` API.')\n \n \n def with_sharding_constraint(x, shardings):\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 024901b746a8..48339bb2a519 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7069,8 +7069,11 @@ def test_wsc_error(self, mesh):\n         \"The spec of NamedSharding passed to with_sharding_constraint\"):\n       jax.lax.with_sharding_constraint(np.arange(8).reshape(4, 2), s)\n \n-    s = NamedSharding(mesh, P())\n-    jax.lax.with_sharding_constraint(np.arange(8), s)\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'with_sharding_constraint cannot be used when all axes of the mesh are'\n+        ' of type `Explicit`'):\n+      jax.lax.with_sharding_constraint(np.arange(8), NamedSharding(mesh, P()))\n \n     s = NamedSharding(Mesh(mesh.devices, mesh.axis_names,\n                            axis_types=(AxisType.Explicit, AxisType.Auto)),\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "jax.random: thread mode parameter through categorical and choice",
        "output": "```diff\nCommit: 437e32bfddabe6c4b5ff5682ca944cb6a4cbaf89\nDate: 2025-05-22T17:22:03Z\nURL: https://github.com/jax-ml/jax/commit/437e32bfddabe6c4b5ff5682ca944cb6a4cbaf89\nFiles changed: 2\nAdditions: +32, Deletions: -10\ndiff --git a/jax/_src/random.py b/jax/_src/random.py\nindex 6f139dd9665c..60dad3a82021 100644\n--- a/jax/_src/random.py\n+++ b/jax/_src/random.py\n@@ -633,7 +633,8 @@ def choice(key: ArrayLike,\n            shape: Shape = (),\n            replace: bool = True,\n            p: RealArray | None = None,\n-           axis: int = 0) -> Array:\n+           axis: int = 0,\n+           mode: str | None = None) -> Array:\n   \"\"\"Generates a random sample from a given array.\n \n   .. warning::\n@@ -656,6 +657,12 @@ def choice(key: ArrayLike,\n       entries in a.\n     axis: int, optional. The axis along which the selection is performed.\n       The default, 0, selects by row.\n+    mode: optional, \"high\" or \"low\" for how many bits to use in the gumbel sampler\n+      when `p is None` and `replace = False`. The default is determined by the\n+      ``use_high_dynamic_range_gumbel`` config, which defaults to \"low\". With mode=\"low\",\n+      in float32 sampling will be biased for choices with probability less than about\n+      1E-7; with mode=\"high\" this limit is pushed down to about 1E-14. mode=\"high\"\n+      approximately doubles the cost of sampling.\n \n   Returns:\n     An array of shape `shape` containing samples from `a`.\n@@ -701,7 +708,7 @@ def choice(key: ArrayLike,\n       ind = jnp.searchsorted(p_cuml, r).astype(int)\n     else:\n       # Gumbel top-k trick: https://timvieira.github.io/blog/post/2019/09/16/algorithms-for-sampling-without-replacement/\n-      g = gumbel(key, (n_inputs,), dtype=p_arr.dtype) + jnp.log(p_arr)\n+      g = gumbel(key, (n_inputs,), dtype=p_arr.dtype, mode=mode) + jnp.log(p_arr)\n       ind = lax.top_k(g, k=n_draws)[1].astype(int)\n     result = ind if arr.ndim == 0 else jnp.take(arr, ind, axis)\n \n@@ -940,7 +947,8 @@ def bernoulli(key: ArrayLike,\n     mode: optional, \"high\" or \"low\" for how many bits to use when sampling.\n       default='low'. Set to \"high\" for correct sampling at small values of\n       `p`. When sampling in float32, bernoulli samples with mode='low' produce\n-      incorrect results for p < ~1E-7.\n+      incorrect results for p < ~1E-7. mode=\"high\" approximately doubles the\n+      cost of sampling.\n \n   Returns:\n     A random array with boolean dtype and shape given by ``shape`` if ``shape``\n@@ -1544,7 +1552,7 @@ def poisson(key: ArrayLike,\n def gumbel(key: ArrayLike,\n            shape: Shape = (),\n            dtype: DTypeLikeFloat = float,\n-           mode: str | None =None) -> Array:\n+           mode: str | None = None) -> Array:\n   \"\"\"Sample Gumbel random values with given shape and float dtype.\n \n   The values are distributed according to the probability density function:\n@@ -1559,6 +1567,11 @@ def gumbel(key: ArrayLike,\n     dtype: optional, a float dtype for the returned values (default float64 if\n       jax_enable_x64 is true, otherwise float32).\n     mode: optional, \"high\" or \"low\" for how many bits to use when sampling.\n+      The default is determined by the ``use_high_dynamic_range_gumbel`` config,\n+      which defaults to \"low\". When drawing float32 samples, with mode=\"low\" the\n+      uniform resolution is such that the largest possible gumbel logit is ~16;\n+      with mode=\"high\" this is increased to ~32, at approximately double the\n+      computational cost.\n \n   Returns:\n     A random array with the specified shape and dtype.\n@@ -1599,6 +1612,7 @@ def categorical(\n   axis: int = -1,\n   shape: Shape | None = None,\n   replace: bool = True,\n+  mode: str | None = None,\n ) -> Array:\n   \"\"\"Sample random values from categorical distributions.\n \n@@ -1615,6 +1629,12 @@ def categorical(\n       The default (None) produces a result shape equal to ``np.delete(logits.shape, axis)``.\n     replace: If True (default), perform sampling with replacement. If False, perform\n       sampling without replacement.\n+    mode: optional, \"high\" or \"low\" for how many bits to use in the gumbel sampler.\n+      The default is determined by the ``use_high_dynamic_range_gumbel`` config,\n+      which defaults to \"low\". With mode=\"low\", in float32 sampling will be biased\n+      for events with probability less than about 1E-7; with mode=\"high\" this limit\n+      is pushed down to about 1E-14. mode=\"high\" approximately doubles the cost of\n+      sampling.\n \n   Returns:\n     A random array with int dtype and shape given by ``shape`` if ``shape``\n@@ -1644,11 +1664,11 @@ def categorical(\n     logits_shape = list(shape[len(shape) - len(batch_shape):])\n     logits_shape.insert(axis % len(logits_arr.shape), logits_arr.shape[axis])\n     return jnp.argmax(\n-        gumbel(key, (*shape_prefix, *logits_shape), logits_arr.dtype) +\n+        gumbel(key, (*shape_prefix, *logits_shape), logits_arr.dtype, mode=mode) +\n         lax.expand_dims(logits_arr, tuple(range(len(shape_prefix)))),\n         axis=axis)\n   else:\n-    logits_arr += gumbel(key, logits_arr.shape, logits_arr.dtype)\n+    logits_arr += gumbel(key, logits_arr.shape, logits_arr.dtype, mode=mode)\n     k = math.prod(shape_prefix)\n     if k > logits_arr.shape[axis]:\n       raise ValueError(\ndiff --git a/tests/random_lax_test.py b/tests/random_lax_test.py\nindex f87b079b759c..9fe4d2ecbda3 100644\n--- a/tests/random_lax_test.py\n+++ b/tests/random_lax_test.py\n@@ -286,8 +286,9 @@ def testTruncatedNormal(self, dtype):\n     ],\n     dtype=jtu.dtypes.floating + jtu.dtypes.integer,\n     weighted=[True, False],\n+    mode=[None, 'low', 'high']\n   )\n-  def testChoice(self, dtype, input_range_or_shape, shape, replace, weighted, axis):\n+  def testChoice(self, dtype, input_range_or_shape, shape, replace, weighted, axis, mode):\n     # This is the function API that we test against (note that self.rng().choice differs)\n     np_choice = np.random.default_rng(0).choice\n     p_dtype = dtypes.to_inexact_dtype(dtype)\n@@ -303,7 +304,7 @@ def testChoice(self, dtype, input_range_or_shape, shape, replace, weighted, axis\n       p /= p.sum()\n     else:\n       p = None\n-    rand = lambda key, x: random.choice(key, x, shape, replace, p, axis)\n+    rand = lambda key, x: random.choice(key, x, shape, replace, p, axis, mode=mode)\n     sample = rand(key(), x)\n     if not is_range:\n       self.assertEqual(dtype, sample.dtype)\n@@ -397,15 +398,16 @@ def testBernoulli(self, p, dtype, mode):\n       ]\n     ],\n     sample_shape=[(10000,), (5000, 2)],\n+    mode=[None, 'low', 'high'],\n     dtype=jtu.dtypes.floating,\n   )\n-  def testCategorical(self, p, axis, dtype, sample_shape):\n+  def testCategorical(self, p, axis, dtype, sample_shape, mode):\n     key = lambda: self.make_key(0)\n     p = np.array(p, dtype=dtype)\n     logits = np.log(p) - 42 # test unnormalized\n     out_shape = tuple(np.delete(logits.shape, axis))\n     shape = sample_shape + out_shape\n-    rand = partial(random.categorical, shape=shape, axis=axis)\n+    rand = partial(random.categorical, shape=shape, axis=axis, mode=mode)\n     crand = jax.jit(rand)\n \n     uncompiled_samples = rand(key(), logits)\n\n```",
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Merge pull request #28945 from hawkinsp:trove\n\nPiperOrigin-RevId: 762016836",
        "output": "```diff\nCommit: 3622c928bc3ec9647023355db2e788c6a3012ee4\nDate: 2025-05-22T17:14:34Z\nURL: https://github.com/jax-ml/jax/commit/3622c928bc3ec9647023355db2e788c6a3012ee4\nFiles changed: 4\nAdditions: +9, Deletions: -2\ndiff --git a/jax_plugins/cuda/plugin_setup.py b/jax_plugins/cuda/plugin_setup.py\nindex c8b70408471c..fc467824fe5f 100644\n--- a/jax_plugins/cuda/plugin_setup.py\n+++ b/jax_plugins/cuda/plugin_setup.py\n@@ -78,10 +78,12 @@ def has_ext_modules(self):\n     url=\"https://github.com/jax-ml/jax\",\n     license=\"Apache-2.0\",\n     classifiers=[\n-        \"Development Status :: 3 - Alpha\",\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Programming Language :: Python :: 3.12\",\n+        \"Programming Language :: Python :: 3.13\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     package_data={\n         package_name: [\ndiff --git a/jax_plugins/cuda/setup.py b/jax_plugins/cuda/setup.py\nindex 1ce555978dac..b2c89285e7fd 100644\n--- a/jax_plugins/cuda/setup.py\n+++ b/jax_plugins/cuda/setup.py\n@@ -51,8 +51,9 @@ def load_version_module(pkg_path):\n     url=\"https://github.com/jax-ml/jax\",\n     license=\"Apache-2.0\",\n     classifiers=[\n-        \"Development Status :: 3 - Alpha\",\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     package_data={\n         package_name: [\"xla_cuda_plugin.so\"],\ndiff --git a/jaxlib/setup.py b/jaxlib/setup.py\nindex 8d7933953851..30e81c9ad671 100644\n--- a/jaxlib/setup.py\n+++ b/jaxlib/setup.py\n@@ -68,10 +68,12 @@ def has_ext_modules(self):\n     url='https://github.com/jax-ml/jax',\n     license='Apache-2.0',\n     classifiers=[\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Programming Language :: Python :: 3.12\",\n         \"Programming Language :: Python :: 3.13\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     package_data={\n         'jaxlib': [\ndiff --git a/setup.py b/setup.py\nindex 4c5c86f588c3..ef78b8f6e7ff 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -118,10 +118,12 @@ def load_version_module(pkg_path):\n     url='https://github.com/jax-ml/jax',\n     license='Apache-2.0',\n     classifiers=[\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Programming Language :: Python :: 3.12\",\n         \"Programming Language :: Python :: 3.13\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     zip_safe=False,\n )\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update the trove classifiers for JAX packages.\n\n* Don't tag packages as alpha.\n* Tag free-threading as supported.\n* Update some python version lists.",
        "output": "```diff\nCommit: 0dc70b93f2e13fae5b097837760bd621e746dae7\nDate: 2025-05-22T16:59:16Z\nURL: https://github.com/jax-ml/jax/commit/0dc70b93f2e13fae5b097837760bd621e746dae7\nFiles changed: 4\nAdditions: +9, Deletions: -2\ndiff --git a/jax_plugins/cuda/plugin_setup.py b/jax_plugins/cuda/plugin_setup.py\nindex c8b70408471c..fc467824fe5f 100644\n--- a/jax_plugins/cuda/plugin_setup.py\n+++ b/jax_plugins/cuda/plugin_setup.py\n@@ -78,10 +78,12 @@ def has_ext_modules(self):\n     url=\"https://github.com/jax-ml/jax\",\n     license=\"Apache-2.0\",\n     classifiers=[\n-        \"Development Status :: 3 - Alpha\",\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Programming Language :: Python :: 3.12\",\n+        \"Programming Language :: Python :: 3.13\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     package_data={\n         package_name: [\ndiff --git a/jax_plugins/cuda/setup.py b/jax_plugins/cuda/setup.py\nindex 1ce555978dac..b2c89285e7fd 100644\n--- a/jax_plugins/cuda/setup.py\n+++ b/jax_plugins/cuda/setup.py\n@@ -51,8 +51,9 @@ def load_version_module(pkg_path):\n     url=\"https://github.com/jax-ml/jax\",\n     license=\"Apache-2.0\",\n     classifiers=[\n-        \"Development Status :: 3 - Alpha\",\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     package_data={\n         package_name: [\"xla_cuda_plugin.so\"],\ndiff --git a/jaxlib/setup.py b/jaxlib/setup.py\nindex 8d7933953851..30e81c9ad671 100644\n--- a/jaxlib/setup.py\n+++ b/jaxlib/setup.py\n@@ -68,10 +68,12 @@ def has_ext_modules(self):\n     url='https://github.com/jax-ml/jax',\n     license='Apache-2.0',\n     classifiers=[\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Programming Language :: Python :: 3.12\",\n         \"Programming Language :: Python :: 3.13\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     package_data={\n         'jaxlib': [\ndiff --git a/setup.py b/setup.py\nindex 4c5c86f588c3..ef78b8f6e7ff 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -118,10 +118,12 @@ def load_version_module(pkg_path):\n     url='https://github.com/jax-ml/jax',\n     license='Apache-2.0',\n     classifiers=[\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Programming Language :: Python :: 3.12\",\n         \"Programming Language :: Python :: 3.13\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     zip_safe=False,\n )\n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Don't require MOSAIC_GPU_NVSHMEM_SO_PATH to be set\n\nPiperOrigin-RevId: 762008659",
        "output": "```diff\nCommit: 0e77a1617343886387369aa46b21cf8afbc7870c\nDate: 2025-05-22T16:55:12Z\nURL: https://github.com/jax-ml/jax/commit/0e77a1617343886387369aa46b21cf8afbc7870c\nFiles changed: 1\nAdditions: +7, Deletions: -7\ndiff --git a/jax/experimental/mosaic/gpu/core.py b/jax/experimental/mosaic/gpu/core.py\nindex 48a877f8c67a..5e1ed6b88412 100644\n--- a/jax/experimental/mosaic/gpu/core.py\n+++ b/jax/experimental/mosaic/gpu/core.py\n@@ -102,15 +102,15 @@\n def supports_cross_device_collectives():\n   try:\n     nvshmem_bc_path = os.environ[\"MOSAIC_GPU_NVSHMEM_BC_PATH\"]\n-    nvshmem_so_path = os.environ[\"MOSAIC_GPU_NVSHMEM_SO_PATH\"]\n   except KeyError:\n     return False\n-  try:\n-    # This both ensures that the file exists, and it populates the dlopen cache\n-    # helping XLA find the library even if the RPATH is not exactly right...\n-    ctypes.CDLL(nvshmem_so_path)\n-  except OSError:\n-    return False\n+  if nvshmem_so_path := os.environ.get(\"MOSAIC_GPU_NVSHMEM_SO_PATH\", \"\"):\n+    try:\n+      # This both ensures that the file exists, and it populates the dlopen\n+      # cache, helping XLA find the library even if the RPATH is not right...\n+      ctypes.CDLL(nvshmem_so_path)\n+    except OSError:\n+      return False\n   xla_flags = os.environ.get(\"XLA_FLAGS\", \"\")\n   return (\n       os.path.exists(nvshmem_bc_path)\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] (Slightly) Clean up the collective matmul test by using explicit sharding\n\nNot a huge difference, but it's a bit nicer.\n\nPiperOrigin-RevId: 761984854",
        "output": "```diff\nCommit: 44d2cc927989b8a6a4681ba49c49f6ac8efba910\nDate: 2025-05-22T15:54:43Z\nURL: https://github.com/jax-ml/jax/commit/44d2cc927989b8a6a4681ba49c49f6ac8efba910\nFiles changed: 2\nAdditions: +12, Deletions: -16\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex cf0d46639559..e4a308c2f10b 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -864,6 +864,7 @@ jax_multiplatform_test(\n         \"notap\",\n     ],\n     deps = [\n+        \"//jax:experimental\",\n         \"//jax:pallas\",\n         \"//jax:pallas_experimental_gpu_ops\",\n         \"//jax:pallas_mosaic_gpu\",\ndiff --git a/tests/pallas/mgpu_collective_matmul_test.py b/tests/pallas/mgpu_collective_matmul_test.py\nindex e0ced79801d8..386162b1992c 100644\n--- a/tests/pallas/mgpu_collective_matmul_test.py\n+++ b/tests/pallas/mgpu_collective_matmul_test.py\n@@ -27,6 +27,7 @@\n from jax._src.pallas import pallas_call\n from jax.experimental.mosaic import gpu as mgpu\n from jax.experimental.pallas.ops.gpu import collective_matmul_mgpu\n+from jax.experimental import shard\n import jax.numpy as jnp\n import numpy as np\n \n@@ -51,8 +52,13 @@ def setUp(self):\n     if os.environ.get(\"XLA_PYTHON_CLIENT_ALLOCATOR\", \"\") == \"platform\":\n       self.skipTest(\"NVSHMEM doesn't work with the platform allocator.\")\n     context_stack = contextlib.ExitStack()\n-    context_stack.enter_context(pallas_call._PALLAS_USE_MOSAIC_GPU(True))\n     self.addCleanup(context_stack.close)\n+    context_stack.enter_context(pallas_call._PALLAS_USE_MOSAIC_GPU(True))\n+    num_devices = jax.device_count()\n+    mesh = jax.make_mesh(\n+        (num_devices,), (\"x\",), axis_types=(jax.sharding.AxisType.Explicit,)\n+    )\n+    context_stack.enter_context(jax.sharding.use_mesh(mesh))\n \n   @parameterized.product(\n       m_shard=(1024, 8192),\n@@ -90,28 +96,17 @@ def test_all_gather_lhs_matmul(\n     k1, k2 = random.split(random.key(1234), num=2)\n     lhs = random.normal(k1, (num_devices * m_shard, k), dtype)\n     rhs = random.normal(k2, (k, num_devices * n_shard), dtype)\n-\n-    mesh = jax.sharding.Mesh(jax.devices(), [\"x\"])\n-    lhs = jax.device_put(lhs, jax.sharding.NamedSharding(mesh, P(\"x\", None)))\n-    rhs = jax.device_put(rhs, jax.sharding.NamedSharding(mesh, P(None, \"x\")))\n+    lhs = shard.reshard(lhs, P(\"x\", None))\n+    rhs = shard.reshard(rhs, P(None, \"x\"))\n \n     def run(body):\n       out = jax.jit(\n-          jax.shard_map(\n-              body,\n-              mesh=mesh,\n-              in_specs=(P(\"x\", None), P(None, \"x\")),\n-              out_specs=P(None, \"x\"),\n-              check_vma=False,\n-          )\n+          jax.shard_map(body, out_specs=P(None, \"x\"), check_vma=False)\n       )(lhs, rhs)\n       # Gather output, for NumPy comparison on the host.\n       out = jax.shard_map(\n           lambda x: lax.all_gather(x, \"x\", axis=1, tiled=True),\n-          mesh=mesh,\n-          in_specs=P(None, \"x\"),\n-          out_specs=P(None),\n-          check_vma=False,\n+          out_specs=P(None), check_vma=False,\n       )(out)\n       return out\n \n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Disable the GPU distributed test when the platform allocator is used\n\nAnd, if we're only running this test, try to override the flag.\n\nPiperOrigin-RevId: 761950583",
        "output": "```diff\nCommit: 4c83b08980f33f208d55667135e9fbe2cc3e8938\nDate: 2025-05-22T14:13:02Z\nURL: https://github.com/jax-ml/jax/commit/4c83b08980f33f208d55667135e9fbe2cc3e8938\nFiles changed: 4\nAdditions: +24, Deletions: -1\ndiff --git a/.github/workflows/bazel_optional_h100_b200.yml b/.github/workflows/bazel_optional_h100_b200.yml\nindex 0c73b238505e..7381ce6d80bf 100644\n--- a/.github/workflows/bazel_optional_h100_b200.yml\n+++ b/.github/workflows/bazel_optional_h100_b200.yml\n@@ -48,6 +48,7 @@ jobs:\n             --test_env=XLA_PYTHON_CLIENT_ALLOCATOR=platform \\\n             --run_under \"$(pwd)/build/parallel_accelerator_execute.sh\" \\\n             --test_output=errors \\\n+            --test_tag_filters=-multiaccelerator \\\n             --test_env=JAX_ACCELERATOR_COUNT=1 \\\n             --test_env=JAX_TESTS_PER_ACCELERATOR=8 \\\n             --strategy=TestRunner=local \\\n@@ -102,7 +103,6 @@ jobs:\n             //tests/pallas:gpu_tests \\\n             //tests:array_interoperability_test_gpu \\\n             //tests:cudnn_fusion_test_gpu \\\n-            //tests:fused_attention_stablehlo_test_gpu\n             //tests:fused_attention_stablehlo_test_gpu \\\n             //tests:gpu_tests \\\n             //tests:python_callback_test_gpu \\\ndiff --git a/jax/experimental/mosaic/gpu/core.py b/jax/experimental/mosaic/gpu/core.py\nindex c20c5252a27f..48a877f8c67a 100644\n--- a/jax/experimental/mosaic/gpu/core.py\n+++ b/jax/experimental/mosaic/gpu/core.py\n@@ -102,8 +102,15 @@\n def supports_cross_device_collectives():\n   try:\n     nvshmem_bc_path = os.environ[\"MOSAIC_GPU_NVSHMEM_BC_PATH\"]\n+    nvshmem_so_path = os.environ[\"MOSAIC_GPU_NVSHMEM_SO_PATH\"]\n   except KeyError:\n     return False\n+  try:\n+    # This both ensures that the file exists, and it populates the dlopen cache\n+    # helping XLA find the library even if the RPATH is not exactly right...\n+    ctypes.CDLL(nvshmem_so_path)\n+  except OSError:\n+    return False\n   xla_flags = os.environ.get(\"XLA_FLAGS\", \"\")\n   return (\n       os.path.exists(nvshmem_bc_path)\ndiff --git a/tests/pallas/gpu_pallas_distributed_test.py b/tests/pallas/gpu_pallas_distributed_test.py\nindex d862e6b9b819..3aeee352ff6d 100644\n--- a/tests/pallas/gpu_pallas_distributed_test.py\n+++ b/tests/pallas/gpu_pallas_distributed_test.py\n@@ -15,6 +15,8 @@\n \"\"\"Tests for distributed pallas GPU operations.\"\"\"\n \n import functools\n+import os\n+\n import jax\n from jax import lax\n from jax._src import test_util as jtu\n@@ -41,6 +43,8 @@ def setUp(self):\n       self.skipTest(\"NVSHMEM library unavailable.\")\n     if jax.process_count() == 1:\n       self.skipTest(\"Test requires multiple processes.\")\n+    if os.environ.get(\"XLA_PYTHON_CLIENT_ALLOCATOR\", \"\") == \"platform\":\n+      self.skipTest(\"NVSHMEM doesn't work with the platform allocator.\")\n     super().setUp()\n \n   def test_basic_remote_dma(self):\n@@ -114,4 +118,9 @@ def kernel(y_ref, sem):\n \n \n if __name__ == '__main__':\n+  # This test doesn't work with the platform allocator, so we override it\n+  # if it's ran alone. If it's part of a larger test suite and the platform\n+  # allocator is used, setUp will skip the test.\n+  os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.01'\n+  os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'default'\n   jt_multiprocess.main()\ndiff --git a/tests/pallas/mgpu_collective_matmul_test.py b/tests/pallas/mgpu_collective_matmul_test.py\nindex bbc50d39d7f6..e0ced79801d8 100644\n--- a/tests/pallas/mgpu_collective_matmul_test.py\n+++ b/tests/pallas/mgpu_collective_matmul_test.py\n@@ -48,6 +48,8 @@ def setUp(self):\n       self.skipTest(\"NVSHMEM library unavailable.\")\n     if jax.process_count() == 1:\n       self.skipTest(\"Test requires multiple processes.\")\n+    if os.environ.get(\"XLA_PYTHON_CLIENT_ALLOCATOR\", \"\") == \"platform\":\n+      self.skipTest(\"NVSHMEM doesn't work with the platform allocator.\")\n     context_stack = contextlib.ExitStack()\n     context_stack.enter_context(pallas_call._PALLAS_USE_MOSAIC_GPU(True))\n     self.addCleanup(context_stack.close)\n@@ -128,6 +130,11 @@ def run(body):\n \n \n if __name__ == \"__main__\":\n+  # This test doesn't work with the platform allocator, so we override it\n+  # if it's ran alone. If it's part of a larger test suite and the platform\n+  # allocator is used, setUp will skip the test.\n+  os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.01\"\n+  os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"default\"\n   os.environ[\"XLA_FLAGS\"] = (\n       os.environ.get(\"XLA_FLAGS\", \"\") + \" --xla_gpu_autotune_level=0\"\n   )\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Add a first prototype of an all_gather collective matmul kernel\n\nIt's not very optimized at the moment and is unlikely to outperform the baseline\nof raw all_gather + matmul, but it computes the right numbers. We are already aware\nof a few places that could be optimized and we'll start rolling them out soon.\n\nPiperOrigin-RevId: 761939624",
        "output": "```diff\nCommit: 7014bde5a57931054ea623ce5a0d3dc85090ce4e\nDate: 2025-05-22T13:36:46Z\nURL: https://github.com/jax-ml/jax/commit/7014bde5a57931054ea623ce5a0d3dc85090ce4e\nFiles changed: 4\nAdditions: +342, Deletions: -1\ndiff --git a/jax/_src/pallas/core.py b/jax/_src/pallas/core.py\nindex fe755d61a310..13c634eb395f 100644\n--- a/jax/_src/pallas/core.py\n+++ b/jax/_src/pallas/core.py\n@@ -971,7 +971,7 @@ def _convert_block_spec_to_block_mapping(\n class ScratchShape(Protocol):\n   def get_array_aval(self) -> jax_core.AbstractValue:\n     ...\n-  def get_ref_aval(self) -> state.AbstractRef:\n+  def get_ref_aval(self) -> state.AbstractRef | TransformedRef:\n     ...\n \n \ndiff --git a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\nnew file mode 100644\nindex 000000000000..a6c372f2cee7\n--- /dev/null\n+++ b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n@@ -0,0 +1,178 @@\n+# Copyright 2025 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"A collective matmul kernel implemented using Mosaic GPU.\"\"\"\n+\n+import functools\n+import jax\n+from jax import lax\n+from jax.experimental import pallas as pl\n+from jax.experimental.pallas import mosaic_gpu as plgpu\n+import jax.numpy as jnp\n+\n+\n+def _find_swizzle(dim_size_bits: int, what: str):\n+  for swizzle_bytes in (128, 64, 32, 16):\n+    if dim_size_bits % (swizzle_bytes * 8) == 0:\n+      return swizzle_bytes\n+  raise ValueError(\n+      f\"No valid out swizzle for {what}: its minor dimension has\"\n+      f\" {dim_size_bits} bits, which is not a multiple of 128\"\n+  )\n+\n+\n+# TODO(apaszke): Add grid tiling\n+def all_gather_lhs_matmul(\n+    lhs: jax.Array,\n+    rhs: jax.Array,\n+    axis_name,\n+    *,\n+    block_m: int,\n+    block_n: int,\n+    block_k: int,\n+    max_concurrent_steps: int,\n+) -> jax.Array:\n+  if (num_devices := jax.device_count()) != jax.process_count():\n+    raise ValueError(\"The kernel only supports one device per process\")\n+  if (axis_size := lax.axis_size(axis_name)) != num_devices:\n+    raise ValueError(\"The kernel can only work over all devices in a Mesh.\")\n+  if max_concurrent_steps < 2:\n+    raise ValueError(\"max_concurrent_steps must be >= 2\")\n+\n+  num_sms = 132  # There are 132 SMs on a H100 SXM GPU.\n+\n+  m_shard, k = lhs.shape\n+  k2, n_shard = rhs.shape\n+  if k != k2:\n+    raise ValueError(\n+        f\"lhs and rhs must have the same contraction size, got {k} and {k2}.\"\n+    )\n+  if (element_type := lhs.dtype) != rhs.dtype:\n+    raise ValueError(\n+        f\"lhs and rhs must have the same element type, got {element_type} and\"\n+        f\" {rhs.dtype}.\"\n+    )\n+  if k % block_k != 0:\n+    raise NotImplementedError(f\"k={k} must be a multiple of block_k={block_k}\")\n+  if m_shard % block_m != 0:\n+    raise NotImplementedError(f\"m_shard={m_shard} must be a multiple of block_m={block_m}\")\n+  if n_shard % block_n != 0:\n+    raise NotImplementedError(f\"n_shard={n_shard} must be a multiple of block_n={block_n}\")\n+  if n_shard != block_n:\n+    raise NotImplementedError(\n+        f\"n_shard={n_shard} must be equal to block_n={block_n}\"\n+    )\n+\n+  swizzle = min(\n+      _find_swizzle(block_k * jnp.finfo(element_type).bits, \"lhs\"),\n+      _find_swizzle(block_n * jnp.finfo(element_type).bits, \"rhs\"),\n+  )\n+  transforms = (\n+      plgpu.TilingTransform((8, swizzle // jnp.dtype(element_type).itemsize)),\n+      plgpu.SwizzleTransform(swizzle),\n+  )\n+\n+  def kernel_body(lhs_ref, rhs_ref, out_ref, scratch_ref, capacity_sem, received_sem):\n+    sm_id = lax.axis_index('sm')\n+    scratch_ref = scratch_ref.at[sm_id]\n+\n+    dev_id = lax.axis_index(axis_name)\n+    send_dev_id = lax.rem(dev_id + axis_size - 1, axis_size)\n+    recv_dev_id = lax.rem(dev_id + 1, axis_size)\n+    # NOTE: Technically we should signal the recv_dev_id (and our signal would\n+    # be received from send_dev_id), but if everyone signals in a ring after a\n+    # barrier then it's equivalent to a local signal.\n+    pl.semaphore_signal(capacity_sem)\n+    send_scratch_ref = plgpu.remote_ref(\n+        scratch_ref, send_dev_id, device_id_type=pl.DeviceIdType.LOGICAL\n+    )\n+\n+    def m_loop(mi, _):\n+      mi = mi * lax.axis_size('sm') + sm_id\n+      m_tile_slice = pl.ds(mi * block_m, block_m)\n+\n+      # For some reason ptxas spills if we unroll the loop over k\n+      copy_block = 32\n+      def k_copy_loop(ki, _):\n+        k_slice = pl.ds(ki * copy_block, copy_block)\n+        scratch_ref[0, :, k_slice] = lhs_ref[m_tile_slice, k_slice]\n+      jax.lax.fori_loop(0, k // copy_block, k_copy_loop, None)\n+\n+      def device_loop(device_offset, _):\n+        # Loop invariant: scratch_ref.at[scratch_slot] is ready to be used\n+        # We're double buffering the scratch space. At each step, we read from\n+        # scratch_ref.at[scratch_slot] and write to scratch_ref.at[next_scratch_slot]\n+        # located on the send_dev_id. We swap the slots after completing a step,\n+        # which lets us overlap the copy with compute.\n+        scratch_slot = lax.rem(device_offset, 2)\n+        next_scratch_slot = 1 - scratch_slot\n+\n+        @functools.partial(\n+            pl.run_scoped,\n+            acc_ref=plgpu.ACC((block_m, block_n)),\n+            out_smem=plgpu.SMEM((block_m, block_n), jnp.float16, transforms=transforms),\n+        )\n+        def _(acc_ref, out_smem):\n+          pl.semaphore_wait(capacity_sem)\n+          @functools.partial(\n+              plgpu.emit_pipeline,\n+              grid=(k // block_k,),\n+              in_specs=[\n+                  plgpu.BlockSpec((block_m, block_k), lambda k: (0, k), transforms=transforms),\n+                  plgpu.BlockSpec((block_k, block_n), lambda k: (k, 0), transforms=transforms),\n+              ],\n+              max_concurrent_steps=max_concurrent_steps,\n+              delay_release=1,\n+          )\n+          def k_loop(idxs, lhs_smem, rhs_smem):\n+            (ki,) = idxs\n+            plgpu.wgmma(acc_ref, lhs_smem, rhs_smem)\n+            k_slice = pl.ds(ki * block_k, block_k)\n+            # TODO(apaszke): No need to send on the last step\n+            # TODO(apaszke): Use an async copy. This is uncoalesced.\n+            send_scratch_ref[next_scratch_slot, :, k_slice] = lhs_smem[...]\n+          k_loop(scratch_ref.at[scratch_slot], rhs_ref)\n+          # TODO(apaszke): Both of those semaphores perform a .sys release.\n+          # This is very expensive and we should only do a single .sys fence.\n+          pl.semaphore_signal(capacity_sem, device_id=recv_dev_id, device_id_type=pl.DeviceIdType.LOGICAL)\n+          pl.semaphore_signal(received_sem, device_id=send_dev_id, device_id_type=pl.DeviceIdType.LOGICAL)\n+          # Make sure all TMAs have read SMEM before we overwrite it.\n+          plgpu.wait_smem_to_gmem(0, wait_read_only=True)\n+          out_smem[...] = acc_ref[...].astype(out_smem.dtype)\n+          plgpu.commit_smem()\n+          device_m_slice = pl.ds(\n+              lax.rem(device_offset + dev_id, num_devices) * m_shard, block_m\n+          )\n+          plgpu.copy_smem_to_gmem(\n+              out_smem, out_ref.at[device_m_slice].at[m_tile_slice]\n+          )\n+          # Wait for the next scratch to arrive --- see the loop invariant.\n+          pl.semaphore_wait(received_sem)\n+      jax.lax.fori_loop(0, num_devices, device_loop, None)\n+    grid_size = m_shard // block_m\n+    m_steps = grid_size // num_sms + jnp.int32(sm_id < grid_size % num_sms)\n+    # TODO(apaszke): Use the ND-loop helper.\n+    jax.lax.fori_loop(0, m_steps, m_loop, None)\n+\n+  result, _ = plgpu.kernel(\n+      kernel_body,\n+      out_shape=[jax.ShapeDtypeStruct((axis_size * m_shard, n_shard), jnp.float16),\n+                  jax.ShapeDtypeStruct((num_sms, 2, block_m, k), jnp.float16)],\n+      scratch_shapes=[\n+          plgpu.SemaphoreType.REGULAR, plgpu.SemaphoreType.REGULAR,\n+      ],\n+      grid=(num_sms,),\n+      grid_names=('sm',),\n+  )(lhs, rhs)\n+  return result\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex c45e52b1fe88..cf0d46639559 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -842,6 +842,35 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"mgpu_collective_matmul_test\",\n+    srcs = [\"mgpu_collective_matmul_test.py\"],\n+    args = [\n+        \"--num_processes=2\",\n+        \"--gpus_per_process=1\",\n+    ],\n+    enable_backends = [],\n+    enable_configs = [\n+        \"gpu_h100x2\",\n+    ],\n+    env = {\n+        \"XLA_FLAGS\": \"--xla_gpu_experimental_enable_nvshmem=true\",\n+        \"JAX_PALLAS_USE_MOSAIC_GPU\": \"1\",\n+    },\n+    shard_count = 4,\n+    tags = [\n+        \"manual\",\n+        \"multiaccelerator\",\n+        \"notap\",\n+    ],\n+    deps = [\n+        \"//jax:pallas\",\n+        \"//jax:pallas_experimental_gpu_ops\",\n+        \"//jax:pallas_mosaic_gpu\",\n+        \"//jax:test_multiprocess\",\n+    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+)\n+\n jax_multiplatform_test(\n     name = \"fuser_block_spec_test\",\n     srcs = [\ndiff --git a/tests/pallas/mgpu_collective_matmul_test.py b/tests/pallas/mgpu_collective_matmul_test.py\nnew file mode 100644\nindex 000000000000..bbc50d39d7f6\n--- /dev/null\n+++ b/tests/pallas/mgpu_collective_matmul_test.py\n@@ -0,0 +1,134 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Test different parameterizations of our Mosaic GPU collective matmul.\"\"\"\n+\n+import contextlib\n+import functools\n+import os\n+\n+from absl.testing import parameterized  # pylint: disable=g-multiple-import\n+import jax\n+from jax import lax\n+from jax import random\n+from jax._src import test_multiprocess as jt_multiprocess\n+from jax._src import test_util as jtu\n+from jax._src.pallas import pallas_call\n+from jax.experimental.mosaic import gpu as mgpu\n+from jax.experimental.pallas.ops.gpu import collective_matmul_mgpu\n+import jax.numpy as jnp\n+import numpy as np\n+\n+\n+P = jax.sharding.PartitionSpec\n+\n+\n+@jtu.with_config(jax_traceback_filtering=\"off\")\n+class CollectiveMatmulTestCase(jtu.JaxTestCase):\n+\n+  def setUp(self):\n+    super().setUp()\n+    if collective_matmul_mgpu is None:\n+      self.skipTest(\"Mosaic GPU not available.\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_equal(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability sm90a\")\n+    if not mgpu.supports_cross_device_collectives():\n+      self.skipTest(\"NVSHMEM library unavailable.\")\n+    if jax.process_count() == 1:\n+      self.skipTest(\"Test requires multiple processes.\")\n+    context_stack = contextlib.ExitStack()\n+    context_stack.enter_context(pallas_call._PALLAS_USE_MOSAIC_GPU(True))\n+    self.addCleanup(context_stack.close)\n+\n+  @parameterized.product(\n+      m_shard=(1024, 8192),\n+      n_shard=(64, 128, 192),\n+      k=(256, 8192),\n+      block_m=(64, 128, 192),\n+      block_n=(64, 128, 192),\n+      block_k=(64, 128),\n+      max_concurrent_steps=(2, 4),\n+  )\n+  def test_all_gather_lhs_matmul(\n+      self,\n+      m_shard,\n+      n_shard,\n+      k,\n+      block_m,\n+      block_n,\n+      block_k,\n+      max_concurrent_steps,\n+  ):\n+    num_devices = jax.device_count()\n+    dtype = jnp.float16\n+    lhs_smem_size = block_m * block_k * max_concurrent_steps * 2\n+    rhs_smem_size = block_k * block_n * max_concurrent_steps * 2\n+    # H100 SMEM limit is 228kB.\n+    if lhs_smem_size + rhs_smem_size > 228_000:\n+      self.skipTest(\"This configuration requires too much SMEM.\")\n+    if n_shard != block_n:\n+      self.skipTest(\"n_shard must be equal to block_n for now.\")\n+    if n_shard % block_n:\n+      self.skipTest(\"n_shard must be divisble by block_n for now.\")\n+    if m_shard % block_m:\n+      self.skipTest(\"m_shard must be divisible by block_m for now.\")\n+\n+    k1, k2 = random.split(random.key(1234), num=2)\n+    lhs = random.normal(k1, (num_devices * m_shard, k), dtype)\n+    rhs = random.normal(k2, (k, num_devices * n_shard), dtype)\n+\n+    mesh = jax.sharding.Mesh(jax.devices(), [\"x\"])\n+    lhs = jax.device_put(lhs, jax.sharding.NamedSharding(mesh, P(\"x\", None)))\n+    rhs = jax.device_put(rhs, jax.sharding.NamedSharding(mesh, P(None, \"x\")))\n+\n+    def run(body):\n+      out = jax.jit(\n+          jax.shard_map(\n+              body,\n+              mesh=mesh,\n+              in_specs=(P(\"x\", None), P(None, \"x\")),\n+              out_specs=P(None, \"x\"),\n+              check_vma=False,\n+          )\n+      )(lhs, rhs)\n+      # Gather output, for NumPy comparison on the host.\n+      out = jax.shard_map(\n+          lambda x: lax.all_gather(x, \"x\", axis=1, tiled=True),\n+          mesh=mesh,\n+          in_specs=P(None, \"x\"),\n+          out_specs=P(None),\n+          check_vma=False,\n+      )(out)\n+      return out\n+\n+    out = run(\n+        functools.partial(\n+            collective_matmul_mgpu.all_gather_lhs_matmul,\n+            axis_name=\"x\",\n+            block_m=block_m,\n+            block_n=block_n,\n+            block_k=block_k,\n+            max_concurrent_steps=max_concurrent_steps,\n+        )\n+    )\n+    ref_out = run(lambda x, y: lax.all_gather(x, \"x\", axis=0, tiled=True) @ y)\n+    np.testing.assert_allclose(out, ref_out)\n+\n+\n+if __name__ == \"__main__\":\n+  os.environ[\"XLA_FLAGS\"] = (\n+      os.environ.get(\"XLA_FLAGS\", \"\") + \" --xla_gpu_autotune_level=0\"\n+  )\n+  jt_multiprocess.main()\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Make semaphores compatible with plgpu.kernel and the profiler\n\nThe previous code was overly specific to pl.pallas_call and did not work with\nplgpu.kernel at all. Now, semaphores can be allocated using run_scoped, which also\nhas the interesting side effect of the allocations being collective within\neach program. For a persistent kernel that means that a program/block can communicate\nwith programs/blocks on other devices that have the same program ID, but it is\ncurrently impossible to e.g. synchronize all programs in a grid. We don't have a\nuse case for it now, so we can add it later.\n\nPiperOrigin-RevId: 761885876",
        "output": "```diff\nCommit: 925e705186ccc5412213c42901099511915096a8\nDate: 2025-05-22T10:15:05Z\nURL: https://github.com/jax-ml/jax/commit/925e705186ccc5412213c42901099511915096a8\nFiles changed: 4\nAdditions: +138, Deletions: -66\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex e212f9770a94..2f81ecb969ac 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -105,6 +105,7 @@ class Resources:\n   barrier_counts: collections.Counter[AnyBarrier] = dataclasses.field(\n       default_factory=collections.Counter\n   )\n+  gmem_semaphores: int = 0\n \n   def __post_init__(self):\n     object.__setattr__(\n@@ -132,6 +133,7 @@ def __add__(self, other: Resources) -> Resources:\n         smem_scratch_bytes=self.smem_scratch_bytes + other.smem_scratch_bytes,\n         tmem_scratch_cols=self.tmem_scratch_cols + other.tmem_scratch_cols,\n         barrier_counts=self.barrier_counts + other.barrier_counts,\n+        gmem_semaphores=self.gmem_semaphores + other.gmem_semaphores,\n     )\n \n   def __or__(self, other: Resources) -> Resources:\n@@ -143,6 +145,7 @@ def __or__(self, other: Resources) -> Resources:\n             self.tmem_scratch_cols, other.tmem_scratch_cols\n         ),\n         barrier_counts=self.barrier_counts | other.barrier_counts,\n+        gmem_semaphores=max(self.gmem_semaphores, other.gmem_semaphores),\n     )\n \n \n@@ -266,6 +269,8 @@ def _run_scoped_resource_estimator(\n     elif aval.memory_space == gpu_core.REGS:\n       # Don't need to allocate anything.\n       pass\n+    elif aval.memory_space == gpu_core.GMEM and jnp.issubdtype(aval.dtype, pallas_core.semaphore):\n+      rs += Resources(gmem_semaphores=math.prod(aval.shape))\n     else:\n       raise NotImplementedError(\n           f\"Unsupported memory space: {aval.memory_space}\")\n@@ -312,6 +317,8 @@ class ModuleContext:\n   tmem_requested_cols: int\n   tmem_used_cols: int\n   tmem_base_ptr: ir.Value\n+  gmem_used_semaphores: int\n+  gmem_semaphore_base_ptr: ir.Value | None\n   runtime_barriers: MutableMapping[AnyBarrier, MutableSequence[AnyBarrierRef]]\n   name_stack: source_info_util.NameStack\n   traceback_caches: mlir.TracebackCaches\n@@ -351,6 +358,21 @@ def reserve_barrier(\n     yield barrier\n     available.append(barrier)\n \n+  @contextlib.contextmanager\n+  def reserve_semaphores(\n+      self, shape: tuple[int, ...]\n+  ):\n+    allocated_sems = math.prod(shape)\n+    ref = mgpu.memref_slice(\n+        self.gmem_semaphore_base_ptr,\n+        mgpu.ds(self.gmem_used_semaphores, allocated_sems),\n+    )\n+    ref = mgpu.memref_reshape(ref, shape)\n+    self.gmem_used_semaphores += allocated_sems\n+    yield ref\n+    # TODO: In debug mode verify the values of all semaphores are again 0\n+    self.gmem_used_semaphores -= allocated_sems\n+\n   @contextlib.contextmanager\n   def alloc_tmem(\n       self,\n@@ -640,42 +662,15 @@ def ref_for_aval(aval: jax_core.AbstractValue):\n     else:\n       return gpu_core.SMEM(aval.shape, aval.dtype)\n \n-  sem_placeholder = None\n-  semaphore_ref_avals = []\n-  scratch_avals = []\n-  # Need to unzip semaphores\n-  for v in jaxpr.invars[grid_mapping.slice_scratch_ops]:\n-    aval = v.aval\n-    if (isinstance(aval, pallas_core.AbstractMemoryRef) and\n-        jnp.issubdtype(aval.dtype, pallas_core.semaphore_dtype)):\n-      if aval.memory_space != gpu_core.GMEM:\n-        raise ValueError(\n-            \"Only GMEM memory space is supported for semaphores in Mosaic GPU.\"\n-        )\n-      semaphore_ref_avals.append(aval)\n-      scratch_avals.append(sem_placeholder)\n-    else:\n-      scratch_avals.append(aval)\n-\n   def pipeline_fn(*refs):\n-    sem_refs = []\n-    if semaphore_ref_avals:\n-      refs, sem_refs = util.split_list(refs, [-len(semaphore_ref_avals)])\n     primitives.run_scoped(\n-        functools.partial(scoped_pipeline_fn, *refs, sem_refs=sem_refs),\n-        scratch_refs=[\n-            ref_for_aval(aval) if aval is not sem_placeholder else aval\n-            for aval in scratch_avals\n-        ],\n+        functools.partial(scoped_pipeline_fn, *refs),\n+        scratch_refs=[ref_for_aval(v.aval) for v in jaxpr.invars[grid_mapping.slice_scratch_ops]],\n         collective_axes=thread_axis,  # scratch_refs are shared across threads\n     )\n     return ()  # ``wrap_init`` does not support functions returning None.\n \n-  def scoped_pipeline_fn(*refs, sem_refs, scratch_refs):\n-    sem_refs_it = iter(sem_refs)\n-    scratch_refs = [\n-        next(sem_refs_it) if r is sem_placeholder else r for r in scratch_refs\n-    ]\n+  def scoped_pipeline_fn(*refs, scratch_refs):\n     def body_fn(indices, *refs):\n       program_ids_template = util.merge_lists(\n           which_parallel, indices, [None] * sum(which_parallel)\n@@ -708,7 +703,7 @@ def body_fn(indices, *refs):\n                 bm.array_shape_dtype.shape, bm.array_shape_dtype.dtype\n             ).get_ref_aval()\n             for bm in block_mappings\n-        ] + semaphore_ref_avals,\n+        ],\n     )\n     assert not new_consts\n \n@@ -726,10 +721,6 @@ def body_fn(indices, *refs):\n         gpu_mesh.cluster if gpu_mesh is not None else (),\n         [bm.array_shape_dtype for bm in in_block_mappings],\n         [bm.array_shape_dtype for bm in out_block_mappings],\n-        [\n-            jax.ShapeDtypeStruct(r.shape, np.dtype(np.int32))\n-            for r in semaphore_ref_avals\n-        ],\n         new_jaxpr,\n         params,\n         new_consts,\n@@ -744,7 +735,6 @@ def lower_jaxpr_to_module(\n     cluster: Sequence[int],\n     in_shapes: Sequence[jax.ShapeDtypeStruct],\n     out_shapes: Sequence[jax.ShapeDtypeStruct],\n-    gmem_scratch_shapes: Sequence[jax.ShapeDtypeStruct],\n     jaxpr: jax_core.Jaxpr,\n     params: gpu_core.CompilerParams,\n     consts=(),\n@@ -767,13 +757,31 @@ def lower_jaxpr_to_module(\n     squashed_dims = grid[:-2]\n     parallel_grid = (math.prod(grid[:-2]), *grid[-2:])\n \n+  rs = _estimate_resources(\n+      ResourceEstimatorContext(\n+          axis_names=axis_names, lowering_semantics=lowering_semantics\n+      ),\n+      jaxpr,\n+  )\n+\n   def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n     *buffers_gmem, (runtime_smem, runtime_barriers, runtime_tmem) = buffers\n-    if gmem_scratch_shapes:\n-      in_buffers, _, out_scratch_buffers = util.split_list(\n-          buffers_gmem, [len(in_shapes), len(gmem_scratch_shapes)]\n+    gmem_semaphores = None\n+    if rs.gmem_semaphores:\n+      # Extract the semaphores local to the current block.\n+      index = ir.IndexType.get()\n+      block_idx = arith_dialect.index_castui(index, mgpu_utils.block_idx())\n+      gmem_semaphores = mgpu.memref_slice(\n+          buffers_gmem[-1],\n+          mgpu.ds(\n+              arith_dialect.muli(\n+                  block_idx, arith_dialect.constant(index, rs.gmem_semaphores)\n+              ),\n+              rs.gmem_semaphores,\n+          ),\n       )\n-      buffers_gmem = in_buffers + out_scratch_buffers\n+      # The semaphore buffer is an aliased input/output, so we need to skip it twice.\n+      buffers_gmem = buffers_gmem[:len(in_shapes)] + buffers_gmem[-len(out_shapes) - 1:-1]\n \n     grouped_barriers = collections.defaultdict(list)\n     for barrier, barrier_ref in zip(rs.barriers, runtime_barriers):\n@@ -804,6 +812,8 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n         tmem_requested_cols=tmem_cols,\n         tmem_used_cols=0,\n         tmem_base_ptr=runtime_tmem.address if runtime_tmem else None,\n+        gmem_used_semaphores=0,\n+        gmem_semaphore_base_ptr=gmem_semaphores,\n         runtime_barriers=grouped_barriers,\n         name_stack=source_info_util.NameStack(),\n         traceback_caches=mlir.TracebackCaches(),\n@@ -817,13 +827,6 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n         module_ctx, launch_ctx, jaxpr, buffers_gmem, consts\n     )\n \n-  rs = _estimate_resources(\n-      ResourceEstimatorContext(\n-          axis_names=axis_names, lowering_semantics=lowering_semantics\n-      ),\n-      jaxpr,\n-  )\n-\n   scratch_buffers = [\n       jax.ShapeDtypeStruct(shape=[rs.smem_scratch_bytes], dtype=np.int8),\n       rs.barriers,\n@@ -842,14 +845,24 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n     # Each range is 2 events, each event is 4 bytes.\n     prof_spec = mgpu_profiler.ProfilerSpec(params.profile_space * 2 * 4)\n     prof_ctx = ProfilerContext(params.profile_dir, prof_spec)\n+  mgpu_grid = tuple(map(operator.mul, parallel_grid, cluster))\n+  semaphores_shape = ()\n+  if rs.gmem_semaphores:\n+    semaphores_shape = (\n+        jax.ShapeDtypeStruct(\n+            shape=(math.prod(mgpu_grid) * rs.gmem_semaphores,), dtype=np.int32\n+        ),\n+    )\n+  # NOTE: new_out_shapes has out_shapes, then semaphores_shape and\n+  # optionally the profiler buffer.\n   module, new_out_shapes, _, launch_ctx = (\n       mgpu_core._lower_as_gpu_kernel(\n           body,\n-          grid=tuple(map(operator.mul, parallel_grid, cluster)),\n+          grid=mgpu_grid,\n           cluster=cluster,\n           block=block,\n-          in_shapes=(*in_shapes, *gmem_scratch_shapes),\n-          out_shape=(*out_shapes, *gmem_scratch_shapes),\n+          in_shapes=(*in_shapes, *semaphores_shape),\n+          out_shape=(*out_shapes, *semaphores_shape),\n           smem_scratch_shape=scratch_buffers,\n           lowering_semantics=lowering_semantics,\n           module_name=mlir.sanitize_name(debug_info.func_name),\n@@ -871,11 +884,8 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n \n   launch_ctx.scratch.finalize_size()\n \n-  if gmem_scratch_shapes:\n-    new_out_shapes = new_out_shapes[:-len(gmem_scratch_shapes)]\n-\n   return LoweringResult(\n-      module, parallel_grid, block, new_out_shapes, prof_ctx, tuple(gmem_scratch_shapes)\n+      module, parallel_grid, block, new_out_shapes, prof_ctx, semaphores_shape\n   )\n \n \n@@ -2283,6 +2293,12 @@ def _run_scoped_lowering_rule(\n         )\n         input_refs.append(input_ref)\n         should_discharge.append(False)\n+      elif aval.memory_space == gpu_core.GMEM and jnp.issubdtype(aval.dtype, pallas_core.semaphore):\n+        input_ref = alloc_stack.enter_context(\n+            ctx.module_ctx.reserve_semaphores(aval.shape)\n+        )\n+        input_refs.append(input_ref)\n+        should_discharge.append(False)\n       else:\n         raise ValueError(f\"Can't convert to ref: {aval}\")\n \ndiff --git a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\nindex ef1ba37f0f5c..a14ccbb7daa9 100644\n--- a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n+++ b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n@@ -85,12 +85,16 @@ def pallas_call_lowering(\n   new_avals_out = list(map(_as_shaped_array, lowering_result.new_out_shapes))\n   scratch_args = ()\n   if lowering_result.gmem_scratch_shapes:\n+    # The new_out_shapes contain the original outputs first, followed by the\n+    # GMEM scratch shapes, and optionally the profiler buffer.\n     input_output_aliases += tuple(\n-        (len(new_avals_in) + i, len(new_avals_out) + i)\n+        (len(ctx.avals_in) + i, len(ctx.avals_out) + i)\n         for i in range(len(lowering_result.gmem_scratch_shapes))\n     )\n+    # The GMEM scratch is an aliased kernel input/output.\n     new_avals_in.extend(map(_as_shaped_array, lowering_result.gmem_scratch_shapes))\n-    new_avals_out.extend(map(_as_shaped_array, lowering_result.gmem_scratch_shapes))\n+    # We guarantee zero-initialization of the GMEM scratch at the moment, which\n+    # is important for semaphores.\n     def zero_init_gmem_scratch():\n       return [lax.zeros_like_array(s) for s in lowering_result.gmem_scratch_shapes]\n     scratch_args = mlir.lower_fun(\n@@ -100,12 +104,10 @@ def zero_init_gmem_scratch():\n       ctx.replace(avals_in=new_avals_in, avals_out=new_avals_out),\n       *args, *scratch_args,\n       module=module,\n-      out_types=(*lowering_result.new_out_shapes, *lowering_result.gmem_scratch_shapes),\n+      out_types=lowering_result.new_out_shapes,\n       input_output_aliases=input_output_aliases,\n       use_custom_barrier=False, # False until we add get_barrier_semaphore() feature\n   )\n-  if lowering_result.gmem_scratch_shapes:  # Drop the GMEM scratch.\n-    outs = outs[:-len(lowering_result.gmem_scratch_shapes)]\n   if (prof_ctx := lowering_result.profiler_context) is not None:\n     *outs, prof_buffer = outs\n     if (dump_path := prof_ctx.dump_path) == \"sponge\":\n@@ -133,6 +135,8 @@ def do_callback(prof_buffer):\n     mlir.lower_fun(do_callback, multiple_results=True)(\n         ctx.replace(avals_in=(new_avals_out[-1],)), prof_buffer\n     )\n+  if lowering_result.gmem_scratch_shapes:  # Drop the GMEM scratch.\n+    outs = outs[:-len(lowering_result.gmem_scratch_shapes)]\n   return outs\n \n \ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex 9eedc3402579..1e20675f7909 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -226,15 +226,19 @@ def when(cond):\n     scf.yield_([])\n \n \n-def thread_idx():\n+def _3d_to_1d_idx(dim_idx_fn, dim_size_fn):\n   i32 = ir.IntegerType.get_signless(32)\n   as_i32 = lambda x: arith.index_cast(i32, x)\n-  tidx = as_i32(gpu.thread_id(gpu.Dimension.x))\n-  stride = as_i32(gpu.block_dim(gpu.Dimension.x))\n+  idx = as_i32(dim_idx_fn(gpu.Dimension.x))\n+  stride = as_i32(dim_size_fn(gpu.Dimension.x))\n   for dim in (gpu.Dimension.y, gpu.Dimension.z):\n-    tidx = arith.addi(tidx, arith.muli(as_i32(gpu.thread_id(dim)), stride))\n-    stride = arith.muli(stride, as_i32(gpu.block_dim(dim)))\n-  return tidx\n+    idx = arith.addi(idx, arith.muli(as_i32(dim_idx_fn(dim)), stride))\n+    stride = arith.muli(stride, as_i32(dim_size_fn(dim)))\n+  return idx\n+\n+\n+thread_idx = functools.partial(_3d_to_1d_idx, gpu.thread_id, gpu.block_dim)\n+block_idx = functools.partial(_3d_to_1d_idx, gpu.block_id, gpu.grid_dim)\n \n \n def _warp_bcast(val, lane_idx=0):\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex aedd79e23194..7173639b879f 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -3394,7 +3394,10 @@ def compute(_, l_smem, r_smem, o_smem):\n \n     np.testing.assert_allclose(kernel(x, x), x + x)\n \n-  def test_semaphore_lowering(self):\n+\n+class SemaphoreTest(PallasTest):\n+\n+  def test_lowering(self):\n     # This is a smoke test until we add support for lowering of semaphore ops.\n     def body(i_ref1, i_ref2, o_ref, sem_ref):\n       del i_ref2  # Only here to have a different number of inputs and outputs.\n@@ -3420,6 +3423,51 @@ def body(i_ref1, i_ref2, o_ref, sem_ref):\n         text,\n     )\n \n+  def test_basic(self):\n+    def body(o_ref, sem_ref):\n+      assert jnp.issubdtype(sem_ref.dtype, pl.semaphore)\n+      pl.semaphore_signal(sem_ref)\n+      o_ref[...] = jnp.ones_like(o_ref)\n+      pl.semaphore_wait(sem_ref)\n+    kernel = plgpu.kernel(\n+        body,\n+        out_shape=jax.ShapeDtypeStruct((128,), jnp.float32),\n+        scratch_shapes=[plgpu.SemaphoreType.REGULAR],\n+        grid=(2,),\n+        grid_names=(\"x\",),\n+    )\n+    text = jax.jit(kernel).lower().as_text()\n+    np.testing.assert_array_equal(kernel(), jnp.ones((128,), jnp.float32))\n+    # The semaphore array is scaled up by the grid size.\n+    self.assertIn(\n+        r\"(tensor<128xf32>, tensor<2xi32>) -> (tensor<128xf32>, tensor<2xi32>)\",\n+        text,\n+    )\n+\n+  def test_with_profiler(self):\n+    # Dealing with profiler and semaphores together is tricky because they both\n+    # add extra outputs to the HLO op.\n+    def body(o_ref, sem_ref):\n+      assert jnp.issubdtype(sem_ref.dtype, pl.semaphore)\n+      with jax.named_scope(\"output\"):\n+        o_ref[...] = jnp.ones_like(o_ref)\n+    with tempfile.TemporaryDirectory() as tmp_dir:\n+      kernel = plgpu.kernel(\n+          body,\n+          out_shape=jax.ShapeDtypeStruct((128,), jnp.float32),\n+          scratch_shapes=[plgpu.SemaphoreType.REGULAR],\n+          grid=(2,),\n+          grid_names=(\"x\",),\n+          compiler_params=plgpu.CompilerParams(profile_space=32, profile_dir=tmp_dir),\n+      )\n+      text = jax.jit(kernel).lower().as_text()\n+      np.testing.assert_array_equal(kernel(), jnp.ones((128,), jnp.float32))\n+    self.assertIn(\n+        r\"(tensor<128xf32>, tensor<2xi32>) ->\"\n+        r\" (tensor<128xf32>, tensor<2xi32>, tensor<512xui32>)\",\n+        text,\n+    )\n+\n \n class ExamplesWGTest(\n     ExamplesTest, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas/Mosaic GPU] Add a Pallas:MGPU implementation of `ragged_dot`.\n\nPiperOrigin-RevId: 761875827",
        "output": "```diff\nCommit: c670a2803897b06f86952a705ab93709bab1fb6f\nDate: 2025-05-22T09:37:46Z\nURL: https://github.com/jax-ml/jax/commit/c670a2803897b06f86952a705ab93709bab1fb6f\nFiles changed: 3\nAdditions: +478, Deletions: -0\ndiff --git a/jax/experimental/pallas/ops/gpu/ragged_dot_mgpu.py b/jax/experimental/pallas/ops/gpu/ragged_dot_mgpu.py\nnew file mode 100644\nindex 000000000000..6d295a36f435\n--- /dev/null\n+++ b/jax/experimental/pallas/ops/gpu/ragged_dot_mgpu.py\n@@ -0,0 +1,327 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Ragged dot Pallas-Mosaic-GPU implementation.\"\"\"\n+\n+import dataclasses\n+import functools\n+import itertools\n+import math\n+import jax\n+from jax import lax\n+from jax import numpy as jnp\n+from jax import random\n+from jax._src import test_util as jtu  # noqa: F401\n+from jax.experimental import pallas as pl\n+from jax.experimental.mosaic.gpu import profiler\n+from jax.experimental.pallas import mosaic_gpu as plgpu\n+import numpy as np\n+\n+\n+@dataclasses.dataclass(frozen=True)\n+class GroupInfo:\n+  \"\"\"Information regarding the group being processed in a block.\"\"\"\n+\n+  group_id: jax.Array\n+  block: jax.Array\n+  block_start: jax.Array\n+  actual_start: jax.Array\n+  actual_end: jax.Array\n+  start_within_block: jax.Array\n+  actual_size: jax.Array\n+\n+  @classmethod\n+  def create(cls, group_lengths, tile, tid):\n+    \"\"\"Get the group info for the current block.\"\"\"\n+\n+    tile = jnp.int32(tile)\n+    group_boundaries = [group_lengths[i] for i in range(group_lengths.shape[0])]\n+\n+    # We usually only have very few groups, so we unroll the loop processing\n+    # them. Normally we'd break out of the loop early, once we'd have found our\n+    # boundary, but we can't do that when unrolling, so we rely on many selects\n+    # to mask out the epilogue of the loop.\n+    group_end = group_start = block = group = end = jnp.array(\n+        0, dtype=jnp.int32\n+    )\n+\n+    for i, b in enumerate(group_boundaries):\n+      # Start/end are inclusive\n+      start = end\n+      end = start + b\n+      final = end - 1\n+      start_block = lax.div(start, tile)\n+      final_block = lax.div(final, tile)\n+      block_end = final_block + 1\n+      tid_begin = start_block + i\n+      tid_end = block_end + i\n+      # How many blocks after is our block?\n+      this_is_group = (tid_begin <= tid) & (tid < tid_end)\n+      block = lax.select(this_is_group, tid - tid_begin + start_block, block)\n+      group = lax.select(this_is_group, jnp.int32(i), group)\n+      group_start = lax.select(this_is_group, start, group_start)\n+      group_end = lax.select(this_is_group, end, group_end)\n+\n+    block_start = block * tile\n+    actual_start = jnp.maximum(group_start, block_start)\n+    actual_end = jnp.minimum(group_end, block_start + tile)\n+    start_within_block = actual_start - block_start\n+    actual_size = actual_end - actual_start\n+    return cls(\n+        group_id=group,\n+        block=block,\n+        block_start=block_start,\n+        actual_start=actual_start,\n+        actual_end=actual_end,\n+        start_within_block=start_within_block,\n+        actual_size=actual_size,\n+    )\n+\n+\n+def _find_swizzle(dim_size_bits: int, what: str):\n+  for swizzle_bytes in (128, 64, 32, 16):\n+    if dim_size_bits % (swizzle_bytes * 8) == 0:\n+      return swizzle_bytes\n+  raise ValueError(\n+      f\"No valid out swizzle for {what}: its minor dimension has\"\n+      f\" {dim_size_bits} bits, which is not a multiple of 128\"\n+  )\n+\n+\n+def ragged_dot(\n+    lhs,  # (M, K)\n+    rhs,  # (G, K, N)\n+    *,\n+    group_sizes,  # (G,)\n+    block_m: int,\n+    block_n: int,\n+    block_k: int,\n+    max_concurrent_steps: int,\n+    grid_block_n: int,\n+) -> jax.Array:\n+  if lhs.dtype != rhs.dtype:\n+    raise NotImplementedError(\n+        f\"lhs and rhs must have the same dtype, got {lhs.dtype} and {rhs.dtype}\"\n+    )\n+\n+  elem_bits = jnp.finfo(lhs.dtype).bits\n+  swizzle = _find_swizzle(elem_bits * block_k, \"lhs\")\n+  swizzle_elems = swizzle * 8 // elem_bits\n+\n+  m, k = lhs.shape\n+  g, k2, n = rhs.shape\n+\n+  if group_sizes.shape[0] != g:\n+    raise ValueError(\n+        f\"Expected group_sizes to have shape {g} but got {group_sizes.shape}\"\n+    )\n+\n+  if k != k2:\n+    raise ValueError(f\"lhs.shape={k} must match rhs.shape={k2}\")\n+\n+  if k % block_k != 0:\n+    raise ValueError(f\"k={k} must be a multiple of block_k={block_k}\")\n+\n+  def body(rows_per_expert_gmem, lhs_gmem, rhs_gmem, o_gmem):\n+    grid = (\n+        grid_block_n,\n+        pl.cdiv(m, block_m) + g - 1,\n+        pl.cdiv(n, grid_block_n * block_n),\n+    )\n+\n+    @functools.partial(\n+        plgpu.nd_loop, grid, init_val=None, collective_axes=\"sm\"\n+    )\n+    def mn_loop(idx, _):  # pylint: disable=unused-variable\n+      block_ni, mi, remainder_ni = idx\n+      ni = block_ni * pl.cdiv(n, block_n * grid_block_n) + remainder_ni\n+      group_info = GroupInfo.create(rows_per_expert_gmem, block_m, mi)\n+\n+      def acc_scope(acc_ref):\n+        transforms = (\n+            plgpu.TilingTransform((8, swizzle_elems)),\n+            plgpu.SwizzleTransform(swizzle),\n+        )\n+        plgpu.emit_pipeline(\n+            lambda _, lhs_smem, rhs_smem: plgpu.wgmma(acc_ref, lhs_smem, rhs_smem),\n+            grid=(k // block_k,),\n+            in_specs=[\n+                plgpu.BlockSpec(\n+                    (block_m, block_k),\n+                    lambda k: (group_info.block, k),\n+                    transforms=transforms,\n+                ),\n+                plgpu.BlockSpec(\n+                    (block_k, block_n), lambda k: (k, ni), transforms=transforms\n+                ),\n+            ],\n+            max_concurrent_steps=max_concurrent_steps,\n+            delay_release=1,\n+        )(lhs_gmem, rhs_gmem.at[group_info.group_id])\n+        return acc_ref[...]\n+\n+      acc = pl.run_scoped(acc_scope, plgpu.ACC((block_m, block_n)))\n+\n+      store_transforms = (\n+          plgpu.TilingTransform((1, swizzle_elems)),\n+          plgpu.SwizzleTransform(swizzle)\n+      )\n+      @functools.partial(\n+          pl.run_scoped,\n+          o_smem=plgpu.SMEM(\n+              (block_m, block_n),\n+              dtype=o_gmem.dtype,\n+              transforms=store_transforms,\n+          )\n+      )\n+      def store_scope(o_smem):  # pylint: disable=unused-variable\n+        o_smem[...] = acc.astype(o_smem.dtype)\n+        plgpu.commit_smem()\n+\n+        smem_start = group_info.start_within_block\n+        remaining_rows = min(block_m, m)\n+        # TMA descriptors need to be generated with static tile sizes along each\n+        # axis, but we do not know at compile time how many rows we will need to\n+        # store. We only know that the number of rows to store is bounded by\n+        # min(block_m, m).\n+        #\n+        # In order to work around that, we construct a logarithmic ladder of\n+        # TMA descriptors, where each descriptor can store 2**i rows for some\n+        # i between 0 and log2(min(block_m, m)). This allows storing any\n+        # number of rows we will need to store, so long as this number of rows\n+        # is between `1` and `min(block_m, m)`.\n+        #\n+        # E.g., imagine we have block_m = 8, m = 16. The loop below will be\n+        # unrolled into 4 iterations, where the first one will generate a TMA\n+        # descriptor that can store 8 rows, the second one will generate a TMA\n+        # descriptor that can store 4 rows, etc. all the way to 1 row.\n+        #\n+        # At run time, we finally know the actual number of rows we need to\n+        # store as we go through the unrolled loop iterations. Let's imagine\n+        # that we need to store 5 rows.\n+        #\n+        # The first unrolled iteration will check whether we can store 8 rows.\n+        # Since we only need to store 5 rows, we won't store anything then.\n+        #\n+        # The second unrolled iteration will check whether we can store 4 rows.\n+        # We're able to store 4 rows, and are left with a single remaining row.\n+        #\n+        # The fourth unrolled iteration will store the single remaining row, and\n+        # we end up with a storing scheme as follows for our 5 rows:\n+        #\n+        #     -----------------------------------------------------------\n+        #  0  |                                                         |\n+        #  1  |                                                         |\n+        #  2  |                       Store 4 rows                      |\n+        #  3  |                                                         |\n+        #     -----------------------------------------------------------\n+        #  4  |                       Store 1 row                       |\n+        #     -----------------------------------------------------------\n+        while remaining_rows > 0:\n+          const_rows_len = 1 << int(math.log2(remaining_rows))\n+          remaining_rows //= 2\n+\n+          @pl.when(group_info.actual_size & const_rows_len != 0)\n+          def _():\n+            o_smem_slice = o_smem.at[pl.ds(smem_start, const_rows_len)]\n+            o_gref_slice = o_gmem.at[\n+                pl.ds(group_info.block_start + smem_start, const_rows_len),\n+                pl.ds(ni * block_n, block_n),\n+            ]\n+            plgpu.copy_smem_to_gmem(o_smem_slice, o_gref_slice)\n+\n+          smem_start += group_info.actual_size & const_rows_len\n+        plgpu.wait_smem_to_gmem(0, wait_read_only=True)\n+\n+  # There are 132 SMs on a H100 SXM GPU.\n+  num_sms = 132\n+  kernel = plgpu.kernel(\n+      body,\n+      out_shape=jax.ShapeDtypeStruct((m, n), lhs.dtype),\n+      grid=(num_sms,),\n+      grid_names=(\"sm\",),\n+  )\n+  return kernel(group_sizes, lhs, rhs)\n+\n+\n+def main(unused_argv):\n+  m, k, n, num_groups = 16 * 1024, 2048, 16 * 1024, 16\n+  kx, ky, kz = random.split(random.key(1234), num=3)\n+\n+  lhs = jax.random.normal(kx, (m, k), jnp.float16)\n+  rhs = jax.random.normal(ky, (num_groups, k, n), jnp.float16)\n+  group_boundaries = jax.lax.sort(\n+      jax.random.randint(kz, (num_groups - 1,), 0, m, jnp.int32)\n+  )\n+  group_starts = lax.concatenate(\n+      [jnp.array([0], dtype=jnp.int32), group_boundaries], 0\n+  )\n+  group_ends = lax.concatenate(\n+      [group_boundaries, jnp.array([m], dtype=jnp.int32)], 0\n+  )\n+  group_sizes = group_ends - group_starts\n+  assert group_sizes.shape == (num_groups,)\n+\n+  block_m = block_n = (64, 128, 192)\n+  block_k = (64,)\n+  max_concurrent_steps = (2, 4, 5, 6)\n+  grid_block_n = (1, 2, 4, 8, 16)\n+  configs = itertools.product(\n+      block_m, block_n, block_k, max_concurrent_steps, grid_block_n\n+  )\n+  names = (\n+      \"block_m\", \"block_n\", \"block_k\", \"max_concurrent_steps\", \"grid_block_n\"\n+  )\n+  best_runtime = float(\"inf\")\n+  best_kwargs = {}\n+  for config in configs:\n+    kwargs = dict(zip(names, config))\n+    if n % (kwargs[\"grid_block_n\"] * kwargs[\"block_n\"]):\n+      continue\n+    try:\n+      f = functools.partial(ragged_dot, group_sizes=group_sizes, **kwargs)\n+      _, runtime = profiler.measure(f, mode=\"cupti\")(lhs, rhs)\n+    except ValueError as e:\n+      if \"Mosaic GPU kernel exceeds available shared memory\" not in str(e):\n+        raise\n+      runtime = float(\"inf\")\n+    # Enable this to get more detailed information.\n+    else:\n+      print(\" \".join(f\"{k}={v}\" for k, v in kwargs.items()), int(runtime * 1000))\n+    if runtime < best_runtime:  # pytype: disable=unsupported-operands\n+      best_runtime = runtime\n+      best_kwargs = kwargs\n+  if not best_kwargs:\n+    raise ValueError(\"No valid configuration found\")\n+\n+  ref, ref_runtime = profiler.measure(jax.lax.ragged_dot)(\n+      lhs, rhs, group_sizes=group_sizes\n+  )\n+  result = ragged_dot(lhs, rhs, group_sizes=group_sizes, **best_kwargs)\n+  np.testing.assert_allclose(result, ref, atol=1e-3, rtol=1e-3)\n+\n+  tflops = float(2 * k * m * n) / (best_runtime / 1e3) / 1e12\n+  ref_tflops = float(2 * k * m * n) / (ref_runtime / 1e3) / 1e12\n+  print(\n+      \"Best parameters: \", \" \".join(f\"{k}={v}\" for k, v in best_kwargs.items())\n+  )\n+  print(f\"Kernel:    {best_runtime * 1000:.1f} us = {tflops:.1f} TFLOPS\")\n+  print(f\"Reference: {ref_runtime * 1000:.1f} us = {ref_tflops:.1f} TFLOPS\")\n+\n+\n+if __name__ == \"__main__\":\n+  from absl import app\n+\n+  jax.config.config_with_absl()\n+  app.run(main)\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex d7df261a1ca9..c45e52b1fe88 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -805,6 +805,43 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"mgpu_ragged_dot_run\",\n+    srcs = [\"//jax/experimental/pallas/ops/gpu:ragged_dot_mgpu.py\"],\n+    enable_backends = [],\n+    enable_configs = [\n+        \"gpu_h100_x32\",\n+        \"gpu_h100\",\n+    ],\n+    env = {\"XLA_FLAGS\": \"--xla_gpu_autotune_level=0\"},\n+    tags = [\n+        \"manual\",\n+        \"notap\",\n+    ],\n+    deps = [\n+        \"//jax:pallas\",\n+        \"//jax:pallas_mosaic_gpu\",\n+    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+)\n+\n+jax_multiplatform_test(\n+    name = \"mgpu_ragged_dot_test\",\n+    srcs = [\"mgpu_ragged_dot_test.py\"],\n+    enable_backends = [],\n+    enable_configs = [\n+        \"gpu_h100\",\n+    ],\n+    shard_count = 12,\n+    deps = [\n+        \"//jax:pallas\",\n+        \"//jax:pallas_experimental_gpu_ops\",\n+        \"//jax:pallas_mosaic_gpu\",\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n+)\n+\n jax_multiplatform_test(\n     name = \"fuser_block_spec_test\",\n     srcs = [\ndiff --git a/tests/pallas/mgpu_ragged_dot_test.py b/tests/pallas/mgpu_ragged_dot_test.py\nnew file mode 100644\nindex 000000000000..e9137df1298a\n--- /dev/null\n+++ b/tests/pallas/mgpu_ragged_dot_test.py\n@@ -0,0 +1,114 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Test different parameterizations of our Mosaic GPU ragged dot kernel.\"\"\"\n+\n+import contextlib\n+import os\n+\n+from absl.testing import absltest, parameterized  # pylint: disable=g-multiple-import\n+from jax import random\n+from jax._src import config\n+from jax._src import test_util as jtu\n+from jax._src.pallas import pallas_call\n+import jax.numpy as jnp\n+import numpy as np\n+\n+# pylint: disable=g-import-not-at-top\n+try:\n+  # We only import this to see if Mosaic is available.\n+  import jax.experimental.mosaic.gpu  # noqa: F401\n+except ImportError:\n+  ragged_dot = None\n+else:\n+  from jax.experimental.pallas.ops.gpu import ragged_dot_mgpu\n+\n+\n+config.parse_flags_with_absl()\n+\n+\n+@jtu.with_config(jax_traceback_filtering=\"off\")\n+class RaggedDotTestCase(jtu.JaxTestCase):\n+\n+  def setUp(self):\n+    super().setUp()\n+    if ragged_dot_mgpu is None:\n+      self.skipTest(\"Mosaic GPU not available.\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_equal(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability sm90a\")\n+    context_stack = contextlib.ExitStack()\n+    context_stack.enter_context(pallas_call._PALLAS_USE_MOSAIC_GPU(True))\n+    self.addCleanup(context_stack.close)\n+\n+  @parameterized.product(\n+      block_m=(64, 128, 192),\n+      block_n=(64, 128, 192),\n+      block_k=(64, 128),\n+      grid_block_n=(2, 4),\n+      max_concurrent_steps=(2, 4),\n+      num_groups=(1, 3, 16),\n+  )\n+  def test_ragged_dot(\n+      self,\n+      block_m,\n+      block_n,\n+      block_k,\n+      grid_block_n,\n+      max_concurrent_steps,\n+      num_groups,\n+  ):\n+    dtype = jnp.float16\n+    lhs_smem_size = block_m * block_k * max_concurrent_steps * 2\n+    rhs_smem_size = block_k * block_n * max_concurrent_steps * 2\n+    # H100 SMEM limit is 228kB.\n+    if lhs_smem_size + rhs_smem_size > 228_000:\n+      self.skipTest(\"This configuration requires too much SMEM.\")\n+\n+    m, k, n = 16 * 1024, 2048, 16 * 1024\n+    kx, ky, kz = random.split(random.key(1234), num=3)\n+\n+    lhs = jax.random.normal(kx, (m, k), dtype)\n+    rhs = jax.random.normal(ky, (num_groups, k, n), dtype)\n+    group_boundaries = jax.lax.sort(\n+        jax.random.randint(kz, (num_groups - 1,), 0, m, jnp.int32)\n+    )\n+    group_starts = jax.lax.concatenate(\n+        [jnp.array([0], dtype=jnp.int32), group_boundaries], 0\n+    )\n+    group_ends = jax.lax.concatenate(\n+        [group_boundaries, jnp.array([m], dtype=jnp.int32)], 0\n+    )\n+    group_sizes = group_ends - group_starts\n+    assert group_sizes.shape == (num_groups,)\n+\n+    out = ragged_dot_mgpu.ragged_dot(\n+        lhs,\n+        rhs,\n+        group_sizes=group_sizes,\n+        block_m=block_m,\n+        block_n=block_n,\n+        block_k=block_k,\n+        max_concurrent_steps=max_concurrent_steps,\n+        grid_block_n=grid_block_n,\n+    )\n+    out_ref = jax.lax.ragged_dot(lhs, rhs, group_sizes=group_sizes)\n+    np.testing.assert_allclose(out, out_ref, atol=1e-3, rtol=1e-3)\n+\n+\n+if __name__ == \"__main__\":\n+  os.environ[\"XLA_FLAGS\"] = (\n+      os.environ.get(\"XLA_FLAGS\", \"\") + \" --xla_gpu_autotune_level=0\"\n+  )\n+  absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas] Simulate multiple cores per device when interpreting kernels on CPU.\n\nThe `TPUInterpreterParams` are extended with a field `num_cores_per_device` that specifies how many cores should be simulated per (TPU) device.\nPer-core devices are mapped along grid dimensions with _parallel_ dimension semantics.\n(I.e. the body of the grid loop is considered to execute on the next device-local core if the multi-dimensional loop index has changed along a _parallel_ grid dimension from one loop iteration to the next.)\n\nEach per-device core is identified by a `local_core_id`. Globally, each core is identified by a `global_core_id` (= `device_id` * `num_cores_per_device` + `local_core_id`).\nThere is one vector clock per core (previously, one per device), and each vector clock has as many entries as there are cores in total (= `num_device` * `num_cores_per_device`).\n\nPiperOrigin-RevId: 761872013",
        "output": "```diff\nCommit: 012f9b2677cbe84053dd5a9a71ab1aba349a8973\nDate: 2025-05-22T09:25:26Z\nURL: https://github.com/jax-ml/jax/commit/012f9b2677cbe84053dd5a9a71ab1aba349a8973\nFiles changed: 2\nAdditions: +713, Deletions: -197\ndiff --git a/jax/_src/pallas/mosaic/interpret.py b/jax/_src/pallas/mosaic/interpret.py\nindex c0e52f54e6f3..4258e52e6541 100644\n--- a/jax/_src/pallas/mosaic/interpret.py\n+++ b/jax/_src/pallas/mosaic/interpret.py\n@@ -99,6 +99,8 @@ class TPUInterpretParams:\n       intended for inspecting the randomization of coordinates along grid\n       dimensions with 'parallel' semantics.\n       Default: None.\n+    num_cores_per_device: The number of cores per device.\n+      Default: 1.\n   \"\"\"\n   dma_execution_mode: Literal[\"eager\", \"on_wait\"] = \"on_wait\"\n   detect_races: bool = False\n@@ -106,6 +108,7 @@ class TPUInterpretParams:\n   uninitialized_memory: Literal[\"nan\", \"zero\"] = \"nan\"\n   random_seed: int | None = None\n   grid_point_recorder: Callable[[tuple[jnp.int32, ...]], None] | None = None\n+  num_cores_per_device: int = 1\n \n \n VectorClock = np.ndarray\n@@ -115,11 +118,12 @@ class TPUInterpretParams:\n # of DMAs.\n #\n # Instead, we use approximate vector clocks of fixed size.  We assign each DMA\n-# a virtual device ID in the range [num_devices + 1, NUM_VIRTUAL_DEVICES] --\n+# a virtual core ID in the range\n+#   [num_devices*num_cores_per_device + 1, NUM_VIRTUAL_CORES],\n # and each operation of a DMA increments the corresponding coordinate in its\n-# vector clock.  (So the \"virtual\" part of a vector clock is effectively\n-# counting, for each virtual device, the number of DMAs that happened-before\n-# the vector clock and were assigned to that virtual device.)\n+# vector clock. (So the \"virtual\" part of a vector clock is effectively\n+# counting, for each virtual core, the number of DMAs that happened-before\n+# the vector clock and were assigned to that virtual core.)\n #\n # If two approximate clocks are unordered, then their corresponding events are\n # not ordered by the happens-before relation.  So this approximation will not\n@@ -128,11 +132,11 @@ class TPUInterpretParams:\n # clocks are ordered, and we will treat the corresponding events as ordered\n # by the happens-before relation, but the corresponding events are not\n # actually ordered.\n-NUM_VIRTUAL_DEVICES = 32\n+NUM_VIRTUAL_CORES = 32\n \n-def make_vector_clock(num_devices: int) -> VectorClock:\n-  del num_devices\n-  return np.zeros(NUM_VIRTUAL_DEVICES, dtype=np.int32)\n+def make_vector_clock(_: int) -> VectorClock:\n+  del _\n+  return np.zeros(NUM_VIRTUAL_CORES, dtype=np.int32)\n \n def copy_vector_clock(x: VectorClock) -> VectorClock:\n   if x is None:\n@@ -140,7 +144,7 @@ def copy_vector_clock(x: VectorClock) -> VectorClock:\n   return x.copy()\n \n def update_vector_clock(x: VectorClock, y: VectorClock):\n-  x[:] = np.maximum(x, y)\n+  x[:] = np.maximum(x[:], y[:])\n \n def lt(x: VectorClock, y: VectorClock) -> bool:\n   return bool((x <= y).all() & (x < y).any())\n@@ -148,11 +152,17 @@ def lt(x: VectorClock, y: VectorClock) -> bool:\n def ordered(x: VectorClock, y: VectorClock) -> bool:\n   return lt(x, y) | lt(y, x)\n \n-def inc_vector_clock(x: VectorClock, device_id: int):\n-  if device_id >= len(x):\n-    raise ValueError(f'device_id={device_id} is out of range for x={x}')\n-  assert device_id < len(x)\n-  x[device_id] += 1\n+def inc_vector_clock(x: VectorClock, global_core_id: int):\n+  if global_core_id >= len(x):\n+    raise ValueError(f'device_id={global_core_id} is out of range for x={x}')\n+  assert global_core_id < len(x)\n+  x[global_core_id] += 1\n+\n+def _get_global_core_id(device_id, local_core_id):\n+  \"\"\"Computes the global core ID from the given device and local core ID.\"\"\"\n+  device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n+  return device_id * _get_shared_memory().num_cores_per_device + local_core_id\n \n \n class Semaphore:\n@@ -165,45 +175,45 @@ def __init__(self, semaphore_id=None):\n     # easier to do when we're using single integer device IDs.)\n     self.cv = threading.Condition()\n \n-    self.counts = np.zeros(shared_memory.num_devices, dtype=np.int32)\n+    self.counts = np.zeros(shared_memory.num_cores, dtype=np.int32)\n \n     self.interpret_params = shared_memory.interpret_params\n     if self.interpret_params.detect_races:\n       # We associate a vector clock with each count in self.counts.  Whenever\n       # self.counts[i] is signaled, self.clocks[i] is updated with the vector\n-      # clock of the signaling device.  Whenever device i successfully waits on\n-      # self.counts[i], the vector clock of device i is updated with\n+      # clock of the signaling core.  Whenever core i successfully waits on\n+      # self.counts[i], the vector clock of core i is updated with\n       # self.clocks[i].\n       #\n       # TODO(jburnim): Model happens-before more precisely for the case where\n       # semaphores are over-signaled.\n-      self.clocks = [None] * shared_memory.num_devices\n+      self.clocks = [None] * shared_memory.num_cores\n \n-  def signal(self, inc, device_id, clock):\n-    \"\"\"Signal the semaphore on `device_id` by `inc`.\n+  def signal(self, inc, global_core_id, clock):\n+    \"\"\"Signal the semaphore on `(device_id, core_id)` by `inc`.\n \n     Args:\n       inc: A positive integer.  The amount by which to increment the semaphore\n         on the target device.\n-      device_id: The ID of the target device.\n+      global_core_id: The ID of the target core.\n       clock: The vector clock of the signaling device at the time of the signal.\n     \"\"\"\n-    device_id = int(device_id)\n+    global_core_id = int(global_core_id)\n     with self.cv:\n-      self.counts[device_id] += inc\n+      self.counts[global_core_id] += inc\n       if self.interpret_params.detect_races:\n-        if self.clocks[device_id] is None:\n-          self.clocks[device_id] = copy_vector_clock(clock)\n+        if self.clocks[global_core_id] is None:\n+          self.clocks[global_core_id] = copy_vector_clock(clock)\n         else:\n-          update_vector_clock(self.clocks[device_id], clock)\n+          update_vector_clock(self.clocks[global_core_id], clock)\n       self.cv.notify_all()\n \n-  def read(self, device_id):\n+  def read(self, global_core_id):\n     with self.cv:\n-      return self.counts[device_id]\n+      return self.counts[global_core_id]\n \n-  def wait(self, value, device_id, *, is_dma=False):\n-    device_id = int(device_id)\n+  def wait(self, value, global_core_id, *, is_dma=False):\n+    global_core_id = int(global_core_id)\n     shared_memory = _get_shared_memory()\n \n     # TODO(jburnim):\n@@ -214,14 +224,14 @@ def wait(self, value, device_id, *, is_dma=False):\n     # Simple implementation for non-DMA semaphores.\n     if not is_dma or (self.interpret_params.dma_execution_mode == \"eager\"):\n       with self.cv:\n-        while self.counts[device_id] < value:\n+        while self.counts[global_core_id] < value:\n           self.cv.wait()\n-        self.counts[device_id] -= value\n+        self.counts[global_core_id] -= value\n         if self.interpret_params.detect_races:\n-          clock = copy_vector_clock(self.clocks[device_id])\n+          clock = copy_vector_clock(self.clocks[global_core_id])\n       if self.interpret_params.detect_races:\n         with shared_memory.lock:\n-          update_vector_clock(shared_memory.clocks[device_id], clock)\n+          update_vector_clock(shared_memory.clocks[global_core_id], clock)\n       return\n \n     # For DMA semaphores (when dma_execution_mode=='on_wait'), while our count\n@@ -235,15 +245,15 @@ def wait(self, value, device_id, *, is_dma=False):\n     while True:\n       clock = None\n       with self.cv:\n-        if self.counts[device_id] >= value:\n-          self.counts[device_id] -= value\n+        if self.counts[global_core_id] >= value:\n+          self.counts[global_core_id] -= value\n           if self.interpret_params.detect_races:\n-            clock = copy_vector_clock(self.clocks[device_id])\n+            clock = copy_vector_clock(self.clocks[global_core_id])\n           else:\n             return\n       if clock is not None:\n         with shared_memory.lock:\n-          update_vector_clock(shared_memory.clocks[device_id], clock)\n+          update_vector_clock(shared_memory.clocks[global_core_id], clock)\n         return\n \n       with shared_memory.lock:\n@@ -258,25 +268,32 @@ def wait(self, value, device_id, *, is_dma=False):\n       with dma.lock:\n         if dma.virtual_device_id is None:\n           dma.virtual_device_id = np.random.randint(\n-              shared_memory.num_devices, NUM_VIRTUAL_DEVICES)\n+              shared_memory.num_devices, NUM_VIRTUAL_CORES)\n \n         if dma.state == DmaState.STARTED:\n           # Do the read.\n           if self.interpret_params.detect_races:\n             inc_vector_clock(dma.clock, dma.virtual_device_id)\n           dma.data = get(dma.src_device_id,\n+                         dma.src_local_core_id,\n                          dma.src_memory_space,\n                          dma.src_buffer_id,\n                          dma.src_transforms,\n                          clock=copy_vector_clock(dma.clock),\n                          src_device_id=dma.id,\n+                         src_local_core_id=0,\n                          source_info=dma.source_info)\n           if self.interpret_params.detect_races:\n             inc_vector_clock(dma.clock, dma.virtual_device_id)\n           if dma.src_sem is not None:\n             data_size = dma.data.itemsize * dma.data.size\n             dma.src_sem.signal(\n-                data_size, device_id=dma.src_device_id, clock=dma.clock)\n+                data_size,\n+                global_core_id=_get_global_core_id(\n+                    dma.src_device_id, dma.src_local_core_id\n+                ),\n+                clock=dma.clock,\n+            )\n           dma.state = DmaState.READ\n \n         if dma.src_sem is self:\n@@ -290,18 +307,25 @@ def wait(self, value, device_id, *, is_dma=False):\n         if self.interpret_params.detect_races:\n           inc_vector_clock(dma.clock, dma.virtual_device_id)\n         store(dma.dst_device_id,\n+              dma.dst_local_core_id,\n               dma.dst_memory_space,\n               dma.dst_buffer_id,\n               dma.dst_transforms,\n               dma.data,\n               clock=copy_vector_clock(dma.clock),\n               src_device_id=dma.id,\n+              src_local_core_id=0,\n               source_info=dma.source_info)\n         if self.interpret_params.detect_races:\n           inc_vector_clock(dma.clock, dma.virtual_device_id)\n         data_size = dma.data.itemsize * dma.data.size\n         dma.dst_sem.signal(\n-            data_size, device_id=dma.dst_device_id, clock=dma.clock)\n+            data_size,\n+            global_core_id=_get_global_core_id(\n+                dma.dst_device_id, dma.dst_local_core_id\n+            ),\n+            clock=dma.clock,\n+        )\n \n         dma.data = None\n         dma.state = DmaState.COMPLETED\n@@ -317,10 +341,12 @@ class DMA:\n   id: int\n \n   src_device_id: int\n+  src_local_core_id: int\n   src_memory_space: int\n   src_buffer_id: int\n   src_transforms: tuple[Any, ...]\n   dst_device_id: int\n+  dst_local_core_id: int\n   dst_memory_space: int\n   dst_buffer_id: int\n   dst_transforms: tuple[Any, ...]\n@@ -339,13 +365,14 @@ class DMA:\n \n @dataclasses.dataclass\n class RaceDetectionState:\n-  num_devices: int\n+  num_cores: int\n+\n \n-  # (memory_space, buffer_id, device_id) -> [(device_id, VectorClock, range)]\n+  # (memory_space, buffer_id, device_id, local_core_id) -> [(device_id, local_core_id, VectorClock, range)]\n   reads: dict = dataclasses.field(\n       default_factory=lambda: collections.defaultdict(list))\n \n-  # (memory_space, buffer_id, device_id) -> [(device_id, VectorClock, range)]\n+  # (memory_space, buffer_id, device_id, local_core_id) -> [(device_id, local_core_id, VectorClock, range)]\n   writes: dict = dataclasses.field(\n       default_factory=lambda: collections.defaultdict(list))\n \n@@ -387,7 +414,10 @@ def ranges_overlap(range1: tuple[slice | int, ...],\n   return all(slices_overlap(r1, r2) for r1, r2\n              in itertools.zip_longest(range1, range2, fillvalue=slice(None)))\n \n-def check_read(device_id, clock, buffer_key, rnge, source_info=None):\n+\n+def check_read(\n+    device_id, local_core_id, clock, buffer_key, rnge, source_info=None\n+):\n   if source_info is not None:\n     user_frame = source_info_util.summarize(source_info)\n   else:\n@@ -396,24 +426,36 @@ def check_read(device_id, clock, buffer_key, rnge, source_info=None):\n   with races.lock:\n     writes = races.writes[buffer_key]\n     num_writes = len(writes)\n-    races.reads[buffer_key].append((device_id, clock, rnge, user_frame))\n+    races.reads[buffer_key].append(\n+        (device_id, local_core_id, clock, rnge, user_frame)\n+    )\n \n   for i in range(num_writes):\n-    write_device_id, write_clock, write_range, write_frame = writes[i]\n+    (\n+        write_device_id,\n+        write_local_core_id,\n+        write_clock,\n+        write_range,\n+        write_frame,\n+    ) = writes[i]\n     if ordered(write_clock, clock):\n       continue\n     if not ranges_overlap(rnge, write_range):\n       continue\n     # TODO(jburnim): When printing device IDs for reads/writes, distinguish\n     # between real device IDs vs. DMA IDs.\n-    print('RACE DETECTED\\n'\n-          f'  read of {buffer_key}[{rnge}] from {device_id}, {user_frame}\\n'\n-          f'  write of {buffer_key}[{write_range}] from {write_device_id}, {write_frame}')\n+    print(\n+        f'RACE DETECTED\\n  read of {buffer_key}[{rnge}] from {device_id},'\n+        f' {local_core_id}, {user_frame}\\n  write of'\n+        f' {buffer_key}[{write_range}] from {write_device_id},'\n+        f' {write_local_core_id} {write_frame}'\n+    )\n     with races.lock:\n       races.races_found = True\n     return\n \n-def check_write(device_id, clock, buffer_key, rnge, source_info=None):\n+\n+def check_write(device_id, local_core_id, clock, buffer_key, rnge, source_info=None):\n   if source_info is not None:\n     user_frame = source_info_util.summarize(source_info)\n   else:\n@@ -424,37 +466,50 @@ def check_write(device_id, clock, buffer_key, rnge, source_info=None):\n     reads = races.reads[buffer_key]\n     num_writes = len(writes)\n     num_reads = len(reads)\n-    races.writes[buffer_key].append((device_id, clock, rnge, user_frame))\n+    races.writes[buffer_key].append((device_id, local_core_id, clock, rnge, user_frame))\n \n   # TODO(jburnim): For performance, we should also probably remove any\n   # conflicting reads and writes that happened-before the current write.\n \n   for i in range(num_writes):\n-    write_device_id, write_clock, write_range, write_frame = writes[i]\n+    (\n+        write_device_id,\n+        write_local_core_id,\n+        write_clock,\n+        write_range,\n+        write_frame,\n+    ) = writes[i]\n     if ordered(write_clock, clock):\n       continue\n     if not ranges_overlap(rnge, write_range):\n       continue\n     # TODO(jburnim): When printing device IDs for reads/writes, distinguish\n     # between real device IDs vs. DMA IDs.\n-    print('RACE DETECTED\\n'\n-          f'  write of {buffer_key}[{rnge}] from {device_id}, {user_frame}\\n'\n-          f'  write of {buffer_key}[{write_range}] from {write_device_id}, {write_frame}')\n+    print(\n+        f'RACE DETECTED\\n  write of {buffer_key}[{rnge}] from {device_id},'\n+        f' {local_core_id}, {user_frame}\\n  write of'\n+        f' {buffer_key}[{write_range}] from {write_device_id},'\n+        f' {write_local_core_id}, {write_frame}'\n+    )\n     with races.lock:\n       races.races_found = True\n     break\n \n   for i in range(num_reads):\n-    read_device_id, read_clock, read_range, read_frame = reads[i]\n+    read_device_id, read_local_core_id, read_clock, read_range, read_frame = (\n+        reads[i]\n+    )\n     if ordered(read_clock, clock):\n       continue\n     if not ranges_overlap(rnge, read_range):\n       continue\n     # TODO(jburnim): When printing device IDs for reads/writes, distinguish\n     # between real device IDs vs. DMA IDs.\n-    print('RACE DETECTED\\n'\n-          f'  write of {buffer_key}[{rnge}] from {device_id}, {user_frame}\\n'\n-          f'  read of {buffer_key}[{read_range}] from {read_device_id}, {read_frame}')\n+    print(\n+        f'RACE DETECTED\\n  write of {buffer_key}[{rnge}] from {device_id},'\n+        f' {local_core_id}, {user_frame}\\n  read of {buffer_key}[{read_range}]'\n+        f' from {read_device_id}, {read_local_core_id}, {read_frame}'\n+    )\n     with races.lock:\n       races.races_found = True\n     return\n@@ -464,13 +519,14 @@ def check_write(device_id, clock, buffer_key, rnge, source_info=None):\n class SharedMemory:\n   interpret_params: TPUInterpretParams\n   num_devices: int\n+  num_cores_per_device: int\n   clocks: list[VectorClock]\n   barrier: threading.Barrier\n   clean_up_barrier: threading.Barrier\n \n-  # (memory_space, buffer_id, device_id) -> NumPy array\n+  # (memory_space, buffer_id, device_id, local_core_id) -> NumPy array\n   # TODO(jburnim): Handle Megacore.\n-  mem: dict[tuple[int, int, int], np.ndarray] = dataclasses.field(\n+  mem: dict[tuple[str, int, int, int], np.ndarray] = dataclasses.field(\n       default_factory=dict)\n \n   # semaphore_id -> Semaphore\n@@ -478,15 +534,18 @@ class SharedMemory:\n \n   # (semaphore_id, device_id)\n   #   -> list of DMAs that will signal the semaphore on the given device\n+  # TODO(jburnim): Fix uses of `dmas_by_sem` to align with the two lines of\n+  # documentation above, i.e. index `dmas_by_sem` with\n+  # `(semaphore_id, device_id)` (currently indexed with `semaphore_id only).\n   dmas_by_sem: dict[tuple[int, int], list[DMA]] = dataclasses.field(\n       default_factory=lambda: collections.defaultdict(list))\n \n   lock: threading.Lock = dataclasses.field(default_factory=threading.Lock)\n \n-  # device_id -> next buffer ID\n-  next_buffer_id: dict[int, int] = dataclasses.field(\n+  # (device_id, local_core_id) -> next buffer ID\n+  next_buffer_id: dict[tuple[int, int], int] = dataclasses.field(\n       default_factory=lambda: collections.defaultdict(lambda: 100))\n-  # device_id -> next semaphore ID\n+  # global_core_id -> next semaphore ID\n   next_semaphore_id: dict[int, int] = dataclasses.field(\n       default_factory=lambda: collections.defaultdict(lambda: 2000))\n \n@@ -494,6 +553,10 @@ class SharedMemory:\n \n   deallocated_bytes: int = 0\n \n+  @property\n+  def num_cores(self) -> int:\n+    return self.num_devices * self.num_cores_per_device\n+\n \n # TODO(jburnim): Do we want to support multiple instances of SharedMemory?\n # Maybe for running multiple distinct interpreted computations in parallel?\n@@ -510,34 +573,54 @@ def _clear_shared_memory():\n   with _shared_memory_init_lock:\n     _shared_memory = None\n \n-def _initialize_shared_memory(device_id, num_devices, *, interpret_params):\n+\n+def _initialize_shared_memory(\n+    device_id, num_devices, num_cores_per_device, *, interpret_params\n+):\n   global _shared_memory\n   del device_id\n   num_devices = int(num_devices)\n+  num_cores_per_device = int(num_cores_per_device)\n+  num_cores = num_devices * num_cores_per_device\n   with _shared_memory_init_lock:\n     if _shared_memory is None:\n       _shared_memory = SharedMemory(\n           interpret_params=interpret_params,\n           num_devices=num_devices,\n-          clocks=[make_vector_clock(num_devices) for _ in range(num_devices)],\n+          num_cores_per_device=num_cores_per_device,\n+          clocks=[make_vector_clock(num_cores) for _ in range(num_cores)],\n           barrier=threading.Barrier(\n               num_devices, action=_update_clocks_for_global_barrier),\n           clean_up_barrier=threading.Barrier(\n               num_devices, action=_clear_shared_memory))\n-  assert _shared_memory.num_devices == num_devices\n+  assert _shared_memory.num_cores == num_cores\n \n   global races\n-  races = RaceDetectionState(num_devices=num_devices)\n+  races = RaceDetectionState(num_cores=num_cores)\n \n-def _update_clocks_for_global_barrier():\n+def _update_clocks(low_global_core_id, high_global_core_id):\n+  \"\"\"Synchronizes the vector clocks for the cores with ids in the range between the two arguments.\"\"\"\n   shared_memory = _get_shared_memory()\n+  # Despite only updating the vector clocks for some cores, we still need to\n+  # hold the global lock to ensure that no other devices are concurrently\n+  # accessing the same vector clocks.\n   with shared_memory.lock:\n-    # Set the vector clock for device 0 to the max over all device clocks.\n-    for c in shared_memory.clocks[1:]:\n-      update_vector_clock(shared_memory.clocks[0], c)\n-    # Set all other device vector clocks to the max over all the clocks.\n-    for c in shared_memory.clocks[1:]:\n-      update_vector_clock(c, shared_memory.clocks[0])\n+    for c in shared_memory.clocks[low_global_core_id + 1 : high_global_core_id]:\n+      update_vector_clock(shared_memory.clocks[low_global_core_id], c)\n+    for c in shared_memory.clocks[low_global_core_id + 1 : high_global_core_id]:\n+      update_vector_clock(c, shared_memory.clocks[low_global_core_id])\n+\n+def _update_clocks_for_device_barrier(device_id):\n+  \"\"\"Synchronizes the vector clocks for the cores on the given device.\"\"\"\n+  shared_memory = _get_shared_memory()\n+  low_core_id = device_id * shared_memory.num_cores_per_device\n+  high_core_id = (device_id + 1) * shared_memory.num_cores_per_device\n+  _update_clocks(low_core_id, high_core_id)\n+\n+def _update_clocks_for_global_barrier():\n+  \"\"\"Synchronizes all vector clocks.\"\"\"\n+  shared_memory = _get_shared_memory()\n+  _update_clocks(0, shared_memory.num_cores)\n \n def _barrier(device_id):\n   device_id = int(device_id)\n@@ -564,30 +647,80 @@ def _validate(device_id):\n               f'Semaphore {sem.id} has non-zero count for {device_id} at '\n               f'kernel exit: {sem.counts[device_id]}')\n \n-def _allocate_buffer(device_id, memory_space, val):\n+def _allocate_buffer(\n+    device_id: Array,\n+    local_core_id: Array | None,\n+    memory_space: Array,\n+    val: Array,\n+):\n+  \"\"\"Allocates a memory buffer on the device with id `device_id` and core with id `local_core_id`.\n+\n+  Args:\n+    device_id: Singleton array holding the device id where the buffer will be\n+      allocated.\n+    local_core_id: None or singleton array holding the core id where the buffer\n+      will be allocated. If None, a buffer will be allocated on each cores on\n+      the device.\n+    memory_space: Singleton array indicating the memory space to allocate the\n+      buffer in. If the corresponding memory space is \"any\" (i.e. HBM), at most\n+      one buffer will be allocated and it will belong to (local) core id 0.\n+    val: Array of values to initialize the allocated buffer with.\n+\n+  Returns:\n+    Integer id for the allocated buffer.\n+  \"\"\"\n   device_id = int(device_id)\n-  memory_space = TPU_MEMORY_SPACE_NAMES[int(memory_space)]\n+  memory_space_str = TPU_MEMORY_SPACE_NAMES[int(memory_space)]\n+  del memory_space\n   val = np.array(val)\n \n   shared_memory = _get_shared_memory()\n+\n+  if local_core_id is None:\n+    local_core_id_int = 0\n+    local_core_ids = tuple(range(shared_memory.num_cores_per_device))\n+  else:\n+    local_core_id_int = int(local_core_id)\n+    local_core_ids = (local_core_id_int,)\n+  del local_core_id\n+\n+  local_core_id_to_buffer_id = {}\n   with shared_memory.lock:\n-    buffer_id = shared_memory.next_buffer_id[device_id]\n-    shared_memory.next_buffer_id[device_id] = buffer_id + 1\n-    # TODO(jburnim): Add options for initializing memory (e.g., with NaNs,\n-    # with zeros, or with the buffer ID).\n-    shared_memory.mem[(memory_space, buffer_id, device_id)] = val\n+    for lci in local_core_ids:\n+      buffer_id = shared_memory.next_buffer_id[(device_id, lci)]\n+      shared_memory.next_buffer_id[(device_id, lci)] = buffer_id + 1\n+      if lci == 0 or memory_space_str != 'any':\n+        # If allocating in HBM, only actually allocate a buffer for local core\n+        # id 0.\n+        # TODO(jburnim): Add options for initializing memory (e.g., with NaNs,\n+        # with zeros, or with the buffer ID).\n+        shared_memory.mem[(memory_space_str, buffer_id, device_id, lci)] = val\n+\n+      local_core_id_to_buffer_id[lci] = buffer_id\n+\n+  # The buffer ids should always be kept in sync across all cores.\n+  assert all(\n+      buffer_id == local_core_id_to_buffer_id[local_core_id_int]\n+      for buffer_id in local_core_id_to_buffer_id.values()\n+  )\n \n   # TODO(jburnim): Raise an error if buffer_id is too big for int16.\n-  return np.int16(buffer_id)\n+  return np.int16(local_core_id_to_buffer_id[local_core_id_int])\n \n-def _deallocate_buffer(device_id, memory_space, buffer_id):\n+def _deallocate_buffer(device_id, local_core_id, memory_space, buffer_id):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   memory_space = TPU_MEMORY_SPACE_NAMES[int(memory_space)]\n   buffer_id = int(buffer_id)\n \n+  if memory_space == 'any':\n+    local_core_id = 0\n+\n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n-    buff = shared_memory.mem.pop((memory_space, buffer_id, device_id))\n+    buff = shared_memory.mem.pop(\n+        (memory_space, buffer_id, device_id, local_core_id)\n+    )\n     shared_memory.deallocated_bytes += buff.size * buff.itemsize\n     del buff\n \n@@ -600,26 +733,80 @@ def _deallocate_buffer(device_id, memory_space, buffer_id):\n     # why arrays are not getting freed without this.\n     gc.collect()\n \n-def _allocate_semaphores(device_id, shape):\n+\n+def _allocate_semaphores(\n+    device_id: Array, local_core_id: Array | None, shape: Array\n+):\n+  \"\"\"Allocates semaphores on the device with id `device_id` and core with id `local_core_id`.\n+\n+  The number of sempahores allocated is given by the product of the entries in\n+  `shape`.\n+\n+  Since for each semaphore id there is really only one global `Semaphore`\n+  object, 'allocation' of semaphores per device and core here means that the\n+  internal counter of semaphore ids that is held by `SharedMemory` is\n+  incremented for each the device and core (or for all cores on the dive if\n+  argument `local_core_id` is None, see below).\n+\n+  Args:\n+    device_id: Singleton array holding the id for the device where the\n+      semaphores will be allocated.\n+    local_core_id: None or singleton array holding the id for the core where the\n+      semaphores will be allocated. If None, semaphores will be allocated on all\n+      cores on the device.\n+    shape: Shape of the semaphore array to allocate.\n+\n+  Returns:\n+    Array of semaphore ids.\n+  \"\"\"\n   device_id = int(device_id)\n   shape = tuple(map(int, shape))\n   num_semaphores = math.prod(shape)\n \n   shared_memory = _get_shared_memory()\n+\n+  if local_core_id is None:\n+    local_core_id_int = 0\n+    global_core_ids = tuple(\n+        _get_global_core_id(device_id, core_id)\n+        for core_id in range(shared_memory.num_cores_per_device)\n+    )\n+  else:\n+    local_core_id_int = int(local_core_id)\n+    global_core_ids = (_get_global_core_id(device_id, local_core_id_int),)\n+  del local_core_id\n+\n+  global_core_id_to_semaphore_id = {}\n   with shared_memory.lock:\n-    semaphore_id = shared_memory.next_semaphore_id[device_id]\n-    shared_memory.next_semaphore_id[device_id] = semaphore_id + num_semaphores\n-    for i in range(semaphore_id, semaphore_id + num_semaphores):\n-      if i not in shared_memory.sem:\n-        shared_memory.sem[i] = Semaphore(i)\n+    for gci in global_core_ids:\n+      semaphore_id = shared_memory.next_semaphore_id[gci]\n+      shared_memory.next_semaphore_id[gci] = (\n+          semaphore_id + num_semaphores\n+      )\n+\n+      # Ensure that only one global `Semaphore` object is allocated for each\n+      # `semaphore_id`.\n+      for i in range(semaphore_id, semaphore_id + num_semaphores):\n+        if i not in shared_memory.sem:\n+          shared_memory.sem[i] = Semaphore(i)\n+\n+      global_core_id_to_semaphore_id[gci] = semaphore_id\n+\n+  global_core_id = _get_global_core_id(device_id, local_core_id_int)\n+  # The semaphore ids should always be kept in sync across all cores.\n+  assert all(\n+      semaphore_id == global_core_id_to_semaphore_id[global_core_id]\n+      for semaphore_id in global_core_id_to_semaphore_id.values()\n+  )\n \n   # NOTE: For now, we use a relatively uncommon datatype (int16) for\n   # semaphore (and buffer) IDs, so these values are more easily identifiable\n   # in kernels.\n   #\n   # TODO(jburnim): Raise an error if any IDs are too big for int16.\n-  return np.int16(\n-      range(semaphore_id, semaphore_id + num_semaphores)\n+  semaphore_id = global_core_id_to_semaphore_id[global_core_id]\n+  return np.arange(\n+      semaphore_id, semaphore_id + num_semaphores, dtype=np.int16\n   ).reshape(shape)\n \n \n@@ -693,24 +880,48 @@ def _to_range(transforms) -> tuple[slice | int, ...]:\n         ret, tuple(_transform_slice_or_index(i) for i in transform.indices))\n   return ret\n \n-def get(device_id, memory_space, buffer_id, transforms, *,\n-        src_device_id=None, clock=None, source_info=None):\n+def _to_int(x : int | Array | None) -> int | None:\n+  \"\"\"Converts a value to an integer, or returns None if the value is None.\"\"\"\n+  if x is None:\n+    return None\n+  return int(x)\n+\n+def get(\n+    device_id,\n+    local_core_id,\n+    memory_space,\n+    buffer_id,\n+    transforms,\n+    *,\n+    src_device_id=None,\n+    src_local_core_id=None,\n+    clock=None,\n+    source_info=None,\n+):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   memory_space = TPU_MEMORY_SPACE_NAMES[int(memory_space)]\n   buffer_id = int(buffer_id)\n   try:\n     transforms = jax.tree.map(int, transforms)\n   except:\n     raise ValueError('Advanced indexers are not supported on TPU')\n+  src_device_id = _to_int(src_device_id)\n+  src_local_core_id = _to_int(src_local_core_id)\n+\n+  local_core_id_for_buffer = 0 if memory_space == 'any' else local_core_id\n+  global_core_id = _get_global_core_id(device_id, local_core_id)\n \n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n     read_range = _to_range(transforms)\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n+      inc_vector_clock(shared_memory.clocks[global_core_id], global_core_id)\n       if clock is None:\n-        clock = copy_vector_clock(shared_memory.clocks[device_id])\n-    buffer = shared_memory.mem[(memory_space, buffer_id, device_id)]\n+        clock = copy_vector_clock(shared_memory.clocks[global_core_id])\n+    buffer = shared_memory.mem[\n+        (memory_space, buffer_id, device_id, local_core_id_for_buffer)\n+    ]\n     ret = buffer[read_range].copy()\n     if transforms:\n       # TODO(jburnim): Instead of using NDIndexer, do the computation ourselves\n@@ -718,20 +929,43 @@ def get(device_id, memory_space, buffer_id, transforms, *,\n       expected_shape = transforms[-1].get_indexer_shape()\n       if expected_shape != ret.shape[:len(expected_shape)]:\n         raise ValueError(\n-            f'Out-of-bounds read of ({device_id} {memory_space} {buffer_id}): '\n-            f'reading [{read_range}] but bufer has shape {buffer.shape} .')\n+            'Out-of-bounds read of'\n+            f' ({device_id} {local_core_id} {memory_space} {buffer_id}):'\n+            f' reading [{read_range}] but bufer has shape {buffer.shape} .'\n+        )\n \n   if shared_memory.interpret_params.detect_races:\n     if src_device_id is None:\n       src_device_id = device_id\n-    check_read(src_device_id, clock, (memory_space, buffer_id, device_id),\n-               read_range, source_info=source_info)\n+    if src_local_core_id is None:\n+      src_local_core_id = local_core_id\n+    check_read(\n+        src_device_id,\n+        src_local_core_id,\n+        clock,\n+        (memory_space, buffer_id, device_id, local_core_id_for_buffer),\n+        read_range,\n+        source_info=source_info,\n+    )\n \n   return ret\n \n-def store(device_id, memory_space, buffer_id, transforms, val, *,\n-          src_device_id=None, clock=None, source_info=None):\n+\n+def store(\n+    device_id,\n+    local_core_id,\n+    memory_space,\n+    buffer_id,\n+    transforms,\n+    val,\n+    *,\n+    src_device_id=None,\n+    src_local_core_id=None,\n+    clock=None,\n+    source_info=None,\n+):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   memory_space = TPU_MEMORY_SPACE_NAMES[int(memory_space)]\n   buffer_id = int(buffer_id)\n   try:\n@@ -739,38 +973,67 @@ def store(device_id, memory_space, buffer_id, transforms, val, *,\n   except:\n     raise ValueError('Advanced indexers are not supported on TPU')\n   val = np.array(val)\n+  src_device_id = _to_int(src_device_id)\n+  src_local_core_id = _to_int(src_local_core_id)\n+\n+  local_core_id_for_buffer = 0 if memory_space == 'any' else local_core_id\n+  global_core_id = _get_global_core_id(device_id, local_core_id)\n \n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n+      inc_vector_clock(shared_memory.clocks[global_core_id], global_core_id)\n       if clock is None:\n-        clock = copy_vector_clock(shared_memory.clocks[device_id])\n+        clock = copy_vector_clock(shared_memory.clocks[global_core_id])\n \n-    buff = shared_memory.mem[(memory_space, buffer_id, device_id)]\n+    buff = shared_memory.mem[\n+        (memory_space, buffer_id, device_id, local_core_id_for_buffer)\n+    ]\n     assert buff.dtype == val.dtype  # TODO(jburnim): Catch this statically.\n     write_range = _to_range(transforms)\n     # TODO(jburnim): Better error message if this raises?\n     in_bounds_shape = buff[write_range].shape\n     if in_bounds_shape != val.shape:\n       raise ValueError(\n-          f'Out-of-bounds write of ({device_id} {memory_space} {buffer_id}): '\n-          f'writing [{write_range}] but buffer has shape {buff.shape} .')\n+          'Out-of-bounds write of'\n+          f' ({device_id} {local_core_id} {memory_space} {buffer_id}): writing'\n+          f' [{write_range}] but buffer has shape {buff.shape} .'\n+      )\n     buff[write_range] = val\n \n   if shared_memory.interpret_params.detect_races:\n     if src_device_id is None:\n       src_device_id = device_id\n-    check_write(src_device_id, clock, (memory_space, buffer_id, device_id),\n-                write_range, source_info=source_info)\n+    if src_local_core_id is None:\n+      src_local_core_id = local_core_id\n+    check_write(\n+        src_device_id,\n+        src_local_core_id,\n+        clock,\n+        (memory_space, buffer_id, device_id, local_core_id_for_buffer),\n+        write_range,\n+        source_info=source_info,\n+    )\n+\n \n-def swap(device_id, memory_space, buffer_id, transforms, val, mask, *,\n-         source_info=None):\n+def swap(\n+    device_id,\n+    local_core_id,\n+    memory_space,\n+    buffer_id,\n+    transforms,\n+    val,\n+    mask,\n+    *,\n+    source_info=None,\n+):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   memory_space = TPU_MEMORY_SPACE_NAMES[int(memory_space)]\n   buffer_id = int(buffer_id)\n   try:\n     transforms = jax.tree.map(int, transforms)\n+    # jax.debug.print(f'swap: {transforms}')\n   except:\n     raise ValueError('Advanced indexers are not supported on TPU')\n   val = np.array(val)\n@@ -778,12 +1041,17 @@ def swap(device_id, memory_space, buffer_id, transforms, val, mask, *,\n   if mask is not None:\n     assert mask.shape == val.shape\n \n+  local_core_id_for_buffer = 0 if memory_space == 'any' else local_core_id\n+  global_core_id = _get_global_core_id(device_id, local_core_id)\n+\n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n-      clock = copy_vector_clock(shared_memory.clocks[device_id])\n-    buff = shared_memory.mem[(memory_space, buffer_id, device_id)]\n+      inc_vector_clock(shared_memory.clocks[global_core_id], global_core_id)\n+      clock = copy_vector_clock(shared_memory.clocks[global_core_id])\n+    buff = shared_memory.mem[\n+        (memory_space, buffer_id, device_id, local_core_id_for_buffer)\n+    ]\n     assert buff.dtype == val.dtype  # TODO(jburnim): Catch this statically.\n     read_write_range = _to_range(transforms)\n     # TODO(jburnim): Better error message if this raises?\n@@ -792,8 +1060,11 @@ def swap(device_id, memory_space, buffer_id, transforms, val, mask, *,\n     if mask is None:\n       if in_bounds_shape != val.shape:\n         raise ValueError(\n-            f'Out-of-bounds swap of ({device_id} {memory_space} {buffer_id}): '\n-            f'swapping [{read_write_range}] but buffer has shape {buff.shape} .')\n+            'Out-of-bounds swap of'\n+            f' ({device_id} {local_core_id} {memory_space} {buffer_id}):'\n+            f' swapping [{read_write_range}] but buffer has shape'\n+            f' {buff.shape} .'\n+        )\n       buff[read_write_range] = val\n       return raw_result.copy()\n \n@@ -804,8 +1075,10 @@ def swap(device_id, memory_space, buffer_id, transforms, val, mask, *,\n       # TODO(jburnim): Include indices of out-of-bounds locations where mask\n       # is True.\n       raise ValueError(\n-          f'Out-of-bounds masked swap of ({device_id} {memory_space} {buffer_id}): '\n-          f'swapping [{read_write_range}] but buffer has shape {buff.shape} . ')\n+          'Out-of-bounds masked swap of'\n+          f' ({device_id} {local_core_id} {memory_space} {buffer_id}): swapping'\n+          f' [{read_write_range}] but buffer has shape {buff.shape} . '\n+      )\n \n     in_bounds_idx = tuple(slice(i) for i in in_bounds_shape)\n     result = val.copy()\n@@ -815,8 +1088,14 @@ def swap(device_id, memory_space, buffer_id, transforms, val, mask, *,\n         mask[in_bounds_idx], val[in_bounds_idx], raw_result)\n \n   if shared_memory.interpret_params.detect_races:\n-    check_write(device_id, clock, (memory_space, buffer_id, device_id),\n-                read_write_range, source_info=source_info)\n+    check_write(\n+        device_id,\n+        local_core_id,\n+        clock,\n+        (memory_space, buffer_id, device_id, local_core_id_for_buffer),\n+        read_write_range,\n+        source_info=source_info,\n+    )\n   return result\n \n def execute_dma(dma):\n@@ -828,17 +1107,19 @@ def execute_dma(dma):\n     if dma.virtual_device_id is None:\n       # See comment in Semaphore.wait .\n       dma.virtual_device_id = np.random.randint(\n-          shared_memory.num_devices, NUM_VIRTUAL_DEVICES)\n+          shared_memory.num_cores, NUM_VIRTUAL_CORES)\n \n     # Do the read.\n     if shared_memory.interpret_params.detect_races:\n       inc_vector_clock(dma.clock, dma.virtual_device_id)\n     dma.data = get(dma.src_device_id,\n+                   dma.src_local_core_id,\n                    dma.src_memory_space,\n                    dma.src_buffer_id,\n                    dma.src_transforms,\n                    clock=copy_vector_clock(dma.clock),\n                    src_device_id=dma.id,\n+                   src_local_core_id=0,\n                    source_info=dma.source_info)\n     data_size = dma.data.itemsize * dma.data.size\n \n@@ -847,19 +1128,26 @@ def execute_dma(dma):\n       inc_vector_clock(dma.clock, dma.virtual_device_id)\n     if dma.src_sem is not None:\n       dma.src_sem.signal(\n-          data_size, device_id=dma.src_device_id, clock=dma.clock)\n+          data_size,\n+          global_core_id=_get_global_core_id(\n+              dma.src_device_id, dma.src_local_core_id\n+          ),\n+          clock=dma.clock,\n+      )\n     dma.state = DmaState.READ\n \n     # Do the write.\n     if shared_memory.interpret_params.detect_races:\n       inc_vector_clock(dma.clock, dma.virtual_device_id)\n     store(dma.dst_device_id,\n+          dma.dst_local_core_id,\n           dma.dst_memory_space,\n           dma.dst_buffer_id,\n           dma.dst_transforms,\n           dma.data,\n           clock=copy_vector_clock(dma.clock),\n           src_device_id=dma.id,\n+          src_local_core_id=0,\n           source_info=dma.source_info)\n \n     # Signal the receive semaphore.\n@@ -867,7 +1155,12 @@ def execute_dma(dma):\n       inc_vector_clock(dma.clock, dma.virtual_device_id)\n     if dma.dst_sem is not None:\n       dma.dst_sem.signal(\n-          data_size, device_id=dma.dst_device_id, clock=dma.clock)\n+          data_size,\n+          global_core_id=_get_global_core_id(\n+              dma.dst_device_id, dma.dst_local_core_id\n+          ),\n+          clock=dma.clock,\n+      )\n \n     dma.data = None\n     dma.state = DmaState.COMPLETED\n@@ -879,11 +1172,24 @@ def print_memory(device_id):\n     with shared_memory.lock:\n       print(shared_memory.mem)\n \n-def dma_start(device_id, src_memory_space, src_id, src_transforms,\n-              dst_memory_space, dst_id, dst_transforms,\n-              dst_sem_id, src_sem_id, dst_device_id,\n-              source_info=None):\n+\n+def dma_start(\n+    device_id,\n+    src_local_core_id,\n+    src_memory_space,\n+    src_id,\n+    src_transforms,\n+    dst_memory_space,\n+    dst_id,\n+    dst_transforms,\n+    dst_sem_id,\n+    src_sem_id,\n+    dst_device_id,\n+    source_info=None,\n+):\n   device_id = int(device_id)\n+  src_local_core_id = int(src_local_core_id)\n+  src_global_core_id = _get_global_core_id(device_id, src_local_core_id)\n   src_memory_space, src_id = int(src_memory_space), int(src_id)\n   src_transforms = jax.tree.map(int, src_transforms)\n   dst_memory_space, dst_id = int(dst_memory_space), int(dst_id)\n@@ -902,15 +1208,25 @@ def dma_start(device_id, src_memory_space, src_id, src_transforms,\n \n     clock = None\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n-      clock = copy_vector_clock(shared_memory.clocks[device_id])\n+      inc_vector_clock(\n+          shared_memory.clocks[src_global_core_id], src_global_core_id\n+      )\n+      clock = copy_vector_clock(shared_memory.clocks[src_global_core_id])\n     dma_id = shared_memory.next_dma_id\n     shared_memory.next_dma_id += 1\n \n     dma = DMA(\n         dma_id,\n-        device_id, src_memory_space, src_id, src_transforms,\n-        dst_device_id, dst_memory_space, dst_id, dst_transforms,\n+        device_id,\n+        src_local_core_id,\n+        src_memory_space,\n+        src_id,\n+        src_transforms,\n+        dst_device_id,\n+        src_local_core_id,  # Same core on destination device as on source.\n+        dst_memory_space,\n+        dst_id,\n+        dst_transforms,\n         src_sem,\n         dst_sem,\n         clock=clock,\n@@ -926,52 +1242,61 @@ def dma_start(device_id, src_memory_space, src_id, src_transforms,\n   assert shared_memory.interpret_params.dma_execution_mode == 'eager'\n   execute_dma(dma)\n \n-def dma_wait(device_id, sem_id, size):\n+def dma_wait(device_id, local_core_id, sem_id, size):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   sem_id = int(sem_id)\n   size = int(size)\n+  global_core_id = _get_global_core_id(device_id, local_core_id)\n \n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n+      inc_vector_clock(shared_memory.clocks[global_core_id], global_core_id)\n     sem = shared_memory.sem[sem_id]\n-  sem.wait(size, device_id, is_dma=True)\n+  sem.wait(size, global_core_id, is_dma=True)\n \n-def semaphore_signal(device_id, sem_id, inc, target_device_id,\n-                     target_core_index):\n+def semaphore_signal(device_id, local_core_id, sem_id, inc, target_device_id,\n+                     target_local_core_id):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   sem_id = int(sem_id)\n   inc = int(inc)\n+  src_global_core_id = _get_global_core_id(device_id, local_core_id)\n   if target_device_id is None:\n     target_device_id = device_id\n   else:\n     target_device_id = int(target_device_id)\n \n-  if target_core_index is not None:\n-    if int(target_core_index) != 0:\n-      raise NotImplementedError('semaphore_signal with target_core_index != 0')\n+  if target_local_core_id is None:\n+    target_local_core_id = 0\n \n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n     clock = None\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n-      clock = copy_vector_clock(shared_memory.clocks[device_id])\n+      inc_vector_clock(\n+          shared_memory.clocks[src_global_core_id], src_global_core_id\n+      )\n+      clock = copy_vector_clock(shared_memory.clocks[src_global_core_id])\n     sem = shared_memory.sem[sem_id]\n-  sem.signal(inc, target_device_id, clock)\n+  sem.signal(\n+      inc, _get_global_core_id(target_device_id, target_local_core_id), clock\n+  )\n \n-def semaphore_wait(device_id, sem_id, value):\n+def semaphore_wait(device_id, local_core_id, sem_id, value):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   sem_id = int(sem_id)\n   value = int(value)\n+  global_core_id = _get_global_core_id(device_id, local_core_id)\n \n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n+      inc_vector_clock(shared_memory.clocks[global_core_id], global_core_id)\n     sem = shared_memory.sem[sem_id]\n-  sem.wait(value, device_id)\n+  sem.wait(value, global_core_id)\n \n def _compute_transformed_shape_and_dtype(shape, dtype, transforms):\n   for transform in transforms:\n@@ -1022,7 +1347,10 @@ class Placeholder:\n   shape: tuple[int, ...]\n   dtype: jnp.dtype\n \n-def _interpret_jaxpr(jaxpr, *args, mesh, compiler_params, interpret_params):\n+\n+def _interpret_jaxpr(\n+    jaxpr, *args, mesh, local_core_id, compiler_params, interpret_params\n+):\n   env = {}\n \n   def read(var):\n@@ -1054,8 +1382,12 @@ def write(var, value):\n   #  - Handle other higher-order primitives?\n   #  - Megacore.\n   _interpret = functools.partial(\n-      _interpret_jaxpr, mesh=mesh, compiler_params=compiler_params,\n-      interpret_params=interpret_params)\n+      _interpret_jaxpr,\n+      mesh=mesh,\n+      local_core_id=local_core_id,\n+      compiler_params=compiler_params,\n+      interpret_params=interpret_params,\n+  )\n   for eqn in jaxpr.eqns:\n     with source_info_util.user_context(\n          eqn.source_info.traceback, name_stack=eqn.source_info.name_stack):\n@@ -1065,7 +1397,9 @@ def write(var, value):\n       # not need to do any reads if `interpret_params.skip_floating_point_ops`\n       # is True. If this is the case, we want to avoid materializing the read\n       # array into the jaxpr when this function is traced.\n-      deferred_invals = functools.partial(jax._src.util.safe_map, read, eqn.invars)\n+      deferred_invals = functools.partial(\n+          jax._src.util.safe_map, read, eqn.invars\n+      )\n \n       if prim is primitives.load_p:\n         (ref, transforms, mask, _) = jax.tree.unflatten(\n@@ -1076,6 +1410,7 @@ def write(var, value):\n             functools.partial(get, source_info=eqn.source_info),\n             eqn.outvars[0].aval,\n             device_id,\n+            local_core_id,\n             TPU_MEMORY_SPACE_IDXS[eqn.invars[0].aval.memory_space],\n             ref,\n             transforms,\n@@ -1088,6 +1423,7 @@ def write(var, value):\n             functools.partial(swap, source_info=eqn.source_info),\n             eqn.outvars[0].aval,\n             device_id,\n+            local_core_id,\n             TPU_MEMORY_SPACE_IDXS[eqn.invars[0].aval.memory_space],\n             ref,\n             transforms,\n@@ -1174,7 +1510,8 @@ def f(*args, jaxpr):\n               'run_scoped_p with collective axes is not supported'\n           )\n         # Allocate a buffer or semaphore for each element of\n-        # eqn.params['jaxpr'].invars .\n+        # eqn.params['jaxpr'].invars. It is assumed that each core\n+        # runs the same sequence of `run_scoped`s.\n         allocs = []\n         for v in eqn.params['jaxpr'].invars:\n           if v.aval.memory_space == mosaic_core.TPUMemorySpace.SEMAPHORE:\n@@ -1182,6 +1519,7 @@ def f(*args, jaxpr):\n                 _allocate_semaphores,\n                 jax.ShapeDtypeStruct(v.aval.shape, jnp.int16),\n                 device_id,\n+                local_core_id,\n                 v.aval.shape,\n                 ordered=True))\n           else:\n@@ -1189,6 +1527,7 @@ def f(*args, jaxpr):\n                 _allocate_buffer,\n                 jax.ShapeDtypeStruct((), jnp.int16),\n                 device_id,\n+                local_core_id,\n                 TPU_MEMORY_SPACE_IDXS[v.aval.memory_space],\n                 _uninitialized_value(\n                     v.aval.shape, v.aval.dtype, interpret_params),\n@@ -1211,6 +1550,7 @@ def f(*args, jaxpr):\n                 _deallocate_buffer,\n                 None,\n                 device_id,\n+                local_core_id,\n                 TPU_MEMORY_SPACE_IDXS[v.aval.memory_space],\n                 a,\n                 ordered=True)\n@@ -1221,6 +1561,7 @@ def f(*args, jaxpr):\n             functools.partial(get, source_info=eqn.source_info),\n             eqn.outvars[0].aval,\n             device_id,\n+            local_core_id,\n             TPU_MEMORY_SPACE_IDXS[eqn.invars[0].aval.memory_space],\n             invals[0],\n             jax.tree.unflatten(eqn.params['tree'], invals[1:]),\n@@ -1232,6 +1573,7 @@ def f(*args, jaxpr):\n             functools.partial(swap, source_info=eqn.source_info),\n             eqn.outvars[0].aval,\n             device_id,\n+            local_core_id,\n             TPU_MEMORY_SPACE_IDXS[eqn.invars[0].aval.memory_space],\n             invals[0],\n             jax.tree.unflatten(eqn.params['tree'], invals[2:]),\n@@ -1259,6 +1601,7 @@ def f(*args, jaxpr):\n             functools.partial(dma_start, source_info=eqn.source_info),\n             (),\n             device_id,\n+            local_core_id,\n             TPU_MEMORY_SPACE_IDXS[getattr(orig_src_ref.aval, 'memory_space', mosaic_core.TPUMemorySpace.ANY)],\n             src, src_transforms,\n             TPU_MEMORY_SPACE_IDXS[getattr(orig_dst_ref.aval, 'memory_space', mosaic_core.TPUMemorySpace.ANY)],\n@@ -1287,6 +1630,7 @@ def f(*args, jaxpr):\n             dma_wait,\n             (),\n             device_id,\n+            local_core_id,\n             state_discharge.transform_array(dst_sem, dst_sem_transforms),\n             math.prod(read_shape) * read_dtype.itemsize,\n             ordered=True)\n@@ -1309,6 +1653,7 @@ def f(*args, jaxpr):\n             semaphore_signal,\n             (),\n             device_id,\n+            local_core_id,\n             state_discharge.transform_array(sem, sem_transforms),\n             inc,\n             target_device_id,\n@@ -1323,6 +1668,7 @@ def f(*args, jaxpr):\n             semaphore_wait,\n             (),\n             device_id,\n+            local_core_id,\n             state_discharge.transform_array(sem, sem_transforms),\n             value,\n             ordered=True)\n@@ -1358,8 +1704,15 @@ def _compute_start_indices(\n     block_mapping, loop_idx, *args, mesh, compiler_params, interpret_params):\n   jaxpr = block_mapping.index_map_jaxpr\n   block_indices = _interpret_jaxpr(\n-      jaxpr.jaxpr, *jaxpr.consts, *loop_idx, *args, mesh=mesh,\n-      compiler_params=compiler_params, interpret_params=interpret_params)\n+      jaxpr.jaxpr,\n+      *jaxpr.consts,\n+      *loop_idx,\n+      *args,\n+      mesh=mesh,\n+      local_core_id=0,\n+      compiler_params=compiler_params,\n+      interpret_params=interpret_params,\n+  )\n   def _get_start_index(i, b):\n     match b:\n       case pallas_core.Squeezed():\n@@ -1397,12 +1750,12 @@ def _get_mosaic_params(compiler_params: dict[str, pallas_core.CompilerParams]) -\n \n \n def _get_parallel_dim_semantics(\n-    compiler_params: dict[str, Any], grid: tuple[int, ...]\n+    compiler_params: dict[str, Any], num_dimensions_in_grid: int,\n ) -> tuple[bool, ...]:\n-  \"\"\"Returns a tuple of booleans indicating whether the corresponding dimension in `grid` is parallel.\"\"\"\n+  \"\"\"Returns a tuple of booleans indicating whether the corresponding dimension in the grid is parallel.\"\"\"\n   mosaic_params = _get_mosaic_params(compiler_params)\n   if mosaic_params.dimension_semantics is None:\n-    return (False,) * len(grid)\n+    return (False,) * num_dimensions_in_grid\n   return tuple(ds == 'parallel' for ds in mosaic_params.dimension_semantics)\n \n _GridPointCoordinatesPerDim = tuple[Array, ...]\n@@ -1432,7 +1785,7 @@ def _get_randomized_grid_coordinates(\n       dimensions.\n   \"\"\"\n   parallel_semantics_per_dim = _get_parallel_dim_semantics(\n-      compiler_params, grid\n+      compiler_params, len(grid)\n   )\n \n   key = jax.random.key(random_seed or 0)\n@@ -1484,6 +1837,23 @@ def _get_grid_point(\n   return jnp.array(grid_point, dtype=np.int32)\n \n \n+def _get_next_local_core_id(\n+    local_core_id: int,\n+    parallel_semantics_per_dim: tuple[bool, ...],\n+    grid_point: Array,\n+    next_grid_point: Array,\n+    interpret_params: TPUInterpretParams,\n+) -> int:\n+  delta = next_grid_point - grid_point\n+  assert delta.shape == (len(parallel_semantics_per_dim),)\n+  parallel_semantics_per_dim = jnp.array(parallel_semantics_per_dim)\n+  deltas_along_parallel_dims = jnp.where(parallel_semantics_per_dim, delta, 0)\n+  return jax.lax.cond(\n+      jnp.any(deltas_along_parallel_dims),\n+      lambda: (local_core_id + 1) % interpret_params.num_cores_per_device,\n+      lambda: local_core_id,\n+  )\n+\n def _uninitialized_value(shape, dtype, interpret_params):\n   if interpret_params.uninitialized_memory == 'nan':\n     if jnp.issubdtype(dtype, jnp.floating):\n@@ -1562,6 +1932,7 @@ def interpret_pallas_call(\n       (),\n       device_id,\n       num_devices,\n+      interpret_params.num_cores_per_device,\n       ordered=True)\n \n   # Pad input arguments.\n@@ -1591,6 +1962,7 @@ def interpret_pallas_call(\n         _allocate_buffer,\n         jax.ShapeDtypeStruct((), jnp.int16),\n         device_id,\n+        None,  # local_core_id\n         TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n         input_args[i],\n         ordered=True))\n@@ -1613,14 +1985,19 @@ def interpret_pallas_call(\n                                      bm.array_shape_dtype.dtype,\n                                      interpret_params)\n       padded_val = _pad_to_block_dimension(\n-          out_val, output_block_shapes[i], interpret_params)\n-      output_buffer_ids.append(callback.io_callback(\n-          _allocate_buffer,\n-          jax.ShapeDtypeStruct((), jnp.int16),\n-          device_id,\n-          TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n-          padded_val,\n-          ordered=True))\n+          out_val, output_block_shapes[i], interpret_params\n+      )\n+      output_buffer_ids.append(\n+          callback.io_callback(\n+              _allocate_buffer,\n+              jax.ShapeDtypeStruct((), jnp.int16),\n+              device_id,\n+              None,  # local_core_id\n+              TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n+              padded_val,\n+              ordered=True,\n+          )\n+      )\n       output_buffer_shapes.append(padded_val.shape)\n       output_vals.append(out_val)\n \n@@ -1630,25 +2007,34 @@ def interpret_pallas_call(\n   for var, val in zip(jaxpr.invars[grid_mapping.slice_index_ops], scalars):\n     assert var.aval.shape == val.shape\n     assert var.aval.dtype == val.dtype\n-    scalar_buffer_ids.append(callback.io_callback(\n-        _allocate_buffer,\n-        jax.ShapeDtypeStruct((), jnp.int16),\n-        device_id,\n-        TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.SMEM],\n-        val,\n-        ordered=True))\n+    scalar_buffer_ids.append(\n+        callback.io_callback(\n+            _allocate_buffer,\n+            jax.ShapeDtypeStruct((), jnp.int16),\n+            device_id,\n+            None,  # local_core_id,\n+            TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.SMEM],\n+            val,\n+            ordered=True,\n+        )\n+    )\n+\n   kernel_buffer_ids = scalar_buffer_ids.copy()\n   for i, var in enumerate(jaxpr.invars[grid_mapping.num_index_operands:]):\n     output_idx = i - grid_mapping.num_inputs\n     is_input = i < grid_mapping.num_inputs\n     is_output = (output_idx >= 0) and (output_idx < grid_mapping.num_outputs)\n     if var.aval.memory_space == mosaic_core.TPUMemorySpace.SEMAPHORE:\n-      kernel_buffer_ids.append(callback.io_callback(\n-          _allocate_semaphores,\n-          jax.ShapeDtypeStruct(var.aval.shape, jnp.int16),\n-          device_id,\n-          var.aval.shape,\n-          ordered=True))\n+      kernel_buffer_ids.append(\n+          callback.io_callback(\n+              _allocate_semaphores,\n+              jax.ShapeDtypeStruct(var.aval.shape, jnp.int16),\n+              device_id,\n+              None,  # local_core_id\n+              var.aval.shape,\n+              ordered=True,\n+          )\n+      )\n     elif _is_any(var.aval.memory_space):\n       # Use the already-allocated HBM input or output buffer.\n       #\n@@ -1661,14 +2047,19 @@ def interpret_pallas_call(\n       if is_output:\n         kernel_buffer_ids.append(output_buffer_ids[output_idx])\n     else:\n-      kernel_buffer_ids.append(callback.io_callback(\n-          _allocate_buffer,\n-          jax.ShapeDtypeStruct((), jnp.int16),\n-          device_id,\n-          TPU_MEMORY_SPACE_IDXS[var.aval.memory_space],\n-          _uninitialized_value(\n-              var.aval.shape, var.aval.dtype, interpret_params),\n-          ordered=True))\n+      kernel_buffer_ids.append(\n+          callback.io_callback(\n+              _allocate_buffer,\n+              jax.ShapeDtypeStruct((), jnp.int16),\n+              device_id,\n+              None,  # local_core_id,\n+              TPU_MEMORY_SPACE_IDXS[var.aval.memory_space],\n+              _uninitialized_value(\n+                  var.aval.shape, var.aval.dtype, interpret_params\n+              ),\n+              ordered=True,\n+          )\n+      )\n \n   if _get_mosaic_params(compiler_params).collective_id is None:\n     # The kernel doesn't specify its own barrier semaphore, so we do a global\n@@ -1687,6 +2078,9 @@ def interpret_pallas_call(\n     # Base case is always one iteration when grid is ()\n     num_iterations = 1\n \n+  parallel_semantics_per_dim = _get_parallel_dim_semantics(\n+      compiler_params, len(grid)\n+  )\n   randomized_grid_coordinates = _get_randomized_grid_coordinates(\n       grid, compiler_params, interpret_params.random_seed  # type: ignore[arg-type]\n   )\n@@ -1703,18 +2097,38 @@ def _get_local_grid_env(loop_idx):\n \n   def body(\n       carry: tuple[\n-          jnp.int32, tuple[jnp.int32, ...], list[jnp.ndarray], list[jnp.ndarray]\n+          jnp.int32,\n+          tuple[jnp.int32, ...],\n+          jnp.ndarray,\n+          jnp.int32,\n+          jnp.int32,\n+          list[jnp.ndarray],\n+          list[jnp.ndarray],\n       ],\n-  ):\n+  ) -> tuple[\n+      jnp.int32,\n+      tuple[jnp.int32, ...],\n+      jnp.ndarray,\n+      jnp.int32,\n+      jnp.int32,\n+      list[jnp.ndarray],\n+      list[jnp.ndarray],\n+  ]:\n     \"\"\"Performs a single iteration of `jaxpr` in the device grid.\n \n     Execution of `jaxpr` is preceded by reading kernel input buffers and\n     followed by writing kernel output buffers.\n \n     Args:\n-      carry: (iteration_idx, loop_idx, prev_start_indices, cur_start_indices).\n+      carry: (iteration_idx, loop_idx, grid_point, prev_local_core_id,\n+              cur_local_core_id, prev_start_indices, cur_start_indices).\n         - iteration_idx is the interation index.\n         - loop_idx are the program ids for each grid axis.\n+        - grid_point is the grid point for the current loop iteration.\n+        - prev_local_core_id is the (device-local) core id from the previous\n+          loop iteration.\n+        - cur_local_core_id is the (device-local) core id for the current loop\n+          iteration.\n         - prev_start_indices is a rank-1 array that contains the start indices\n           for the slices of inputs and outputs processed in the previous loop\n           iteration.\n@@ -1729,9 +2143,16 @@ def body(\n     Returns:\n       The carry for the next iteration.\n     \"\"\"\n-    iteration_idx, loop_idx, prev_start_indices, cur_start_indices = carry\n+    (\n+        iteration_idx,\n+        loop_idx,\n+        grid_point,\n+        prev_local_core_id,\n+        cur_local_core_id,\n+        prev_start_indices,\n+        cur_start_indices,\n+    ) = carry\n     if interpret_params.grid_point_recorder is not None:\n-      grid_point = _get_grid_point(loop_idx, randomized_grid_coordinates)\n       callback.io_callback(interpret_params.grid_point_recorder, (), grid_point)\n \n     with pallas_core.grid_env(_get_local_grid_env(loop_idx)):\n@@ -1739,6 +2160,13 @@ def body(\n       next_grid_point = _get_grid_point(\n           next_loop_idx, randomized_grid_coordinates\n       )\n+      next_local_core_id = _get_next_local_core_id(\n+          cur_local_core_id,\n+          parallel_semantics_per_dim,\n+          grid_point,\n+          next_grid_point,\n+          interpret_params,\n+      )\n       next_start_indices = [\n           _compute_start_indices(\n               bm,\n@@ -1774,6 +2202,7 @@ def _store_slice_to_kernel_input(index, input_var):\n             get,\n             jax.ShapeDtypeStruct(input_var.aval.shape, input_var.aval.dtype),\n             device_id,\n+            cur_local_core_id,\n             TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n             input_buffer_ids[index],\n             (transform,),\n@@ -1785,6 +2214,7 @@ def _store_slice_to_kernel_input(index, input_var):\n             store,\n             (),\n             device_id,\n+            cur_local_core_id,\n             TPU_MEMORY_SPACE_IDXS[input_var.aval.memory_space],\n             input_ids[index],\n             (),\n@@ -1799,6 +2229,7 @@ def _store_slice_to_kernel_input(index, input_var):\n         assert len(prev_start_indices[j].shape) == 1\n         jax.lax.cond(\n             (iteration_idx == 0)\n+            | (cur_local_core_id != prev_local_core_id)\n             | jax.lax.reduce_or(\n                 cur_start_indices[j] != prev_start_indices[j], axes=(0,)\n             ),\n@@ -1807,9 +2238,14 @@ def _store_slice_to_kernel_input(index, input_var):\n         )\n \n       # Invoke the kernel.\n-      _interpret_jaxpr(jaxpr, *kernel_buffer_ids, mesh=mesh,\n-                       compiler_params=compiler_params,\n-                       interpret_params=interpret_params)\n+      _interpret_jaxpr(\n+          jaxpr,\n+          *kernel_buffer_ids,\n+          mesh=mesh,\n+          local_core_id=cur_local_core_id,\n+          compiler_params=compiler_params,\n+          interpret_params=interpret_params,\n+      )\n \n       # Copy from the kernel buffers to slices of the output in HBM.\n       def _store_to_output_buffer(index, output_var):\n@@ -1819,6 +2255,7 @@ def _store_to_output_buffer(index, output_var):\n             get,\n             output_var.aval,\n             device_id,\n+            cur_local_core_id,\n             TPU_MEMORY_SPACE_IDXS[output_var.aval.memory_space],\n             kernel_output_ids[j],\n             (),\n@@ -1842,6 +2279,7 @@ def _store_to_output_buffer(index, output_var):\n             store,\n             (),\n             device_id,\n+            cur_local_core_id,\n             TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n             output_buffer_ids[index],\n             (transform,),\n@@ -1856,6 +2294,7 @@ def _store_to_output_buffer(index, output_var):\n         assert len(next_start_indices[num_inputs + j].shape) == 1\n         jax.lax.cond(\n             (iteration_idx + 1 == num_iterations)\n+            | (cur_local_core_id != next_local_core_id)\n             | jax.lax.reduce_or(\n                 cur_start_indices[num_inputs + j]\n                 != next_start_indices[num_inputs + j],\n@@ -1865,7 +2304,15 @@ def _store_to_output_buffer(index, output_var):\n             lambda: None,\n         )\n \n-      return iteration_idx + 1, next_loop_idx, cur_start_indices, next_start_indices\n+      return (\n+          iteration_idx + 1,\n+          next_loop_idx,\n+          next_grid_point,\n+          cur_local_core_id,\n+          next_local_core_id,\n+          cur_start_indices,\n+          next_start_indices,\n+      )\n \n   initial_loop_idx = (jnp.int32(0),) * len(grid)\n   initial_grid_point = _get_grid_point(\n@@ -1884,16 +2331,25 @@ def _store_to_output_buffer(index, output_var):\n         for bm in grid_mapping.block_mappings\n     ]\n   # TODO(jburnim): Handle parallel grid dimensions + megacore.\n+  callback.io_callback(\n+      _update_clocks_for_device_barrier, (), device_id, ordered=True\n+  )\n   _ = lax.while_loop(\n       lambda carry: carry[0] < num_iterations,\n       body,\n       (\n           jnp.int32(0),\n           initial_loop_idx,\n+          initial_grid_point,\n+          jnp.int32(0),  # Previous core id is ignored on the first iteration.\n+          jnp.int32(0),  # Current core id is set to 0 for the first iteration.\n           initial_start_indices,  # Previous start indices are ignored on the first iteration.\n           initial_start_indices,\n       ),\n   )\n+  callback.io_callback(\n+      _update_clocks_for_device_barrier, (), device_id, ordered=True\n+  )\n \n   # Read the output from the allocated output buffers.\n   ret = [\n@@ -1903,6 +2359,7 @@ def _store_to_output_buffer(index, output_var):\n           get,\n           val,\n           device_id,\n+          0,  # local_core_id\n           TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n           output_buffer_id,\n           (indexing.NDIndexer.from_indices_shape(\ndiff --git a/tests/pallas/tpu_pallas_interpret_test.py b/tests/pallas/tpu_pallas_interpret_test.py\nindex 1af4b29d60ff..28c63dc3bd9b 100644\n--- a/tests/pallas/tpu_pallas_interpret_test.py\n+++ b/tests/pallas/tpu_pallas_interpret_test.py\n@@ -521,6 +521,65 @@ def alloc(x_vmem_ref, y_vmem_ref, sem):\n     y = f(x)\n     np.testing.assert_array_equal(y, x + 1)\n \n+  def test_two_cores_along_parallel_dimension_with_race(self):\n+    def kernel(x_ref, o_ref, vmem_ref):\n+      vmem_ref[...] = x_ref[...]\n+      o_ref[...] = x_ref[...] + vmem_ref[...]\n+\n+    x = jnp.ones((8, 128), jnp.float32)\n+    y = pl.pallas_call(\n+        kernel,\n+        grid=(2,),\n+        out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype),\n+        in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)],\n+        scratch_shapes=[\n+            pltpu.VMEM(x.shape, x.dtype),\n+        ],\n+        interpret=mosaic_interpret.TPUInterpretParams(\n+            num_cores_per_device=2,\n+            detect_races=True,\n+        ),\n+        compiler_params=pltpu.TPUCompilerParams(\n+            dimension_semantics=('parallel',),\n+        ),\n+    )(x)\n+    self.assertTrue(mosaic_interpret.races.races_found)\n+    np.testing.assert_allclose(y, 2.0 * x)\n+\n+  def test_two_cores_along_parallel_dimension_no_race(self):\n+    def kernel(x_ref, o_ref, vmem_ref):\n+      vmem_ref[...] = x_ref[...]\n+      o_ref[...] = x_ref[...] + vmem_ref[...]\n+\n+    x = jnp.ones((16, 128), jnp.float32)\n+    y = pl.pallas_call(\n+        kernel,\n+        grid=(2,),\n+        out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype),\n+        out_specs=pl.BlockSpec(\n+            (8, 128),\n+            lambda i: (i, 0),\n+        ),\n+        in_specs=[\n+            pl.BlockSpec(\n+                (8, 128),\n+                lambda i: (i, 0),\n+            ),\n+        ],\n+        scratch_shapes=[\n+            pltpu.VMEM((8, 128), x.dtype),\n+        ],\n+        interpret=mosaic_interpret.TPUInterpretParams(\n+            num_cores_per_device=2,\n+            detect_races=True,\n+        ),\n+        compiler_params=pltpu.TPUCompilerParams(\n+            dimension_semantics=('parallel',)\n+        ),\n+    )(x)\n+    self.assertFalse(mosaic_interpret.races.races_found)\n+    np.testing.assert_allclose(y, 2.0 * x)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix skip condition in Pallas Triton tests\n\nIt should not run in TPU configs at all.\n\nPiperOrigin-RevId: 761865943",
        "output": "```diff\nCommit: 421aa54b269bb9ec0e003fd2736b0c0f2edb4759\nDate: 2025-05-22T09:05:19Z\nURL: https://github.com/jax-ml/jax/commit/421aa54b269bb9ec0e003fd2736b0c0f2edb4759\nFiles changed: 1\nAdditions: +8, Deletions: -5\ndiff --git a/tests/pallas/triton_pallas_test.py b/tests/pallas/triton_pallas_test.py\nindex 4e2b10e72eb1..fe13716705de 100644\n--- a/tests/pallas/triton_pallas_test.py\n+++ b/tests/pallas/triton_pallas_test.py\n@@ -33,11 +33,14 @@ class PallasBaseTest(jtu.JaxTestCase):\n   INTERPRET = False\n \n   def setUp(self):\n-    if jtu.test_device_matches([\"cpu\"]) and not self.INTERPRET:\n-      self.skipTest(\"On CPU the test works only in interpret mode\")\n-    if (jtu.test_device_matches([\"cuda\"]) and\n-        not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n-      self.skipTest(\"Only works on GPU with capability >= sm90\")\n+    if jtu.test_device_matches([\"cpu\"]):\n+      if not self.INTERPRET:\n+        self.skipTest(\"On CPU the test works only in interpret mode\")\n+    elif jtu.test_device_matches([\"gpu\"]):\n+      if not jtu.is_cuda_compute_capability_at_least(\"9.0\"):\n+        self.skipTest(\"Only works on GPU with capability >= sm90\")\n+    else:\n+      self.skipTest(\"Test only works on CPU and GPU\")\n \n     super().setUp()\n     _trace_kernel_to_jaxpr.cache_clear()\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28923 from mattjj:rahul-fix3\n\nPiperOrigin-RevId: 761806312",
        "output": "```diff\nCommit: 2531d31d9645a12ecbc99f8b79d4251cc40a7d35\nDate: 2025-05-22T05:27:51Z\nURL: https://github.com/jax-ml/jax/commit/2531d31d9645a12ecbc99f8b79d4251cc40a7d35\nFiles changed: 3\nAdditions: +31, Deletions: -9\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex e49173c3df45..b20b85a43b6e 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -2745,10 +2745,8 @@ def __lt__(self, other):\n @dataclass(frozen=True)\n class NamedAxisEffect(effects.Effect):\n   \"\"\"A side-effect introducing a new named axis into the current scope.\"\"\"\n-\n   name: AxisName\n \n-\n effects.control_flow_allowed_effects.add_type(NamedAxisEffect)\n effects.custom_derivatives_allowed_effects.add_type(NamedAxisEffect)\n effects.lowerable_effects.add_type(NamedAxisEffect)\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex 9b28595e1835..d76d145fd0a6 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -1619,21 +1619,19 @@ def wrapped_fwd(*args, **kwargs) -> tuple[ReturnValue, Any]:\n     prim_tree, res_tree = out_trees()\n     num_res = res_tree.num_leaves\n \n-    if fwd_jaxpr.effects:\n+    disallowed_effects = effects.custom_derivatives_allowed_effects.filter_not_in(fwd_jaxpr.effects)\n+    if disallowed_effects:\n       raise NotImplementedError(\n           \"remat optimization for custom_vjp does not support forward \"\n-          f\"functions with side effects, but {fwd_name} has the following \"\n-          f\"effects: {fwd_jaxpr.effects}\")\n+          f\"functions with these side effects: {disallowed_effects}\")\n \n     @pe._memoize\n     def fun_jaxpr_thunk():\n       jaxpr, _, consts, () = pe.trace_to_jaxpr_dynamic(flat_fun, in_avals)\n       return jaxpr, consts\n \n-    out_flat = remat_opt_p.bind(*consts, *args_flat,\n-                                num_consts=len(consts),\n-                                num_res=num_res,\n-                                fwd_jaxpr=fwd_jaxpr,\n+    out_flat = remat_opt_p.bind(*consts, *args_flat, num_consts=len(consts),\n+                                num_res=num_res, fwd_jaxpr=fwd_jaxpr,\n                                 fun_jaxpr_thunk=fun_jaxpr_thunk)\n     res, out_flat = split_list(out_flat, [num_res])\n     out_tree = treedef_tuple((prim_tree, res_tree))\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 9b4ca76c3bc5..5fbace3c98e1 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -621,6 +621,32 @@ def f():\n     x = f()\n     self.assertAllClose(x, jnp.arange(4), check_dtypes=False)\n \n+  def test_optimize_remat(self):\n+    mesh = jtu.create_mesh((4,), 'x')\n+\n+    @jax.custom_vjp\n+    def f(x):\n+      return jnp.tan(x)\n+\n+    def f_fwd(x):\n+      return jax.lax.psum(x, 'x'), (x,)\n+\n+    def f_bwd(res, g):\n+      x, = res\n+      cos_x = jnp.cos(x)\n+      return (cos_x * g,)\n+\n+    f.defvjp(f_fwd, f_bwd, optimize_remat=True)\n+\n+    @jax.jit\n+    @jax.shard_map(mesh=mesh, in_specs=P(), out_specs=P())\n+    def temp(x):\n+      out = jax.remat(f)(x)\n+      out = out ** 2\n+      return out\n+\n+    jax.grad(lambda x: temp(x).sum())(jnp.arange(4.))\n+\n   def test_remat_basic(self):\n     # this tests remat-of-shmap\n     mesh = Mesh(np.array(jax.devices()[:4]), ('x',))\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "fix custom_vjp optimize_remat=True with collectives",
        "output": "```diff\nCommit: bddb877c217e045f1210f0c831018ee0a54078a9\nDate: 2025-05-22T05:02:30Z\nURL: https://github.com/jax-ml/jax/commit/bddb877c217e045f1210f0c831018ee0a54078a9\nFiles changed: 3\nAdditions: +31, Deletions: -9\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex e49173c3df45..b20b85a43b6e 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -2745,10 +2745,8 @@ def __lt__(self, other):\n @dataclass(frozen=True)\n class NamedAxisEffect(effects.Effect):\n   \"\"\"A side-effect introducing a new named axis into the current scope.\"\"\"\n-\n   name: AxisName\n \n-\n effects.control_flow_allowed_effects.add_type(NamedAxisEffect)\n effects.custom_derivatives_allowed_effects.add_type(NamedAxisEffect)\n effects.lowerable_effects.add_type(NamedAxisEffect)\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex 9b28595e1835..d76d145fd0a6 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -1619,21 +1619,19 @@ def wrapped_fwd(*args, **kwargs) -> tuple[ReturnValue, Any]:\n     prim_tree, res_tree = out_trees()\n     num_res = res_tree.num_leaves\n \n-    if fwd_jaxpr.effects:\n+    disallowed_effects = effects.custom_derivatives_allowed_effects.filter_not_in(fwd_jaxpr.effects)\n+    if disallowed_effects:\n       raise NotImplementedError(\n           \"remat optimization for custom_vjp does not support forward \"\n-          f\"functions with side effects, but {fwd_name} has the following \"\n-          f\"effects: {fwd_jaxpr.effects}\")\n+          f\"functions with these side effects: {disallowed_effects}\")\n \n     @pe._memoize\n     def fun_jaxpr_thunk():\n       jaxpr, _, consts, () = pe.trace_to_jaxpr_dynamic(flat_fun, in_avals)\n       return jaxpr, consts\n \n-    out_flat = remat_opt_p.bind(*consts, *args_flat,\n-                                num_consts=len(consts),\n-                                num_res=num_res,\n-                                fwd_jaxpr=fwd_jaxpr,\n+    out_flat = remat_opt_p.bind(*consts, *args_flat, num_consts=len(consts),\n+                                num_res=num_res, fwd_jaxpr=fwd_jaxpr,\n                                 fun_jaxpr_thunk=fun_jaxpr_thunk)\n     res, out_flat = split_list(out_flat, [num_res])\n     out_tree = treedef_tuple((prim_tree, res_tree))\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 9b4ca76c3bc5..5fbace3c98e1 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -621,6 +621,32 @@ def f():\n     x = f()\n     self.assertAllClose(x, jnp.arange(4), check_dtypes=False)\n \n+  def test_optimize_remat(self):\n+    mesh = jtu.create_mesh((4,), 'x')\n+\n+    @jax.custom_vjp\n+    def f(x):\n+      return jnp.tan(x)\n+\n+    def f_fwd(x):\n+      return jax.lax.psum(x, 'x'), (x,)\n+\n+    def f_bwd(res, g):\n+      x, = res\n+      cos_x = jnp.cos(x)\n+      return (cos_x * g,)\n+\n+    f.defvjp(f_fwd, f_bwd, optimize_remat=True)\n+\n+    @jax.jit\n+    @jax.shard_map(mesh=mesh, in_specs=P(), out_specs=P())\n+    def temp(x):\n+      out = jax.remat(f)(x)\n+      out = out ** 2\n+      return out\n+\n+    jax.grad(lambda x: temp(x).sum())(jnp.arange(4.))\n+\n   def test_remat_basic(self):\n     # this tests remat-of-shmap\n     mesh = Mesh(np.array(jax.devices()[:4]), ('x',))\n\n```",
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "Merge pull request #28909 from levskaya:tree_broadcast\n\nPiperOrigin-RevId: 761758158",
        "output": "```diff\nCommit: c9934912885bb7c4b72c5a9271598235a6789a81\nDate: 2025-05-22T02:09:32Z\nURL: https://github.com/jax-ml/jax/commit/c9934912885bb7c4b72c5a9271598235a6789a81\nFiles changed: 8\nAdditions: +108, Deletions: -5\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 1e866fae6af5..b34bf36997af 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -16,6 +16,9 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n \n ## Unreleased\n \n+* New features:\n+  * Added {func}`jax.tree.broadcast` which implements a pytree prefix broadcasting helper.\n+\n ## JAX 0.6.1 (May 21, 2025)\n \n * New features:\ndiff --git a/docs/jax.tree.rst b/docs/jax.tree.rst\nindex e65c77c757c1..1a0ddaec86d0 100644\n--- a/docs/jax.tree.rst\n+++ b/docs/jax.tree.rst\n@@ -12,6 +12,7 @@ List of Functions\n    :toctree: _autosummary\n \n    all\n+   broadcast\n    flatten\n    flatten_with_path\n    leaves\ndiff --git a/docs/jax.tree_util.rst b/docs/jax.tree_util.rst\nindex 73fd1f376e9f..c89b777ca548 100644\n--- a/docs/jax.tree_util.rst\n+++ b/docs/jax.tree_util.rst\n@@ -38,6 +38,7 @@ These APIs are now accessed via :mod:`jax.tree`.\n    :toctree: _autosummary\n \n    tree_all\n+   tree_broadcast\n    tree_flatten\n    tree_leaves\n    tree_map\ndiff --git a/jax/_src/tree.py b/jax/_src/tree.py\nindex 70d75a126804..9a3e001d902b 100644\n--- a/jax/_src/tree.py\n+++ b/jax/_src/tree.py\n@@ -378,3 +378,34 @@ def map_with_path(\n     - :func:`jax.tree_util.register_pytree_with_keys`\n   \"\"\"\n   return tree_util.tree_map_with_path(f, tree, *rest, is_leaf=is_leaf)\n+\n+\n+def broadcast(prefix_tree: Any, full_tree: Any,\n+              is_leaf: Callable[[Any], bool] | None = None\n+              ) -> list[Any]:\n+  \"\"\"Broadcasts a tree prefix into the full structure of a given tree.\n+\n+    Args:\n+      prefix_tree: a pytree that is a tree prefix of full_tree.\n+      full_tree: a pytree with the structure to broadcast the prefix leaves into.\n+      is_leaf: an optionally specified function that will be called at each\n+        flattening step. It should return a boolean, with true stopping the\n+        traversal and the whole subtree being treated as a leaf, and false\n+        indicating the flattening should traverse the current object.\n+\n+    Returns:\n+      A pytree matching the structure of full_tree where the leaves of prefix_tree have been\n+      broadcasted into the leaves of each corresponding subtree.\n+\n+    Examples:\n+      >>> import jax\n+      >>> prefix = (1, 2, 3)\n+      >>> full = (0, {'a': 0, 'b': 0}, (0, 0))\n+      >>> jax.tree.broadcast(prefix, full)\n+      (1, {'a': 2, 'b': 2}, (3, 3))\n+\n+    See Also:\n+      - :func:`jax.tree.leaves`\n+      - :func:`jax.tree.structure`\n+  \"\"\"\n+  return tree_util.tree_broadcast(prefix_tree, full_tree, is_leaf=is_leaf)\ndiff --git a/jax/_src/tree_util.py b/jax/_src/tree_util.py\nindex e2e97c90f120..6edbbfd62d12 100644\n--- a/jax/_src/tree_util.py\n+++ b/jax/_src/tree_util.py\n@@ -560,17 +560,42 @@ def __new__(klass, func, *args, **kw):\n )\n \n \n-# broadcast_prefix is not exported.\n+@export\n+def tree_broadcast(prefix_tree: Any, full_tree: Any,\n+                   is_leaf: Callable[[Any], bool] | None = None\n+                  ) -> list[Any]:\n+  \"\"\"Alias of :func:`jax.tree.broadcast`.\"\"\"\n+  broadcast_leaves = broadcast_prefix(prefix_tree, full_tree, is_leaf=is_leaf)\n+  return tree_structure(full_tree).unflatten(broadcast_leaves)\n+\n+\n+# broadcast_prefix is not exported\n def broadcast_prefix(prefix_tree: Any, full_tree: Any,\n                      is_leaf: Callable[[Any], bool] | None = None\n                      ) -> list[Any]:\n-  # If prefix_tree is not a tree prefix of full_tree, this code can raise a\n-  # ValueError; use prefix_errors to find disagreements and raise more precise\n-  # error messages.\n+  \"\"\"Broadcasts tree prefix leaves into the full set of leaves for a given full tree.\n+\n+    Args:\n+      prefix_tree: a pytree that is a tree prefix of full_tree.\n+      full_tree: a pytree with the structure to broadcast the prefix leaves into.\n+      is_leaf: an optionally specified function that will be called at each\n+        flattening step. It should return a boolean, with true stopping the\n+        traversal and the whole subtree being treated as a leaf, and false\n+        indicating the flattening should traverse the current object.\n+\n+    Returns:\n+      A list of leaves matching the expected count for the full tree,\n+      with the leaf of each prefix tree being duplicated to match the count of\n+      its corresponding subtree.\n+  \"\"\"\n   result = []\n   num_leaves = lambda t: tree_structure(t).num_leaves\n   add_leaves = lambda x, subtree: result.extend([x] * num_leaves(subtree))\n-  tree_map(add_leaves, prefix_tree, full_tree, is_leaf=is_leaf)\n+  try:\n+    tree_map(add_leaves, prefix_tree, full_tree, is_leaf=is_leaf)\n+  except ValueError:\n+      e, *_ = prefix_errors(prefix_tree, full_tree)\n+      raise e('broadcast_prefix prefix_tree') from None\n   return result\n \n \ndiff --git a/jax/tree.py b/jax/tree.py\nindex 270c34fe9647..03ca503f3a41 100644\n--- a/jax/tree.py\n+++ b/jax/tree.py\n@@ -19,6 +19,7 @@\n \n from jax._src.tree import (\n     all as all,\n+    broadcast as broadcast,\n     flatten_with_path as flatten_with_path,\n     flatten as flatten,\n     leaves_with_path as leaves_with_path,\ndiff --git a/jax/tree_util.py b/jax/tree_util.py\nindex 9f42284144ec..b35890dfc887 100644\n--- a/jax/tree_util.py\n+++ b/jax/tree_util.py\n@@ -58,6 +58,7 @@\n     register_pytree_with_keys as register_pytree_with_keys,\n     register_static as register_static,\n     tree_all as tree_all,\n+    tree_broadcast as tree_broadcast,\n     tree_flatten_with_path as tree_flatten_with_path,\n     tree_flatten as tree_flatten,\n     tree_leaves_with_path as tree_leaves_with_path,\ndiff --git a/tests/tree_util_test.py b/tests/tree_util_test.py\nindex 0df811d9da28..8d4cd5854e7d 100644\n--- a/tests/tree_util_test.py\n+++ b/tests/tree_util_test.py\n@@ -627,6 +627,39 @@ def testTransposeWithCustomObject(self):\n                                       FlatCache({\"a\": [3, 4], \"b\": [5, 6]}))\n     self.assertEqual(expected, actual)\n \n+  @parameterized.parameters(*TREES)\n+  def testBroadcast(self, tree):\n+    if isinstance(tree, FlatCache):\n+      # The tree_map construction below fails for FlatCache, because\n+      # the cached metadata becomes out of sync.\n+      self.skipTest(\"Test does not work properly for FlatCache.\")\n+    def make_inner(x):\n+      return [x, x, x]\n+    nested = tree_util.tree_map(make_inner, tree)\n+    actual = tree_util.tree_broadcast(tree, nested)\n+    self.assertEqual(actual, nested)\n+\n+  def testBroadcastSimple(self):\n+    prefix = (1, 2, 3)\n+    full = (0, {'a': 0, 'b': 0}, (0, 0))\n+    actual = tree_util.tree_broadcast(prefix, full)\n+    expected = (1, {'a': 2, 'b': 2}, (3, 3))\n+    self.assertEqual(actual, expected)\n+\n+  def testBroadcastError(self):\n+    prefix = (1, 2, 3)\n+    full = (0, {'a': 0, 'b': 0})\n+    with self.assertRaisesRegex(ValueError, \"pytree structure error\"):\n+      tree_util.tree_broadcast(prefix, full)\n+    prefix = (1, 2)\n+    full = (0, {'a': 0, 'b': 0}, (0, 0))\n+    with self.assertRaisesRegex(ValueError, \"pytree structure error\"):\n+      tree_util.tree_broadcast(prefix, full)\n+    prefix = (1, {'a': 0})\n+    full = (0, {'a': 0, 'b': 0})\n+    with self.assertRaisesRegex(ValueError, \"pytree structure error\"):\n+      tree_util.tree_broadcast(prefix, full)\n+\n   @parameterized.parameters([(*t, s) for t, s in zip(TREES, TREE_STRINGS)])\n   def testStringRepresentation(self, tree, correct_string):\n     \"\"\"Checks that the string representation of a tree works.\"\"\"\n@@ -1444,6 +1477,13 @@ def test_tree_transpose(self):\n       tree_util.tree_transpose(outer_treedef, inner_treedef, obj)\n     )\n \n+  def test_tree_broadcast(self):\n+    prefix = (1, 2, 3)\n+    full = (0, {'a': 0, 'b': 0}, (0, 0))\n+    actual = jax.tree.broadcast(prefix, full)\n+    expected = (1, {'a': 2, 'b': 2}, (3, 3))\n+    self.assertEqual(actual, expected)\n+\n   def test_tree_unflatten(self):\n     leaves, treedef = jax.tree.flatten([1, 2, (3, 4)])\n     self.assertEqual(\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add out_sharding to the function returned by `jax.nn.initializers.he_normal` and other APIs implementing the `Initializer` protocol. Currently it takes `key, shape, dtype` and now we added an optional out_sharding parameter to it.\n\nPiperOrigin-RevId: 761742909",
        "output": "```diff\nCommit: 29e6647577ad98b2d1a35cccdfe9da8dfa0f1f24\nDate: 2025-05-22T01:06:08Z\nURL: https://github.com/jax-ml/jax/commit/29e6647577ad98b2d1a35cccdfe9da8dfa0f1f24\nFiles changed: 2\nAdditions: +58, Deletions: -15\ndiff --git a/jax/_src/nn/initializers.py b/jax/_src/nn/initializers.py\nindex 6f117eef749f..855729fa16ff 100644\n--- a/jax/_src/nn/initializers.py\n+++ b/jax/_src/nn/initializers.py\n@@ -30,6 +30,7 @@\n from jax import random\n from jax._src import core\n from jax._src import dtypes\n+from jax._src.sharding_impls import canonicalize_sharding\n from jax._src.typing import Array, ArrayLike\n from jax._src.util import set_module\n \n@@ -48,7 +49,8 @@ class Initializer(Protocol):\n   def __call__(self,\n                key: Array,\n                shape: core.Shape,\n-               dtype: DTypeLikeInexact = jnp.float_) -> Array:\n+               dtype: DTypeLikeInexact = jnp.float_,\n+               out_sharding=None) -> Array:\n     raise NotImplementedError\n \n @export\n@@ -100,9 +102,12 @@ def constant(value: ArrayLike,\n   \"\"\"\n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n     dtype = dtypes.canonicalize_dtype(dtype)\n-    return jnp.full(shape, value, dtype=dtype)\n+    out_sharding = canonicalize_sharding(\n+        out_sharding, 'nn.initializers.constant')\n+    return jnp.full(shape, value, dtype=dtype, device=out_sharding)\n   return init\n \n @export\n@@ -126,9 +131,11 @@ def uniform(scale: RealNumeric = 1e-2,\n   \"\"\"\n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n     dtype = dtypes.canonicalize_dtype(dtype)\n-    return random.uniform(key, shape, dtype) * jnp.array(scale, dtype)\n+    return random.uniform(key, shape, dtype,\n+                          out_sharding=out_sharding) * jnp.array(scale, dtype)\n   return init\n \n @export\n@@ -152,9 +159,11 @@ def normal(stddev: RealNumeric = 1e-2,\n   \"\"\"\n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n     dtype = dtypes.canonicalize_dtype(dtype)\n-    return random.normal(key, shape, dtype) * jnp.array(stddev, dtype)\n+    return random.normal(key, shape, dtype,\n+                         out_sharding=out_sharding) * jnp.array(stddev, dtype)\n   return init\n \n @export\n@@ -189,10 +198,12 @@ def truncated_normal(stddev: RealNumeric = 1e-2,\n \n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n     dtype = dtypes.canonicalize_dtype(dtype)\n     return random.truncated_normal(\n-        key, lower, upper, shape, dtype) * jnp.array(stddev, dtype)\n+        key, lower, upper, shape, dtype,\n+        out_sharding=out_sharding) * jnp.array(stddev, dtype)\n   return init\n \n @export\n@@ -315,7 +326,8 @@ def variance_scaling(\n \n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n     shape = core.canonicalize_shape(shape)\n     dtype = dtypes.canonicalize_dtype(dtype)\n     fan_in, fan_out = _compute_fans(shape, in_axis, out_axis, batch_axis)\n@@ -332,16 +344,19 @@ def init(key: Array,\n       if jnp.issubdtype(dtype, jnp.floating):\n         # constant is stddev of standard normal truncated to (-2, 2)\n         stddev = jnp.sqrt(variance) / jnp.array(.87962566103423978, dtype)\n-        return random.truncated_normal(key, -2, 2, shape, dtype) * stddev\n+        return random.truncated_normal(key, -2, 2, shape, dtype,\n+                                       out_sharding=out_sharding) * stddev\n       else:\n         # constant is stddev of complex standard normal truncated to 2\n         stddev = jnp.sqrt(variance) / jnp.array(.95311164380491208, dtype)\n         return _complex_truncated_normal(key, 2, shape, dtype) * stddev\n     elif distribution == \"normal\":\n-      return random.normal(key, shape, dtype) * jnp.sqrt(variance)\n+      return random.normal(key, shape, dtype,\n+                           out_sharding=out_sharding) * jnp.sqrt(variance)\n     elif distribution == \"uniform\":\n       if jnp.issubdtype(dtype, jnp.floating):\n-        return random.uniform(key, shape, dtype, -1) * jnp.sqrt(3 * variance)\n+        return random.uniform(key, shape, dtype, -1,\n+                              out_sharding=out_sharding) * jnp.sqrt(3 * variance)\n       else:\n         return _complex_uniform(key, shape, dtype) * jnp.sqrt(variance)\n     else:\n@@ -601,7 +616,10 @@ def orthogonal(scale: RealNumeric = 1.0,\n   \"\"\"\n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n+    if out_sharding is not None:\n+      raise NotImplementedError\n     dtype = dtypes.canonicalize_dtype(dtype)\n     if len(shape) < 2:\n       raise ValueError(\"orthogonal initializer requires at least a 2D shape\")\n@@ -651,7 +669,10 @@ def delta_orthogonal(\n   \"\"\"\n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n+    if out_sharding is not None:\n+      raise NotImplementedError\n     dtype = dtypes.canonicalize_dtype(dtype)\n     if len(shape) not in [3, 4, 5]:\n       raise ValueError(\"Delta orthogonal initializer requires a 3D, 4D or 5D \"\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 5d616c43ce54..024901b746a8 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7868,6 +7868,28 @@ def f(x):\n     self.assertEqual(out.sharding,\n                      NamedSharding(mesh.abstract_mesh, P('x', 'y')))\n \n+  @jtu.with_explicit_mesh((2,), ('x',))\n+  def test_he_normal(self, mesh):\n+    init = jax.nn.initializers.he_normal(in_axis=0, out_axis=1)\n+    key = jax.random.key(0)\n+    out = init(key, (8, 2), jnp.float32, out_sharding=P('x'))\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P('x', None)))\n+\n+  @jtu.with_explicit_mesh((2,), ('x',))\n+  def test_nn_uniform(self, mesh):\n+    init = jax.nn.initializers.uniform()\n+    key = jax.random.key(0)\n+    out = init(key, (8, 2), jnp.float32, out_sharding=P('x'))\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P('x', None)))\n+\n+  @jtu.with_explicit_mesh((2,), ('x',))\n+  def test_nn_constant(self, mesh):\n+    init = jax.nn.initializers.constant(-7)\n+    key = jax.random.key(0)\n+    out = init(key, (8, 2), jnp.float32, out_sharding=P('x'))\n+    self.assertArraysEqual(out, jnp.full((8, 2), -7, dtype=jnp.float32))\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P('x', None)))\n+\n \n @jtu.pytest_mark_if_available('multiaccelerator')\n class PJitErrorTest(jtu.JaxTestCase):\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "remove jaxlib_extension_version, ifrt_version and jaxlib.__version_info__ guards after 0.6.1 release.\n\nPiperOrigin-RevId: 761737523",
        "output": "```diff\nCommit: 0169f32fa2ea3166bcef7e113c9e3158195a9db3\nDate: 2025-05-22T00:45:02Z\nURL: https://github.com/jax-ml/jax/commit/0169f32fa2ea3166bcef7e113c9e3158195a9db3\nFiles changed: 34\nAdditions: +59, Deletions: -346\ndiff --git a/docs/autodidax.ipynb b/docs/autodidax.ipynb\nindex 07c7d7e84ff0..16d4da37b3f2 100644\n--- a/docs/autodidax.ipynb\n+++ b/docs/autodidax.ipynb\n@@ -1986,7 +1986,6 @@\n     \"from jax.extend.mlir import ir\\n\",\n     \"from jax.extend.mlir.dialects import func\\n\",\n     \"from jax.extend.mlir.dialects import stablehlo as hlo\\n\",\n-    \"import jax._src.lib\\n\",\n     \"from jax._src import xla_bridge as xb\\n\",\n     \"\\n\",\n     \"class MlirContext(NamedTuple):\\n\",\n@@ -2021,11 +2020,7 @@\n     \"  output = io.StringIO()\\n\",\n     \"  c.module.operation.print(file=output)\\n\",\n     \"  backend = xb.get_backend(None)\\n\",\n-    \"  if jax._src.lib.version >= (0, 6, 1):\\n\",\n-    \"    compiled = backend.compile(\\n\",\n-    \"      output.getvalue(), backend.devices()[:1])\\n\",\n-    \"  else:\\n\",\n-    \"    compiled = backend.compile(output.getvalue())\\n\",\n+    \"  compiled = backend.compile(output.getvalue(), backend.devices()[:1])\\n\",\n     \"  return partial(execute_compiled, compiled, [v.aval for v in jaxpr.outs])\\n\",\n     \"\\n\",\n     \"def _mlir_dtype(dtype: np.dtype) -> ir.Type:\\n\",\ndiff --git a/docs/autodidax.md b/docs/autodidax.md\nindex e78aeded41c0..870ee20f0f9a 100644\n--- a/docs/autodidax.md\n+++ b/docs/autodidax.md\n@@ -1556,7 +1556,6 @@ import io\n from jax.extend.mlir import ir\n from jax.extend.mlir.dialects import func\n from jax.extend.mlir.dialects import stablehlo as hlo\n-import jax._src.lib\n from jax._src import xla_bridge as xb\n \n class MlirContext(NamedTuple):\n@@ -1591,11 +1590,7 @@ def xla_callable(hashable_jaxpr: IDHashable,\n   output = io.StringIO()\n   c.module.operation.print(file=output)\n   backend = xb.get_backend(None)\n-  if jax._src.lib.version >= (0, 6, 1):\n-    compiled = backend.compile(\n-      output.getvalue(), backend.devices()[:1])\n-  else:\n-    compiled = backend.compile(output.getvalue())\n+  compiled = backend.compile(output.getvalue(), backend.devices()[:1])\n   return partial(execute_compiled, compiled, [v.aval for v in jaxpr.outs])\n \n def _mlir_dtype(dtype: np.dtype) -> ir.Type:\ndiff --git a/docs/autodidax.py b/docs/autodidax.py\nindex 9531ef7694c5..b0dbf9f73d9f 100644\n--- a/docs/autodidax.py\n+++ b/docs/autodidax.py\n@@ -1548,7 +1548,6 @@ def __eq__(self, other):\n from jax.extend.mlir import ir\n from jax.extend.mlir.dialects import func\n from jax.extend.mlir.dialects import stablehlo as hlo\n-import jax._src.lib\n from jax._src import xla_bridge as xb\n \n class MlirContext(NamedTuple):\n@@ -1583,11 +1582,7 @@ def main(*params):\n   output = io.StringIO()\n   c.module.operation.print(file=output)\n   backend = xb.get_backend(None)\n-  if jax._src.lib.version >= (0, 6, 1):\n-    compiled = backend.compile(\n-      output.getvalue(), backend.devices()[:1])\n-  else:\n-    compiled = backend.compile(output.getvalue())\n+  compiled = backend.compile(output.getvalue(), backend.devices()[:1])\n   return partial(execute_compiled, compiled, [v.aval for v in jaxpr.outs])\n \n def _mlir_dtype(dtype: np.dtype) -> ir.Type:\ndiff --git a/jax/_src/buffer_callback.py b/jax/_src/buffer_callback.py\nindex 739fdb4c408d..a1dfb5c2ff18 100644\n--- a/jax/_src/buffer_callback.py\n+++ b/jax/_src/buffer_callback.py\n@@ -27,16 +27,12 @@\n from jax._src.interpreters import ad\n from jax._src.interpreters import batching\n from jax._src.interpreters import mlir\n-from jax._src.lib import jaxlib_extension_version\n+from jax._src.lib import ffi as ffi_lib\n \n export = util.set_module(\"jax.experimental.buffer_callback\")\n-\n-if jaxlib_extension_version >= 334:\n-  from jax._src.lib import ffi as ffi_lib\n-\n-  Buffer = export(ffi_lib.Buffer)\n-  ExecutionStage = export(ffi_lib.ExecutionStage)\n-  ExecutionContext = export(ffi_lib.ExecutionContext)\n+Buffer = export(ffi_lib.Buffer)\n+ExecutionStage = export(ffi_lib.ExecutionStage)\n+ExecutionContext = export(ffi_lib.ExecutionContext)\n \n \n def buffer_callback(\ndiff --git a/jax/_src/compilation_cache.py b/jax/_src/compilation_cache.py\nindex aa1bd6ab65ba..058670642f41 100644\n--- a/jax/_src/compilation_cache.py\n+++ b/jax/_src/compilation_cache.py\n@@ -31,7 +31,6 @@\n from jax._src import config\n from jax._src import monitoring\n from jax._src.compilation_cache_interface import CacheInterface\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.lib import xla_client\n from jax._src.lib.mlir import ir\n from jax._src.lru_cache import LRUCache\n@@ -224,12 +223,8 @@ def get_executable_and_time(\n   executable_and_time = decompress_executable(executable_and_time)\n   serialized_executable, compile_time = extract_executable_and_time(\n       executable_and_time)\n-  if jaxlib_extension_version < 332:\n-    xla_executable_deserialized = backend.deserialize_executable(\n-        serialized_executable, compile_options)\n-  else:\n-    xla_executable_deserialized = backend.deserialize_executable(\n-        serialized_executable, executable_devices, compile_options)\n+  xla_executable_deserialized = backend.deserialize_executable(\n+      serialized_executable, executable_devices, compile_options)\n   return xla_executable_deserialized, compile_time\n \n \ndiff --git a/jax/_src/compiler.py b/jax/_src/compiler.py\nindex 04f993fed799..e1b8e7c35697 100644\n--- a/jax/_src/compiler.py\n+++ b/jax/_src/compiler.py\n@@ -35,7 +35,6 @@\n from jax._src import traceback_util\n from jax._src.interpreters import mlir\n from jax._src.lib import xla_client as xc\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.lib.mlir import ir\n import numpy as np\n \n@@ -314,12 +313,6 @@ def backend_compile(\n     )\n \n   try:\n-    if jaxlib_extension_version < 332:\n-      if host_callbacks:\n-        return backend.compile(\n-            built_c, compile_options=options, host_callbacks=host_callbacks)  # type: ignore\n-      return backend.compile(built_c, compile_options=options)  # type: ignore\n-\n     # we use a separate function call to ensure that XLA compilation appears\n     # separately in Python profiling results\n     if host_callbacks:\n@@ -692,12 +685,8 @@ def _compile_and_share_module(\n     serialized_executable = compilation_cache.decompress_executable(\n         serialized_executable\n     )\n-    if jaxlib_extension_version < 332:\n-      executable = backend.deserialize_executable(\n-          serialized_executable, compile_options)  # type: ignore\n-    else:\n-      executable = backend.deserialize_executable(\n-          serialized_executable, executable_devices, compile_options)  # type: ignore\n+    executable = backend.deserialize_executable(\n+        serialized_executable, executable_devices, compile_options)  # type: ignore\n \n   _compile_and_share_module.modules_cache[cache_key] = executable\n   return executable\ndiff --git a/jax/_src/lax/ann.py b/jax/_src/lax/ann.py\nindex bfcd45fba574..61d383ee29c2 100644\n--- a/jax/_src/lax/ann.py\n+++ b/jax/_src/lax/ann.py\n@@ -83,8 +83,6 @@ def pmap_mips(qy, db, db_offset, db_size, k, recall_target):\n from jax._src.interpreters import mlir\n from jax._src.lax import lax\n from jax._src.lib import _jax\n-from jax._src.lib import jaxlib_extension_version\n-from jax._src.lib import xla_client as xc\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import func\n from jax._src.lib.mlir.dialects import hlo\n@@ -233,14 +231,9 @@ def _approx_top_k_abstract_eval(operand, *, k, reduction_dimension,\n   if aggregate_to_topk:\n     dims[reduction_dimension] = k\n   elif core.is_constant_shape((reduction_input_size, k)):\n-    if jaxlib_extension_version >= 331:\n-      dims[reduction_dimension] = _jax.approx_top_k_reduction_output_size(\n-          reduction_input_size, len(dims), k, recall_target, aggregate_to_topk,\n-          reduction_input_size_override)[0]\n-    else:\n-      dims[reduction_dimension] = xc.ops.ApproxTopKReductionOutputSize(  # type: ignore  # pytype: disable=module-attr\n-          reduction_input_size, len(dims), k, recall_target, aggregate_to_topk,\n-          reduction_input_size_override)[0]\n+    dims[reduction_dimension] = _jax.approx_top_k_reduction_output_size(\n+        reduction_input_size, len(dims), k, recall_target, aggregate_to_topk,\n+        reduction_input_size_override)[0]\n   else:\n     raise NotImplementedError(\n          \"approx_top_k with aggregate_to_topk=False not yet implemented when \"\ndiff --git a/jax/_src/lax/linalg.py b/jax/_src/lax/linalg.py\nindex 857b115b06d8..2fda4a90369d 100644\n--- a/jax/_src/lax/linalg.py\n+++ b/jax/_src/lax/linalg.py\n@@ -48,7 +48,7 @@\n from jax._src.lib import gpu_solver\n from jax._src.lib import gpu_sparse\n from jax._src.lib import lapack\n-from jax._src.lib import version as jaxlib_version, jaxlib_extension_version\n+from jax._src.lib import version as jaxlib_version\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import chlo\n from jax._src.lib.mlir.dialects import hlo\n@@ -2530,30 +2530,6 @@ def _tridiagonal_solve_shape_rule(dl_shape, d_shape, du_shape, b_shape, **_):\n   return b_shape\n \n def _tridiagonal_solve_gpu_lowering(ctx, dl, d, du, b, *, target_name_prefix):\n-  if jaxlib_extension_version < 340:\n-    _, _, _, b_aval = ctx.avals_in\n-    *batch_dims, m, n = b_aval.shape\n-    batch_size = math.prod(batch_dims)\n-    mod = gpu_sparse._cusparse if target_name_prefix == \"cu\" else gpu_sparse._hipsparse\n-    assert mod is not None\n-    opaque = mod.build_gtsv2_descriptor(batch_size, m, n, m)\n-    if b_aval.dtype == np.float32:\n-      buffer_size = mod.gtsv2_f32_buffer_size(m, n, m)\n-      target_name = \"sparse_gtsv2_f32_ffi\"\n-    elif b_aval.dtype == np.float64:\n-      buffer_size = mod.gtsv2_f64_buffer_size(m, n, m)\n-      target_name = \"sparse_gtsv2_f64_ffi\"\n-    else:\n-      raise NotImplementedError(\n-          \"tridiagonal_solve is only implemented for float32 and float64 on GPU.\")\n-\n-    buffer_aval = core.ShapedArray(shape=(buffer_size,), dtype=np.int8)\n-    sub_ctx = ctx.replace(avals_out=[*ctx.avals_out, buffer_aval])\n-    rule = _linalg_ffi_lowering(\n-        f\"{target_name_prefix}{target_name}\", operand_output_aliases={3: 0},\n-        batch_partitionable=False)\n-    return rule(sub_ctx, dl, d, du, b, opaque=opaque)[:1]\n-\n   target_name = f\"{target_name_prefix}sparse_gtsv2_ffi\"\n   rule = _linalg_ffi_lowering(target_name, operand_output_aliases={3: 0})\n   return rule(ctx, dl, d, du, b)\ndiff --git a/jax/_src/lib/__init__.py b/jax/_src/lib/__init__.py\nindex 5cdcaf400c8a..8de05061ec99 100644\n--- a/jax/_src/lib/__init__.py\n+++ b/jax/_src/lib/__init__.py\n@@ -85,23 +85,13 @@ def _parse_version(v: str) -> tuple[int, ...]:\n \n import jaxlib.lapack as lapack  # noqa: F401\n import jaxlib.utils as utils  # noqa: F401\n-\n-if version >= (0, 6, 1):\n-  import jaxlib._jax as _jax  # noqa: F401\n-  from jaxlib._jax import guard_lib as guard_lib  # noqa: F401\n-  from jaxlib._jax import jax_jit as jax_jit  # noqa: F401\n-  from jaxlib._jax import pmap_lib as pmap_lib  # noqa: F401\n-  from jaxlib._jax import pytree as pytree  # noqa: F401\n-  from jaxlib._jax import Device as Device  # noqa: F401\n-  from jaxlib import _profiler as _profiler  # noqa: F401\n-else:\n-  import jaxlib.xla_extension as _jax  # type: ignore  # pytype: disable=import-error  # noqa: F401\n-  from jaxlib.xla_extension import guard_lib as guard_lib  # type: ignore  # pytype: disable=import-error  # noqa: F401\n-  from jaxlib.xla_extension import jax_jit as jax_jit  # type: ignore  # pytype: disable=import-error  # noqa: F401\n-  from jaxlib.xla_extension import pmap_lib as pmap_lib  # type: ignore  # pytype: disable=import-error  # noqa: F401\n-  from jaxlib.xla_extension import pytree as pytree  # type: ignore  # pytype: disable=import-error  # noqa: F401\n-  from jaxlib.xla_extension import Device as Device  # type: ignore  # pytype: disable=import-error  # noqa: F401\n-  from jaxlib.xla_extension import profiler as _profiler  # type: ignore  # pytype: disable=import-error  # noqa: F401\n+import jaxlib._jax as _jax  # noqa: F401\n+from jaxlib._jax import guard_lib as guard_lib  # noqa: F401\n+from jaxlib._jax import jax_jit as jax_jit  # noqa: F401\n+from jaxlib._jax import pmap_lib as pmap_lib  # noqa: F401\n+from jaxlib._jax import pytree as pytree  # noqa: F401\n+from jaxlib._jax import Device as Device  # noqa: F401\n+from jaxlib import _profiler as _profiler  # noqa: F401\n \n import jaxlib.xla_client as xla_client  # noqa: F401\n \n@@ -112,15 +102,9 @@ def _parse_version(v: str) -> tuple[int, ...]:\n jaxlib_extension_version: int = getattr(xla_client, '_version', 0)\n ifrt_version: int = getattr(xla_client, '_ifrt_version', 0)\n \n-if jaxlib_extension_version >= 334:\n-  from jaxlib._jax import ffi as ffi  # noqa: F401\n-\n-if jaxlib_extension_version >= 335:\n-  import jaxlib.cpu_sparse as cpu_sparse  # noqa: F401\n-\n-  has_cpu_sparse = True\n-else:\n-  has_cpu_sparse = False\n+from jaxlib._jax import ffi as ffi  # noqa: F401\n+import jaxlib.cpu_sparse as cpu_sparse  # noqa: F401\n+has_cpu_sparse = True\n \n import jaxlib.weakref_lru_cache as weakref_lru_cache  # noqa: F401\n \ndiff --git a/jax/_src/lib/mlir/dialects/__init__.py b/jax/_src/lib/mlir/dialects/__init__.py\nindex 5584afee2116..b49154e7936a 100644\n--- a/jax/_src/lib/mlir/dialects/__init__.py\n+++ b/jax/_src/lib/mlir/dialects/__init__.py\n@@ -57,7 +57,4 @@\n from jaxlib.mlir.dialects import stablehlo as hlo\n \n from jax._src import lib\n-if lib.version >= (0, 6, 1):\n-  from jaxlib.mlir.dialects import cf\n-else:\n-  cf = None  # type: ignore[no-redef]\n+from jaxlib.mlir.dialects import cf\ndiff --git a/jax/_src/named_sharding.py b/jax/_src/named_sharding.py\nindex ae99236a6cdc..faf0b2a9f2b2 100644\n--- a/jax/_src/named_sharding.py\n+++ b/jax/_src/named_sharding.py\n@@ -23,7 +23,6 @@\n from jax._src import config\n from jax._src.util import use_cpp_class, cache, use_cpp_method\n from jax._src.lib import xla_client as xc\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.lib.mlir.dialects import sdy\n from jax._src import mesh as mesh_lib\n from jax._src.mesh import AxisType\n@@ -317,17 +316,11 @@ def build(self) -> sdy.TensorShardingAttr:\n \n     replicated_axes = _get_axes(self.replicated_axes, self.mesh_shape)\n     unreduced_axes = _get_axes(self.unreduced_axes, self.mesh_shape)\n-    if jaxlib_extension_version >= 342:\n-      return sdy.TensorShardingAttr.get(\n-          mesh_attr,\n-          [dim_sharding.build() for dim_sharding in self.dim_shardings],\n-          replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes],\n-          unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in unreduced_axes])\n-    else:\n-      return sdy.TensorShardingAttr.get(\n-          mesh_attr,\n-          [dim_sharding.build() for dim_sharding in self.dim_shardings],\n-          replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes])\n+    return sdy.TensorShardingAttr.get(\n+        mesh_attr,\n+        [dim_sharding.build() for dim_sharding in self.dim_shardings],\n+        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes],\n+        unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in unreduced_axes])\n \n   def __repr__(self):\n     dim_sharding_repr = ', '.join(\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex b6960c479558..e212f9770a94 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -31,7 +31,6 @@\n from jax import lax\n from jax._src import checkify\n from jax._src import core as jax_core\n-from jax._src import lib as jaxlib\n from jax._src import linear_util as lu\n from jax._src import mesh as mesh_lib\n from jax._src import pjit\n@@ -1569,11 +1568,6 @@ def _broadcast_in_dim_lowering_rule_wg(\n         ir.VectorType.get(shape, mgpu_utils.dtype_to_ir_type(x_aval.dtype)),\n         x,\n     )\n-\n-  # TODO(dasenov): Remove this after the minimal jaxlib version is 0.6.1.\n-  if jaxlib.version < (0, 6, 1):\n-    raise NotImplementedError()\n-\n   mlir_type = mgpu_utils.dtype_to_ir_type(x_aval.dtype)\n   result_ty = ir.VectorType.get(shape, mlir_type)\n   return mgpu.dialect.broadcast_in_dim(result_ty, x, broadcast_dimensions)\ndiff --git a/jax/_src/profiler.py b/jax/_src/profiler.py\nindex 6b58b2ba6326..424e2b81035f 100644\n--- a/jax/_src/profiler.py\n+++ b/jax/_src/profiler.py\n@@ -215,7 +215,7 @@ def stop_trace():\n     if _profile_state.profile_session is None:\n       raise RuntimeError(\"No profile started\")\n     sess = _profile_state.profile_session\n-    sess.stop_and_export(str(_profile_state.log_dir))\n+    sess.stop_and_export(str(_profile_state.log_dir))  # type: ignore\n     if _profile_state.create_perfetto_trace:\n       abs_filename = _write_perfetto_trace_file(_profile_state.log_dir)\n       if _profile_state.create_perfetto_link:\ndiff --git a/jax/_src/test_util.py b/jax/_src/test_util.py\nindex bb1ef6595ec3..f6810b533b31 100644\n--- a/jax/_src/test_util.py\n+++ b/jax/_src/test_util.py\n@@ -54,7 +54,6 @@\n from jax._src import mesh as mesh_lib\n from jax._src.cloud_tpu_init import running_in_cloud_tpu_vm\n from jax._src.interpreters import mlir\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.lib.mlir.dialects import hlo\n from jax._src.numpy.util import promote_dtypes, promote_dtypes_inexact\n from jax._src.public_test_util import (  # noqa: F401\n@@ -357,18 +356,16 @@ def assert_num_jit_and_pmap_compilations(times):\n \n @contextmanager\n def count_internal_device_puts():\n-  if jaxlib_extension_version >= 341:\n-    before = jax._src.lib._jax.get_internal_device_put_info()\n+  before = jax._src.lib._jax.get_internal_device_put_info()\n   counts = {}\n   try:\n     yield lambda: counts\n   finally:\n-    if jaxlib_extension_version >= 341:\n-      after = jax._src.lib._jax.get_internal_device_put_info()\n-      for k, v in after.items():\n-        diff = v - before.get(k, 0)\n-        if diff != 0:\n-          counts[k] = diff\n+    after = jax._src.lib._jax.get_internal_device_put_info()\n+    for k, v in after.items():\n+      diff = v - before.get(k, 0)\n+      if diff != 0:\n+        counts[k] = diff\n \n def jaxlib_version() -> tuple[int, ...]:\n   return _jaxlib.version\ndiff --git a/jax/experimental/buffer_callback.py b/jax/experimental/buffer_callback.py\nindex 6c8514340af0..f919cfa10208 100644\n--- a/jax/experimental/buffer_callback.py\n+++ b/jax/experimental/buffer_callback.py\n@@ -12,16 +12,9 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from jax._src.lib import jaxlib_extension_version as _jaxlib_extension_version\n-\n-if _jaxlib_extension_version >= 334:\n-  from jax._src.buffer_callback import (\n-      Buffer as Buffer,\n-      ExecutionContext as ExecutionContext,\n-      ExecutionStage as ExecutionStage,\n-      buffer_callback as buffer_callback,\n-  )\n-\n-from jax._src.buffer_callback import buffer_callback as buffer_callback\n-\n-del _jaxlib_extension_version\n+from jax._src.buffer_callback import (\n+    Buffer as Buffer,\n+    ExecutionContext as ExecutionContext,\n+    ExecutionStage as ExecutionStage,\n+    buffer_callback as buffer_callback,\n+)\ndiff --git a/jax/experimental/jax2tf/tests/sharding_test.py b/jax/experimental/jax2tf/tests/sharding_test.py\nindex 5fc45df218cd..fa15522cbe90 100644\n--- a/jax/experimental/jax2tf/tests/sharding_test.py\n+++ b/jax/experimental/jax2tf/tests/sharding_test.py\n@@ -33,7 +33,6 @@\n from jax._src import config\n from jax._src import test_util as jtu\n from jax._src import xla_bridge\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.lib import xla_client as xc\n from jax import lax\n from jax.experimental import jax2tf\n@@ -111,12 +110,8 @@ def log_jax_hlo(self, f_jax, args: Sequence[Any], *,\n           device_assignment=device_assignment,\n           use_spmd_partitioning=use_spmd_partitioning,\n       )\n-      if jaxlib_extension_version < 332:\n-        executable = backend.compile(\n-            jax_hlo, compile_options=compile_options)  # type: ignore\n-      else:\n-        executable = backend.compile_and_load(\n-            jax_hlo, xc.DeviceList(tuple(self.devices.flat)), compile_options)  # type: ignore\n+      executable = backend.compile_and_load(\n+          jax_hlo, xc.DeviceList(tuple(self.devices.flat)), compile_options)  # type: ignore\n       jax_optimized_hlo = executable.hlo_modules()[0].to_string()\n       logging.info(\"[%s] got JAX optimized HLO for platform %s %s\",\n                    self._testMethodName, backend.platform, jax_optimized_hlo)\ndiff --git a/jax/experimental/serialize_executable.py b/jax/experimental/serialize_executable.py\nindex 6f5062d4ce99..7c112f56ef42 100644\n--- a/jax/experimental/serialize_executable.py\n+++ b/jax/experimental/serialize_executable.py\n@@ -19,7 +19,6 @@\n import io\n \n import jax\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.lib import xla_client as xc\n from typing import Sequence\n \n@@ -110,8 +109,6 @@ def __init__(self, file, backend, execution_devices=None):\n \n   def persistent_load(self, pid):\n     if pid[0] == 'exec':\n-      if jaxlib_extension_version < 332:\n-        return self.backend.deserialize_executable(pid[1])\n       return self.backend.deserialize_executable(\n           pid[1], executable_devices=self.execution_devices)\n     if pid[0] == 'device':\ndiff --git a/jaxlib/py_client.cc b/jaxlib/py_client.cc\nindex 0a99d94f81cc..842bdfecad3d 100644\n--- a/jaxlib/py_client.cc\n+++ b/jaxlib/py_client.cc\n@@ -373,14 +373,9 @@ std::unique_ptr<ifrt::CompileOptions> MakeIfrtCompileOptions(\n     ifrt_loaded_host_callbacks.push_back(tsl::FormRef(\n         static_cast<ifrt::LoadedHostCallback*>(host_callback.data())));\n   }\n-#if JAX_IFRT_VERSION_NUMBER >= 6\n   return std::make_unique<ifrt::XlaCompileOptions>(\n       std::move(options), std::move(executable_devices),\n       std::move(ifrt_loaded_host_callbacks));\n-#else\n-  return std::make_unique<ifrt::XlaCompileOptions>(\n-      std::move(options), std::move(ifrt_loaded_host_callbacks));\n-#endif\n }\n \n // Makes IFRT `DeserializeExecutableOptions` from XLA `CompileOptions` and\n@@ -398,14 +393,9 @@ MakeIfrtDeserializeExecutableOptions(std::optional<CompileOptions> options,\n     ifrt_loaded_host_callbacks.push_back(tsl::FormRef(\n         static_cast<ifrt::LoadedHostCallback*>(host_callback.data())));\n   }\n-#if JAX_IFRT_VERSION_NUMBER >= 6\n   return std::make_unique<ifrt::XlaDeserializeExecutableOptions>(\n       std::move(options), std::move(executable_devices),\n       std::move(ifrt_loaded_host_callbacks));\n-#else\n-  return std::make_unique<ifrt::XlaDeserializeExecutableOptions>(\n-      std::move(options), std::move(ifrt_loaded_host_callbacks));\n-#endif\n }\n \n }  // namespace\n@@ -504,14 +494,9 @@ PyClient::CompileAndLoad(nb_class_ptr<PyClient> client, std::string mlir_module,\n         client->ifrt_client(), std::move(host_callback));\n     ifrt_loaded_host_callbacks.push_back(callback);\n   }\n-#if JAX_IFRT_VERSION_NUMBER >= 6\n   auto compile_options = std::make_unique<ifrt::XlaCompileOptions>(\n       std::move(options), std::move(executable_devices),\n       std::move(ifrt_loaded_host_callbacks));\n-#else\n-  auto compile_options = std::make_unique<ifrt::XlaCompileOptions>(\n-      std::move(options), std::move(ifrt_loaded_host_callbacks));\n-#endif\n   return CompileAndLoadIfrtProgram(\n       client, std::make_unique<xla::ifrt::HloProgram>(module.get()),\n       std::move(compile_options));\ndiff --git a/jaxlib/py_compile_only_client.cc b/jaxlib/py_compile_only_client.cc\nindex 0fa2f4b48fd7..2de896d80bef 100644\n--- a/jaxlib/py_compile_only_client.cc\n+++ b/jaxlib/py_compile_only_client.cc\n@@ -91,12 +91,8 @@ class CompileOnlyPyClient : public PyClient {\n         llvm::dyn_cast_or_null<CompileOnlyIfRtClient>(this->ifrt_client());\n     CHECK(ifrt_client) << \"CompileOnlyPyClient requires ifrt_client be a \"\n                           \"CompileOnlyIfRtClient\";\n-#if JAX_IFRT_VERSION_NUMBER >= 6\n     auto xla_options = std::make_unique<ifrt::XlaCompileOptions>(\n         options, std::move(executable_devices));\n-#else\n-    auto xla_options = std::make_unique<ifrt::XlaCompileOptions>(options);\n-#endif\n     TF_ASSIGN_OR_RETURN(auto executable,\n                         PjRtCompile(std::move(options), module.get(),\n                                     *ifrt_client->topology().description()));\ndiff --git a/jaxlib/py_program.cc b/jaxlib/py_program.cc\nindex 40bfd3497ebd..ee2d3eef9973 100644\n--- a/jaxlib/py_program.cc\n+++ b/jaxlib/py_program.cc\n@@ -236,16 +236,11 @@ absl::StatusOr<std::unique_ptr<ifrt::CompileOptions>> MakeXlaCompileOptions(\n     ifrt_loaded_host_callbacks.push_back(tsl::FormRef(\n         static_cast<ifrt::LoadedHostCallback*>(host_callback.data())));\n   }\n-#if JAX_IFRT_VERSION_NUMBER >= 6\n   TF_ASSIGN_OR_RETURN(ifrt::DeviceListRef executable_devices,\n                       py_executable_devices.ifrt_device_list());\n   return std::make_unique<ifrt::XlaCompileOptions>(\n       std::move(options), std::move(executable_devices),\n       std::move(ifrt_loaded_host_callbacks));\n-#else\n-  return std::make_unique<ifrt::XlaCompileOptions>(\n-      std::move(options), std::move(ifrt_loaded_host_callbacks));\n-#endif\n }\n \n constexpr absl::string_view kColocatedPythonProgramType =\ndiff --git a/jaxlib/py_socket_transfer.cc b/jaxlib/py_socket_transfer.cc\nindex fde63df8da47..8086196b9df8 100644\n--- a/jaxlib/py_socket_transfer.cc\n+++ b/jaxlib/py_socket_transfer.cc\n@@ -110,58 +110,10 @@ absl::StatusOr<xla::PjRtMemorySpace*> MemorySpaceFromSharding(\n   }\n }\n \n-#if JAX_IFRT_VERSION_NUMBER < 8\n-class IfrtArrayEntry : public PullTable::Entry {\n- public:\n-  struct BufferRef {\n-    xla::ifrt::ArrayRef arr;\n-    xla::PjRtBuffer* buffer;\n-    size_t buf_size;\n-  };\n-  explicit IfrtArrayEntry(std::vector<BufferRef> arrs,\n-                          std::shared_ptr<PremappedCopierState> state,\n-                          size_t xfer_size)\n-      : arrs_(std::move(arrs)), state_(state), xfer_size_(xfer_size) {}\n-  bool Handle(tsl::RCReference<ConnectionState> state,\n-              const SocketTransferPullRequest& req,\n-              size_t base_req_id) override {\n-    for (uint64_t bid : req.buffer_ids()) {\n-      auto req_id = base_req_id;\n-      ++base_req_id;\n-      for (size_t i = 0; i * xfer_size_ < arrs_[bid].buf_size; ++i) {\n-        DmaCopyChunk blob = DmaCopyChunk::Make(\n-            std::move(arrs_[bid].arr), arrs_[bid].buffer, bid, i * xfer_size_,\n-            std::min(xfer_size_, arrs_[bid].buf_size - i * xfer_size_));\n-        bool is_largest = blob.size + blob.offset == arrs_[bid].buf_size;\n-        state_->ScheduleCopy(\n-            std::move(blob), [req_id, state, copier_state = state_, is_largest](\n-                                 PremappedCopierState* copier_state_ptr,\n-                                 void* buf, const DmaCopyChunk& chunk) {\n-              state->Send(\n-                  req_id, buf, chunk.offset, chunk.size, is_largest,\n-                  [copier_state, buf]() { copier_state->ReturnBuffer(buf); });\n-            });\n-      }\n-    }\n-\n-    num_consumed_bufs_ += req.buffer_ids().size();\n-    return num_consumed_bufs_ == arrs_.size();\n-  }\n-\n- private:\n-  absl::Mutex mu_;\n-  size_t num_consumed_bufs_ = 0;\n-  std::vector<BufferRef> arrs_;\n-  std::shared_ptr<PremappedCopierState> state_;\n-  size_t xfer_size_;\n-};\n-#endif\n-\n absl::StatusOr<tsl::RCReference<PullTable::Entry>> CreatePullEntry(\n     const std::vector<xla::ifrt::ArrayRef>& arrs,\n     std::shared_ptr<PremappedCopierState> state, size_t xfer_size,\n     bool use_raw_buffers) {\n-#if JAX_IFRT_VERSION_NUMBER >= 8\n   if (use_raw_buffers) {\n     std::vector<RawBufferEntry::BufferRef> refs;\n     for (auto& arr : arrs) {\n@@ -196,21 +148,6 @@ absl::StatusOr<tsl::RCReference<PullTable::Entry>> CreatePullEntry(\n     }\n   }\n   return tsl::MakeRef<PjRtBufferEntry>(std::move(refs), state, xfer_size);\n-#else\n-  std::vector<IfrtArrayEntry::BufferRef> refs;\n-  for (auto& arr : arrs) {\n-    auto* pjrt_arr = llvm::dyn_cast_or_null<xla::ifrt::PjRtArray>(arr.get());\n-    if (pjrt_arr == nullptr) {\n-      return absl::InvalidArgumentError(\n-          \"Cannot remote transfer non-pjrt arrays.\");\n-    }\n-    for (auto& pjrt_buf : pjrt_arr->pjrt_buffers()) {\n-      TF_ASSIGN_OR_RETURN(size_t buf_size, pjrt_buf->GetOnDeviceSizeInBytes());\n-      refs.push_back({arr, pjrt_buf.get(), buf_size});\n-    }\n-  }\n-  return tsl::MakeRef<IfrtArrayEntry>(std::move(refs), state, xfer_size);\n-#endif\n }\n \n class PyTransferServerConnection {\ndiff --git a/jaxlib/util.cc b/jaxlib/util.cc\nindex a014afa5bebe..a8d45749f4d1 100644\n--- a/jaxlib/util.cc\n+++ b/jaxlib/util.cc\n@@ -36,7 +36,6 @@ limitations under the License.\n namespace xla {\n \n void BlockUntilReadyWithCancel(xla::PjRtFuture<>& future) {\n-#if JAX_IFRT_VERSION_NUMBER >= 5\n   future.BlockUntilReady([](tsl::AsyncValue* value) {\n     auto state = std::make_shared<absl::Notification>();\n     value->AndThen([state]() { state->Notify(); });\n@@ -50,7 +49,6 @@ void BlockUntilReadyWithCancel(xla::PjRtFuture<>& future) {\n       }\n     }\n   });\n-#endif\n }\n \n absl::Status AwaitBuffersReady(absl::Span<ifrt::Array* const> ifrt_arrays) {\ndiff --git a/jaxlib/xla.cc b/jaxlib/xla.cc\nindex 3412766de6bd..d97c6868a04b 100644\n--- a/jaxlib/xla.cc\n+++ b/jaxlib/xla.cc\n@@ -490,27 +490,8 @@ NB_MODULE(_jax, m) {\n               &CompiledMemoryStats::host_temp_size_in_bytes)\n       .def_prop_ro(\"serialized_buffer_assignment_proto\",\n                    [](const CompiledMemoryStats& cms) -> nb::bytes {\n-#if JAX_IFRT_VERSION_NUMBER >= 9\n                      const std::string& s = cms.serialized_buffer_assignment;\n                      return nb::bytes(s.data(), s.size());\n-#elif JAX_IFRT_VERSION_NUMBER >= 7\n-                     if (cms.buffer_assignment.has_value()) {\n-                       std::string s =\n-                           cms.buffer_assignment->SerializeAsString();\n-                       return nb::bytes(s.data(), s.size());\n-                     } else {\n-                       return nb::bytes();\n-                     }\n-#else\n-                     xla::HloProto hlo;\n-                     if (!cms.serialized_hlo_proto.empty() &&\n-                         hlo.ParseFromString(cms.serialized_hlo_proto)) {\n-                       std::string s =\n-                           hlo.buffer_assignment().SerializeAsString();\n-                       return nb::bytes(s.data(), s.size());\n-                     }\n-                     return nb::bytes();\n-#endif\n                    })\n       .def(\"__str__\", &CompiledMemoryStats::DebugString);\n \ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 9963e2603588..f5b74e1e10d6 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -61,7 +61,6 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.compilation_cache import is_persistent_cache_enabled\n from jax._src.lib import _jax\n-from jax._src.lib import jaxlib_extension_version\n import jax._src.util as jax_util\n from jax.ad_checkpoint import checkpoint_name, checkpoint as new_checkpoint\n from jax.errors import (UnexpectedTracerError, TracerIntegerConversionError,\n@@ -1975,11 +1974,6 @@ def test_device_put_sharding_mismatched_tree_different_leaf_count(self):\n       jax.device_put((x, y, z), device=(s1, s2))\n \n   def test_internal_device_put_with_device(self):\n-    if jaxlib_extension_version < 341:\n-      raise unittest.SkipTest(\n-          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n-          \" DevicePut calls\")\n-\n     # Hitting the cache for a single-device jitted execution while using a numpy\n     # array calls internal `DevicePutWithDevice`.\n     f = jax.jit(lambda x: x + 1)\n@@ -1990,10 +1984,6 @@ def test_internal_device_put_with_device(self):\n     self.assertEqual(counts(), {\"device_put_with_device\": 1})\n \n   def test_internal_device_put_fully_replicated(self):\n-    if jaxlib_extension_version < 341:\n-      raise unittest.SkipTest(\n-          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n-          \" DevicePut calls\")\n     if jax.device_count() < 2:\n       raise unittest.SkipTest(\"Test requires >= 2 devices\")\n \n@@ -2011,10 +2001,6 @@ def test_internal_device_put_fully_replicated(self):\n     )\n \n   def test_internal_device_put_batched(self):\n-    if jaxlib_extension_version < 341:\n-      raise unittest.SkipTest(\n-          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n-          \" DevicePut calls\")\n     if jax.device_count() < 2:\n       raise unittest.SkipTest(\"Test requires >= 2 devices\")\n \n@@ -2031,10 +2017,6 @@ def test_internal_device_put_batched(self):\n     )\n \n   def test_internal_device_put_assembled(self):\n-    if jaxlib_extension_version < 341:\n-      raise unittest.SkipTest(\n-          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n-          \" DevicePut calls\")\n     if jax.device_count() < 2:\n       raise unittest.SkipTest(\"Test requires >= 2 devices\")\n \ndiff --git a/tests/buffer_callback_test.py b/tests/buffer_callback_test.py\nindex e77ee4af687f..8bef4135f5d5 100644\n--- a/tests/buffer_callback_test.py\n+++ b/tests/buffer_callback_test.py\n@@ -19,7 +19,6 @@\n import jax\n import jax.numpy as jnp\n from jax._src import test_util as jtu\n-from jax._src.lib import jaxlib_extension_version\n from jax.experimental import buffer_callback\n \n jax.config.parse_flags_with_absl()\n@@ -29,10 +28,6 @@ class BufferCallbackTest(jtu.JaxTestCase):\n \n   def setUp(self):\n     super().setUp()\n-    if jaxlib_extension_version < 334:\n-      self.skipTest(\n-          \"Requires a version of jaxlib with buffer callback support.\"\n-      )\n     if jtu.test_device_matches([\"tpu\"]):\n       self.skipTest(\"Not supported on TPU.\")\n \n@@ -102,7 +97,7 @@ def callback(ctx, out, arg):\n   )\n   @jtu.run_on_devices(\"cuda\")\n   def test_cuda_array_interface(self, dtype, command_buffer_compatible):\n-    if command_buffer_compatible and jaxlib_extension_version < 337:\n+    if command_buffer_compatible:\n       self.skipTest(\"Requires jaxlib extension version of at least 337.\")\n \n     def callback(ctx, out, arg):\ndiff --git a/tests/compilation_cache_test.py b/tests/compilation_cache_test.py\nindex 5a76d732bd76..3f1bb7fab4b1 100644\n--- a/tests/compilation_cache_test.py\n+++ b/tests/compilation_cache_test.py\n@@ -146,14 +146,10 @@ def test_diff_executables(self):\n     )\n     backend = xla_bridge.get_backend()\n     executable_devices = xc.DeviceList(tuple(backend.local_devices()))\n-    if jax._src.lib.jaxlib_extension_version < 331:\n-      executable1 = backend.compile(computation1, compile_options)\n-      executable2 = backend.compile(computation2, compile_options)\n-    else:\n-      executable1 = backend.compile_and_load(\n-          computation1, executable_devices, compile_options)\n-      executable2 = backend.compile_and_load(\n-          computation2, executable_devices, compile_options)\n+    executable1 = backend.compile_and_load(\n+        computation1, executable_devices, compile_options)\n+    executable2 = backend.compile_and_load(\n+        computation2, executable_devices, compile_options)\n     cc.put_executable_and_time(\n         \"key1\", \"computation1\", executable1, backend, FAKE_COMPILE_TIME)\n     cc.put_executable_and_time(\n@@ -177,11 +173,8 @@ def test_put_executable(self):\n     )\n     backend = xla_bridge.get_backend()\n     executable_devices = xc.DeviceList(tuple(devices.flat))\n-    if jax._src.lib.jaxlib_extension_version < 331:\n-      executable = backend.compile(str(computation), compile_options)\n-    else:\n-      executable = backend.compile_and_load(\n-          str(computation), executable_devices, compile_options)\n+    executable = backend.compile_and_load(\n+        str(computation), executable_devices, compile_options)\n     key = cc.get_cache_key(computation, devices, compile_options, backend)\n     cc.put_executable_and_time(\n         key, \"alambda\", executable, backend, FAKE_COMPILE_TIME)\n@@ -577,13 +570,9 @@ def test_backend_serialization_deserialization(self):\n         .runtime_executable()\n     )\n     serialized_executable = backend.serialize_executable(executable)\n-    if jax._src.lib.jaxlib_extension_version < 331:\n-      deserialized_executable = backend.deserialize_executable(  # type: ignore\n-          serialized_executable, None)\n-    else:\n-      deserialized_executable = backend.deserialize_executable(  # type: ignore\n-          serialized_executable,\n-          xc.DeviceList(tuple(jax.local_devices(backend=backend))), None)\n+    deserialized_executable = backend.deserialize_executable(  # type: ignore\n+        serialized_executable,\n+        xc.DeviceList(tuple(jax.local_devices(backend=backend))), None)\n     self.assertEqual(\n         executable.fingerprint, deserialized_executable.fingerprint)\n \ndiff --git a/tests/fused_attention_stablehlo_test.py b/tests/fused_attention_stablehlo_test.py\nindex 925fc2ed4825..64e0f4377462 100644\n--- a/tests/fused_attention_stablehlo_test.py\n+++ b/tests/fused_attention_stablehlo_test.py\n@@ -503,9 +503,6 @@ def test_sdpa_broadcast_bias_and_dbias(self):\n   )\n   @jtu.run_on_devices(\"cuda\")\n   def test_sdpa_dbias(self, batch_size: int):\n-    # TODO: Delete once 0.6.0 is no longer supported.\n-    if jtu.jaxlib_version() == (0, 6, 0):\n-      self.skipTest(\"jaxlib 0.6.0 has a bug\")\n     if jax.device_count() < 4:\n       self.skipTest(\"Requires more than 4 devices.\")\n     # cuDNN only supports dbias when batch size is 1. If the batch size is\ndiff --git a/tests/linalg_sharding_test.py b/tests/linalg_sharding_test.py\nindex e68e94e16494..5d7b3b8a637b 100644\n--- a/tests/linalg_sharding_test.py\n+++ b/tests/linalg_sharding_test.py\n@@ -22,7 +22,6 @@\n from jax import lax\n from jax._src import config\n from jax._src import test_util as jtu\n-from jax._src.lib import jaxlib_extension_version\n from jax.sharding import PartitionSpec as P\n \n config.parse_flags_with_absl()\n@@ -70,8 +69,7 @@ def get_fun_and_shapes(self, fun_and_shapes, grad=False):\n         self.skipTest(\n             f\"Partitioning {fun_and_shapes[0].__name__} only supported on GPU \"\n             \"when shardy is enabled.\")\n-      if (fun_and_shapes[0] == lax.linalg.tridiagonal_solve and\n-          jaxlib_extension_version < 340):\n+      if fun_and_shapes[0] == lax.linalg.tridiagonal_solve:\n         self.skipTest(\n             f\"Partitioning {fun_and_shapes[0].__name__} on GPU, requires a \"\n             \"more recent jaxlib version.\")\ndiff --git a/tests/linalg_test.py b/tests/linalg_test.py\nindex cba3dbb7189d..99cb66c92857 100644\n--- a/tests/linalg_test.py\n+++ b/tests/linalg_test.py\n@@ -33,7 +33,6 @@\n from jax._src.lax import linalg as lax_linalg\n from jax._src import test_util as jtu\n from jax._src import xla_bridge\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.numpy.util import promote_dtypes_inexact\n \n config.parse_flags_with_absl()\n@@ -2205,7 +2204,7 @@ def testSelect(self, dtype):\n   @jtu.sample_product(shape=[(3,), (3, 4), (3, 4, 5)],\n                       dtype=float_types + complex_types)\n   def test_tridiagonal_solve(self, shape, dtype):\n-    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]) and jaxlib_extension_version < 340:\n+    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]):\n       self.skipTest(\"Data type not supported on GPU\")\n     rng = self.rng()\n     d = 1.0 + jtu.rand_positive(rng)(shape, dtype)\n@@ -2244,7 +2243,7 @@ def test_tridiagonal_solve_endpoints(self):\n \n   @jtu.sample_product(shape=[(3,), (3, 4)], dtype=float_types + complex_types)\n   def test_tridiagonal_solve_grad(self, shape, dtype):\n-    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]) and jaxlib_extension_version < 340:\n+    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]):\n       self.skipTest(\"Data type not supported on GPU\")\n     rng = self.rng()\n     d = 1.0 + jtu.rand_positive(rng)(shape, dtype)\ndiff --git a/tests/mosaic/gpu_layout_inference_test.py b/tests/mosaic/gpu_layout_inference_test.py\nindex 038766542f3b..cdc840b0a6f1 100644\n--- a/tests/mosaic/gpu_layout_inference_test.py\n+++ b/tests/mosaic/gpu_layout_inference_test.py\n@@ -19,7 +19,6 @@\n from absl.testing import parameterized\n import jax\n from jax._src import config\n-from jax._src import lib as jaxlib\n from jax._src import test_util as jtu\n from jax._src.interpreters import mlir as mlir_interpreter\n from jax._src.lib.mlir import ir\n@@ -245,12 +244,7 @@ def body(x):\n   def test_infer_broadcast_in_dim_layout(\n       self, broadcast_dim, in_cast, out_cast, in_layout, out_layout\n   ):\n-    # TODO(dasenov): Remove this after the minimal jaxlib version is 0.6.1.\n-    if jaxlib.version < (0, 6, 1):\n-      self.skipTest(\"Test requires jaxlib version >= 0.6.1\")\n-\n     bcast = None\n-\n     in_shape = (64,)\n     out_shape = (64, 64)\n \ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 4e0544d1758e..e7fc9723347b 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -27,7 +27,6 @@\n from absl.testing import absltest, parameterized\n import jax\n from jax._src import config\n-from jax._src import lib as jaxlib\n from jax._src import test_util as jtu\n from jax._src.interpreters import mlir\n from jax._src.lib.mlir import ir\n@@ -3116,10 +3115,6 @@ def add(\n       ((64,), (128, 64), [1]),\n   )\n   def test_broadcast_in_dim(self, input_shape, output_shape, bcast_dims):\n-    # TODO(dasenov): Remove this after the minimal jaxlib version is 0.6.1.\n-    if jaxlib.version < (0, 6, 1):\n-      self.skipTest(\"Test requires jaxlib version >= 0.6.1\")\n-\n     element_value = 42.0\n     def body(ctx, result_gmem_ref, smem):\n       del ctx\ndiff --git a/tests/pallas/gpu_pallas_distributed_test.py b/tests/pallas/gpu_pallas_distributed_test.py\nindex 81433b8c5067..d862e6b9b819 100644\n--- a/tests/pallas/gpu_pallas_distributed_test.py\n+++ b/tests/pallas/gpu_pallas_distributed_test.py\n@@ -34,8 +34,6 @@\n class PallasCallRemoteDMATest(jt_multiprocess.MultiProcessTest):\n \n   def setUp(self):\n-    if jtu.jaxlib_version() < (0, 6, 1):\n-      self.skipTest(\"Test requires jaxlib >= 0.6.1\")\n     if (not jtu.test_device_matches([\"cuda\"]) or\n         not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n       self.skipTest(\"Only works on GPU with capability >= sm90\")\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex aa2e0af2a57c..5d616c43ce54 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -63,7 +63,6 @@\n from jax._src import xla_bridge\n from jax._src.lib import xla_client as xc\n from jax._src.lib import _jax\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.util import curry, unzip2\n \n config.parse_flags_with_absl()\n@@ -7762,8 +7761,6 @@ def f(x):\n   @config.use_shardy_partitioner(True)\n   @jtu.with_explicit_mesh((2, 2), ('x', 'y'))\n   def test_unreduced_basic(self, mesh):\n-    if jaxlib_extension_version < 342:\n-      self.skipTest(\"Test requires a newer jaxlib\")\n     np_inp = np.arange(16).reshape(8, 2)\n     x = jax.device_put(np_inp, P('x', 'y'))\n     y = jax.device_put(np_inp.T, P('y', None))\ndiff --git a/tests/python_callback_test.py b/tests/python_callback_test.py\nindex 26664faa6faf..eef45b3b412b 100644\n--- a/tests/python_callback_test.py\n+++ b/tests/python_callback_test.py\n@@ -31,7 +31,6 @@\n from jax.experimental import io_callback\n from jax.experimental import pjit\n from jax._src.shard_map import shard_map\n-from jax._src.lib import jaxlib_extension_version\n import jax.numpy as jnp\n from jax.sharding import Mesh\n import numpy as np\n@@ -588,8 +587,6 @@ def fun(x):\n \n   @parameterized.parameters(\"int2\", \"int4\", \"uint2\", \"uint4\", \"float4_e2m1fn\")\n   def test_subbyte_operands(self, dtype: str):\n-    if jaxlib_extension_version < 336:\n-      self.skipTest(\"Requires jaxlib_extension_version >= 336.\")\n     if \"2\" in dtype and jtu.test_device_matches([\"tpu\"]):\n       self.skipTest(\n           \"TODO(dsuo): TPU callbacks send SIGABRT for int2, uint2, and\"\n@@ -609,8 +606,6 @@ def f(x):\n \n   @parameterized.parameters(\"int2\", \"int4\", \"uint2\", \"uint4\", \"float4_e2m1fn\")\n   def test_subbyte_results(self, dtype: str):\n-    if jaxlib_extension_version < 336:\n-      self.skipTest(\"Requires jaxlib_extension_version >= 336.\")\n     if \"2\" in dtype and jtu.test_device_matches([\"tpu\"]):\n       self.skipTest(\n           \"TODO(dsuo): TPU callbacks send SIGABRT for int2, uint2, and\"\n@@ -630,8 +625,6 @@ def f():\n \n   @parameterized.parameters(\"int2\", \"int4\", \"uint2\", \"uint4\", \"float4_e2m1fn\")\n   def test_non_default_stride_subbyte_results(self, dtype: str):\n-    if jaxlib_extension_version < 336:\n-      self.skipTest(\"Requires jaxlib_extension_version >= 336.\")\n     if \"2\" in dtype and jtu.test_device_matches([\"tpu\"]):\n       self.skipTest(\n           \"TODO(dsuo): TPU callbacks send SIGABRT for int2, uint2, and\"\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add initial support aligned jnp.swapaxes on major/minor dims\n\nNext steps:\n  - non-tile aligned\n  - Clean up fn and utilize it for general changeTiling\n\nPiperOrigin-RevId: 761731600",
        "output": "```diff\nCommit: 62c46ff976db981912d2182760d4f2abffb972fa\nDate: 2025-05-22T00:24:34Z\nURL: https://github.com/jax-ml/jax/commit/62c46ff976db981912d2182760d4f2abffb972fa\nFiles changed: 4\nAdditions: +375, Deletions: -13\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 919c548adc9b..dd0b9ba4b4a7 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -2383,7 +2383,9 @@ def _gather_lowering_rule(\n \n @register_lowering_rule(lax.transpose_p)\n def _transpose_lowering_rule(ctx: LoweringRuleContext, x, *, permutation):\n-  if permutation != (1, 0):\n+  minormost_transpose = (1, 0)\n+  untiled_tiled_swap = (1, 0, 2)\n+  if permutation not in (minormost_transpose, untiled_tiled_swap):\n     raise NotImplementedError\n   out_type = aval_to_ir_type(\n       ctx.lowering_context.dynamic_shape_replacement_fn, ctx.avals_out[0]\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 99134b46315f..6502a9c6682e 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -5013,11 +5013,315 @@ LogicalResult vector_transpose_rule(RewriteContext &ctx, Operation &op,\n                   ctx.target_shape));\n   ArrayRef<int64_t> permutation = transpose_op.getPermutation();\n   const auto tile_perm = permutation.take_back(2);\n+\n+  // Major minor pemute\n   if (tile_perm != ArrayRef<int64_t>{rank - 2, rank - 1} &&\n       tile_perm != ArrayRef<int64_t>{rank - 1, rank - 2}) {\n-    return transpose_op->emitOpError(\n-        \"Not implemented: Unsupported permutation\");\n+    // This is a 3 stage algorithm that uses combinations and shuffles\n+    // to do a transposition of an 8x8 block of sublanes.\n+    // In the following algorithm description, A, B, ..., H represent 8\n+    // distinct input vregs that form an 8x8 block of data\n+    // to be transposed. In our notation, B2 identifies the third\n+    // sublane (2) of the second vreg (B)\".\n+    //\n+    //\n+    // If we think of each starting input vreg as a row in an 8x8 block of\n+    // elements:\n+    // A: A0 A1 A2 A3 A4 A5 A6 A7\n+    // B: B0 B1 B2 B3 B4 B5 B6 B7\n+    // ...\n+    // H: H0 H1 H2 H3 H4 H5 H6 H7\n+    //\n+    // The goal is to transpose this block, so the output vregs are:\n+    // out0: A0 B0 C0 D0 E0 F0 G0 H0\n+    // out1: A1 B1 C1 D1 E1 F1 G1 H1\n+    // ...\n+    // out7: A7 B7 C7 D7 E7 F7 G7 H7\n+    //\n+    // Stage 1: Operates on pairs of input vregs (e.g., A and B).\n+    //\n+    // Input to Stage 1 (example pair A, B):\n+    // A: A0 A1 A2 A3 A4 A5 A6 A7\n+    // B: B0 B1 B2 B3 B4 B5 B6 B7\n+    //\n+    // Step 1.1: Combine low/high halves.\n+    //   combine_low(A, B)  -> CL_AB: [A0 A1 A2 A3 | B0 B1 B2 B3] (8 elements)\n+    //   combine_high(A, B) -> CH_AB: [A4 A5 A6 A7 | B4 B5 B6 B7] (8 elements)\n+    //   (Notation: '|' separates the 4 elements from A and 4 from B)\n+    //\n+    // Step 1.2: Shuffle.\n+    //   The shuffle pattern for the low part (applied to CL_AB using\n+    //   `shuffle(CL_AB, CH_AB, pattern)`) is {0, 4, 1, 5, 2, 6, 3, 7}.\n+    //   The shuffle pattern for the high part (applied to CH_AB using\n+    //   `shuffle(CL_AB, CH_AB, pattern)`) is {8, 12, 9, 13, 10, 14, 11, 15}.\n+    //   (Indices 0-7 in shuffle refer to CL_AB, 8-15 to CH_AB).\n+    // This results in:\n+    //   s1_AB_0: A0 B0 A1 B1 A2 B2 A3 B3 (from shuffling CL_AB elements)\n+    //   s1_AB_1: A4 B4 A5 B5 A6 B6 A7 B7 (from shuffling CH_AB elements)\n+    //\n+    // Output of Stage 1 / Input to Stage 2 (example for A,B,C,D processing):\n+    //   s1_vregs[0] (from A,B): A0 B0 A1 B1 A2 B2 A3 B3\n+    //   s1_vregs[1] (from A,B): A4 B4 A5 B5 A6 B6 A7 B7\n+    //   s1_vregs[2] (from C,D): C0 D0 C1 D1 C2 D2 C3 D3\n+    //   s1_vregs[3] (from C,D): C4 D4 C5 D5 C6 D6 C7 D7\n+    //   ... (and so on for E,F,G,H into s1_vregs[4-7])\n+\n+    // Stage 2: Operates on groups of 4 vregs from Stage 1 output.\n+    //          (e.g., s1_vregs[0], s1_vregs[1], s1_vregs[2], s1_vregs[3])\n+    //\n+    // Input to Stage 2 (example processing s1_vregs[0] and s1_vregs[2]):\n+    //   X = s1_vregs[0] = [A0 B0 A1 B1 | A2 B2 A3 B3]\n+    //   Y = s1_vregs[2] = [C0 D0 C1 D1 | C2 D2 C3 D3]\n+    //\n+    // Step 2.1: Combine low/high halves.\n+    //   combine_low(X, Y)  -> CL_XY: [A0 B0 A1 B1 | C0 D0 C1 D1]\n+    //   combine_high(X, Y) -> CH_XY: [A2 B2 A3 B3 | C2 D2 C3 D3]\n+    //\n+    //   (Similarly for s1_vregs[1] and s1_vregs[3], let them be X' and Y')\n+    //   combine_low(X', Y')  -> CL_X'Y': [A4 B4 A5 B5 | C4 D4 C5 D5]\n+    //   combine_high(X', Y') -> CH_X'Y': [A6 B6 A7 B7 | C6 D6 C7 D7]\n+    //\n+    // Step 2.2: Shuffle.\n+    //   The shuffle pattern for the low part (e.g., applied to CL_XY) is {0, 1,\n+    //   4, 5, 2, 3, 6, 7}. The shuffle pattern for the high part (e.g., applied\n+    //   to CH_XY, effectively) is {8, 9, 12, 13, 10, 11, 14, 15}.\n+    //\n+    // This results in (for the first group of 4 input vregs A,B,C,D):\n+    //   s2_vregs[0]: A0 B0 C0 D0 A1 B1 C1 D1 (from shuffling CL_XY elements)\n+    //   s2_vregs[1]: A2 B2 C2 D2 A3 B3 C3 D3 (from shuffling CH_XY elements)\n+    //   s2_vregs[2]: A4 B4 C4 D4 A5 B5 C5 D5 (from shuffling CL_X'Y' elements)\n+    //   s2_vregs[3]: A6 B6 C6 D6 A7 B7 C7 D7 (from shuffling CH_X'Y' elements)\n+    //\n+    // Output of Stage 2 / Input to Stage 3:\n+    //   s2_vregs[0]: A0 B0 C0 D0 A1 B1 C1 D1\n+    //   s2_vregs[1]: A2 B2 C2 D2 A3 B3 C3 D3\n+    //   s2_vregs[2]: A4 B4 C4 D4 A5 B5 C5 D5\n+    //   s2_vregs[3]: A6 B6 C6 D6 A7 B7 C7 D7\n+    //   s2_vregs[4]: E0 F0 G0 H0 E1 F1 G1 H1 (from E,F,G,H processing)\n+    //   s2_vregs[5]: E2 F2 G2 H2 E3 F3 G3 H3\n+    //   s2_vregs[6]: E4 F4 G4 H4 E5 F5 G5 H5\n+    //   s2_vregs[7]: E6 F6 G6 H6 E7 F7 G7 H7\n+\n+    // Stage 3: Combine results from Stage 2. No shuffle needed after combine.\n+    // Input to Stage 3 (example for the first two rows of the final transpose):\n+    //   L = s2_vregs[0] = [A0 B0 C0 D0 | A1 B1 C1 D1]\n+    //   R = s2_vregs[4] = [E0 F0 G0 H0 | E1 F1 G1 H1]\n+    //\n+    // Step 3.1: Combine low/high halves.\n+    //   combine_low(L, R)  -> [A0 B0 C0 D0 | E0 F0 G0 H0] ->\n+    //     Final out0: A0 B0 C0 D0 E0 F0 G0 H0\n+    //   combine_high(L, R) -> [A1 B1 C1 D1 | E1 F1 G1 H1] ->\n+    //     Final out1: A1 B1 C1 D1 E1 F1 G1 H1\n+    //   ... and so on for other pairs from Stage 2 output\n+    // (e.g. L=s2_vregs[1], R=s2_vregs[5]).\n+    //\n+    // This results in the correctly transposed 8x8 block.\n+\n+    constexpr int64_t kMajorDimOriginalIdx = 0;\n+    constexpr int64_t kSecondMinorDimOriginalIdx = 1;\n+    constexpr int64_t kMinorMostDimOriginalIdx = 2;\n+\n+    auto vec_shape = src_ty.getShape();\n+    auto major_dim_size = vec_shape[kMajorDimOriginalIdx];\n+    auto second_minor_dim_size = vec_shape[kSecondMinorDimOriginalIdx];\n+\n+    if (layout_in.offsets() != LayoutOffsets{0, 0}) {\n+      return transpose_op.emitOpError(\"Not implemented: Layout with offset.\");\n+    }\n+    if (layout_in.implicit_dim() != VectorLayout::ImplicitDim::kNone) {\n+      return transpose_op.emitOpError(\n+          \"Not implemented: Layout with implicit dimension.\");\n+    }\n+\n+    auto sublane_count = ctx.target_shape[0];\n+    if (second_minor_dim_size % sublane_count != 0 ||\n+        major_dim_size % sublane_count != 0) {\n+      return transpose_op.emitOpError(\n+          \"Not implemented: Swapping major and second minor dimensions must \"\n+          \"result in dimension sizes that are multiples of sublane_count.\");\n+    }\n+\n+    if (!layout_in.hasNativeTiling(ctx.target_shape)) {\n+      return transpose_op.emitOpError(\n+          \"Not implemented: Expected native input tiling.\");\n+    }\n+    if (layout_in != layout_out) {\n+      return transpose_op.emitOpError(\n+          \"Not implemented: Expected same input and output layouts.\");\n+    }\n+    xla::Array<Value> dst_vregs(\n+        layout_out.tileArrayShape(dst_ty.getShape(), ctx.target_shape));\n+\n+    if (layout_in.bitwidth() != 32) {\n+      return transpose_op.emitOpError(\n+          \"Not implemented: Major-second-minor transpose only supported for \"\n+          \"32-bit vectors. Also, input must be a vector type.\");\n+    }\n+    if (ctx.target_shape[0] != 8) {\n+      return transpose_op.emitOpError(\n+          \"Not implemented: Major-second-minor transpose expects 8 sublanes.\");\n+    }\n+\n+    auto vreg_dimensions = src_vregs.dimensions();\n+    // Note(mvoz): Slice is a weird word here, This is used for constructing\n+    // the output vregs - the reason we divide here is because we multiply it\n+    // back later on to get the correct index into src_vregs, but the reason\n+    // we cannot just resolve that in our outer loop is because of the nature\n+    // of a transpose - this dim value goes unmultiplied into the output vregs.\n+    // effectively, our indexing:\n+    // {major_dim_slice_idx * sublane_count, second_minor_dim_slice_idx,\n+    // minor_most_dim_slice_idx} becomes {second_minor_dim_slice_idx *\n+    // sublane_count, major_dim_slice_idx, minor_most_dim_slice_idx}\n+    auto num_slices_in_major_dim =\n+        vreg_dimensions[kMajorDimOriginalIdx] / sublane_count;\n+    auto num_slices_in_second_minor_dim =\n+        vreg_dimensions[kSecondMinorDimOriginalIdx];\n+    auto num_slices_in_minor_most_dim =\n+        vreg_dimensions[kMinorMostDimOriginalIdx];\n+\n+    auto shuffle = [&](Value lhs_vreg, Value rhs_vreg, ArrayRef<int> pattern) {\n+      auto lhs_vreg_type = lhs_vreg.getType();\n+      auto pattern_attr = builder.getDenseI32ArrayAttr(pattern);\n+      return builder\n+          .create<tpu::SublaneShuffleOp>(transpose_op.getLoc(), lhs_vreg_type,\n+                                         lhs_vreg, rhs_vreg, pattern_attr)\n+          .getResult();\n+    };\n+\n+    static constexpr std::array<int, 8> combine_low_pattern = {0, 1, 2,  3,\n+                                                               8, 9, 10, 11};\n+    static constexpr std::array<int, 8> combine_high_pattern = {4,  5,  6,  7,\n+                                                                12, 13, 14, 15};\n+\n+    auto combine_low = [&](Value lhs_vreg, Value rhs_vreg) {\n+      return shuffle(lhs_vreg, rhs_vreg, combine_low_pattern);\n+    };\n+    auto combine_high = [&](Value lhs_vreg, Value rhs_vreg) {\n+      return shuffle(lhs_vreg, rhs_vreg, combine_high_pattern);\n+    };\n+\n+    // Shuffle patterns for Stage 1\n+    // Input to shuffle: (combine_low_val, combine_high_val)\n+    // combine_low_val has A0-A3, B0-B3. Indices 0-7 for shuffle.\n+    // combine_high_val has A4-A7, B4-B7. Indices 8-15 for shuffle.\n+    static constexpr std::array<int, 8> permute_pattern_stage1_low_arr = {\n+        0, 4, 1, 5,\n+        2, 6, 3, 7};  // Selects from combine_low_val to make A0B0A1B1A2B2A3B3\n+    static constexpr std::array<int, 8> permute_pattern_stage1_high_arr = {\n+        8,  12, 9, 13, 10,\n+        14, 11, 15};  // Selects from combine_high_val to make A4B4A5B5A6B6A7B7\n+\n+    // Shuffle patterns for Stage 2\n+    // Input to shuffle: (CL_XY, CH_XY) from Step 2.1 in comments.\n+    // CL_XY has A0B0A1B1C0D0C1D1. Indices 0-7 for shuffle.\n+    // CH_XY has A2B2A3B3C2D2C3D3. Indices 8-15 for shuffle.\n+    static constexpr std::array<int, 8> permute_pattern_stage2_low_arr = {\n+        0, 1, 4, 5, 2, 3, 6, 7};  // Selects from CL_XY to make A0B0C0D0A1B1C1D1\n+    static constexpr std::array<int, 8> permute_pattern_stage2_high_arr = {\n+        8,  9,  12, 13,\n+        10, 11, 14, 15};  // Selects from CH_XY to make A2B2C2D2A3B3C3D3\n+\n+    for (int major_dim_slice_idx = 0;\n+         major_dim_slice_idx < num_slices_in_major_dim; ++major_dim_slice_idx) {\n+      for (int second_minor_dim_slice_idx = 0;\n+           second_minor_dim_slice_idx < num_slices_in_second_minor_dim;\n+           ++second_minor_dim_slice_idx) {\n+        for (int minor_most_dim_slice_idx = 0;\n+             minor_most_dim_slice_idx < num_slices_in_minor_most_dim;\n+             ++minor_most_dim_slice_idx) {\n+          // STAGE 1!\n+          std::array<Value, 8>\n+              stage1_output_vregs;  // Stores s1_vregs from comments\n+          constexpr int num_pairs_stage1 =\n+              4;  // Processes 4 pairs of vregs (A,B), (C,D), (E,F), (G,H)\n+\n+          for (int i = 0; i < num_pairs_stage1; ++i) {\n+            Value first_vreg = src_vregs(\n+                {(2 * i) + (sublane_count * major_dim_slice_idx),\n+                 second_minor_dim_slice_idx, minor_most_dim_slice_idx});\n+            Value second_vreg = src_vregs(\n+                {(2 * i) + (sublane_count * major_dim_slice_idx) + 1,\n+                 second_minor_dim_slice_idx, minor_most_dim_slice_idx});\n+\n+            auto combined_low_val = combine_low(first_vreg, second_vreg);\n+            auto combined_high_val = combine_high(first_vreg, second_vreg);\n+\n+            stage1_output_vregs[2 * i] =\n+                shuffle(combined_low_val, combined_high_val,\n+                        permute_pattern_stage1_low_arr);\n+            stage1_output_vregs[2 * i + 1] =\n+                shuffle(combined_low_val, combined_high_val,\n+                        permute_pattern_stage1_high_arr);\n+          }\n+\n+          // STAGE 2!\n+          std::array<Value, 8>\n+              stage2_output_vregs;  // Stores s2_vregs from comments\n+          constexpr int num_pairs_stage2 =\n+              4;  // Processes 4 pairs of vregs from stage1_output_vregs\n+\n+          for (int i = 0; i < num_pairs_stage2; ++i) {\n+            // Determine the indices for the input pair from\n+            // stage1_output_vregs. The 4 pairs processed in this stage are:\n+            // i=0: (s1_vregs[0], s1_vregs[2])\n+            // i=1: (s1_vregs[1], s1_vregs[3])\n+            // i=2: (s1_vregs[4], s1_vregs[6])\n+            // i=3: (s1_vregs[5], s1_vregs[7])\n+            int s1_lhs_idx = (i / 2) * 4 + (i % 2);\n+            int s1_rhs_idx = s1_lhs_idx + 2;\n+\n+            Value s1_lhs_vreg = stage1_output_vregs[s1_lhs_idx];\n+            Value s1_rhs_vreg = stage1_output_vregs[s1_rhs_idx];\n+\n+            auto combined_low_val = combine_low(s1_lhs_vreg, s1_rhs_vreg);\n+            auto combined_high_val = combine_high(s1_lhs_vreg, s1_rhs_vreg);\n+\n+            // Determine the output indices for stage2_output_vregs.\n+            // Each pair from Stage 1 produces a pair of vregs for Stage 2.\n+            // Results are stored pair-wise:\n+            // i=0 -> s2_vregs[0], s2_vregs[1]\n+            // i=1 -> s2_vregs[2], s2_vregs[3]\n+            // i=2 -> s2_vregs[4], s2_vregs[5]\n+            // i=3 -> s2_vregs[6], s2_vregs[7]\n+            int s2_out_idx_base = 2 * i;\n+\n+            stage2_output_vregs[s2_out_idx_base] =\n+                shuffle(combined_low_val, combined_high_val,\n+                        permute_pattern_stage2_low_arr);\n+            stage2_output_vregs[s2_out_idx_base + 1] =\n+                shuffle(combined_low_val, combined_high_val,\n+                        permute_pattern_stage2_high_arr);\n+          }\n+\n+          // STAGE 3! Combine results from stage 2.\n+          std::array<int64_t, 3> output_idx_parts{\n+              second_minor_dim_slice_idx * sublane_count, major_dim_slice_idx,\n+              minor_most_dim_slice_idx};\n+\n+          constexpr int num_final_combines =\n+              4;  // Corresponds to s2_vregs[0]..s2_vregs[3] pairing with\n+                  // s2_vregs[4]..s2_vregs[7]\n+          for (int i = 0; i < num_final_combines; ++i) {\n+            Value lhs = stage2_output_vregs[i];      // e.g., s2_ABCD_0\n+            Value rhs = stage2_output_vregs[i + 4];  // e.g., s2_EFGH_0\n+            auto final_combined_low = combine_low(lhs, rhs);\n+            auto final_combined_high = combine_high(lhs, rhs);\n+\n+            dst_vregs(output_idx_parts) = final_combined_low;\n+            output_idx_parts[0] += 1;\n+            dst_vregs(output_idx_parts) = final_combined_high;\n+            output_idx_parts[0] += 1;\n+          }\n+        }\n+      }\n+    }\n+    auto assembled =\n+        assemble(builder, dst_ty, layout_out, dst_vregs, ctx.target_shape);\n+    transpose_op.getOperation()->replaceAllUsesWith(assembled);\n+    transpose_op.erase();\n+    return success();\n   }\n+\n   {\n     SmallVector<int64_t> p(permutation);\n     p[rank - 2] = rank - 2;\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex f01d0b4c5888..976e31cb55f4 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -1680,17 +1680,27 @@ class VectorLayoutInferer {\n     auto src_ty = op.getSourceVectorType();\n     TPU_CHECK_OP(permutation.size() == src_ty.getRank(),\n                  \"Transpose permutation has incorrect rank\");\n-    for (auto dim : permutation.drop_back(2)) {\n-      TPU_CHECK_OP(dim < src_ty.getRank() - 2,\n-                   \"Unsupported transpose permutation - minor dims into major\");\n-    }\n-    for (auto dim : permutation.take_back(2)) {\n-      TPU_CHECK_OP(dim >= src_ty.getRank() - 2,\n-                   \"Unsupported transpose permutation - major dims into minor\");\n+    bool untiled_tiled_swap = false;\n+    // TODO(mvoz): Expand to more general cases. b/419268277\n+    if (permutation.size() == 3 && permutation[0] == 1 && permutation[1] == 0) {\n+      untiled_tiled_swap = true;\n+    } else {\n+      for (auto dim : permutation.drop_back(2)) {\n+        TPU_CHECK_OP(dim < src_ty.getRank() - 2,\n+                     \"Unsupported transpose permutation - minor dims into \"\n+                     \"major > 3 dimensions\");\n+      }\n+      for (auto dim : permutation.take_back(2)) {\n+        TPU_CHECK_OP(dim >= src_ty.getRank() - 2,\n+                     \"Unsupported transpose permutation - major dims into \"\n+                     \"minor > 3 dimensions\");\n+      }\n     }\n     Layout required_layout = some_layout;\n-    // Require native tiling if we're going to use the XLU.\n-    if (permutation[permutation.size() - 1] == permutation.size() - 2) {\n+    // Require native tiling if we're going to use the XLU, or doing a\n+    // major/minor permute.\n+    if (untiled_tiled_swap ||\n+        permutation[permutation.size() - 1] == permutation.size() - 2) {\n       auto native_tiling = nativeTiling(layout.bitwidth());\n       required_layout = VectorLayout(layout.bitwidth(), LayoutOffsets{0, 0},\n                                      native_tiling, ImplicitDim::kNone);\ndiff --git a/tests/pallas/ops_test.py b/tests/pallas/ops_test.py\nindex bcda0ca9f71e..3e777ac7ea2c 100644\n--- a/tests/pallas/ops_test.py\n+++ b/tests/pallas/ops_test.py\n@@ -300,7 +300,7 @@ def pallas_call(cls, *args, **kwargs):\n     return pl.pallas_call(*args, interpret=cls.INTERPRET, **kwargs)\n \n   def skip_if_mosaic_gpu(self):\n-    if jtu.test_device_matches([\"cuda\"]) and use_mosaic_gpu:\n+    if jtu.test_device_matches([\"gpu\"]) and use_mosaic_gpu:\n       self.skipTest(\"TODO: Mosaic GPU does not support this yet\")\n \n \n@@ -2569,6 +2569,52 @@ def kernel(x_ref, out_ref):\n     )(x)\n     np.testing.assert_array_equal(out, np.diagonal(x))\n \n+  @parameterized.product(\n+      # Skip some steps to just run less cases\n+      # TODO(mvoz): Hypothesis?\n+      x_dim_size=tuple(8 * i for i in range(1, 5)),\n+      y_dim_size=tuple(8 * i for i in range(1, 5)),\n+      z_dim_size=tuple(128 * i for i in range(1, 3)),\n+      dtype=(jnp.float32,),\n+  )\n+  def test_jnp_swapaxes_major_minor(\n+      self, x_dim_size, y_dim_size, z_dim_size, dtype\n+  ):\n+    if jtu.test_device_matches([\"gpu\"]):\n+      if any(\n+          not is_power_of_two(x) for x in [x_dim_size, y_dim_size, z_dim_size]\n+      ):\n+        self.skipTest(\n+            \"the Pallas Triton lowering currently requires that all operations\"\n+            \" have array arguments and results whose size is a power of 2.\"\n+            f\" Encountered an array of shape ({x_dim_size}, {y_dim_size},\"\n+            f\" {z_dim_size})\"\n+        )\n+      if x_dim_size * y_dim_size * z_dim_size * 4 > 32768:\n+        self.skipTest(\n+            \"Mosaic GPU kernel exceeds available shared memory\"\n+            f\" smem_bytes={x_dim_size * y_dim_size * z_dim_size * 4} > 32768\"\n+        )\n+    self.skip_if_mosaic_gpu()\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 22):\n+      self.skipTest(\"Requires libtpu built after 2025-5-22\")\n+\n+    x = jnp.arange(x_dim_size * y_dim_size * z_dim_size, dtype=dtype).reshape(\n+        (x_dim_size, y_dim_size, z_dim_size)\n+    )\n+\n+    def kernel(x_ref, out_ref):\n+      out_ref[...] = jnp.swapaxes(x_ref[...], 0, 1)\n+\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct(\n+            (y_dim_size, x_dim_size, z_dim_size), dtype\n+        ),\n+    )(x)\n+    expected = jnp.swapaxes(x, 0, 1)\n+    np.testing.assert_array_equal(out, expected)\n+\n \n class OpsInterpretTest(OpsTest):\n   INTERPRET = True\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move _src/interpreters/ad.py to its own BUILD rule.\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nThis required moving some internal utilities out of dispatch.py, which is part of the main JAX build rule. I chose api_util.py because they seem to fit there.\n\nPiperOrigin-RevId: 761722054",
        "output": "```diff\nCommit: 8da86ea0a3128d2cff517251f40a5b3e285f4d7b\nDate: 2025-05-21T23:54:46Z\nURL: https://github.com/jax-ml/jax/commit/8da86ea0a3128d2cff517251f40a5b3e285f4d7b\nFiles changed: 9\nAdditions: +81, Deletions: -61\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 4218cd3f0a77..e431a3c5056c 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -316,7 +316,6 @@ py_library_providing_imports_info(\n         \"_src/ffi.py\",\n         \"_src/flatten_util.py\",\n         \"_src/interpreters/__init__.py\",\n-        \"_src/interpreters/ad.py\",\n         \"_src/interpreters/batching.py\",\n         \"_src/interpreters/pxla.py\",\n         \"_src/pjit.py\",\n@@ -381,6 +380,7 @@ py_library_providing_imports_info(\n     visibility = [\"//visibility:public\"],\n     deps = [\n         \":abstract_arrays\",\n+        \":ad\",\n         \":ad_util\",\n         \":api_util\",\n         \":basearray\",\n@@ -671,6 +671,23 @@ pytype_strict_library(\n     ] + py_deps(\"numpy\"),\n )\n \n+pytype_strict_library(\n+    name = \"ad\",\n+    srcs = [\"_src/interpreters/ad.py\"],\n+    deps = [\n+        \":ad_util\",\n+        \":api_util\",\n+        \":config\",\n+        \":core\",\n+        \":dtypes\",\n+        \":mesh\",\n+        \":partial_eval\",\n+        \":source_info_util\",\n+        \":tree_util\",\n+        \":util\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"mlir\",\n     srcs = [\"_src/interpreters/mlir.py\"],\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 059db1c92c98..3ff103997dc7 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -37,6 +37,7 @@\n import numpy as np\n from contextlib import contextmanager\n \n+from jax._src import api_util\n from jax._src import deprecations\n from jax._src import linear_util as lu\n from jax._src import stages\n@@ -113,14 +114,14 @@ def _nan_check_posthook(fun, args, kwargs, output):\n \n   try:\n     dispatch.check_special(pjit.pjit_p.name, buffers)\n-  except dispatch.InternalFloatingPointError as e:\n+  except api_util.InternalFloatingPointError as e:\n     assert config.debug_nans.value or config.debug_infs.value\n     if hasattr(fun, '_fun'):\n       f = fun._fun\n       if getattr(f, '_apply_primitive', False):\n         raise FloatingPointError(f\"invalid value ({e.ty}) encountered in {f.__qualname__}\") from None\n       # compiled_fun can only raise in this case\n-      dispatch.maybe_recursive_nan_check(e, f, args, kwargs)\n+      api_util.maybe_recursive_nan_check(e, f, args, kwargs)\n       raise AssertionError(\"Unreachable\") from e\n     else:\n       # TODO(emilyaf): Shouldn't need this fallback.\n@@ -1707,7 +1708,7 @@ def cache_miss(*args, **kwargs):\n           out = execute(*p.flat_args)\n         else:\n           out = pxla.xla_pmap_p.bind_with_trace(trace, (p.flat_fun, *p.flat_args), params)\n-      except dispatch.InternalFloatingPointError as e:\n+      except api_util.InternalFloatingPointError as e:\n         raise FloatingPointError(f'Invalid value ({e.ty}) encountered in parallel computation.')\n \n     out_tree, out_flat = p.out_tree, out\ndiff --git a/jax/_src/api_util.py b/jax/_src/api_util.py\nindex 163bade2065c..2e7ba551c624 100644\n--- a/jax/_src/api_util.py\n+++ b/jax/_src/api_util.py\n@@ -767,3 +767,41 @@ def _check_no_aliased_closed_over_refs(dbg: core.DebugInfo, consts, args) -> Non\n           f\"array reference of type {a.str_short()} was both closed over and \"\n           f\"passed as the argument \"\n           f\"{dbg.safe_arg_names(len(args))[i]}\" if dbg else \"at flat index {i}\")\n+\n+class InternalFloatingPointError(Exception):\n+  name: str\n+  ty: str\n+\n+  def __init__(self, name: str, ty: str):\n+    self.name = name\n+    self.ty = ty\n+\n+def maybe_recursive_nan_check(e: Exception, fun: Callable, args, kwargs,\n+) -> None:  # always raises an exception\n+  print(\"Invalid nan value encountered in the output of a jax.jit \"\n+        \"function. Calling the de-optimized version.\")\n+  try:\n+    _ = fun(*args, **kwargs)\n+  except (FloatingPointError, ZeroDivisionError) as e2:\n+    raise e2 from None\n+  else:\n+    _raise_no_nan_in_deoptimized(e)\n+\n+\n+def _raise_no_nan_in_deoptimized(e) -> None:\n+  msg = (f\"{str(e)}. Because \"\n+        \"jax_config.debug_nans.value and/or config.jax_debug_infs is set, the \"\n+        \"de-optimized function (i.e., the function as if the `jit` \"\n+        \"decorator were removed) was called in an attempt to get a more \"\n+        \"precise error message. However, the de-optimized function did not \"\n+        \"produce invalid values during its execution. This behavior can \"\n+        \"result from `jit` optimizations causing the invalid value to be \"\n+        \"produced. It may also arise from having nan/inf literals as \"\n+        \"inputs or outputs, like `jax.jit(lambda ...: jax.numpy.nan)(...)`. \"\n+        \"\\n\\n\"\n+        \"It may be possible to avoid the invalid value by removing the \"\n+        \"`jit` decorator, at the cost of losing optimizations. \"\n+        \"\\n\\n\"\n+        \"If you see this error, consider opening a bug report at \"\n+        \"https://github.com/jax-ml/jax.\")\n+  raise FloatingPointError(msg) from None\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex 9a11ffa104a8..d1ea7439cb0c 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -24,7 +24,7 @@\n import logging\n import threading\n import time\n-from typing import Any, Callable\n+from typing import Any\n \n import jax\n from jax._src import api\n@@ -42,6 +42,7 @@\n from jax._src.interpreters import mlir\n from jax._src.interpreters import pxla\n from jax._src.interpreters import xla\n+from jax._src.api_util import InternalFloatingPointError\n from jax._src.layout import DeviceLocalLayout, Layout\n from jax._src.lib import xla_client as xc\n from jax._src.mesh import AbstractMesh, Mesh\n@@ -341,43 +342,6 @@ class CopySemantics(enum.Enum):\n   COPY = enum.auto()\n   DONATE = enum.auto()\n \n-class InternalFloatingPointError(Exception):\n-  name: str\n-  ty: str\n-\n-  def __init__(self, name: str, ty: str):\n-    self.name = name\n-    self.ty = ty\n-\n-def maybe_recursive_nan_check(e: Exception, fun: Callable, args, kwargs,\n-) -> None:  # always raises an exception\n-  print(\"Invalid nan value encountered in the output of a jax.jit \"\n-        \"function. Calling the de-optimized version.\")\n-  try:\n-    _ = fun(*args, **kwargs)\n-  except (FloatingPointError, ZeroDivisionError) as e2:\n-    raise e2 from None\n-  else:\n-    _raise_no_nan_in_deoptimized(e)\n-\n-def _raise_no_nan_in_deoptimized(e) -> None:\n-  msg = (f\"{str(e)}. Because \"\n-        \"jax_config.debug_nans.value and/or config.jax_debug_infs is set, the \"\n-        \"de-optimized function (i.e., the function as if the `jit` \"\n-        \"decorator were removed) was called in an attempt to get a more \"\n-        \"precise error message. However, the de-optimized function did not \"\n-        \"produce invalid values during its execution. This behavior can \"\n-        \"result from `jit` optimizations causing the invalid value to be \"\n-        \"produced. It may also arise from having nan/inf literals as \"\n-        \"inputs or outputs, like `jax.jit(lambda ...: jax.numpy.nan)(...)`. \"\n-        \"\\n\\n\"\n-        \"It may be possible to avoid the invalid value by removing the \"\n-        \"`jit` decorator, at the cost of losing optimizations. \"\n-        \"\\n\\n\"\n-        \"If you see this error, consider opening a bug report at \"\n-        \"https://github.com/jax-ml/jax.\")\n-  raise FloatingPointError(msg) from None\n-\n def _identity_fn(x):\n   return x\n \ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 29af03416a76..9366b91f8022 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -21,12 +21,12 @@\n from functools import partial\n from typing import Any\n \n+from jax._src import api_util\n from jax._src import config\n-from jax._src import dispatch\n from jax._src import linear_util as lu\n from jax._src.interpreters import partial_eval as pe\n-from jax.tree_util import (tree_flatten, tree_unflatten,\n-                           register_pytree_node, Partial, PyTreeDef)\n+from jax._src.tree_util import (tree_flatten, tree_unflatten,\n+                                register_pytree_node, Partial, PyTreeDef)\n from jax._src import mesh as mesh_lib\n from jax._src import core\n from jax._src import source_info_util\n@@ -1125,7 +1125,7 @@ def out_axes_thunk():\n \n   try:\n     out_flat = primitive.bind(fun, *all_args, **new_params)\n-  except dispatch.InternalFloatingPointError as e:\n+  except api_util.InternalFloatingPointError as e:\n     print(\"Invalid nan value encountered in the backward pass of a jax.jit \"\n           \"function. Calling the de-optimized backward pass.\")\n     try:\n@@ -1135,7 +1135,7 @@ def out_axes_thunk():\n     else:\n       # If control reaches this line, we got a NaN on the output of `compiled`\n       # but not `fun.call_wrapped` on the same arguments. Let's tell the user.\n-      dispatch._raise_no_nan_in_deoptimized(e)\n+      api_util._raise_no_nan_in_deoptimized(e)\n   arg_cts = tree_unflatten(out_tree(), out_flat)\n \n   # The freevars are being fanned out (not mapped). During transpose the\n@@ -1266,11 +1266,3 @@ def __init__(self):\n \n # TODO(mattjj): remove this vestigial dict\n reducing_transposes: dict[core.Primitive, Callable] = {}\n-\n-########################### pvary ##################################\n-\n-def _pvary_transpose_rule(cts, *_, axes, axis_index_groups):\n-  from jax._src.lax import parallel as lax_parallel\n-  return lax_parallel.psum_invariant_p.bind(\n-      *cts, axes=axes, axis_index_groups=axis_index_groups)\n-deflinear2(core.pvary_p, _pvary_transpose_rule)\ndiff --git a/jax/_src/lax/parallel.py b/jax/_src/lax/parallel.py\nindex a9abf8f12939..bf27261a2c8e 100644\n--- a/jax/_src/lax/parallel.py\n+++ b/jax/_src/lax/parallel.py\n@@ -2059,3 +2059,10 @@ def _psum_invariant_transpose_rule(cts, *args, axes, axis_index_groups):\n   del args\n   return core.pvary_p.bind(*cts, axes=axes, axis_index_groups=axis_index_groups)\n ad.deflinear2(psum_invariant_p, _psum_invariant_transpose_rule)\n+\n+########################### pvary ##################################\n+\n+def _pvary_transpose_rule(cts, *_, axes, axis_index_groups):\n+  return psum_invariant_p.bind(\n+      *cts, axes=axes, axis_index_groups=axis_index_groups)\n+ad.deflinear2(core.pvary_p, _pvary_transpose_rule)\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 6340d96a55ee..0624dad88a2b 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -176,10 +176,10 @@ def _python_pjit_helper(fun: Callable, jit_info: PjitInfo, *args, **kwargs):\n               f\"Argument '{name}' of shape {aval.str_short()} of type\"\n               f' {type(arg)} is not a valid JAX type.') from e\n       raise AssertionError(\"Unreachable\") from e\n-  except dispatch.InternalFloatingPointError as e:\n+  except api_util.InternalFloatingPointError as e:\n     if getattr(fun, '_apply_primitive', False):\n       raise FloatingPointError(f\"invalid value ({e.ty}) encountered in {fun.__qualname__}\") from None\n-    dispatch.maybe_recursive_nan_check(e, fun, args, kwargs)\n+    api_util.maybe_recursive_nan_check(e, fun, args, kwargs)\n \n   if p.box_data:\n     box_treedef, out_tree = p.out_tree.children()\n@@ -2562,7 +2562,7 @@ def prune_type(ty, xs, maybe_zeros):\n         keep_unused=keep_unused,\n         inline=inline,\n         compiler_options_kvs=compiler_options_kvs)\n-  except dispatch.InternalFloatingPointError as e:\n+  except api_util.InternalFloatingPointError as e:\n     print(\"Invalid nan value encountered in the backward pass of a jax.jit \"\n           \"function. Calling the de-optimized backward pass.\")\n     try:\n@@ -2572,7 +2572,7 @@ def prune_type(ty, xs, maybe_zeros):\n     else:\n       # If control reaches this line, we got a NaN on the output of `compiled`\n       # but not `fun.call_wrapped` on the same arguments. Let's tell the user.\n-      dispatch._raise_no_nan_in_deoptimized(e)\n+      api_util._raise_no_nan_in_deoptimized(e)\n \n   if attrs_tracked:\n     final_states, nz_cts_out = split_list(nz_cts_out, [num_attr_outs])\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 72e1420b0b2b..66df2505100c 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -1054,7 +1054,7 @@ def _maybe_check_special(outs):\n           for s in getattr(leaf, 'addressable_shards', [])]\n   try:\n     dispatch.check_special('shard_map', bufs)\n-  except dispatch.InternalFloatingPointError as e:\n+  except api_util.InternalFloatingPointError as e:\n     raise FloatingPointError(f'Invalid value ({e.ty}) encountered in sharded computation.') from None\n \n class ShardMapTrace(core.Trace):\n@@ -1562,7 +1562,7 @@ def new_out_specs_thunk():\n     except (FloatingPointError, ZeroDivisionError) as e2:\n       raise e2 from None\n     else:\n-      dispatch._raise_no_nan_in_deoptimized(e)\n+      api_util._raise_no_nan_in_deoptimized(e)\n   return tree_unflatten(out_tree(), out_flat)\n ad.primitive_transposes[shard_map_p] = _shard_map_transpose\n \ndiff --git a/jax/extend/BUILD b/jax/extend/BUILD\nindex 59958c1da389..06fb8e671120 100644\n--- a/jax/extend/BUILD\n+++ b/jax/extend/BUILD\n@@ -43,6 +43,7 @@ py_library_providing_imports_info(\n     deps = [\n         \"//jax\",\n         \"//jax:abstract_arrays\",\n+        \"//jax:ad\",\n         \"//jax:ad_util\",\n         \"//jax:core\",\n     ],\n\n```",
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Expose tree prefix broadcasting as a public API in tree utils.",
        "output": "```diff\nCommit: f9a1475e1a286a8d21ec8685aea9f92b2c40d941\nDate: 2025-05-21T23:18:40Z\nURL: https://github.com/jax-ml/jax/commit/f9a1475e1a286a8d21ec8685aea9f92b2c40d941\nFiles changed: 8\nAdditions: +108, Deletions: -5\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 1e866fae6af5..b34bf36997af 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -16,6 +16,9 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n \n ## Unreleased\n \n+* New features:\n+  * Added {func}`jax.tree.broadcast` which implements a pytree prefix broadcasting helper.\n+\n ## JAX 0.6.1 (May 21, 2025)\n \n * New features:\ndiff --git a/docs/jax.tree.rst b/docs/jax.tree.rst\nindex e65c77c757c1..1a0ddaec86d0 100644\n--- a/docs/jax.tree.rst\n+++ b/docs/jax.tree.rst\n@@ -12,6 +12,7 @@ List of Functions\n    :toctree: _autosummary\n \n    all\n+   broadcast\n    flatten\n    flatten_with_path\n    leaves\ndiff --git a/docs/jax.tree_util.rst b/docs/jax.tree_util.rst\nindex 73fd1f376e9f..c89b777ca548 100644\n--- a/docs/jax.tree_util.rst\n+++ b/docs/jax.tree_util.rst\n@@ -38,6 +38,7 @@ These APIs are now accessed via :mod:`jax.tree`.\n    :toctree: _autosummary\n \n    tree_all\n+   tree_broadcast\n    tree_flatten\n    tree_leaves\n    tree_map\ndiff --git a/jax/_src/tree.py b/jax/_src/tree.py\nindex 70d75a126804..9a3e001d902b 100644\n--- a/jax/_src/tree.py\n+++ b/jax/_src/tree.py\n@@ -378,3 +378,34 @@ def map_with_path(\n     - :func:`jax.tree_util.register_pytree_with_keys`\n   \"\"\"\n   return tree_util.tree_map_with_path(f, tree, *rest, is_leaf=is_leaf)\n+\n+\n+def broadcast(prefix_tree: Any, full_tree: Any,\n+              is_leaf: Callable[[Any], bool] | None = None\n+              ) -> list[Any]:\n+  \"\"\"Broadcasts a tree prefix into the full structure of a given tree.\n+\n+    Args:\n+      prefix_tree: a pytree that is a tree prefix of full_tree.\n+      full_tree: a pytree with the structure to broadcast the prefix leaves into.\n+      is_leaf: an optionally specified function that will be called at each\n+        flattening step. It should return a boolean, with true stopping the\n+        traversal and the whole subtree being treated as a leaf, and false\n+        indicating the flattening should traverse the current object.\n+\n+    Returns:\n+      A pytree matching the structure of full_tree where the leaves of prefix_tree have been\n+      broadcasted into the leaves of each corresponding subtree.\n+\n+    Examples:\n+      >>> import jax\n+      >>> prefix = (1, 2, 3)\n+      >>> full = (0, {'a': 0, 'b': 0}, (0, 0))\n+      >>> jax.tree.broadcast(prefix, full)\n+      (1, {'a': 2, 'b': 2}, (3, 3))\n+\n+    See Also:\n+      - :func:`jax.tree.leaves`\n+      - :func:`jax.tree.structure`\n+  \"\"\"\n+  return tree_util.tree_broadcast(prefix_tree, full_tree, is_leaf=is_leaf)\ndiff --git a/jax/_src/tree_util.py b/jax/_src/tree_util.py\nindex e2e97c90f120..6edbbfd62d12 100644\n--- a/jax/_src/tree_util.py\n+++ b/jax/_src/tree_util.py\n@@ -560,17 +560,42 @@ def __new__(klass, func, *args, **kw):\n )\n \n \n-# broadcast_prefix is not exported.\n+@export\n+def tree_broadcast(prefix_tree: Any, full_tree: Any,\n+                   is_leaf: Callable[[Any], bool] | None = None\n+                  ) -> list[Any]:\n+  \"\"\"Alias of :func:`jax.tree.broadcast`.\"\"\"\n+  broadcast_leaves = broadcast_prefix(prefix_tree, full_tree, is_leaf=is_leaf)\n+  return tree_structure(full_tree).unflatten(broadcast_leaves)\n+\n+\n+# broadcast_prefix is not exported\n def broadcast_prefix(prefix_tree: Any, full_tree: Any,\n                      is_leaf: Callable[[Any], bool] | None = None\n                      ) -> list[Any]:\n-  # If prefix_tree is not a tree prefix of full_tree, this code can raise a\n-  # ValueError; use prefix_errors to find disagreements and raise more precise\n-  # error messages.\n+  \"\"\"Broadcasts tree prefix leaves into the full set of leaves for a given full tree.\n+\n+    Args:\n+      prefix_tree: a pytree that is a tree prefix of full_tree.\n+      full_tree: a pytree with the structure to broadcast the prefix leaves into.\n+      is_leaf: an optionally specified function that will be called at each\n+        flattening step. It should return a boolean, with true stopping the\n+        traversal and the whole subtree being treated as a leaf, and false\n+        indicating the flattening should traverse the current object.\n+\n+    Returns:\n+      A list of leaves matching the expected count for the full tree,\n+      with the leaf of each prefix tree being duplicated to match the count of\n+      its corresponding subtree.\n+  \"\"\"\n   result = []\n   num_leaves = lambda t: tree_structure(t).num_leaves\n   add_leaves = lambda x, subtree: result.extend([x] * num_leaves(subtree))\n-  tree_map(add_leaves, prefix_tree, full_tree, is_leaf=is_leaf)\n+  try:\n+    tree_map(add_leaves, prefix_tree, full_tree, is_leaf=is_leaf)\n+  except ValueError:\n+      e, *_ = prefix_errors(prefix_tree, full_tree)\n+      raise e('broadcast_prefix prefix_tree') from None\n   return result\n \n \ndiff --git a/jax/tree.py b/jax/tree.py\nindex 270c34fe9647..03ca503f3a41 100644\n--- a/jax/tree.py\n+++ b/jax/tree.py\n@@ -19,6 +19,7 @@\n \n from jax._src.tree import (\n     all as all,\n+    broadcast as broadcast,\n     flatten_with_path as flatten_with_path,\n     flatten as flatten,\n     leaves_with_path as leaves_with_path,\ndiff --git a/jax/tree_util.py b/jax/tree_util.py\nindex 9f42284144ec..b35890dfc887 100644\n--- a/jax/tree_util.py\n+++ b/jax/tree_util.py\n@@ -58,6 +58,7 @@\n     register_pytree_with_keys as register_pytree_with_keys,\n     register_static as register_static,\n     tree_all as tree_all,\n+    tree_broadcast as tree_broadcast,\n     tree_flatten_with_path as tree_flatten_with_path,\n     tree_flatten as tree_flatten,\n     tree_leaves_with_path as tree_leaves_with_path,\ndiff --git a/tests/tree_util_test.py b/tests/tree_util_test.py\nindex 0df811d9da28..8d4cd5854e7d 100644\n--- a/tests/tree_util_test.py\n+++ b/tests/tree_util_test.py\n@@ -627,6 +627,39 @@ def testTransposeWithCustomObject(self):\n                                       FlatCache({\"a\": [3, 4], \"b\": [5, 6]}))\n     self.assertEqual(expected, actual)\n \n+  @parameterized.parameters(*TREES)\n+  def testBroadcast(self, tree):\n+    if isinstance(tree, FlatCache):\n+      # The tree_map construction below fails for FlatCache, because\n+      # the cached metadata becomes out of sync.\n+      self.skipTest(\"Test does not work properly for FlatCache.\")\n+    def make_inner(x):\n+      return [x, x, x]\n+    nested = tree_util.tree_map(make_inner, tree)\n+    actual = tree_util.tree_broadcast(tree, nested)\n+    self.assertEqual(actual, nested)\n+\n+  def testBroadcastSimple(self):\n+    prefix = (1, 2, 3)\n+    full = (0, {'a': 0, 'b': 0}, (0, 0))\n+    actual = tree_util.tree_broadcast(prefix, full)\n+    expected = (1, {'a': 2, 'b': 2}, (3, 3))\n+    self.assertEqual(actual, expected)\n+\n+  def testBroadcastError(self):\n+    prefix = (1, 2, 3)\n+    full = (0, {'a': 0, 'b': 0})\n+    with self.assertRaisesRegex(ValueError, \"pytree structure error\"):\n+      tree_util.tree_broadcast(prefix, full)\n+    prefix = (1, 2)\n+    full = (0, {'a': 0, 'b': 0}, (0, 0))\n+    with self.assertRaisesRegex(ValueError, \"pytree structure error\"):\n+      tree_util.tree_broadcast(prefix, full)\n+    prefix = (1, {'a': 0})\n+    full = (0, {'a': 0, 'b': 0})\n+    with self.assertRaisesRegex(ValueError, \"pytree structure error\"):\n+      tree_util.tree_broadcast(prefix, full)\n+\n   @parameterized.parameters([(*t, s) for t, s in zip(TREES, TREE_STRINGS)])\n   def testStringRepresentation(self, tree, correct_string):\n     \"\"\"Checks that the string representation of a tree works.\"\"\"\n@@ -1444,6 +1477,13 @@ def test_tree_transpose(self):\n       tree_util.tree_transpose(outer_treedef, inner_treedef, obj)\n     )\n \n+  def test_tree_broadcast(self):\n+    prefix = (1, 2, 3)\n+    full = (0, {'a': 0, 'b': 0}, (0, 0))\n+    actual = jax.tree.broadcast(prefix, full)\n+    expected = (1, {'a': 2, 'b': 2}, (3, 3))\n+    self.assertEqual(actual, expected)\n+\n   def test_tree_unflatten(self):\n     leaves, treedef = jax.tree.flatten([1, 2, (3, 4)])\n     self.assertEqual(\n\n```",
        "from_id": [
            "levskaya"
        ]
    },
    {
        "text_input": "Allow eval_shape to propagate shardings if the aval has shardings in full explicit mode\n\nPiperOrigin-RevId: 761708753",
        "output": "```diff\nCommit: 61a9bd2b3d58640452c9f3c8514736cff9ac4cfe\nDate: 2025-05-21T23:15:06Z\nURL: https://github.com/jax-ml/jax/commit/61a9bd2b3d58640452c9f3c8514736cff9ac4cfe\nFiles changed: 2\nAdditions: +25, Deletions: -5\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 0503e58b2e45..6340d96a55ee 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -350,11 +350,17 @@ def jit_lower(jit_func, *args, **kwargs):\n @api_boundary\n def jit_eval_shape(jit_func, *args, **kwargs):\n   p, _ = _infer_params(jit_func._fun, jit_func._jit_info, args, kwargs)\n-  out_s = [None if isinstance(s, UnspecifiedValue) else s for s in p.params['out_shardings']]\n-  # TODO(yashkatariya): Add `Layout` to SDS.\n-  out = [api.ShapeDtypeStruct(x.shape, x.dtype, sharding=s,\n-                              weak_type=x.weak_type)\n-         for x, s in zip(p.params['jaxpr'].out_avals, out_s)]\n+  out_shardings = [None if isinstance(s, UnspecifiedValue) else s\n+                   for s in p.params['out_shardings']]\n+  out = []\n+  for a, out_s in zip(p.params['jaxpr'].out_avals, out_shardings):\n+    if out_s is None:\n+      s = a.sharding if a.sharding.mesh._are_all_axes_explicit else out_s\n+    else:\n+      s = out_s\n+    # TODO(yashkatariya): Add `Layout` to SDS.\n+    out.append(api.ShapeDtypeStruct(a.shape, a.dtype, sharding=s,\n+                                    weak_type=a.weak_type))\n   return tree_unflatten(p.out_tree, out)\n \n def jit_evict_fn(self):\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 523601691a97..aa2e0af2a57c 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7857,6 +7857,20 @@ def g(x, y):\n         core.ShardingTypeError, \"lhs is unreduced while rhs is not\"):\n       g.trace(x, y)\n \n+  @jtu.with_explicit_mesh((2, 2), ('x', 'y'))\n+  def test_eval_shape(self, mesh):\n+    np_inp = np.arange(16).reshape(8, 2)\n+    arr = jax.device_put(np_inp, P('x', 'y'))\n+\n+    @jax.jit\n+    def f(x):\n+      return x * 2\n+\n+    out = jax.eval_shape(f, arr)\n+    self.assertIsInstance(out, jax.ShapeDtypeStruct)\n+    self.assertEqual(out.sharding,\n+                     NamedSharding(mesh.abstract_mesh, P('x', 'y')))\n+\n \n @jtu.pytest_mark_if_available('multiaccelerator')\n class PJitErrorTest(jtu.JaxTestCase):\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic:TPU][Relayout] Remove minor implicit dimension for 32-bit native tiling\n\nPiperOrigin-RevId: 761692972",
        "output": "```diff\nCommit: efc70a06e29c6f4a6f18d86e261a2fe1546572a5\nDate: 2025-05-21T22:32:06Z\nURL: https://github.com/jax-ml/jax/commit/efc70a06e29c6f4a6f18d86e261a2fe1546572a5\nFiles changed: 1\nAdditions: +13, Deletions: -1\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 4d1323d68057..99134b46315f 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -6887,7 +6887,7 @@ FailureOr<std::pair<VectorLayout, xla::Array<Value>>> changeTiling(\n \n FailureOr<std::pair<VectorLayout, xla::Array<Value>>> changeImplicitDim(\n     RewriteContext &ctx, OpBuilder &builder, const Location loc, VectorType vty,\n-    const VectorLayout src, xla::Array<Value> vregs,\n+    VectorLayout src, xla::Array<Value> vregs,\n     const VectorLayout::ImplicitDim dst_implicit_dim,\n     const LayoutOffsets dst_offset_hints) {\n   const auto &target_shape = ctx.target_shape;\n@@ -7032,6 +7032,18 @@ FailureOr<std::pair<VectorLayout, xla::Array<Value>>> changeImplicitDim(\n                      src.tiling(), VectorLayout::ImplicitDim::kSecondMinor);\n     return std::make_pair(dst, std::move(dst_vregs));\n   }\n+  if (src.implicit_dim() == VectorLayout::ImplicitDim::kMinor &&\n+      dst_implicit_dim == VectorLayout::ImplicitDim::kNone &&\n+      src.bitwidth() == 32 && src.hasNativeTiling(ctx.target_shape)) {\n+    FAILUREOR_ASSIGN_OR_RETURN(\n+        std::tie(src, vregs),\n+        changeImplicitDim(ctx, builder, loc, vty, src, std::move(vregs),\n+                          VectorLayout::ImplicitDim::kSecondMinor,\n+                          dst_offset_hints));\n+    return changeImplicitDim(ctx, builder, loc, vty, src, std::move(vregs),\n+                             VectorLayout::ImplicitDim::kNone,\n+                             dst_offset_hints);\n+  }\n   return emitError(loc,\n                    \"Not implemented: Unsupported implicit dim change: from \")\n          << src << \" to \" << dst_implicit_dim;\n\n```",
        "from_id": [
            "tlongeri",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28904 from hawkinsp:postrelease\n\nPiperOrigin-RevId: 761690584",
        "output": "```diff\nCommit: ba8120d28a76ee2e5d687eb500aeab57a420fcdc\nDate: 2025-05-21T22:25:01Z\nURL: https://github.com/jax-ml/jax/commit/ba8120d28a76ee2e5d687eb500aeab57a420fcdc\nFiles changed: 4\nAdditions: +9, Deletions: -5\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 9fd4e50304d0..1e866fae6af5 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -16,6 +16,8 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n \n ## Unreleased\n \n+## JAX 0.6.1 (May 21, 2025)\n+\n * New features:\n   * Added {func}`jax.lax.axis_size` which returns the size of the mapped axis\n     given its name.\ndiff --git a/jax/experimental/jax2tf/tests/jax2tf_test.py b/jax/experimental/jax2tf/tests/jax2tf_test.py\nindex db608adc3dde..ece88841fdc5 100644\n--- a/jax/experimental/jax2tf/tests/jax2tf_test.py\n+++ b/jax/experimental/jax2tf/tests/jax2tf_test.py\n@@ -48,6 +48,7 @@\n config.parse_flags_with_absl()\n \n \n+@unittest.skip(\"Failing after jax 0.6.1 release\")\n class Jax2TfTest(tf_test_util.JaxToTfTestCase):\n \n   def setUp(self):\n@@ -1782,6 +1783,7 @@ def func():\n     jax_result = func()\n     self.assertEqual(tf_result, jax_result)\n \n+@unittest.skip(\"Failing after jax 0.6.1 release\")\n class Jax2TfVersioningTest(tf_test_util.JaxToTfTestCase):\n   # Use a separate test case with the default jax_serialization_version\n   def setUp(self):\ndiff --git a/jax/version.py b/jax/version.py\nindex 9301848b0cfb..e15af7ab50fc 100644\n--- a/jax/version.py\n+++ b/jax/version.py\n@@ -21,7 +21,7 @@\n import pathlib\n import subprocess\n \n-_version = \"0.6.1\"\n+_version = \"0.6.2\"\n # The following line is overwritten by build scripts in distributions &\n # releases. Do not modify this manually, or jax/jaxlib build will fail.\n _release_version: str | None = None\n@@ -152,7 +152,7 @@ def make_release_tree(self, base_dir, files):\n \n \n __version__ = _get_version_string()\n-_minimum_jaxlib_version = \"0.6.0\"\n+_minimum_jaxlib_version = \"0.6.1\"\n \n def _version_as_tuple(version_str):\n   return tuple(int(i) for i in version_str.split(\".\") if i.isdigit())\ndiff --git a/setup.py b/setup.py\nindex 823354adb70d..4c5c86f588c3 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -19,11 +19,11 @@\n \n project_name = 'jax'\n \n-_current_jaxlib_version = '0.6.0'\n+_current_jaxlib_version = '0.6.1'\n # The following should be updated after each new jaxlib release.\n-_latest_jaxlib_version_on_pypi = '0.6.0'\n+_latest_jaxlib_version_on_pypi = '0.6.1'\n \n-_libtpu_version = '0.0.13.*'\n+_libtpu_version = '0.0.15.*'\n \n def load_version_module(pkg_path):\n   spec = importlib.util.spec_from_file_location(\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move _src/stages.py to its own build target\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nThis refactor required moving the definitions of a few private utilities from pjit and pxla, because these files are part of the larger jax build target.\n\nPiperOrigin-RevId: 761689391",
        "output": "```diff\nCommit: a1d28dc2df6c8545f63b93466ef54442e56bd00b\nDate: 2025-05-21T22:22:03Z\nURL: https://github.com/jax-ml/jax/commit/a1d28dc2df6c8545f63b93466ef54442e56bd00b\nFiles changed: 5\nAdditions: +150, Deletions: -132\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 18e670e0269c..4218cd3f0a77 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -326,7 +326,6 @@ py_library_providing_imports_info(\n         \"_src/shard_alike.py\",\n         \"_src/shard_map.py\",\n         \"_src/sourcemap.py\",\n-        \"_src/stages.py\",\n         \"_src/tree.py\",\n     ] + glob(\n         [\n@@ -415,6 +414,7 @@ py_library_providing_imports_info(\n         \":sharding_impls\",\n         \":sharding_specs\",\n         \":source_info_util\",\n+        \":stages\",\n         \":traceback_util\",\n         \":tree_util\",\n         \":typing\",\n@@ -1001,6 +1001,25 @@ pytype_strict_library(\n     ],\n )\n \n+pytype_strict_library(\n+    name = \"stages\",\n+    srcs = [\"_src/stages.py\"],\n+    deps = [\n+        \":config\",\n+        \":core\",\n+        \":layout\",\n+        \":mlir\",\n+        \":sharding\",\n+        \":sharding_impls\",\n+        \":source_info_util\",\n+        \":traceback_util\",\n+        \":tree_util\",\n+        \":typing\",\n+        \":util\",\n+        \"//jax/_src/lib\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"compute_on\",\n     srcs = [\"_src/compute_on.py\"],\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex 8f553ea884d7..9a11ffa104a8 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -24,7 +24,7 @@\n import logging\n import threading\n import time\n-from typing import Any, Callable, NamedTuple\n+from typing import Any, Callable\n \n import jax\n from jax._src import api\n@@ -34,7 +34,6 @@\n from jax._src import core\n from jax._src import dtypes\n from jax._src import lib\n-from jax._src import source_info_util\n from jax._src import traceback_util\n from jax._src import util\n from jax._src.abstract_arrays import array_types\n@@ -52,6 +51,7 @@\n from jax._src.sharding_impls import (\n     NamedSharding, SingleDeviceSharding, TransferToMemoryKind, GSPMDSharding,\n     is_single_device_sharding)\n+from jax._src.stages import SourceInfo\n import numpy as np\n \n \n@@ -240,11 +240,6 @@ def jaxpr_has_prim_requiring_devices(jaxpr: core.Jaxpr) -> bool:\n   return False\n \n \n-class SourceInfo(NamedTuple):\n-  source_info: source_info_util.SourceInfo\n-  eqn_name: str\n-\n-\n @util.weakref_lru_cache\n def get_intermediate_shardings(\n     jaxpr: core.Jaxpr) -> Sequence[tuple[Sharding, SourceInfo]]:\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex 4a21fae59e52..d0a22cd784b4 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -15,7 +15,6 @@\n \n from __future__ import annotations\n \n-import enum\n import collections\n from collections import namedtuple\n from collections.abc import Callable, Sequence, Iterable\n@@ -1660,67 +1659,10 @@ def check_if_any_auto(\n       return True\n   return False\n \n-class MismatchType(enum.Enum):\n-  ARG_SHARDING = 0\n-  OUT_SHARDING = 1\n-  SHARDING_INSIDE_COMPUTATION = 2\n-  CONTEXT_DEVICES = 3\n-  IN_SHARDING = 4\n-\n-  def __str__(self):\n-    if self.name == 'IN_SHARDING':\n-      return 'explicit input sharding'\n-    elif self.name == 'OUT_SHARDING':\n-      return 'explicit output sharding'\n-    elif self.name == 'CONTEXT_DEVICES':\n-      return 'context mesh'\n-    return f'{self.name}'\n-\n-\n-@dataclasses.dataclass\n-class DeviceAssignmentMismatch:\n-  da: Sequence[xc.Device]\n-  m_type: MismatchType\n-  source_info: dispatch.SourceInfo | None\n-\n-  @property\n-  def device_ids(self) -> Sequence[int]:\n-    return [d.id for d in self.da]\n-\n-  @property\n-  def platform(self) -> str:\n-    return self.da[0].platform.upper()\n-\n-  def _maybe_api_name(self, api_name) -> str:\n-    return f\" {api_name}'s\" if self.m_type == MismatchType.CONTEXT_DEVICES else \"\"\n-\n-  @property\n-  def source_info_str(self):\n-    return (\n-        \"\" if self.source_info is None\n-        else f\" at {source_info_util.summarize(self.source_info.source_info)}\"\n-    )\n-\n-  @property\n-  def _dev_ids_plat_str(self):\n-    return f\"device ids {self.device_ids} on platform {self.platform}\"\n-\n-  def m_type_str(self, api_name):\n-    return (f'{self.source_info and self.source_info.eqn_name} inside {api_name}'\n-            if self.m_type == MismatchType.SHARDING_INSIDE_COMPUTATION else self.m_type)\n-\n-  def _str(self, api_name):\n-    return (f\"{self._maybe_api_name(api_name)} {self.m_type_str(api_name)} with \"\n-            f\"{self._dev_ids_plat_str}{self.source_info_str}\")\n-\n-\n-class DeviceAssignmentMismatchError(Exception):\n-  pass\n-\n \n ShardingInfo = tuple[\n     Union[JSharding, UnspecifiedValue, AUTO],\n-    MismatchType,\n+    stages.MismatchType,\n     Union[Any, None],  # Any is dispatch.SourceInfo to avoid circular imports\n ]\n \n@@ -1752,14 +1694,14 @@ def _get_and_check_device_assignment(\n                              else sh._device_assignment)\n     if not devices:\n       if first_sharding_info[0] != arr_device_assignment:\n-        raise DeviceAssignmentMismatchError([\n-            DeviceAssignmentMismatch(*first_sharding_info),\n-            DeviceAssignmentMismatch(arr_device_assignment, s_type, source_info)])\n+        raise stages.DeviceAssignmentMismatchError([\n+            stages.DeviceAssignmentMismatch(*first_sharding_info),\n+            stages.DeviceAssignmentMismatch(arr_device_assignment, s_type, source_info)])\n     else:\n       if devices != arr_device_assignment:\n-        raise DeviceAssignmentMismatchError([\n-            DeviceAssignmentMismatch(devices, MismatchType.CONTEXT_DEVICES, None),\n-            DeviceAssignmentMismatch(arr_device_assignment, s_type, source_info)])\n+        raise stages.DeviceAssignmentMismatchError([\n+            stages.DeviceAssignmentMismatch(devices, stages.MismatchType.CONTEXT_DEVICES, None),\n+            stages.DeviceAssignmentMismatch(arr_device_assignment, s_type, source_info)])\n   if first_sharding_info is None and devices:\n     final_device_assignment = devices\n   elif first_sharding_info is None:\n@@ -2283,9 +2225,9 @@ def lower_sharding_computation(\n   unique_out_shardings = util.stable_unique(out_shardings)\n   backend, device_assignment = _get_and_check_device_assignment(\n       it.chain(\n-          ((i, MismatchType.ARG_SHARDING, None) for i in unique_in_shardings),\n-          ((o, MismatchType.OUT_SHARDING, None) for o in unique_out_shardings),\n-          ((js, MismatchType.SHARDING_INSIDE_COMPUTATION, source_info)\n+          ((i, stages.MismatchType.ARG_SHARDING, None) for i in unique_in_shardings),\n+          ((o, stages.MismatchType.OUT_SHARDING, None) for o in unique_out_shardings),\n+          ((js, stages.MismatchType.SHARDING_INSIDE_COMPUTATION, source_info)\n            for js, source_info in unique_intermediate_shardings)),\n       devices_from_context)\n   unique_intermediate_shardings = [js for js, _ in unique_intermediate_shardings]\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex de01f4c05983..0503e58b2e45 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -96,48 +96,6 @@\n logger = logging.getLogger(__name__)\n \n \n-def _find_arg_mismatch(arg_list, fails, fun_name):\n-  mismatched_args_msg = []\n-  def mismatch(err):\n-    for name, inp_da, aval in arg_list:\n-      if err.m_type == pxla.MismatchType.ARG_SHARDING and err.da == inp_da:\n-        mismatched_args_msg.append(\n-            f\"argument {name} of {fun_name} with shape {aval.str_short()} and \"\n-            f\"{err._dev_ids_plat_str}\")\n-        break\n-  first_err, second_err = fails\n-  mismatch(first_err)\n-  mismatch(second_err)\n-  return mismatched_args_msg\n-\n-\n-def _device_assignment_mismatch_error(fun_name, fails, args_flat, api_name,\n-                                      arg_names):\n-  arg_list = []\n-  if arg_names is None:\n-    arg_names = [''] * len(args_flat)\n-  for a, n in zip(args_flat, arg_names):\n-    da = (a.sharding._device_assignment\n-          if getattr(a, 'sharding', None) is not None else None)\n-    arg_list.append((n, da, core.shaped_abstractify(a)))\n-\n-  mismatched_args_msg = _find_arg_mismatch(arg_list, fails, fun_name)\n-\n-  if len(mismatched_args_msg) == 2:\n-    first, second = mismatched_args_msg  # pytype: disable=bad-unpacking\n-    extra_msg = f\" Got {first} and {second}\"\n-  elif len(mismatched_args_msg) == 1:\n-    first, second  = fails\n-    # Choose the failure left which is not already covered by ARG_SHARDING.\n-    left = second if first.m_type == pxla.MismatchType.ARG_SHARDING else first\n-    extra_msg = f\" Got {mismatched_args_msg[0]} and{left._str(api_name)}\"\n-  else:\n-    first, second = fails\n-    extra_msg = f\" Got{first._str(api_name)} and{second._str(api_name)}\"\n-  msg = (f\"Received incompatible devices for {api_name}ted computation.{extra_msg}\")\n-  return msg\n-\n-\n class PjitInfo(NamedTuple):\n   \"\"\"Things that we know about a jit instance before it is called.\n \n@@ -197,10 +155,10 @@ def _python_pjit_helper(fun: Callable, jit_info: PjitInfo, *args, **kwargs):\n       out_flat = pjit_p.bind(*args_flat, **p.params)\n       compiled = None\n       profiler = None\n-  except pxla.DeviceAssignmentMismatchError as e:\n+  except stages.DeviceAssignmentMismatchError as e:\n     fails, = e.args\n     fun_name = getattr(fun, '__qualname__', getattr(fun, '__name__', str(fun)))\n-    msg = _device_assignment_mismatch_error(\n+    msg = stages._device_assignment_mismatch_error(\n         fun_name, fails, args_flat, 'jit', p.arg_names)\n     raise ValueError(msg) from None\n   except xla.InvalidInputException as e:\n@@ -1740,7 +1698,7 @@ def _resolve_in_shardings(args, pjit_in_shardings: Sequence[PjitSharding]\n     if isinstance(arg_s, PmapSharding):\n       continue\n     if getattr(a, '_committed', True):\n-      committed_arg_shardings.append((arg_s, pxla.MismatchType.ARG_SHARDING, None))\n+      committed_arg_shardings.append((arg_s, stages.MismatchType.ARG_SHARDING, None))\n \n   resolved_in_shardings: list[PjitSharding] = []\n   for arg, pjit_in_s in zip(args, pjit_in_shardings):\ndiff --git a/jax/_src/stages.py b/jax/_src/stages.py\nindex 3c5d710f3bdc..d92d1ccb2aa3 100644\n--- a/jax/_src/stages.py\n+++ b/jax/_src/stages.py\n@@ -30,18 +30,20 @@\n \"\"\"\n from __future__ import annotations\n \n+import dataclasses\n+import enum\n import functools\n from collections.abc import Sequence\n from dataclasses import dataclass\n from typing import Any, NamedTuple, Protocol, Union, runtime_checkable\n \n-import jax\n-\n from jax._src import core\n from jax._src import config\n+from jax._src import sharding as sharding_lib\n from jax._src import source_info_util\n from jax._src import traceback_util\n from jax._src import tree_util\n+from jax._src import typing\n from jax._src import util\n from jax._src.sharding_impls import UnspecifiedValue, AUTO\n from jax._src.layout import Layout\n@@ -79,7 +81,7 @@ def create_cpp_call(self, no_kwargs, in_tree, out_tree) -> Any:\n     \"\"\"Optionally constructs a fast c++ dispatcher.\"\"\"\n     return None\n \n-  def input_shardings(self) -> Sequence[jax.sharding.Sharding]:\n+  def input_shardings(self) -> Sequence[sharding_lib.Sharding]:\n     \"\"\"Flat sequence of input shardings.\n \n     May raise ``NotImplementedError`` if unavailable, e.g. based on backend,\n@@ -88,7 +90,7 @@ def input_shardings(self) -> Sequence[jax.sharding.Sharding]:\n     raise NotImplementedError(\n         \"compiled executable carries no input sharding information\")\n \n-  def output_shardings(self) -> Sequence[jax.sharding.Sharding]:\n+  def output_shardings(self) -> Sequence[sharding_lib.Sharding]:\n     \"\"\"Flat sequence of output shardings.\n \n     May raise ``NotImplementedError`` if unavailable, e.g. based on backend,\n@@ -310,8 +312,8 @@ def dtype(self):\n @dataclass(frozen=True)\n class OutInfo:\n   shape: tuple[int, ...]\n-  dtype: jax.typing.DTypeLike\n-  sharding: jax.sharding.Sharding | None = None\n+  dtype: typing.DTypeLike\n+  sharding: sharding_lib.Sharding | None = None\n \n \n class Stage:\n@@ -689,9 +691,6 @@ def out_info(self):\n   def lower(self, *, lowering_platforms: tuple[str, ...] | None = None,\n             _private_parameters: mlir.LoweringParameters | None = None):\n     \"\"\"Lower to compiler input, returning a ``Lowered`` instance.\"\"\"\n-    from jax._src.interpreters import pxla\n-    from jax._src import pjit\n-\n     if _private_parameters is None:\n       _private_parameters = mlir.LoweringParameters()\n     new_callable = functools.partial(\n@@ -699,9 +698,9 @@ def lower(self, *, lowering_platforms: tuple[str, ...] | None = None,\n         lowering_parameters=_private_parameters)\n     try:\n       lowering = new_callable()\n-    except pxla.DeviceAssignmentMismatchError as e:\n+    except DeviceAssignmentMismatchError as e:\n       fails, = e.args\n-      msg = pjit._device_assignment_mismatch_error(\n+      msg = _device_assignment_mismatch_error(\n           self.fun_name, fails, self._args_flat, 'jit', self._arg_names)\n       raise ValueError(msg) from None\n     return Lowered(lowering, self.args_info, self._out_tree)\n@@ -745,3 +744,108 @@ def lower(self, *args, **kwargs) -> Lowered:\n       A ``Lowered`` instance representing the lowering.\n     \"\"\"\n     raise NotImplementedError\n+\n+\n+class MismatchType(enum.Enum):\n+  ARG_SHARDING = 0\n+  OUT_SHARDING = 1\n+  SHARDING_INSIDE_COMPUTATION = 2\n+  CONTEXT_DEVICES = 3\n+  IN_SHARDING = 4\n+\n+  def __str__(self):\n+    if self.name == 'IN_SHARDING':\n+      return 'explicit input sharding'\n+    elif self.name == 'OUT_SHARDING':\n+      return 'explicit output sharding'\n+    elif self.name == 'CONTEXT_DEVICES':\n+      return 'context mesh'\n+    return f'{self.name}'\n+\n+\n+class SourceInfo(NamedTuple):\n+  source_info: source_info_util.SourceInfo\n+  eqn_name: str\n+\n+\n+@dataclasses.dataclass\n+class DeviceAssignmentMismatch:\n+  da: Sequence[xc.Device]\n+  m_type: MismatchType\n+  source_info: SourceInfo | None\n+\n+  @property\n+  def device_ids(self) -> Sequence[int]:\n+    return [d.id for d in self.da]\n+\n+  @property\n+  def platform(self) -> str:\n+    return self.da[0].platform.upper()\n+\n+  def _maybe_api_name(self, api_name) -> str:\n+    return f\" {api_name}'s\" if self.m_type == MismatchType.CONTEXT_DEVICES else \"\"\n+\n+  @property\n+  def source_info_str(self):\n+    return (\n+        \"\" if self.source_info is None\n+        else f\" at {source_info_util.summarize(self.source_info.source_info)}\"\n+    )\n+\n+  @property\n+  def _dev_ids_plat_str(self):\n+    return f\"device ids {self.device_ids} on platform {self.platform}\"\n+\n+  def m_type_str(self, api_name):\n+    return (f'{self.source_info and self.source_info.eqn_name} inside {api_name}'\n+            if self.m_type == MismatchType.SHARDING_INSIDE_COMPUTATION else self.m_type)\n+\n+  def _str(self, api_name):\n+    return (f\"{self._maybe_api_name(api_name)} {self.m_type_str(api_name)} with \"\n+            f\"{self._dev_ids_plat_str}{self.source_info_str}\")\n+\n+\n+class DeviceAssignmentMismatchError(Exception):\n+  pass\n+\n+\n+def _find_arg_mismatch(arg_list, fails, fun_name):\n+  mismatched_args_msg = []\n+  def mismatch(err):\n+    for name, inp_da, aval in arg_list:\n+      if err.m_type == MismatchType.ARG_SHARDING and err.da == inp_da:\n+        mismatched_args_msg.append(\n+            f\"argument {name} of {fun_name} with shape {aval.str_short()} and \"\n+            f\"{err._dev_ids_plat_str}\")\n+        break\n+  first_err, second_err = fails\n+  mismatch(first_err)\n+  mismatch(second_err)\n+  return mismatched_args_msg\n+\n+\n+def _device_assignment_mismatch_error(fun_name, fails, args_flat, api_name,\n+                                      arg_names):\n+  arg_list = []\n+  if arg_names is None:\n+    arg_names = [''] * len(args_flat)\n+  for a, n in zip(args_flat, arg_names):\n+    da = (a.sharding._device_assignment\n+          if getattr(a, 'sharding', None) is not None else None)\n+    arg_list.append((n, da, core.shaped_abstractify(a)))\n+\n+  mismatched_args_msg = _find_arg_mismatch(arg_list, fails, fun_name)\n+\n+  if len(mismatched_args_msg) == 2:\n+    first, second = mismatched_args_msg  # pytype: disable=bad-unpacking\n+    extra_msg = f\" Got {first} and {second}\"\n+  elif len(mismatched_args_msg) == 1:\n+    first, second  = fails\n+    # Choose the failure left which is not already covered by ARG_SHARDING.\n+    left = second if first.m_type == MismatchType.ARG_SHARDING else first\n+    extra_msg = f\" Got {mismatched_args_msg[0]} and{left._str(api_name)}\"\n+  else:\n+    first, second = fails\n+    extra_msg = f\" Got{first._str(api_name)} and{second._str(api_name)}\"\n+  msg = (f\"Received incompatible devices for {api_name}ted computation.{extra_msg}\")\n+  return msg\n\n```",
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update version numbers after 0.6.1 release.",
        "output": "```diff\nCommit: 7e9c7e69427628bce16a39701710e3d781b18468\nDate: 2025-05-21T21:59:50Z\nURL: https://github.com/jax-ml/jax/commit/7e9c7e69427628bce16a39701710e3d781b18468\nFiles changed: 4\nAdditions: +7, Deletions: -3\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 939177e01311..1e866fae6af5 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -14,7 +14,9 @@ Remember to align the itemized text with the first line of an item within a list\n When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.md.\n -->\n \n-## JAX 0.6.1\n+## Unreleased\n+\n+## JAX 0.6.1 (May 21, 2025)\n \n * New features:\n   * Added {func}`jax.lax.axis_size` which returns the size of the mapped axis\ndiff --git a/jax/experimental/jax2tf/tests/jax2tf_test.py b/jax/experimental/jax2tf/tests/jax2tf_test.py\nindex db608adc3dde..ece88841fdc5 100644\n--- a/jax/experimental/jax2tf/tests/jax2tf_test.py\n+++ b/jax/experimental/jax2tf/tests/jax2tf_test.py\n@@ -48,6 +48,7 @@\n config.parse_flags_with_absl()\n \n \n+@unittest.skip(\"Failing after jax 0.6.1 release\")\n class Jax2TfTest(tf_test_util.JaxToTfTestCase):\n \n   def setUp(self):\n@@ -1782,6 +1783,7 @@ def func():\n     jax_result = func()\n     self.assertEqual(tf_result, jax_result)\n \n+@unittest.skip(\"Failing after jax 0.6.1 release\")\n class Jax2TfVersioningTest(tf_test_util.JaxToTfTestCase):\n   # Use a separate test case with the default jax_serialization_version\n   def setUp(self):\ndiff --git a/jax/version.py b/jax/version.py\nindex acbfb7577e49..e15af7ab50fc 100644\n--- a/jax/version.py\n+++ b/jax/version.py\n@@ -21,7 +21,7 @@\n import pathlib\n import subprocess\n \n-_version = \"0.6.1\"\n+_version = \"0.6.2\"\n # The following line is overwritten by build scripts in distributions &\n # releases. Do not modify this manually, or jax/jaxlib build will fail.\n _release_version: str | None = None\ndiff --git a/setup.py b/setup.py\nindex 85ba1fdb4e96..4c5c86f588c3 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -21,7 +21,7 @@\n \n _current_jaxlib_version = '0.6.1'\n # The following should be updated after each new jaxlib release.\n-_latest_jaxlib_version_on_pypi = '0.6.0'\n+_latest_jaxlib_version_on_pypi = '0.6.1'\n \n _libtpu_version = '0.0.15.*'\n \n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "[Mosaic:TPU][Relayout] Support minor to 2nd minor implicit dimension for unpacked types and native tiling on TPUv5\n\nPiperOrigin-RevId: 761676578",
        "output": "```diff\nCommit: 2e070cade219ddb0033ec3ca2438bd8f74853218\nDate: 2025-05-21T21:45:27Z\nURL: https://github.com/jax-ml/jax/commit/2e070cade219ddb0033ec3ca2438bd8f74853218\nFiles changed: 1\nAdditions: +128, Deletions: -0\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 4200551ff450..4d1323d68057 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -441,6 +441,121 @@ FailureOr<Value> maskOOB(RewriteContext &ctx, ImplicitLocOpBuilder &builder,\n       .getResult();\n }\n \n+// Transpose the 2nd minor dimension of the implicit shape.\n+//\n+// Shape of (..., N, 1) becomes (..., 1, N)\n+FailureOr<xla::Array<Value>> transposeSingletonMinorDimension(\n+    RewriteContext &ctx, OpBuilder &builder, const Location loc,\n+    xla::Array<Value> vregs, const ArrayRef<int64_t> ishape,\n+    VectorLayout layout, const int64_t new_minor_offset) {\n+  if (layout.bitwidth() != 32 || !layout.hasNativeTiling(ctx.target_shape)) {\n+    // Note: For non-native tilings it is probably better to retile first, to\n+    //       to make the most out of each lane rotate (they are expensive).\n+    return emitError(loc, \"Not implemented: Unsupported bitwidth or tiling\");\n+  }\n+  auto create_index_const = [&](const int64_t idx) {\n+    return builder.create<arith::ConstantIndexOp>(loc, idx);\n+  };\n+  auto create_i32_vreg_const = [&](const int64_t val) {\n+    return I32Const(val, ctx.target_shape, builder, loc);\n+  };\n+  if (layout.offsets()[1].has_value()) {\n+    // Replicate minor dimension\n+    // TODO(tlongeri): Move into its own function (it will be needed for\n+    // relayout) and make this a precondition of this function, so that we have\n+    // \"building block\" functions with minimal overlap\n+    vregs.Each([&](const absl::Span<const int64_t> idxs, Value *vreg) {\n+      *vreg = builder.create<tpu::DynamicGatherOp>(\n+          loc, vreg->getType(), *vreg,\n+          create_i32_vreg_const(*layout.offsets()[1]), 1);\n+    });\n+    layout =\n+        VectorLayout(layout.bitwidth(), {layout.offsets()[0], std::nullopt},\n+                     layout.tiling(), VectorLayout::ImplicitDim::kNone);\n+  }\n+  if (!layout.offsets()[0].has_value()) {\n+    return vregs;\n+  }\n+  const int64_t old_2nd_minor_offset = *layout.offsets()[0];\n+  SmallVector<int64_t> new_ishape(ishape);\n+  CHECK_EQ(new_ishape.back(), 1);\n+  std::iter_swap(new_ishape.end() - 2, new_ishape.end() - 1);\n+  // new_layout is only to get the new vreg array shape, the implicit dim is\n+  // irrelevant (since we already have the implicit shape):\n+  const VectorLayout new_layout(\n+      layout.bitwidth(), {std::nullopt, new_minor_offset}, layout.tiling(),\n+      VectorLayout::ImplicitDim::kNone);\n+  xla::Array<Value> new_vregs(new_layout.tileArrayShape(\n+      /*src_is_implicit=*/true, /*res_is_implicit=*/true, new_ishape,\n+      ctx.target_shape));\n+  VectorType iota_vreg_ty =\n+      getNativeVregType(builder.getI32Type(), ctx.target_shape);\n+  // Preallocate an indices vector to avoid repeated allocations:\n+  SmallVector<int64_t> old_idxs;\n+  new_vregs.Each([&](const absl::Span<const int64_t> new_idxs,\n+                     Value *new_vreg) {\n+    const int64_t uncorrected_shape_start =\n+        ctx.target_shape[1] * new_idxs.back() - new_minor_offset;\n+    // The start and end of the data contained by new_vreg in the implicit shape\n+    const int64_t shape_start = std::max<int64_t>(uncorrected_shape_start, 0);\n+    const int64_t shape_end = std::min(\n+        uncorrected_shape_start + ctx.target_shape[1], new_ishape.back());\n+    old_idxs.assign(new_idxs.begin(), new_idxs.end());\n+    CHECK_EQ(*(old_idxs.end() - 2), 0);\n+    old_idxs.back() = 0;\n+    *new_vreg = nullptr;\n+    VectorType vmask_ty =\n+        getNativeVregOrVmaskType(builder.getI1Type(), 32, ctx.target_shape);\n+    int64_t shape_offset = shape_start;\n+    // The data in the new vreg is composed of data from multiple of the old\n+    // vregs, so iterate over them until the new vreg is full\n+    while (shape_offset < shape_end) {\n+      // Find the vreg that contains the data at shape_offset\n+      *(old_idxs.end() - 2) =\n+          (shape_offset + old_2nd_minor_offset) / ctx.target_shape[0];\n+      const int64_t old_sublane_offset =\n+          (shape_offset + old_2nd_minor_offset) % ctx.target_shape[0];\n+      const int64_t new_lane_offset =\n+          (shape_offset + new_minor_offset) % ctx.target_shape[1];\n+      // We will blend in all the relevant data contained by the old vreg\n+      const int64_t data_size =\n+          std::min(ctx.target_shape[0] - old_sublane_offset,\n+                   ctx.target_shape[1] - new_lane_offset);\n+      // [ a a a a a a a a ]    [ . . a b c . . . ]\n+      // [ b b b b b b b b ] => [ . . a b c . . . ]\n+      // [ c c c c c c c c ]    [ . . a b c . . . ]\n+      // [ . . . . . . . . ]    [ . . a b c . . . ]\n+      // Every lane has all the data, so at each sublane we can just pick out\n+      // the element that we want using a sublane shuffle.\n+      Value vreg = vregs(old_idxs);\n+      Value iota_vreg = builder.create<tpu::IotaOp>(\n+          loc, iota_vreg_ty,\n+          /*dimension =*/builder.getI32IntegerAttr(1));\n+      iota_vreg = builder.create<arith::AddIOp>(\n+          loc, iota_vreg,\n+          create_i32_vreg_const(old_sublane_offset - new_lane_offset));\n+      vreg = builder.create<tpu::DynamicGatherOp>(loc, vreg.getType(), vreg,\n+                                                  iota_vreg, 0);\n+      // Now, blend the transposed data into new_vreg\n+      if (*new_vreg == nullptr) {\n+        *new_vreg = vreg;\n+      } else {\n+        Value mask = builder.create<tpu::CreateMaskOp>(\n+            loc, vmask_ty,\n+            ArrayRef<Value>{create_index_const(0),\n+                            create_index_const(new_lane_offset)},\n+            ArrayRef<Value>{create_index_const(ctx.target_shape[0]),\n+                            create_index_const(new_lane_offset + data_size)});\n+        *new_vreg = builder.create<arith::SelectOp>(loc, mask, vreg, *new_vreg);\n+      }\n+      shape_offset += data_size;\n+      ++*(old_idxs.end() - 2);\n+    }\n+    CHECK(*new_vreg != nullptr);\n+  });\n+  return new_vregs;\n+}\n+\n // Insert a minor dimension to the implicit shape. The original minor dimension\n // becomes the new second minor dimension, laid out across sublanes.\n //\n@@ -6904,6 +7019,19 @@ FailureOr<std::pair<VectorLayout, xla::Array<Value>>> changeImplicitDim(\n                                      dst.offsets()));\n     return std::make_pair(dst, std::move(dst_vregs));\n   }\n+  if (src.implicit_dim() == VectorLayout::ImplicitDim::kMinor &&\n+      dst_implicit_dim == VectorLayout::ImplicitDim::kSecondMinor &&\n+      src.bitwidth() == 32 && src.hasNativeTiling(ctx.target_shape)) {\n+    const int64_t dst_minor_offset = dst_offset_hints[1].value_or(0);\n+    FAILUREOR_ASSIGN_OR_RETURN(\n+        xla::Array<Value> dst_vregs,\n+        transposeSingletonMinorDimension(ctx, builder, loc, vregs,\n+                                         src.implicitShape(vty.getShape()), src,\n+                                         dst_minor_offset));\n+    VectorLayout dst(src.bitwidth(), {std::nullopt, dst_minor_offset},\n+                     src.tiling(), VectorLayout::ImplicitDim::kSecondMinor);\n+    return std::make_pair(dst, std::move(dst_vregs));\n+  }\n   return emitError(loc,\n                    \"Not implemented: Unsupported implicit dim change: from \")\n          << src << \" to \" << dst_implicit_dim;\n\n```",
        "from_id": [
            "tlongeri",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge branch 'release/0.6.1'",
        "output": "```diff\nCommit: 880d31d6b105e1deb470b60690bbf5c8a86c7d06\nDate: 2025-05-21T21:22:49Z\nURL: https://github.com/jax-ml/jax/commit/880d31d6b105e1deb470b60690bbf5c8a86c7d06\nFiles changed: 3\nAdditions: +4, Deletions: -4\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 9fd4e50304d0..939177e01311 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -14,7 +14,7 @@ Remember to align the itemized text with the first line of an item within a list\n When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.md.\n -->\n \n-## Unreleased\n+## JAX 0.6.1\n \n * New features:\n   * Added {func}`jax.lax.axis_size` which returns the size of the mapped axis\ndiff --git a/jax/version.py b/jax/version.py\nindex 9301848b0cfb..acbfb7577e49 100644\n--- a/jax/version.py\n+++ b/jax/version.py\n@@ -152,7 +152,7 @@ def make_release_tree(self, base_dir, files):\n \n \n __version__ = _get_version_string()\n-_minimum_jaxlib_version = \"0.6.0\"\n+_minimum_jaxlib_version = \"0.6.1\"\n \n def _version_as_tuple(version_str):\n   return tuple(int(i) for i in version_str.split(\".\") if i.isdigit())\ndiff --git a/setup.py b/setup.py\nindex 823354adb70d..85ba1fdb4e96 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -19,11 +19,11 @@\n \n project_name = 'jax'\n \n-_current_jaxlib_version = '0.6.0'\n+_current_jaxlib_version = '0.6.1'\n # The following should be updated after each new jaxlib release.\n _latest_jaxlib_version_on_pypi = '0.6.0'\n \n-_libtpu_version = '0.0.13.*'\n+_libtpu_version = '0.0.15.*'\n \n def load_version_module(pkg_path):\n   spec = importlib.util.spec_from_file_location(\n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "[ragged-paged-attn] Use select for initialization in flash attention.\n\nPiperOrigin-RevId: 761612390",
        "output": "```diff\nCommit: 08c2a36e280b3380ed715301a0d6e396bccd9310\nDate: 2025-05-21T18:55:14Z\nURL: https://github.com/jax-ml/jax/commit/08c2a36e280b3380ed715301a0d6e396bccd9310\nFiles changed: 1\nAdditions: +8, Deletions: -26\ndiff --git a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\nindex df47674a59a9..d9d952d5a378 100644\n--- a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n+++ b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n@@ -454,6 +454,11 @@ def masked_store(ref, val, start, end, group=1):\n         mask = jnp.logical_and(iota >= start, iota < end)\n         pl.store(ref, idx=tuple(slice(None) for _ in ref.shape), val=val, mask=mask)\n \n+      def load_with_init(ref, init_val):\n+        return jnp.where(\n+            kv_blk_idx == 0, jnp.full_like(ref, init_val), ref[...]\n+        )\n+\n       # kv lens will be contracting dim, we should mask out the NaNs.\n       kv_mask = (\n           lax.broadcasted_iota(jnp.int32, k.shape, 0) < kv_len - kv_len_start\n@@ -468,29 +473,6 @@ def masked_store(ref, val, start, end, group=1):\n       store_start = jnp.maximum(q_start - q_len_start, 0)\n       store_end = jnp.minimum(q_end - q_len_start, num_q_per_blk)\n \n-      @pl.when(kv_blk_idx == 0)\n-      def init_scratch_ref():\n-        masked_store(\n-            head_m_ref,\n-            jnp.full_like(head_m_ref, -jnp.inf),\n-            store_start,\n-            store_end,\n-            num_q_heads_per_kv_head,\n-        )\n-        masked_store(\n-            head_l_ref,\n-            jnp.zeros_like(head_l_ref),\n-            store_start,\n-            store_end,\n-            num_q_heads_per_kv_head,\n-        )\n-        masked_store(\n-            head_acc_ref,\n-            jnp.zeros_like(head_acc_ref),\n-            store_start,\n-            store_end,\n-        )\n-\n       row_ids = (\n           (kv_len - q_len)\n           + q_len_start\n@@ -522,8 +504,8 @@ def init_scratch_ref():\n       l_curr = jnp.broadcast_to(\n           s_curr.sum(axis=1, keepdims=True), lm_store_shape\n       )\n-      m_prev = head_m_ref[...]\n-      l_prev = head_l_ref[...]\n+      m_prev = load_with_init(head_m_ref, -jnp.inf)\n+      l_prev = load_with_init(head_l_ref, 0.0)\n       m_next = jnp.maximum(m_prev, m_curr)\n       masked_store(\n           head_m_ref, m_next, store_start, store_end, num_q_heads_per_kv_head\n@@ -552,7 +534,7 @@ def broadcast_to_shape(arr, shape):\n             [arr for _ in range(shape[1] // arr.shape[1])], axis=1\n         )\n \n-      o_curr = head_acc_ref[...].reshape(-1, head_dim)\n+      o_curr = load_with_init(head_acc_ref, 0.0).reshape(-1, head_dim)\n       l_alpha = broadcast_to_shape(l_alpha, qkv.shape)\n       beta = broadcast_to_shape(beta, qkv.shape)\n       l_next_safe = broadcast_to_shape(l_next_safe, qkv.shape)\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Show exactly what the copy paths/patterns are when copying wheels.\n\nThis will make it easier to track down unexpected path mismatches in the future.\n\nPiperOrigin-RevId: 761584888",
        "output": "```diff\nCommit: 06c323ccc9a73f25b869555cf7b4822bbf35971e\nDate: 2025-05-21T17:48:55Z\nURL: https://github.com/jax-ml/jax/commit/06c323ccc9a73f25b869555cf7b4822bbf35971e\nFiles changed: 1\nAdditions: +5, Deletions: -2\ndiff --git a/build/tools/utils.py b/build/tools/utils.py\nindex 4bf871067501..7ed7f74d07a5 100644\n--- a/build/tools/utils.py\n+++ b/build/tools/utils.py\n@@ -293,9 +293,12 @@ def copy_dir_recursively(src, dst):\n   logging.info(\"Editable wheel path: %s\" % dst)\n \n \n-def copy_individual_files(src, dst, regex):\n+def copy_individual_files(src: str, dst: str, glob_pattern: str):\n   os.makedirs(dst, exist_ok=True)\n-  for f in glob.glob(os.path.join(src, regex)):\n+  logging.debug(\n+    f\"Copying files matching pattern {glob_pattern!r} from {src!r} to {dst!r}\"\n+  )\n+  for f in glob.glob(os.path.join(src, glob_pattern)):\n     dst_file = os.path.join(dst, os.path.basename(f))\n     if os.path.exists(dst_file):\n       os.remove(dst_file)\n\n```",
        "from_id": [
            "belitskiy",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Add faster implementation for s8->bf16 and s4->bf16 on TPUv6+.\n\nPiperOrigin-RevId: 761578503",
        "output": "```diff\nCommit: 3179e5d84a624f8ac456a2588191f92813ddf2f6\nDate: 2025-05-21T17:35:07Z\nURL: https://github.com/jax-ml/jax/commit/3179e5d84a624f8ac456a2588191f92813ddf2f6\nFiles changed: 4\nAdditions: +61, Deletions: -0\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu.td b/jaxlib/mosaic/dialect/tpu/tpu.td\nindex b6ae1e52e822..505478b9ad72 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu.td\n+++ b/jaxlib/mosaic/dialect/tpu/tpu.td\n@@ -510,6 +510,14 @@ def TPU_FPToSIOp : TPU_Op<\"fptosi\", [Pure, ElementwiseMappable]> {\n   let hasCanonicalizeMethod = 1;\n }\n \n+// Internal operation. All arith.sitofp operations that change the bitwidth\n+// must be canonicalized to this operation.\n+def TPU_SIToFPOp : TPU_Op<\"sitofp\", [Pure, ElementwiseMappable]> {\n+  let arguments = (ins AnyVectorOfAnyRank:$in, TPU_RoundingModeEnum:$rounding_mode);\n+  let results = (outs AnyVectorOfAnyRank:$output);\n+  let assemblyFormat = [{ $in attr-dict `:` type($in) `->` type($output) }];\n+}\n+\n def TPU_DotDimensionNumbersAttr : TPU_Attr<\"DotDimensionNumbers\", \"dot_dimension_numbers\"> {\n   let parameters = (ins\n     ArrayRefParameter<\"int64_t\", \"\">:$lhs_contracting_dims,\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex fa14c8ef9238..4200551ff450 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -1027,6 +1027,40 @@ LogicalResult tpu_fptosi_rule(RewriteContext &ctx, Operation &op,\n   return op.emitOpError(\"Unsupported FPToSI conversion\");\n }\n \n+LogicalResult tpu_sitofp_rule(RewriteContext &ctx, Operation &op,\n+                              const ArrayRef<Layout> layouts_in,\n+                              const ArrayRef<Layout> layouts_out) {\n+  TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  auto &layout_in = *layouts_in.front();\n+  auto &layout_out = *layouts_out.front();\n+  if (layout_in.bitwidth() == layout_out.bitwidth()) {\n+    return elementwise_op_rule(ctx, op, layouts_in, layouts_out);\n+  } else if (layout_in.bitwidth() < layout_out.bitwidth()) {\n+    auto sitofp_op = cast<tpu::SIToFPOp>(op);\n+    switch (sitofp_op.getRoundingMode()) {\n+      case tpu::RoundingMode::kToNearestEven: {\n+        ImplicitLocOpBuilder builder(op.getLoc(), &op);\n+        FAILUREOR_ASSIGN_OR_RETURN(\n+            xla::Array<Value> vregs,\n+            ext_op_rule_impl(ctx, builder, sitofp_op, layout_in, layout_out));\n+        sitofp_op.replaceAllUsesWith(assemble(builder, sitofp_op.getType(),\n+                                              layout_out, std::move(vregs),\n+                                              ctx.target_shape)\n+                                         .getResult());\n+        sitofp_op.erase();\n+        return success();\n+      }\n+      case tpu::RoundingMode::kTowardsZero:\n+        return op.emitOpError(\n+            \"Not implemented: SIToFP with rounding mode kTowardsZero\");\n+    }\n+  }\n+  return op.emitOpError(\"Unsupported SIToFP conversion\");\n+}\n+\n LogicalResult func_return_rule(RewriteContext &ctx, Operation &op,\n                                const ArrayRef<Layout> layouts_in,\n                                const ArrayRef<Layout> layouts_out) {\n@@ -7164,6 +7198,7 @@ const llvm::StringMap<rule_type> &rules() {\n         {tpu::PRNGRandomBitsOp::getOperationName(), tpu_prng_random_bits_rule},\n         {tpu::RelayoutOp::getOperationName(), tpu_relayout_rule},\n         {tpu::FPToSIOp::getOperationName(), tpu_fptosi_rule},\n+        {tpu::SIToFPOp::getOperationName(), tpu_sitofp_rule},\n         {vector::BroadcastOp::getOperationName(), vector_broadcast_rule},\n         {vector::ExtractOp::getOperationName(), vector_extract_rule},\n         {vector::LoadOp::getOperationName(), vector_load_rule},\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\nindex 368bfc596732..1d8ea1299f04 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n@@ -685,6 +685,18 @@ LogicalResult canonicalize_sitofp(const CanonicalizeContext &ctx,\n   FAILUREOR_ASSIGN_OR_RETURN(const unsigned dst_bitwidth,\n                              getElementTypeBitwidth(op.getType()));\n \n+  // We have low-level optimized code for s8->bf16 and s4->bf16 casts on v6.\n+  if (ctx.hardware_generation >= 6 && is_vector &&\n+      (src_vty.getElementType().isSignlessInteger(8) ||\n+       src_vty.getElementType().isSignlessInteger(4)) &&\n+      dst_vty.getElementType().isBF16()) {\n+    auto new_op = builder.create<tpu::SIToFPOp>(\n+        op.getType(), op.getIn(), tpu::RoundingMode::kToNearestEven);\n+    op.replaceAllUsesWith(new_op.getResult());\n+    op.erase();\n+    return success();\n+  }\n+\n   if ((src_bitwidth < 32 || dst_bitwidth < 32) && !ctx.compatibility_mode) {\n     return op.emitOpError(\n         \"On this target integer-to-float conversions can only happen on \"\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex 9c4a7b4c397d..f01d0b4c5888 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -154,6 +154,12 @@ class VectorLayoutInferer {\n         if (inferExt(&any_op).failed()) {\n           return failure();\n         }\n+      } else if (auto op = dyn_cast<tpu::SIToFPOp>(any_op);\n+                 op && op.getIn().getType().getElementTypeBitWidth() <\n+                           op.getType().getElementTypeBitWidth()) {\n+        if (inferExt(&any_op).failed()) {\n+          return failure();\n+        }\n       } else if (isa<arith::TruncFOp, arith::TruncIOp>(any_op)) {\n         if (inferTrunc(&any_op).failed()) {\n           return failure();\n\n```",
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Adjust triton dialect lowering rounding mode to allow upcasting fp8 types\n\nFix: https://github.com/jax-ml/jax/issues/28416\nPiperOrigin-RevId: 761577943",
        "output": "```diff\nCommit: f227b13613fe8d1a9c263006b40d31484b24f4c7\nDate: 2025-05-21T17:33:13Z\nURL: https://github.com/jax-ml/jax/commit/f227b13613fe8d1a9c263006b40d31484b24f4c7\nFiles changed: 3\nAdditions: +98, Deletions: -4\ndiff --git a/jax/_src/pallas/triton/lowering.py b/jax/_src/pallas/triton/lowering.py\nindex 2cddb623b33f..bd70dc8d470c 100644\n--- a/jax/_src/pallas/triton/lowering.py\n+++ b/jax/_src/pallas/triton/lowering.py\n@@ -1572,11 +1572,10 @@ def _float_float_cast(src: ir.Value, dst_type: ir.Type) -> ir.Value:\n   src_element_type = ir.FloatType(_element_type(src.type))\n   dst_element_type = ir.FloatType(_element_type(dst_type))\n   if src_element_type.width == 8 or dst_element_type.width == 8:\n-    return tt_dialect.fp_to_fp(\n-        dst_type,\n-        src,\n-        rounding=tt_dialect.RoundingMode.RTNE,\n+    rounding = (\n+        tt_dialect.RoundingMode.RTNE if src_element_type.width > 8 else None\n     )\n+    return tt_dialect.fp_to_fp(dst_type, src, rounding=rounding)\n   if src_element_type.width > dst_element_type.width:\n     return arith_dialect.truncf(dst_type, src)\n   elif src_element_type.width < dst_element_type.width:\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 6690fb2dac62..d7df261a1ca9 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -748,6 +748,24 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"triton_pallas_test\",\n+    srcs = [\n+        \"triton_pallas_test.py\",\n+    ],\n+    enable_backends = [\"cpu\"],\n+    enable_configs = [\n+        \"gpu_h100_x32\",\n+    ],\n+    shard_count = 1,\n+    deps = [\n+        \"//jax:pallas\",\n+        \"//jax:pallas_gpu\",\n+    ] + py_deps([\n+        \"absl/testing\",\n+    ]),\n+)\n+\n jax_multiplatform_test(\n     name = \"mgpu_attention_run\",\n     srcs = [\"//jax/experimental/pallas/ops/gpu:attention_mgpu.py\"],\ndiff --git a/tests/pallas/triton_pallas_test.py b/tests/pallas/triton_pallas_test.py\nnew file mode 100644\nindex 000000000000..4e2b10e72eb1\n--- /dev/null\n+++ b/tests/pallas/triton_pallas_test.py\n@@ -0,0 +1,77 @@\n+# Copyright 2025 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+\"\"\"Test the Triton dialect lowering for a variety of atomic operations.\"\"\"\n+\n+from absl.testing import absltest\n+from absl.testing import parameterized\n+import jax\n+from jax._src import config\n+from jax._src import dtypes\n+from jax._src import test_util as jtu\n+from jax._src.pallas.pallas_call import _trace_kernel_to_jaxpr\n+from jax.experimental import pallas as pl\n+import jax.numpy as jnp\n+\n+config.parse_flags_with_absl()\n+\n+\n+@jtu.with_config(jax_traceback_filtering=\"off\")\n+class PallasBaseTest(jtu.JaxTestCase):\n+  INTERPRET = False\n+\n+  def setUp(self):\n+    if jtu.test_device_matches([\"cpu\"]) and not self.INTERPRET:\n+      self.skipTest(\"On CPU the test works only in interpret mode\")\n+    if (jtu.test_device_matches([\"cuda\"]) and\n+        not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability >= sm90\")\n+\n+    super().setUp()\n+    _trace_kernel_to_jaxpr.cache_clear()\n+\n+  def pallas_call(self, *args, **kwargs):\n+    return pl.pallas_call(*args, **kwargs, interpret=self.INTERPRET)\n+\n+\n+DTYPE_LIST = [jnp.float32, jnp.float16, jnp.bfloat16,\n+              jnp.float8_e4m3fn, jnp.float8_e5m2]\n+\n+\n+class TritonPallasTest(PallasBaseTest):\n+  INTERPRET = False\n+\n+  @parameterized.product(src_dtype=DTYPE_LIST, dst_dtype=DTYPE_LIST)\n+  def test_fp_dtype_cast(self, src_dtype, dst_dtype):\n+    if src_dtype == dst_dtype:\n+      self.skipTest(\"No need to test the same dtype\")\n+    if dtypes.bit_width(src_dtype) == 8 and dtypes.bit_width(dst_dtype) == 8:\n+      self.skipTest(\"Not casting between 8-bit types\")\n+\n+    def body(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].astype(dst_dtype)\n+\n+    x = 10 * jax.random.normal(jax.random.key(0), (64, 64), dtype=src_dtype)\n+    y = self.pallas_call(body,\n+        in_specs=[pl.BlockSpec((64, 64), lambda i: (0, 0))],\n+        out_specs=pl.BlockSpec((64, 64), lambda i: (0, 0)),\n+        out_shape=jax.ShapeDtypeStruct((64, 64), dst_dtype),\n+        grid=(1,),\n+    )(x)\n+    self.assertEqual(y.dtype, dst_dtype)\n+    self.assertArraysEqual(y, x.astype(dst_dtype))\n+\n+if __name__ == \"__main__\":\n+  absltest.main()\n\n```",
        "from_id": [
            "rdyro",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Introduce the flag `add_pypi_cuda_wheel_deps` that controls if the tests depend on NVIDIA CUDA wheels hermetically.\n\nThe flag is enabled by default.\n\nTo disable the dependency, pass `add_pypi_cuda_wheel_deps=False` in the Bazel options.\n\nPiperOrigin-RevId: 761568590",
        "output": "```diff\nCommit: 8e4f3b5dab9f88a8218b7ec79369210bb95305c6\nDate: 2025-05-21T17:09:37Z\nURL: https://github.com/jax-ml/jax/commit/8e4f3b5dab9f88a8218b7ec79369210bb95305c6\nFiles changed: 2\nAdditions: +27, Deletions: -5\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex eceb38e35aab..678d92bc434a 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -637,3 +637,10 @@ def wheel_sources(\n         \":{}_data\".format(name),\n         \":{}_hdrs\".format(name),\n     ] + static_srcs)\n+\n+def if_pypi_cuda_wheel_deps(if_true, if_false = []):\n+    \"\"\" select() on whether we're adding pypi CUDA wheel deps. \"\"\"\n+    return select({\n+        \"//jaxlib/tools:pypi_cuda_wheel_deps\": if_true,\n+        \"//conditions:default\": if_false,\n+    })\ndiff --git a/jaxlib/tools/BUILD.bazel b/jaxlib/tools/BUILD.bazel\nindex d6a5f94dfd4b..433747e2bb8d 100644\n--- a/jaxlib/tools/BUILD.bazel\n+++ b/jaxlib/tools/BUILD.bazel\n@@ -15,7 +15,7 @@\n # JAX is Autograd and XLA\n \n load(\"@bazel_skylib//lib:selects.bzl\", \"selects\")\n-load(\"@bazel_skylib//rules:common_settings.bzl\", \"string_flag\")\n+load(\"@bazel_skylib//rules:common_settings.bzl\", \"bool_flag\", \"string_flag\")\n load(\"@local_config_cuda//cuda:build_defs.bzl\", \"if_cuda\")\n load(\"@local_config_rocm//rocm:build_defs.bzl\", \"if_rocm\")\n load(\n@@ -29,6 +29,7 @@ load(\n load(\n     \"//jaxlib:jax.bzl\",\n     \"PLATFORM_TAGS_DICT\",\n+    \"if_pypi_cuda_wheel_deps\",\n     \"jax_py_test\",\n     \"jax_wheel\",\n     \"pytype_strict_library\",\n@@ -470,6 +471,20 @@ filegroup(\n     ],\n )\n \n+# The flag configures whether to add the pypi NVIDIA CUDA deps to py_import.\n+bool_flag(\n+    name = \"add_pypi_cuda_wheel_deps\",\n+    build_setting_default = True,\n+)\n+\n+config_setting(\n+    name = \"pypi_cuda_wheel_deps\",\n+    flag_values = {\n+        \":add_pypi_cuda_wheel_deps\": \"True\",\n+        \"@local_config_cuda//:enable_cuda\": \"True\",\n+    },\n+)\n+\n py_import(\n     name = \"jaxlib_py_import\",\n     wheel = \":jaxlib_wheel\",\n@@ -478,26 +493,26 @@ py_import(\n py_import(\n     name = \"jax_cuda_plugin_py_import\",\n     wheel = \":jax_cuda_plugin_wheel\",\n-    wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n+    wheel_deps = if_pypi_cuda_wheel_deps([\":nvidia_wheel_deps\"]),\n )\n \n py_import(\n     name = \"jax_cuda_pjrt_py_import\",\n     wheel = \":jax_cuda_pjrt_wheel\",\n-    wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n+    wheel_deps = if_pypi_cuda_wheel_deps([\":nvidia_wheel_deps\"]),\n )\n \n # The targets below are used for GPU tests with `--//jax:build_jaxlib=false`.\n py_import(\n     name = \"pypi_jax_cuda_plugin_with_cuda_deps\",\n     wheel = \"@pypi_jax_cuda12_plugin//:whl\",\n-    wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n+    wheel_deps = if_pypi_cuda_wheel_deps([\":nvidia_wheel_deps\"]),\n )\n \n py_import(\n     name = \"pypi_jax_cuda_pjrt_with_cuda_deps\",\n     wheel = \"@pypi_jax_cuda12_pjrt//:whl\",\n-    wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n+    wheel_deps = if_pypi_cuda_wheel_deps([\":nvidia_wheel_deps\"]),\n )\n \n # Wheel tests.\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28886 from dfm:np-doc-links\n\nPiperOrigin-RevId: 761552874",
        "output": "```diff\nCommit: fb4279a584b4c50f83625cd95aea2c8b85dc5176\nDate: 2025-05-21T16:28:27Z\nURL: https://github.com/jax-ml/jax/commit/fb4279a584b4c50f83625cd95aea2c8b85dc5176\nFiles changed: 1\nAdditions: +6, Deletions: -1\ndiff --git a/docs/conf.py b/docs/conf.py\nindex a44177407344..dd7533aecf83 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -29,6 +29,7 @@\n import inspect\n import operator\n import os\n+from pathlib import Path\n import sys\n \n sys.path.insert(0, os.path.abspath('..'))\n@@ -354,7 +355,11 @@ def linkcode_resolve(domain, info):\n     source, linenum = inspect.getsourcelines(obj)\n   except:\n     return None\n-  filename = os.path.relpath(filename, start=os.path.dirname(jax.__file__))\n+  try:\n+    filename = Path(filename).relative_to(Path(jax.__file__).parent)\n+  except ValueError:\n+    # Source file is not a relative to jax; this must be a re-exported function.\n+    return None\n   lines = f\"#L{linenum}-L{linenum + len(source)}\" if linenum else \"\"\n   return f\"https://github.com/jax-ml/jax/blob/main/jax/{filename}{lines}\"\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Skip generating source links for re-exported numpy functions in docs.",
        "output": "```diff\nCommit: 6d2dc34bb05e4f4fca3488b2f7ad587762ead497\nDate: 2025-05-21T16:17:03Z\nURL: https://github.com/jax-ml/jax/commit/6d2dc34bb05e4f4fca3488b2f7ad587762ead497\nFiles changed: 1\nAdditions: +6, Deletions: -1\ndiff --git a/docs/conf.py b/docs/conf.py\nindex a44177407344..dd7533aecf83 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -29,6 +29,7 @@\n import inspect\n import operator\n import os\n+from pathlib import Path\n import sys\n \n sys.path.insert(0, os.path.abspath('..'))\n@@ -354,7 +355,11 @@ def linkcode_resolve(domain, info):\n     source, linenum = inspect.getsourcelines(obj)\n   except:\n     return None\n-  filename = os.path.relpath(filename, start=os.path.dirname(jax.__file__))\n+  try:\n+    filename = Path(filename).relative_to(Path(jax.__file__).parent)\n+  except ValueError:\n+    # Source file is not a relative to jax; this must be a re-exported function.\n+    return None\n   lines = f\"#L{linenum}-L{linenum + len(source)}\" if linenum else \"\"\n   return f\"https://github.com/jax-ml/jax/blob/main/jax/{filename}{lines}\"\n \n\n```",
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Merge pull request #28887 from dfm:uv-on-rtds\n\nPiperOrigin-RevId: 761545482",
        "output": "```diff\nCommit: 609fb7f6085b52861f65c7aa3b339c40dfd207fa\nDate: 2025-05-21T16:05:16Z\nURL: https://github.com/jax-ml/jax/commit/609fb7f6085b52861f65c7aa3b339c40dfd207fa\nFiles changed: 2\nAdditions: +12, Deletions: -9\ndiff --git a/.readthedocs.yml b/.readthedocs.yml\nindex 3b7ba275a0d6..0ac20301cee2 100644\n--- a/.readthedocs.yml\n+++ b/.readthedocs.yml\n@@ -6,15 +6,23 @@\n version: 2\n \n build:\n-  os: \"ubuntu-22.04\"\n+  os: \"ubuntu-24.04\"\n   tools:\n-    python: \"3.10\"\n+    python: \"3.12\"\n   jobs:\n     post_checkout:\n       # Skip building PRs unless tagged with the \"documentation\" label.\n       - |\n         [ \"${READTHEDOCS_VERSION_TYPE}\" != \"external\" ] && echo \"Building latest\" && exit 0\n         (curl -sL https://api.github.com/repos/jax-ml/jax/issues/${READTHEDOCS_VERSION}/labels | grep -q \"https://api.github.com/repos/jax-ml/jax/labels/documentation\") && echo \"Building PR with label\" || exit 183\n+    create_environment:\n+      - asdf plugin add uv\n+      - asdf install uv latest\n+      - asdf global uv latest\n+      - uv venv $READTHEDOCS_VIRTUALENV_PATH\n+      - UV_PROJECT_ENVIRONMENT=$READTHEDOCS_VIRTUALENV_PATH uv pip install -r docs/requirements.txt\n+    install:\n+      - \"true\"  # skip\n \n # Build documentation in the docs/ directory with Sphinx\n sphinx:\n@@ -24,8 +32,3 @@ sphinx:\n # Optionally build your docs in additional formats such as PDF and ePub\n formats:\n   - htmlzip\n-\n-# Optionally set the version of Python and requirements required to build your docs\n-python:\n-  install:\n-    - requirements: docs/requirements.txt\ndiff --git a/docs/conf.py b/docs/conf.py\nindex a7a52c9db38c..a44177407344 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -38,11 +38,11 @@\n from typing import ForwardRef\n \n def _do_not_evaluate_in_jax(\n-    self, globalns, *args, _evaluate=ForwardRef._evaluate,\n+    self, globalns, *args, _evaluate=ForwardRef._evaluate, **kwargs,\n ):\n   if globalns.get('__name__', '').startswith('jax'):\n     return self\n-  return _evaluate(self, globalns, *args)\n+  return _evaluate(self, globalns, *args, **kwargs)\n \n ForwardRef._evaluate = _do_not_evaluate_in_jax\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Try using uv for installing packages on Read the Docs.",
        "output": "```diff\nCommit: 99a0e678c8b6747efa3ed10e1ed4d28fecde525a\nDate: 2025-05-21T15:15:15Z\nURL: https://github.com/jax-ml/jax/commit/99a0e678c8b6747efa3ed10e1ed4d28fecde525a\nFiles changed: 2\nAdditions: +12, Deletions: -9\ndiff --git a/.readthedocs.yml b/.readthedocs.yml\nindex 3b7ba275a0d6..0ac20301cee2 100644\n--- a/.readthedocs.yml\n+++ b/.readthedocs.yml\n@@ -6,15 +6,23 @@\n version: 2\n \n build:\n-  os: \"ubuntu-22.04\"\n+  os: \"ubuntu-24.04\"\n   tools:\n-    python: \"3.10\"\n+    python: \"3.12\"\n   jobs:\n     post_checkout:\n       # Skip building PRs unless tagged with the \"documentation\" label.\n       - |\n         [ \"${READTHEDOCS_VERSION_TYPE}\" != \"external\" ] && echo \"Building latest\" && exit 0\n         (curl -sL https://api.github.com/repos/jax-ml/jax/issues/${READTHEDOCS_VERSION}/labels | grep -q \"https://api.github.com/repos/jax-ml/jax/labels/documentation\") && echo \"Building PR with label\" || exit 183\n+    create_environment:\n+      - asdf plugin add uv\n+      - asdf install uv latest\n+      - asdf global uv latest\n+      - uv venv $READTHEDOCS_VIRTUALENV_PATH\n+      - UV_PROJECT_ENVIRONMENT=$READTHEDOCS_VIRTUALENV_PATH uv pip install -r docs/requirements.txt\n+    install:\n+      - \"true\"  # skip\n \n # Build documentation in the docs/ directory with Sphinx\n sphinx:\n@@ -24,8 +32,3 @@ sphinx:\n # Optionally build your docs in additional formats such as PDF and ePub\n formats:\n   - htmlzip\n-\n-# Optionally set the version of Python and requirements required to build your docs\n-python:\n-  install:\n-    - requirements: docs/requirements.txt\ndiff --git a/docs/conf.py b/docs/conf.py\nindex a7a52c9db38c..a44177407344 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -38,11 +38,11 @@\n from typing import ForwardRef\n \n def _do_not_evaluate_in_jax(\n-    self, globalns, *args, _evaluate=ForwardRef._evaluate,\n+    self, globalns, *args, _evaluate=ForwardRef._evaluate, **kwargs,\n ):\n   if globalns.get('__name__', '').startswith('jax'):\n     return self\n-  return _evaluate(self, globalns, *args)\n+  return _evaluate(self, globalns, *args, **kwargs)\n \n ForwardRef._evaluate = _do_not_evaluate_in_jax\n \n\n```",
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Prepare for jax release v0.6.1",
        "output": "```diff\nCommit: 382506f1705db9c9ac348b9783497e310feef6a5\nDate: 2025-05-21T15:01:15Z\nURL: https://github.com/jax-ml/jax/commit/382506f1705db9c9ac348b9783497e310feef6a5\nFiles changed: 3\nAdditions: +4, Deletions: -4\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 9fd4e50304d0..939177e01311 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -14,7 +14,7 @@ Remember to align the itemized text with the first line of an item within a list\n When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.md.\n -->\n \n-## Unreleased\n+## JAX 0.6.1\n \n * New features:\n   * Added {func}`jax.lax.axis_size` which returns the size of the mapped axis\ndiff --git a/jax/version.py b/jax/version.py\nindex 9301848b0cfb..acbfb7577e49 100644\n--- a/jax/version.py\n+++ b/jax/version.py\n@@ -152,7 +152,7 @@ def make_release_tree(self, base_dir, files):\n \n \n __version__ = _get_version_string()\n-_minimum_jaxlib_version = \"0.6.0\"\n+_minimum_jaxlib_version = \"0.6.1\"\n \n def _version_as_tuple(version_str):\n   return tuple(int(i) for i in version_str.split(\".\") if i.isdigit())\ndiff --git a/setup.py b/setup.py\nindex 823354adb70d..85ba1fdb4e96 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -19,11 +19,11 @@\n \n project_name = 'jax'\n \n-_current_jaxlib_version = '0.6.0'\n+_current_jaxlib_version = '0.6.1'\n # The following should be updated after each new jaxlib release.\n _latest_jaxlib_version_on_pypi = '0.6.0'\n \n-_libtpu_version = '0.0.13.*'\n+_libtpu_version = '0.0.15.*'\n \n def load_version_module(pkg_path):\n   spec = importlib.util.spec_from_file_location(\n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "[Pallas/Mosaic GPU] Loosen tiling requirements for `get` and `swap`.\n\nWe now allow arbitrary 2D tilings where the minor dimension fits the associated\nswizzle.\n\nPiperOrigin-RevId: 761490845",
        "output": "```diff\nCommit: 5f764b55d82595141837a7a141a650625b4f7679\nDate: 2025-05-21T12:58:01Z\nURL: https://github.com/jax-ml/jax/commit/5f764b55d82595141837a7a141a650625b4f7679\nFiles changed: 2\nAdditions: +47, Deletions: -7\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex eb5fc136082e..b6960c479558 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -1298,11 +1298,14 @@ def _get_lowering_rule(ctx: LoweringRuleContext, x_ref, *leaves, tree):\n \n   match transforms:\n     case (gpu_core.UnswizzleRef(swizzle), gpu_core.UntileRef(tiling)):\n-      if tiling != (\n-          8,\n-          (swizzle * 8) // pallas_utils.dtype_bitwidth(dtype),\n-      ):\n-        raise NotImplementedError(\"Tiling does not fit swizzle\")\n+      if len(tiling) != 2:\n+        raise NotImplementedError(f\"Only 2D tiling is supported, got: {tiling}\")\n+      expected_minor_tiling = swizzle * 8 // pallas_utils.dtype_bitwidth(dtype)\n+      if tiling[-1] != expected_minor_tiling:\n+        raise NotImplementedError(\n+            \"Minor tiling dimension does not fit swizzle: \"\n+            f\" expected {expected_minor_tiling}, got {tiling[-1]}\"\n+        )\n       return mgpu.FragmentedArray.load_tiled(\n           x_smem, is_signed=mgpu_utils.is_signed(dtype), swizzle=swizzle\n       )\n@@ -1383,8 +1386,15 @@ def _swap_lowering_rule(\n         gpu_core.UntileRef(tiling),\n         *maybe_transpose,\n     ):\n-      if tiling != (8, swizzle // v_aval.dtype.itemsize):\n-        raise NotImplementedError(\"Tiling does not fit swizzle\")\n+      if len(tiling) != 2:\n+        raise NotImplementedError(f\"Only 2D tiling is supported, got: {tiling}\")\n+      bw = pallas_utils.dtype_bitwidth(v_aval.dtype)\n+      expected_minor_tiling = swizzle * 8 // bw\n+      if tiling[-1] != expected_minor_tiling:\n+        raise NotImplementedError(\n+            \"Minor tiling dimension does not fit swizzle: \"\n+            f\" expected {expected_minor_tiling}, got {tiling[-1]}\"\n+        )\n \n       if transposed_value != bool(maybe_transpose):\n         raise ValueError(\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex b10bc0f390b0..aedd79e23194 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -998,6 +998,36 @@ def kernel(x_ref, o_ref):\n \n     self.assertIn(\"x: [1, 0, 43, 23]: 6871\\n\", output())\n \n+  @parameterized.parameters(\n+          (plgpu.TilingTransform((1, 32)), plgpu.SwizzleTransform(128)),\n+          (plgpu.TilingTransform((8, 32)), plgpu.SwizzleTransform(128)),\n+          (),\n+  )\n+  def test_get_swap_with_transforms(self, *transforms):\n+    self.skip_if_wg_semantics()\n+\n+    shape = (128, 128)\n+\n+    @functools.partial(\n+        self.pallas_call,\n+        in_specs=[plgpu.BlockSpec(memory_space=plgpu.GMEM)],\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.GMEM),\n+        out_shape=jax.ShapeDtypeStruct(shape, jnp.int32),\n+        scratch_shapes=[\n+            plgpu.SMEM(shape, jnp.int32, transforms=tuple(transforms)),\n+            plgpu.Barrier(num_arrivals=1),\n+        ]\n+    )\n+    def kernel(x_ref, o_ref, scratch_ref, barrier_ref):\n+      plgpu.copy_gmem_to_smem(x_ref, scratch_ref, barrier_ref)\n+      plgpu.barrier_wait(barrier_ref)\n+      scratch_ref[...] = scratch_ref[...] * 2\n+      plgpu.copy_smem_to_gmem(scratch_ref, o_ref)\n+      plgpu.wait_smem_to_gmem(0)\n+\n+    x = jnp.arange(math.prod(shape), dtype=jnp.int32).reshape(shape)\n+    np.testing.assert_array_equal(kernel(x), x * 2)\n+\n   def test_check(self):\n     self.skip_if_wg_semantics()\n \n\n```",
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Support `s8xs8->s32` WGMMA.\n\nPiperOrigin-RevId: 761489756",
        "output": "```diff\nCommit: 8653a78b80eb7bd6dce2a9c78bce6af949dd53d2\nDate: 2025-05-21T12:52:57Z\nURL: https://github.com/jax-ml/jax/commit/8653a78b80eb7bd6dce2a9c78bce6af949dd53d2\nFiles changed: 3\nAdditions: +136, Deletions: -17\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex 62b43b4de737..f69d3f33fe7c 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -2551,7 +2551,7 @@ def _repack(regs_it, reg_ty):\n   for array in arrays:\n     reg_ty = array.registers.flat[0].type\n     dtype = array.mlir_dtype\n-    if ir.F32Type.isinstance(dtype):\n+    if ir.F32Type.isinstance(dtype) or dtype == i32:\n       if ir.VectorType.isinstance(reg_ty):\n         [vec_len] = ir.VectorType(reg_ty).shape\n         array_regs = [  # pylint: disable=g-complex-comprehension\n@@ -2561,7 +2561,7 @@ def _repack(regs_it, reg_ty):\n         ]\n       else:\n         array_regs = list(array.registers.flat)\n-      reg_constraint = \"f\"\n+      reg_constraint = \"r\" if dtype == i32 else \"f\"\n     elif ir.BF16Type.isinstance(dtype) or ir.F16Type.isinstance(dtype):\n       if not ir.VectorType.isinstance(reg_ty):\n         raise NotImplementedError(array.mlir_dtype)\ndiff --git a/jax/experimental/mosaic/gpu/wgmma.py b/jax/experimental/mosaic/gpu/wgmma.py\nindex 3637778c371b..abbd517fb37d 100644\n--- a/jax/experimental/mosaic/gpu/wgmma.py\n+++ b/jax/experimental/mosaic/gpu/wgmma.py\n@@ -63,7 +63,10 @@ def zero(cls, m, n, dtype=None, *, is_signed: bool | None = None):\n     f32 = ir.F32Type.get()\n     if dtype is None:\n       dtype = f32\n-    zero = arith.constant(dtype, ir.FloatAttr.get(dtype, 0.0))\n+    if ir.IntegerType.isinstance(dtype):\n+      zero = arith.constant(dtype, ir.IntegerAttr.get(dtype, 0))\n+    else:\n+      zero = arith.constant(dtype, ir.FloatAttr.get(dtype, 0.0))\n     return cls(\n         _value=fa.FragmentedArray.splat(\n             zero, (m, n), fa.WGMMA_LAYOUT, is_signed=is_signed\n@@ -90,6 +93,8 @@ def _supported_wgmma_types(dtype, abtype) -> bool:\n     return any(input_types_are(ty) for ty in (ir.FloatTF32Type, ir.BF16Type, *f16_acc_types))\n   elif ir.F16Type.isinstance(dtype):\n     return any(input_types_are(ty) for ty in f16_acc_types)\n+  elif ir.IntegerType.get_signless(32).isinstance(dtype):\n+    return input_types_are(ir.IntegerType.get_signless(8))\n   else:\n     return False\n \n@@ -135,7 +140,7 @@ def wgmma_m64(\n     if a_transpose is None:\n       raise ValueError\n \n-  if ir.F32Type.isinstance(out_ty):\n+  if ir.F32Type.isinstance(out_ty) or out_ty == i32:\n     num_acc_regs = n // 2\n     out_ty_field = out_ty\n     acc_regs = [  # pylint: disable=g-complex-comprehension\n@@ -143,8 +148,9 @@ def wgmma_m64(\n         for reg in acc.flat\n         for pos in range(2)\n     ]\n-    to_acc_vec_regs = functools.partial(_as_fragmented_reg_ndarray, dtype=out_ty, shape=acc.shape)\n-    acc_constraint = \"f\"\n+    to_acc_vec_regs = functools.partial(\n+        _as_fragmented_reg_ndarray, dtype=out_ty, shape=acc.shape)\n+    acc_constraint = \"r\" if ir.IntegerType.isinstance(out_ty) else \"f\"\n   elif ir.F16Type.isinstance(out_ty):\n     num_acc_regs = n // 4\n     out_ty_field = i32\n@@ -153,9 +159,15 @@ def wgmma_m64(\n     to_acc_vec_regs = lambda regs : np.array([_unpack_i32(vec_ty, reg) for reg in regs]).reshape(acc.shape)\n     acc_constraint = \"r\"\n   else:\n-    raise ValueError(f\"WGMMA instruciton only supports f32 and f16 out (got {out_ty})\")\n+    raise ValueError(\n+        f\"WGMMA instruction only supports f32, f16 and s32 out (got {out_ty})\")\n \n-  num_imm_regs = 4 if supports_transpose else 2\n+  if supports_transpose:\n+    num_imm_regs = 4\n+  elif out_ty == i32:\n+    num_imm_regs = 0\n+  else:\n+    num_imm_regs = 2\n \n   if a_in_regs:\n     a_reg_constraints = [\"r\"] * 4  # 4x f16x2 registers\n@@ -172,7 +184,6 @@ def wgmma_m64(\n       + [\"n\"] * (1 + num_imm_regs)  # literal constants\n   )\n   reg_constraints = \",\".join(reg_constraints_list)\n-\n   reg_count = itertools.count()\n \n   def take_regs(n):\n@@ -186,7 +197,8 @@ def take_regs(n):\n   else:\n     a_regs, = take_regs(1)\n   b_desc_reg, use_out_reg = take_regs(2)\n-  imm_regs = \", \".join(take_regs(num_imm_regs))  # Immediate regs (scale, ...).\n+  # Immediate regs (scale, ...).\n+  imm_regs = \"\".join(f\", {r}\" for r in take_regs(num_imm_regs))\n   assert next(reg_count) == len(reg_constraints_list)\n   k_instr = 32 // bytewidth(element_type)\n   el_ty = str(element_type)\n@@ -194,9 +206,19 @@ def take_regs(n):\n     el_ty = \"e5m2\"\n   elif ir.Float8E4M3FNType.isinstance(element_type):\n     el_ty = \"e4m3\"\n+  elif ir.IntegerType.get_signless(8).isinstance(element_type):\n+    # TODO(bchetioui): add u8 support in the future. Currently we always assume\n+    # that 8-bit integers are s8, and we would need to change the signature of\n+    # `wgmma` to indicate whether the input should be treated as signed or not.\n+    el_ty = \"s8\"\n+\n+  out_ty_str = str(out_ty)\n+  if out_ty == i32:\n+    out_ty_str = \"s32\"\n+\n   wgmma_instr = (\n-      f\"wgmma.mma_async.sync.aligned.m64n{n}k{k_instr}.{out_ty}.{el_ty}.{el_ty} \"\n-      f\"{acc_reg_vector}, {a_regs}, {b_desc_reg}, p, {imm_regs};\"\n+      f\"wgmma.mma_async.sync.aligned.m64n{n}k{k_instr}.{out_ty_str}.{el_ty}.{el_ty} \"\n+      f\"{acc_reg_vector}, {a_regs}, {b_desc_reg}, p{imm_regs};\"\n   )\n   ptx = f\"{{ .reg .pred p; setp.ne.b32 p, {use_out_reg}, 0; {wgmma_instr} }}\\n\"\n \n@@ -297,6 +319,8 @@ def wgmma(\n     )\n   f32 = ir.F32Type.get()\n   f16 = ir.F16Type.get()\n+  i32 = ir.IntegerType.get_signless(32)\n+  i8 = ir.IntegerType.get_signless(8)\n   if element_type == f32 or element_type == ir.BF16Type.get():\n     if acc.value.mlir_dtype != f32:\n       raise ValueError(\n@@ -312,6 +336,14 @@ def wgmma(\n           f\"WGMMA with element type {element_type} only supports accumulators \"\n           f\"of type f32 or f16, but got: {acc.value.mlir_dtype}\"\n       )\n+  elif element_type == i8:\n+    if a_in_regs and not a.is_signed:\n+      raise NotImplementedError(\"WGMMA with lhs of type u8\")\n+    if acc.value.mlir_dtype != i32 or not acc.value.is_signed:\n+      raise ValueError(\n+          f\"WGMMA with element type {element_type} only supports accumulators \"\n+          f\"of type s32, but got: {acc.value.mlir_dtype}\"\n+      )\n   else:\n     raise NotImplementedError(f\"Unsupported element type: {element_type}\")\n \ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 80e67b20e1ef..4e0544d1758e 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -645,6 +645,19 @@ def kernel(ctx, in_, out, smem):\n     np.testing.assert_array_equal(iota, expected)\n \n \n+class I8Type:\n+  \"\"\"A type that represents a 8-bit signed integer.\n+\n+  This is a workaround to bypass the fact that we don't have a proper 8-bit\n+  integer type class available in MLIR, and can't instantiate types without a\n+  MLIR context.\n+  \"\"\"\n+\n+  @staticmethod\n+  def get():  # pylint: disable=no-method-argument\n+    return ir.IntegerType.get_signless(8)\n+\n+\n class WGMMATest(TestCase):\n \n   def setUp(self):\n@@ -670,7 +683,67 @@ def setUp(self):\n       rhs_tiling_kind=(\"large\", \"small\", \"small+no_transpose\"),\n       lhs_tiling_kind=(\"large\", \"small\", \"small+no_transpose\"),\n   )\n-  def test_wgmma_basic(\n+  def test_wgmma_basic_float(\n+      self,\n+      lhs_transpose,\n+      rhs_transpose,\n+      in_mlir_dtype_cls,\n+      m,\n+      n,\n+      k_steps,\n+      swizzle,\n+      jax_out_dtype,\n+      rhs_tiling_kind,\n+      lhs_tiling_kind,\n+  ):\n+    self._test_wgmma_basic(\n+        m,\n+        n,\n+        k_steps,\n+        in_mlir_dtype_cls,\n+        lhs_transpose,\n+        rhs_transpose,\n+        swizzle,\n+        jax_out_dtype,\n+        rhs_tiling_kind,\n+        lhs_tiling_kind,\n+    )\n+\n+  @parameterized.product(\n+      in_mlir_dtype_cls=(I8Type,),\n+      m=(64, 128, 192),\n+      n=(64, 128, 192),\n+      k_steps=(1, 2),\n+      swizzle=(32, 64, 128),\n+      jax_out_dtype=(jnp.int32,),\n+      rhs_tiling_kind=(\"large\", \"small\", \"small+no_transpose\"),\n+      lhs_tiling_kind=(\"large\", \"small\"),\n+  )\n+  def test_wgmma_basic_int(\n+      self,\n+      in_mlir_dtype_cls,\n+      m,\n+      n,\n+      k_steps,\n+      swizzle,\n+      jax_out_dtype,\n+      rhs_tiling_kind,\n+      lhs_tiling_kind,\n+  ):\n+    self._test_wgmma_basic(\n+        m,\n+        n,\n+        k_steps,\n+        in_mlir_dtype_cls,\n+        lhs_transpose=False,\n+        rhs_transpose=True,\n+        swizzle=swizzle,\n+        jax_out_dtype=jax_out_dtype,\n+        rhs_tiling_kind=rhs_tiling_kind,\n+        lhs_tiling_kind=lhs_tiling_kind,\n+    )\n+\n+  def _test_wgmma_basic(\n       self,\n       m,\n       n,\n@@ -683,6 +756,10 @@ def test_wgmma_basic(\n       rhs_tiling_kind,\n       lhs_tiling_kind,\n   ):\n+    if jax_out_dtype == jnp.int32 and in_mlir_dtype_cls != I8Type:\n+      self.skipTest(\"s32 accumulator only supported with s8 inputs\")\n+    if jax_out_dtype != jnp.int32 and in_mlir_dtype_cls == I8Type:\n+      self.skipTest(\"s8 inputs only supported with s32 accumulator\")\n     if jax_out_dtype == jnp.float16 and in_mlir_dtype_cls in {ir.F32Type, ir.BF16Type}:\n       self.skipTest(f\"{in_mlir_dtype_cls.get()} does not support f16 output.\")\n     if swizzle != 128 and lhs_transpose and lhs_tiling_kind == \"large\":\n@@ -716,6 +793,9 @@ def test_wgmma_basic(\n     elif in_mlir_dtype_cls == ir.Float8E4M3FNType:\n       in_jax_dtype = jnp.float8_e4m3fn\n       exponent_bits, mantissa_bits = 4, 3\n+    elif in_mlir_dtype_cls == I8Type:\n+      in_jax_dtype = jnp.int8\n+      exponent_bits = mantissa_bits = None\n     else:\n       raise NotImplementedError(in_mlir_dtype)\n     nk_tile = swizzle // bytewidth(in_mlir_dtype)\n@@ -755,7 +835,8 @@ def kernel(ctx, lhs, rhs, out, scratch):\n       )\n       for i in range(2):\n         barriers[i].wait()\n-      init_acc = mgpu.WGMMAAccumulator.zero(m=m, n=n, dtype=out_mlir_dtype)\n+      is_signed = True if ir.IntegerType.isinstance(in_mlir_dtype) else None\n+      init_acc = mgpu.WGMMAAccumulator.zero(m=m, n=n, dtype=out_mlir_dtype, is_signed=is_signed)\n       if lhs_transpose:\n         perm = (0, 1, 3, 2) if transpose_lhs_tiles else (1, 0, 3, 2)\n         lhs_smem = memref_transpose(lhs_smem, perm)\n@@ -772,9 +853,13 @@ def quantize(x):\n       return jax.lax.reduce_precision(x, exponent_bits, mantissa_bits)\n \n     x_shape = (k, m) if lhs_transpose else (m, k)\n-    x = quantize(self.prng.uniform(-1, 1, x_shape)).astype(in_jax_dtype)\n     y_shape = (n, k) if rhs_transpose else (k, n)\n-    y = quantize(self.prng.uniform(-1, 1, y_shape)).astype(in_jax_dtype)\n+    if in_mlir_dtype_cls == I8Type:\n+      x = self.prng.integers(-128, 127, x_shape).astype(in_jax_dtype)\n+      y = self.prng.integers(-128, 127, y_shape).astype(in_jax_dtype)\n+    else:\n+      x = quantize(self.prng.uniform(-1, 1, x_shape)).astype(in_jax_dtype)\n+      y = quantize(self.prng.uniform(-1, 1, y_shape)).astype(in_jax_dtype)\n     out_shape = jax.ShapeDtypeStruct((m, n), jax_out_dtype)\n     if transpose_rhs_tiles:\n       rhs_tiling_t = rhs_tiling[::-1] if rhs_transpose else rhs_tiling\n@@ -797,7 +882,9 @@ def quantize(x):\n     x32, y32 = x.astype(np.float32), y.astype(np.float32)\n     ref = (x32.T if lhs_transpose else x32) @ (y32.T if rhs_transpose else y32)\n     atol = 2e-2 if jax_out_dtype == jnp.float16 else 5e-6\n-    if utils.bitwidth(in_mlir_dtype) == 8:\n+    if ir.IntegerType.isinstance(in_mlir_dtype) and ir.IntegerType.isinstance(out_mlir_dtype):\n+      atol = 0\n+    elif utils.bitwidth(in_mlir_dtype) == 8:\n       atol = 3e-2\n     np.testing.assert_allclose(z, ref, atol=atol)\n \n\n```",
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Removed `GPU*` aliases\n\nPiperOrigin-RevId: 761482778",
        "output": "```diff\nCommit: 2f0ca797f0a90801714cd5093f32983275db3296\nDate: 2025-05-21T12:26:06Z\nURL: https://github.com/jax-ml/jax/commit/2f0ca797f0a90801714cd5093f32983275db3296\nFiles changed: 1\nAdditions: +0, Deletions: -6\ndiff --git a/jax/experimental/pallas/mosaic_gpu.py b/jax/experimental/pallas/mosaic_gpu.py\nindex cc0e185e296a..8c7870412403 100644\n--- a/jax/experimental/pallas/mosaic_gpu.py\n+++ b/jax/experimental/pallas/mosaic_gpu.py\n@@ -69,9 +69,3 @@\n SMEM = MemorySpace.SMEM\n #: Alias of :data:`jax.experimental.pallas.mosaic_gpu.MemorySpace.TMEM`.\n TMEM = MemorySpace.TMEM\n-\n-# TODO(slebedev): Deprecate and remove these aliases.\n-GPUBlockSpec = BlockSpec\n-GPUCompilerParams = CompilerParams\n-GPUMemorySpace = MemorySpace\n-GPUMesh = Mesh\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Disable the newly added tfrt targets that never worked\n\nPiperOrigin-RevId: 761442340",
        "output": "```diff\nCommit: 4d6e39e4dd38e60dcd4b999284c54030a4076cea\nDate: 2025-05-21T09:53:32Z\nURL: https://github.com/jax-ml/jax/commit/4d6e39e4dd38e60dcd4b999284c54030a4076cea\nFiles changed: 1\nAdditions: +6, Deletions: -0\ndiff --git a/tests/BUILD b/tests/BUILD\nindex aa777080fd92..2418c8224869 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -460,6 +460,9 @@ jax_multiplatform_test(\n     backend_tags = {\n         \"gpu\": [\"noasan\"],  # Memory leaks in NCCL, see https://github.com/NVIDIA/nccl/pull/1143\n     },\n+    disable_configs = [\n+        \"gpu_h100x2_tfrt\",  # TODO(b/419192167): Doesn't work\n+    ],\n     enable_backends = [\"gpu\"],\n     tags = [\n         \"config-cuda-only\",\n@@ -1844,6 +1847,9 @@ jax_multiplatform_test(\n jax_multiplatform_test(\n     name = \"shard_map_test\",\n     srcs = [\"shard_map_test.py\"],\n+    disable_configs = [\n+        \"gpu_h100x2_tfrt\",  # TODO(b/419192167): Doesn't work\n+    ],\n     enable_configs = [\n         \"gpu_p100x2_shardy\",\n         \"tpu_v3_x4_shardy\",\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add missing test skips (for too old jaxlib) + bump minimum libtpu version\n\nPiperOrigin-RevId: 761429155",
        "output": "```diff\nCommit: d8804aae654bf902fb1cc7c92e22ad65b1c59e6b\nDate: 2025-05-21T09:08:19Z\nURL: https://github.com/jax-ml/jax/commit/d8804aae654bf902fb1cc7c92e22ad65b1c59e6b\nFiles changed: 4\nAdditions: +7, Deletions: -2\ndiff --git a/.github/workflows/cloud-tpu-ci-nightly.yml b/.github/workflows/cloud-tpu-ci-nightly.yml\nindex 5cc2aebe3cd0..061b399132e2 100644\n--- a/.github/workflows/cloud-tpu-ci-nightly.yml\n+++ b/.github/workflows/cloud-tpu-ci-nightly.yml\n@@ -44,7 +44,7 @@ jobs:\n             jaxlib-version: \"pypi_latest\"\n     name: \"TPU test (jaxlib=${{ matrix.jaxlib-version }}, ${{ matrix.tpu.type }})\"\n     env:\n-      LIBTPU_OLDEST_VERSION_DATE: 20241205\n+      LIBTPU_OLDEST_VERSION_DATE: 20250226\n       PYTHON: python${{ matrix.python-version }}\n     runs-on: ${{ matrix.tpu.runner }}\n     container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 55a0b4cc1a5f..8ecccbe274e9 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -82,7 +82,7 @@ jobs:\n     # End Presubmit Naming Check github-tpu-presubmits\n \n     env:\n-      LIBTPU_OLDEST_VERSION_DATE: 20241205\n+      LIBTPU_OLDEST_VERSION_DATE: 20250226\n       JAXCI_HERMETIC_PYTHON_VERSION: \"${{ inputs.python }}\"\n       JAXCI_PYTHON: \"python${{ inputs.python }}\"\n       JAXCI_RUN_FULL_TPU_TEST_SUITE: \"${{ inputs.run-full-tpu-test-suite }}\"\ndiff --git a/tests/pallas/gpu_pallas_distributed_test.py b/tests/pallas/gpu_pallas_distributed_test.py\nindex d862e6b9b819..81433b8c5067 100644\n--- a/tests/pallas/gpu_pallas_distributed_test.py\n+++ b/tests/pallas/gpu_pallas_distributed_test.py\n@@ -34,6 +34,8 @@\n class PallasCallRemoteDMATest(jt_multiprocess.MultiProcessTest):\n \n   def setUp(self):\n+    if jtu.jaxlib_version() < (0, 6, 1):\n+      self.skipTest(\"Test requires jaxlib >= 0.6.1\")\n     if (not jtu.test_device_matches([\"cuda\"]) or\n         not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n       self.skipTest(\"Only works on GPU with capability >= sm90\")\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 88ec58ec37cd..523601691a97 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -63,6 +63,7 @@\n from jax._src import xla_bridge\n from jax._src.lib import xla_client as xc\n from jax._src.lib import _jax\n+from jax._src.lib import jaxlib_extension_version\n from jax._src.util import curry, unzip2\n \n config.parse_flags_with_absl()\n@@ -7761,6 +7762,8 @@ def f(x):\n   @config.use_shardy_partitioner(True)\n   @jtu.with_explicit_mesh((2, 2), ('x', 'y'))\n   def test_unreduced_basic(self, mesh):\n+    if jaxlib_extension_version < 342:\n+      self.skipTest(\"Test requires a newer jaxlib\")\n     np_inp = np.arange(16).reshape(8, 2)\n     x = jax.device_put(np_inp, P('x', 'y'))\n     y = jax.device_put(np_inp.T, P('y', None))\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Support passing PartitionSpecs to ShapeDtypeStruct when there is a mesh in context.\n\nPiperOrigin-RevId: 761322712",
        "output": "```diff\nCommit: eef1f6cf9af4fabd47a70b71f78bf1339c9a36cd\nDate: 2025-05-21T02:36:59Z\nURL: https://github.com/jax-ml/jax/commit/eef1f6cf9af4fabd47a70b71f78bf1339c9a36cd\nFiles changed: 3\nAdditions: +35, Deletions: -10\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 33802e494304..059db1c92c98 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -2826,9 +2826,10 @@ def __init__(self, shape, dtype, *, sharding=None, weak_type=False):\n     if dtype is None:\n       raise ValueError(\"ShapeDtypeStruct: dtype must be specified.\")\n     self.dtype = dtype if dtypes.issubdtype(dtype, dtypes.extended) else np.dtype(dtype)\n-    if sharding is not None and not isinstance(sharding, (Sharding, Layout)):\n+    if sharding is not None and not isinstance(sharding, (Sharding, Layout, P)):\n       raise ValueError(\n-          \"sharding should be an instance of `jax.sharding.Sharding` or\"\n+          \"sharding should be an instance of `jax.sharding.Sharding`, \"\n+          \"`jax.sharding.PartitionSpec` or\"\n           f\" `jax.experimental.layout.Layout`. Got {sharding} of type\"\n           f\" {type(sharding)}.\")\n     if (isinstance(sharding, Layout) and\n@@ -2836,7 +2837,19 @@ def __init__(self, shape, dtype, *, sharding=None, weak_type=False):\n       raise TypeError(\n           \"`DeviceLocalLayout.AUTO` cannot be used in place of a device-local\"\n           f\" layout in a `ShapeDtypeStruct`. Got {sharding}\")\n-    self.sharding = sharding.sharding if isinstance(sharding, Layout) else sharding\n+    if isinstance(sharding, Layout):\n+      self.sharding = sharding.sharding\n+    elif isinstance(sharding, P):\n+      # TODO(yashkatariya): Should this be abstract mesh?\n+      cur_mesh = get_concrete_mesh()\n+      if cur_mesh is None:\n+        raise TypeError(\n+            \"When specifying PartitionSpec to `ShapeDtypeStruct`, the context\"\n+            \" mesh cannot be empty. Please use `jax.sharding.use_mesh` to set\"\n+            \" the mesh context.\")\n+      self.sharding = NamedSharding(cur_mesh, sharding)\n+    else:\n+      self.sharding = sharding\n     self._dll = sharding.device_local_layout if isinstance(sharding, Layout) else None\n     self.weak_type = weak_type\n \ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 3fe3d6fa7514..9963e2603588 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -4525,13 +4525,6 @@ def foo(x):\n     with self.assertRaisesRegex(TypeError, \"applied to foo\"):\n       f_vjp(1.0, 1.0)\n \n-  def test_shapedtypestruct_sharding_error(self):\n-    with self.assertRaisesRegex(\n-        ValueError,\n-        \"sharding should be an instance of `jax.sharding.Sharding`.\"):\n-      jax.ShapeDtypeStruct((8, 2), np.float32,\n-                           sharding=jax.sharding.PartitionSpec('x'))\n-\n   def test_make_jaxpr_weakref(self):\n     class Foo(NamedTuple):\n       x: int\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 7d87705e20bd..88ec58ec37cd 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -5006,6 +5006,25 @@ def test_sds_update(self):\n     with self.assertRaisesRegex(ValueError, \"updating ShapeDtypeStruct\"):\n       s4.update(sharding=NamedSharding(mesh, P('x')))\n \n+  @jtu.with_explicit_mesh((2, 1), ('x', 'y'), axis_types=(AxisType.Auto,) * 2)\n+  def test_sds_pspec_input(self, mesh):\n+    inp = jax.ShapeDtypeStruct((2, 2), np.float32, sharding=P('x'))\n+    lowered = jax.jit(lambda x: x * 2).lower(inp)\n+    self.assertIn('num_partitions = 2', lowered.as_text())\n+\n+    np_inp = np.arange(4, dtype=np.float32).reshape(2, 2)\n+    arr = jax.device_put(np_inp, P('x'))\n+    out = lowered.compile()(arr)\n+    self.assertArraysEqual(out, np_inp * 2)\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P('x')))\n+\n+  def test_sds_pspec_no_mesh_ctx_error(self):\n+    with self.assertRaisesRegex(\n+        TypeError,\n+        'When specifying PartitionSpec to `ShapeDtypeStruct`, the context mesh'\n+        ' cannot be empty'):\n+      jax.ShapeDtypeStruct((2, 2), np.float32, sharding=P('x'))\n+\n \n def spec_regex(s):\n   return str(s).replace(r\"(\", r\"\\(\").replace(r\")\", r\"\\)\")\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28871 from hawkinsp:ci\n\nPiperOrigin-RevId: 761306262",
        "output": "```diff\nCommit: 56d4cb0fff1bd24f14c026efffda64252598fd43\nDate: 2025-05-21T01:22:34Z\nURL: https://github.com/jax-ml/jax/commit/56d4cb0fff1bd24f14c026efffda64252598fd43\nFiles changed: 1\nAdditions: +6, Deletions: -0\ndiff --git a/.bazelrc b/.bazelrc\nindex 53676637c839..8906234c9061 100644\n--- a/.bazelrc\n+++ b/.bazelrc\n@@ -244,6 +244,9 @@ build:ci_linux_aarch64_base --config=clang --verbose_failures=true\n build:ci_linux_aarch64_base --action_env=TF_SYSROOT=\"/dt10\"\n build:ci_linux_aarch64_base --color=yes\n \n+# This appears to help avoid a timeout in CI for linalg_test.\n+build:ci_linux_aarch64_base --test_env=OMP_NUM_THREADS=8\n+\n build:ci_linux_aarch64 --config=ci_linux_aarch64_base\n build:ci_linux_aarch64 --host_crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n build:ci_linux_aarch64 --crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n@@ -379,6 +382,9 @@ build:rbe_cross_compile_base --remote_instance_name=projects/tensorflow-testing/\n build:rbe_cross_compile_linux_aarch64 --config=cross_compile_linux_aarch64\n build:rbe_cross_compile_linux_aarch64 --config=rbe_cross_compile_base\n \n+# Avoids a timeout in linalg_test on ARM.\n+build:rbe_cross_compile_linux_aarch64 --test_env=OMP_NUM_THREADS=8\n+\n # Mac x86\n build:cross_compile_darwin_x86_64 --config=cross_compile_base\n build:cross_compile_darwin_x86_64 --config=nonccl\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reland the limit on the number of OpenBLAS threads.\n\nThis was previously removed in https://github.com/jax-ml/jax/commit/18ff6caa4f767701dd7cca3a1333d9b99465e045, and that promptly broke our CI again. I am guessing the problem is actually too few threads, not a NumPy deadlock as I originally guessed.",
        "output": "```diff\nCommit: 862791a91e2a8c87fe4c2c46ae9fd6a9e01adbec\nDate: 2025-05-21T01:15:10Z\nURL: https://github.com/jax-ml/jax/commit/862791a91e2a8c87fe4c2c46ae9fd6a9e01adbec\nFiles changed: 1\nAdditions: +6, Deletions: -0\ndiff --git a/.bazelrc b/.bazelrc\nindex 53676637c839..8906234c9061 100644\n--- a/.bazelrc\n+++ b/.bazelrc\n@@ -244,6 +244,9 @@ build:ci_linux_aarch64_base --config=clang --verbose_failures=true\n build:ci_linux_aarch64_base --action_env=TF_SYSROOT=\"/dt10\"\n build:ci_linux_aarch64_base --color=yes\n \n+# This appears to help avoid a timeout in CI for linalg_test.\n+build:ci_linux_aarch64_base --test_env=OMP_NUM_THREADS=8\n+\n build:ci_linux_aarch64 --config=ci_linux_aarch64_base\n build:ci_linux_aarch64 --host_crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n build:ci_linux_aarch64 --crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n@@ -379,6 +382,9 @@ build:rbe_cross_compile_base --remote_instance_name=projects/tensorflow-testing/\n build:rbe_cross_compile_linux_aarch64 --config=cross_compile_linux_aarch64\n build:rbe_cross_compile_linux_aarch64 --config=rbe_cross_compile_base\n \n+# Avoids a timeout in linalg_test on ARM.\n+build:rbe_cross_compile_linux_aarch64 --test_env=OMP_NUM_THREADS=8\n+\n # Mac x86\n build:cross_compile_darwin_x86_64 --config=cross_compile_base\n build:cross_compile_darwin_x86_64 --config=nonccl\n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Merge pull request #28863 from hawkinsp:tsan2\n\nPiperOrigin-RevId: 761283866",
        "output": "```diff\nCommit: 7de62de0b613b23fd9519ce69b71263539397b3e\nDate: 2025-05-21T00:01:51Z\nURL: https://github.com/jax-ml/jax/commit/7de62de0b613b23fd9519ce69b71263539397b3e\nFiles changed: 3\nAdditions: +0, Deletions: -7\ndiff --git a/.github/workflows/tsan-suppressions_3.13.txt b/.github/workflows/tsan-suppressions_3.13.txt\nindex a929a8c44728..483e3f0b3c2a 100644\n--- a/.github/workflows/tsan-suppressions_3.13.txt\n+++ b/.github/workflows/tsan-suppressions_3.13.txt\n@@ -23,9 +23,6 @@ race_top:PyMember_GetOne\n race_top:new_reference\n race:_Py_IsOwnedByCurrentThread\n \n-# https://github.com/python/cpython/issues/129748\n-race:mi_block_set_nextx\n-\n # https://github.com/python/cpython/issues/128130\n race_top:run_eval_code_obj\n \ndiff --git a/.github/workflows/tsan-suppressions_3.14.txt b/.github/workflows/tsan-suppressions_3.14.txt\nindex 384560128cfc..008b61933a0b 100644\n--- a/.github/workflows/tsan-suppressions_3.14.txt\n+++ b/.github/workflows/tsan-suppressions_3.14.txt\n@@ -8,9 +8,6 @@ race:dnnl_sgemm\n # https://github.com/python/cpython/issues/128050\n race:partial_vectorcall_fallback\n \n-# https://github.com/python/cpython/issues/129748\n-race:mi_block_set_nextx\n-\n # Races because the LAPACK and BLAS in our scipy isn't TSAN instrumented.\n race:heevd_ffi\n race:gesdd_ffi\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex 882e140b91ad..ce4130c31a30 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -14,7 +14,6 @@ on:\n     paths:\n       - '**/workflows/tsan.yaml'\n       - '**/workflows/tsan-suppressions*.txt'\n-      - '**/workflows/requirements_lock_3_13_ft.patch'\n \n jobs:\n   tsan:\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[JAX] Fix float typo in a code example in the sharded-computation doc.\n\nPiperOrigin-RevId: 761268857",
        "output": "```diff\nCommit: b96a63904faf82b9404c8b551cff9c1c6bbb962f\nDate: 2025-05-20T23:15:28Z\nURL: https://github.com/jax-ml/jax/commit/b96a63904faf82b9404c8b551cff9c1c6bbb962f\nFiles changed: 2\nAdditions: +4, Deletions: -4\ndiff --git a/docs/sharded-computation.ipynb b/docs/sharded-computation.ipynb\nindex 1bae4014b5a8..72cc2d193bfd 100644\n--- a/docs/sharded-computation.ipynb\n+++ b/docs/sharded-computation.ipynb\n@@ -7,7 +7,7 @@\n     \"(sharded-computation)=\\n\",\n     \"# Introduction to parallel programming\\n\",\n     \"\\n\",\n-    \"<!--* freshness: { reviewed: '2024-05-10' } *-->\\n\",\n+    \"<!--* freshness: { reviewed: '2025-05-19' } *-->\\n\",\n     \"\\n\",\n     \"This tutorial serves as an introduction to device parallelism for Single-Program Multi-Data (SPMD) code in JAX. SPMD is a parallelism technique where the same computation, such as the forward pass of a neural network, can be run on different input data (for example, different inputs in a batch) in parallel on different devices, such as several GPUs or Google TPUs.\\n\",\n     \"\\n\",\n@@ -495,7 +495,7 @@\n    \"id\": \"c09acf7d\",\n    \"metadata\": {},\n    \"source\": [\n-    \"We should read the type `f32[4@X, 2]` as \\\"a 4-by-2 array of 32-bit floats whose first dimension\\n\",\n+    \"We should read the type `int32[4@X, 2]` as \\\"a 4-by-2 array of 32-bit ints whose first dimension\\n\",\n     \"is sharded along mesh axis 'X'. The array is replicated along all other mesh\\n\",\n     \"axes\\\"\\n\",\n     \"\\n\",\ndiff --git a/docs/sharded-computation.md b/docs/sharded-computation.md\nindex 16a5dc8cfa08..89ffbc07da38 100644\n--- a/docs/sharded-computation.md\n+++ b/docs/sharded-computation.md\n@@ -14,7 +14,7 @@ kernelspec:\n (sharded-computation)=\n # Introduction to parallel programming\n \n-<!--* freshness: { reviewed: '2024-05-10' } *-->\n+<!--* freshness: { reviewed: '2025-05-19' } *-->\n \n This tutorial serves as an introduction to device parallelism for Single-Program Multi-Data (SPMD) code in JAX. SPMD is a parallelism technique where the same computation, such as the forward pass of a neural network, can be run on different input data (for example, different inputs in a batch) in parallel on different devices, such as several GPUs or Google TPUs.\n \n@@ -193,7 +193,7 @@ print(f\"replicated_array type: {jax.typeof(replicated_array)}\")\n print(f\"sharded_array type: {jax.typeof(sharded_array)}\")\n ```\n \n-We should read the type `f32[4@X, 2]` as \"a 4-by-2 array of 32-bit floats whose first dimension\n+We should read the type `int32[4@X, 2]` as \"a 4-by-2 array of 32-bit ints whose first dimension\n is sharded along mesh axis 'X'. The array is replicated along all other mesh\n axes\"\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reverts 06448864abd6e8187e5b4d9b1ff08ab14fe3b8e0\n\nPiperOrigin-RevId: 761237485",
        "output": "```diff\nCommit: 12e07c963286c9fa7eb7d8651ce968537be1ab8a\nDate: 2025-05-20T21:44:40Z\nURL: https://github.com/jax-ml/jax/commit/12e07c963286c9fa7eb7d8651ce968537be1ab8a\nFiles changed: 4\nAdditions: +11, Deletions: -54\ndiff --git a/jax/_src/compiler.py b/jax/_src/compiler.py\nindex e8ef647a1312..04f993fed799 100644\n--- a/jax/_src/compiler.py\n+++ b/jax/_src/compiler.py\n@@ -292,19 +292,6 @@ def backend_compile(\n     executable_devices: xc.DeviceList,\n     options: xc.CompileOptions,\n     host_callbacks: Sequence[Any],\n-) -> xc.LoadedExecutable:\n-  return backend_compile_and_load(\n-      backend, module, executable_devices, options, host_callbacks\n-  )\n-\n-\n-@profiler.annotate_function\n-def backend_compile_and_load(\n-    backend: xc.Client,\n-    module: ir.Module,\n-    executable_devices: xc.DeviceList,\n-    options: xc.CompileOptions,\n-    host_callbacks: Sequence[Any],\n ) -> xc.LoadedExecutable:\n   sym_name = module.operation.attributes['sym_name']\n   module_name = ir.StringAttr(sym_name).value\n@@ -335,35 +322,18 @@ def backend_compile_and_load(\n \n     # we use a separate function call to ensure that XLA compilation appears\n     # separately in Python profiling results\n-    elif jaxlib_extension_version < 342 or isinstance(backend, xc.CompileOnlyPyClient):\n-      if host_callbacks:\n-        return backend.compile(\n-            built_c,\n-            executable_devices=executable_devices,  # type: ignore\n-            compile_options=options,\n-            host_callbacks=host_callbacks,\n-        )\n-      # Some backends don't have `host_callbacks` option yet\n-      # TODO(sharadmv): remove this fallback when all backends allow `compile`\n-      # to take in `host_callbacks`\n+    if host_callbacks:\n       return backend.compile(\n-          built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n-    else:\n-      if host_callbacks:\n-        return backend.compile_and_load(\n-            built_c,\n-            executable_devices=executable_devices,\n-            compile_options=options,\n-            host_callbacks=host_callbacks,\n-        )\n-      # Some backends don't have `host_callbacks` option yet\n-      # TODO(sharadmv): remove this fallback when all backends allow `compile`\n-      # to take in `host_callbacks`\n-      return backend.compile_and_load(\n           built_c,\n-          executable_devices=executable_devices,\n+          executable_devices=executable_devices,  # type: ignore\n           compile_options=options,\n+          host_callbacks=host_callbacks,\n       )\n+    # Some backends don't have `host_callbacks` option yet\n+    # TODO(sharadmv): remove this fallback when all backends allow `compile`\n+    # to take in `host_callbacks`\n+    return backend.compile(\n+        built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n   except xc.XlaRuntimeError as e:\n     for error_handler in _XLA_RUNTIME_ERROR_HANDLERS:\n       handler_result = error_handler(e)\n@@ -428,7 +398,7 @@ def compile_or_get_cached(\n   )\n \n   if cache_key is None:\n-    return backend_compile_and_load(\n+    return backend_compile(\n         backend, computation, executable_devices, compile_options,\n         host_callbacks)\n \n@@ -456,7 +426,7 @@ def compile_or_get_cached(\n       config.share_binary_between_hosts.value\n       and is_multi_process\n       and distributed.global_state.client is not None\n-      # Host callbacks are currently baked into the HLO module so we can't share\n+      # Host callbacks are currently baked into the HLO module so we cant share\n       # them.\n       and len(host_callbacks) == 0\n   ):\n@@ -746,7 +716,7 @@ def _compile_and_write_cache(\n     cache_key: str,\n ) -> xc.LoadedExecutable:\n   start_time = time.monotonic()\n-  executable = backend_compile_and_load(\n+  executable = backend_compile(\n       backend, computation, executable_devices, compile_options, host_callbacks\n   )\n   compile_time = time.monotonic() - start_time\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 000c05acacad..1d7f3042e8a3 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -551,17 +551,6 @@ class Client:\n   ) -> PjRtLayout: ...\n   def __getattr__(self, name: str) -> Any: ...\n \n-\n-class CompileOnlyPyClient(Client):\n-  def compile(\n-      self,\n-      computation: str | bytes,\n-      executable_devices: DeviceList | Sequence[Device],\n-      compile_options: CompileOptions = ...,\n-      host_callbacks: Sequence[Any] = ...,\n-  ) -> LoadedExecutable: ...\n-\n-\n class CpuCollectives: ...\n \n def make_gloo_tcp_collectives(\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex b1bbc464610e..8f8c829ee6c7 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -304,7 +304,6 @@ def computation_count():\n \n XlaComputation = _xla.XlaComputation\n Client = _xla.Client\n-CompileOnlyPyClient = _xla.CompileOnlyPyClient\n Memory = _xla.Memory\n Array = _xla.Array\n ArrayImpl = _xla.ArrayImpl\ndiff --git a/jaxlib/xla_client.pyi b/jaxlib/xla_client.pyi\nindex fce114f45474..80599e86676b 100644\n--- a/jaxlib/xla_client.pyi\n+++ b/jaxlib/xla_client.pyi\n@@ -24,7 +24,6 @@ from jaxlib._jax import ArrayCopySemantics as ArrayCopySemantics\n from jaxlib._jax import ArrayImpl as ArrayImpl\n from jaxlib._jax import AutotuneCacheMode as AutotuneCacheMode\n from jaxlib._jax import Client as Client\n-from jaxlib._jax import CompileOnlyPyClient as CompileOnlyPyClient\n from jaxlib._jax import CompileOptions as CompileOptions\n from jaxlib._jax import Device as Device\n from jaxlib._jax import DeviceAssignment as DeviceAssignment\n\n```",
        "from_id": [
            "hawkinsp",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Make sure tests with `--build_jaxlib=false` depend on NVIDIA CUDA wheels hermetically.\n\nPiperOrigin-RevId: 761231648",
        "output": "```diff\nCommit: 5d3134e9fa3d40f0a24ce08a6225d507d65b634c\nDate: 2025-05-20T21:28:35Z\nURL: https://github.com/jax-ml/jax/commit/5d3134e9fa3d40f0a24ce08a6225d507d65b634c\nFiles changed: 2\nAdditions: +15, Deletions: -2\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex a8dc67eb3804..eceb38e35aab 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -190,8 +190,8 @@ def _gpu_test_deps():\n             \"@pypi//nvidia_nvshmem_cu12\",\n         ],\n         \"//jax:config_build_jaxlib_false\": [\n-            \"@pypi//jax_cuda12_plugin\",\n-            \"@pypi//jax_cuda12_pjrt\",\n+            \"//jaxlib/tools:pypi_jax_cuda_plugin_with_cuda_deps\",\n+            \"//jaxlib/tools:pypi_jax_cuda_pjrt_with_cuda_deps\",\n             \"@pypi//nvidia_nvshmem_cu12\",\n         ],\n         \"//jax:config_build_jaxlib_wheel\": [\ndiff --git a/jaxlib/tools/BUILD.bazel b/jaxlib/tools/BUILD.bazel\nindex 22bae26a4420..d6a5f94dfd4b 100644\n--- a/jaxlib/tools/BUILD.bazel\n+++ b/jaxlib/tools/BUILD.bazel\n@@ -487,6 +487,19 @@ py_import(\n     wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n )\n \n+# The targets below are used for GPU tests with `--//jax:build_jaxlib=false`.\n+py_import(\n+    name = \"pypi_jax_cuda_plugin_with_cuda_deps\",\n+    wheel = \"@pypi_jax_cuda12_plugin//:whl\",\n+    wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n+)\n+\n+py_import(\n+    name = \"pypi_jax_cuda_pjrt_with_cuda_deps\",\n+    wheel = \"@pypi_jax_cuda12_pjrt//:whl\",\n+    wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n+)\n+\n # Wheel tests.\n \n AARCH64_MANYLINUX_TAG = \"_\".join(PLATFORM_TAGS_DICT[(\"Linux\", \"aarch64\")])\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic:TPU] Enforce that tpu.dynamic_gather operands and result have the same shape\n\nPiperOrigin-RevId: 761230458",
        "output": "```diff\nCommit: 4ff6eb25f21e7a8296bcb6f8adbc5ffec4e8ae6b\nDate: 2025-05-20T21:25:26Z\nURL: https://github.com/jax-ml/jax/commit/4ff6eb25f21e7a8296bcb6f8adbc5ffec4e8ae6b\nFiles changed: 3\nAdditions: +23, Deletions: -10\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex a9fbf8dcd982..eb5e6df7b381 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -2369,7 +2369,7 @@ def _gather_lowering_rule(\n         operand_batching_dims=(1,),\n         start_indices_batching_dims=(1,),\n     ):\n-      return tpu.dynamic_gather(out_type, x, recovered_indices, 0)\n+      return tpu.dynamic_gather(x, recovered_indices, 0)\n     if dimension_numbers == lax.GatherDimensionNumbers(\n         offset_dims=(),\n         collapsed_slice_dims=(1,),\n@@ -2377,7 +2377,7 @@ def _gather_lowering_rule(\n         operand_batching_dims=(0,),\n         start_indices_batching_dims=(0,),\n     ):\n-      return tpu.dynamic_gather(out_type, x, recovered_indices, 1)\n+      return tpu.dynamic_gather(x, recovered_indices, 1)\n   raise NotImplementedError(\"Unsupported gather\")\n \n \ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu.td b/jaxlib/mosaic/dialect/tpu/tpu.td\nindex 29ce9c84de07..b6ae1e52e822 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu.td\n+++ b/jaxlib/mosaic/dialect/tpu/tpu.td\n@@ -466,10 +466,19 @@ def TPU_GatherOp : TPU_Op<\"gather\", [Pure]> {\n   }];\n }\n \n-def TPU_DynamicGatherOp : TPU_Op<\"dynamic_gather\", [Pure]> {\n+def TPU_DynamicGatherOp : TPU_Op<\"dynamic_gather\", [Pure, SameOperandsAndResultShape, AllTypesMatch<[\"source\", \"output\"]>]> {\n+  let description = [{\n+    Gathers elements from `source` using `indices`.\n+\n+    Given a shape `N0 x N1 x ...`, `output[i0, i1, ...]` is given by\n+    `input[j0, j1, ...]` where `jn = indices[i0, i1, ...] mod Ni` for\n+    `n = dimension` and `jn = in` otherwise.\n+\n+    Similar to `np.take_along_axis`, except that OOB indices wrap.\n+  }];\n   let arguments = (ins\n     AnyVectorOfNonZeroRank:$source,\n-    AnyVectorOfNonZeroRank:$indices,\n+    VectorOfNonZeroRankOf<[AnyInteger]>:$indices,\n     I32Attr:$dimension\n   );\n   let results = (outs AnyVectorOfNonZeroRank:$output);\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 5ddff9d9ee53..fa14c8ef9238 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -3537,12 +3537,13 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n                  std::array{false, true}) {  // Lane broadcast\n         TPU_ASSERT_EQ_OP(*(src_tiles.dimensions().end() - 1), 1);\n         TPU_ASSERT_OP(offsets_in[1].has_value());\n+        VectorType i32_vreg_ty =\n+            getNativeVregType(builder.getI32Type(), ctx.target_shape);\n         const int64_t offset = *offsets_in[1];\n         const int64_t lane_offset = offset % ctx.target_shape[1];\n         const int64_t tile_offset = offset / ctx.target_shape[1];\n         Value lane_offset_cst = getFullVector(\n-            builder, getNativeVregType(builder.getI32Type(), ctx.target_shape),\n-            builder.getI32IntegerAttr(lane_offset));\n+            builder, i32_vreg_ty, builder.getI32IntegerAttr(lane_offset));\n         DenseI32ArrayAttr sublane_pattern;\n         if (num_tiles != 1) {\n           SmallVector<int32_t> pattern;\n@@ -3555,7 +3556,7 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n           sublane_pattern = builder.getDenseI32ArrayAttr(pattern);\n         }\n         src_tiles.Each([&](const absl::Span<const int64_t> src_idx,\n-                           Value *const src_tile) {\n+                           Value *const src_vreg) {\n           SmallVector<int64_t> dst_starts(dst_tiles_implicit_shape.size());\n           SmallVector<int64_t> dst_limits(dst_tiles_implicit_shape.size());\n           for (int64_t i = 0; i < dst_tiles.num_dimensions(); ++i) {\n@@ -3567,10 +3568,13 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n               dst_limits[i] = dst_starts[i] + 1;\n             }\n           }\n-          Value res_vreg = builder.create<tpu::DynamicGatherOp>(\n-              broadcast_op.getLoc(), src_tile->getType(), *src_tile,\n-              lane_offset_cst,\n+          Value src_vreg_i32 =\n+              builder.create<tpu::BitcastVregOp>(i32_vreg_ty, *src_vreg);\n+          Value res_vreg_i32 = builder.create<tpu::DynamicGatherOp>(\n+              broadcast_op.getLoc(), i32_vreg_ty, src_vreg_i32, lane_offset_cst,\n               /*dimension=*/1);\n+          Value res_vreg = builder.create<tpu::BitcastVregOp>(\n+              src_vreg->getType(), res_vreg_i32);\n           if (num_tiles != 1) {\n             res_vreg = builder.create<tpu::GatherOp>(\n                 broadcast_op.getLoc(), res_vreg.getType(), res_vreg,\n\n```",
        "from_id": [
            "tlongeri",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[ragged-paged-attn] Apply kv mask to filter out NaNs\n\nExpected small regression as we insert kv masking logic which needs to unpack/pack.\n\nPiperOrigin-RevId: 761217303",
        "output": "```diff\nCommit: 11cf85deb0776f800144e829d4ba8e38eb9d76fc\nDate: 2025-05-20T20:53:11Z\nURL: https://github.com/jax-ml/jax/commit/11cf85deb0776f800144e829d4ba8e38eb9d76fc\nFiles changed: 2\nAdditions: +76, Deletions: -40\ndiff --git a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\nindex cd5de96ccca7..df47674a59a9 100644\n--- a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n+++ b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n@@ -39,19 +39,16 @@ def __init__(\n       vmem_buf,  # [num_kv_pages_per_blk, page_size, num_combined_kv_heads_per_blk, head_dim]\n       sem,\n       page_indices_ref,  # i32[max_num_seqs, pages_per_seq]\n-      offset,  # [seq_idx, kv_pages_start]\n+      metadata,  # [seq_idx, start_page_idx, end_page_idx]\n   ):\n     self._vmem_buf = vmem_buf\n-    seq_id, kv_pages_start = offset\n-    pages_per_seq = page_indices_ref.shape[1]\n+    seq_id, start_page_idx, end_page_idx = metadata\n     self._async_copies = []\n     # TODO(jevinjiang): Only fetch dynamic shape in need! This will insert\n     # a bunch of if-ops. Check the performance when we have benchmarking setup.\n     for i in range(vmem_buf.shape[0]):\n-      page_idx = kv_pages_start + i\n-      page_idx = jax.lax.select(\n-          page_idx < pages_per_seq, page_idx, pages_per_seq - 1\n-      )\n+      page_idx = start_page_idx + i\n+      page_idx = jax.lax.select(page_idx < end_page_idx, page_idx, 0)\n       self._async_copies.append(\n           pltpu.make_async_copy(\n               pages_hbm_ref.at[page_indices_ref[seq_id, page_idx]],\n@@ -298,6 +295,7 @@ def ragged_paged_attention_kernel(\n   if mask_value is None:\n     mask_value = DEFAULT_MASK_VALUE\n   num_q_per_blk, num_q_heads_per_blk, head_dim = q_ref.shape\n+  pages_per_seq = page_indices_ref.shape[-1]\n   num_seqs = num_seqs_ref[0]\n   _, num_kv_pages_per_blk, page_size, num_combined_kv_heads_per_blk, _ = (\n       kv_bufs.shape\n@@ -318,7 +316,11 @@ def ragged_paged_attention_kernel(\n   def create_kv_async_copy_descriptors(\n       heads_blk_idx, seq_idx, kv_blk_idx, buf_idx\n   ):\n-    offset = (seq_idx, kv_blk_idx * num_kv_pages_per_blk)\n+    start_kv_page_idx = kv_blk_idx * num_kv_pages_per_blk\n+    end_kv_page_idx = jnp.minimum(\n+        pages_per_seq, cdiv(kv_lens_ref[seq_idx], page_size)\n+    )\n+    metadata = (seq_idx, start_kv_page_idx, end_kv_page_idx)\n     heads_start = heads_blk_idx * num_combined_kv_heads_per_blk\n     async_copy_kv = MultiPageAsyncCopyDescriptor(\n         kv_pages_hbm_ref.at[\n@@ -327,7 +329,7 @@ def create_kv_async_copy_descriptors(\n         kv_bufs.at[buf_idx],\n         sems.at[buf_idx],\n         page_indices_ref,\n-        offset,\n+        metadata,\n     )\n     return async_copy_kv\n \n@@ -423,18 +425,22 @@ def flash_attention(\n           num_q_per_blk * num_q_heads_per_kv_head,\n           head_dim,\n       )\n-      assert k.shape == (\n-          num_kv_per_blk,\n-          head_dim,\n-      ), f\"{k.shape=}, {(num_kv_per_blk, head_dim)=} {k.dtype=}\"\n-      assert v.shape == (num_kv_per_blk, head_dim)\n-      assert head_m_ref.shape == (\n-          num_q_per_blk * num_q_heads_per_kv_head,\n-          128,\n+      assert (\n+          k.shape\n+          == v.shape\n+          == (\n+              num_kv_per_blk,\n+              head_dim,\n+          )\n       )\n-      assert head_l_ref.shape == (\n-          num_q_per_blk * num_q_heads_per_kv_head,\n-          128,\n+      assert k.dtype == v.dtype\n+      assert (\n+          head_m_ref.shape\n+          == head_l_ref.shape\n+          == (\n+              num_q_per_blk * num_q_heads_per_kv_head,\n+              128,\n+          )\n       )\n       assert head_acc_ref.shape == (\n           num_q_per_blk,\n@@ -448,6 +454,13 @@ def masked_store(ref, val, start, end, group=1):\n         mask = jnp.logical_and(iota >= start, iota < end)\n         pl.store(ref, idx=tuple(slice(None) for _ in ref.shape), val=val, mask=mask)\n \n+      # kv lens will be contracting dim, we should mask out the NaNs.\n+      kv_mask = (\n+          lax.broadcasted_iota(jnp.int32, k.shape, 0) < kv_len - kv_len_start\n+      )\n+      k = jnp.where(kv_mask, k.astype(jnp.float32), 0).astype(k.dtype)\n+      v = jnp.where(kv_mask, v.astype(jnp.float32), 0).astype(v.dtype)\n+\n       qk = (\n           jnp.einsum(\"nd,md->nm\", q, k, preferred_element_type=jnp.float32)\n           * sm_scale\n@@ -709,7 +722,7 @@ def ragged_paged_attention(\n \n   Args:\n     q: concatenated all sequences' queries.\n-    kv_pages: paged K cache. Normally in HBM.\n+    kv_pages: paged KV cache. Normally in HBM.\n     kv_lens: padded kv lengths. Only the first num_seqs values are valid.\n     page_indices: the first index indicates which page to use in the kv cache\n       for each sequence. Only the first num_seqs values are valid.\ndiff --git a/tests/pallas/tpu_ragged_paged_attention_test.py b/tests/pallas/tpu_ragged_paged_attention_test.py\nindex f86d54575519..4265445c69c7 100644\n--- a/tests/pallas/tpu_ragged_paged_attention_test.py\n+++ b/tests/pallas/tpu_ragged_paged_attention_test.py\n@@ -19,6 +19,7 @@\n import jax\n from jax._src import test_util as jtu\n from jax.experimental.pallas.ops.tpu.ragged_paged_attention import (\n+    cdiv,\n     dynamic_validate_inputs,\n     ragged_paged_attention,\n     ref_ragged_paged_attention,\n@@ -29,13 +30,8 @@\n jax.config.parse_flags_with_absl()\n \n \n-def ceil_div(x, a):\n-  assert a != 0\n-  return (x + a - 1) // a\n-\n-\n @jtu.with_config(jax_numpy_dtype_promotion=\"standard\")\n-class PagedAttentionKernelTest(jtu.JaxTestCase):\n+class RaggedPagedAttentionKernelTest(jtu.JaxTestCase):\n \n   def _test_ragged_paged_attention(\n       self,\n@@ -66,29 +62,56 @@ def _test_ragged_paged_attention(\n     max_num_batched_tokens = max(cu_q_lens[-1], max_num_batched_tokens)\n     max_num_seq = max(len(seq_lens), max_num_seq)\n     max_kv_len = max(kv_lens)\n-    pages_per_seq = ceil_div(max_kv_len, page_size)\n+    pages_per_seq = cdiv(max_kv_len, page_size)\n     num_q_heads, num_kv_heads = num_heads\n \n-    cu_q_lens = jnp.array(cu_q_lens, dtype=jnp.int32)\n-    kv_lens = jnp.array(kv_lens, dtype=jnp.int32)\n-    cu_q_lens = jnp.pad(cu_q_lens, (0, max_num_seq + 1 - cu_q_lens.shape[0]))\n-    kv_lens = jnp.pad(kv_lens, (0, max_num_seq - kv_lens.shape[0]))\n     prng_key = jax.random.key(1234)\n-    k0, k1, k2 = jax.random.split(prng_key, 3)\n+    k0, k1 = jax.random.split(prng_key, 2)\n     q = jax.random.normal(\n         k0,\n         (max_num_batched_tokens, num_q_heads, head_dim),\n         dtype=dtype,\n     )\n-    kv_pages = jax.random.normal(\n-        k1,\n-        (num_pages, page_size, num_kv_heads * 2, head_dim),\n-        dtype=dtype,\n+    page_cnt = 0\n+    page_indices_list = []\n+    kv_pages_list = []\n+    for kv_len in kv_lens:\n+      kv = jax.random.normal(\n+          k1,\n+          (kv_len, num_kv_heads * 2, head_dim),\n+          dtype=dtype,\n+      )\n+      kv = jnp.pad(\n+          kv,\n+          ((0, cdiv(kv_len, page_size) * page_size - kv_len), (0, 0), (0, 0)),\n+          constant_values=jnp.nan,\n+      ).reshape(-1, page_size, num_kv_heads * 2, head_dim)\n+      indices = page_cnt + jnp.arange(kv.shape[0], dtype=jnp.int32)\n+      indices = jnp.pad(\n+          indices,\n+          ((0, pages_per_seq - indices.shape[0]),),\n+          constant_values=jnp.nan,\n+      )\n+      page_indices_list.append(indices)\n+      page_cnt += kv.shape[0]\n+      kv_pages_list.append(kv)\n+\n+    kv_pages = jnp.concatenate(kv_pages_list, axis=0)\n+    kv_pages = jnp.pad(\n+        kv_pages,\n+        ((0, num_pages - kv_pages.shape[0]), (0, 0), (0, 0), (0, 0)),\n+        constant_values=jnp.nan,\n     )\n-    page_indices = jax.random.randint(\n-        k2, (max_num_seq, pages_per_seq), 0, num_pages, dtype=jnp.int32\n+    page_indices = jnp.stack(page_indices_list, axis=0)\n+    page_indices = jnp.pad(\n+        page_indices,\n+        ((0, max_num_seq - page_indices.shape[0]), (0, 0)),\n+        constant_values=jnp.nan,\n     )\n-\n+    cu_q_lens = jnp.array(cu_q_lens, dtype=jnp.int32)\n+    cu_q_lens = jnp.pad(cu_q_lens, (0, max_num_seq + 1 - cu_q_lens.shape[0]))\n+    kv_lens = jnp.array(kv_lens, dtype=jnp.int32)\n+    kv_lens = jnp.pad(kv_lens, (0, max_num_seq - kv_lens.shape[0]))\n     num_seqs = jnp.array([len(seq_lens)], dtype=jnp.int32)\n \n     dynamic_validate_inputs(\n\n```",
        "from_id": [
            "bythew3i",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[JAX] Enable tfrt gpu in jax multi platform test\n\nPiperOrigin-RevId: 761204561",
        "output": "```diff\nCommit: 048db94ed914fd656818f21efd70d7356326a893\nDate: 2025-05-20T20:20:38Z\nURL: https://github.com/jax-ml/jax/commit/048db94ed914fd656818f21efd70d7356326a893\nFiles changed: 1\nAdditions: +11, Deletions: -1\ndiff --git a/tests/BUILD b/tests/BUILD\nindex 1b33f292d503..aa777080fd92 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -121,6 +121,10 @@ jax_py_test(\n jax_multiplatform_test(\n     name = \"array_interoperability_test\",\n     srcs = [\"array_interoperability_test.py\"],\n+    disable_configs = [\n+        \"gpu_h100_tfrt\",  # TODO(b/411472145): Re-enable once fixed.\n+        \"gpu_h100x2_tfrt\",\n+    ],\n     enable_backends = [\n         \"cpu\",\n         \"gpu\",\n@@ -128,7 +132,9 @@ jax_multiplatform_test(\n     enable_configs = [\n         \"gpu_h100x2\",\n     ],\n-    tags = [\"multiaccelerator\"],\n+    tags = [\n+        \"multiaccelerator\",\n+    ],\n     deps = py_deps([\n         \"absl/testing\",\n         \"numpy\",\n@@ -1134,6 +1140,10 @@ jax_multiplatform_test(\n jax_multiplatform_test(\n     name = \"pytorch_interoperability_test\",\n     srcs = [\"pytorch_interoperability_test.py\"],\n+    disable_configs = [\n+        \"gpu_h100_tfrt\",  # TODO(b/411472145): Re-enable once fixed.\n+        \"gpu_h100x2_tfrt\",\n+    ],\n     enable_backends = [\n         \"cpu\",\n         \"gpu\",\n\n```",
        "from_id": [
            "sizhit2",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Remove pspec -> names conversion during `shard_map_p.bind` and instead preserve partition specs everywhere internally.\n\n**This is because spec -> names canonicalization gets rid of unreduced axes present on PartitionSpecs and we want to preserve that**. We can thread 2 new parameters called `in_unreduced` and `out_unreduced` and keep `in_names`, `out_names` but that doesn't buy us anything except for more lines added and complexity :)\n\nIt's better to just use pspecs everywhere. It's a net reduction in lines of code too!\n\nPiperOrigin-RevId: 761196531",
        "output": "```diff\nCommit: e896282219481c3c6edbd1334c186c7bfbdbdae6\nDate: 2025-05-20T19:59:40Z\nURL: https://github.com/jax-ml/jax/commit/e896282219481c3c6edbd1334c186c7bfbdbdae6\nFiles changed: 6\nAdditions: +258, Deletions: -278\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex f26b4222b23b..0061c9c63f7b 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -53,6 +53,7 @@\n from jax._src.tree_util import tree_map\n from jax._src.tree_util import tree_unflatten\n from jax._src.typing import Array\n+from jax._src.partition_spec import PartitionSpec as P\n from jax._src.util import (as_hashable_function, split_list, safe_map, safe_zip,\n                            unzip3, weakref_lru_cache, HashableWrapper, foreach)\n \n@@ -958,7 +959,7 @@ def remat_error_check(error, enabled_errors, *vals_in, jaxpr, **params):\n \n def shard_map_error_check(\n     error: Error, enabled_errors, *vals_in,\n-    jaxpr: core.Jaxpr, in_names, out_names, **kwargs\n+    jaxpr: core.Jaxpr, in_specs, out_specs, **kwargs\n ):\n   if (mesh := kwargs.get('mesh')) is None:\n     raise ValueError('Mesh must be provided for shard_map with checkify.')\n@@ -966,7 +967,7 @@ def shard_map_error_check(\n   err_vals, err_tree = jtu.tree_flatten(error)\n   num_error_vals = len(err_vals)\n   # Replicated sharding for in errors.\n-  new_in_names = (*([{}] * num_error_vals), *in_names)\n+  new_in_specs = (*([P()] * num_error_vals), *in_specs)\n   new_vals_in = [*err_vals, *vals_in]\n   in_avals = list(map(core.get_aval, new_vals_in))\n   manual_axes = kwargs.get('manual_axes')\n@@ -974,7 +975,7 @@ def shard_map_error_check(\n   for i, v in enumerate(in_avals):\n     if not (sharder := core.shard_aval_handlers.get(type(v))):\n       raise ValueError(f'Unsupported aval type: {type(v)}')\n-    in_avals[i] = sharder(mesh, manual_axes, check_vma, new_in_names[i], v)\n+    in_avals[i] = sharder(mesh, manual_axes, check_vma, new_in_specs[i], v)\n \n   with (jshmap._extend_axis_env(mesh, manual_axes),\n         mesh_lib.use_abstract_mesh(jshmap._as_manual_mesh(mesh, manual_axes)),  # type: ignore[arg-type]\n@@ -983,7 +984,7 @@ def shard_map_error_check(\n     checked_jaxpr, out_tree, _ = jaxpr_to_checkify_jaxpr(\n         pe.close_jaxpr(jaxpr), enabled_errors, err_tree, *in_avals\n     )\n-  num_out_error_vals = out_tree.num_leaves - len(out_names)\n+  num_out_error_vals = out_tree.num_leaves - len(out_specs)\n \n   def expand_errors_leading_dim(*xs):\n     outs = core.eval_jaxpr(checked_jaxpr.jaxpr, checked_jaxpr.consts, *xs)\n@@ -1001,15 +1002,15 @@ def expand_errors_leading_dim(*xs):\n \n   # Update shard_map params to account for extra error values.\n   # Use fully sharded partitioning for out errors.\n-  new_out_names = (*([{0: mesh.axis_names}] * num_out_error_vals), *out_names)\n+  new_out_specs = (*([P(mesh.axis_names)] * num_out_error_vals), *out_specs)\n   subfun = lu.hashable_partial(\n       lu.wrap_init(core.eval_jaxpr, debug_info=checked_jaxpr.jaxpr.debug_info),\n       checked_jaxpr.jaxpr, checked_jaxpr.consts\n   )\n   new_params = dict(\n       jaxpr=checked_jaxpr.jaxpr,\n-      in_names=new_in_names,\n-      out_names=new_out_names,\n+      in_specs=new_in_specs,\n+      out_specs=new_out_specs,\n       **kwargs,\n   )\n   _, new_params = jshmap.shard_map_p.get_bind_params(new_params)\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex 1ab560fb58c5..8f553ea884d7 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -264,14 +264,12 @@ def get_intermediate_shardings(\n       out.extend((i, source_info) for i in eqn.params['in_shardings'])\n       out.extend((o, source_info) for o in eqn.params['out_shardings'])\n     elif eqn.primitive is shard_map.shard_map_p:\n-      if isinstance(eqn.params['mesh'], AbstractMesh):\n+      mesh = eqn.params['mesh']\n+      if isinstance(mesh, AbstractMesh):\n         continue\n       source_info = SourceInfo(eqn.source_info, eqn.primitive.name)\n-      def _names_to_pspec(names):\n-        ndmin = max(names) + 1 if names else 0\n-        return PartitionSpec(*(names.get(i) for i in range(ndmin)))\n-      out.extend((NamedSharding(eqn.params['mesh'], _names_to_pspec(names)), source_info)\n-                 for names in [*eqn.params['in_names'], *eqn.params['out_names']])\n+      out.extend((NamedSharding(mesh, spec), source_info)\n+                 for spec in [*eqn.params['in_specs'], *eqn.params['out_specs']])\n     elif eqn.primitive is device_put_p:\n       source_info = SourceInfo(eqn.source_info, eqn.primitive.name)\n       out.extend((s, source_info) for s in eqn.params['devices']\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex a7782063491c..4a21fae59e52 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -72,7 +72,8 @@\n     PartitionSpec as P)\n from jax._src.util import (safe_map, safe_zip, partition_list, wrap_name,\n                            tuple_update, tuple_delete, distributed_debug_log,\n-                           unzip2, HashableFunction, weakref_lru_cache)\n+                           unzip2, HashableFunction, weakref_lru_cache,\n+                           tuple_insert)\n from jax._src.state.types import AbstractRef, RefEffect\n \n \n@@ -3339,6 +3340,12 @@ def check_array_xla_sharding_layout_match(\n           \"compiled with. \"\n           f\"Here are {num_mismatch_str}:\\n{str_errors}\")\n \n+def batch_spec(spec, dim, val):\n+  too_short = dim - len(spec)\n+  if too_short > 0:\n+    spec += (None,) * too_short\n+  new_partitions = tuple_insert(spec, dim, val)  # type: ignore\n+  return PartitionSpec(*new_partitions)\n \n def get_array_mapping(pspec: PartitionSpec) -> ArrayMappingOrAutoOrUnspecified:\n   pspec = sharding_impls.prepare_axis_resources(pspec, \"pspec to array_mapping\")\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 10e7e697e706..de01f4c05983 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -77,7 +77,7 @@\n     treedef_children, broadcast_prefix, all_leaves, prefix_errors, keystr,\n     PyTreeDef, none_leaf_registry as none_lr, tree_map, tree_flatten_with_path)\n from jax._src.util import (\n-    HashableFunction, safe_map, safe_zip, wraps, tuple_insert,\n+    HashableFunction, safe_map, safe_zip, wraps,\n     distributed_debug_log, split_list, split_list_checked, weakref_lru_cache,\n     merge_lists, subs_list, fun_name, fun_qual_name)\n from jax._src.attrs import (Box, List, dne_sentinel, jax_setattr, jax_getattr,\n@@ -2190,12 +2190,6 @@ def _pjit_batcher(axis_data, vals_in,\n batching.fancy_primitive_batchers[pjit_p] = _pjit_batcher\n batching.ragged_prop_rules[pjit_p] = batching.ragged_mask_no_op_rule\n \n-def _insert_axis_partitions(spec, dim, val):\n-  too_short = dim - len(spec)\n-  if too_short > 0:\n-    spec += (None,) * too_short\n-  new_partitions = tuple_insert(spec, dim, val)  # type: ignore\n-  return PartitionSpec(*new_partitions)\n \n def _pjit_batcher_for_sharding(\n     s: Sharding | UnspecifiedValue,\n@@ -2209,7 +2203,7 @@ def _pjit_batcher_for_sharding(\n       return s\n     if isinstance(s, NamedSharding) and isinstance(s.mesh, AbstractMesh):\n       return NamedSharding(\n-          s.mesh, _insert_axis_partitions(s.spec, dim, PartitionSpec.UNCONSTRAINED))\n+          s.mesh, pxla.batch_spec(s.spec, dim, PartitionSpec.UNCONSTRAINED))\n     new_op = hlo_s.to_proto().clone()\n     tad = list(new_op.tile_assignment_dimensions)\n     tad.insert(dim, 1)  # type: ignore\n@@ -2221,7 +2215,7 @@ def _pjit_batcher_for_sharding(\n   else:\n     if isinstance(s, NamedSharding) and isinstance(s.mesh, AbstractMesh):\n       return NamedSharding(\n-          s.mesh, _insert_axis_partitions(s.spec, dim, spmd_axis_name))\n+          s.mesh, pxla.batch_spec(s.spec, dim, spmd_axis_name))\n     if isinstance(s, NamedSharding):\n       mesh = s.mesh\n     if mesh is None or mesh.empty:\n@@ -2234,7 +2228,7 @@ def _pjit_batcher_for_sharding(\n           f' manager scope{s!r}')\n     spec = parse_flatten_op_sharding(hlo_s, mesh)[0]\n     return NamedSharding(\n-        mesh, _insert_axis_partitions(spec, dim, spmd_axis_name))\n+        mesh, pxla.batch_spec(spec, dim, spmd_axis_name))\n \n \n def _pjit_jvp(primals_in, tangents_in,\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 010773f74d0c..72e1420b0b2b 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -47,7 +47,8 @@\n from jax._src.lib.mlir.dialects import hlo, sdy\n from jax._src.util import (HashableFunction, HashablePartial, unzip2,\n                            as_hashable_function, memoize, partition_list,\n-                           merge_lists, split_list, subs_list2)\n+                           merge_lists, split_list, subs_list2,\n+                           fun_name as util_fun_name)\n from jax._src.interpreters import batching\n from jax._src.interpreters import mlir\n from jax._src.interpreters import partial_eval as pe\n@@ -209,11 +210,11 @@ def wrapped(*args):\n     dyn_argnums, in_specs_flat = unzip2((i, s) for i, s in enumerate(in_specs_flat)\n                                         if s is not None)\n     fun, args_flat = api_util.argnums_partial(fun, dyn_argnums, args_flat, False)\n-    _check_specs_vs_args(f, mesh, in_tree, in_specs, dyn_argnums, in_specs_flat, args_flat)\n-    in_names_flat = tuple(map(_canonicalize_spec, in_specs_flat))\n+    _check_specs_vs_args(f, mesh, in_tree, in_specs, dyn_argnums, in_specs_flat,\n+                         args_flat)\n \n     @memoize\n-    def out_names_thunk():\n+    def out_specs_thunk():\n       if callable(out_specs):\n         out_specs_ = out_specs()\n         _check_specs(SpecErrorType.out, out_specs_, axis_names)\n@@ -225,15 +226,15 @@ def out_names_thunk():\n       except ValueError:\n         e, *_ = prefix_errors(out_specs_, dummy)\n         raise e('shard_map out_specs') from None\n-      return tuple(map(_canonicalize_spec, out_specs_flat))\n+      return tuple(out_specs_flat)\n \n     if check_vma:\n-      fun = _implicit_pvary_on_output(fun, out_names_thunk)\n+      fun = _implicit_pvary_on_output(fun, out_specs_thunk)\n \n     try:\n       out_flat = shard_map_p.bind(\n-          fun, *args_flat, mesh=mesh, in_names=in_names_flat,\n-          out_names_thunk=out_names_thunk, check_vma=check_vma,\n+          fun, *args_flat, mesh=mesh, in_specs=in_specs_flat,\n+          out_specs_thunk=out_specs_thunk, check_vma=check_vma,\n           manual_axes=axis_names)\n     except _SpecError as e:\n       fails, = e.args\n@@ -305,16 +306,6 @@ def _shmap_checks(mesh, axis_names, in_specs, out_specs, _skip_mesh_check,\n     _check_specs(SpecErrorType.out, out_specs, axis_names)\n   return mesh, axis_names\n \n-\n-# Internally use AxisNames = dict[int, tuple[AxisName, ...]], not PartitionSpecs\n-AxisNames = dict[int, tuple[AxisName, ...]]  # TODO(mattjj): make it hashable\n-def _canonicalize_spec(spec: PartitionSpec) -> AxisNames:\n-  if isinstance(spec, PartitionSpec):\n-    return {i: names if isinstance(names, tuple) else (names,)\n-            for i, names in enumerate(spec) if names is not None}\n-  else:\n-    return spec\n-\n def _manual_spec(manual_axes, spec: P) -> P:\n   out = []  # type: ignore\n   for s in spec:\n@@ -391,7 +382,7 @@ def _check_specs_vs_args(\n     fail = _expand_fail(in_tree, dyn_argnums, fail)\n     msg = _spec_rank_error(SpecErrorType.input, f, in_tree, in_specs, fail)\n     raise ValueError(msg)\n-  in_names_flat = tuple(map(_canonicalize_spec, in_specs_flat))\n+  in_names_flat = tuple(map(_spec_to_names, in_specs_flat))\n   fail = [a if any(a.shape[d] % prod(mesh.shape[n] for n in ns)\n                    for d, ns in names.items()) else no_fail\n           for a, names in zip(in_avals, in_names_flat)]\n@@ -411,7 +402,7 @@ def _expand_fail(in_tree: PyTreeDef, dyn_argnums: Sequence[int],\n def _spec_rank_error(\n     error_type: SpecErrorType, f: Callable, tree: PyTreeDef, specs: Specs,\n     fails: list[core.ShapedArray | NoFail]) -> str:\n-  fun_name = getattr(f, '__name__', str(f))\n+  fun_name = util_fun_name(f)\n   if error_type == SpecErrorType.input:\n     prefix, base = 'in', 'args'\n     ba = _try_infer_args(f, tree)\n@@ -472,7 +463,7 @@ def _spec_divisibility_error(\n         extra = (f\", where args{arg_key} is the index \"\n                  f\"{arg_key.idx - len(ba.signature.parameters) + 1} component \"\n                  f\"of {fun_name}'s varargs parameter '{param.name}',\")\n-    names = _canonicalize_spec(spec)\n+    names = _spec_to_names(spec)\n     for d, ns in names.items():\n       if aval.shape[d] % prod(mesh.shape[n] for n in ns):\n         axis = f\"axes {ns}\" if len(ns) > 1 else f\"axis '{ns[0]}'\"\n@@ -504,8 +495,7 @@ def _inout_vma_error(f: Callable, mesh: Mesh | AbstractMesh, tree: PyTreeDef,\n   fun_name = getattr(f, '__name__', str(f))\n   msgs = []\n   for (spec_key, spec), (fail_key, vma) in _iter_paths(tree, specs, fails):\n-    dst = _canonicalize_spec(spec)\n-    unmentioned = _unmentioned(mesh, dst)\n+    unmentioned = _unmentioned(mesh, spec)\n     if len(unmentioned) > 1:\n       need_vma = ','.join(map(str, order_wrt_mesh(mesh, _spec_to_vma(spec))))\n       got_vma = ','.join(map(str, order_wrt_mesh(mesh, vma)))\n@@ -536,9 +526,9 @@ def _inout_vma_error(f: Callable, mesh: Mesh | AbstractMesh, tree: PyTreeDef,\n          \"check_vma=False argument to `jax.shard_map`.\")\n   return msg\n \n-def _unmentioned(mesh: Mesh | AbstractMesh, names: AxisNames) -> list[AxisName]:\n-  name_set = {n for ns in names.values() for n in ns}\n-  return [n for n in mesh.axis_names if n not in name_set]\n+def _unmentioned(mesh: Mesh | AbstractMesh, spec) -> list[AxisName]:\n+  vma_set = _spec_to_vma(spec)\n+  return [n for n in mesh.axis_names if n not in vma_set]\n \n \n def _try_infer_args(f, tree):\n@@ -563,10 +553,10 @@ def _iter_paths(tree: PyTreeDef, specs: Specs, fails: list[T | NoFail]\n # Primitive\n \n @lu.transformation2\n-def _implicit_pvary_on_output(f, out_names_thunk, *args, **kwargs):\n+def _implicit_pvary_on_output(f, out_specs_thunk, *args, **kwargs):\n   out_flat = f(*args, **kwargs)\n-  return [pvary(o, tuple(_names_to_vma(n) - typeof(o).vma))\n-          for o, n in zip(out_flat, out_names_thunk())]\n+  return [pvary(o, tuple(_spec_to_vma(sp) - typeof(o).vma))\n+          for o, sp in zip(out_flat, out_specs_thunk())]\n \n JaxType = Any\n MaybeTracer = Union[JaxType, Tracer]\n@@ -588,8 +578,8 @@ def get_bind_params(self, params):\n     subfun = lu.hashable_partial(lu.wrap_init(core.eval_jaxpr,\n                                               debug_info=jaxpr.debug_info),\n                                  jaxpr, ())\n-    axes = new_params.pop('out_names')\n-    new_params['out_names_thunk'] = HashableFunction(lambda: axes, closure=axes)\n+    axes = new_params.pop('out_specs')\n+    new_params['out_specs_thunk'] = HashableFunction(lambda: axes, closure=axes)\n     return [subfun], new_params\n \n shard_map_p = ShardMapPrimitive('shard_map')\n@@ -631,38 +621,35 @@ def _extend_axis_env(mesh, manual_axes):\n def _shard_map_staging(\n     trace: pe.DynamicJaxprTrace, prim: core.Primitive, f: lu.WrappedFun,\n     in_tracers: Sequence[Any], *, mesh: Mesh,\n-    in_names: tuple[AxisNames, ...],\n-    out_names_thunk: Callable[[], tuple[AxisNames, ...]],\n-    check_vma: bool,\n-    manual_axes: frozenset,\n+    in_specs, out_specs_thunk, check_vma: bool, manual_axes: frozenset,\n   ) -> Sequence[pe.DynamicJaxprTracer]:\n   source_info = source_info_util.current()\n   to_jaxpr_tracer = partial(trace.to_jaxpr_tracer, source_info=source_info)\n   in_tracers = map(to_jaxpr_tracer, in_tracers)\n   inner_mesh = _as_manual_mesh(mesh, manual_axes | set(mesh.manual_axes))\n   in_avals = [t.aval for t in in_tracers]\n-  in_avals_ = map(partial(_shard_aval, mesh, manual_axes, check_vma), in_names,\n+  in_avals_ = map(partial(_shard_aval, mesh, manual_axes, check_vma), in_specs,\n                   in_avals)\n   with (_extend_axis_env(mesh, manual_axes), use_abstract_mesh(inner_mesh),\n         config._check_vma(check_vma)):\n     jaxpr, out_avals_, consts, () = pe.trace_to_jaxpr_dynamic(f, in_avals_)\n-  _check_names(out_names_thunk(), out_avals_)\n+  _check_names(out_specs_thunk(), out_avals_)\n   if check_vma:\n     out_vma = [v.aval.vma for v in jaxpr.outvars]\n-    _check_vmas(mesh, out_names_thunk(), out_vma)\n+    _check_vmas(mesh, out_specs_thunk(), out_vma)\n   out_avals = map(_check_shapedarray, out_avals_)\n-  out_avals = [_check_shapedarray(_unshard_aval(mesh, check_vma, names, aval))\n-               for names, aval in zip(out_names_thunk(), out_avals)]\n+  out_avals = [_check_shapedarray(_unshard_aval(mesh, check_vma, spec, aval))\n+               for spec, aval in zip(out_specs_thunk(), out_avals)]\n   out_tracers = [pe.DynamicJaxprTracer(trace, a, source_info) for a in out_avals]\n   invars = map(trace.getvar, in_tracers)\n   constvars = map(trace.getvar, map(to_jaxpr_tracer, consts))\n   outvars = map(trace.makevar, out_tracers)\n-  in_names_staged = ({},) * len(consts) + tuple(in_names)  # type: ignore\n+  in_specs_staged = (P(),) * len(consts) + tuple(in_specs)  # type: ignore\n   with (_extend_axis_env(mesh, manual_axes), use_abstract_mesh(inner_mesh),\n         config._check_vma(check_vma)):\n     jaxpr = pe.convert_constvars_jaxpr(jaxpr)\n-  params = dict(mesh=mesh, in_names=in_names_staged,\n-                out_names=tuple(out_names_thunk()), jaxpr=jaxpr,\n+  params = dict(mesh=mesh, in_specs=in_specs_staged,\n+                out_specs=tuple(out_specs_thunk()), jaxpr=jaxpr,\n                 check_vma=check_vma, manual_axes=manual_axes)\n   effs = core.filter_named_axis_effects(jaxpr.effects, mesh.axis_names)\n   eqn = pe.new_jaxpr_eqn([*constvars, *invars], outvars, prim, params,\n@@ -673,44 +660,48 @@ def _shard_map_staging(\n \n # TODO add underscore version, for direct-linearize to consume\n \n+def _spec_to_names(spec: PartitionSpec):\n+  return {i: names if isinstance(names, tuple) else (names,)\n+          for i, names in enumerate(spec) if names is not None}\n+\n def _check_shapedarray(aval: core.AbstractValue) -> core.ShapedArray:\n   assert isinstance(aval, core.ShapedArray)\n   return aval\n \n-def _shard_aval(mesh: Mesh, manual_axes, check_vma, names: AxisNames,\n+def _shard_aval(mesh: Mesh, manual_axes, check_vma, spec,\n                 aval: core.AbstractValue) -> core.AbstractValue:\n   if type(aval) in core.shard_aval_handlers:\n     return core.shard_aval_handlers[type(aval)](mesh, manual_axes, check_vma,\n-                                                names, aval)\n+                                                spec, aval)\n   raise NotImplementedError(f\"Unsupported aval type: {type(aval)}\")\n \n-def _unshard_aval(mesh: Mesh, check_vma, names: AxisNames,\n+def _unshard_aval(mesh: Mesh, check_vma, spec,\n                   aval: core.AbstractValue) -> core.AbstractValue:\n   if type(aval) in core.unshard_aval_handlers:\n-    return core.unshard_aval_handlers[type(aval)](mesh, check_vma, names, aval)\n+    return core.unshard_aval_handlers[type(aval)](mesh, check_vma, spec, aval)\n   else:\n     raise NotImplementedError(f\"Unsupported aval type: {type(aval)}\")\n \n def _shard_shaped_array(mesh: Mesh, manual_axes: frozenset, check_vma,\n-                        names: AxisNames, aval: core.AbstractValue\n-                        ) -> core.AbstractValue:\n+                        spec, aval: core.AbstractValue) -> core.AbstractValue:\n   assert isinstance(aval, core.ShapedArray)\n+  names = _spec_to_names(spec)\n   new_shape = tuple(sz // prod(mesh.shape[n] for n in names.get(i, ()))\n                     for i, sz in enumerate(aval.shape))\n   manual_mesh = _as_manual_mesh(mesh, manual_axes | set(mesh.manual_axes))\n   new_sharding = NamedSharding(manual_mesh, aval.sharding.spec)\n-  vma = (frozenset({n for ns in names.values() for n in ns})\n-         if check_vma else frozenset())\n+  vma = _spec_to_vma(spec) if check_vma else frozenset()\n   vma = vma | aval.vma\n   return aval.update(shape=new_shape, sharding=new_sharding, vma=vma)\n core.shard_aval_handlers[core.ShapedArray] = _shard_shaped_array\n \n-def _unshard_shaped_array(mesh: Mesh, check_vma, names: AxisNames,\n-                          aval: core.AbstractValue,) -> core.AbstractValue:\n+def _unshard_shaped_array(mesh: Mesh, check_vma, spec, aval: core.AbstractValue\n+                          ) -> core.AbstractValue:\n   assert isinstance(aval, core.ShapedArray)\n+  names = _spec_to_names(spec)\n   new_shape = tuple(sz * prod(mesh.shape[n] for n in names.get(i, ()))\n                     for i, sz in enumerate(aval.shape))\n-  names_spec = _names_to_pspec(names)._normalized_spec_for_aval(aval.ndim)\n+  names_spec = spec._normalized_spec_for_aval(aval.ndim)\n   if aval.ndim == 0:\n     out_spec = P()\n   else:\n@@ -739,32 +730,32 @@ def _unshard_shaped_array(mesh: Mesh, check_vma, names: AxisNames,\n \n # Type-checking\n \n-def _shard_map_typecheck(_, *in_atoms, jaxpr, mesh, in_names, out_names,\n+def _shard_map_typecheck(_, *in_atoms, jaxpr, mesh, in_specs, out_specs,\n                          check_vma, manual_axes):\n   # TODO(mattjj,parkers): check auto\n-  for v, x, in_name in zip(jaxpr.invars, in_atoms, in_names):\n+  for v, x, in_spec in zip(jaxpr.invars, in_atoms, in_specs):\n     if not core.typecompat(v.aval, _shard_aval(\n-        mesh, manual_axes, check_vma, in_name, x.aval)):\n+        mesh, manual_axes, check_vma, in_spec, x.aval)):\n       raise core.JaxprTypeError(\"shard_map argument avals not compatible with \"\n-                                \"jaxpr binder avals and in_names\")\n+                                \"jaxpr binder avals and in_specs\")\n   with _extend_axis_env(mesh, manual_axes), config._check_vma(check_vma):\n     core.check_jaxpr(jaxpr)\n   if check_vma:\n     out_vma = [v.aval.vma for v in jaxpr.outvars]\n-    for vma, dst in zip(out_vma, out_names):\n-      if not _valid_repeats(mesh, vma, dst):\n+    for vma, out_spec in zip(out_vma, out_specs):\n+      if not _valid_repeats(mesh, vma, out_spec):\n         raise core.JaxprTypeError(\n             \"shard_map can't prove output is sufficiently replicated\")\n   out_avals_sharded = [x.aval for x in jaxpr.outvars]\n-  out_avals = map(partial(_unshard_aval, mesh, check_vma), out_names,\n+  out_avals = map(partial(_unshard_aval, mesh, check_vma), out_specs,\n                   out_avals_sharded)\n   effs = core.filter_named_axis_effects(jaxpr.effects, mesh.axis_names)\n   return out_avals, effs\n core.custom_typechecks[shard_map_p] = _shard_map_typecheck\n \n \n-def _valid_repeats(mesh: Mesh, vma: Set[AxisName], names: AxisNames) -> bool:\n-  um = set(_unmentioned(mesh, names)) - set(mesh.manual_axes)\n+def _valid_repeats(mesh: Mesh, vma: Set[AxisName], spec) -> bool:\n+  um = set(_unmentioned(mesh, spec)) - set(mesh.manual_axes)\n   if any(u in vma for u in um):\n     return False\n   return True\n@@ -772,10 +763,9 @@ def _valid_repeats(mesh: Mesh, vma: Set[AxisName], names: AxisNames) -> bool:\n # Lowering\n \n def _shardy_shard_map_sharding(\n-    ctx: mlir.LoweringRuleContext, mesh, manual_axes, names, aval_in\n+    ctx: mlir.LoweringRuleContext, mesh, manual_axes, spec, aval_in\n ) -> sharding_impls.SdyArray:\n-  axes = {name: i for i, ns in names.items() for name in ns}\n-  ns = _make_scoped_manual_sharding(ctx, mesh, axes)\n+  ns = _make_scoped_manual_sharding(ctx, mesh, spec)\n   if dtypes.issubdtype(aval_in.dtype, dtypes.extended):\n     ns = sharding_impls.physical_sharding(aval_in, ns)\n     aval_in = core.physical_aval(aval_in)\n@@ -789,12 +779,12 @@ def _shardy_shard_map_sharding(\n def _shardy_shard_map_token_sharding(\n     ctx: mlir.LoweringRuleContext, mesh\n   ) -> ir.Attribute:\n-  ns = _make_scoped_manual_sharding(ctx, mesh, {})\n+  ns = _make_scoped_manual_sharding(ctx, mesh, P())\n   return ns._to_sdy_sharding(0)\n \n \n def _shard_map_lowering_shardy(\n-    ctx, in_nodes, jaxpr, mesh, in_names, out_names, manual_axes, check_vma):\n+    ctx, in_nodes, jaxpr, mesh, in_specs, out_specs, manual_axes, check_vma):\n   axis_ctx = ctx.module_context.axis_context\n   in_avals_ = [v.aval for v in jaxpr.invars]\n   if isinstance(axis_ctx, sharding_impls.SPMDAxisContext):\n@@ -820,17 +810,17 @@ def _shard_map_lowering_shardy(\n       ctx.set_tokens_out(tokens_out)\n     return out_nodes\n \n-  in_shardings = list(map(\n-      partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n-      in_names, ctx.avals_in))\n+  in_shardings = list(\n+      map(partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n+          in_specs, ctx.avals_in))\n   num_dim_vars = len(ctx.dim_var_values)\n   in_shardings = ([_shardy_shard_map_token_sharding(ctx, mesh)]\n                   * (num_tokens + num_dim_vars) + in_shardings)\n   in_shardings = sharding_impls.SdyArrayList(in_shardings).build()\n \n-  out_shardings = list(map(\n-      partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n-      out_names, ctx.avals_out))\n+  out_shardings = list(\n+      map(partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n+          out_specs, ctx.avals_out))\n   out_shardings = [\n       _shardy_shard_map_token_sharding(ctx, mesh)] * num_tokens + out_shardings\n   out_shardings = sharding_impls.SdyArrayList(out_shardings).build()\n@@ -868,15 +858,15 @@ def _shard_map_lowering_shardy(\n   return manual_computation_op.results[num_tokens:]\n \n \n-def _shard_map_lowering(ctx, *in_nodes, jaxpr, mesh, in_names, out_names,\n+def _shard_map_lowering(ctx, *in_nodes, jaxpr, mesh, in_specs, out_specs,\n                         check_vma, manual_axes):\n   if config.use_shardy_partitioner.value:\n     return _shard_map_lowering_shardy(\n-        ctx, in_nodes, jaxpr, mesh, in_names, out_names, manual_axes, check_vma)\n+        ctx, in_nodes, jaxpr, mesh, in_specs, out_specs, manual_axes, check_vma)\n \n   in_avals_ = [v.aval for v in jaxpr.invars]\n   out_avals_ = [x.aval for x in jaxpr.outvars]\n-  in_nodes_ = map(partial(_xla_shard, ctx, mesh, manual_axes), in_names,\n+  in_nodes_ = map(partial(_xla_shard, ctx, mesh, manual_axes), in_specs,\n                   ctx.avals_in, in_avals_, in_nodes)\n   new_axis_context = sharding_impls.SPMDAxisContext(mesh, manual_axes)\n   sub_ctx = ctx.module_context.replace(axis_context=new_axis_context)\n@@ -885,28 +875,26 @@ def _shard_map_lowering(ctx, *in_nodes, jaxpr, mesh, in_names, out_names,\n         \"shmap_body\", ctx.name_stack, jaxpr, None, sub_ctx, in_avals_,\n         out_avals_, ctx.tokens_in, *in_nodes_,\n         dim_var_values=ctx.dim_var_values,\n-        arg_names=map(_pspec_mhlo_attrs, in_names, in_avals_),\n-        result_names=map(_pspec_mhlo_attrs, out_names, out_avals_))\n+        arg_names=map(_pspec_mhlo_attrs, in_specs, in_avals_),\n+        result_names=map(_pspec_mhlo_attrs, out_specs, out_avals_))\n   ctx.set_tokens_out(tokens_out)\n-  return map(partial(_xla_unshard, ctx, mesh, manual_axes), out_names,\n+  return map(partial(_xla_unshard, ctx, mesh, manual_axes), out_specs,\n              out_avals_, ctx.avals_out, out_nodes_)\n mlir.register_lowering(shard_map_p, _shard_map_lowering)\n \n-def _make_scoped_manual_sharding(ctx, mesh, axes):\n+def _make_scoped_manual_sharding(ctx, mesh, spec):\n   axis_ctx = ctx.module_context.axis_context\n   mesh = mesh.abstract_mesh\n   if isinstance(axis_ctx, sharding_impls.SPMDAxisContext):\n     mesh = mesh.update_axis_types(\n         {a: AxisType.Manual for a in axis_ctx.manual_axes})\n-  return NamedSharding(\n-      mesh, sharding_impls.array_mapping_to_axis_resources(axes))  # type: ignore\n+  return NamedSharding(mesh, spec)\n \n-def _xla_shard(ctx: mlir.LoweringRuleContext, mesh, manual_axes, names,\n+def _xla_shard(ctx: mlir.LoweringRuleContext, mesh, manual_axes, spec,\n                aval_in, aval_out, x):\n   if prod([size for n, size in mesh.shape.items() if n in manual_axes]) == 1:\n     return x\n-  axes = {name: i for i, ns in names.items() for name in ns}\n-  ns = _make_scoped_manual_sharding(ctx, mesh, axes)\n+  ns = _make_scoped_manual_sharding(ctx, mesh, spec)\n   if dtypes.issubdtype(aval_in.dtype, dtypes.extended):\n     ns = sharding_impls.physical_sharding(aval_in, ns)\n     aval_in = core.physical_aval(aval_in)\n@@ -920,12 +908,11 @@ def _xla_shard(ctx: mlir.LoweringRuleContext, mesh, manual_axes, names,\n   return mlir.wrap_with_full_to_shard_op(ctx, sx, aval_out, manual_proto,\n                                          unspecified)\n \n-def _xla_unshard(ctx: mlir.LoweringRuleContext, mesh, manual_axes, names,\n+def _xla_unshard(ctx: mlir.LoweringRuleContext, mesh, manual_axes, spec,\n                  aval_in, aval_out, x):\n   if prod([size for n, size in mesh.shape.items() if n in manual_axes]) == 1:\n     return x\n-  axes = {name: i for i, ns in names.items() for name in ns}\n-  ns = _make_scoped_manual_sharding(ctx, mesh, axes)\n+  ns = _make_scoped_manual_sharding(ctx, mesh, spec)\n   if dtypes.issubdtype(aval_out.dtype, dtypes.extended):\n     ns = sharding_impls.physical_sharding(aval_out, ns)\n     aval_out = core.physical_aval(aval_out)\n@@ -941,8 +928,9 @@ def _xla_unshard(ctx: mlir.LoweringRuleContext, mesh, manual_axes, names,\n   return mlir.wrap_with_shard_to_full_op(ctx, sx, aval_out, shard_proto,\n                                          unspecified)\n \n-def _pspec_mhlo_attrs(names: AxisNames, aval: core.AbstractValue) -> str:\n+def _pspec_mhlo_attrs(spec, aval: core.AbstractValue) -> str:\n   if isinstance(aval, core.ShapedArray):\n+    names = _spec_to_names(spec)\n     return str(map(names.get, range(aval.ndim)))\n   return ''\n \n@@ -969,15 +957,13 @@ def _vma_to_spec(mesh, vma):\n   return P(order_wrt_mesh(mesh, vma))\n \n def _spec_to_vma(spec):\n-  return _names_to_vma(_canonicalize_spec(spec))\n-\n-def _names_to_vma(names):\n-  return {n for ns in names.values() for n in ns}\n+  return frozenset(p for s in spec if s is not None\n+                   for p in (s if isinstance(s, tuple) else (s,)))\n \n def order_wrt_mesh(mesh, x):\n   return tuple(a for a in mesh.axis_names if a in x)\n \n-def _shard_map_impl(trace, prim, fun, args, *, mesh, in_names, out_names_thunk,\n+def _shard_map_impl(trace, prim, fun, args, *, mesh, in_specs, out_specs_thunk,\n                     check_vma, manual_axes):\n   if len(manual_axes) < len(mesh.axis_names):\n     raise NotImplementedError\n@@ -988,18 +974,18 @@ def _shard_map_impl(trace, prim, fun, args, *, mesh, in_names, out_names_thunk,\n     mesh = get_mesh_from_args(args, mesh)\n   cur_mesh = get_abstract_mesh()\n   args = map(partial(_unmatch_spec, mesh, check_vma, context_mesh=cur_mesh),\n-             in_names, args)\n-  in_vma = map(_names_to_vma, in_names)\n+             in_specs, args)\n+  in_vma = map(_spec_to_vma, in_specs)\n   outs, out_vma = _run_shmap(fun, mesh, manual_axes, args, in_vma, check_vma,\n                              cur_mesh)\n   out_avals = [core.mapped_aval(x.shape[0], 0, core.get_aval(x)) for x in outs]\n-  _check_names(out_names_thunk(), out_avals)  # pytype: disable=wrong-arg-types\n+  _check_names(out_specs_thunk(), out_avals)  # pytype: disable=wrong-arg-types\n   if check_vma:\n-    _check_vmas(mesh, out_names_thunk(), out_vma)\n+    _check_vmas(mesh, out_specs_thunk(), out_vma)\n     src_pspecs = tuple(_vma_to_spec(mesh, r) for r in out_vma)\n   else:\n     src_pspecs = tuple(P(mesh.axis_names) for _ in out_vma)\n-  dst_pspecs = map(_names_to_pspec, out_names_thunk())\n+  dst_pspecs = out_specs_thunk()\n   return map(partial(_match_spec, mesh, check_vma), src_pspecs, dst_pspecs,\n              outs)\n core.EvalTrace.process_shard_map = _shard_map_impl\n@@ -1014,42 +1000,35 @@ def _run_shmap(f, mesh, manual_axes, args, vmas, check_vma, context_mesh):\n     outs, out_vma = unzip2(map(trace.to_val_vma_pair, ans))\n   return outs, out_vma\n \n-def _names_to_pspec(names: AxisNames) -> PartitionSpec:\n-  ndmin = max(names) + 1 if names else 0\n-  unpack = lambda t: t[0] if t is not None and len(t) == 1 else t\n-  return PartitionSpec(*(unpack(names.get(i)) for i in range(ndmin)))\n \n-def _unmatch_spec(mesh: Mesh, check_vma, src: AxisNames, x: JaxType,\n-                  context_mesh) -> JaxType:\n+def _unmatch_spec(mesh: Mesh, check_vma, in_spec, x: JaxType, context_mesh\n+                  ) -> JaxType:\n   with (core.eval_context(), jax.disable_jit(False),\n         use_abstract_mesh(context_mesh)):\n-    return jax.jit(HashablePartial(_unmatch, mesh, check_vma,\n-                                   tuple(src.items())))(x)\n+    return jax.jit(HashablePartial(_unmatch, mesh, check_vma, in_spec))(x)\n \n-def _unmatch(mesh, check_vma, src_tup, x):\n-  src = _names_to_pspec(dict(src_tup))\n+def _unmatch(mesh, check_vma, in_spec, x):\n   if check_vma:\n-    used_axes = {i for _, ns in src_tup for i in ns}\n+    used_axes = _spec_to_vma(in_spec)\n     dst = P(order_wrt_mesh(mesh, used_axes))\n   else:\n     dst = P(mesh.axis_names)\n     check_vma = False\n-  return shard_map(_add_singleton, mesh=mesh, in_specs=(src,), out_specs=dst,\n-                   check_vma=check_vma)(x)\n+  return shard_map(_add_singleton, mesh=mesh, in_specs=(in_spec,),\n+                   out_specs=dst, check_vma=check_vma)(x)\n \n-def _check_names(names: Sequence[AxisNames], avals: Sequence[core.ShapedArray]\n-                 ) -> None:\n-  fail = [a if n and not max(n) < a.ndim else no_fail\n-          for n, a in zip(names, avals)]\n+def _check_names(specs, avals: Sequence[core.ShapedArray]) -> None:\n+  fail = [a if sp and len(sp) > a.ndim else no_fail\n+          for sp, a in zip(specs, avals)]\n   if any(f is not no_fail for f in fail):\n     raise _SpecError(fail)\n \n class _SpecError(Exception):\n   pass\n \n-def _check_vmas(mesh, names, vmas):\n-  fail = [vma if not _valid_repeats(mesh, vma, n) else no_fail\n-          for n, vma in zip(names, vmas)]\n+def _check_vmas(mesh, specs, vmas):\n+  fail = [vma if not _valid_repeats(mesh, vma, sp) else no_fail\n+          for sp, vma in zip(specs, vmas)]\n   if any(f is not no_fail for f in fail):\n     raise _RepError(fail)\n \n@@ -1099,7 +1078,7 @@ def to_val_vma_pair(self, val):\n     elif isinstance(val, Tracer):\n       raise Exception(f\"Shouldn't have any non-shard_map tracers: {val}\")\n     else:\n-      val_ = _unmatch_spec(self.mesh, self.check, {}, val, self.context_mesh)\n+      val_ = _unmatch_spec(self.mesh, self.check, P(), val, self.context_mesh)\n       return val_, frozenset()\n \n   def process_primitive(self, prim, tracers, params):\n@@ -1205,6 +1184,7 @@ def __str__(self) -> str:\n     return '\\n'.join(\n         f\"On {device} at mesh coordinates {axis_names} = {idx}:\\n{block}\\n\"\n         for (idx, device), block in zip(np.ndenumerate(mesh.devices), blocks))\n+\n   __repr__ = __str__  # for debuggers, like `p x`\n \n def _prim_applier(prim, check_vma, params_tup, mesh, in_specs, out_specs, *args):\n@@ -1251,34 +1231,33 @@ def _device_put_eager_rule(mesh, *xs, srcs, devices, copy_semantics):\n def _shard_map_batch(\n     trace: batching.BatchTrace, prim: core.Primitive, fun: lu.WrappedFun,\n     in_tracers: Sequence[batching.BatchTracer], mesh: Mesh,\n-    in_names: tuple[AxisNames, ...],\n-    out_names_thunk: Callable[[], tuple[AxisNames, ...]],\n-    check_vma: bool,\n-    manual_axes: frozenset) -> Sequence[batching.BatchTracer]:\n+    in_specs, out_specs_thunk, check_vma: bool, manual_axes: frozenset\n+    ) -> Sequence[batching.BatchTracer]:\n   in_vals, in_dims = unzip2(map(trace.to_batch_info, in_tracers))\n   if any(isinstance(d, batching.RaggedAxis) for d in in_dims):\n     raise NotImplementedError\n-  new_in_names = [{ax + (d is not batching.not_mapped and d <= ax): names[ax]\n-                   for ax in names} for names, d in zip(in_names, in_dims)]\n   spmd_axis_name = trace.axis_data.spmd_name\n   if spmd_axis_name is not None:\n-    used = {n for names in in_names for ns in names.values() for n in ns}\n+    used = {n for spec in in_specs for n in _spec_to_vma(spec)}\n     if not config.disable_vmap_shmap_error.value and set(spmd_axis_name) & used:\n       raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map in_specs\")\n-    new_in_names = [{**ns, d:spmd_axis_name} if d is not batching.not_mapped\n-                    else ns for ns, d in zip(new_in_names, in_dims)]\n+    new_in_specs = [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, spmd_axis_name)\n+                    for sp, d in zip(in_specs, in_dims)]\n     new_size = trace.axis_data.size // prod(mesh.shape[n] for n in spmd_axis_name)\n     new_axis_data = batching.AxisData(trace.axis_data.name, new_size,\n                                       trace.axis_data.spmd_name, None)\n   else:\n+    new_in_specs = [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, None)\n+                    for sp, d in zip(in_specs, in_dims)]\n     new_axis_data = trace.axis_data\n   fun, out_dims = batching.batch_subtrace(fun, trace.tag, new_axis_data, tuple(in_dims))\n-  @as_hashable_function(closure=out_names_thunk)\n-  def new_out_names_thunk():\n-    return _batch_out_names(spmd_axis_name, out_dims(), out_names_thunk())\n \n-  new_params = dict(mesh=mesh, in_names=new_in_names,\n-                    out_names_thunk=new_out_names_thunk, check_vma=check_vma,\n+  @as_hashable_function(closure=out_specs_thunk)\n+  def new_out_specs_thunk():\n+    return _batch_out_specs(spmd_axis_name, out_dims(), out_specs_thunk())\n+\n+  new_params = dict(mesh=mesh, in_specs=new_in_specs,\n+                    out_specs_thunk=new_out_specs_thunk, check_vma=check_vma,\n                     manual_axes=manual_axes)\n   with core.set_current_trace(trace.parent_trace):\n     out_vals = prim.bind(fun, *in_vals, **new_params)\n@@ -1287,36 +1266,36 @@ def new_out_names_thunk():\n   return map(make_tracer, out_vals, out_dims())\n batching.BatchTrace.process_shard_map = _shard_map_batch\n \n-def _batch_out_names(spmd_axis_name, dims, out_names):\n-  out_names_ = [{ax + (d is not batching.not_mapped and d <= ax): names[ax]\n-                  for ax in names} for names, d in zip(out_names, dims)]\n-  if spmd_axis_name is not None:\n-    used = {n for names in out_names for ns in names.values() for n in ns}\n-    if not config.disable_vmap_shmap_error.value and set(spmd_axis_name) & used:\n+def _batch_out_specs(spmd_name, dims, out_specs):\n+  if spmd_name is not None:\n+    used = {n for spec in out_specs for n in _spec_to_vma(spec)}\n+    if not config.disable_vmap_shmap_error.value and set(spmd_name) & used:\n       raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map out_specs\")\n-    out_names_ = [{**ns, d:spmd_axis_name} if d is not batching.not_mapped\n-                  else ns for ns, d in zip(out_names_, dims)]\n-  return out_names_\n+    return [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, spmd_name)\n+            for sp, d in zip(out_specs, dims)]\n+  else:\n+    return [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, None)\n+            for sp, d in zip(out_specs, dims)]\n \n \n # Autodiff\n \n-def _shard_map_jvp(trace, shard_map_p, f, tracers, mesh, in_names,\n-                   out_names_thunk, check_vma, manual_axes):\n+def _shard_map_jvp(trace, shard_map_p, f, tracers, mesh, in_specs,\n+                   out_specs_thunk, check_vma, manual_axes):\n   primals, tangents = unzip2(map(trace.to_primal_tangent_pair, tracers))\n   which_nz = [     type(t) is not ad.Zero           for t in tangents]\n   tangents = [t if type(t) is not ad.Zero else None for t in tangents]\n   args, in_tree = tree_flatten((primals, tangents))\n   f_jvp = ad.jvp_subtrace(f, trace.tag)\n   f_jvp, which_nz_out = ad.nonzero_tangent_outputs(f_jvp)\n-  tangent_in_names = [ax for ax, nz in zip(in_names, which_nz) if nz]\n+  tangent_in_specs = [sp for sp, nz in zip(in_specs, which_nz) if nz]\n \n-  @as_hashable_function(closure=out_names_thunk)\n-  def new_out_names_thunk():\n-    out_ax = out_names_thunk()\n+  @as_hashable_function(closure=out_specs_thunk)\n+  def new_out_specs_thunk():\n+    out_ax = out_specs_thunk()\n     return (*out_ax, *(ax for ax, nz in zip(out_ax, which_nz_out()) if nz))\n-  params = dict(mesh=mesh, in_names=(*in_names, *tangent_in_names),\n-                out_names_thunk=new_out_names_thunk, check_vma=check_vma,\n+  params = dict(mesh=mesh, in_specs=(*in_specs, *tangent_in_specs),\n+                out_specs_thunk=new_out_specs_thunk, check_vma=check_vma,\n                 manual_axes=manual_axes)\n   f_jvp, out_tree = ad.traceable(f_jvp, in_tree)\n   result = shard_map_p.bind_with_trace(trace.parent_trace, (f_jvp,) + tuple(args), params)\n@@ -1327,32 +1306,32 @@ def new_out_names_thunk():\n ad.JVPTrace.process_shard_map = _shard_map_jvp\n \n def _shard_map_partial_eval(trace: pe.JaxprTrace, shard_map_p,\n-                            f: lu.WrappedFun, tracers, mesh, in_names,\n-                            out_names_thunk, check_vma, manual_axes):\n+                            f: lu.WrappedFun, tracers, mesh, in_specs,\n+                            out_specs_thunk, check_vma, manual_axes):\n   tracers = map(trace.to_jaxpr_tracer, tracers)\n   in_pvals = [t.pval for t in tracers]\n   in_knowns, in_avals, in_consts = pe.partition_pvals(in_pvals)\n-  unk_in_names, known_in_names = pe.partition_list(in_knowns, in_names)\n+  unk_in_specs, known_in_specs = pe.partition_list(in_knowns, in_specs)\n   in_avals_sharded = map(partial(_shard_aval, mesh, manual_axes, check_vma),\n-                         unk_in_names, in_avals)\n+                         unk_in_specs, in_avals)\n   f = pe.trace_to_subjaxpr_nounits_fwd2(f, trace.tag, f.debug_info, False)\n   f = _promote_scalar_residuals(f)\n   f_known, aux = pe.partial_eval_wrapper_nounits2(\n       f, (*in_knowns,), (*in_avals_sharded,))\n   all_names = _all_newly_manual_mesh_names(mesh, manual_axes)\n \n-  @as_hashable_function(closure=out_names_thunk)\n-  def known_out_names():\n+  @as_hashable_function(closure=out_specs_thunk)\n+  def known_out_specs():\n     _, _, out_knowns, res_avals, _, _ = aux()\n-    _, out_known_names = pe.partition_list(out_knowns, out_names_thunk())\n+    _, out_known_specs = pe.partition_list(out_knowns, out_specs_thunk())\n     if check_vma:\n-      res_names = [{0: order_wrt_mesh(mesh, a.vma)} for a in res_avals]\n+      res_specs = [P(order_wrt_mesh(mesh, a.vma)) for a in res_avals]\n     else:\n-      res_names = [{0: all_names}] * len(res_avals)\n-    return (*out_known_names, *res_names)\n+      res_specs = [P(all_names)] * len(res_avals)\n+    return (*out_known_specs, *res_specs)\n \n-  known_params = dict(mesh=mesh, in_names=(*known_in_names,),\n-                      out_names_thunk=known_out_names, check_vma=check_vma,\n+  known_params = dict(mesh=mesh, in_specs=(*known_in_specs,),\n+                      out_specs_thunk=known_out_specs, check_vma=check_vma,\n                       manual_axes=manual_axes)\n   out = shard_map_p.bind_with_trace(trace.parent_trace, (f_known, *in_consts),\n                                     known_params)\n@@ -1360,32 +1339,32 @@ def known_out_names():\n   num_res = sum(f1 is None and f2 is None for f1, f2 in zip(in_fwd, out_fwd))\n   out_consts, non_fwd_res = split_list(out, [len(out) - num_res])\n   assert not jaxpr.constvars\n-  unk_out_names, _ = pe.partition_list(out_knowns, out_names_thunk())\n-  known_out_names_ = known_out_names()\n+  unk_out_specs, _ = pe.partition_list(out_knowns, out_specs_thunk())\n+  known_out_specs_ = known_out_specs()\n   res = subs_list2(in_fwd, out_fwd, in_consts, out_consts, non_fwd_res)\n   # TODO make res_avals be the full set, not just the non-fwd ones\n   res_avals_iter = iter(res_avals)\n-  res_names = []\n+  res_specs = []\n   for f1, f2 in zip(in_fwd, out_fwd):\n     if f1 is not None:\n-      res_names.append(known_in_names[f1])\n+      res_specs.append(known_in_specs[f1])\n     elif f2 is not None:\n-      res_names.append(known_out_names_[f2])\n+      res_specs.append(known_out_specs_[f2])\n     else:\n       if check_vma:\n         res_vma = next(res_avals_iter).vma\n-        res_names.append({0: order_wrt_mesh(mesh, res_vma)})\n+        res_specs.append(P(order_wrt_mesh(mesh, res_vma)))\n       else:\n-        res_names.append({0: all_names})\n-  unk_in_names = (*res_names,) + ({},) * len(env) + (*unk_in_names,)  # type: ignore[assignment]\n+        res_specs.append(P(all_names))\n+  unk_in_specs = (*res_specs,) + (P(),) * len(env) + (*unk_in_specs,)  # type: ignore[assignment]\n   const_tracers = map(trace.new_instantiated_const, res)\n   env_tracers = map(trace.to_jaxpr_tracer, env)\n   unk_arg_tracers = [t for t in tracers if not t.is_known()]\n   out_avals_sharded = [v.aval for v in jaxpr.outvars]\n-  unk_params = dict(mesh=mesh, in_names=unk_in_names,\n-                    out_names=unk_out_names, jaxpr=jaxpr,\n+  unk_params = dict(mesh=mesh, in_specs=unk_in_specs,\n+                    out_specs=unk_out_specs, jaxpr=jaxpr,\n                     check_vma=check_vma, manual_axes=manual_axes)\n-  out_avals = map(partial(_unshard_aval, mesh, check_vma), unk_out_names,\n+  out_avals = map(partial(_unshard_aval, mesh, check_vma), unk_out_specs,\n                   out_avals_sharded)\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(a), None)\n                  for a in out_avals]\n@@ -1398,8 +1377,8 @@ def known_out_names():\n pe.JaxprTrace.process_shard_map = _shard_map_partial_eval\n \n def _shard_map_linearize(trace, shard_map_p, f: lu.WrappedFun,\n-                         tracers, mesh, in_names,\n-                         out_names_thunk, check_vma, manual_axes):\n+                         tracers, mesh, in_specs, out_specs_thunk, check_vma,\n+                         manual_axes):\n   primals, tangents = unzip2(map(trace.to_primal_tangent_pair, tracers))\n   nzs_in = tuple(type(t) is not ad.Zero for t in tangents)\n   f_primal, linearize_outs_thunk = ad.linearize_subtrace(f, trace.tag, nzs_in, f.debug_info)\n@@ -1407,19 +1386,19 @@ def _shard_map_linearize(trace, shard_map_p, f: lu.WrappedFun,\n   all_names = _all_newly_manual_mesh_names(mesh, manual_axes)\n \n   @as_hashable_function(closure=linearize_outs_thunk)\n-  def fwd_out_names_thunk():\n+  def fwd_out_specs_thunk():\n     res_avals, _, _, _, in_fwd, out_fwd = linearize_outs_thunk()\n     res_avals = [r for r, f1, f2 in zip(res_avals, in_fwd, out_fwd)\n                  if f1 is None and f2 is None]\n-    out_names = out_names_thunk()\n+    out_specs = out_specs_thunk()\n     if check_vma:\n-      res_names = [{0: order_wrt_mesh(mesh, a.vma)} for a in res_avals]\n+      res_specs = [P(order_wrt_mesh(mesh, a.vma)) for a in res_avals]\n     else:\n-      res_names = [{0: all_names}] * len(res_avals)\n-    return (*res_names, *out_names)\n+      res_specs = [P(all_names)] * len(res_avals)\n+    return (*res_specs, *out_specs)\n   fwd_params = dict(\n-      mesh=mesh, in_names=in_names,\n-      out_names_thunk=fwd_out_names_thunk, check_vma=check_vma,\n+      mesh=mesh, in_specs=in_specs,\n+      out_specs_thunk=fwd_out_specs_thunk, check_vma=check_vma,\n       manual_axes=manual_axes)\n   all_fwd_results = shard_map_p.bind_with_trace(\n       trace.parent_trace, (f_primal, *primals), fwd_params)\n@@ -1434,30 +1413,31 @@ def fwd_out_names_thunk():\n         use_abstract_mesh(_as_manual_mesh(mesh, manual_axes | set(mesh.manual_axes))),\n         config._check_vma(check_vma)):\n     lin_jaxpr = _promote_scalar_residuals_jaxpr(lin_jaxpr, args_to_promote)\n-  out_names = out_names_thunk()\n+  out_specs = out_specs_thunk()\n   res_avals2 = [r for r, f1, f2 in zip(res_avals, in_fwd, out_fwd)\n                 if f1 is None and f2 is None]\n   res_avals_iter = iter(res_avals2)\n-  res_names = []\n+  res_specs = []\n   for f1, f2 in zip(in_fwd, out_fwd):\n     if f1 is not None:\n-      res_names.append(in_names[f1])\n+      res_specs.append(in_specs[f1])\n     elif f2 is not None:\n-      res_names.append(out_names[f2])\n+      res_specs.append(out_specs[f2])\n     else:\n       if check_vma:\n         res_vma = next(res_avals_iter).vma\n-        res_names.append({0: order_wrt_mesh(mesh, res_vma)})\n+        res_specs.append(P(order_wrt_mesh(mesh, res_vma)))\n       else:\n-        res_names.append({0: all_names})\n-  new_in_names = (*res_names, *({} for _ in range(len(env))),\n-                  *(ax for ax, nz in zip(in_names, nzs_in) if nz))\n-  tangent_out_names = tuple(ax for ax, nz in zip(out_names_thunk(), nzs_out) if nz)\n-  @as_hashable_function(closure=tangent_out_names)\n-  def tangent_out_names_thunk():\n-    return tangent_out_names\n+        res_specs.append(P(all_names))\n+  new_in_specs = (*res_specs, *(P(),) * len(env),\n+                  *(ax for ax, nz in zip(in_specs, nzs_in) if nz))\n+  tangent_out_specs = tuple(ax for ax, nz in zip(out_specs_thunk(), nzs_out)\n+                            if nz)\n+  @as_hashable_function(closure=tangent_out_specs)\n+  def tangent_out_specs_thunk():\n+    return tangent_out_specs\n   tangent_params = dict(\n-      mesh=mesh, in_names=new_in_names, out_names_thunk=tangent_out_names_thunk,\n+      mesh=mesh, in_specs=new_in_specs, out_specs_thunk=tangent_out_specs_thunk,\n       check_vma=check_vma, manual_axes=manual_axes)\n \n   # TODO(mattjj): avoid round-tripping the jaxpr through eval_jaxpr here\n@@ -1509,29 +1489,29 @@ def fun(*res_and_args):\n   return jaxpr\n \n \n-def _unmentioned2(mesh: Mesh, names: AxisNames,\n-                  manual_axes: frozenset[AxisName]) -> list[AxisName]:\n+def _unmentioned2(mesh: Mesh, spec, manual_axes: frozenset[AxisName]\n+                  ) -> list[AxisName]:\n   # We use a filtered-down version of unmentioned to avoid defensive-psum over\n   # more chips than required in the transpose-no-check-vma case.\n-  name_set = {n for ns in names.values() for n in ns}\n+  name_set = _spec_to_vma(spec)\n   return [n for n in _all_mesh_names_except_spmd(mesh, manual_axes)\n           if n not in name_set]\n \n \n def _shard_map_transpose(out_cts, *args,\n-                         jaxpr: core.Jaxpr, mesh, in_names, out_names,\n+                         jaxpr: core.Jaxpr, mesh, in_specs, out_specs,\n                          check_vma, manual_axes):\n   mb_div = lambda x, y: x / y if y != 1 else x\n   out_cts = [\n-      ad.Zero(_shard_aval(mesh, manual_axes, check_vma, ns, x.aval))\n+      ad.Zero(_shard_aval(mesh, manual_axes, check_vma, sp, x.aval))\n       if type(x) is ad.Zero else x if check_vma or dtypes.dtype(x) == dtypes.float0\n-      else mb_div(x, prod(map(mesh.shape.get, _unmentioned2(mesh, ns, manual_axes))))\n-      for ns, x in zip(out_names, out_cts)\n+      else mb_div(x, prod(map(mesh.shape.get, _unmentioned2(mesh, sp, manual_axes))))\n+      for sp, x in zip(out_specs, out_cts)\n   ]\n   args = tuple(x if type(x) is not ad.UndefinedPrimal else\n                ad.UndefinedPrimal(\n-                   _shard_aval(mesh, manual_axes, check_vma, ns, x.aval))\n-               for ns, x in zip(in_names, args))\n+                   _shard_aval(mesh, manual_axes, check_vma, sp, x.aval))\n+               for sp, x in zip(in_specs, args))\n   all_args, in_tree = tree_flatten((out_cts, args))\n \n   def fun_trans_callable(out_cts, args):\n@@ -1544,11 +1524,11 @@ def fun_trans_callable(out_cts, args):\n     in_cts = ad.backward_pass(\n         jaxpr_unknown.jaxpr, False, (), (*res_reshaped, *undefs), out_cts\n     )[len(res_reshaped):]\n-    _, in_ct_names = partition_list(in_undef, in_names)\n-    in_cts = [ad.Zero(_unshard_aval(mesh, check_vma, ns, x.aval))\n+    _, in_ct_specs = partition_list(in_undef, in_specs)\n+    in_cts = [ad.Zero(_unshard_aval(mesh, check_vma, sp, x.aval))\n               if type(x) is ad.Zero else x if check_vma\n-              else jax.lax.psum(x, tuple(_unmentioned2(mesh, ns, manual_axes)))\n-              for ns, x in zip(in_ct_names, in_cts)]\n+              else jax.lax.psum(x, tuple(_unmentioned2(mesh, sp, manual_axes)))\n+              for sp, x in zip(in_ct_specs, in_cts)]\n     res_zeros = [ad_util.zero_from_primal(r) for r in res]\n     return merge_lists(in_undef, res_zeros, in_cts)\n \n@@ -1556,17 +1536,17 @@ def fun_trans_callable(out_cts, args):\n   fun_trans, nz_arg_cts = ad.nonzero_outputs(fun_trans)\n   fun_trans_flat, out_tree = api_util.flatten_fun_nokwargs(fun_trans, in_tree)\n \n-  new_in_names = \\\n-      [n for n, x in zip(out_names, out_cts) if type(x) is not ad.Zero] + \\\n-      [n for n, x in zip(in_names, args) if type(x) is not ad.UndefinedPrimal]\n+  new_in_specs = (\n+      [n for n, x in zip(out_specs, out_cts) if type(x) is not ad.Zero] +\n+      [n for n, x in zip(in_specs, args) if type(x) is not ad.UndefinedPrimal])\n \n-  def new_out_names_thunk():\n-    return tuple(names for names, nz in zip(in_names, nz_arg_cts()) if nz)\n+  def new_out_specs_thunk():\n+    return tuple(sp for sp, nz in zip(in_specs, nz_arg_cts()) if nz)\n \n   try:\n     out_flat = shard_map_p.bind(\n-        fun_trans_flat, *all_args, mesh=mesh, in_names=tuple(new_in_names),\n-        out_names_thunk=new_out_names_thunk, check_vma=check_vma,\n+        fun_trans_flat, *all_args, mesh=mesh, in_specs=tuple(new_in_specs),\n+        out_specs_thunk=new_out_specs_thunk, check_vma=check_vma,\n         manual_axes=manual_axes)\n   except (FloatingPointError, ZeroDivisionError) as e:\n     print(\"Invalid nan value encountered in the backward pass of a shard_map \"\n@@ -1576,8 +1556,8 @@ def new_out_names_thunk():\n       # in eager mode so that output of shmap are not manual.\n       with jax.disable_jit(True):\n         _ = shard_map_p.bind(\n-            fun_trans_flat, *all_args, mesh=mesh, in_names=tuple(new_in_names),\n-            out_names_thunk=new_out_names_thunk, check_vma=check_vma,\n+            fun_trans_flat, *all_args, mesh=mesh, in_specs=tuple(new_in_specs),\n+            out_specs_thunk=new_out_specs_thunk, check_vma=check_vma,\n             manual_axes=manual_axes)\n     except (FloatingPointError, ZeroDivisionError) as e2:\n       raise e2 from None\n@@ -1618,22 +1598,22 @@ def _partial_eval_jaxpr_custom_rule(\n   _, ins_staged = partition_list(inst_in, eqn.invars)\n   _, out_binders_staged = partition_list(inst_out, eqn.outvars)\n   newvar = core.gensym()\n-  residuals, staged_in_res_names = [], []\n+  residuals, staged_in_res_specs = [], []\n   for var, w in zip(jaxpr_staged.invars[:num_res], which):\n     if w:\n-      rn = ({0: order_wrt_mesh(mesh, var.aval.vma)}  # type: ignore\n-            if check_vma else {0: _all_newly_manual_mesh_names(mesh, manual_axes)})\n+      rn = (P(order_wrt_mesh(mesh, var.aval.vma))  # type: ignore\n+            if check_vma else P(_all_newly_manual_mesh_names(mesh, manual_axes)))\n       residuals.append(newvar(_unshard_aval(mesh, check_vma, rn, var.aval)))\n-      staged_in_res_names.append(rn)\n+      staged_in_res_specs.append(rn)\n   if check_vma:\n-    out_res_names_known = [{0: order_wrt_mesh(mesh, var.aval.vma)}  # type: ignore\n+    out_res_specs_known = [P(order_wrt_mesh(mesh, var.aval.vma))  # type: ignore\n                            for var, o in zip(res_vars, out_fwd) if o is None]\n   else:\n-    out_res_names_known = [\n-        {0: _all_newly_manual_mesh_names(mesh, manual_axes)}] * sum(which)\n+    out_res_specs_known = [\n+        P(_all_newly_manual_mesh_names(mesh, manual_axes))] * sum(which)\n   params_known, params_staged = _pe_custom_params(\n       unks_in, inst_in, map(op.not_, unks_out), inst_out, in_fwd, out_fwd,\n-      out_res_names_known, staged_in_res_names,\n+      out_res_specs_known, staged_in_res_specs,\n       dict(eqn.params, jaxpr=jaxpr_known), dict(eqn.params, jaxpr=jaxpr_staged))\n   eqn_known = pe.new_jaxpr_eqn(ins_known, [*out_binders_known, *residuals],\n                                eqn.primitive, params_known, jaxpr_known.effects,\n@@ -1681,27 +1661,27 @@ def staged(*args):\n   return jaxpr_known, jaxpr_staged\n \n def _pe_custom_params(unks_in, inst_in, kept_outs_known, kept_outs_staged,\n-                      in_fwd, out_fwd, out_res_names_known, staged_in_res_names,\n+                      in_fwd, out_fwd, out_res_specs_known, staged_in_res_specs,\n                       params_known, params_staged):\n   # prune inputs to jaxpr_known according to unks_in\n-  in_names_known, _ = partition_list(unks_in, params_known['in_names'])\n-  _, out_names_known = partition_list(kept_outs_known, params_known['out_names'])\n-  out_names_known = out_names_known + out_res_names_known\n-  assert len(out_names_known) == len(params_known['jaxpr'].outvars)\n-  new_params_known = dict(params_known, in_names=tuple(in_names_known),\n-                          out_names=tuple(out_names_known))\n+  in_specs_known, _ = partition_list(unks_in, params_known['in_specs'])\n+  _, out_specs_known = partition_list(kept_outs_known, params_known['out_specs'])\n+  out_specs_known = out_specs_known + out_res_specs_known\n+  assert len(out_specs_known) == len(params_known['jaxpr'].outvars)\n+  new_params_known = dict(params_known, in_specs=tuple(in_specs_known),\n+                          out_specs=tuple(out_specs_known))\n \n   # added num_res new inputs to jaxpr_staged, pruning according to inst_in\n-  _, in_names_staged = partition_list(inst_in, params_staged['in_names'])\n-  iter_staged = iter(staged_in_res_names)\n-  res_names = [in_names_known[f1] if f1 is not None else\n-               out_names_known[f2] if f2 is not None else\n+  _, in_specs_staged = partition_list(inst_in, params_staged['in_specs'])\n+  iter_staged = iter(staged_in_res_specs)\n+  res_specs = [in_specs_known[f1] if f1 is not None else\n+               out_specs_known[f2] if f2 is not None else\n                next(iter_staged) for f1, f2 in zip(in_fwd, out_fwd)]\n \n-  in_names_staged = res_names + in_names_staged\n-  _, out_names_staged = partition_list(kept_outs_staged, params_staged['out_names'])\n-  new_params_staged = dict(params_staged, in_names=tuple(in_names_staged),\n-                           out_names=tuple(out_names_staged))\n+  in_specs_staged = res_specs + in_specs_staged\n+  _, out_specs_staged = partition_list(kept_outs_staged, params_staged['out_specs'])\n+  new_params_staged = dict(params_staged, in_specs=tuple(in_specs_staged),\n+                           out_specs=tuple(out_specs_staged))\n   return new_params_known, new_params_staged\n \n # TODO(mattjj): remove this mechanism when we revise mesh scopes\n@@ -1742,10 +1722,10 @@ def _shard_map_dce(used_outputs: list[bool], eqn: core.JaxprEqn\n   if not any(used_inputs) and not any(used_outputs) and not jaxpr.effects:\n     return used_inputs, None\n   else:\n-    _, in_names = partition_list(used_inputs, eqn.params['in_names'])\n-    _, out_names = partition_list(used_outputs, eqn.params['out_names'])\n-    new_params = dict(eqn.params, jaxpr=jaxpr, in_names=tuple(in_names),\n-                      out_names=tuple(out_names))\n+    _, in_specs = partition_list(used_inputs, eqn.params['in_specs'])\n+    _, out_specs = partition_list(used_outputs, eqn.params['out_specs'])\n+    new_params = dict(eqn.params, jaxpr=jaxpr, in_specs=tuple(in_specs),\n+                      out_specs=tuple(out_specs))\n     effs = core.filter_named_axis_effects(jaxpr.effects, mesh.axis_names)\n     new_eqn = pe.new_jaxpr_eqn(\n         [v for v, used in zip(eqn.invars, used_inputs) if used],\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 1bebba095896..9b4ca76c3bc5 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -736,10 +736,10 @@ def f(x):\n     x = jnp.arange(4 * 4).reshape(4, 4)\n     jaxpr = jax.make_jaxpr(jax.vmap(f, spmd_axis_name='y'))(x).jaxpr\n     e, = jaxpr.eqns\n-    self.assertIn('in_names', e.params)\n-    self.assertEqual(e.params['in_names'], ({0: ('y',), 1: ('x',)},))\n-    self.assertIn('out_names', e.params)\n-    self.assertEqual(e.params['out_names'], ({0: ('y',), 1: ('x',)},))\n+    self.assertIn('in_specs', e.params)\n+    self.assertEqual(e.params['in_specs'], (P('y', 'x'),))\n+    self.assertIn('out_specs', e.params)\n+    self.assertEqual(e.params['out_specs'], (P('y', 'x'),))\n \n   def test_vmap_of_grad_spmd_axis_name(self):\n     mesh = jtu.create_mesh((2, 2), ('x', 'y'))\n@@ -771,10 +771,10 @@ def f(x):\n     x = jnp.arange(4 * 4).reshape(4, 4)\n     jaxpr = jax.make_jaxpr(jax.vmap(f, spmd_axis_name=('x', 'y')))(x).jaxpr\n     e, = jaxpr.eqns\n-    self.assertIn('in_names', e.params)\n-    self.assertEqual(e.params['in_names'], ({0: ('x', 'y',)},))\n-    self.assertIn('out_names', e.params)\n-    self.assertEqual(e.params['out_names'], ({0: ('x', 'y',)},))\n+    self.assertIn('in_specs', e.params)\n+    self.assertEqual(e.params['in_specs'][0], P(('x', 'y')))\n+    self.assertIn('out_specs', e.params)\n+    self.assertEqual(e.params['out_specs'][0], P(('x', 'y')))\n \n   def test_nested_vmap_with_capture_spmd_axis_name(self):\n     self.skipTest('https://github.com/jax-ml/jax/issues/23476')\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Remove tsan suppression from python/cpython#129748\n\nThis bug is marked as fixed upstream.",
        "output": "```diff\nCommit: 0144ec1edf538a1e19dc7736cdf403861975087b\nDate: 2025-05-20T19:55:08Z\nURL: https://github.com/jax-ml/jax/commit/0144ec1edf538a1e19dc7736cdf403861975087b\nFiles changed: 3\nAdditions: +0, Deletions: -7\ndiff --git a/.github/workflows/tsan-suppressions_3.13.txt b/.github/workflows/tsan-suppressions_3.13.txt\nindex a929a8c44728..483e3f0b3c2a 100644\n--- a/.github/workflows/tsan-suppressions_3.13.txt\n+++ b/.github/workflows/tsan-suppressions_3.13.txt\n@@ -23,9 +23,6 @@ race_top:PyMember_GetOne\n race_top:new_reference\n race:_Py_IsOwnedByCurrentThread\n \n-# https://github.com/python/cpython/issues/129748\n-race:mi_block_set_nextx\n-\n # https://github.com/python/cpython/issues/128130\n race_top:run_eval_code_obj\n \ndiff --git a/.github/workflows/tsan-suppressions_3.14.txt b/.github/workflows/tsan-suppressions_3.14.txt\nindex 384560128cfc..008b61933a0b 100644\n--- a/.github/workflows/tsan-suppressions_3.14.txt\n+++ b/.github/workflows/tsan-suppressions_3.14.txt\n@@ -8,9 +8,6 @@ race:dnnl_sgemm\n # https://github.com/python/cpython/issues/128050\n race:partial_vectorcall_fallback\n \n-# https://github.com/python/cpython/issues/129748\n-race:mi_block_set_nextx\n-\n # Races because the LAPACK and BLAS in our scipy isn't TSAN instrumented.\n race:heevd_ffi\n race:gesdd_ffi\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex 882e140b91ad..ce4130c31a30 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -14,7 +14,6 @@ on:\n     paths:\n       - '**/workflows/tsan.yaml'\n       - '**/workflows/tsan-suppressions*.txt'\n-      - '**/workflows/requirements_lock_3_13_ft.patch'\n \n jobs:\n   tsan:\n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "[Mosaic] Tweak `tpu.log` verification on SC.\n\nPiperOrigin-RevId: 761182341",
        "output": "```diff\nCommit: 683a215a154eebc94a3a980e2b80453fb2392553\nDate: 2025-05-20T19:21:39Z\nURL: https://github.com/jax-ml/jax/commit/683a215a154eebc94a3a980e2b80453fb2392553\nFiles changed: 1\nAdditions: +20, Deletions: -10\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\nindex b5e68bf08370..3733bf5d4465 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n+++ b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/Support/Casting.h\"\n #include \"llvm/Support/FormatVariadic.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n@@ -52,15 +53,15 @@ LogicalResult UnrollVectorsOp::canonicalize(UnrollVectorsOp op,\n   RollVectorsOp roll_op =\n       dyn_cast_or_null<RollVectorsOp>(op.getOperand().getDefiningOp());\n   if (!roll_op) {\n-     return failure();\n+    return failure();\n   }\n   if (roll_op.getNumOperands() != op.getNumResults()) {\n-     return failure();\n+    return failure();\n   }\n   for (auto [v1, v2] :\n        llvm::zip(roll_op.getOperandTypes(), op.getResultTypes())) {\n     if (v1 != v2) {\n-       return failure();\n+      return failure();\n     }\n   }\n   rewriter.replaceOp(op, roll_op.getOperands());\n@@ -499,8 +500,7 @@ LogicalResult MemRefReshapeOp::canonicalize(MemRefReshapeOp op,\n   }\n   auto layout_ref = erase_layout_op.getOperand();\n   auto layout_ty = layout_ref.getType();\n-  auto layout =\n-      dyn_cast<tpu::TiledLayoutAttr>(layout_ty.getLayout());\n+  auto layout = dyn_cast<tpu::TiledLayoutAttr>(layout_ty.getLayout());\n   CHECK(!layout.getTiles().empty());\n   auto tile = layout.getTiles().front().dimensions();\n   auto new_tile_strides = ComputeTileStrides(dst_ty, tile);\n@@ -594,8 +594,8 @@ LogicalResult MemRefBitcastOp::canonicalize(MemRefBitcastOp op,\n   if (tile[0] * src_bitwidth % tgt_bitwidth != 0) {\n     return failure();\n   }\n-  SmallVector<xla::Tile, 2> new_tiles =\n-      {xla::Tile({tile[0] * src_bitwidth / tgt_bitwidth, 128})};\n+  SmallVector<xla::Tile, 2> new_tiles = {\n+      xla::Tile({tile[0] * src_bitwidth / tgt_bitwidth, 128})};\n   if (tgt_bitwidth < 32) {\n     new_tiles.push_back(xla::Tile({32 / tgt_bitwidth, 1}));\n   }\n@@ -1325,11 +1325,21 @@ LogicalResult LogOp::verify() {\n     return failure();\n   }\n   CoreType logging_core_type = logging_core_type_maybe->value_or(CoreType::kTc);\n-  if ((logging_core_type == CoreType::kScScalarSubcore ||\n-       logging_core_type == CoreType::kScVectorSubcore) &&\n-      getFormattedAttr() != nullptr && getFormattedAttr().getValue()) {\n+  bool is_sc_core = logging_core_type == CoreType::kScScalarSubcore ||\n+                    logging_core_type == CoreType::kScVectorSubcore;\n+  if (is_sc_core && getFormattedAttr() != nullptr &&\n+      getFormattedAttr().getValue()) {\n     return emitOpError(\"Formatted logging is not supported on SC\");\n   }\n+  if (is_sc_core && getInputs().size() > 1) {\n+    return emitOpError(\"SC logging only supports 0 or 1 inputs\");\n+  }\n+  if (is_sc_core && getInputs().size() == 1) {\n+    Type input_type = getInputs().front().getType();\n+    if (!llvm::isa<MemRefType, IntegerType, FloatType, IndexType>(input_type)) {\n+      return emitOpError(\"SC logging only supports memrefs or scalars\");\n+    }\n+  }\n   switch (logging_core_type) {\n     case CoreType::kTc:\n     case CoreType::kScScalarSubcore:\n\n```",
        "from_id": [
            "sashabu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Migrate users of backend.compile to backend.compile_and_load.\n\nPiperOrigin-RevId: 761150621",
        "output": "```diff\nCommit: 8cfabd7d545e601fdd111d36acc2142e048d11b5\nDate: 2025-05-20T18:01:48Z\nURL: https://github.com/jax-ml/jax/commit/8cfabd7d545e601fdd111d36acc2142e048d11b5\nFiles changed: 3\nAdditions: +6, Deletions: -6\ndiff --git a/jax/experimental/jax2tf/tests/sharding_test.py b/jax/experimental/jax2tf/tests/sharding_test.py\nindex 55ccb1328c87..5fc45df218cd 100644\n--- a/jax/experimental/jax2tf/tests/sharding_test.py\n+++ b/jax/experimental/jax2tf/tests/sharding_test.py\n@@ -115,7 +115,7 @@ def log_jax_hlo(self, f_jax, args: Sequence[Any], *,\n         executable = backend.compile(\n             jax_hlo, compile_options=compile_options)  # type: ignore\n       else:\n-        executable = backend.compile(\n+        executable = backend.compile_and_load(\n             jax_hlo, xc.DeviceList(tuple(self.devices.flat)), compile_options)  # type: ignore\n       jax_optimized_hlo = executable.hlo_modules()[0].to_string()\n       logging.info(\"[%s] got JAX optimized HLO for platform %s %s\",\ndiff --git a/jax/experimental/jax2tf/tests/tf_test_util.py b/jax/experimental/jax2tf/tests/tf_test_util.py\nindex e87a8af5d15e..faecf9f0f09e 100644\n--- a/jax/experimental/jax2tf/tests/tf_test_util.py\n+++ b/jax/experimental/jax2tf/tests/tf_test_util.py\n@@ -346,7 +346,7 @@ def log_message(extra):\n \n         backend = xla_bridge.get_backend()\n         device_list = xc.DeviceList(tuple(backend.local_devices()))\n-        modules = backend.compile(\n+        modules = backend.compile_and_load(\n             str(jax_lowered.compiler_ir()), device_list).hlo_modules()\n         jax_opt_hlo = modules[0].to_string()\n         logging.info(\"[%s] JAX OPT HLO\\n%s\", self._testMethodName,\ndiff --git a/tests/compilation_cache_test.py b/tests/compilation_cache_test.py\nindex 1ba6b1221a88..5a76d732bd76 100644\n--- a/tests/compilation_cache_test.py\n+++ b/tests/compilation_cache_test.py\n@@ -150,9 +150,9 @@ def test_diff_executables(self):\n       executable1 = backend.compile(computation1, compile_options)\n       executable2 = backend.compile(computation2, compile_options)\n     else:\n-      executable1 = backend.compile(\n+      executable1 = backend.compile_and_load(\n           computation1, executable_devices, compile_options)\n-      executable2 = backend.compile(\n+      executable2 = backend.compile_and_load(\n           computation2, executable_devices, compile_options)\n     cc.put_executable_and_time(\n         \"key1\", \"computation1\", executable1, backend, FAKE_COMPILE_TIME)\n@@ -180,7 +180,7 @@ def test_put_executable(self):\n     if jax._src.lib.jaxlib_extension_version < 331:\n       executable = backend.compile(str(computation), compile_options)\n     else:\n-      executable = backend.compile(\n+      executable = backend.compile_and_load(\n           str(computation), executable_devices, compile_options)\n     key = cc.get_cache_key(computation, devices, compile_options, backend)\n     cc.put_executable_and_time(\n@@ -251,7 +251,7 @@ def test_enable_compilation_cache(self):\n           g = jit(lambda x: x * 3)\n           g(2)\n           cache = cc._get_cache(backend)\n-          self.assertIsNotNone(cache) # Cache should be initalized\n+          self.assertIsNotNone(cache) # Cache should be initialized\n \n   def test_xla_autofdo_profile_version(self):\n     original_profile_version = config.jax_xla_profile_version.value\n\n```",
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Include Pallas TPU random ops in JAX wheel.\n\nSince the `pallas/tpu/ops/random` directory was missing an `__init__.py` file, it was inadvertently excluded from the released JAX distribution. I don't see any reason why this submodule shouldn't be included so let's fix that!\n\nTo deal with the fact that they weren't included in the distribution, we were also monkey patching these files into the wheel when testing, but that's no longer needed.\n\nPiperOrigin-RevId: 761138525",
        "output": "```diff\nCommit: 86680a9b1282d00398f3d3d3a56336c0452a76b8\nDate: 2025-05-20T17:33:45Z\nURL: https://github.com/jax-ml/jax/commit/86680a9b1282d00398f3d3d3a56336c0452a76b8\nFiles changed: 2\nAdditions: +14, Deletions: -3\ndiff --git a/BUILD.bazel b/BUILD.bazel\nindex 59fd949b7ad8..887f28d4583e 100644\n--- a/BUILD.bazel\n+++ b/BUILD.bazel\n@@ -108,9 +108,6 @@ genrule(\n         \"//jax:internal_test_harnesses\",\n         \"//jax:internal_test_util\",\n         \"//jax:internal_export_back_compat_test_data\",\n-        \"//jax:experimental/pallas/ops/tpu/random/philox.py\",\n-        \"//jax:experimental/pallas/ops/tpu/random/prng_utils.py\",\n-        \"//jax:experimental/pallas/ops/tpu/random/threefry.py\",\n         \"//jax/experimental/mosaic/gpu/examples:flash_attention.py\",\n         \"//jax/experimental/mosaic/gpu/examples:matmul.py\",\n         \"//jax:test_multiprocess\",\ndiff --git a/jax/experimental/pallas/ops/tpu/random/__init__.py b/jax/experimental/pallas/ops/tpu/random/__init__.py\nnew file mode 100644\nindex 000000000000..3da0dd1fa3ca\n--- /dev/null\n+++ b/jax/experimental/pallas/ops/tpu/random/__init__.py\n@@ -0,0 +1,14 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n\n```",
        "from_id": [
            "dfm",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Use `MemorySpace` aliases\n\nThis just makes the corresponding conditions a bit easier to read.\n\nPiperOrigin-RevId: 761137840",
        "output": "```diff\nCommit: 4a3ce2b2dc75bdb02b52687c24dfb7d182278be5\nDate: 2025-05-20T17:31:35Z\nURL: https://github.com/jax-ml/jax/commit/4a3ce2b2dc75bdb02b52687c24dfb7d182278be5\nFiles changed: 2\nAdditions: +5, Deletions: -6\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex db959b1dbc24..eb5fc136082e 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -649,7 +649,7 @@ def ref_for_aval(aval: jax_core.AbstractValue):\n     aval = v.aval\n     if (isinstance(aval, pallas_core.AbstractMemoryRef) and\n         jnp.issubdtype(aval.dtype, pallas_core.semaphore_dtype)):\n-      if aval.memory_space != gpu_core.MemorySpace.GMEM:\n+      if aval.memory_space != gpu_core.GMEM:\n         raise ValueError(\n             \"Only GMEM memory space is supported for semaphores in Mosaic GPU.\"\n         )\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex af9d4138cfbb..53c890932e38 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -1200,19 +1200,18 @@ def _tcgen05_mma_abstract_eval(acc, a, b, barrier, accumulate,\n                                collective_axis):\n   del (accumulate, transforms_leaves, a_transforms_tree, b_transforms_tree)\n \n-  if acc.memory_space != gpu_core.MemorySpace.TMEM:\n+  if acc.memory_space != gpu_core.TMEM:\n     raise ValueError(\"Accumulator must be a TMEM Ref.\")\n-  if a.memory_space not in (gpu_core.MemorySpace.SMEM,\n-                            gpu_core.MemorySpace.TMEM):\n+  if a.memory_space not in (gpu_core.SMEM, gpu_core.TMEM):\n     raise ValueError(\"LHS must be a TMEM/SMEM Ref.\")\n-  if b.memory_space != gpu_core.MemorySpace.SMEM:\n+  if b.memory_space != gpu_core.SMEM:\n     raise ValueError(\"RHS must be an SMEM Ref.\")\n \n   if collective_axis is not None:\n     if not acc.collective:\n       raise ValueError(\n           \"Accumulator Ref must be collective if collective_axis is set.\")\n-    if a.memory_space == gpu_core.MemorySpace.TMEM and not a.collective:\n+    if a.memory_space == gpu_core.TMEM and not a.collective:\n       raise ValueError(\n           \"LHS TMEM Ref must be collective if collective_axis is set.\")\n \n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Replace `core_map` + `run_state` with `plgpu.kernel` for simpler code. Slightly more efficient because we dont initialize the outputs now.\n\nPiperOrigin-RevId: 761113870",
        "output": "```diff\nCommit: 496cbd07cea5134e9ab83a72fc33941acc6149b8\nDate: 2025-05-20T16:27:22Z\nURL: https://github.com/jax-ml/jax/commit/496cbd07cea5134e9ab83a72fc33941acc6149b8\nFiles changed: 1\nAdditions: +22, Deletions: -38\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex 6da468f2cc3e..c7e9f95e3f99 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -658,10 +658,9 @@ def attention_with_pipeline_emitter(q, k, v, config: TuningConfig, save_residual\n   if rem:\n     raise NotImplementedError(f\"{q_seq_len=} must be a multiple of {block_q * 2=}\")\n \n-  def fa3_kernel(q_ref, k_ref, v_ref, out_ref, lse_ref, scoped):\n+  def fa3_kernel(q_ref, k_ref, v_ref, out_ref, lse_ref, smem_buffers, q_barriers, schedule_barrier):\n     batch = lax.axis_index(\"batch\")\n     wg_idx = lax.axis_index(\"wg\")\n-    smem_buffers, q_barriers, schedule_barrier = scoped\n     qo_smem2, lse_smem2 = smem_buffers\n     q_seq_base = lax.axis_index(\"q_seq\") * (2 * block_q) + wg_idx * block_q\n     q_head = lax.axis_index(\"heads\")\n@@ -758,46 +757,31 @@ def compute_pv(acc_ref):\n     k_ref = k_ref.at[batch, :, kv_head, :]\n     v_ref = v_ref.at[batch, :, kv_head, :]\n     pipeline(k_ref, v_ref)\n-  mesh = plgpu.Mesh(\n+\n+  out_shape = [q, None]\n+  if save_residuals:\n+    out_shape[1] = jax.ShapeDtypeStruct((batch_size, num_q_heads, q_seq_len), jnp.float32)\n+\n+  qo_scratch = plgpu.SMEM((compute_wgs, block_q, head_dim), jnp.float16)\n+  smem_scratch = [qo_scratch, None]\n+  if save_residuals:\n+    smem_scratch[1] = plgpu.SMEM((compute_wgs, block_q), jnp.float32)\n+\n+  out, lse = plgpu.kernel(\n+      fa3_kernel,\n       grid=(batch_size, num_q_tiles, num_q_heads),\n       grid_names=(\"batch\", \"q_seq\", \"heads\"),\n       num_threads=3,\n       thread_name=\"wg\",\n-  )\n-  def run(refs):\n-    q_ref, k_ref, v_ref, out_ref, lse_ref = refs\n-\n-    @pl.core_map(\n-        mesh,\n-        compiler_params=plgpu.CompilerParams(\n-            approx_math=True, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n-        ),\n-    )\n-    def _kernel_entry():\n-      qo_scratch = plgpu.SMEM(\n-          (compute_wgs, block_q, head_dim), jnp.float16,\n-      )\n-      scratch = [qo_scratch, None]\n-      if save_residuals:\n-        scratch[1] = plgpu.SMEM((compute_wgs, block_q), jnp.float32)\n-      pl.run_scoped(\n-          lambda *args: fa3_kernel(q_ref, k_ref, v_ref, out_ref, lse_ref, args),\n-          scratch,\n-          plgpu.Barrier(1, num_barriers=compute_wgs),\n-          plgpu.Barrier(num_arrivals=compute_wgs),\n-          collective_axes=\"wg\",\n-      )\n-  @jax.jit\n-  def run_function(q, k, v, o, lse):\n-    *_, out, lse = pl.run_state(run)((q, k, v, o, lse))\n-    return out, lse\n-\n-  lse = (\n-      jnp.full((batch_size, num_q_heads, q_seq_len), -jnp.inf, dtype=jnp.float32)\n-      if save_residuals\n-      else None\n-  )\n-  out, lse = run_function(q, k, v, jnp.full_like(q, jnp.inf), lse)\n+            out_shape=out_shape,\n+      scratch_shapes=(\n+          tuple(smem_scratch),  # type: ignore\n+          plgpu.Barrier(1, num_barriers=compute_wgs),  # type: ignore\n+          plgpu.Barrier(num_arrivals=compute_wgs),),  # type: ignore\n+      compiler_params=plgpu.CompilerParams(\n+          approx_math=True, lowering_semantics=plgpu.LoweringSemantics.Warpgroup,\n+      ),\n+  )(q, k, v)\n \n   if save_residuals:\n     assert lse is not None\n\n```",
        "from_id": [
            "Rifur13",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28848 from dfm:fix-welch-ci\n\nPiperOrigin-RevId: 761101581",
        "output": "```diff\nCommit: 69a182599ae953807a56ff99ff11bdf3b46c904b\nDate: 2025-05-20T15:52:51Z\nURL: https://github.com/jax-ml/jax/commit/69a182599ae953807a56ff99ff11bdf3b46c904b\nFiles changed: 2\nAdditions: +3, Deletions: -4\ndiff --git a/jax/_src/third_party/scipy/signal_helper.py b/jax/_src/third_party/scipy/signal_helper.py\nindex 4a021675804d..ad7bdfbef62a 100644\n--- a/jax/_src/third_party/scipy/signal_helper.py\n+++ b/jax/_src/third_party/scipy/signal_helper.py\n@@ -57,7 +57,7 @@ def _triage_segments(window: ArrayLike | str | tuple[Any, ...], nperseg: int | N\n       win = get_window(window, nperseg_int)\n     win = jnp.array(win, dtype=dtype)\n   else:\n-    win = jnp.asarray(window)\n+    win = jnp.asarray(window, dtype=dtype)\n     nperseg_int = win.size if nperseg is None else int(nperseg)\n     if win.ndim != 1:\n       raise ValueError('window must be 1-D')\ndiff --git a/tests/scipy_signal_test.py b/tests/scipy_signal_test.py\nindex 7ff3c87435c7..b1c5d9c98fed 100644\n--- a/tests/scipy_signal_test.py\n+++ b/tests/scipy_signal_test.py\n@@ -357,12 +357,11 @@ def testWelchWithDefaultStepArgsAgainstNumpy(\n     if use_nperseg:\n       kwargs['nperseg'] = nperseg\n     if use_window:\n-      kwargs['window'] = jnp.array(osp_signal.get_window('hann', nperseg),\n-                                   dtype=dtypes.to_complex_dtype(dtype))\n+      kwargs['window'] = jnp.array(osp_signal.get_window('hann', nperseg))\n     if use_noverlap:\n       kwargs['noverlap'] = noverlap\n \n-    @jtu.ignore_warning(message=\"nperseg = 256 is greater than\")\n+    @jtu.ignore_warning(message=\"nperseg\")\n     def osp_fun(x):\n       freqs, Pxx = osp_signal.welch(x, **kwargs)\n       return freqs.astype(_real_dtype(dtype)), Pxx.astype(_real_dtype(dtype))\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Rename backend.compile to backend.compile_and_load.\n\nPart of a larger refactor. Today, `compile` returns a loaded executable i.e., fuses the compile and load functions. Eventually, `compile` should return an unloaded executable and `load` should return a loaded exectuable; the default jit path will still return a loaded executable.\n\nPiperOrigin-RevId: 761098001",
        "output": "```diff\nCommit: 06448864abd6e8187e5b4d9b1ff08ab14fe3b8e0\nDate: 2025-05-20T15:41:36Z\nURL: https://github.com/jax-ml/jax/commit/06448864abd6e8187e5b4d9b1ff08ab14fe3b8e0\nFiles changed: 4\nAdditions: +54, Deletions: -11\ndiff --git a/jax/_src/compiler.py b/jax/_src/compiler.py\nindex 04f993fed799..e8ef647a1312 100644\n--- a/jax/_src/compiler.py\n+++ b/jax/_src/compiler.py\n@@ -292,6 +292,19 @@ def backend_compile(\n     executable_devices: xc.DeviceList,\n     options: xc.CompileOptions,\n     host_callbacks: Sequence[Any],\n+) -> xc.LoadedExecutable:\n+  return backend_compile_and_load(\n+      backend, module, executable_devices, options, host_callbacks\n+  )\n+\n+\n+@profiler.annotate_function\n+def backend_compile_and_load(\n+    backend: xc.Client,\n+    module: ir.Module,\n+    executable_devices: xc.DeviceList,\n+    options: xc.CompileOptions,\n+    host_callbacks: Sequence[Any],\n ) -> xc.LoadedExecutable:\n   sym_name = module.operation.attributes['sym_name']\n   module_name = ir.StringAttr(sym_name).value\n@@ -322,18 +335,35 @@ def backend_compile(\n \n     # we use a separate function call to ensure that XLA compilation appears\n     # separately in Python profiling results\n-    if host_callbacks:\n+    elif jaxlib_extension_version < 342 or isinstance(backend, xc.CompileOnlyPyClient):\n+      if host_callbacks:\n+        return backend.compile(\n+            built_c,\n+            executable_devices=executable_devices,  # type: ignore\n+            compile_options=options,\n+            host_callbacks=host_callbacks,\n+        )\n+      # Some backends don't have `host_callbacks` option yet\n+      # TODO(sharadmv): remove this fallback when all backends allow `compile`\n+      # to take in `host_callbacks`\n       return backend.compile(\n+          built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n+    else:\n+      if host_callbacks:\n+        return backend.compile_and_load(\n+            built_c,\n+            executable_devices=executable_devices,\n+            compile_options=options,\n+            host_callbacks=host_callbacks,\n+        )\n+      # Some backends don't have `host_callbacks` option yet\n+      # TODO(sharadmv): remove this fallback when all backends allow `compile`\n+      # to take in `host_callbacks`\n+      return backend.compile_and_load(\n           built_c,\n-          executable_devices=executable_devices,  # type: ignore\n+          executable_devices=executable_devices,\n           compile_options=options,\n-          host_callbacks=host_callbacks,\n       )\n-    # Some backends don't have `host_callbacks` option yet\n-    # TODO(sharadmv): remove this fallback when all backends allow `compile`\n-    # to take in `host_callbacks`\n-    return backend.compile(\n-        built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n   except xc.XlaRuntimeError as e:\n     for error_handler in _XLA_RUNTIME_ERROR_HANDLERS:\n       handler_result = error_handler(e)\n@@ -398,7 +428,7 @@ def compile_or_get_cached(\n   )\n \n   if cache_key is None:\n-    return backend_compile(\n+    return backend_compile_and_load(\n         backend, computation, executable_devices, compile_options,\n         host_callbacks)\n \n@@ -426,7 +456,7 @@ def compile_or_get_cached(\n       config.share_binary_between_hosts.value\n       and is_multi_process\n       and distributed.global_state.client is not None\n-      # Host callbacks are currently baked into the HLO module so we cant share\n+      # Host callbacks are currently baked into the HLO module so we can't share\n       # them.\n       and len(host_callbacks) == 0\n   ):\n@@ -716,7 +746,7 @@ def _compile_and_write_cache(\n     cache_key: str,\n ) -> xc.LoadedExecutable:\n   start_time = time.monotonic()\n-  executable = backend_compile(\n+  executable = backend_compile_and_load(\n       backend, computation, executable_devices, compile_options, host_callbacks\n   )\n   compile_time = time.monotonic() - start_time\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 1d7f3042e8a3..000c05acacad 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -551,6 +551,17 @@ class Client:\n   ) -> PjRtLayout: ...\n   def __getattr__(self, name: str) -> Any: ...\n \n+\n+class CompileOnlyPyClient(Client):\n+  def compile(\n+      self,\n+      computation: str | bytes,\n+      executable_devices: DeviceList | Sequence[Device],\n+      compile_options: CompileOptions = ...,\n+      host_callbacks: Sequence[Any] = ...,\n+  ) -> LoadedExecutable: ...\n+\n+\n class CpuCollectives: ...\n \n def make_gloo_tcp_collectives(\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 8f8c829ee6c7..b1bbc464610e 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -304,6 +304,7 @@ def computation_count():\n \n XlaComputation = _xla.XlaComputation\n Client = _xla.Client\n+CompileOnlyPyClient = _xla.CompileOnlyPyClient\n Memory = _xla.Memory\n Array = _xla.Array\n ArrayImpl = _xla.ArrayImpl\ndiff --git a/jaxlib/xla_client.pyi b/jaxlib/xla_client.pyi\nindex 80599e86676b..fce114f45474 100644\n--- a/jaxlib/xla_client.pyi\n+++ b/jaxlib/xla_client.pyi\n@@ -24,6 +24,7 @@ from jaxlib._jax import ArrayCopySemantics as ArrayCopySemantics\n from jaxlib._jax import ArrayImpl as ArrayImpl\n from jaxlib._jax import AutotuneCacheMode as AutotuneCacheMode\n from jaxlib._jax import Client as Client\n+from jaxlib._jax import CompileOnlyPyClient as CompileOnlyPyClient\n from jaxlib._jax import CompileOptions as CompileOptions\n from jaxlib._jax import Device as Device\n from jaxlib._jax import DeviceAssignment as DeviceAssignment\n\n```",
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Removed the `GPU*` prefix from Mosaic GPU-specific types\n\nThese APIs are always used qualified, e.g. `plgpu.GPUCompilerParams`, so the\nprefix is redundant.\n\nPiperOrigin-RevId: 761088896",
        "output": "```diff\nCommit: d1511c1d781cf0b3992ee9e62ae0773deb9cb833\nDate: 2025-05-20T15:13:24Z\nURL: https://github.com/jax-ml/jax/commit/d1511c1d781cf0b3992ee9e62ae0773deb9cb833\nFiles changed: 11\nAdditions: +157, Deletions: -135\ndiff --git a/docs/jax.experimental.pallas.mosaic_gpu.rst b/docs/jax.experimental.pallas.mosaic_gpu.rst\nindex 2d3452609c75..4191dde74df7 100644\n--- a/docs/jax.experimental.pallas.mosaic_gpu.rst\n+++ b/docs/jax.experimental.pallas.mosaic_gpu.rst\n@@ -10,9 +10,9 @@ Classes\n    :toctree: _autosummary\n \n    Barrier\n-   GPUBlockSpec\n-   GPUCompilerParams\n-   GPUMemorySpace\n+   BlockSpec\n+   CompilerParams\n+   MemorySpace\n    Layout\n    SwizzleTransform\n    TilingTransform\ndiff --git a/docs/pallas/gpu/reference.md b/docs/pallas/gpu/reference.md\nindex 0db31e11b459..7b4a1e6e9c7d 100644\n--- a/docs/pallas/gpu/reference.md\n+++ b/docs/pallas/gpu/reference.md\n@@ -225,17 +225,20 @@ def body(..., scratch_ref):\n There are two ways in which references are allocated and each has a way to select\n the desired transforms:\n \n-**1. Using `GPUBlockSpec`**\n+**1. Using `plgpu.BlockSpec`**\n \n ```python\n transforms = (plgpu.TileTransform((8, 64)), plgpu.SwizzleTransform(128))\n f = pl.pallas_call(\n-  in_specs=plgpu.GPUBlockSpec(in_block_shape, in_index_map, transforms=transforms),\n-  out_specs=plgpu.GPUBlockSpec(out_block_shape, out_index_map, transforms=transforms),\n+  in_specs=plgpu.BlockSpec(in_block_shape, in_index_map, transforms=transforms),\n+  out_specs=plgpu.BlockSpec(out_block_shape, out_index_map, transforms=transforms),\n   ...\n )\n ```\n \n+Note that unlike `plgpu.BlockSpec`, `pl.BlockSpec` does *not* allow specifying\n+transforms.\n+\n **2. Specifying the `transforms` argument on the allocated `SMEM`**\n \n ```python\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex d13977ac4fbf..08e47cec4b2e 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -71,7 +71,7 @@ def _slices(d):\n \n \n @dataclasses.dataclass(frozen=True, kw_only=True)\n-class GPUCompilerParams(pallas_core.CompilerParams):\n+class CompilerParams(pallas_core.CompilerParams):\n   \"\"\"Mosaic GPU compiler parameters.\n \n   Attributes:\n@@ -108,7 +108,7 @@ def __post_init__(self):\n       )\n \n \n-class GPUMemorySpace(enum.Enum):\n+class MemorySpace(enum.Enum):\n   #: Global memory.\n   GMEM = \"gmem\"\n   #: Shared memory.\n@@ -145,7 +145,7 @@ def __call__(self, shape: tuple[int, ...]):\n       dtype = pallas_core.BarrierSemaphore()\n     else:\n       dtype = pallas_core.Semaphore()\n-    return pallas_core.MemoryRef(shape, dtype, GPUMemorySpace.GMEM)\n+    return pallas_core.MemoryRef(shape, dtype, MemorySpace.GMEM)\n \n   def get_array_aval(self) -> jax_core.ShapedArray:\n     return self(()).get_array_aval()\n@@ -183,7 +183,7 @@ def kernel(\n   def wrapper(*operands):\n     def stateful(operand_and_out_refs):\n       operand_refs, out_refs = operand_and_out_refs\n-      mesh = GPUMesh(**mesh_kwargs)\n+      mesh = Mesh(**mesh_kwargs)\n       thread_name = mesh.thread_name if mesh.thread_name is not None else ()\n       def cmap_body():\n         pallas_primitives.run_scoped(\n@@ -234,7 +234,7 @@ class GPUMemoryRef(pallas_core.MemoryRef):\n   collective: bool | None = dataclasses.field(default=None, kw_only=True)\n \n   def __post_init__(self):\n-    if self.memory_space != GPUMemorySpace.TMEM:\n+    if self.memory_space != MemorySpace.TMEM:\n       if self.packed is not None:\n         raise ValueError(\"Packed option is only supported for TMEM.\")\n       if self.collective is not None:\n@@ -244,7 +244,7 @@ def get_ref_aval(self) -> _Ref:\n     aval = jax_core.ShapedArray(self.shape, self.dtype)\n     for t in self.transforms:\n       aval = t(aval)\n-    if self.memory_space == GPUMemorySpace.TMEM:\n+    if self.memory_space == MemorySpace.TMEM:\n       ref = pallas_core.TransformedRef(\n           AbstractTMEMRef(aval,\n                           memory_space=self.memory_space,\n@@ -785,7 +785,7 @@ def pretty_print(self, context: jax_core.JaxprPpContext) -> pp.Doc:\n \n \n @dataclasses.dataclass\n-class GPUBlockSpec(pallas_core.BlockSpec):\n+class BlockSpec(pallas_core.BlockSpec):\n   transforms: Sequence[MemoryRefTransform] = ()\n \n   def to_block_mapping(\n@@ -817,10 +817,10 @@ def to_block_mapping(\n     )\n \n \n-GMEM = GPUMemorySpace.GMEM\n-SMEM = GPUMemorySpace.SMEM\n-TMEM = GPUMemorySpace.TMEM\n-REGS = GPUMemorySpace.REGS\n+GMEM = MemorySpace.GMEM\n+SMEM = MemorySpace.SMEM\n+TMEM = MemorySpace.TMEM\n+REGS = MemorySpace.REGS\n \n \n class barrier_dtype(dtypes.extended):\n@@ -903,7 +903,7 @@ def get_ref_aval(self) -> AbstractMemoryRef:\n           \"Preinitialized WGMMAAccumulatorRef only supported in pl.run_state.\"\n       )\n     return WGMMAAbstractAccumulatorRef(\n-        jax_core.ShapedArray(shape=self.shape, dtype=self.dtype), GPUMemorySpace.REGS\n+        jax_core.ShapedArray(shape=self.shape, dtype=self.dtype), MemorySpace.REGS\n     )\n \n   @staticmethod\n@@ -913,7 +913,7 @@ def init(array):\n \n def _wgmma_ref_type_mapping(ref: WGMMAAccumulatorRef):\n   aval = WGMMAAbstractAccumulatorRef(\n-      jax_core.ShapedArray(shape=ref.shape, dtype=ref.dtype), GPUMemorySpace.REGS\n+      jax_core.ShapedArray(shape=ref.shape, dtype=ref.dtype), MemorySpace.REGS\n   )\n   return aval, ref._init\n state_types._ref_type_aval_mappings[WGMMAAccumulatorRef] = _wgmma_ref_type_mapping\n@@ -962,7 +962,7 @@ def __repr__(self) -> str:\n _WARPGROUP_AXIS_NAME = object()\n \n @dataclasses.dataclass(frozen=True, kw_only=True)\n-class GPUMesh:\n+class Mesh:\n   grid: Sequence[int] = ()\n   grid_names: Sequence[str] = ()\n   cluster: Sequence[int] = ()\n@@ -1049,15 +1049,15 @@ def _gpu_mesh_discharge_rule(\n     cost_estimate,\n     name,\n ):\n-  if not isinstance(mesh, GPUMesh):\n-    raise TypeError(f\"Mesh must be a GPUMesh, got {type(mesh)}\")\n-  if compiler_params and not isinstance(compiler_params, GPUCompilerParams):\n+  if not isinstance(mesh, Mesh):\n+    raise TypeError(f\"Mesh must be a `plgpu.Mesh`, got {type(mesh)}\")\n+  if compiler_params and not isinstance(compiler_params, CompilerParams):\n     raise TypeError(\n-        \"Compiler params must be a GPUCompilerParams, got\"\n+        \"Compiler params must be a `plgpu.CompilerParams`, got\"\n         f\" {type(compiler_params)}\"\n     )\n   if not compiler_params:\n-    compiler_params = GPUCompilerParams()\n+    compiler_params = CompilerParams()\n   return pallas_core.default_mesh_discharge_rule(\n       in_avals,\n       out_avals,\n@@ -1073,7 +1073,7 @@ def _gpu_mesh_discharge_rule(\n   )\n \n \n-pallas_core._core_map_mesh_rules[GPUMesh] = _gpu_mesh_discharge_rule\n+pallas_core._core_map_mesh_rules[Mesh] = _gpu_mesh_discharge_rule\n \n \n class MemoryEffect(jax_core.Effect):\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 751a2bae2ed0..db959b1dbc24 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -568,7 +568,7 @@ def index_map(*indices):\n     )\n     return eval_index_map(*new_indices)\n \n-  return gpu_core.GPUBlockSpec(\n+  return gpu_core.BlockSpec(\n       bm.block_shape,\n       index_map,\n       memory_space=bm.transformed_block_aval.memory_space,\n@@ -581,7 +581,7 @@ def lower_pipelined_jaxpr_to_module(\n     gpu_mesh: pallas_core.Mesh | None,\n     jax_mesh: mesh_lib.Mesh | None,\n     jaxpr: jax_core.Jaxpr,\n-    params: gpu_core.GPUCompilerParams,\n+    params: gpu_core.CompilerParams,\n     cost_estimate: pallas_core.CostEstimate | None,\n ) -> LoweringResult:\n   del cost_estimate  # Unused.\n@@ -604,7 +604,7 @@ def lower_pipelined_jaxpr_to_module(\n   )\n \n   if gpu_mesh:\n-    assert isinstance(gpu_mesh, gpu_core.GPUMesh)\n+    assert isinstance(gpu_mesh, gpu_core.Mesh)\n     block = (128 * (gpu_mesh.num_threads or 1), 1, 1)\n     grid = gpu_mesh.grid\n     thread_axis = (\n@@ -649,7 +649,7 @@ def ref_for_aval(aval: jax_core.AbstractValue):\n     aval = v.aval\n     if (isinstance(aval, pallas_core.AbstractMemoryRef) and\n         jnp.issubdtype(aval.dtype, pallas_core.semaphore_dtype)):\n-      if aval.memory_space != gpu_core.GPUMemorySpace.GMEM:\n+      if aval.memory_space != gpu_core.MemorySpace.GMEM:\n         raise ValueError(\n             \"Only GMEM memory space is supported for semaphores in Mosaic GPU.\"\n         )\n@@ -747,7 +747,7 @@ def lower_jaxpr_to_module(\n     out_shapes: Sequence[jax.ShapeDtypeStruct],\n     gmem_scratch_shapes: Sequence[jax.ShapeDtypeStruct],\n     jaxpr: jax_core.Jaxpr,\n-    params: gpu_core.GPUCompilerParams,\n+    params: gpu_core.CompilerParams,\n     consts=(),\n ) -> LoweringResult:\n   debug_info = jaxpr.debug_info\n@@ -2048,7 +2048,7 @@ def _resolve_cluster_axis(axis_names: _AxisNames | None, axis_name: str):\n   if not axis_names:\n     raise LookupError(\n         \"No axis names are available. Make sure you are using `pl.core_map`\"\n-        \" with a `plgpu.GPUMesh`.\"\n+        \" with a `plgpu.Mesh`.\"\n     )\n   if not axis_names or axis_name not in axis_names.cluster:\n     raise LookupError(\n@@ -2066,7 +2066,7 @@ def _axis_index_rule(ctx: LoweringRuleContext, *, axis_name: Hashable):\n   if gpu_axis_names is None and not jax_axis_names:\n     raise LookupError(\n         \"No axis names are available. Make sure you are using `pl.core_map`\"\n-        \" with a `plgpu.GPUMesh` or an appropriate JAX device mesh.\"\n+        \" with a `plgpu.Mesh` or an appropriate JAX device mesh.\"\n     )\n   if axis_name not in itertools.chain((gpu_axis_names or ()), jax_axis_names):\n     raise LookupError(\ndiff --git a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\nindex 72e6f96c125a..ef1ba37f0f5c 100644\n--- a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n+++ b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n@@ -63,9 +63,9 @@ def pallas_call_lowering(\n   mgpu.dialect.register_dialect(ctx.module_context.context)  # pytype: disable=attribute-error\n \n   if \"mosaic_gpu\" in compiler_params:\n-    params = cast(gpu_core.GPUCompilerParams, compiler_params[\"mosaic_gpu\"])\n+    params = cast(gpu_core.CompilerParams, compiler_params[\"mosaic_gpu\"])\n   else:\n-    params = gpu_core.GPUCompilerParams()\n+    params = gpu_core.CompilerParams()\n \n   jax_mesh = None\n   axis_context = ctx.module_context.axis_context\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 0e9319972949..af9d4138cfbb 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -56,7 +56,7 @@\n \n \n def _check_ref(\n-    aval: object, name: str, memory_space: gpu_core.GPUMemorySpace\n+    aval: object, name: str, memory_space: gpu_core.MemorySpace\n ) -> None:\n   if not isinstance(aval, state_types.AbstractRef):\n     raise TypeError(f\"{name} must be a reference, got {aval}\")\n@@ -1200,19 +1200,19 @@ def _tcgen05_mma_abstract_eval(acc, a, b, barrier, accumulate,\n                                collective_axis):\n   del (accumulate, transforms_leaves, a_transforms_tree, b_transforms_tree)\n \n-  if acc.memory_space != gpu_core.GPUMemorySpace.TMEM:\n+  if acc.memory_space != gpu_core.MemorySpace.TMEM:\n     raise ValueError(\"Accumulator must be a TMEM Ref.\")\n-  if a.memory_space not in (gpu_core.GPUMemorySpace.SMEM,\n-                            gpu_core.GPUMemorySpace.TMEM):\n+  if a.memory_space not in (gpu_core.MemorySpace.SMEM,\n+                            gpu_core.MemorySpace.TMEM):\n     raise ValueError(\"LHS must be a TMEM/SMEM Ref.\")\n-  if b.memory_space != gpu_core.GPUMemorySpace.SMEM:\n+  if b.memory_space != gpu_core.MemorySpace.SMEM:\n     raise ValueError(\"RHS must be an SMEM Ref.\")\n \n   if collective_axis is not None:\n     if not acc.collective:\n       raise ValueError(\n           \"Accumulator Ref must be collective if collective_axis is set.\")\n-    if a.memory_space == gpu_core.GPUMemorySpace.TMEM and not a.collective:\n+    if a.memory_space == gpu_core.MemorySpace.TMEM and not a.collective:\n       raise ValueError(\n           \"LHS TMEM Ref must be collective if collective_axis is set.\")\n \ndiff --git a/jax/_src/pallas/pallas_call.py b/jax/_src/pallas/pallas_call.py\nindex 2d27bd3cc485..964709b4c915 100644\n--- a/jax/_src/pallas/pallas_call.py\n+++ b/jax/_src/pallas/pallas_call.py\n@@ -1551,7 +1551,7 @@ def pallas_call(\n       backend-specific dataclass\n       (:class:`jax.experimental.pallas.tpu.TPUCompilerParams`,\n       :class:`jax.experimental.pallas.triton.TritonCompilerParams`,\n-      :class:`jax.experimental.pallas.mosaic_gpu.GPUCompilerParams`) or a dict\n+      :class:`jax.experimental.pallas.mosaic_gpu.CompilerParams`) or a dict\n       mapping backend name to the corresponding platform-specific dataclass.\n     backend: Optional string literal one of  ``\"mosaic_tpu\"``, ``\"triton\"`` or\n       ``\"mosaic_gpu\"`` determining the backend to be used. None means let Pallas\ndiff --git a/jax/experimental/pallas/mosaic_gpu.py b/jax/experimental/pallas/mosaic_gpu.py\nindex a7d8c3e34223..cc0e185e296a 100644\n--- a/jax/experimental/pallas/mosaic_gpu.py\n+++ b/jax/experimental/pallas/mosaic_gpu.py\n@@ -19,10 +19,10 @@\n \n from jax._src.pallas.mosaic_gpu.core import Barrier as Barrier\n from jax._src.pallas.mosaic_gpu.core import ClusterBarrier as ClusterBarrier\n-from jax._src.pallas.mosaic_gpu.core import GPUBlockSpec as GPUBlockSpec\n-from jax._src.pallas.mosaic_gpu.core import GPUCompilerParams as GPUCompilerParams\n-from jax._src.pallas.mosaic_gpu.core import GPUMesh as GPUMesh\n-from jax._src.pallas.mosaic_gpu.core import GPUMemorySpace as GPUMemorySpace\n+from jax._src.pallas.mosaic_gpu.core import BlockSpec as BlockSpec\n+from jax._src.pallas.mosaic_gpu.core import CompilerParams as CompilerParams\n+from jax._src.pallas.mosaic_gpu.core import Mesh as Mesh\n+from jax._src.pallas.mosaic_gpu.core import MemorySpace as MemorySpace\n from jax._src.pallas.mosaic_gpu.core import kernel as kernel\n from jax._src.pallas.mosaic_gpu.core import PeerMemRef as PeerMemRef\n from jax._src.pallas.mosaic_gpu.core import RefUnion as RefUnion\n@@ -63,9 +63,15 @@\n from jax.experimental.mosaic.gpu.core import LoweringSemantics as LoweringSemantics\n \n \n-#: Alias of :data:`jax.experimental.pallas.mosaic_gpu.GPUMemorySpace.GMEM`.\n-GMEM = GPUMemorySpace.GMEM\n-#: Alias of :data:`jax.experimental.pallas.mosaic_gpu.GPUMemorySpace.SMEM`.\n-SMEM = GPUMemorySpace.SMEM\n-#: Alias of :data:`jax.experimental.pallas.mosaic_gpu.GPUMemorySpace.TMEM`.\n-TMEM = GPUMemorySpace.TMEM\n+#: Alias of :data:`jax.experimental.pallas.mosaic_gpu.MemorySpace.GMEM`.\n+GMEM = MemorySpace.GMEM\n+#: Alias of :data:`jax.experimental.pallas.mosaic_gpu.MemorySpace.SMEM`.\n+SMEM = MemorySpace.SMEM\n+#: Alias of :data:`jax.experimental.pallas.mosaic_gpu.MemorySpace.TMEM`.\n+TMEM = MemorySpace.TMEM\n+\n+# TODO(slebedev): Deprecate and remove these aliases.\n+GPUBlockSpec = BlockSpec\n+GPUCompilerParams = CompilerParams\n+GPUMemorySpace = MemorySpace\n+GPUMesh = Mesh\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex a100aa96faba..6da468f2cc3e 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -306,7 +306,7 @@ def entry(q_ref, k_ref, v_ref, out_ref, lse_ref):\n       grid_names=(\"batch\", \"q_seq\", \"heads\"),\n       num_threads=3,\n       thread_name=\"wg\",\n-      compiler_params=plgpu.GPUCompilerParams(approx_math=True),\n+      compiler_params=plgpu.CompilerParams(approx_math=True),\n   )(q, k, v)\n \n   if save_residuals:\n@@ -451,11 +451,11 @@ def compute_dq(acc_ref):\n         manual_consumed_barriers=True,\n         compute_context=_compute_thread,\n         in_specs=[\n-            plgpu.GPUBlockSpec(  # k\n+            plgpu.BlockSpec(  # k\n                 block_shape=(block_kv, head_dim),\n                 index_map=lambda i: (i, 0),\n                 transforms=[tiling, swizzle]),\n-            plgpu.GPUBlockSpec(  # v\n+            plgpu.BlockSpec(  # v\n                 block_shape=(block_kv, head_dim),\n                 index_map=lambda i: (i, 0),\n                 transforms=[tiling, swizzle]),\n@@ -558,16 +558,16 @@ def compute_dk(acc_ref):\n       manual_consumed_barriers=True,\n       compute_context=_compute_thread,\n       in_specs=[\n-          plgpu.GPUBlockSpec(  # q\n+          plgpu.BlockSpec(  # q\n               block_shape=(block_q, head_dim),\n               index_map=lambda i: (i, 0),\n               transforms=[tiling, swizzle]),\n-          plgpu.GPUBlockSpec(  # do\n+          plgpu.BlockSpec(  # do\n               block_shape=(block_q, head_dim),\n               index_map=lambda i: (i, 0),\n               transforms=[tiling, swizzle]),\n-          plgpu.GPUBlockSpec(block_shape=(block_q,), index_map=lambda i: (i,)),\n-          plgpu.GPUBlockSpec(block_shape=(block_q,), index_map=lambda i: (i,))\n+          plgpu.BlockSpec(block_shape=(block_q,), index_map=lambda i: (i,)),\n+          plgpu.BlockSpec(block_shape=(block_q,), index_map=lambda i: (i,))\n       ])\n     q_ref = q_ref.at[batch, :, q_head, :]\n     do_ref = do_ref.at[batch, :, q_head, :]\n@@ -589,7 +589,7 @@ def compute_dk(acc_ref):\n           (q_scratch, do_scratch, lse_scratch, delta_scratch),  # type: ignore\n           (plgpu.Barrier(1, num_barriers=compute_wgs),) * 4  # type: ignore\n       ],\n-      compiler_params=plgpu.GPUCompilerParams(approx_math=True),\n+      compiler_params=plgpu.CompilerParams(approx_math=True),\n       grid=(batch_size, num_q_tiles, num_q_heads),\n       grid_names=(\"batch\", \"q_seq\", \"heads\"),\n       num_threads=compute_wgs + 1,\n@@ -610,7 +610,7 @@ def compute_dk(acc_ref):\n         (k_scratch, v_scratch),  # type: ignore\n         (plgpu.Barrier(1, num_barriers=compute_wgs),) * 2  # type: ignore\n   ],\n-    compiler_params=plgpu.GPUCompilerParams(approx_math=True),\n+    compiler_params=plgpu.CompilerParams(approx_math=True),\n     grid=(batch_size, num_kv_tiles, num_q_heads),\n     grid_names=(\"batch\", \"kv_seq\", \"heads\"),\n     num_threads=compute_wgs + 1,\n@@ -746,10 +746,10 @@ def compute_pv(acc_ref):\n         manual_consumed_barriers=True,\n         compute_context=_compute_thread,\n         in_specs=[\n-            plgpu.GPUBlockSpec(  # k\n+            plgpu.BlockSpec(  # k\n                 block_shape=(block_kv, head_dim),\n                 index_map=lambda i: (i, 0)),\n-            plgpu.GPUBlockSpec(  # v\n+            plgpu.BlockSpec(  # v\n                 block_shape=(block_kv, head_dim),\n                 index_map=lambda i: (i, 0)),\n         ],\n@@ -758,7 +758,7 @@ def compute_pv(acc_ref):\n     k_ref = k_ref.at[batch, :, kv_head, :]\n     v_ref = v_ref.at[batch, :, kv_head, :]\n     pipeline(k_ref, v_ref)\n-  mesh = plgpu.GPUMesh(\n+  mesh = plgpu.Mesh(\n       grid=(batch_size, num_q_tiles, num_q_heads),\n       grid_names=(\"batch\", \"q_seq\", \"heads\"),\n       num_threads=3,\n@@ -769,7 +769,7 @@ def run(refs):\n \n     @pl.core_map(\n         mesh,\n-        compiler_params=plgpu.GPUCompilerParams(\n+        compiler_params=plgpu.CompilerParams(\n             approx_math=True, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n         ),\n     )\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 68aeea5a03e8..b10bc0f390b0 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -94,14 +94,14 @@ def skip_if_wg_semantics(self):\n \n   def kernel(self, *args, **kwargs):\n     compiler_params = dataclasses.replace(\n-        kwargs.pop(\"compiler_params\", plgpu.GPUCompilerParams()),\n+        kwargs.pop(\"compiler_params\", plgpu.CompilerParams()),\n         lowering_semantics=self.LOWERING_SEMANTICS,\n     )\n     return plgpu.kernel(*args, compiler_params=compiler_params, **kwargs)\n \n   def pallas_call(self, *args, **kwargs):\n     compiler_params = dataclasses.replace(\n-        kwargs.pop(\"compiler_params\", plgpu.GPUCompilerParams()),\n+        kwargs.pop(\"compiler_params\", plgpu.CompilerParams()),\n         lowering_semantics=self.LOWERING_SEMANTICS,\n     )\n     return pl.pallas_call(*args, compiler_params=compiler_params, **kwargs)\n@@ -153,7 +153,7 @@ def test_unary_op(self, op, approx_math):\n     @functools.partial(\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct([256], dtype),\n-        compiler_params=plgpu.GPUCompilerParams(approx_math=approx_math),\n+        compiler_params=plgpu.CompilerParams(approx_math=approx_math),\n     )\n     def kernel(x_ref, o_ref):\n       o_ref[...] = op(x_ref[...])\n@@ -296,12 +296,13 @@ def kernel(x_ref, o_ref, scratch_ref):\n \n   @parameterized.product(max_concurrent_steps=[1, 2, 3, 4, 16])\n   def test_add_one_grid_pipelined(self, max_concurrent_steps):\n+\n     @functools.partial(\n         self.pallas_call,\n         in_specs=[pl.BlockSpec((128, 16), lambda i, j: (i, j))],\n         out_specs=pl.BlockSpec((128, 16), lambda i, j: (i, j)),\n         out_shape=jax.ShapeDtypeStruct([128 * 2, 64], jnp.float32),\n-        compiler_params=plgpu.GPUCompilerParams(\n+        compiler_params=plgpu.CompilerParams(\n             dimension_semantics=[\"parallel\", \"sequential\"],\n             max_concurrent_steps=max_concurrent_steps,\n         ),\n@@ -314,11 +315,12 @@ def kernel(x_ref, o_ref):\n     np.testing.assert_array_equal(kernel(x), x + 1.0)\n \n   def test_add_one_grid_pipelined_program_id(self):\n+\n     @functools.partial(\n         self.pallas_call,\n         out_specs=pl.BlockSpec((16, 16), lambda i, j: (i, j)),\n         out_shape=jax.ShapeDtypeStruct([16, 64], jnp.int32),\n-        compiler_params=plgpu.GPUCompilerParams(\n+        compiler_params=plgpu.CompilerParams(\n             dimension_semantics=[\"parallel\", \"sequential\"],\n             max_concurrent_steps=2,\n         ),\n@@ -339,7 +341,7 @@ def test_add_one_grid_pipelined_sequential_invariant_output(self):\n         in_specs=[pl.BlockSpec((32, 16), lambda i, j: (i, j))],\n         out_specs=pl.BlockSpec((32, 16), lambda i, j: (i, 0)),\n         out_shape=jax.ShapeDtypeStruct([32 * 2, 64], jnp.float32),\n-        compiler_params=plgpu.GPUCompilerParams(\n+        compiler_params=plgpu.CompilerParams(\n             dimension_semantics=[\"parallel\", \"sequential\"],\n             max_concurrent_steps=2,\n         ),\n@@ -634,7 +636,7 @@ def test_gmem_to_smem_with_multiple_smem_indexers_and_transforms(self):\n         grid=(4, 4),\n         out_shape=jax.ShapeDtypeStruct((256, 128), jnp.int32),\n         in_specs=(\n-            plgpu.GPUBlockSpec(\n+            plgpu.BlockSpec(\n                 block_shape=(128, 128),\n                 index_map=lambda i, j: (i, j),\n                 memory_space=plgpu.SMEM,\n@@ -645,7 +647,7 @@ def test_gmem_to_smem_with_multiple_smem_indexers_and_transforms(self):\n             ),\n         ),\n         out_specs=(\n-            plgpu.GPUBlockSpec(\n+            plgpu.BlockSpec(\n                 block_shape=(64, 32),\n                 index_map=lambda i, j: (i, j),\n                 memory_space=plgpu.SMEM,\n@@ -696,7 +698,7 @@ def kernel(x_ref, o_ref, barrier_ref):\n         plgpu.wait_smem_to_gmem(0)\n \n     in_spec = pl.BlockSpec(memory_space=plgpu.GMEM)\n-    out_spec = plgpu.GPUBlockSpec(\n+    out_spec = plgpu.BlockSpec(\n         transforms=(\n             plgpu.TilingTransform((8, 32)),\n             plgpu.SwizzleTransform(128),\n@@ -727,7 +729,7 @@ def body(tmp_ref):\n       pl.run_scoped(body, plgpu.SMEM((128, 128), jnp.float32, transforms=ts))\n \n     in_spec = pl.BlockSpec(memory_space=plgpu.GMEM)\n-    out_spec = plgpu.GPUBlockSpec(transforms=ts, memory_space=plgpu.SMEM)\n+    out_spec = plgpu.BlockSpec(transforms=ts, memory_space=plgpu.SMEM)\n     f = self.pallas_call(\n         kernel,\n         out_shape=jax.ShapeDtypeStruct([128, 128], jnp.float32),\n@@ -767,7 +769,7 @@ def kernel(x_ref, o_ref, barrier_ref):\n         plgpu.barrier_wait(barrier_ref)\n \n     in_spec = pl.BlockSpec(memory_space=plgpu.GMEM)\n-    out_spec = plgpu.GPUBlockSpec(\n+    out_spec = plgpu.BlockSpec(\n         transforms=(\n             plgpu.TilingTransform((8, 32)),\n             plgpu.TransposeTransform((0, 2, 1, 3, 4)),\n@@ -797,7 +799,7 @@ def test_load_to_strided_layout_with_indexing(self, src_memory_space, layout):\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct([2, 128], jnp.float32),\n         in_specs=[pl.BlockSpec(memory_space=src_memory_space)],\n-        out_specs=plgpu.GPUBlockSpec(memory_space=plgpu.SMEM),\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.SMEM),\n     )\n     def kernel(x_ref, o_ref):\n       for i in range(2):\n@@ -818,7 +820,7 @@ def kernel(x_ref, o_ref, barrier_ref):\n         plgpu.barrier_wait(barrier_ref)\n \n     in_spec = pl.BlockSpec(memory_space=plgpu.GMEM)\n-    out_spec = plgpu.GPUBlockSpec(memory_space=plgpu.SMEM)\n+    out_spec = plgpu.BlockSpec(memory_space=plgpu.SMEM)\n     f = self.pallas_call(\n         kernel,\n         out_shape=jax.ShapeDtypeStruct([2, 64, 2, 128], jnp.float32),\n@@ -920,7 +922,7 @@ def test_print_wgmma_tiled_layout(self):\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct(shape, jnp.float32),\n         in_specs=[\n-            plgpu.GPUBlockSpec(\n+            plgpu.BlockSpec(\n                 transforms=(\n                     plgpu.TilingTransform((8, 32)),\n                     plgpu.SwizzleTransform(128),\n@@ -1013,10 +1015,11 @@ def kernel(x_ref, o_ref):\n     np.testing.assert_array_equal(kernel(x), x)\n \n   def test_load_scalar(self):\n+\n     @functools.partial(\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct((128,), jnp.int32),\n-        in_specs=[plgpu.GPUBlockSpec(memory_space=plgpu.GMEM)],\n+        in_specs=[plgpu.BlockSpec(memory_space=plgpu.GMEM)],\n     )\n     def kernel(x_ref, o_ref):\n       o_ref[...] = jnp.broadcast_to(x_ref[10], (128,))\n@@ -1135,7 +1138,7 @@ def kernel(o_ref):\n   def test_swizzled_blockspec_shapes(self):\n     self.skip_if_wg_semantics()\n \n-    spec = plgpu.GPUBlockSpec(\n+    spec = plgpu.BlockSpec(\n         (128, 64),\n         lambda *i: i,\n         transforms=(\n@@ -1344,7 +1347,7 @@ def test_tile_slicing(self):\n     self.skip_if_wg_semantics()\n \n     shape = (256, 128)\n-    block_spec = plgpu.GPUBlockSpec(\n+    block_spec = plgpu.BlockSpec(\n         transforms=(plgpu.TilingTransform((8, 64)), plgpu.SwizzleTransform(128))\n     )\n     @functools.partial(\n@@ -1375,8 +1378,8 @@ def kernel(a_ref, b_ref):\n     a = np.zeros((64, 64), dtype=jnp.float32)\n     b = self.pallas_call(\n         kernel,\n-        in_specs=[plgpu.GPUBlockSpec(memory_space=plgpu.GMEM)],\n-        out_specs=plgpu.GPUBlockSpec(memory_space=plgpu.GMEM),\n+        in_specs=[plgpu.BlockSpec(memory_space=plgpu.GMEM)],\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.GMEM),\n         input_output_aliases={0: 0},\n         out_shape=a,\n     )(a)\n@@ -1395,7 +1398,7 @@ def rotate(src, dst):\n       dst[lower, left] = src[lower, right]\n \n     x = jnp.arange(128 * 128).astype(jnp.float16).reshape(128, 128)\n-    spec = plgpu.GPUBlockSpec(\n+    spec = plgpu.BlockSpec(\n         transforms=(plgpu.TilingTransform((8, 64)), plgpu.SwizzleTransform(128))\n     )\n     f = self.pallas_call(rotate, out_shape=x, in_specs=[spec], out_specs=spec)\n@@ -1472,7 +1475,7 @@ def kernel(x_ref, o_ref):\n       y = self.pallas_call(\n           kernel,\n           out_shape=jax.ShapeDtypeStruct([256], jnp.float32),\n-          compiler_params=plgpu.GPUCompilerParams(\n+          compiler_params=plgpu.CompilerParams(\n               profile_space=16, profile_dir=tmpdir\n           ),\n       )(x)\n@@ -1836,11 +1839,12 @@ def test_fori_loop_accumulator(self, force_while):\n       transforms = (plgpu.TilingTransform((8, 64)), plgpu.SwizzleTransform(128))\n     else:\n       transforms = ()\n+\n     @functools.partial(\n         self.pallas_call,\n-        in_specs=[plgpu.GPUBlockSpec((64, 64), transforms=transforms)],\n+        in_specs=[plgpu.BlockSpec((64, 64), transforms=transforms)],\n         out_shape=jax.ShapeDtypeStruct((64, 64), jnp.float16),\n-        out_specs=plgpu.GPUBlockSpec((64, 64)),\n+        out_specs=plgpu.BlockSpec((64, 64)),\n     )\n     def kernel(i_ref, o_ref):\n       def scope(acc_ref):\n@@ -1907,7 +1911,7 @@ def _epilogue():\n     )\n \n     if self.LOWERING_SEMANTICS == plgpu.LoweringSemantics.Lane:\n-      lhs_spec = plgpu.GPUBlockSpec(\n+      lhs_spec = plgpu.BlockSpec(\n           lhs_spec.block_shape,\n           lhs_spec.index_map,\n           transforms=(\n@@ -1915,7 +1919,7 @@ def _epilogue():\n               plgpu.SwizzleTransform(128),\n           ),\n       )\n-      rhs_spec = plgpu.GPUBlockSpec(\n+      rhs_spec = plgpu.BlockSpec(\n           rhs_spec.block_shape,\n           rhs_spec.index_map,\n           transforms=(\n@@ -1923,7 +1927,7 @@ def _epilogue():\n               plgpu.SwizzleTransform(128),\n           ),\n       )\n-      out_spec = plgpu.GPUBlockSpec(\n+      out_spec = plgpu.BlockSpec(\n           out_spec.block_shape,\n           out_spec.index_map,\n           transforms=(\n@@ -1939,7 +1943,7 @@ def _epilogue():\n         out_shape=jax.ShapeDtypeStruct((m, n), jnp.float16),\n         scratch_shapes=[plgpu.ACC((tile_m, tile_n), jnp.float32)],\n         grid=(grid_m, grid_n, grid_k),\n-        compiler_params=plgpu.GPUCompilerParams(\n+        compiler_params=plgpu.CompilerParams(\n             dimension_semantics=[\"parallel\", \"parallel\", \"sequential\"],\n             max_concurrent_steps=2,\n             delay_release=1,\n@@ -1980,7 +1984,7 @@ def scope(acc_ref):\n     res = self.pallas_call(\n         kernel,\n         in_specs=[\n-            plgpu.GPUBlockSpec(\n+            plgpu.BlockSpec(\n                 (64, 128),\n                 lambda i, j: (i, j),\n                 transforms=(\n@@ -1988,13 +1992,13 @@ def scope(acc_ref):\n                     plgpu.SwizzleTransform(128),\n                 ),\n             ),\n-            plgpu.GPUBlockSpec(\n+            plgpu.BlockSpec(\n                 b_shape,\n                 lambda *i: i,\n                 transforms=(*rhs_transforms, plgpu.SwizzleTransform(128)),\n             ),\n         ],\n-        out_specs=plgpu.GPUBlockSpec((64, 192), lambda *i: i),\n+        out_specs=plgpu.BlockSpec((64, 192), lambda *i: i),\n         out_shape=jax.ShapeDtypeStruct((64, 192), jnp.float32),\n         grid=(1, 1),\n     )(a, b)\n@@ -2019,8 +2023,8 @@ def scope(acc_ref):\n     res = self.pallas_call(\n         kernel,\n         in_specs=[\n-            plgpu.GPUBlockSpec(transforms=transforms),\n-            plgpu.GPUBlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n         ],\n         out_shape=jax.ShapeDtypeStruct((64, 192), jnp.float32),\n     )(a, b)\n@@ -2044,9 +2048,9 @@ def scope(acc_ref):\n     res = self.pallas_call(\n         kernel,\n         in_specs=[\n-            plgpu.GPUBlockSpec(transforms=transforms),\n-            plgpu.GPUBlockSpec(transforms=transforms),\n-            plgpu.GPUBlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n         ],\n         out_shape=jax.ShapeDtypeStruct((64, 192), jnp.float16),\n     )(a, b, i)\n@@ -2073,8 +2077,8 @@ def scope(acc_ref):\n     res = self.pallas_call(\n         kernel,\n         in_specs=[\n-            plgpu.GPUBlockSpec(transforms=transforms),\n-            plgpu.GPUBlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n         ],\n         out_shape=jax.ShapeDtypeStruct((64, 192), jnp.float32),\n     )(a, b)\n@@ -2104,14 +2108,10 @@ def scope(acc_ref):\n     res = self.pallas_call(\n         kernel,\n         in_specs=[\n-            plgpu.GPUBlockSpec(\n-                (64, 128), lambda *ij: ij, transforms=transforms\n-            ),\n-            plgpu.GPUBlockSpec(\n-                (128, 128), lambda *ij: ij, transforms=transforms\n-            ),\n+            plgpu.BlockSpec((64, 128), lambda *ij: ij, transforms=transforms),\n+            plgpu.BlockSpec((128, 128), lambda *ij: ij, transforms=transforms),\n         ],\n-        out_specs=plgpu.GPUBlockSpec((64, 128), lambda *ij: ij),\n+        out_specs=plgpu.BlockSpec((64, 128), lambda *ij: ij),\n         out_shape=jax.ShapeDtypeStruct((64, 128), jnp.float32),\n         grid=(1, 1),\n     )(a, b)\n@@ -2129,7 +2129,7 @@ def test_load_to_wgmma_row_col_layout_with_indexing(self, src_memory_space, layo\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct([2, m], jnp.float32),\n         in_specs=[pl.BlockSpec(memory_space=src_memory_space)],\n-        out_specs=plgpu.GPUBlockSpec(memory_space=plgpu.SMEM),\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.SMEM),\n     )\n     def kernel(x_ref, o_ref):\n       for i in range(2):\n@@ -2175,14 +2175,14 @@ def compute(acc_ref):\n         out_shape=jax.ShapeDtypeStruct([m, n], jnp.float32),\n         in_specs=(\n             pl.BlockSpec(memory_space=src_memory_space),\n-            plgpu.GPUBlockSpec(\n+            plgpu.BlockSpec(\n                 transforms=(\n                     plgpu.TilingTransform((8, 64)),\n                     plgpu.SwizzleTransform(128),\n                 ),\n             ),\n         ),\n-        out_specs=plgpu.GPUBlockSpec(memory_space=plgpu.SMEM),\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.SMEM),\n     )\n \n     out_ref = (\n@@ -2294,12 +2294,10 @@ def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref,\n     f = self.pallas_call(\n         kernel,\n         in_specs=(\n-            plgpu.GPUBlockSpec(transforms=transforms,\n-                               memory_space=plgpu.SMEM),\n-            plgpu.GPUBlockSpec(transforms=transforms,\n-                               memory_space=plgpu.SMEM),\n+            plgpu.BlockSpec(transforms=transforms, memory_space=plgpu.SMEM),\n+            plgpu.BlockSpec(transforms=transforms, memory_space=plgpu.SMEM),\n         ),\n-        out_specs=plgpu.GPUBlockSpec(memory_space=plgpu.GMEM),\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.GMEM),\n         out_shape=jax.ShapeDtypeStruct(shape, dtype),\n         scratch_shapes=scratch_shapes,\n     )\n@@ -2511,12 +2509,12 @@ def kernel(x_gmem, o_gmem):\n         plgpu.emit_pipeline(\n             kernel_body,\n             in_specs=[\n-                plgpu.GPUBlockSpec(\n+                plgpu.BlockSpec(\n                     (64, 64), lambda i: (0, i), transforms=transforms\n                 )\n             ],\n             out_specs=[\n-                plgpu.GPUBlockSpec(\n+                plgpu.BlockSpec(\n                     (64, 64), lambda i: (0, i), transforms=transforms\n                 )\n             ],\n@@ -2705,10 +2703,10 @@ def kernel_body(_, a_smem, b_smem):\n       plgpu.emit_pipeline(\n           kernel_body,\n           in_specs=[\n-              plgpu.GPUBlockSpec(\n+              plgpu.BlockSpec(\n                   (tile_m, tile_k), lambda k: (pid_m, k), transforms=transforms\n               ),\n-              plgpu.GPUBlockSpec(\n+              plgpu.BlockSpec(\n                   (tile_k, tile_n), lambda k: (k, pid_n), transforms=transforms\n               ),\n           ],\n@@ -2729,7 +2727,7 @@ def kernel_body(_, a_smem, b_smem):\n             pl.BlockSpec(memory_space=plgpu.GMEM),\n             pl.BlockSpec(memory_space=plgpu.GMEM),\n         ],\n-        out_specs=plgpu.GPUBlockSpec(\n+        out_specs=plgpu.BlockSpec(\n             (tile_m, tile_n), lambda m, n: (m, n), transforms=transforms\n         ),\n         out_shape=jax.ShapeDtypeStruct((m, n), jnp.float16),\n@@ -2794,7 +2792,7 @@ def body(*gmem_refs):\n             jax.ShapeDtypeStruct((m, n), jnp.float16),\n             jax.ShapeDtypeStruct((blk_m, blk_n), jnp.float16),\n         ),\n-        compiler_params=plgpu.GPUCompilerParams(approx_math=True),\n+        compiler_params=plgpu.CompilerParams(approx_math=True),\n         grid=(1,),\n         grid_names=(\"_\",),\n         num_threads=3,\n@@ -2839,7 +2837,7 @@ def pipeline(*gmem_refs):\n     kernel = self.kernel(\n         pipeline,\n         out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32),\n-        compiler_params=plgpu.GPUCompilerParams(approx_math=True),\n+        compiler_params=plgpu.CompilerParams(approx_math=True),\n         grid=(1,),\n         grid_names=(\"_\",),\n         num_threads=num_compute_wgs + 1,\n@@ -2858,7 +2856,7 @@ def test_carry_accumulate(self, m=256, n=256, num_compute_wgs=2):\n         scratch_shapes=[\n             plgpu.SMEM((blk_m, blk_n), jnp.float32),\n         ],\n-        compiler_params=plgpu.GPUCompilerParams(approx_math=True),\n+        compiler_params=plgpu.CompilerParams(approx_math=True),\n         grid=(1,),\n         grid_names=(\"_\",),\n         num_threads=num_compute_wgs + 1,\n@@ -3176,11 +3174,12 @@ class CoreMapWGTest(\n class PrettyPrintingTest(PallasTest):\n \n   def test_load(self):\n+\n     @functools.partial(\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct([2, 128], jnp.float32),\n         in_specs=[pl.BlockSpec(memory_space=plgpu.GMEM)],\n-        out_specs=plgpu.GPUBlockSpec(memory_space=plgpu.SMEM),\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.SMEM),\n     )\n     def kernel(x_ref, o_ref):\n       for i in range(2):\n@@ -3228,8 +3227,8 @@ def test_wgmma(self):\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct((64, 192), jnp.float32),\n         in_specs=[\n-            plgpu.GPUBlockSpec(transforms=transforms),\n-            plgpu.GPUBlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n         ],\n     )\n     def kernel(a_ref, b_ref, o_ref):\n@@ -3348,9 +3347,13 @@ def kernel(l_ref, r_ref, o_ref):\n       def compute(_, l_smem, r_smem, o_smem):\n         o_smem[...] = l_smem[...] + r_smem[...]\n       r = lax.axis_index(\"rows\")\n-      block = plgpu.GPUBlockSpec(\n-          (row_block, col_block), lambda c: (r, c),\n-          transforms=(plgpu.TilingTransform((8, 32)), plgpu.SwizzleTransform(64)),\n+      block = plgpu.BlockSpec(\n+          (row_block, col_block),\n+          lambda c: (r, c),\n+          transforms=(\n+              plgpu.TilingTransform((8, 32)),\n+              plgpu.SwizzleTransform(64),\n+          ),\n       )\n       plgpu.emit_pipeline(\n           compute,\n@@ -3420,9 +3423,19 @@ def do_wgmma(acc_ref):\n       plgpu.emit_pipeline(\n           compute,\n           grid=(l_ref.shape[1] // k_block,),\n-          in_specs=[plgpu.GPUBlockSpec((m_block, k_block), lambda k: (m, k), transforms=lo_transforms),\n-                    plgpu.GPUBlockSpec((k_block, n_block), lambda k: (k, n), transforms=r_transforms)],\n-          out_specs=[plgpu.GPUBlockSpec((m_block, n_block), lambda k: (m, n), transforms=lo_transforms)],\n+          in_specs=[\n+              plgpu.BlockSpec(\n+                  (m_block, k_block), lambda k: (m, k), transforms=lo_transforms\n+              ),\n+              plgpu.BlockSpec(\n+                  (k_block, n_block), lambda k: (k, n), transforms=r_transforms\n+              ),\n+          ],\n+          out_specs=[\n+              plgpu.BlockSpec(\n+                  (m_block, n_block), lambda k: (m, n), transforms=lo_transforms\n+              )\n+          ],\n       )(l_ref, r_ref, o_ref)\n \n     np.testing.assert_allclose(kernel(x, x), x @ x)\ndiff --git a/tests/pallas/ops_test.py b/tests/pallas/ops_test.py\nindex 3baa26e5efd7..bcda0ca9f71e 100644\n--- a/tests/pallas/ops_test.py\n+++ b/tests/pallas/ops_test.py\n@@ -292,7 +292,7 @@ def setUp(self):\n   def pallas_call(cls, *args, **kwargs):\n     if jtu.test_device_matches([\"cuda\"]) and use_mosaic_gpu:\n       assert plgpu_mgpu is not None\n-      compiler_params = plgpu_mgpu.GPUCompilerParams(\n+      compiler_params = plgpu_mgpu.CompilerParams(\n           lowering_semantics=plgpu_mgpu.LoweringSemantics.Warpgroup\n       )\n       kwargs[\"compiler_params\"] = compiler_params\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[mosaic_gpu] Removed `uniform=` from `async_copy`\n\nIt is redundant in the presence of `predicate=`.\n\nPiperOrigin-RevId: 761085680",
        "output": "```diff\nCommit: 3c2cb8026e4ba3df1ad9e943c90da0cf4cfed3f8\nDate: 2025-05-20T15:03:49Z\nURL: https://github.com/jax-ml/jax/commit/3c2cb8026e4ba3df1ad9e943c90da0cf4cfed3f8\nFiles changed: 5\nAdditions: +65, Deletions: -75\ndiff --git a/jax/experimental/mosaic/gpu/dialect_lowering.py b/jax/experimental/mosaic/gpu/dialect_lowering.py\nindex 320ae32607e9..20138bbe6fd4 100644\n--- a/jax/experimental/mosaic/gpu/dialect_lowering.py\n+++ b/jax/experimental/mosaic/gpu/dialect_lowering.py\n@@ -713,7 +713,6 @@ def _mgpu_async_load_op_lowering_rule(\n       gmem_slice=tuple(gmem_slice),\n       barrier=barrier.barrier_ref,\n       arrive=False,\n-      uniform=True,\n       swizzle=swizzle,\n       gmem_transform=transforms,\n       predicate=ctx.single_thread_per_warpgroup_predicate,\n@@ -755,7 +754,6 @@ def _mgpu_async_store_op_lowering_rule(\n       gmem_slice=tuple(gmem_slice),\n       swizzle=swizzle,\n       gmem_transform=transforms,\n-      uniform=True,\n       predicate=ctx.single_thread_per_warpgroup_predicate,\n       arrive=store_op.commit_group,\n   )\ndiff --git a/jax/experimental/mosaic/gpu/examples/flash_attention.py b/jax/experimental/mosaic/gpu/examples/flash_attention.py\nindex 071a4dec81fd..78ef1faddc59 100644\n--- a/jax/experimental/mosaic/gpu/examples/flash_attention.py\n+++ b/jax/experimental/mosaic/gpu/examples/flash_attention.py\n@@ -309,7 +309,7 @@ def start_kv_copy(slot, kv_seq_base, smem, gmem, barrier, transform):\n                 gmem_slice=(kv_head_idx, ds(kv_seq_base, blocks.kv)),\n                 gmem_transform=transform,\n                 barrier=barrier,\n-                uniform=False,\n+                predicate=None,\n                 swizzle=128,\n             )\n         def start_k_copy(slot, kv_seq_base):\n@@ -403,7 +403,7 @@ def kv_copy_init(slot, kv_seq_base):\n               gmem_transform=t,\n               barrier=barriers[slot],\n               arrive=False,\n-              uniform=False,\n+              predicate=None,\n               swizzle=128,\n           )\n \ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul.py b/jax/experimental/mosaic/gpu/examples/matmul.py\nindex a5dd29e0dc4d..5c8363fa8b27 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul.py\n@@ -206,7 +206,7 @@ def fetch(slot, ki):\n       rhs_tma_tile_bytes = int(np.prod(block_tiling.kn) * rhs_elem_bytes)\n       txcount = lhs_tma_tile_bytes + rhs_tma_tile_bytes\n       common_copy_args = dict(\n-          swizzle=swizzle, barrier=barrier, arrive=False, uniform=False,\n+          swizzle=swizzle, barrier=barrier, arrive=False, predicate=None,\n       )\n       with single_thread():\n         barrier.arrive_expect_tx(txcount)\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex 03363c1e365f..f771c8bc1ef1 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -114,7 +114,7 @@ def _tma_body(ki, _):\n             swizzle=swizzle,\n             barrier=full_barrier,\n             arrive=False,\n-            uniform=False,\n+            predicate=None,\n             collective=gpu.Dimension.x,\n             partitioned=0,  # Non-contracting dim is always 0.\n         )\ndiff --git a/jax/experimental/mosaic/gpu/launch_context.py b/jax/experimental/mosaic/gpu/launch_context.py\nindex 175dc8b0ac74..aaae007a67f0 100644\n--- a/jax/experimental/mosaic/gpu/launch_context.py\n+++ b/jax/experimental/mosaic/gpu/launch_context.py\n@@ -321,6 +321,10 @@ def finalize_size(self):\n         init_callback(self._alloc_op.result)\n \n \n+class _DefaultPredicate:\n+  pass\n+\n+\n @dataclasses.dataclass()\n class LaunchContext:\n   module: ir.Module\n@@ -506,12 +510,10 @@ def async_copy(\n       barrier: utils.BarrierRef | None = None,\n       swizzle: int | None = None,\n       arrive: bool | None = None,\n-      uniform: bool = True,\n       collective: Sequence[gpu.Dimension] | gpu.Dimension | None = None,\n       partitioned: int | None = None,\n-      predicate: (\n-          ir.Value | None\n-      ) = None,  # Should select 0 or 1 threads from the WG.\n+      # Should select 0 or 1 threads from the WG.\n+      predicate: ir.Value | None | _DefaultPredicate = _DefaultPredicate(),\n       reduction_op: ReductionOp | None = None,\n   ):\n     \"\"\"Initiates an async copy between GMEM and SMEM.\n@@ -553,8 +555,8 @@ def async_copy(\n           f\"Expected same element type, got {element_type} and\"\n           f\" {dst_ref_ty.element_type}\"\n       )\n-    if predicate is not None and not uniform:\n-      raise ValueError(\"Predicate can only be defined when uniform is True\")\n+    if isinstance(predicate, _DefaultPredicate):\n+      predicate = utils.single_thread_predicate(utils.ThreadSubset.WARPGROUP)\n     if not isinstance(gmem_transform, tuple):\n       gmem_transform = (gmem_transform,)\n \n@@ -756,13 +758,6 @@ def partition_dim(dim: int, idx: ir.Value, num_chunks: int):\n         arith.index_cast(i32, idx) for idx in reversed(dyn_base_indices)\n     ]\n \n-    uniform_ctx = (\n-        functools.partial(\n-            utils.single_thread, scope=utils.ThreadSubset.WARPGROUP)\n-        if uniform and predicate is None\n-        else contextlib.nullcontext\n-    )\n-\n     if max(slice_shape) > 256:\n       raise ValueError(\n           \"Async copies only support copying <=256 elements along each\"\n@@ -792,68 +787,65 @@ def partition_dim(dim: int, idx: ir.Value, num_chunks: int):\n           np.prod(slice_shape) * element_bitwidth * collective_size // 8, i32\n       )\n       barrier_ptr = barrier.get_ptr()\n-      with uniform_ctx():\n-        assert reduction_op is None\n-        if collective_size > 1 and partitioned is not None:\n-          if predicate is None:\n-            predicate = c(1, ir.IntegerType.get_signless(1))\n-          if arrive:\n-            first_block = arith.cmpi(\n-                arith.CmpIPredicate.eq, self.cluster_idx(collective), c(0, index),\n-            )\n-            arrive_predicate = arith.andi(predicate, first_block)\n-            nvvm.mbarrier_arrive_expect_tx_shared(\n-                barrier_ptr, transfer_bytes, predicate=arrive_predicate\n-            )\n-          rank = len(slice_shape)\n-          idx_operands = \",\".join(f\"${i}\" for i in range(4, 4 + rank))\n-          llvm.inline_asm(\n-              ir.Type.parse(\"!llvm.void\"),\n-              [predicate, smem_ptr, tma_desc, barrier_ptr, *rev_dyn_base_indices],\n-              f\"\"\"\n-              {{\n-              .reg .b32 mapped_addr;\n-              @$0 mapa.shared::cluster.u32 mapped_addr, $3, 0;\n-              @$0 cp.async.bulk.tensor.{rank}d.shared::cta.global.tile.mbarrier::complete_tx::bytes.cta_group::2\n-                                   [$1], [$2, {{{idx_operands}}}], [mapped_addr];\n-              }}\n-              \"\"\",\n-              \"b,r,l,r\" + \",r\" * rank,\n-              has_side_effects=True,\n+      assert reduction_op is None\n+      if collective_size > 1 and partitioned is not None:\n+        if predicate is None:\n+          predicate = c(1, ir.IntegerType.get_signless(1))\n+        if arrive:\n+          first_block = arith.cmpi(\n+              arith.CmpIPredicate.eq, self.cluster_idx(collective), c(0, index),\n           )\n-        else:\n-          if arrive:\n-            nvvm.mbarrier_arrive_expect_tx_shared(\n-                barrier_ptr, transfer_bytes, predicate=predicate\n-            )\n-          nvvm.cp_async_bulk_tensor_shared_cluster_global(\n-              smem_ptr, tma_desc, rev_dyn_base_indices, barrier_ptr, [],\n-              multicast_mask=multicast_mask, predicate=predicate\n+          arrive_predicate = arith.andi(predicate, first_block)\n+          nvvm.mbarrier_arrive_expect_tx_shared(\n+              barrier_ptr, transfer_bytes, predicate=arrive_predicate\n           )\n-    else:\n-      assert multicast_mask is None\n-      if reduction_op is not None:\n-        with uniform_ctx():\n-          if predicate is None:\n-            predicate = c(1, ir.IntegerType.get_signless(1))\n-          rank = len(slice_shape)\n-          idx_operands = \",\".join(f\"${i}\" for i in range(3, 3 + rank))\n-          llvm.inline_asm(\n+        rank = len(slice_shape)\n+        idx_operands = \",\".join(f\"${i}\" for i in range(4, 4 + rank))\n+        llvm.inline_asm(\n             ir.Type.parse(\"!llvm.void\"),\n-            [predicate,smem_ptr,tma_desc,*rev_dyn_base_indices],\n-            f\"@$0 cp.reduce.async.bulk.tensor.{rank}d.global.shared::cta.{reduction_op}.tile.bulk_group [$2,{{{idx_operands}}}], [$1];\",\n-            \"b,r,l\" + \",r\" * rank,\n+            [predicate, smem_ptr, tma_desc, barrier_ptr, *rev_dyn_base_indices],\n+            f\"\"\"\n+            {{\n+            .reg .b32 mapped_addr;\n+            @$0 mapa.shared::cluster.u32 mapped_addr, $3, 0;\n+            @$0 cp.async.bulk.tensor.{rank}d.shared::cta.global.tile.mbarrier::complete_tx::bytes.cta_group::2\n+                                  [$1], [$2, {{{idx_operands}}}], [mapped_addr];\n+            }}\n+            \"\"\",\n+            \"b,r,l,r\" + \",r\" * rank,\n             has_side_effects=True,\n-          )\n-          if arrive:\n-            nvvm.cp_async_bulk_commit_group()\n+        )\n       else:\n-        with uniform_ctx():\n-          nvvm.cp_async_bulk_tensor_global_shared_cta(\n-              tma_desc, smem_ptr, rev_dyn_base_indices, predicate=predicate\n+        if arrive:\n+          nvvm.mbarrier_arrive_expect_tx_shared(\n+              barrier_ptr, transfer_bytes, predicate=predicate\n           )\n-          if arrive:\n-            nvvm.cp_async_bulk_commit_group()\n+        nvvm.cp_async_bulk_tensor_shared_cluster_global(\n+            smem_ptr, tma_desc, rev_dyn_base_indices, barrier_ptr, [],\n+            multicast_mask=multicast_mask, predicate=predicate\n+        )\n+    else:\n+      assert multicast_mask is None\n+      if reduction_op is not None:\n+        if predicate is None:\n+          predicate = c(1, ir.IntegerType.get_signless(1))\n+        rank = len(slice_shape)\n+        idx_operands = \",\".join(f\"${i}\" for i in range(3, 3 + rank))\n+        llvm.inline_asm(\n+          ir.Type.parse(\"!llvm.void\"),\n+          [predicate,smem_ptr,tma_desc,*rev_dyn_base_indices],\n+          f\"@$0 cp.reduce.async.bulk.tensor.{rank}d.global.shared::cta.{reduction_op}.tile.bulk_group [$2,{{{idx_operands}}}], [$1];\",\n+          \"b,r,l\" + \",r\" * rank,\n+          has_side_effects=True,\n+        )\n+        if arrive:\n+          nvvm.cp_async_bulk_commit_group()\n+      else:\n+        nvvm.cp_async_bulk_tensor_global_shared_cta(\n+            tma_desc, smem_ptr, rev_dyn_base_indices, predicate=predicate\n+        )\n+        if arrive:\n+          nvvm.cp_async_bulk_commit_group()\n \n   def await_async_copy(\n       self, allow_groups: int, await_read_only: bool = False\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Only run relevant GPU tests in the optional GPU CI\n\nThis aims to prepare those tests to be used as a presubmit.\nWe can later add a more complete nightly configuration for testing.\n\nPiperOrigin-RevId: 761044847",
        "output": "```diff\nCommit: 414db00e807a4faeeba953fd4a61b4f5fe3d864c\nDate: 2025-05-20T12:51:14Z\nURL: https://github.com/jax-ml/jax/commit/414db00e807a4faeeba953fd4a61b4f5fe3d864c\nFiles changed: 1\nAdditions: +22, Deletions: -11\ndiff --git a/.github/workflows/bazel_optional_h100_b200.yml b/.github/workflows/bazel_optional_h100_b200.yml\nindex bde033361609..0c73b238505e 100644\n--- a/.github/workflows/bazel_optional_h100_b200.yml\n+++ b/.github/workflows/bazel_optional_h100_b200.yml\n@@ -49,19 +49,23 @@ jobs:\n             --run_under \"$(pwd)/build/parallel_accelerator_execute.sh\" \\\n             --test_output=errors \\\n             --test_env=JAX_ACCELERATOR_COUNT=1 \\\n-            --test_env=JAX_TESTS_PER_ACCELERATOR=32 \\\n+            --test_env=JAX_TESTS_PER_ACCELERATOR=8 \\\n             --strategy=TestRunner=local \\\n-            --local_test_jobs=32 \\\n-            --test_env=JAX_EXCLUDE_TEST_TARGETS=PmapTest.testSizeOverflow \\\n-            --test_tag_filters=-multiaccelerator \\\n+            --local_test_jobs=8 \\\n+            --test_env=JAX_EXCLUDE_TEST_TARGETS='PmapTest.testSizeOverflow|.*InterpretTest.*' \\\n             --test_env=TF_CPP_MIN_LOG_LEVEL=0 \\\n             --test_env=JAX_SKIP_SLOW_TESTS=true \\\n             --action_env=JAX_ENABLE_X64=\"1\" \\\n             --action_env=NCCL_DEBUG=WARN \\\n+            --flaky_test_attempts=1 \\\n+            --test_timeout=420 \\\n             --color=yes \\\n-            //tests:gpu_tests //tests:backend_independent_tests \\\n-            //tests/pallas:gpu_tests //tests/pallas:backend_independent_tests \\\n-            //tests/mosaic:gpu_tests //tests/mosaic:backend_independent_tests\n+            //tests:cudnn_fusion_test_gpu \\\n+            //tests:scaled_matmul_stablehlo_test_gpu \\\n+            //tests:fused_attention_stablehlo_test_gpu \\\n+            //tests:nn_test_gpu \\\n+            //tests/pallas:gpu_tests \\\n+            //tests/mosaic:gpu_tests\n   run_multiaccelerator_tests:\n     if: ${{ github.event.repository.fork == false && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || contains(github.event.pull_request.labels.*.name, 'CI Optional GPU Presubmit')) }}\n     runs-on: linux-x86-a3-8g-h100-8gpu\n@@ -86,13 +90,20 @@ jobs:\n             --test_output=errors \\\n             --strategy=TestRunner=local \\\n             --local_test_jobs=8 \\\n-            --test_env=JAX_EXCLUDE_TEST_TARGETS=PmapTest.testSizeOverflow \\\n+            --test_env=JAX_EXCLUDE_TEST_TARGETS='PmapTest.testSizeOverflow|.*InterpretTest.*' \\\n             --test_tag_filters=multiaccelerator \\\n             --test_env=TF_CPP_MIN_LOG_LEVEL=0 \\\n             --test_env=JAX_SKIP_SLOW_TESTS=true \\\n             --action_env=JAX_ENABLE_X64=\"1\" \\\n             --action_env=NCCL_DEBUG=WARN \\\n+            --flaky_test_attempts=1 \\\n             --color=yes \\\n-            //tests:gpu_tests //tests:backend_independent_tests \\\n-            //tests/pallas:gpu_tests //tests/pallas:backend_independent_tests \\\n-            //tests/mosaic:gpu_tests //tests/mosaic:backend_independent_tests\n\\ No newline at end of file\n+            //tests/mosaic:gpu_tests \\\n+            //tests/pallas:gpu_tests \\\n+            //tests:array_interoperability_test_gpu \\\n+            //tests:cudnn_fusion_test_gpu \\\n+            //tests:fused_attention_stablehlo_test_gpu\n+            //tests:fused_attention_stablehlo_test_gpu \\\n+            //tests:gpu_tests \\\n+            //tests:python_callback_test_gpu \\\n+            //tests:ragged_collective_test_gpu\n\\ No newline at end of file\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add Python 3.13 support to traceback code under PLATFORM_GOOGLE.\n\nThis build options uses CPython internals, which changed under Python 3.13.\n\nPiperOrigin-RevId: 761042927",
        "output": "```diff\nCommit: b8383df255fb9b056146911807a9a786cc4110e6\nDate: 2025-05-20T12:42:46Z\nURL: https://github.com/jax-ml/jax/commit/b8383df255fb9b056146911807a9a786cc4110e6\nFiles changed: 1\nAdditions: +16, Deletions: -5\ndiff --git a/jaxlib/traceback.cc b/jaxlib/traceback.cc\nindex 3eba5288335c..48edc584c94f 100644\n--- a/jaxlib/traceback.cc\n+++ b/jaxlib/traceback.cc\n@@ -68,11 +68,12 @@ Traceback::Traceback() {\n #else  // PY_VERSION_HEX < 0x030b0000\n \n #ifdef PLATFORM_GOOGLE\n-  // This code is equivalent to the version using public APIs, but it saves us\n-  // an allocation of one object per stack frame. However, this is definitely\n-  // violating the API contract of CPython, so we only use this where we can be\n-  // confident we know exactly which CPython we are using (internal to Google).\n-  // Feel free to turn this on if you like, but it might break at any time!\n+// This code is equivalent to the version using public APIs, but it saves us\n+// an allocation of one object per stack frame. However, this is definitely\n+// violating the API contract of CPython, so we only use this where we can be\n+// confident we know exactly which CPython we are using (internal to Google).\n+// Feel free to turn this on if you like, but it might break at any time!\n+#if PY_VERSION_HEX < 0x030d0000\n   for (_PyInterpreterFrame* f = thread_state->cframe->current_frame;\n        f != nullptr; f = f->previous) {\n     if (_PyFrame_IsIncomplete(f)) continue;\n@@ -80,6 +81,16 @@ Traceback::Traceback() {\n     frames_.emplace_back(f->f_code,\n                          _PyInterpreterFrame_LASTI(f) * sizeof(_Py_CODEUNIT));\n   }\n+#else   // PY_VERSION_HEX < 0x030d0000\n+  for (_PyInterpreterFrame* f = thread_state->current_frame; f != nullptr;\n+       f = f->previous) {\n+    if (_PyFrame_IsIncomplete(f)) continue;\n+    Py_INCREF(f->f_executable);\n+    frames_.emplace_back(reinterpret_cast<PyCodeObject*>(f->f_executable),\n+                         _PyInterpreterFrame_LASTI(f) * sizeof(_Py_CODEUNIT));\n+  }\n+#endif  // PY_VERSION_HEX < 0x030d0000\n+\n #else   // PLATFORM_GOOGLE\n   PyFrameObject* next;\n   for (PyFrameObject* py_frame = PyThreadState_GetFrame(thread_state);\n\n```",
        "from_id": [
            "hawkinsp",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Implement reshapes from and to refs with empty shapes\n\nPiperOrigin-RevId: 761038562",
        "output": "```diff\nCommit: e77df81f14f44bfe5ed1bf47b989673a13f3ceee\nDate: 2025-05-20T12:24:57Z\nURL: https://github.com/jax-ml/jax/commit/e77df81f14f44bfe5ed1bf47b989673a13f3ceee\nFiles changed: 2\nAdditions: +35, Deletions: -3\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex bf0b06ccb9c9..9eedc3402579 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -511,12 +511,42 @@ def memref_reshape(ref: ir.Value, shape: tuple[int, ...]) -> ir.Value:\n         f\" allowed) {shape}\"\n     )\n \n-  return _reshape(ref, list(ref_ty.shape), list(shape))\n+  src_shape = list(ref_ty.shape)\n+  dst_shape = list(shape)\n+  if src_shape == dst_shape:\n+    return ref\n+  if not src_shape:\n+    _, offset = ref_ty.get_strides_and_offset()\n+    identity = ir.AffineMapAttr.get(ir.AffineMap.get_identity(0))\n+    if ref_ty.layout == identity:\n+      new_layout = ir.AffineMapAttr.get(ir.AffineMap.get_identity(len(dst_shape)))\n+    else:\n+      new_layout = ir.StridedLayoutAttr.get(offset, [1] * len(dst_shape))\n+    result_ty = ir.MemRefType.get(dst_shape, ref_ty.element_type, new_layout, ref_ty.memory_space)\n+    return memref.expand_shape(result_ty, ref, [], [], dst_shape)\n+  if not dst_shape:\n+    _, offset = ref_ty.get_strides_and_offset()\n+    identity = ir.AffineMapAttr.get(ir.AffineMap.get_identity(ref_ty.rank))\n+    contig_strided_1d = ir.Attribute.parse(\"strided<[1]>\")\n+    if ref_ty.layout == identity or ref_ty.layout == contig_strided_1d:\n+      new_layout = ir.AffineMapAttr.get(ir.AffineMap.get_identity(0))\n+    else:\n+      new_layout = ir.StridedLayoutAttr.get(offset, [])\n+    result_ty = ir.MemRefType.get((), ref_ty.element_type, new_layout, ref_ty.memory_space)\n+    return memref.collapse_shape(result_ty, ref, [])\n+  return _reshape(ref, src_shape, dst_shape)\n \n \n def memref_fold(ref: ir.Value, dim, fold_rank) -> ir.Value:\n   ref_ty = ir.MemRefType(ref.type)\n   new_shape = list(ref_ty.shape)\n+  if dim < 0:\n+    raise ValueError(f\"Dimension {dim} is negative\")\n+  if dim + fold_rank > len(new_shape):\n+    raise ValueError(\n+        f\"Folding {fold_rank} dimensions starting from {dim} is out of bounds\"\n+        f\" for shape {new_shape}\"\n+    )\n   new_shape[dim : dim + fold_rank] = [np.prod(new_shape[dim : dim + fold_rank])]\n   identity = ir.AffineMapAttr.get(ir.AffineMap.get_identity(ref_ty.rank))\n   contig_strided_1d = ir.Attribute.parse(\"strided<[1]>\")\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 39bd8aa77331..80e67b20e1ef 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -389,17 +389,19 @@ def kernel(ctx, inp, out, _):\n       (\"add_1s\", (5, 1, 2), (1, 1, 5, 1, 1, 2, 1, 1)),\n       (\"fold\", (1, 5, 2, 1,), (1, 10, 1)),\n       (\"un\", (1, 10, 1), (1, 5, 2, 1,)),\n+      (\"to_scalar\", (1, 1, 1), ()),\n+      (\"from_scalar\", (), (1, 1, 1)),\n   )\n   def test_reshape(self, inp_shape, out_shape):\n     def kernel(ctx, inp, out, _):\n       copy(memref_reshape(inp, out_shape), out)\n \n-    x = np.arange(math.prod(inp_shape), dtype=jnp.float32).reshape(*inp_shape)\n+    x = np.arange(math.prod(inp_shape), dtype=jnp.float32).reshape(inp_shape)\n     out_ty = jax.ShapeDtypeStruct(out_shape, jnp.float32)\n     y = mgpu.as_gpu_kernel(\n         kernel, (1, 1, 1), (128, 1, 1), x, out_ty, ()\n     )(x)\n-    np.testing.assert_array_equal(y, x.reshape(*out_shape))\n+    np.testing.assert_array_equal(y, x.reshape(out_shape))\n \n   @parameterized.named_parameters([\n       (\"packed\", (4, 4, 4), (16, 4, 1), 1, 2, False),\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update scipy.signal.welch tests to be compatible with upstream dev version.",
        "output": "```diff\nCommit: e9cdbaca8dec2ed39bbe592372f203fce428027a\nDate: 2025-05-20T10:39:03Z\nURL: https://github.com/jax-ml/jax/commit/e9cdbaca8dec2ed39bbe592372f203fce428027a\nFiles changed: 2\nAdditions: +3, Deletions: -4\ndiff --git a/jax/_src/third_party/scipy/signal_helper.py b/jax/_src/third_party/scipy/signal_helper.py\nindex 4a021675804d..ad7bdfbef62a 100644\n--- a/jax/_src/third_party/scipy/signal_helper.py\n+++ b/jax/_src/third_party/scipy/signal_helper.py\n@@ -57,7 +57,7 @@ def _triage_segments(window: ArrayLike | str | tuple[Any, ...], nperseg: int | N\n       win = get_window(window, nperseg_int)\n     win = jnp.array(win, dtype=dtype)\n   else:\n-    win = jnp.asarray(window)\n+    win = jnp.asarray(window, dtype=dtype)\n     nperseg_int = win.size if nperseg is None else int(nperseg)\n     if win.ndim != 1:\n       raise ValueError('window must be 1-D')\ndiff --git a/tests/scipy_signal_test.py b/tests/scipy_signal_test.py\nindex 7ff3c87435c7..b1c5d9c98fed 100644\n--- a/tests/scipy_signal_test.py\n+++ b/tests/scipy_signal_test.py\n@@ -357,12 +357,11 @@ def testWelchWithDefaultStepArgsAgainstNumpy(\n     if use_nperseg:\n       kwargs['nperseg'] = nperseg\n     if use_window:\n-      kwargs['window'] = jnp.array(osp_signal.get_window('hann', nperseg),\n-                                   dtype=dtypes.to_complex_dtype(dtype))\n+      kwargs['window'] = jnp.array(osp_signal.get_window('hann', nperseg))\n     if use_noverlap:\n       kwargs['noverlap'] = noverlap\n \n-    @jtu.ignore_warning(message=\"nperseg = 256 is greater than\")\n+    @jtu.ignore_warning(message=\"nperseg\")\n     def osp_fun(x):\n       freqs, Pxx = osp_signal.welch(x, **kwargs)\n       return freqs.astype(_real_dtype(dtype)), Pxx.astype(_real_dtype(dtype))\n\n```",
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Added `plgpu.nd_loop`\n\nThis is a generalization of `lax.fori_loop` which partitions the flat iteration\nspace across the given axes, and is useful for writing persistent kernels.\n\nPiperOrigin-RevId: 761007326",
        "output": "```diff\nCommit: e5e9be55950d1358a93cd3eed10a91e6f5ae0168\nDate: 2025-05-20T10:36:06Z\nURL: https://github.com/jax-ml/jax/commit/e5e9be55950d1358a93cd3eed10a91e6f5ae0168\nFiles changed: 5\nAdditions: +123, Deletions: -0\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 4018bff873bd..61b5a99dfe31 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -877,6 +877,7 @@ pytype_strict_library(\n     deps = [\n         \":mosaic_gpu\",\n         \"//jax/_src/pallas/mosaic_gpu:core\",\n+        \"//jax/_src/pallas/mosaic_gpu:helpers\",\n         \"//jax/_src/pallas/mosaic_gpu:pallas_call_registration\",  # build_cleaner: keep\n         \"//jax/_src/pallas/mosaic_gpu:pipeline\",\n         \"//jax/_src/pallas/mosaic_gpu:primitives\",\ndiff --git a/jax/_src/pallas/mosaic_gpu/BUILD b/jax/_src/pallas/mosaic_gpu/BUILD\nindex 2652be7a7c9a..74b44fb8f991 100644\n--- a/jax/_src/pallas/mosaic_gpu/BUILD\n+++ b/jax/_src/pallas/mosaic_gpu/BUILD\n@@ -123,3 +123,9 @@ pytype_strict_library(\n         \"//jax/_src/pallas\",\n     ],\n )\n+\n+pytype_strict_library(\n+    name = \"helpers\",\n+    srcs = [\"helpers.py\"],\n+    deps = [\"//jax\"],\n+)\ndiff --git a/jax/_src/pallas/mosaic_gpu/helpers.py b/jax/_src/pallas/mosaic_gpu/helpers.py\nnew file mode 100644\nindex 000000000000..54c4910059d5\n--- /dev/null\n+++ b/jax/_src/pallas/mosaic_gpu/helpers.py\n@@ -0,0 +1,86 @@\n+# Copyright 2025 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"Helpers for Pallas Mosaic GPU kernels.\"\"\"\n+\n+from collections.abc import Callable, Hashable, Sequence\n+import math\n+from typing import TypeVar\n+\n+import jax\n+from jax import lax\n+\n+_T = TypeVar(\"_T\")\n+\n+\n+def nd_loop(\n+    grid: Sequence[int],\n+    body: Callable[[Sequence[jax.Array], _T], _T],\n+    init_val: _T,\n+    *,\n+    collective_axes: Sequence[Hashable] | Hashable,\n+) -> _T:\n+  \"\"\"A loop over a multi-dimensional grid partitioned along the given axes.\n+\n+  For example, if ``collective_axes`` is ``\"x\"`` with :func:`lax.axis_size`\n+  equal to 4 and the grid is (2, 3), the implementation would produce the\n+  following iteration order\n+\n+      loop step    index    axis index\n+\n+          0        (0, 0)       0\n+          1        (0, 1)       1\n+          2        (0, 2)       2\n+          3        (1, 0)       3\n+          4        (1, 1)       0\n+          5        (1, 2)       1\n+\n+  which comes from partitioning the flat iteration space into chunks in an\n+  interleaved fashion wrt the ``\"x\"`` axis index.\n+\n+  Note that in the example the total number of loop steps is not divisible\n+  by the axis size of ``\"x\"``, and thus for some ``\"x\"`` axis indices the\n+  loop will do one iteration less.\n+\n+      axis index       indices\n+\n+          0         (0, 0), (1, 1)\n+          1         (0, 1), (1, 2)\n+          2         (0, 2)\n+          3         (1, 0)\n+\n+  See also:\n+    - :func:`jax.lax.fori_loop`: A single-dimensional indexed loop.\n+  \"\"\"\n+  axis_index = lax.axis_index(collective_axes)\n+  axis_size = lax.axis_size(collective_axes)\n+  grid_size = math.prod(grid)\n+\n+  def wrapper(step, carry):\n+    step = step * axis_size + axis_index\n+    # The loop below is conceptually ``jnp.unravel_index``, but it uses\n+    # ``lax`` APIs instead of ``jax.numpy`` to minimize the number of\n+    # primitives used.\n+    index = []\n+    for grid_dim in reversed(grid):\n+      grid_dim = lax.convert_element_type(grid_dim, step.dtype)\n+      index.append(lax.rem(step, grid_dim))\n+      step = lax.div(step, grid_dim)\n+    index.reverse()\n+    return body(tuple(index), carry)\n+\n+  upper = lax.div(grid_size, axis_size) + lax.convert_element_type(\n+      axis_index < grid_size % axis_size, axis_index.dtype\n+  )\n+  return lax.fori_loop(0, upper, wrapper, init_val)\ndiff --git a/jax/experimental/pallas/mosaic_gpu.py b/jax/experimental/pallas/mosaic_gpu.py\nindex 7b300b8cfbfa..a7d8c3e34223 100644\n--- a/jax/experimental/pallas/mosaic_gpu.py\n+++ b/jax/experimental/pallas/mosaic_gpu.py\n@@ -38,6 +38,7 @@\n from jax._src.pallas.mosaic_gpu.core import WarpMesh as WarpMesh\n from jax._src.pallas.mosaic_gpu.core import WGMMAAccumulatorRef as ACC  # noqa: F401\n from jax._src.pallas.mosaic_gpu.core import WGMMAAccumulatorRef as WGMMAAccumulatorRef\n+from jax._src.pallas.mosaic_gpu.helpers import nd_loop as nd_loop\n from jax._src.pallas.mosaic_gpu.pipeline import emit_pipeline as emit_pipeline\n from jax._src.pallas.mosaic_gpu.pipeline import emit_pipeline_warp_specialized as emit_pipeline_warp_specialized\n from jax._src.pallas.mosaic_gpu.primitives import barrier_arrive as barrier_arrive\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex ba7f2d74bbb1..68aeea5a03e8 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1762,6 +1762,35 @@ def kernel(x_ref, o_ref128, aliased_ref):\n     with self.assertRaisesRegex(ValueError, \"can't be assigned to\"):\n       kernel(jnp.arange(128).astype(jnp.float32))\n \n+  @parameterized.parameters(1, 2, 3)\n+  def test_nd_loop(self, sm_steps):\n+    @functools.partial(\n+        self.kernel,\n+        out_shape=jax.ShapeDtypeStruct((sm_steps, 132, 128), jnp.int32),\n+        grid=(132,),\n+        grid_names=(\"sm\",),\n+    )\n+    def kernel(o_ref):\n+      def body(idx, _):\n+        assert len(idx) == 3\n+        # We need to use `mode=\"clip\"`, because the indices are not static.\n+        flat_idx = jnp.ravel_multi_index(idx, (sm_steps, 4, 33), mode=\"clip\")\n+        sm_step = lax.div(\n+            flat_idx, lax.convert_element_type(lax.axis_size(\"sm\"), jnp.int32)\n+        )\n+        o_ref[sm_step, lax.axis_index(\"sm\")] = lax.broadcast(\n+            flat_idx, o_ref.shape[-1:]\n+        )\n+\n+      plgpu.nd_loop((sm_steps, 4, 33), body, None, collective_axes=\"sm\")\n+\n+    result = kernel()\n+    for sm_step in range(sm_steps):\n+      np.testing.assert_array_equal(\n+          result[sm_step],\n+          jnp.tile((132 * sm_step + jnp.arange(132))[:, None], 128),\n+      )\n+\n \n class PallasCallWGTest(\n     PallasCallTest, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] Pulled `runtime_assert_enabled` from `pltpu` to `pl`\n\nPiperOrigin-RevId: 760983479",
        "output": "```diff\nCommit: 7025c2310b576030875327275c761cfb64c3720e\nDate: 2025-05-20T09:10:13Z\nURL: https://github.com/jax-ml/jax/commit/7025c2310b576030875327275c761cfb64c3720e\nFiles changed: 7\nAdditions: +93, Deletions: -38\ndiff --git a/jax/_src/pallas/core.py b/jax/_src/pallas/core.py\nindex 709bb4640241..fe755d61a310 100644\n--- a/jax/_src/pallas/core.py\n+++ b/jax/_src/pallas/core.py\n@@ -44,6 +44,23 @@\n from jax._src.state.types import TransformedRef\n import jax.numpy as jnp\n \n+# TODO(slebedev): Rename to --jax_pallas_debug_assertions.\n+_ENABLE_RUNTIME_ASSERT = config.bool_state(\n+    \"jax_pallas_enable_runtime_assert\",\n+    default=False,\n+    help=(\n+        \"If set, enables runtime assertions in the kernel via checkify.check.\"\n+        \" Otherwise, runtime asserts will be ignored unless functionalized\"\n+        \" using checkify.checkify.\"\n+    ),\n+)\n+\n+\n+def runtime_assert_enabled() -> bool:\n+  \"\"\"Returns whether runtime asserts are enabled.\"\"\"\n+  return _ENABLE_RUNTIME_ASSERT.value\n+\n+\n class DynamicGridDim:\n   def __repr__(self):\n     return \"DynamicGridDim\"\ndiff --git a/jax/_src/pallas/mosaic/core.py b/jax/_src/pallas/mosaic/core.py\nindex c04fc6f155b9..49ff632f5c14 100644\n--- a/jax/_src/pallas/mosaic/core.py\n+++ b/jax/_src/pallas/mosaic/core.py\n@@ -23,7 +23,6 @@\n from typing import Any, ClassVar, Literal\n \n import jax\n-from jax._src import config\n from jax._src import core as jax_core\n from jax._src import util\n from jax._src.pallas import core as pallas_core\n@@ -48,16 +47,6 @@\n _out_shape_to_aval_mapping = pallas_core._out_shape_to_aval_mapping\n split_list = util.split_list\n \n-_ENABLE_RUNTIME_ASSERT = config.bool_state(\n-    \"jax_pallas_enable_runtime_assert\",\n-    default=False,\n-    help=(\n-        \"If set, enables runtime assertions in the kernel via checkify.check.\"\n-        \" Otherwise, runtime asserts will be ignored unless functionalized\"\n-        \" using checkify.checkify.\"\n-    ),\n-)\n-\n \n class KernelType(enum.Enum):\n   TC = 0\n@@ -221,11 +210,6 @@ def create_tensorcore_mesh(\n   )\n \n \n-def runtime_assert_enabled() -> bool:\n-  \"\"\"Returns whether runtime asserts are enabled.\"\"\"\n-  return _ENABLE_RUNTIME_ASSERT.value\n-\n-\n def _tensorcore_mesh_discharge_rule(\n     in_avals,\n     out_avals,\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex a67a71dd40f7..a9fbf8dcd982 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -235,7 +235,7 @@ def _memory_space_to_mosaic_attribute(memory_space: MemorySpace | None\n   tpu_memory_space = _memory_space_to_tpu_memory_space(memory_space)\n   return ir.Attribute.parse(f\"#tpu.memory_space<{tpu_memory_space}>\")\n \n-def _dtype_to_ir_type(dtype: jnp.dtype,\n+def _dtype_to_ir_type(dtype: jax.typing.DTypeLike,\n                       is_kernel_boundary: bool = False) -> ir.Type:\n   if jnp.issubdtype(dtype, pallas_core.semaphore_dtype):\n     if jnp.issubdtype(dtype, tpu_core.dma_semaphore):\n@@ -246,11 +246,11 @@ def _dtype_to_ir_type(dtype: jnp.dtype,\n       return ir.Type.parse(\"!tpu.semaphore\")\n     else:\n       raise NotImplementedError\n-  if is_kernel_boundary and jnp.issubdtype(dtype, jnp.dtype('bool')):\n+  if is_kernel_boundary and jnp.issubdtype(dtype, jnp.bool):\n     dtype = BOOL_MEMREF_TYPE\n   # TODO(justinfu): Remove after mosaic supports unsigned types.\n   # This conversion makes mosaic interpret all unsigned types as signed types.\n-  type =  mlir.dtype_to_ir_type(dtype)\n+  type =  mlir.dtype_to_ir_type(jnp.dtype(dtype))\n   if isinstance(type, ir.IntegerType):\n     return ir.IntegerType.get_signless(type.width)\n   else:\n@@ -3766,14 +3766,15 @@ def _join_key_lowering_rule(ctx: LoweringRuleContext, *scalars, impl):\n @register_lowering_rule(checkify.check_p)\n def _checkify_lowering_rule(\n     ctx: LoweringRuleContext, *err_args, err_tree, debug):\n-  if not tpu_core.runtime_assert_enabled():\n+  if not pallas_core.runtime_assert_enabled():\n     if debug:\n       return []\n     else:\n-      raise LoweringException(\"Non-debug check must be functionalized. \"\n-                              \"Enable runtime asserts with \"\n-                              \"--jax_pallas_enable_runtime_assert \"\n-                              \"or functionalize with checkify.check.\")\n+      raise LoweringException(\n+          \"Non-debug check must be functionalized. Enable runtime asserts via\"\n+          \" ``pl.enable_runtime_assert`` or --jax_pallas_enable_runtime_assert\"\n+          \" or, alternatively, functionalize with ``checkify.check``.\"\n+      )\n \n   if cf is None:\n     # TODO(slebedev): Remove once the minimal jaxlib version is 0.6.1.\n@@ -3782,20 +3783,16 @@ def _checkify_lowering_rule(\n     )\n \n   error = jax.tree.unflatten(err_tree, err_args)\n-  assert len(error._pred) == 1\n-  assert len(error._metadata) == 1\n-  assert len(error._payload) == 1\n-  pred = list(error._pred.items())[0][1]\n-  metadata = list(error._metadata.items())[0]\n-  payload = list(error._payload.items())[0][1]\n-  exception_tree = metadata[1]\n+  [pred] = error._pred.values()\n+  [exception_tree] = error._metadata.values()\n+  [payload] = error._payload.values()\n   exception = jax.tree.unflatten(exception_tree, payload)\n   assert isinstance(exception, checkify.FailedCheckError)\n+  assert isinstance(exception, checkify.FailedCheckError)\n \n-  # check_p has an inverted predicate compared to assert,\n-  # so we need to compute not(pred) here.\n-  out_scalar_type = _dtype_to_ir_type(jnp.dtype('bool'))\n-  minus_one = ir_constant(-1, out_scalar_type)\n+  # check_p has an inverted predicate compared to assert, so we need to compute\n+  # ``not pred`` here.\n+  minus_one = ir_constant(-1, _dtype_to_ir_type(jnp.bool))\n   not_pred = arith.xori(pred, minus_one)\n   cf.assert_(not_pred, exception.fmt_string)\n   return []\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex bd304e8b6745..751a2bae2ed0 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -29,6 +29,7 @@\n import jax\n from jax import api_util\n from jax import lax\n+from jax._src import checkify\n from jax._src import core as jax_core\n from jax._src import lib as jaxlib\n from jax._src import linear_util as lu\n@@ -41,6 +42,7 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import arith as arith_dialect\n+from jax._src.lib.mlir.dialects import cf as cf_dialect\n from jax._src.lib.mlir.dialects import gpu as gpu_dialect\n from jax._src.lib.mlir.dialects import llvm as llvm_dialect\n from jax._src.lib.mlir.dialects import math as math_dialect\n@@ -3051,3 +3053,38 @@ def _semaphore_wait_lowering_rule(ctx: LoweringRuleContext, *args, args_tree):\n       scf_dialect.yield_(after_block.arguments)\n   mgpu_utils.warpgroup_barrier()\n   return ()\n+\n+\n+@register_lowering_rule(checkify.check_p, mgpu.LoweringSemantics.Lane)\n+def _checkify_lowering_rule(\n+    ctx: LoweringRuleContext, *err_args, err_tree, debug\n+):\n+  if not pallas_core.runtime_assert_enabled():\n+    if debug:\n+      return []\n+    else:\n+      raise LoweringError(\n+          \"Non-debug check must be functionalized. Enable runtime asserts via\"\n+          \" ``pl.enable_runtime_assert`` or --jax_pallas_enable_runtime_assert\"\n+          \" or, alternatively, functionalize with ``checkify.check``.\"\n+      )\n+\n+  if cf_dialect is None:\n+    # TODO(slebedev): Remove once the minimal jaxlib version is 0.6.1.\n+    raise ValueError(\n+        \"cf dialect is not available. Make sure you have jaxlib 0.6.1 or later.\"\n+    )\n+\n+  error = jax.tree.unflatten(err_tree, err_args)\n+  [pred] = error._pred.values()\n+  [exception_tree] = error._metadata.values()\n+  [payload] = error._payload.values()\n+  exception = jax.tree.unflatten(exception_tree, payload)\n+  assert isinstance(exception, checkify.FailedCheckError)\n+\n+  # check_p has an inverted predicate compared to assert, so we need to compute\n+  # ``not pred`` here.\n+  minus_one = _ir_constant(-1, mgpu_utils.dtype_to_ir_type(jnp.bool))\n+  not_pred = arith_dialect.xori(pred.registers.item(), minus_one)\n+  cf_dialect.assert_(not_pred, exception.fmt_string)\n+  return []\ndiff --git a/jax/experimental/pallas/__init__.py b/jax/experimental/pallas/__init__.py\nindex 1e631ad407fd..406d6e965322 100644\n--- a/jax/experimental/pallas/__init__.py\n+++ b/jax/experimental/pallas/__init__.py\n@@ -18,6 +18,7 @@\n https://docs.jax.dev/en/latest/pallas.html.\n \"\"\"\n \n+from jax._src.pallas.core import _ENABLE_RUNTIME_ASSERT as enable_runtime_assert  # noqa: F401\n from jax._src.pallas.core import BlockDim as BlockDim\n from jax._src.pallas.core import Blocked as Blocked\n from jax._src.pallas.core import BlockSpec as BlockSpec\n@@ -32,6 +33,7 @@\n from jax._src.pallas.core import MemoryRef as MemoryRef\n from jax._src.pallas.core import MemorySpace as MemorySpace\n from jax._src.pallas.core import no_block_spec as no_block_spec\n+from jax._src.pallas.core import runtime_assert_enabled as runtime_assert_enabled\n from jax._src.pallas.core import semaphore as semaphore\n from jax._src.pallas.core import Squeezed as Squeezed\n from jax._src.pallas.core import squeezed as squeezed\ndiff --git a/jax/experimental/pallas/tpu.py b/jax/experimental/pallas/tpu.py\nindex 5ed6968c673e..c8e2ba131a9b 100644\n--- a/jax/experimental/pallas/tpu.py\n+++ b/jax/experimental/pallas/tpu.py\n@@ -25,8 +25,6 @@\n from jax._src.pallas.mosaic.core import SemaphoreType as SemaphoreType\n from jax._src.pallas.mosaic.core import TPUMemorySpace as TPUMemorySpace\n from jax._src.pallas.mosaic.core import TPUCompilerParams as TPUCompilerParams\n-from jax._src.pallas.mosaic.core import runtime_assert_enabled as runtime_assert_enabled\n-from jax._src.pallas.mosaic.core import _ENABLE_RUNTIME_ASSERT as enable_runtime_assert  # noqa: F401\n from jax._src.pallas.mosaic.helpers import sync_copy as sync_copy\n from jax._src.pallas.mosaic.helpers import core_barrier as core_barrier\n from jax._src.pallas.mosaic.helpers import run_on_first_core as run_on_first_core\n@@ -53,6 +51,8 @@\n # Those primitives got moved to Pallas core. Keeping the updated imports\n # here for backward compatibility.\n from jax._src.pallas.core import semaphore as semaphore\n+from jax._src.pallas.core import runtime_assert_enabled as runtime_assert_enabled\n+from jax._src.pallas.core import _ENABLE_RUNTIME_ASSERT as enable_runtime_assert  # noqa: F401\n from jax._src.pallas.primitives import DeviceIdType as DeviceIdType\n from jax._src.pallas.primitives import semaphore_read as semaphore_read\n from jax._src.pallas.primitives import semaphore_signal as semaphore_signal\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex d71593dc9078..ba7f2d74bbb1 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -28,14 +28,15 @@\n import jax\n from jax import export\n from jax import lax\n+from jax._src import checkify\n from jax._src import test_util as jtu\n from jax._src.pallas import core as pallas_core\n from jax._src.pallas import pallas_call\n+from jax._src.pallas import primitives as pallas_primitives\n from jax._src.pallas.mosaic_gpu import core as gpu_core\n from jax._src.pallas.mosaic_gpu import lowering as mgpu_lowering\n from jax._src.pallas.mosaic_gpu import pipeline as mgpu_pipeline\n from jax._src.pallas.mosaic_gpu import primitives as mgpu_primitives\n-from jax._src.pallas import primitives as pallas_primitives\n from jax._src.state import types as state_types\n from jax.experimental import pallas as pl\n import jax.experimental.mosaic.gpu as mgpu\n@@ -995,6 +996,22 @@ def kernel(x_ref, o_ref):\n \n     self.assertIn(\"x: [1, 0, 43, 23]: 6871\\n\", output())\n \n+  def test_check(self):\n+    self.skip_if_wg_semantics()\n+\n+    self.enter_context(pallas_core._ENABLE_RUNTIME_ASSERT(True))\n+\n+    @functools.partial(\n+        self.pallas_call,\n+        out_shape=jax.ShapeDtypeStruct([256], jnp.int32),\n+    )\n+    def kernel(x_ref, o_ref):\n+      checkify.check(_sum_same_dtype(x_ref[...]) > 0, \"x.sum() is negative\")\n+      o_ref[...] = x_ref[...]\n+\n+    x = jnp.arange(256, dtype=jnp.int32)\n+    np.testing.assert_array_equal(kernel(x), x)\n+\n   def test_load_scalar(self):\n     @functools.partial(\n         self.pallas_call,\n@@ -1776,6 +1793,7 @@ def test_missing_primitive_lowerings_are_tracked(self):\n         pallas_primitives.semaphore_signal_p,\n         pallas_primitives.semaphore_wait_p,\n         pallas_primitives.semaphore_read_p,\n+        checkify.check_p,\n     }\n \n     self.assertSetEqual(actual_missing_primitives, expected_missing_primitives)\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Use native bf16 ops for tanh, exp and log on TPUv6+.\n\nReplace `needs_cast` condition during canonicalization with `need_elementwise_canonicalization`.\n\nPiperOrigin-RevId: 760934277",
        "output": "```diff\nCommit: fc786d7422812d48637d03a47baf2b5b3bf15738\nDate: 2025-05-20T06:28:03Z\nURL: https://github.com/jax-ml/jax/commit/fc786d7422812d48637d03a47baf2b5b3bf15738\nFiles changed: 1\nAdditions: +13, Deletions: -18\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\nindex 645e6d615722..368bfc596732 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n@@ -359,12 +359,7 @@ LogicalResult canonicalize_elementwise(const CanonicalizeContext &ctx,\n       auto element_type = ty.getElementType();\n       // There's an annoying hodgepodge of elementwise ops that need to be\n       // rewritten to f32 on later hardware.\n-      // TODO(mvoz): Look into (1) what it would take to support these ops\n-      // natively on later hardware, and (2) how to better organize this list.\n-      bool needs_cast = ctx.hardware_generation <= 5 || isa<math::PowFOp>(op) ||\n-                        isa<math::TanhOp>(op) || isa<math::ExpOp>(op) ||\n-                        isa<math::LogOp>(op);\n-      if (needs_cast && element_type.isBF16()) {\n+      if (element_type.isBF16()) {\n         if (ctx.compatibility_mode) {\n           auto target_f32 =\n               builder.create<arith::ExtFOp>(op.getLoc(), target_f32_ty, operand)\n@@ -918,21 +913,22 @@ const llvm::StringMap<canonicalize_rule_type> &rules() {\n   return *rules;\n }\n \n-const llvm::StringMap<int> &bf16_upcast_min_supported_versions() {\n+const llvm::StringMap<int> &bf16_ops_min_supported_versions() {\n   constexpr int kAlwaysUpcast = std::numeric_limits<int>::max();\n   static const auto m = new llvm::StringMap<int>{\n       {arith::DivFOp::getOperationName(), 4},\n       {arith::SelectOp::getOperationName(), 5},\n       {arith::CmpFOp::getOperationName(), 5},\n-      {arith::MulFOp::getOperationName(), kAlwaysUpcast},\n-      {arith::AddFOp::getOperationName(), kAlwaysUpcast},\n-      {arith::SubFOp::getOperationName(), kAlwaysUpcast},\n-      {arith::MaximumFOp::getOperationName(), kAlwaysUpcast},\n-      {arith::MinimumFOp::getOperationName(), kAlwaysUpcast},\n+      {arith::MulFOp::getOperationName(), 6},\n+      {arith::AddFOp::getOperationName(), 6},\n+      {arith::SubFOp::getOperationName(), 6},\n+      {arith::MaximumFOp::getOperationName(), 6},\n+      {arith::MinimumFOp::getOperationName(), 6},\n       {math::PowFOp::getOperationName(), kAlwaysUpcast},\n-      {math::TanhOp::getOperationName(), kAlwaysUpcast},\n-      {math::ExpOp::getOperationName(), kAlwaysUpcast},\n-      {math::LogOp::getOperationName(), kAlwaysUpcast},\n+      {math::TanhOp::getOperationName(), 6},\n+      {math::ExpOp::getOperationName(), 6},\n+      {math::Exp2Op::getOperationName(), 6},\n+      {math::LogOp::getOperationName(), 6},\n   };\n   return *m;\n }\n@@ -941,9 +937,8 @@ bool need_elementwise_canonicalization(const CanonicalizeContext &ctx,\n                                        Operation &op) {\n   // Only rewrite when the hardware generation is below the minimum supported\n   // version.\n-  auto it =\n-      bf16_upcast_min_supported_versions().find(op.getName().getStringRef());\n-  if (it == bf16_upcast_min_supported_versions().end() ||\n+  auto it = bf16_ops_min_supported_versions().find(op.getName().getStringRef());\n+  if (it == bf16_ops_min_supported_versions().end() ||\n       ctx.hardware_generation >= it->second) {\n     return false;\n   }\n\n```",
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Add support for currently unsupported reshapes for 32-bit datatypes with native tiling and adds tests for those cases. The cases supported are (k % 128 == 0 in the below):\n- (q, m, n, k) -> (q, m, n * k)\n- (p, q, m, n, k) -> (p, q * m * n * k)\n- (q, m, n, k) -> (q, m, 1, n * k) (in 2 steps, first to n*k then add unit dim)\n- (q, m, n, k) -> (q * m, n * k)\n- (q * m, n, k) -> (q, m, n * k)\n- (q * m, n * k) -> (q, m, n, k)\n- (q, m, n * k) -> (q * m, n, k)\n\nPiperOrigin-RevId: 760904758",
        "output": "```diff\nCommit: 30339b08e1f88d7e9808589b67fdd40550324c77\nDate: 2025-05-20T04:38:26Z\nURL: https://github.com/jax-ml/jax/commit/30339b08e1f88d7e9808589b67fdd40550324c77\nFiles changed: 5\nAdditions: +433, Deletions: -44\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex ba1dfc95c66c..5ddff9d9ee53 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -4301,6 +4301,43 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n   return success();\n }\n \n+// Copy one sublane from a vreg to another vreg.\n+//\n+// Arguments:\n+//  src_vreg: The source vreg to copy a sublane from.\n+//  src_sl_idx: The sublane index in src_vreg to copy from.\n+//  dst_vreg: The base vreg to copy the sublane into. May be null.\n+//  dst_sl_idx: The sublane index in the result.\n+//\n+// Returns:\n+//  A new dst_vreg with the copied sublane.\n+Value copyOneSublane(OpBuilder &builder, Value src_vreg, int src_sl_idx,\n+                     Value dst_vreg, int dst_sl_idx,\n+                     const std::array<int64_t, 2> target_shape) {\n+  src_vreg = builder.create<tpu::RotateOp>(\n+      src_vreg.getLoc(), src_vreg,\n+      /*amount=*/(dst_sl_idx - src_sl_idx + target_shape[0]) % target_shape[0],\n+      /*dimension=*/0, /*stride=*/nullptr, /*stride_dimension=*/nullptr);\n+  if (dst_vreg) {\n+    auto boundIdxConst =\n+        std::bind(IdxConst, std::placeholders::_1, builder, src_vreg.getLoc());\n+    const int bitwidth =\n+        cast<VectorType>(src_vreg.getType()).getElementTypeBitWidth();\n+    CHECK_EQ(bitwidth,\n+             cast<VectorType>(dst_vreg.getType()).getElementTypeBitWidth());\n+    const VectorType vmask_ty =\n+        getNativeVregOrVmaskType(builder.getI1Type(), bitwidth, target_shape);\n+    auto sublanes_mask = builder.create<tpu::CreateMaskOp>(\n+        src_vreg.getLoc(), vmask_ty,\n+        ValueRange{boundIdxConst(dst_sl_idx), boundIdxConst(0)},\n+        ValueRange{boundIdxConst(dst_sl_idx + 1),\n+                   boundIdxConst(target_shape[1])});\n+    src_vreg = builder.create<arith::SelectOp>(src_vreg.getLoc(), sublanes_mask,\n+                                               src_vreg, dst_vreg);\n+  }\n+  return src_vreg;\n+}\n+\n LogicalResult vector_shape_cast_rule(RewriteContext &ctx, Operation &op,\n                                      const ArrayRef<Layout> layouts_in,\n                                      const ArrayRef<Layout> layouts_out) {\n@@ -4397,6 +4434,132 @@ LogicalResult vector_shape_cast_rule(RewriteContext &ctx, Operation &op,\n       dst_vregs_local.Reshape(\n           layout_out.tileArrayImplicitShape(dst_shape, ctx.target_shape));\n       return dst_vregs_local;\n+    } else if (\n+        // Lower shape_casts for 32-bit types where the minor dimension both\n+        // before and after the shape cast is a multiple of 128. We allow\n+        // folding or unfolding multiple number of minor dimensions and folding\n+        // or unfolding some number of leading dimensions. For example (given\n+        // k % 128 == 0 in the following):\n+        // (q, m, n, k) -> (q, m, n * k)\n+        // (p, q, m, n, k) -> (p, q * m * n * k)\n+        // (q, m, n, k) -> (q, m, 1, n * k) (in 2 steps, first to fold n, k then\n+        //    to add the unit dimension)\n+        // (q, m, n, k) -> (q * m, n * k)\n+        // (q * m, n, k) -> (q, m, n * k)\n+        // (q * m, n * k) -> (q, m, n, k)\n+        // (q, m, n * k) -> (q * m, n, k)\n+        dst_shape.size() > 1 && src_shape.size() > 1 &&\n+        (mlir::tpu::canFoldMinorDimsToSize(src_shape, dst_shape.back()) ||\n+         mlir::tpu::canFoldMinorDimsToSize(dst_shape, src_shape.back())) &&\n+        dst_shape.back() % ctx.target_shape[1] == 0 &&\n+        src_shape.back() % ctx.target_shape[1] == 0 &&\n+        layout_in.offsets() == LayoutOffsets{0, 0} &&\n+        layout_in.hasNativeTiling(ctx.target_shape) &&\n+        layout_in.bitwidth() == 32 &&\n+        layout_in.implicit_dim() == VectorLayout::ImplicitDim::kNone &&\n+        layout_out == layout_in) {\n+      auto target_sublanes = ctx.target_shape[0];\n+      auto target_lanes = ctx.target_shape[1];\n+      xla::Array<Value> dst_vregs(\n+          layout_out.tileArrayShape(false, false, dst_shape, ctx.target_shape));\n+\n+      auto to_linear_index = [&](absl::Span<const int64_t> indices,\n+                                 absl::Span<const int64_t> bounds) {\n+        CHECK_EQ(indices.size(), bounds.size());\n+        int linear_index = 0;\n+        int multiplier = 1;\n+        for (int i = indices.size() - 1; i >= 0; --i) {\n+          linear_index += multiplier * indices[i];\n+          multiplier *= bounds[i];\n+        }\n+        return linear_index;\n+      };\n+      auto from_linear_index = [&](int linear_index,\n+                                   absl::Span<const int64_t> bounds) {\n+        SmallVector<int64_t> indices(bounds.size(), 0);\n+        int64_t divisor = std::accumulate(bounds.begin(), bounds.end(), 1,\n+                                          std::multiplies<int64_t>());\n+        CHECK_GT(divisor, 0);\n+        int64_t remainder = linear_index % divisor;\n+        for (int i = 0; i < bounds.size(); ++i) {\n+          int64_t radix = bounds[i];\n+          CHECK_GT(radix, 0);\n+          divisor /= radix;\n+          CHECK_GT(divisor, 0);\n+          indices[i] = remainder / divisor;\n+          remainder = remainder % divisor;\n+        }\n+        return indices;\n+      };\n+      // Gather sublanes from src_vregs via rotating and selecting each relevant\n+      // sublane from the source, into the destination vreg.\n+      // Args:\n+      // * src_sublane_indices: the mixed-radix indices of the sublanes to\n+      // gather in the order they should be gathered.\n+      // * src_vregs: the vregs to gather from.\n+      // Returns:\n+      // * a vreg with the gathered sublanes.\n+      auto gather_sublanes = [target_sublanes](\n+                                 RewriteContext &ctx, Operation &op,\n+                                 SmallVector<SmallVector<int64_t>>\n+                                     src_sublane_indices,\n+                                 const xla::Array<Value> &src_vregs) {\n+        ImplicitLocOpBuilder builder(op.getLoc(), &op);\n+        Value dst_vreg = getZerosVector(\n+            builder, cast<VectorType>(src_vregs.begin()->getType()));\n+        for (int sublane_number = 0;\n+             sublane_number < src_sublane_indices.size(); ++sublane_number) {\n+          SmallVector<int64_t> src_vreg_index =\n+              src_sublane_indices[sublane_number];\n+          src_vreg_index[src_vreg_index.size() - 2] /= target_sublanes;\n+          Value src_vreg = src_vregs(src_vreg_index);\n+          int sublane_within_src_vreg =\n+              src_sublane_indices[sublane_number]\n+                                 [src_sublane_indices[sublane_number].size() -\n+                                  2] %\n+              target_sublanes;\n+          dst_vreg = copyOneSublane(builder, src_vreg, sublane_within_src_vreg,\n+                                    dst_vreg, sublane_number, ctx.target_shape);\n+        }\n+        return dst_vreg;\n+      };\n+      SmallVector<int64_t> dst_shape_in_sublanes(dst_shape);\n+      dst_shape_in_sublanes[dst_shape.size() - 1] =\n+          dst_shape[dst_shape.size() - 1] / target_lanes;\n+      SmallVector<int64_t> src_shape_in_sublanes(src_shape);\n+      src_shape_in_sublanes[src_shape.size() - 1] =\n+          src_shape[src_shape.size() - 1] / target_lanes;\n+      // The algorithm operates on 1 destination vreg at a time:\n+      // 1. For each destination vreg, compute the linear index of each sublane\n+      // within it\n+      // 2. Map the destination sublane linear index to a source sublane linear\n+      // index\n+      // 3. convert that to a mixed-radix index into the source shape\n+      // 4. Gather from those source sublane indices.\n+      SmallVector<int64_t> indices;\n+      dst_vregs.Each([&](absl::Span<const int64_t> dst_vreg_indices,\n+                         Value *dst_vreg) {\n+        indices.assign(dst_vreg_indices.begin(), dst_vreg_indices.end());\n+        indices[indices.size() - 2] *= target_sublanes;\n+        int sublane_offset = to_linear_index(indices, dst_shape_in_sublanes);\n+\n+        // Only move non-padding sublanes to the destination vreg.\n+        int num_non_padding_sublanes = std::min(\n+            dst_shape_in_sublanes[dst_shape_in_sublanes.size() - 2] -\n+                dst_vreg_indices[dst_vreg_indices.size() - 2] * target_sublanes,\n+            target_sublanes);\n+        CHECK_EQ(dst_shape.back() % target_lanes, 0);\n+        int stride_in_sublanes = dst_shape.back() / target_lanes;\n+        SmallVector<SmallVector<int64_t>> gathered_sublanes(\n+            num_non_padding_sublanes);\n+        for (int i = 0; i < gathered_sublanes.size(); ++i) {\n+          gathered_sublanes[i] =\n+              from_linear_index(sublane_offset, src_shape_in_sublanes);\n+          sublane_offset += stride_in_sublanes;\n+        }\n+        *dst_vreg = gather_sublanes(ctx, op, gathered_sublanes, src_vregs);\n+      });\n+      return dst_vregs;\n     } else {\n       return shape_cast_op.emitOpError(\n                  \"Not implemented: Unsupported vector.shape_cast: \")\n@@ -5262,45 +5425,6 @@ xla::Array<Value> retileToReducedSublanes(\n   return dst_vreg_array;\n }\n \n-\n-// Copy one sublane from a vreg to another vreg.\n-//\n-// Arguments:\n-//  src_vreg: The source vreg to copy a sublane from.\n-//  src_sl_idx: The sublane index in src_vreg to copy from.\n-//  dst_vreg: The base vreg to copy the sublane into. May be null.\n-//  dst_sl_idx: The sublane index in the result.\n-//\n-// Returns:\n-//  A new dst_vreg with the copied sublane.\n-Value copy_one_sublane(OpBuilder &builder, Value src_vreg, int src_sl_idx,\n-                       Value dst_vreg, int dst_sl_idx,\n-                       const std::array<int64_t, 2> target_shape) {\n-  src_vreg = builder.create<tpu::RotateOp>(\n-      src_vreg.getLoc(), src_vreg,\n-      /*amount=*/(dst_sl_idx - src_sl_idx + target_shape[0]) % target_shape[0],\n-      /*dimension=*/0, /*stride=*/nullptr, /*stride_dimension=*/nullptr);\n-  if (dst_vreg) {\n-    auto boundIdxConst =\n-        std::bind(IdxConst, std::placeholders::_1, builder, src_vreg.getLoc());\n-    const int bitwidth =\n-        cast<VectorType>(src_vreg.getType()).getElementTypeBitWidth();\n-    CHECK_EQ(bitwidth,\n-             cast<VectorType>(dst_vreg.getType()).getElementTypeBitWidth());\n-    const VectorType vmask_ty =\n-        getNativeVregOrVmaskType(builder.getI1Type(), bitwidth, target_shape);\n-    auto sublanes_mask = builder.create<tpu::CreateMaskOp>(\n-        src_vreg.getLoc(), vmask_ty,\n-        ValueRange{boundIdxConst(dst_sl_idx), boundIdxConst(0)},\n-        ValueRange{boundIdxConst(dst_sl_idx + 1),\n-                   boundIdxConst(target_shape[1])});\n-    src_vreg = builder.create<arith::SelectOp>(src_vreg.getLoc(), sublanes_mask,\n-                                               src_vreg, dst_vreg);\n-  }\n-  return src_vreg;\n-}\n-\n-\n void rotateVregs(OpBuilder &builder, xla::Array<Value> &vregs,\n                  const int64_t amount, const int dimension) {\n   if (amount != 0) {\n@@ -6714,9 +6838,9 @@ FailureOr<std::pair<VectorLayout, xla::Array<Value>>> changeImplicitDim(\n         for (int tile_idx = 0; tile_idx < tiles_per_vreg; ++tile_idx) {\n           int tile_off = tile_idx * sublanes_per_tile;\n           *tile =\n-              copy_one_sublane(builder, vregs(src_idx),\n-                               tile_off + src.offsets()[0].value_or(dst_sl_idx),\n-                               *tile, tile_off + dst_sl_idx, target_shape);\n+              copyOneSublane(builder, vregs(src_idx),\n+                             tile_off + src.offsets()[0].value_or(dst_sl_idx),\n+                             *tile, tile_off + dst_sl_idx, target_shape);\n         }\n       }\n     });\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex f42cfb139a37..9c4a7b4c397d 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -60,7 +60,6 @@ using ImplicitDim = VectorLayout::ImplicitDim;\n \n static constexpr int kLayoutLog = 10;\n \n-\n bool is_fully_replicated(const Layout &layout) {\n   static LayoutOffsets replicated_offsets = {std::nullopt, std::nullopt};\n   return layout.has_value() && layout->offsets() == replicated_offsets;\n@@ -1520,7 +1519,30 @@ class VectorLayoutInferer {\n                              native_tiling, ImplicitDim::kNone));\n       return success();\n     }\n-    op.emitOpError(\"unsupported shape cast\");\n+\n+    // Shape cast (..., m, n, k * target_shape_[1]) -> (..., m, n * k *\n+    // target_shape_[1]) for 32-bit types. We allow multiple major or minor\n+    // dimensions to be folded or unfolded.\n+    if (kNativeBitwidth == bitwidth && res_shape.size() >= 2 &&\n+        src_shape.size() >= 2 && src_shape.back() % native_tiling[1] == 0 &&\n+        res_shape.back() % native_tiling[1] == 0 &&\n+        (mlir::tpu::canFoldMinorDimsToSize(src_shape, res_shape.back()) ||\n+         mlir::tpu::canFoldMinorDimsToSize(res_shape, src_shape.back()))) {\n+      // TODO(jsreeram): Add support for picking space-efficient tilings for\n+      // small 2nd minor dim shapes.\n+      // Example 1: (4, 2, 1024) -> (4, 2048) If we infer src and tgt layout to\n+      // be (1, 128), it is no-op because essentially we just shufflle the VREGs\n+      // in VREG array.\n+      // Example 2: (4, 256) -> (1, 1024) is actually sublane\n+      // shuffle inside each vreg from [0, 1, 2, 3, 4,..7] to [0, 4, 1, 5, ...]\n+      setLayout(op,\n+                VectorLayout(layout.bitwidth(), {0, 0}, native_tiling,\n+                             ImplicitDim::kNone),\n+                VectorLayout(layout.bitwidth(), {0, 0}, native_tiling,\n+                             ImplicitDim::kNone));\n+      return success();\n+    }\n+    op.emitOpError(\"infer-vector-layout: unsupported shape cast\");\n     return failure();\n   }\n \ndiff --git a/jaxlib/mosaic/dialect/tpu/util.cc b/jaxlib/mosaic/dialect/tpu/util.cc\nindex b562f81ad534..02598bd16f9a 100644\n--- a/jaxlib/mosaic/dialect/tpu/util.cc\n+++ b/jaxlib/mosaic/dialect/tpu/util.cc\n@@ -301,4 +301,16 @@ std::optional<int64_t> getIntConst(Value v) {\n   return std::nullopt;\n }\n \n+bool canFoldMinorDimsToSize(ArrayRef<int64_t> shape, int64_t target_size) {\n+  CHECK_GE(shape.size(), 2);\n+  int64_t product = shape.back();\n+  for (int i = shape.size() - 2; i >= 1; --i) {\n+    product *= shape[i];\n+    if (product >= target_size) {\n+      break;\n+    }\n+  }\n+  return product == target_size;\n+}\n+\n }  // namespace mlir::tpu\ndiff --git a/jaxlib/mosaic/dialect/tpu/util.h b/jaxlib/mosaic/dialect/tpu/util.h\nindex ac83d95b715e..2a7325ee7b24 100644\n--- a/jaxlib/mosaic/dialect/tpu/util.h\n+++ b/jaxlib/mosaic/dialect/tpu/util.h\n@@ -284,6 +284,12 @@ inline arith::ConstantOp I32Const(int32_t value, ArrayRef<int64_t> shape,\n }\n \n std::optional<int64_t> getIntConst(Value v);\n+\n+// Returns true if the product of up to `shape.size() - 1` minor-most dimensions\n+// in `shape` equals `target_size`. The major-most dimension is not considered.\n+// Precondition: `shape` has at least 2 dimensions.\n+bool canFoldMinorDimsToSize(ArrayRef<int64_t> shape, int64_t target_size);\n+\n }  // namespace mlir::tpu\n \n #endif  // THIRD_PARTY_PY_JAX_JAXLIB_MOSAIC_DIALECT_TPU_UTIL_H_\ndiff --git a/tests/pallas/tpu_pallas_test.py b/tests/pallas/tpu_pallas_test.py\nindex 83f21bca7fc1..aac249251e2b 100644\n--- a/tests/pallas/tpu_pallas_test.py\n+++ b/tests/pallas/tpu_pallas_test.py\n@@ -3022,6 +3022,231 @@ def kernel(x_ref, out_ref):\n         out, np.zeros((8, 8, 2, 128), dtype=jnp.float32)\n     )\n \n+  # (q, m, n) -> (q, m * n) where n % 128 == 0\n+  @parameterized.parameters(\n+      (32, 16, 512, jnp.float32),\n+      (24, 1, 512, jnp.uint32),\n+      (3, 3, 256, jnp.uint32),\n+      (9, 15, 256, jnp.float32),\n+      (3, 2, 256, jnp.float32),\n+  )\n+  def test_reshape_two_minor_dims_to_R2(self, q, m, n, dtype):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(\n+          x_ref.shape[0], x_ref.shape[1] * x_ref.shape[2]\n+      )\n+\n+    x = np.arange(q * m * n, dtype=dtype).reshape(q, m, n)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m * n), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m * n]))\n+\n+  # (q, m, n, k) -> (q, m, n * k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (3, 8, 17, 512, jnp.float32),\n+      (1, 8, 9, 256, jnp.float32),\n+      (1, 8, 3, 256, jnp.uint32),\n+      (10, 1, 4, 256, jnp.uint32),\n+      (1, 2, 2, 256, jnp.float32),\n+  )\n+  def test_reshape_two_minor_dims_to_R3(self, q, m, n, k, dtype):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(\n+          x_ref.shape[0], x_ref.shape[1], x_ref.shape[2] * x_ref.shape[3]\n+      )\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q, m, n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m, n * k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m, n * k]))\n+\n+  # (p, q, m, n, k) -> (p, q * m * n * k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (5, 3, 8, 17, 512, jnp.float32),\n+      (6, 1, 8, 9, 256, jnp.float32),\n+      (16, 1, 8, 3, 256, jnp.uint32),\n+      (3, 2, 1, 4, 256, jnp.uint32),\n+      (1, 7, 2, 2, 256, jnp.float32),\n+  )\n+  def test_reshape_four_minor_dims_to_R2(self, p, q, m, n, k, dtype):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(\n+          x_ref.shape[0],\n+          x_ref.shape[1] * x_ref.shape[2] * x_ref.shape[3] * x_ref.shape[4],\n+      )\n+\n+    x = np.arange(p * q * m * n * k, dtype=dtype).reshape(p, q, m, n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((p, q * m * n * k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([p, q * m * n * k]))\n+\n+  # (q, m, n, k) -> (q, m, 1, n * k) where k % 128 == 0\n+  def test_reshape_two_minor_dims_preserve_rank(self):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = (\n+          x_ref[...]\n+          .reshape(\n+              x_ref.shape[0], x_ref.shape[1], x_ref.shape[2] * x_ref.shape[3]\n+          )\n+          .reshape(\n+              x_ref.shape[0], 1, x_ref.shape[1], x_ref.shape[2] * x_ref.shape[3]\n+          )\n+      )\n+\n+    q, m, n, k = 10, 1, 4, 256\n+    x = np.arange(q * m * n * k, dtype=jnp.float32).reshape(q, m, n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m, 1, n * k), jnp.float32),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m, 1, n * k]))\n+\n+  # (q, m, n, k) -> (q * m, n * k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (3, 8, 17, 512, jnp.float32),\n+      (1, 8, 9, 256, jnp.float32),\n+      (1, 8, 3, 256, jnp.uint32),\n+      (10, 1, 4, 256, jnp.uint32),\n+      (1, 2, 2, 256, jnp.float32),\n+  )\n+  def test_reshape_fold_two_leading_dims_and_two_minor_dims_R4_to_R2(\n+      self, q, m, n, k, dtype\n+  ):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(\n+          x_ref.shape[0] * x_ref.shape[1], x_ref.shape[2] * x_ref.shape[3]\n+      )\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q, m, n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q * m, n * k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q * m, n * k]))\n+\n+  # (q * m, n, k) -> (q, m, n * k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (2, 2, 17, 512, jnp.float32),\n+      (3, 2, 3, 256, jnp.float32),\n+      (1, 5, 4, 384, jnp.uint32),\n+  )\n+  def test_reshape_unfold_leading_dim_and_fold_two_minor_dims_R3_to_R3(\n+      self, q, m, n, k, dtype\n+  ):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(\n+          q,\n+          m,\n+          x_ref.shape[1] * x_ref.shape[2],\n+      )\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q * m, n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m, n * k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m, n * k]))\n+\n+  # (q * m, n * k) -> (q, m, n, k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (2, 2, 17, 512, jnp.float32),\n+      (3, 2, 3, 256, jnp.float32),\n+      (1, 5, 4, 384, jnp.uint32),\n+  )\n+  def test_reshape_unfold_leading_and_minor_dims_R2_to_R4(\n+      self, q, m, n, k, dtype\n+  ):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(q, m, n, k)\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q * m, n * k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m, n, k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m, n, k]))\n+\n+  # (q, m, n * k) -> (q * m, n, k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (2, 2, 17, 512, jnp.float32),\n+      (3, 2, 8, 256, jnp.float32),\n+      (1, 5, 4, 384, jnp.uint32),\n+  )\n+  def test_reshape_fold_leading_dims_and_unfold_minor_dim(\n+      self, q, m, n, k, dtype\n+  ):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(q * m, n, k)\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q, m, n * k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q * m, n, k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q * m, n, k]))\n+\n+  # (q, m, n, k) -> (q, m * n, k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (2, 2, 17, 512, jnp.float32),\n+      (3, 2, 8, 256, jnp.float32),\n+      (1, 5, 4, 384, jnp.uint32),\n+  )\n+  def test_reshape_fold_middle_dims(self, q, m, n, k, dtype):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(q, m * n, k)\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q, m, n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m * n, k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m * n, k]))\n+\n+  # (q, m * n, k) -> (q, m, n, k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (2, 2, 17, 512, jnp.float32),\n+      (3, 2, 8, 256, jnp.float32),\n+      (1, 5, 4, 384, jnp.uint32),\n+  )\n+  def test_reshape_unfold_middle_dims(self, q, m, n, k, dtype):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(q, m, n, k)\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q, m * n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m, n, k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m, n, k]))\n+\n \n class MiscellaneousInterpretTest(MiscellaneousTest):\n   INTERPRET: bool = True\n\n```",
        "from_id": [
            "jaswanth-",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Propagate use_shardy_partitioner to XlaCallModule op.\n\nPiperOrigin-RevId: 760876735",
        "output": "```diff\nCommit: a17810d1afd77add4fa7bc827d897346ca67a240\nDate: 2025-05-20T02:53:00Z\nURL: https://github.com/jax-ml/jax/commit/a17810d1afd77add4fa7bc827d897346ca67a240\nFiles changed: 2\nAdditions: +7, Deletions: -0\ndiff --git a/jax/experimental/jax2tf/README.md b/jax/experimental/jax2tf/README.md\nindex cb1c97bc7b7c..06cc5c86a109 100644\n--- a/jax/experimental/jax2tf/README.md\n+++ b/jax/experimental/jax2tf/README.md\n@@ -1007,6 +1007,8 @@ We list here a history of the serialization version numbers:\n     available in JAX since October 20th, 2023 (JAX 0.4.20),\n     and the default since February 1st, 2024 (JAX 0.4.24).\n     This is the only supported version as of 27th of March, 2024.\n+  * Version 10 propagate the `jax.config.use_shardy_partitioner` value to\n+    XlaCallModule.\n \n ## Known issues\n \ndiff --git a/jax/experimental/jax2tf/jax2tf.py b/jax/experimental/jax2tf/jax2tf.py\nindex 536bf1f201f0..3c34a26af982 100644\n--- a/jax/experimental/jax2tf/jax2tf.py\n+++ b/jax/experimental/jax2tf/jax2tf.py\n@@ -944,6 +944,11 @@ def _convert_value(val, aval):\n       if DisabledSafetyCheck.platform() in exported.disabled_safety_checks:\n         call_module_attrs[\"platforms\"] = ()  # No platform checking\n \n+  if version >= 10:\n+    call_module_attrs[\"use_shardy_partitioner\"] = (\n+        config.use_shardy_partitioner.value\n+    )\n+\n   if logging.vlog_is_on(3):\n     # We already logged the MLIR module when we exported it.\n     logging.vlog(3, \"XlaCallModule %s\", str(call_module_attrs))\n\n```",
        "from_id": [
            "bixia1",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas] Fix 1D Iota\n\nPiperOrigin-RevId: 760705572",
        "output": "```diff\nCommit: 62d59ace3087877347f606405bbcbc7196fe5201\nDate: 2025-05-19T18:33:43Z\nURL: https://github.com/jax-ml/jax/commit/62d59ace3087877347f606405bbcbc7196fe5201\nFiles changed: 2\nAdditions: +7, Deletions: -5\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 9af3cf1e3c0a..a67a71dd40f7 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -2302,11 +2302,13 @@ def _iota_lowering_rule(ctx: LoweringRuleContext, dtype, shape, dimension,\n   if len(shape) == 1:\n     if dimension != 0:\n       raise ValueError(\"Dimension must be 0 for 1D iota.\")\n-    def _1d_iota_helper(dtype, shape, dimension, sharding):\n-      iota_2d = lax.iota_p.bind(dtype, (1,) + shape, dimension, sharding)\n+    def _1d_iota_helper():\n+      iota_2d = lax.iota_p.bind(dtype=dtype,\n+                                shape=(1,) + shape,\n+                                dimension=1,\n+                                sharding=sharding)\n       return iota_2d[0]\n-    return lower_fun(_1d_iota_helper, multiple_results=False)(\n-        ctx, dtype, shape, dimension, sharding)\n+    return lower_fun(_1d_iota_helper, multiple_results=False)(ctx)\n   out_type = aval_to_ir_type(\n       ctx.lowering_context.dynamic_shape_replacement_fn, ctx.avals_out[0]\n   )\ndiff --git a/tests/pallas/ops_test.py b/tests/pallas/ops_test.py\nindex 9bb6d31d15e1..3baa26e5efd7 100644\n--- a/tests/pallas/ops_test.py\n+++ b/tests/pallas/ops_test.py\n@@ -1554,7 +1554,7 @@ def kernel(x_ref, y_ref, o_ref):\n   def test_iota(self, shape, dtype, dimension):\n     self.skip_if_mosaic_gpu()\n \n-    if jtu.test_device_matches([\"tpu\"]):\n+    if jtu.test_device_matches([\"tpu\"]) and dtype != jnp.int32:\n       self.skipTest(\"Only 32-bit integer iota supported\")\n \n     f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n```",
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28812 from yliu120:yl/debug_pbroadcast\n\nPiperOrigin-RevId: 760640954",
        "output": "```diff\nCommit: bb86bd64794146ef792820e1e2aff19c644c1aef\nDate: 2025-05-19T15:54:18Z\nURL: https://github.com/jax-ml/jax/commit/bb86bd64794146ef792820e1e2aff19c644c1aef\nFiles changed: 2\nAdditions: +49, Deletions: -3\ndiff --git a/jax/_src/lax/parallel.py b/jax/_src/lax/parallel.py\nindex 6df8690f1123..a9abf8f12939 100644\n--- a/jax/_src/lax/parallel.py\n+++ b/jax/_src/lax/parallel.py\n@@ -1122,14 +1122,27 @@ def _pbroadcast_lowering(ctx, x, *, axis_name, source):\n   def source_to_front(group):\n     return [group[source]] + list(group[:source]) + list(group[source + 1:])\n   replica_groups = [source_to_front(group) for group in replica_groups]\n-  channel = ctx.module_context.new_channel()\n+  is_spmd = isinstance(\n+      ctx.module_context.axis_context,\n+      (SPMDAxisContext, ShardingContext),\n+  )\n+  if is_spmd:\n+    # We want to emit the collective-broadcast with global device IDs and a unique\n+    # channel ID, as otherwise it interprets the devices as replicas instead\n+    # of partitions - and XLA is configured with only a single replica.\n+    channel = ctx.module_context.new_channel()\n+    channel_handle = hlo.ChannelHandle.get(channel, mlir.DEVICE_TO_DEVICE_TYPE)\n+    other_args = dict(channel_handle=channel_handle)\n+  else:\n+    other_args = {}\n   return hlo.CollectiveBroadcastOp(\n-      x, replica_groups=_replica_groups_hlo(replica_groups)).results\n+      x, replica_groups=_replica_groups_hlo(replica_groups), **other_args\n+  ).results\n \n pbroadcast_p = core.Primitive('pbroadcast')\n pbroadcast_p.def_abstract_eval(_raise_to_shaped_abstract_eval)\n ad.deflinear2(pbroadcast_p, _pbroadcast_transpose_rule)\n-mlir.register_lowering(pbroadcast_p, _pbroadcast_lowering)\n+mlir.register_lowering(pbroadcast_p, _pbroadcast_lowering, platform='gpu')\n batching.fancy_primitive_batchers[pbroadcast_p] = _pbroadcast_batcher\n batching.skippable_batchers[pbroadcast_p] = partial(_names_in_param, 'axis_name')\n \ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 2fdc846a356b..1bebba095896 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -290,6 +290,39 @@ def fwd(a):\n     c = fwd(a)\n     assert (c == jnp.reshape(a.T, (1, 64))).all()\n \n+  @parameterized.named_parameters(\n+      dict(\n+          testcase_name='_partial_replicated', replicate_on_axes='x',\n+      ),\n+      dict(\n+          testcase_name='_fully_replicated',\n+          replicate_on_axes=('x', 'y'),\n+      ),\n+  )\n+  @jtu.run_on_devices(\"gpu\")\n+  def test_pbroadcast(self, replicate_on_axes):\n+    mesh = jtu.create_mesh((4, 2), ('x', 'y'))\n+    sharded_axes = set(mesh.axis_names) - set(replicate_on_axes)\n+    sharded_axes = None if not sharded_axes else list(sharded_axes)\n+    in_out_sharding = jax.sharding.NamedSharding(mesh, P(sharded_axes, None))\n+    a = jax.device_put(jnp.arange(16).reshape((4, 4)), in_out_sharding)\n+\n+    @jax.jit\n+    @partial(\n+        shard_map,\n+        mesh=mesh,\n+        in_specs=(in_out_sharding.spec,),\n+        out_specs=in_out_sharding.spec,\n+        check_vma=False,\n+    )\n+    def fwd(x):\n+      axis_index = lax.axis_index(replicate_on_axes)\n+      x = jnp.where(axis_index == 0, x + 1, x)\n+      return lax.pbroadcast(x, replicate_on_axes, source=0)\n+\n+    c = fwd(a)  # Don't crash\n+    self.assertAllClose(c, a + 1)\n+\n   def test_all_to_all_with_axis_index_groups(self):\n     mesh = jtu.create_mesh((4,), ('x',))\n     a = jax.device_put(\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28829 from hawkinsp:req\n\nPiperOrigin-RevId: 760636732",
        "output": "```diff\nCommit: 74b595fe0d8b78e3d775adac5995243ff7b28e05\nDate: 2025-05-19T15:41:15Z\nURL: https://github.com/jax-ml/jax/commit/74b595fe0d8b78e3d775adac5995243ff7b28e05\nFiles changed: 10\nAdditions: +72, Deletions: -80\ndiff --git a/.bazelrc b/.bazelrc\nindex 79df03863b02..53676637c839 100644\n--- a/.bazelrc\n+++ b/.bazelrc\n@@ -244,10 +244,6 @@ build:ci_linux_aarch64_base --config=clang --verbose_failures=true\n build:ci_linux_aarch64_base --action_env=TF_SYSROOT=\"/dt10\"\n build:ci_linux_aarch64_base --color=yes\n \n-# Workaround for https://github.com/numpy/numpy/issues/28843\n-# TODO(phawkins): remove this after upgrading to NumPy 2.2.6.\n-build:ci_linux_aarch64_base --test_env=OMP_NUM_THREADS=8\n-\n build:ci_linux_aarch64 --config=ci_linux_aarch64_base\n build:ci_linux_aarch64 --host_crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n build:ci_linux_aarch64 --crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n@@ -383,10 +379,6 @@ build:rbe_cross_compile_base --remote_instance_name=projects/tensorflow-testing/\n build:rbe_cross_compile_linux_aarch64 --config=cross_compile_linux_aarch64\n build:rbe_cross_compile_linux_aarch64 --config=rbe_cross_compile_base\n \n-# Workaround for https://github.com/numpy/numpy/issues/28843\n-# TODO(phawkins): remove this after upgrading to NumPy 2.2.6.\n-build:rbe_cross_compile_linux_aarch64 --test_env=OMP_NUM_THREADS=8\n-\n # Mac x86\n build:cross_compile_darwin_x86_64 --config=cross_compile_base\n build:cross_compile_darwin_x86_64 --config=nonccl\ndiff --git a/build/freethreading-requirements.txt b/build/freethreading-requirements.txt\nindex cc302cffdd0c..467578870ee9 100644\n--- a/build/freethreading-requirements.txt\n+++ b/build/freethreading-requirements.txt\n@@ -1,3 +1,3 @@\n # Under free-threading, we need an up-to-date numpy at least for the moment.\n-numpy~=2.2.5; python_version==\"3.13\"\n-numpy>=2.2.5; python_version>=\"3.14\"\n+numpy~=2.2.6; python_version==\"3.13\"\n+numpy>=2.2.6; python_version>=\"3.14\"\ndiff --git a/build/nonfreethreading-requirements.txt b/build/nonfreethreading-requirements.txt\nindex f8171559a142..8bd139bf99ac 100644\n--- a/build/nonfreethreading-requirements.txt\n+++ b/build/nonfreethreading-requirements.txt\n@@ -1,6 +1,6 @@\n numpy~=2.0.0; python_version<=\"3.12\"\n numpy~=2.1.0; python_version==\"3.13\"\n-numpy>=2.2.5; python_version>=\"3.14\"\n+numpy>=2.2.6; python_version>=\"3.14\"\n \n # These packages have not released free-threaded wheels.\n zstandard\ndiff --git a/build/requirements.in b/build/requirements.in\nindex 8b8af9d6b591..c5ce2ea279bd 100644\n--- a/build/requirements.in\n+++ b/build/requirements.in\n@@ -19,8 +19,8 @@ wheel\n jaxlib\n \n # The with-cuda extra also includes NVIDIA's pip packages.\n-jax-cuda12-plugin[with-cuda]\n-jax-cuda12-pjrt\n+jax-cuda12-plugin[with-cuda] ; sys_platform == \"linux\"\n+jax-cuda12-pjrt ; sys_platform == \"linux\"\n \n # TPU dependencies\n libtpu ; sys_platform == \"linux\" and platform_machine == \"x86_64\"\ndiff --git a/build/requirements_lock_3_10.txt b/build/requirements_lock_3_10.txt\nindex c4ca6088e4bf..a4c6b1bf2b77 100644\n--- a/build/requirements_lock_3_10.txt\n+++ b/build/requirements_lock_3_10.txt\n@@ -160,13 +160,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_11.txt b/build/requirements_lock_3_11.txt\nindex 1f667115af04..0633e733414b 100644\n--- a/build/requirements_lock_3_11.txt\n+++ b/build/requirements_lock_3_11.txt\n@@ -154,13 +154,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_12.txt b/build/requirements_lock_3_12.txt\nindex 20ca67a3e921..1ab77a6ec36e 100644\n--- a/build/requirements_lock_3_12.txt\n+++ b/build/requirements_lock_3_12.txt\n@@ -154,13 +154,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_13.txt b/build/requirements_lock_3_13.txt\nindex 804373b03899..c20068b732e6 100644\n--- a/build/requirements_lock_3_13.txt\n+++ b/build/requirements_lock_3_13.txt\n@@ -181,13 +181,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_13_ft.txt b/build/requirements_lock_3_13_ft.txt\nindex c7a1c882fc73..3795343df0cb 100644\n--- a/build/requirements_lock_3_13_ft.txt\n+++ b/build/requirements_lock_3_13_ft.txt\n@@ -172,13 +172,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n@@ -371,62 +371,62 @@ mpmath==1.3.0 \\\n     --hash=sha256:7a28eb2a9774d00c7bc92411c19a89209d5da7c4c9a9e227be8330a23a25b91f \\\n     --hash=sha256:a0b2b9fe80bbcd81a6647ff13108738cfb482d481d826cc0e02f5b35e5c88d2c\n     # via -r build/test-requirements.txt\n-numpy==2.2.5 ; python_version == \"3.13\" \\\n-    --hash=sha256:0255732338c4fdd00996c0421884ea8a3651eea555c3a56b84892b66f696eb70 \\\n-    --hash=sha256:02f226baeefa68f7d579e213d0f3493496397d8f1cff5e2b222af274c86a552a \\\n-    --hash=sha256:059b51b658f4414fff78c6d7b1b4e18283ab5fa56d270ff212d5ba0c561846f4 \\\n-    --hash=sha256:0bcb1d057b7571334139129b7f941588f69ce7c4ed15a9d6162b2ea54ded700c \\\n-    --hash=sha256:0cd48122a6b7eab8f06404805b1bd5856200e3ed6f8a1b9a194f9d9054631beb \\\n-    --hash=sha256:19f4718c9012e3baea91a7dba661dcab2451cda2550678dc30d53acb91a7290f \\\n-    --hash=sha256:1a161c2c79ab30fe4501d5a2bbfe8b162490757cf90b7f05be8b80bc02f7bb8e \\\n-    --hash=sha256:1f4a922da1729f4c40932b2af4fe84909c7a6e167e6e99f71838ce3a29f3fe26 \\\n-    --hash=sha256:261a1ef047751bb02f29dfe337230b5882b54521ca121fc7f62668133cb119c9 \\\n-    --hash=sha256:262d23f383170f99cd9191a7c85b9a50970fe9069b2f8ab5d786eca8a675d60b \\\n-    --hash=sha256:2ba321813a00e508d5421104464510cc962a6f791aa2fca1c97b1e65027da80d \\\n-    --hash=sha256:2c1a1c6ccce4022383583a6ded7bbcda22fc635eb4eb1e0a053336425ed36dfa \\\n-    --hash=sha256:352d330048c055ea6db701130abc48a21bec690a8d38f8284e00fab256dc1376 \\\n-    --hash=sha256:369e0d4647c17c9363244f3468f2227d557a74b6781cb62ce57cf3ef5cc7c610 \\\n-    --hash=sha256:36ab5b23915887543441efd0417e6a3baa08634308894316f446027611b53bf1 \\\n-    --hash=sha256:37e32e985f03c06206582a7323ef926b4e78bdaa6915095ef08070471865b906 \\\n-    --hash=sha256:3a801fef99668f309b88640e28d261991bfad9617c27beda4a3aec4f217ea073 \\\n-    --hash=sha256:3d14b17b9be5f9c9301f43d2e2a4886a33b53f4e6fdf9ca2f4cc60aeeee76372 \\\n-    --hash=sha256:422cc684f17bc963da5f59a31530b3936f57c95a29743056ef7a7903a5dbdf88 \\\n-    --hash=sha256:4520caa3807c1ceb005d125a75e715567806fed67e315cea619d5ec6e75a4191 \\\n-    --hash=sha256:47834cde750d3c9f4e52c6ca28a7361859fcaf52695c7dc3cc1a720b8922683e \\\n-    --hash=sha256:47f9ed103af0bc63182609044b0490747e03bd20a67e391192dde119bf43d52f \\\n-    --hash=sha256:498815b96f67dc347e03b719ef49c772589fb74b8ee9ea2c37feae915ad6ebda \\\n-    --hash=sha256:54088a5a147ab71a8e7fdfd8c3601972751ded0739c6b696ad9cb0343e21ab73 \\\n-    --hash=sha256:55f09e00d4dccd76b179c0f18a44f041e5332fd0e022886ba1c0bbf3ea4a18d0 \\\n-    --hash=sha256:5a0ac90e46fdb5649ab6369d1ab6104bfe5854ab19b645bf5cda0127a13034ae \\\n-    --hash=sha256:6411f744f7f20081b1b4e7112e0f4c9c5b08f94b9f086e6f0adf3645f85d3a4d \\\n-    --hash=sha256:6413d48a9be53e183eb06495d8e3b006ef8f87c324af68241bbe7a39e8ff54c3 \\\n-    --hash=sha256:7451f92eddf8503c9b8aa4fe6aa7e87fd51a29c2cfc5f7dbd72efde6c65acf57 \\\n-    --hash=sha256:8b4c0773b6ada798f51f0f8e30c054d32304ccc6e9c5d93d46cb26f3d385ab19 \\\n-    --hash=sha256:8dfa94b6a4374e7851bbb6f35e6ded2120b752b063e6acdd3157e4d2bb922eba \\\n-    --hash=sha256:97c8425d4e26437e65e1d189d22dff4a079b747ff9c2788057bfb8114ce1e133 \\\n-    --hash=sha256:9d75f338f5f79ee23548b03d801d28a505198297534f62416391857ea0479571 \\\n-    --hash=sha256:9de6832228f617c9ef45d948ec1cd8949c482238d68b2477e6f642c33a7b0a54 \\\n-    --hash=sha256:a4cbdef3ddf777423060c6f81b5694bad2dc9675f110c4b2a60dc0181543fac7 \\\n-    --hash=sha256:a9c0d994680cd991b1cb772e8b297340085466a6fe964bc9d4e80f5e2f43c291 \\\n-    --hash=sha256:aa70fdbdc3b169d69e8c59e65c07a1c9351ceb438e627f0fdcd471015cd956be \\\n-    --hash=sha256:abe38cd8381245a7f49967a6010e77dbf3680bd3627c0fe4362dd693b404c7f8 \\\n-    --hash=sha256:b13f04968b46ad705f7c8a80122a42ae8f620536ea38cf4bdd374302926424dd \\\n-    --hash=sha256:b4ea7e1cff6784e58fe281ce7e7f05036b3e1c89c6f922a6bfbc0a7e8768adbe \\\n-    --hash=sha256:b6f91524d31b34f4a5fee24f5bc16dcd1491b668798b6d85585d836c1e633a6a \\\n-    --hash=sha256:c26843fd58f65da9491165072da2cccc372530681de481ef670dcc8e27cfb066 \\\n-    --hash=sha256:c42365005c7a6c42436a54d28c43fe0e01ca11eb2ac3cefe796c25a5f98e5e9b \\\n-    --hash=sha256:c8b82a55ef86a2d8e81b63da85e55f5537d2157165be1cb2ce7cfa57b6aef38b \\\n-    --hash=sha256:ced69262a8278547e63409b2653b372bf4baff0870c57efa76c5703fd6543282 \\\n-    --hash=sha256:d2e3bdadaba0e040d1e7ab39db73e0afe2c74ae277f5614dad53eadbecbbb169 \\\n-    --hash=sha256:d403c84991b5ad291d3809bace5e85f4bbf44a04bdc9a88ed2bb1807b3360bb8 \\\n-    --hash=sha256:d7543263084a85fbc09c704b515395398d31d6395518446237eac219eab9e55e \\\n-    --hash=sha256:d8882a829fd779f0f43998e931c466802a77ca1ee0fe25a3abe50278616b1471 \\\n-    --hash=sha256:e4f0b035d9d0ed519c813ee23e0a733db81ec37d2e9503afbb6e54ccfdee0fa7 \\\n-    --hash=sha256:e8b025c351b9f0e8b5436cf28a07fa4ac0204d67b38f01433ac7f9b870fa38c6 \\\n-    --hash=sha256:eb7fd5b184e5d277afa9ec0ad5e4eb562ecff541e7f60e69ee69c8d59e9aeaba \\\n-    --hash=sha256:ec31367fd6a255dc8de4772bd1658c3e926d8e860a0b6e922b615e532d320ddc \\\n-    --hash=sha256:ee461a4eaab4f165b68780a6a1af95fb23a29932be7569b9fab666c407969051 \\\n-    --hash=sha256:f5045039100ed58fa817a6227a356240ea1b9a1bc141018864c306c1a16d4175\n+numpy==2.2.6 ; python_version == \"3.13\" \\\n+    --hash=sha256:038613e9fb8c72b0a41f025a7e4c3f0b7a1b5d768ece4796b674c8f3fe13efff \\\n+    --hash=sha256:0678000bb9ac1475cd454c6b8c799206af8107e310843532b04d49649c717a47 \\\n+    --hash=sha256:0811bb762109d9708cca4d0b13c4f67146e3c3b7cf8d34018c722adb2d957c84 \\\n+    --hash=sha256:0b605b275d7bd0c640cad4e5d30fa701a8d59302e127e5f79138ad62762c3e3d \\\n+    --hash=sha256:0bca768cd85ae743b2affdc762d617eddf3bcf8724435498a1e80132d04879e6 \\\n+    --hash=sha256:1bc23a79bfabc5d056d106f9befb8d50c31ced2fbc70eedb8155aec74a45798f \\\n+    --hash=sha256:287cc3162b6f01463ccd86be154f284d0893d2b3ed7292439ea97eafa8170e0b \\\n+    --hash=sha256:37c0ca431f82cd5fa716eca9506aefcabc247fb27ba69c5062a6d3ade8cf8f49 \\\n+    --hash=sha256:37e990a01ae6ec7fe7fa1c26c55ecb672dd98b19c3d0e1d1f326fa13cb38d163 \\\n+    --hash=sha256:389d771b1623ec92636b0786bc4ae56abafad4a4c513d36a55dce14bd9ce8571 \\\n+    --hash=sha256:3d70692235e759f260c3d837193090014aebdf026dfd167834bcba43e30c2a42 \\\n+    --hash=sha256:41c5a21f4a04fa86436124d388f6ed60a9343a6f767fced1a8a71c3fbca038ff \\\n+    --hash=sha256:481b49095335f8eed42e39e8041327c05b0f6f4780488f61286ed3c01368d491 \\\n+    --hash=sha256:4eeaae00d789f66c7a25ac5f34b71a7035bb474e679f410e5e1a94deb24cf2d4 \\\n+    --hash=sha256:55a4d33fa519660d69614a9fad433be87e5252f4b03850642f88993f7b2ca566 \\\n+    --hash=sha256:5a6429d4be8ca66d889b7cf70f536a397dc45ba6faeb5f8c5427935d9592e9cf \\\n+    --hash=sha256:5bd4fc3ac8926b3819797a7c0e2631eb889b4118a9898c84f585a54d475b7e40 \\\n+    --hash=sha256:5beb72339d9d4fa36522fc63802f469b13cdbe4fdab4a288f0c441b74272ebfd \\\n+    --hash=sha256:6031dd6dfecc0cf9f668681a37648373bddd6421fff6c66ec1624eed0180ee06 \\\n+    --hash=sha256:71594f7c51a18e728451bb50cc60a3ce4e6538822731b2933209a1f3614e9282 \\\n+    --hash=sha256:74d4531beb257d2c3f4b261bfb0fc09e0f9ebb8842d82a7b4209415896adc680 \\\n+    --hash=sha256:7befc596a7dc9da8a337f79802ee8adb30a552a94f792b9c9d18c840055907db \\\n+    --hash=sha256:894b3a42502226a1cac872f840030665f33326fc3dac8e57c607905773cdcde3 \\\n+    --hash=sha256:8e41fd67c52b86603a91c1a505ebaef50b3314de0213461c7a6e99c9a3beff90 \\\n+    --hash=sha256:8e9ace4a37db23421249ed236fdcdd457d671e25146786dfc96835cd951aa7c1 \\\n+    --hash=sha256:8fc377d995680230e83241d8a96def29f204b5782f371c532579b4f20607a289 \\\n+    --hash=sha256:9551a499bf125c1d4f9e250377c1ee2eddd02e01eac6644c080162c0c51778ab \\\n+    --hash=sha256:b0544343a702fa80c95ad5d3d608ea3599dd54d4632df855e4c8d24eb6ecfa1c \\\n+    --hash=sha256:b093dd74e50a8cba3e873868d9e93a85b78e0daf2e98c6797566ad8044e8363d \\\n+    --hash=sha256:b412caa66f72040e6d268491a59f2c43bf03eb6c96dd8f0307829feb7fa2b6fb \\\n+    --hash=sha256:b4f13750ce79751586ae2eb824ba7e1e8dba64784086c98cdbbcc6a42112ce0d \\\n+    --hash=sha256:b64d8d4d17135e00c8e346e0a738deb17e754230d7e0810ac5012750bbd85a5a \\\n+    --hash=sha256:ba10f8411898fc418a521833e014a77d3ca01c15b0c6cdcce6a0d2897e6dbbdf \\\n+    --hash=sha256:bd48227a919f1bafbdda0583705e547892342c26fb127219d60a5c36882609d1 \\\n+    --hash=sha256:c1f9540be57940698ed329904db803cf7a402f3fc200bfe599334c9bd84a40b2 \\\n+    --hash=sha256:c820a93b0255bc360f53eca31a0e676fd1101f673dda8da93454a12e23fc5f7a \\\n+    --hash=sha256:ce47521a4754c8f4593837384bd3424880629f718d87c5d44f8ed763edd63543 \\\n+    --hash=sha256:d042d24c90c41b54fd506da306759e06e568864df8ec17ccc17e9e884634fd00 \\\n+    --hash=sha256:de749064336d37e340f640b05f24e9e3dd678c57318c7289d222a8a2f543e90c \\\n+    --hash=sha256:e1dda9c7e08dc141e0247a5b8f49cf05984955246a327d4c48bda16821947b2f \\\n+    --hash=sha256:e29554e2bef54a90aa5cc07da6ce955accb83f21ab5de01a62c8478897b264fd \\\n+    --hash=sha256:e3143e4451880bed956e706a3220b4e5cf6172ef05fcc397f6f36a550b1dd868 \\\n+    --hash=sha256:e8213002e427c69c45a52bbd94163084025f533a55a59d6f9c5b820774ef3303 \\\n+    --hash=sha256:efd28d4e9cd7d7a8d39074a4d44c63eda73401580c5c76acda2ce969e0a38e83 \\\n+    --hash=sha256:f0fd6321b839904e15c46e0d257fdd101dd7f530fe03fd6359c1ea63738703f3 \\\n+    --hash=sha256:f1372f041402e37e5e633e586f62aa53de2eac8d98cbfb822806ce4bbefcb74d \\\n+    --hash=sha256:f2618db89be1b4e05f7a1a847a9c1c0abd63e63a1607d892dd54668dd92faf87 \\\n+    --hash=sha256:f447e6acb680fd307f40d3da4852208af94afdfab89cf850986c3ca00562f4fa \\\n+    --hash=sha256:f92729c95468a2f4f15e9bb94c432a9229d0d50de67304399627a943201baa2f \\\n+    --hash=sha256:f9f1adb22318e121c5c69a09142811a201ef17ab257a1e66ca3025065b7f53ae \\\n+    --hash=sha256:fc0c5673685c508a142ca65209b4e79ed6740a4ed6b2267dbba90f34b0b3cfda \\\n+    --hash=sha256:fc7b73d02efb0e18c000e9ad8b83480dfcd5dfd11065997ed4c6747470ae8915 \\\n+    --hash=sha256:fd83c01228a688733f1ded5201c678f0c53ecc1006ffbc404db9f7a899ac6249 \\\n+    --hash=sha256:fe27749d33bb772c80dcd84ae7e8df2adc920ae8297400dabec45f0dedb3f6de \\\n+    --hash=sha256:fee4236c876c4e8369388054d02d0e9bb84821feb1a64dd59e137e6511a551f8\n     # via\n     #   -r build/freethreading-requirements.txt\n     #   contourpy\ndiff --git a/build/requirements_lock_3_14.txt b/build/requirements_lock_3_14.txt\nindex 6edcd30ebe16..157dca5adbab 100644\n--- a/build/requirements_lock_3_14.txt\n+++ b/build/requirements_lock_3_14.txt\n@@ -48,7 +48,7 @@ ml-dtypes==0.5.1\n     #   tensorstore\n mpmath==1.4.0a4\n     # via -r build/test-requirements.txt\n-numpy==2.2.5\n+numpy==2.2.6\n     # via\n     #   -r build/nonfreethreading-requirements.txt\n     #   contourpy\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28828 from hawkinsp:vercheck\n\nPiperOrigin-RevId: 760633986",
        "output": "```diff\nCommit: 2765178b10b8b42d14c322a5275c402d2fcde0c1\nDate: 2025-05-19T15:34:03Z\nURL: https://github.com/jax-ml/jax/commit/2765178b10b8b42d14c322a5275c402d2fcde0c1\nFiles changed: 1\nAdditions: +8, Deletions: -5\ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 9df7fc69ff1a..02bcbcf16dbc 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -180,11 +180,14 @@ def _version_check(name: str,\n                  cuda_versions.cufft_build_version,\n                  # Ignore patch versions.\n                  scale_for_comparison=100)\n-  _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n-                 cuda_versions.cusolver_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=11400)\n+  # TODO(phawkins): for some reason this check fails with a cusolver internal\n+  # error when fetching the version. This may be a path error from our stubs.\n+  # Figure out what's happening here and reenable.\n+  # _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n+  #                cuda_versions.cusolver_build_version,\n+  #                # Ignore patch versions.\n+  #                scale_for_comparison=100,\n+  #                min_supported_version=11400)\n   _version_check(\"cuPTI\", cuda_versions.cupti_get_version,\n                  cuda_versions.cupti_build_version,\n                  min_supported_version=18)\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "shard map pbroadcast",
        "output": "```diff\nCommit: 3143214ff69c62523ed1a6baf47fd89dc40bfef9\nDate: 2025-05-19T15:25:27Z\nURL: https://github.com/jax-ml/jax/commit/3143214ff69c62523ed1a6baf47fd89dc40bfef9\nFiles changed: 2\nAdditions: +49, Deletions: -3\ndiff --git a/jax/_src/lax/parallel.py b/jax/_src/lax/parallel.py\nindex 6df8690f1123..a9abf8f12939 100644\n--- a/jax/_src/lax/parallel.py\n+++ b/jax/_src/lax/parallel.py\n@@ -1122,14 +1122,27 @@ def _pbroadcast_lowering(ctx, x, *, axis_name, source):\n   def source_to_front(group):\n     return [group[source]] + list(group[:source]) + list(group[source + 1:])\n   replica_groups = [source_to_front(group) for group in replica_groups]\n-  channel = ctx.module_context.new_channel()\n+  is_spmd = isinstance(\n+      ctx.module_context.axis_context,\n+      (SPMDAxisContext, ShardingContext),\n+  )\n+  if is_spmd:\n+    # We want to emit the collective-broadcast with global device IDs and a unique\n+    # channel ID, as otherwise it interprets the devices as replicas instead\n+    # of partitions - and XLA is configured with only a single replica.\n+    channel = ctx.module_context.new_channel()\n+    channel_handle = hlo.ChannelHandle.get(channel, mlir.DEVICE_TO_DEVICE_TYPE)\n+    other_args = dict(channel_handle=channel_handle)\n+  else:\n+    other_args = {}\n   return hlo.CollectiveBroadcastOp(\n-      x, replica_groups=_replica_groups_hlo(replica_groups)).results\n+      x, replica_groups=_replica_groups_hlo(replica_groups), **other_args\n+  ).results\n \n pbroadcast_p = core.Primitive('pbroadcast')\n pbroadcast_p.def_abstract_eval(_raise_to_shaped_abstract_eval)\n ad.deflinear2(pbroadcast_p, _pbroadcast_transpose_rule)\n-mlir.register_lowering(pbroadcast_p, _pbroadcast_lowering)\n+mlir.register_lowering(pbroadcast_p, _pbroadcast_lowering, platform='gpu')\n batching.fancy_primitive_batchers[pbroadcast_p] = _pbroadcast_batcher\n batching.skippable_batchers[pbroadcast_p] = partial(_names_in_param, 'axis_name')\n \ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 2fdc846a356b..1bebba095896 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -290,6 +290,39 @@ def fwd(a):\n     c = fwd(a)\n     assert (c == jnp.reshape(a.T, (1, 64))).all()\n \n+  @parameterized.named_parameters(\n+      dict(\n+          testcase_name='_partial_replicated', replicate_on_axes='x',\n+      ),\n+      dict(\n+          testcase_name='_fully_replicated',\n+          replicate_on_axes=('x', 'y'),\n+      ),\n+  )\n+  @jtu.run_on_devices(\"gpu\")\n+  def test_pbroadcast(self, replicate_on_axes):\n+    mesh = jtu.create_mesh((4, 2), ('x', 'y'))\n+    sharded_axes = set(mesh.axis_names) - set(replicate_on_axes)\n+    sharded_axes = None if not sharded_axes else list(sharded_axes)\n+    in_out_sharding = jax.sharding.NamedSharding(mesh, P(sharded_axes, None))\n+    a = jax.device_put(jnp.arange(16).reshape((4, 4)), in_out_sharding)\n+\n+    @jax.jit\n+    @partial(\n+        shard_map,\n+        mesh=mesh,\n+        in_specs=(in_out_sharding.spec,),\n+        out_specs=in_out_sharding.spec,\n+        check_vma=False,\n+    )\n+    def fwd(x):\n+      axis_index = lax.axis_index(replicate_on_axes)\n+      x = jnp.where(axis_index == 0, x + 1, x)\n+      return lax.pbroadcast(x, replicate_on_axes, source=0)\n+\n+    c = fwd(a)  # Don't crash\n+    self.assertAllClose(c, a + 1)\n+\n   def test_all_to_all_with_axis_index_groups(self):\n     mesh = jtu.create_mesh((4,), ('x',))\n     a = jax.device_put(\n\n```",
        "from_id": [
            "yliu120"
        ]
    },
    {
        "text_input": "Fix the CAS loop in semaphore_wait lowering\n\nThe loop incorrectly assumed that we're waiting for the semaphore\nvalue to be equal to the wait value, so that we can reset it to 0.\nThis, however, is not how semaphores work. The CAS loop should wait\nuntil the value is _at least_ the wait value, and should update its\nexpectation at every step in case the swap failed.\n\nThe attached test deadlocks with the original implementation, but\nworks fine with the new one.\n\nPiperOrigin-RevId: 760619094",
        "output": "```diff\nCommit: ea5a47d44a69f782d153fbecd5cf6999af77caec\nDate: 2025-05-19T14:41:17Z\nURL: https://github.com/jax-ml/jax/commit/ea5a47d44a69f782d153fbecd5cf6999af77caec\nFiles changed: 2\nAdditions: +41, Deletions: -8\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 73623de43e39..bd304e8b6745 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -2999,6 +2999,11 @@ def _semaphore_signal_lowering_rule(\n     )\n   # TODO(apaszke): Narrow the scope from .sys to .gpu when the semaphore is local.\n   val = _ir_constant(value, i32)\n+  # We only signal the semaphore from a single lane, which does not guarantee\n+  # anything about the state of the other three warps in the warpgroup (they\n+  # might still be e.g. reading memory that someone will overwrite once they\n+  # receive a signal).\n+  mgpu.utils.warpgroup_barrier()\n   pred = ctx.module_ctx.single_wg_lane_predicate\n   llvm_dialect.inline_asm(\n     i32,\n@@ -3022,23 +3027,25 @@ def _semaphore_wait_lowering_rule(ctx: LoweringRuleContext, *args, args_tree):\n   sem_ptr = mgpu.utils.memref_ptr(sem)\n   i32_ty = ir.IntegerType.get_signless(32)\n   ne_pred = arith_dialect.CmpIPredicate.ne\n-  zero_const = mgpu.utils.c(0, i32_ty)\n   val = _ir_constant(value, i32_ty)\n \n   with mgpu.single_thread(scope=mgpu.ThreadSubset.WARPGROUP):\n     # Create the while loop for busy waiting\n-    while_op = scf_dialect.WhileOp([i32_ty], [zero_const])\n+    while_op = scf_dialect.WhileOp([i32_ty], [val])\n     before_block = while_op.before.blocks.append(i32_ty)\n     with ir.InsertionPoint.at_block_begin(before_block):\n-      old_val = llvm_dialect.inline_asm(\n+      [expected_in_memory] = before_block.arguments\n+      new_val = arith_dialect.subi(expected_in_memory, val)\n+      in_memory = llvm_dialect.inline_asm(\n         i32_ty,\n-        [sem_ptr, val, zero_const],\n+        [sem_ptr, expected_in_memory, new_val],\n         \"atom.acquire.sys.global.cas.b32 $0, [$1], $2, $3;\",\n         \"=r,l,r,r\",\n         has_side_effects=True,\n       )\n-      comparison = arith_dialect.cmpi(ne_pred, old_val, val)\n-      scf_dialect.condition(comparison, before_block.arguments)\n+      comparison = arith_dialect.cmpi(ne_pred, in_memory, expected_in_memory)\n+      new_expected_in_memory = arith_dialect.maxui(in_memory, val)\n+      scf_dialect.condition(comparison, [new_expected_in_memory])\n     after_block = while_op.after.blocks.append(i32_ty)\n     with ir.InsertionPoint.at_block_begin(after_block):\n       scf_dialect.yield_(after_block.arguments)\ndiff --git a/tests/pallas/gpu_pallas_distributed_test.py b/tests/pallas/gpu_pallas_distributed_test.py\nindex 3da39ba925c7..d862e6b9b819 100644\n--- a/tests/pallas/gpu_pallas_distributed_test.py\n+++ b/tests/pallas/gpu_pallas_distributed_test.py\n@@ -44,8 +44,6 @@ def setUp(self):\n     super().setUp()\n \n   def test_basic_remote_dma(self):\n-    if jax.process_count() < 2:\n-      self.skipTest(\"Test requires multiple processes.\")\n     if jax.process_index() > 2:\n       return  # Only 2 processes needed.\n     def kernel(x_ref, y_ref, ready_sem, recv_sem):\n@@ -86,6 +84,34 @@ def body(x):\n     expected = x[8:] if jax.process_index() == 0 else x[:8]\n     np.testing.assert_allclose(y.addressable_shards[0].data, expected)\n \n+  def test_wait_twice(self):\n+    if jax.process_index() > 2:\n+      return  # Only 2 processes needed.\n+\n+    def kernel(y_ref, sem):\n+      other_dev_id = 1 - lax.axis_index('x')\n+      pl.semaphore_signal(sem, 2, device_id=other_dev_id,\n+                          device_id_type=pl.DeviceIdType.LOGICAL)\n+      pl.semaphore_wait(sem)\n+      pl.semaphore_wait(sem)\n+      y_ref[...] = jnp.ones_like(y_ref)\n+\n+    kernel_call = pl.pallas_call(\n+        kernel,\n+        out_specs=pl.BlockSpec(memory_space=plgpu.GMEM),\n+        out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32),\n+        scratch_shapes=[plgpu.SemaphoreType.REGULAR],\n+    )\n+\n+    devices = jax.devices()[:2]\n+    mesh = jax.sharding.Mesh(devices, ['x'])\n+    y = jax.jit(\n+        shard_map.shard_map(\n+            kernel_call, mesh, in_specs=(), out_specs=P(None), check_rep=False,\n+        )\n+    )()\n+    np.testing.assert_allclose(y, jnp.ones_like(y))\n+\n \n if __name__ == '__main__':\n   jt_multiprocess.main()\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Adding an optional causal masking to the manual pipelining example.\nTesting shows the runtime is about half the flops with causal masking vs without.\n\nTests pass if we revert to cuda 12.0 with `--//third_party/gpus/cuda:by_exception_only_cuda_version_override=12_0`\n\nPiperOrigin-RevId: 760616647",
        "output": "```diff\nCommit: ec9e71e1e71e015bc6cdccf825a554da41f5368d\nDate: 2025-05-19T14:29:19Z\nURL: https://github.com/jax-ml/jax/commit/ec9e71e1e71e015bc6cdccf825a554da41f5368d\nFiles changed: 3\nAdditions: +95, Deletions: -23\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex 3256953cd332..a100aa96faba 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -33,6 +33,7 @@ class TuningConfig:\n   block_kv: int\n   max_concurrent_steps: int\n   use_schedule_barrier: bool = True\n+  causal: bool = False\n   compute_wgs_bwd: int = 1\n \n   block_q_dkv: int | None = None\n@@ -84,6 +85,8 @@ def _attention_forward(q, k, v, config: TuningConfig, save_residuals: bool = Fal\n       config.max_concurrent_steps, kv_seq_len // config.block_kv\n   )\n   block_q, block_kv = config.block_q, config.block_kv\n+  if kv_seq_len % block_kv:\n+    raise ValueError(f\"{kv_seq_len=} must be a multiple of {block_kv=}\")\n \n   def kernel(q_ref, k_ref, v_ref, out_ref, lse_ref, scoped):\n     batch = lax.axis_index(\"batch\")\n@@ -97,6 +100,12 @@ def perform_schedule_barrier():\n       plgpu.barrier_arrive(schedule_barrier)\n       plgpu.barrier_wait(schedule_barrier)\n \n+    if config.causal:\n+      block_q_end = (lax.axis_index(\"q_seq\") + 1) * (2 * block_q)\n+      block_max_kv_steps = pl.cdiv(block_q_end, jnp.array(block_kv, jnp.int32))\n+    else:\n+      block_max_kv_steps = kv_seq_len // block_kv\n+\n     @pl.when(wg_idx < 2)\n     def _compute_wg():\n       plgpu.set_max_registers(232, action=\"increase\")\n@@ -104,6 +113,11 @@ def _compute_wg():\n       lse_smem = lse_smem2.at[wg_idx] if lse_smem2 is not None else None\n       q_seq_base = lax.axis_index(\"q_seq\") * (2 * block_q) + wg_idx * block_q\n \n+      if config.causal:\n+        kv_steps = pl.cdiv(q_seq_base + block_q, jnp.array(block_kv, jnp.int32))\n+      else:\n+        kv_steps = block_max_kv_steps\n+\n       plgpu.copy_gmem_to_smem(\n           q_ref.at[batch, pl.ds(q_seq_base, block_q), q_head],\n           qo_smem,\n@@ -121,12 +135,14 @@ def _compute_wg():\n           jnp.full((block_q, head_dim), 0, dtype=jnp.float32), plgpu.Layout.WGMMA,\n       )\n \n-      plgpu.barrier_wait(k_barriers.at[0])\n+      @pl.when(kv_steps > 0)\n+      def _():\n+        plgpu.barrier_wait(k_barriers.at[0])\n \n       pl.when(wg_idx == 1)(perform_schedule_barrier)\n-      def kv_loop(kv_step, carry):\n+      def kv_loop(kv_step, carry, causal: bool = False):\n         acc, m_i, l_i = carry\n-        slot = lax.rem(kv_step, max_concurrent_steps)\n+        slot = lax.rem(kv_step, jnp.array(max_concurrent_steps, kv_step.dtype))\n \n         # QK\n         def compute_qk(acc_ref):\n@@ -136,6 +152,12 @@ def compute_qk(acc_ref):\n         qk = pl.run_scoped(compute_qk, plgpu.ACC((block_q, block_kv), jnp.float32))\n         plgpu.barrier_arrive(k_consumed_barriers.at[slot])\n \n+        if causal:\n+          q_ids = plgpu.broadcasted_iota(jnp.int32, (block_q, block_kv), 0, layout=plgpu.Layout.WGMMA)\n+          kv_ids = plgpu.broadcasted_iota(jnp.int32, (block_q, block_kv), 1, layout=plgpu.Layout.WGMMA)\n+          mask = (q_ids + q_seq_base) >= (kv_ids + kv_step * block_kv)\n+          qk = jnp.where(mask, qk, -jnp.inf)\n+\n         # Softmax\n         # We keep m scaled by log2e to use FMA instructions when computing p.\n         log2e = math.log2(math.e)\n@@ -166,18 +188,35 @@ def compute_pv(acc_ref):\n           plgpu.wgmma(acc_ref, p16, v_smem.at[slot])\n \n           wait_step = kv_step + 1\n-          wait_slot = lax.rem(wait_step, max_concurrent_steps)\n-          @pl.when(wait_step < kv_seq_len // block_kv)\n+          wait_slot = lax.rem(wait_step, jnp.array(max_concurrent_steps, kv_step.dtype))\n+          @pl.when(wait_step < kv_steps)\n           def _wait():\n             plgpu.barrier_wait(k_barriers.at[wait_slot])\n         acc = pl.run_state(compute_pv)(plgpu.ACC.init(acc))\n         plgpu.barrier_arrive(v_consumed_barriers.at[slot])\n         return acc, m_i, l_i\n-      if kv_seq_len % block_kv:\n-        raise ValueError(f\"{kv_seq_len=} must be a multiple of {block_kv=}\")\n-      acc, m_i, l_i = lax.fori_loop(\n-          0, kv_seq_len // block_kv, kv_loop, (acc, m_i, l_i)\n-      )\n+\n+      if not config.causal:\n+        acc, m_i, l_i = lax.fori_loop(0, block_max_kv_steps, kv_loop, (acc, m_i, l_i))\n+      else:\n+        def epilogue_kv_loop(kv_step, _):\n+          # This loop makes sure that all the pipelined KV data is processed, even\n+          # if one compute wg finishes early like with causal masking.\n+          slot = lax.rem(kv_step, jnp.array(max_concurrent_steps, kv_step.dtype))\n+          plgpu.barrier_arrive(k_consumed_barriers.at[slot])\n+          plgpu.barrier_arrive(v_consumed_barriers.at[slot])\n+          perform_schedule_barrier()\n+          perform_schedule_barrier()\n+\n+        causal_kv_loop = functools.partial(kv_loop, causal=True)\n+        full_kv_steps = lax.div(q_seq_base, jnp.array(block_kv, jnp.int32))\n+        # With causal masking, the KV loop unrolling is split in 3 sections:\n+        # 1. A fast path where no causal mask is needed.\n+        acc, m_i, l_i = lax.fori_loop(0, full_kv_steps, kv_loop, (acc, m_i, l_i))\n+        # 2. Causal masking.\n+        acc, m_i, l_i = lax.fori_loop(full_kv_steps, kv_steps, causal_kv_loop, (acc, m_i, l_i))\n+        # 3. Epilogue to flush the data pipeline.\n+        lax.fori_loop(kv_steps, block_max_kv_steps, epilogue_kv_loop, None)\n       pl.when(wg_idx == 0)(perform_schedule_barrier)\n \n       # TODO(apaszke): Invert and multiply to avoid expensive divisions.\n@@ -208,13 +247,13 @@ def _memory_wg():\n \n       def kv_loop(kv_step, _):\n         tma_step = kv_step + max_concurrent_steps\n-        tma_slot = lax.rem(kv_step, max_concurrent_steps)\n+        tma_slot = lax.rem(kv_step, jnp.array(max_concurrent_steps, kv_step.dtype))\n         s = (batch, pl.ds(tma_step * block_kv, block_kv), kv_head)\n         plgpu.barrier_wait(k_consumed_barriers.at[tma_slot])\n         plgpu.copy_gmem_to_smem(k_ref.at[s], k_smem.at[tma_slot], k_barriers.at[tma_slot])\n         plgpu.barrier_wait(v_consumed_barriers.at[tma_slot])\n         plgpu.copy_gmem_to_smem(v_ref.at[s], v_smem.at[tma_slot], v_barriers.at[tma_slot])\n-      lax.fori_loop(0, kv_seq_len // block_kv - max_concurrent_steps, kv_loop, None)\n+      lax.fori_loop(0, block_max_kv_steps - max_concurrent_steps, kv_loop, None)\n \n   def entry(q_ref, k_ref, v_ref, out_ref, lse_ref):\n     compute_wgs = 2\n@@ -291,6 +330,9 @@ def _attention_bwd(config: TuningConfig, save_residuals: bool, res, do):\n   del save_residuals\n   q, k, v, out, lse = res\n \n+  if config.causal:\n+    raise NotImplementedError(\"Causal attention not supported in the backwards pass yet.\")\n+\n   if not config.has_backward_blocks:\n     raise ValueError(\"Need to specify backward blocks.\")\n \n@@ -586,6 +628,8 @@ def compute_dk(acc_ref):\n \n @functools.partial(jax.jit, static_argnames=[\"config\", \"save_residuals\"])\n def attention_with_pipeline_emitter(q, k, v, config: TuningConfig, save_residuals=False):\n+  if config.causal:\n+    raise NotImplementedError(\"Causal attention is not supported with the pipeline emitter yet.\")\n   if q.ndim != 4 or k.ndim != 4 or v.ndim != 4:\n     raise ValueError(f\"q, k, and v should all be 4D, got: {q.ndim=}, {k.ndim=}, {v.ndim=}\")\n   batch_size, q_seq_len, num_q_heads, head_dim = q.shape\n@@ -762,15 +806,21 @@ def run_function(q, k, v, o, lse):\n   return out\n \n \n-@functools.partial(jax.jit, static_argnames=[\"save_residuals\"])\n-def attention_reference(q, k, v, save_residuals=False):\n+@functools.partial(jax.jit, static_argnames=[\"causal\", \"save_residuals\"])\n+def attention_reference(q, k, v, causal=False, save_residuals=False):\n   batch_size, q_seq_len, num_q_heads, head_dim = q.shape\n-  num_kv_heads = k.shape[2]\n+  kv_seq_len, num_kv_heads = k.shape[1], k.shape[2]\n   q, k, v = map(lambda x: x.astype(jnp.float32), (q, k, v))\n   q_reshaped = q.reshape(\n       batch_size, q_seq_len, num_kv_heads, num_q_heads // num_kv_heads, head_dim\n   )\n   logits = jnp.einsum(\"bqHhc,bkHc->bqHhk\", q_reshaped, k)\n+\n+  if causal:\n+    mask = jnp.arange(q_seq_len)[:, None] >= jnp.arange(kv_seq_len)[None, :]\n+    mask = jnp.broadcast_to(mask[:, None, None, :], logits.shape)\n+    logits = jnp.where(mask, logits, -jnp.inf)\n+\n   m = logits.max(axis=-1, keepdims=True)\n   unnormalized = jnp.exp(logits - m)\n   l = unnormalized.sum(axis=-1, keepdims=True)\n@@ -798,11 +848,13 @@ def main(unused_argv):\n     schedule_barrier_opts = (True,)\n \n   problem_it = itertools.product(\n-      (1,), (4096, 32768,), (64, 128, 256,), schedule_barrier_opts)\n-  for batch_size, seq_len, head_dim, use_schedule_barrier in problem_it:\n+      (1,), (4096, 32768,), (64, 128, 256,), schedule_barrier_opts, (False, True))\n+  for batch_size, seq_len, head_dim, use_schedule_barrier, causal in problem_it:\n+    if causal and use_pipeline_emitter:\n+      continue\n     q_seq_len = kv_seq_len = seq_len\n     print(f\"==== {batch_size=:<6} {kv_seq_len=:<6} {q_seq_len=:<6}\"\n-          f\"{num_q_heads=:<4} {head_dim=:<6} {use_schedule_barrier=:} ====\")\n+          f\"{num_q_heads=:<4} {head_dim=:<6} {use_schedule_barrier=:} {causal=:} ====\")\n     k1, k2, k3 = jax.random.split(jax.random.key(42), 3)\n     q = jax.random.normal(k1, (batch_size, q_seq_len, num_q_heads, head_dim), jnp.float16)\n     k = jax.random.normal(k2, (batch_size, kv_seq_len, num_kv_heads, head_dim), jnp.float16)\n@@ -810,11 +862,11 @@ def main(unused_argv):\n     block_q = 64\n     best = None\n     for block_kv in (256, 128, 64):\n-      config = TuningConfig(block_q=block_q, block_kv=block_kv, max_concurrent_steps=2, use_schedule_barrier=use_schedule_barrier)\n+      config = TuningConfig(block_q=block_q, block_kv=block_kv, max_concurrent_steps=2, use_schedule_barrier=use_schedule_barrier, causal=causal)\n       try:\n         out, runtime_ms = profiler.measure(functools.partial(attention_impl, config=config))(q, k, v)\n         if seq_len < 32768:\n-          out_ref = attention_reference(q, k, v)\n+          out_ref = attention_reference(q, k, v, causal=causal)\n           np.testing.assert_allclose(out, out_ref, atol=2e-3, rtol=1e-3)\n       except ValueError as e:\n         if \"exceeds available shared memory\" in e.args[0]:\n@@ -824,6 +876,8 @@ def main(unused_argv):\n       matmul_flops = (\n           4 * q_seq_len * kv_seq_len * head_dim * num_q_heads * batch_size\n       )\n+      if causal:\n+        matmul_flops //= 2\n       peak_flops = 1e15  # f16 TensorCore peak = 1000TFLOPS\n       optimal_time = matmul_flops / peak_flops * 1e6  # us\n       achieved_tc_util = optimal_time / runtime_us * 100\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 86c3fe187b79..6690fb2dac62 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -776,7 +776,7 @@ jax_multiplatform_test(\n         \"gpu_h100\",\n     ],\n     env = {\"XLA_FLAGS\": \"--xla_gpu_autotune_level=0\"},\n-    shard_count = 4,\n+    shard_count = 8,\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_experimental_gpu_ops\",\ndiff --git a/tests/pallas/mgpu_attention_test.py b/tests/pallas/mgpu_attention_test.py\nindex 50f9a455c9a2..f86793174c16 100644\n--- a/tests/pallas/mgpu_attention_test.py\n+++ b/tests/pallas/mgpu_attention_test.py\n@@ -21,6 +21,7 @@\n from absl.testing import absltest, parameterized\n from jax._src import config\n from jax._src import test_util as jtu\n+from jax._src.lib import cuda_versions\n from jax._src.pallas import pallas_call\n import jax.numpy as jnp\n \n@@ -63,11 +64,13 @@ def setUp(self):\n           (4, 4),\n       ),  # MHA\n       head_dim=(64, 128, 256),\n+      blocks=((64, 64), (64, 128), (128, 64)),\n       attention_impl=(\n           attention_mgpu.attention,\n           attention_mgpu.attention_with_pipeline_emitter,\n       ),\n       save_residuals=(True,),\n+      causal=(True, False,),\n   )\n   def test_flash_attention(\n       self,\n@@ -76,10 +79,24 @@ def test_flash_attention(\n       kv_seq_len,\n       num_q_and_kv_heads,\n       head_dim,\n+      blocks,\n       attention_impl,\n       save_residuals,\n+      causal,\n   ):\n+    cuda_runtime_version = cuda_versions.cuda_runtime_get_version()\n+    # TODO(pobudzey): Undo when we upgrade to cuda 12.9.1.\n+    if causal and (cuda_runtime_version >= 12080 and cuda_runtime_version < 12091):\n+      self.skipTest(\"Skipping because of ptxas miscompilation.\")\n+\n+    if causal and attention_impl == attention_mgpu.attention_with_pipeline_emitter:\n+      self.skipTest(\"Pipeline emitter does not support causal attention.\")\n+\n+    if head_dim >= 256 and max(blocks) >= 128:\n+      self.skipTest(\"Head dim too large for block sizes.\")\n+\n     num_q_heads, num_kv_heads = num_q_and_kv_heads\n+    block_q, block_kv = blocks\n     k1, k2, k3 = jax.random.split(jax.random.key(42), 3)\n     q = jax.random.normal(k1, (batch_size, q_seq_len, num_q_heads, head_dim), jnp.float16)\n     k = jax.random.normal(k2, (batch_size, kv_seq_len, num_kv_heads, head_dim), jnp.float16)\n@@ -89,11 +106,12 @@ def test_flash_attention(\n         k,\n         v,\n         attention_mgpu.TuningConfig(\n-            block_q=64, block_kv=64, max_concurrent_steps=2\n+            block_q=block_q, block_kv=block_kv, max_concurrent_steps=2, causal=causal\n         ),\n         save_residuals=save_residuals,\n     )\n-    out_ref, *res_ref = attention_mgpu.attention_reference(q, k, v, save_residuals=save_residuals)\n+    out_ref, *res_ref = attention_mgpu.attention_reference(\n+        q, k, v, causal=causal, save_residuals=save_residuals)\n     np.testing.assert_allclose(out, out_ref, atol=2e-3, rtol=1e-3)\n     if save_residuals:\n       (lse,) = res[0]\n\n```",
        "from_id": [
            "Rifur13",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Bump NumPy version to 2.2.6 for 3.13+ Python versions.\n\nRemove workaround for NumPy 2.2 bug on aarch64.\n\nAlso add a linux system constraint to GPU wheels.",
        "output": "```diff\nCommit: 18ff6caa4f767701dd7cca3a1333d9b99465e045\nDate: 2025-05-19T14:08:25Z\nURL: https://github.com/jax-ml/jax/commit/18ff6caa4f767701dd7cca3a1333d9b99465e045\nFiles changed: 10\nAdditions: +72, Deletions: -80\ndiff --git a/.bazelrc b/.bazelrc\nindex 79df03863b02..53676637c839 100644\n--- a/.bazelrc\n+++ b/.bazelrc\n@@ -244,10 +244,6 @@ build:ci_linux_aarch64_base --config=clang --verbose_failures=true\n build:ci_linux_aarch64_base --action_env=TF_SYSROOT=\"/dt10\"\n build:ci_linux_aarch64_base --color=yes\n \n-# Workaround for https://github.com/numpy/numpy/issues/28843\n-# TODO(phawkins): remove this after upgrading to NumPy 2.2.6.\n-build:ci_linux_aarch64_base --test_env=OMP_NUM_THREADS=8\n-\n build:ci_linux_aarch64 --config=ci_linux_aarch64_base\n build:ci_linux_aarch64 --host_crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n build:ci_linux_aarch64 --crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n@@ -383,10 +379,6 @@ build:rbe_cross_compile_base --remote_instance_name=projects/tensorflow-testing/\n build:rbe_cross_compile_linux_aarch64 --config=cross_compile_linux_aarch64\n build:rbe_cross_compile_linux_aarch64 --config=rbe_cross_compile_base\n \n-# Workaround for https://github.com/numpy/numpy/issues/28843\n-# TODO(phawkins): remove this after upgrading to NumPy 2.2.6.\n-build:rbe_cross_compile_linux_aarch64 --test_env=OMP_NUM_THREADS=8\n-\n # Mac x86\n build:cross_compile_darwin_x86_64 --config=cross_compile_base\n build:cross_compile_darwin_x86_64 --config=nonccl\ndiff --git a/build/freethreading-requirements.txt b/build/freethreading-requirements.txt\nindex cc302cffdd0c..467578870ee9 100644\n--- a/build/freethreading-requirements.txt\n+++ b/build/freethreading-requirements.txt\n@@ -1,3 +1,3 @@\n # Under free-threading, we need an up-to-date numpy at least for the moment.\n-numpy~=2.2.5; python_version==\"3.13\"\n-numpy>=2.2.5; python_version>=\"3.14\"\n+numpy~=2.2.6; python_version==\"3.13\"\n+numpy>=2.2.6; python_version>=\"3.14\"\ndiff --git a/build/nonfreethreading-requirements.txt b/build/nonfreethreading-requirements.txt\nindex f8171559a142..8bd139bf99ac 100644\n--- a/build/nonfreethreading-requirements.txt\n+++ b/build/nonfreethreading-requirements.txt\n@@ -1,6 +1,6 @@\n numpy~=2.0.0; python_version<=\"3.12\"\n numpy~=2.1.0; python_version==\"3.13\"\n-numpy>=2.2.5; python_version>=\"3.14\"\n+numpy>=2.2.6; python_version>=\"3.14\"\n \n # These packages have not released free-threaded wheels.\n zstandard\ndiff --git a/build/requirements.in b/build/requirements.in\nindex 8b8af9d6b591..c5ce2ea279bd 100644\n--- a/build/requirements.in\n+++ b/build/requirements.in\n@@ -19,8 +19,8 @@ wheel\n jaxlib\n \n # The with-cuda extra also includes NVIDIA's pip packages.\n-jax-cuda12-plugin[with-cuda]\n-jax-cuda12-pjrt\n+jax-cuda12-plugin[with-cuda] ; sys_platform == \"linux\"\n+jax-cuda12-pjrt ; sys_platform == \"linux\"\n \n # TPU dependencies\n libtpu ; sys_platform == \"linux\" and platform_machine == \"x86_64\"\ndiff --git a/build/requirements_lock_3_10.txt b/build/requirements_lock_3_10.txt\nindex c4ca6088e4bf..a4c6b1bf2b77 100644\n--- a/build/requirements_lock_3_10.txt\n+++ b/build/requirements_lock_3_10.txt\n@@ -160,13 +160,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_11.txt b/build/requirements_lock_3_11.txt\nindex 1f667115af04..0633e733414b 100644\n--- a/build/requirements_lock_3_11.txt\n+++ b/build/requirements_lock_3_11.txt\n@@ -154,13 +154,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_12.txt b/build/requirements_lock_3_12.txt\nindex 20ca67a3e921..1ab77a6ec36e 100644\n--- a/build/requirements_lock_3_12.txt\n+++ b/build/requirements_lock_3_12.txt\n@@ -154,13 +154,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_13.txt b/build/requirements_lock_3_13.txt\nindex 804373b03899..c20068b732e6 100644\n--- a/build/requirements_lock_3_13.txt\n+++ b/build/requirements_lock_3_13.txt\n@@ -181,13 +181,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_13_ft.txt b/build/requirements_lock_3_13_ft.txt\nindex c7a1c882fc73..3795343df0cb 100644\n--- a/build/requirements_lock_3_13_ft.txt\n+++ b/build/requirements_lock_3_13_ft.txt\n@@ -172,13 +172,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n@@ -371,62 +371,62 @@ mpmath==1.3.0 \\\n     --hash=sha256:7a28eb2a9774d00c7bc92411c19a89209d5da7c4c9a9e227be8330a23a25b91f \\\n     --hash=sha256:a0b2b9fe80bbcd81a6647ff13108738cfb482d481d826cc0e02f5b35e5c88d2c\n     # via -r build/test-requirements.txt\n-numpy==2.2.5 ; python_version == \"3.13\" \\\n-    --hash=sha256:0255732338c4fdd00996c0421884ea8a3651eea555c3a56b84892b66f696eb70 \\\n-    --hash=sha256:02f226baeefa68f7d579e213d0f3493496397d8f1cff5e2b222af274c86a552a \\\n-    --hash=sha256:059b51b658f4414fff78c6d7b1b4e18283ab5fa56d270ff212d5ba0c561846f4 \\\n-    --hash=sha256:0bcb1d057b7571334139129b7f941588f69ce7c4ed15a9d6162b2ea54ded700c \\\n-    --hash=sha256:0cd48122a6b7eab8f06404805b1bd5856200e3ed6f8a1b9a194f9d9054631beb \\\n-    --hash=sha256:19f4718c9012e3baea91a7dba661dcab2451cda2550678dc30d53acb91a7290f \\\n-    --hash=sha256:1a161c2c79ab30fe4501d5a2bbfe8b162490757cf90b7f05be8b80bc02f7bb8e \\\n-    --hash=sha256:1f4a922da1729f4c40932b2af4fe84909c7a6e167e6e99f71838ce3a29f3fe26 \\\n-    --hash=sha256:261a1ef047751bb02f29dfe337230b5882b54521ca121fc7f62668133cb119c9 \\\n-    --hash=sha256:262d23f383170f99cd9191a7c85b9a50970fe9069b2f8ab5d786eca8a675d60b \\\n-    --hash=sha256:2ba321813a00e508d5421104464510cc962a6f791aa2fca1c97b1e65027da80d \\\n-    --hash=sha256:2c1a1c6ccce4022383583a6ded7bbcda22fc635eb4eb1e0a053336425ed36dfa \\\n-    --hash=sha256:352d330048c055ea6db701130abc48a21bec690a8d38f8284e00fab256dc1376 \\\n-    --hash=sha256:369e0d4647c17c9363244f3468f2227d557a74b6781cb62ce57cf3ef5cc7c610 \\\n-    --hash=sha256:36ab5b23915887543441efd0417e6a3baa08634308894316f446027611b53bf1 \\\n-    --hash=sha256:37e32e985f03c06206582a7323ef926b4e78bdaa6915095ef08070471865b906 \\\n-    --hash=sha256:3a801fef99668f309b88640e28d261991bfad9617c27beda4a3aec4f217ea073 \\\n-    --hash=sha256:3d14b17b9be5f9c9301f43d2e2a4886a33b53f4e6fdf9ca2f4cc60aeeee76372 \\\n-    --hash=sha256:422cc684f17bc963da5f59a31530b3936f57c95a29743056ef7a7903a5dbdf88 \\\n-    --hash=sha256:4520caa3807c1ceb005d125a75e715567806fed67e315cea619d5ec6e75a4191 \\\n-    --hash=sha256:47834cde750d3c9f4e52c6ca28a7361859fcaf52695c7dc3cc1a720b8922683e \\\n-    --hash=sha256:47f9ed103af0bc63182609044b0490747e03bd20a67e391192dde119bf43d52f \\\n-    --hash=sha256:498815b96f67dc347e03b719ef49c772589fb74b8ee9ea2c37feae915ad6ebda \\\n-    --hash=sha256:54088a5a147ab71a8e7fdfd8c3601972751ded0739c6b696ad9cb0343e21ab73 \\\n-    --hash=sha256:55f09e00d4dccd76b179c0f18a44f041e5332fd0e022886ba1c0bbf3ea4a18d0 \\\n-    --hash=sha256:5a0ac90e46fdb5649ab6369d1ab6104bfe5854ab19b645bf5cda0127a13034ae \\\n-    --hash=sha256:6411f744f7f20081b1b4e7112e0f4c9c5b08f94b9f086e6f0adf3645f85d3a4d \\\n-    --hash=sha256:6413d48a9be53e183eb06495d8e3b006ef8f87c324af68241bbe7a39e8ff54c3 \\\n-    --hash=sha256:7451f92eddf8503c9b8aa4fe6aa7e87fd51a29c2cfc5f7dbd72efde6c65acf57 \\\n-    --hash=sha256:8b4c0773b6ada798f51f0f8e30c054d32304ccc6e9c5d93d46cb26f3d385ab19 \\\n-    --hash=sha256:8dfa94b6a4374e7851bbb6f35e6ded2120b752b063e6acdd3157e4d2bb922eba \\\n-    --hash=sha256:97c8425d4e26437e65e1d189d22dff4a079b747ff9c2788057bfb8114ce1e133 \\\n-    --hash=sha256:9d75f338f5f79ee23548b03d801d28a505198297534f62416391857ea0479571 \\\n-    --hash=sha256:9de6832228f617c9ef45d948ec1cd8949c482238d68b2477e6f642c33a7b0a54 \\\n-    --hash=sha256:a4cbdef3ddf777423060c6f81b5694bad2dc9675f110c4b2a60dc0181543fac7 \\\n-    --hash=sha256:a9c0d994680cd991b1cb772e8b297340085466a6fe964bc9d4e80f5e2f43c291 \\\n-    --hash=sha256:aa70fdbdc3b169d69e8c59e65c07a1c9351ceb438e627f0fdcd471015cd956be \\\n-    --hash=sha256:abe38cd8381245a7f49967a6010e77dbf3680bd3627c0fe4362dd693b404c7f8 \\\n-    --hash=sha256:b13f04968b46ad705f7c8a80122a42ae8f620536ea38cf4bdd374302926424dd \\\n-    --hash=sha256:b4ea7e1cff6784e58fe281ce7e7f05036b3e1c89c6f922a6bfbc0a7e8768adbe \\\n-    --hash=sha256:b6f91524d31b34f4a5fee24f5bc16dcd1491b668798b6d85585d836c1e633a6a \\\n-    --hash=sha256:c26843fd58f65da9491165072da2cccc372530681de481ef670dcc8e27cfb066 \\\n-    --hash=sha256:c42365005c7a6c42436a54d28c43fe0e01ca11eb2ac3cefe796c25a5f98e5e9b \\\n-    --hash=sha256:c8b82a55ef86a2d8e81b63da85e55f5537d2157165be1cb2ce7cfa57b6aef38b \\\n-    --hash=sha256:ced69262a8278547e63409b2653b372bf4baff0870c57efa76c5703fd6543282 \\\n-    --hash=sha256:d2e3bdadaba0e040d1e7ab39db73e0afe2c74ae277f5614dad53eadbecbbb169 \\\n-    --hash=sha256:d403c84991b5ad291d3809bace5e85f4bbf44a04bdc9a88ed2bb1807b3360bb8 \\\n-    --hash=sha256:d7543263084a85fbc09c704b515395398d31d6395518446237eac219eab9e55e \\\n-    --hash=sha256:d8882a829fd779f0f43998e931c466802a77ca1ee0fe25a3abe50278616b1471 \\\n-    --hash=sha256:e4f0b035d9d0ed519c813ee23e0a733db81ec37d2e9503afbb6e54ccfdee0fa7 \\\n-    --hash=sha256:e8b025c351b9f0e8b5436cf28a07fa4ac0204d67b38f01433ac7f9b870fa38c6 \\\n-    --hash=sha256:eb7fd5b184e5d277afa9ec0ad5e4eb562ecff541e7f60e69ee69c8d59e9aeaba \\\n-    --hash=sha256:ec31367fd6a255dc8de4772bd1658c3e926d8e860a0b6e922b615e532d320ddc \\\n-    --hash=sha256:ee461a4eaab4f165b68780a6a1af95fb23a29932be7569b9fab666c407969051 \\\n-    --hash=sha256:f5045039100ed58fa817a6227a356240ea1b9a1bc141018864c306c1a16d4175\n+numpy==2.2.6 ; python_version == \"3.13\" \\\n+    --hash=sha256:038613e9fb8c72b0a41f025a7e4c3f0b7a1b5d768ece4796b674c8f3fe13efff \\\n+    --hash=sha256:0678000bb9ac1475cd454c6b8c799206af8107e310843532b04d49649c717a47 \\\n+    --hash=sha256:0811bb762109d9708cca4d0b13c4f67146e3c3b7cf8d34018c722adb2d957c84 \\\n+    --hash=sha256:0b605b275d7bd0c640cad4e5d30fa701a8d59302e127e5f79138ad62762c3e3d \\\n+    --hash=sha256:0bca768cd85ae743b2affdc762d617eddf3bcf8724435498a1e80132d04879e6 \\\n+    --hash=sha256:1bc23a79bfabc5d056d106f9befb8d50c31ced2fbc70eedb8155aec74a45798f \\\n+    --hash=sha256:287cc3162b6f01463ccd86be154f284d0893d2b3ed7292439ea97eafa8170e0b \\\n+    --hash=sha256:37c0ca431f82cd5fa716eca9506aefcabc247fb27ba69c5062a6d3ade8cf8f49 \\\n+    --hash=sha256:37e990a01ae6ec7fe7fa1c26c55ecb672dd98b19c3d0e1d1f326fa13cb38d163 \\\n+    --hash=sha256:389d771b1623ec92636b0786bc4ae56abafad4a4c513d36a55dce14bd9ce8571 \\\n+    --hash=sha256:3d70692235e759f260c3d837193090014aebdf026dfd167834bcba43e30c2a42 \\\n+    --hash=sha256:41c5a21f4a04fa86436124d388f6ed60a9343a6f767fced1a8a71c3fbca038ff \\\n+    --hash=sha256:481b49095335f8eed42e39e8041327c05b0f6f4780488f61286ed3c01368d491 \\\n+    --hash=sha256:4eeaae00d789f66c7a25ac5f34b71a7035bb474e679f410e5e1a94deb24cf2d4 \\\n+    --hash=sha256:55a4d33fa519660d69614a9fad433be87e5252f4b03850642f88993f7b2ca566 \\\n+    --hash=sha256:5a6429d4be8ca66d889b7cf70f536a397dc45ba6faeb5f8c5427935d9592e9cf \\\n+    --hash=sha256:5bd4fc3ac8926b3819797a7c0e2631eb889b4118a9898c84f585a54d475b7e40 \\\n+    --hash=sha256:5beb72339d9d4fa36522fc63802f469b13cdbe4fdab4a288f0c441b74272ebfd \\\n+    --hash=sha256:6031dd6dfecc0cf9f668681a37648373bddd6421fff6c66ec1624eed0180ee06 \\\n+    --hash=sha256:71594f7c51a18e728451bb50cc60a3ce4e6538822731b2933209a1f3614e9282 \\\n+    --hash=sha256:74d4531beb257d2c3f4b261bfb0fc09e0f9ebb8842d82a7b4209415896adc680 \\\n+    --hash=sha256:7befc596a7dc9da8a337f79802ee8adb30a552a94f792b9c9d18c840055907db \\\n+    --hash=sha256:894b3a42502226a1cac872f840030665f33326fc3dac8e57c607905773cdcde3 \\\n+    --hash=sha256:8e41fd67c52b86603a91c1a505ebaef50b3314de0213461c7a6e99c9a3beff90 \\\n+    --hash=sha256:8e9ace4a37db23421249ed236fdcdd457d671e25146786dfc96835cd951aa7c1 \\\n+    --hash=sha256:8fc377d995680230e83241d8a96def29f204b5782f371c532579b4f20607a289 \\\n+    --hash=sha256:9551a499bf125c1d4f9e250377c1ee2eddd02e01eac6644c080162c0c51778ab \\\n+    --hash=sha256:b0544343a702fa80c95ad5d3d608ea3599dd54d4632df855e4c8d24eb6ecfa1c \\\n+    --hash=sha256:b093dd74e50a8cba3e873868d9e93a85b78e0daf2e98c6797566ad8044e8363d \\\n+    --hash=sha256:b412caa66f72040e6d268491a59f2c43bf03eb6c96dd8f0307829feb7fa2b6fb \\\n+    --hash=sha256:b4f13750ce79751586ae2eb824ba7e1e8dba64784086c98cdbbcc6a42112ce0d \\\n+    --hash=sha256:b64d8d4d17135e00c8e346e0a738deb17e754230d7e0810ac5012750bbd85a5a \\\n+    --hash=sha256:ba10f8411898fc418a521833e014a77d3ca01c15b0c6cdcce6a0d2897e6dbbdf \\\n+    --hash=sha256:bd48227a919f1bafbdda0583705e547892342c26fb127219d60a5c36882609d1 \\\n+    --hash=sha256:c1f9540be57940698ed329904db803cf7a402f3fc200bfe599334c9bd84a40b2 \\\n+    --hash=sha256:c820a93b0255bc360f53eca31a0e676fd1101f673dda8da93454a12e23fc5f7a \\\n+    --hash=sha256:ce47521a4754c8f4593837384bd3424880629f718d87c5d44f8ed763edd63543 \\\n+    --hash=sha256:d042d24c90c41b54fd506da306759e06e568864df8ec17ccc17e9e884634fd00 \\\n+    --hash=sha256:de749064336d37e340f640b05f24e9e3dd678c57318c7289d222a8a2f543e90c \\\n+    --hash=sha256:e1dda9c7e08dc141e0247a5b8f49cf05984955246a327d4c48bda16821947b2f \\\n+    --hash=sha256:e29554e2bef54a90aa5cc07da6ce955accb83f21ab5de01a62c8478897b264fd \\\n+    --hash=sha256:e3143e4451880bed956e706a3220b4e5cf6172ef05fcc397f6f36a550b1dd868 \\\n+    --hash=sha256:e8213002e427c69c45a52bbd94163084025f533a55a59d6f9c5b820774ef3303 \\\n+    --hash=sha256:efd28d4e9cd7d7a8d39074a4d44c63eda73401580c5c76acda2ce969e0a38e83 \\\n+    --hash=sha256:f0fd6321b839904e15c46e0d257fdd101dd7f530fe03fd6359c1ea63738703f3 \\\n+    --hash=sha256:f1372f041402e37e5e633e586f62aa53de2eac8d98cbfb822806ce4bbefcb74d \\\n+    --hash=sha256:f2618db89be1b4e05f7a1a847a9c1c0abd63e63a1607d892dd54668dd92faf87 \\\n+    --hash=sha256:f447e6acb680fd307f40d3da4852208af94afdfab89cf850986c3ca00562f4fa \\\n+    --hash=sha256:f92729c95468a2f4f15e9bb94c432a9229d0d50de67304399627a943201baa2f \\\n+    --hash=sha256:f9f1adb22318e121c5c69a09142811a201ef17ab257a1e66ca3025065b7f53ae \\\n+    --hash=sha256:fc0c5673685c508a142ca65209b4e79ed6740a4ed6b2267dbba90f34b0b3cfda \\\n+    --hash=sha256:fc7b73d02efb0e18c000e9ad8b83480dfcd5dfd11065997ed4c6747470ae8915 \\\n+    --hash=sha256:fd83c01228a688733f1ded5201c678f0c53ecc1006ffbc404db9f7a899ac6249 \\\n+    --hash=sha256:fe27749d33bb772c80dcd84ae7e8df2adc920ae8297400dabec45f0dedb3f6de \\\n+    --hash=sha256:fee4236c876c4e8369388054d02d0e9bb84821feb1a64dd59e137e6511a551f8\n     # via\n     #   -r build/freethreading-requirements.txt\n     #   contourpy\ndiff --git a/build/requirements_lock_3_14.txt b/build/requirements_lock_3_14.txt\nindex 6edcd30ebe16..157dca5adbab 100644\n--- a/build/requirements_lock_3_14.txt\n+++ b/build/requirements_lock_3_14.txt\n@@ -48,7 +48,7 @@ ml-dtypes==0.5.1\n     #   tensorstore\n mpmath==1.4.0a4\n     # via -r build/test-requirements.txt\n-numpy==2.2.5\n+numpy==2.2.6\n     # via\n     #   -r build/nonfreethreading-requirements.txt\n     #   contourpy\n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Disable cusolver version check.\n\nThis appears to be failing for some reason, and it's safest just to disable it for now.",
        "output": "```diff\nCommit: d62c10f1b5075b3aa5b3327c71ed43d133fc975c\nDate: 2025-05-19T13:44:11Z\nURL: https://github.com/jax-ml/jax/commit/d62c10f1b5075b3aa5b3327c71ed43d133fc975c\nFiles changed: 1\nAdditions: +8, Deletions: -5\ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 9df7fc69ff1a..02bcbcf16dbc 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -180,11 +180,14 @@ def _version_check(name: str,\n                  cuda_versions.cufft_build_version,\n                  # Ignore patch versions.\n                  scale_for_comparison=100)\n-  _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n-                 cuda_versions.cusolver_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=11400)\n+  # TODO(phawkins): for some reason this check fails with a cusolver internal\n+  # error when fetching the version. This may be a path error from our stubs.\n+  # Figure out what's happening here and reenable.\n+  # _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n+  #                cuda_versions.cusolver_build_version,\n+  #                # Ignore patch versions.\n+  #                scale_for_comparison=100,\n+  #                min_supported_version=11400)\n   _version_check(\"cuPTI\", cuda_versions.cupti_get_version,\n                  cuda_versions.cupti_build_version,\n                  min_supported_version=18)\n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Add version guards on unreduced lowering to shardy\n\nPiperOrigin-RevId: 760603883",
        "output": "```diff\nCommit: 169ad4ae521872a85d5c5a93102f3c31663dd497\nDate: 2025-05-19T13:41:14Z\nURL: https://github.com/jax-ml/jax/commit/169ad4ae521872a85d5c5a93102f3c31663dd497\nFiles changed: 2\nAdditions: +13, Deletions: -6\ndiff --git a/jax/_src/named_sharding.py b/jax/_src/named_sharding.py\nindex faf0b2a9f2b2..ae99236a6cdc 100644\n--- a/jax/_src/named_sharding.py\n+++ b/jax/_src/named_sharding.py\n@@ -23,6 +23,7 @@\n from jax._src import config\n from jax._src.util import use_cpp_class, cache, use_cpp_method\n from jax._src.lib import xla_client as xc\n+from jax._src.lib import jaxlib_extension_version\n from jax._src.lib.mlir.dialects import sdy\n from jax._src import mesh as mesh_lib\n from jax._src.mesh import AxisType\n@@ -316,11 +317,17 @@ def build(self) -> sdy.TensorShardingAttr:\n \n     replicated_axes = _get_axes(self.replicated_axes, self.mesh_shape)\n     unreduced_axes = _get_axes(self.unreduced_axes, self.mesh_shape)\n-    return sdy.TensorShardingAttr.get(\n-        mesh_attr,\n-        [dim_sharding.build() for dim_sharding in self.dim_shardings],\n-        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes],\n-        unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in unreduced_axes])\n+    if jaxlib_extension_version >= 342:\n+      return sdy.TensorShardingAttr.get(\n+          mesh_attr,\n+          [dim_sharding.build() for dim_sharding in self.dim_shardings],\n+          replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes],\n+          unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in unreduced_axes])\n+    else:\n+      return sdy.TensorShardingAttr.get(\n+          mesh_attr,\n+          [dim_sharding.build() for dim_sharding in self.dim_shardings],\n+          replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes])\n \n   def __repr__(self):\n     dim_sharding_repr = ', '.join(\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 69e168de9c2d..8f8c829ee6c7 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 341\n+_version = 342\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] A handful of minor bug fixes\n\nPiperOrigin-RevId: 760601235",
        "output": "```diff\nCommit: b2418681d7b55dc62bbc7a1646ea3d7b1d1da2ee\nDate: 2025-05-19T13:32:26Z\nURL: https://github.com/jax-ml/jax/commit/b2418681d7b55dc62bbc7a1646ea3d7b1d1da2ee\nFiles changed: 3\nAdditions: +14, Deletions: -4\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 3d43e2c0ab61..d13977ac4fbf 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -627,7 +627,7 @@ def remote_ref(\n ) -> pallas_core.TransformedRef:\n   \"\"\"Translate memref to a symmetric memref on a peer device.\"\"\"\n   if not isinstance(ref, pallas_core.TransformedRef):\n-    if not isinstance(jax_core.get_aval(ref), pallas_core.AbstractMemoryRef):\n+    if not isinstance(jax_core.get_aval(ref), state_types.AbstractRef):\n       raise TypeError(\"ref must be a reference\")\n     ref = pallas_core.TransformedRef(ref, transforms=())\n   return pallas_core.TransformedRef(\n@@ -640,7 +640,7 @@ def transform_ref(\n     transform: state_types.Transform\n ) -> pallas_core.TransformedRef:\n   if not isinstance(ref, pallas_core.TransformedRef):\n-    if not isinstance(jax_core.get_aval(ref), pallas_core.AbstractMemoryRef):\n+    if not isinstance(jax_core.get_aval(ref), state_types.AbstractRef):\n       raise TypeError(\"ref must be a reference\")\n     ref = pallas_core.TransformedRef(ref, transforms=())\n   return pallas_core.TransformedRef(\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex acbcf7c2cda1..62b43b4de737 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -1993,7 +1993,7 @@ def _store_untiled_wg_strided(self, ref: ir.Value):\n       idxs = ([i] for i in self.layout.linear_thread_idxs())\n     except NotImplementedError:\n       ref_ = ref\n-      idxs = self.layout.thread_idxs()\n+      idxs = self.layout.thread_idxs(self.shape)\n     ref_shape = tuple(ref_ty.shape)\n     if ref_shape != self.shape:\n       raise ValueError((ref_shape, self.shape))\ndiff --git a/jax/experimental/mosaic/gpu/launch_context.py b/jax/experimental/mosaic/gpu/launch_context.py\nindex eccb363f7537..175dc8b0ac74 100644\n--- a/jax/experimental/mosaic/gpu/launch_context.py\n+++ b/jax/experimental/mosaic/gpu/launch_context.py\n@@ -878,8 +878,18 @@ def _ensure_nvshmem_decls(self):\n   def to_remote(self, ref: ir.Value, peer: ir.Value):\n     self._ensure_nvshmem_decls()\n     if ir.MemRefType.isinstance(ref.type):\n+      # We replace the offset in the ref type by 0, because memref_ptr always\n+      # folds the offset into the pointer.\n+      ref_ty = ir.MemRefType(ref.type)\n+      strides, _ = ref_ty.get_strides_and_offset()\n+      result_type = ir.MemRefType.get(\n+          ref_ty.shape,\n+          ref_ty.element_type,\n+          ir.StridedLayoutAttr.get(0, strides),\n+          ref_ty.memory_space,\n+      )\n       return utils.ptr_as_memref(\n-          self.to_remote(utils.memref_ptr(ref), peer), ref.type\n+          self.to_remote(utils.memref_ptr(ref), peer), result_type\n       )\n     if ref.type != ir.Type.parse(\"!llvm.ptr\"):\n       raise ValueError(f\"Unsupported type for to_remote: {ref.type}\")\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add missing test skips and improve compatibility with older jaxlib versions\n\nPiperOrigin-RevId: 760589385",
        "output": "```diff\nCommit: 168f771c93cc8773fdc53e924defe8bae2b07c6d\nDate: 2025-05-19T12:53:20Z\nURL: https://github.com/jax-ml/jax/commit/168f771c93cc8773fdc53e924defe8bae2b07c6d\nFiles changed: 3\nAdditions: +10, Deletions: -3\ndiff --git a/jax/_src/distributed.py b/jax/_src/distributed.py\nindex dad445b8e539..ae1baf8052c0 100644\n--- a/jax/_src/distributed.py\n+++ b/jax/_src/distributed.py\n@@ -159,7 +159,9 @@ def shutdown(self):\n     if self.preemption_sync_manager:\n       # It's important to shut down the preemption sync manager before the\n       # client because the preemption sync manager depends on the client.\n-      self.preemption_sync_manager.shutdown()\n+      # TODO: Delete hasattr check once 0.6.1 is the minimum jaxlib version\n+      if hasattr(self.preemption_sync_manager, \"shutdown\"):\n+        self.preemption_sync_manager.shutdown()\n       self.preemption_sync_manager = None\n     if self.client:\n       self.client.shutdown()\ndiff --git a/tests/fused_attention_stablehlo_test.py b/tests/fused_attention_stablehlo_test.py\nindex 64e0f4377462..925fc2ed4825 100644\n--- a/tests/fused_attention_stablehlo_test.py\n+++ b/tests/fused_attention_stablehlo_test.py\n@@ -503,6 +503,9 @@ def test_sdpa_broadcast_bias_and_dbias(self):\n   )\n   @jtu.run_on_devices(\"cuda\")\n   def test_sdpa_dbias(self, batch_size: int):\n+    # TODO: Delete once 0.6.0 is no longer supported.\n+    if jtu.jaxlib_version() == (0, 6, 0):\n+      self.skipTest(\"jaxlib 0.6.0 has a bug\")\n     if jax.device_count() < 4:\n       self.skipTest(\"Requires more than 4 devices.\")\n     # cuDNN only supports dbias when batch size is 1. If the batch size is\ndiff --git a/tests/multiprocess_gpu_test.py b/tests/multiprocess_gpu_test.py\nindex 20a2b9ba972b..c2ec44916745 100644\n--- a/tests/multiprocess_gpu_test.py\n+++ b/tests/multiprocess_gpu_test.py\n@@ -82,9 +82,11 @@ def test_gpu_distributed_initialize(self):\n \n       try:\n         for proc in subprocesses:\n-          out, _ = proc.communicate()\n+          out, err = proc.communicate()\n           self.assertEqual(proc.returncode, 0)\n-          self.assertEqual(out, f'{num_gpus_per_task},{num_gpus}')\n+          self.assertEqual(\n+              out, f\"{num_gpus_per_task},{num_gpus}\", msg=f\"Process failed:\\n\\n{err}\",\n+          )\n       finally:\n         for proc in subprocesses:\n           proc.kill()\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "#sdy fix `shard_map` lowering if there is only one device.\n\nThis was wrong when the shmap contains callbacks, which cause tokens to be created. I was calling `jaxpr_subcomp` incorrectly.\n\nPiperOrigin-RevId: 760588882",
        "output": "```diff\nCommit: 0f5e952975a15435a4e71395c45f4f3998ab79d9\nDate: 2025-05-19T12:51:20Z\nURL: https://github.com/jax-ml/jax/commit/0f5e952975a15435a4e71395c45f4f3998ab79d9\nFiles changed: 2\nAdditions: +14, Deletions: -9\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex ac529a667d04..010773f74d0c 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -812,17 +812,13 @@ def _shard_map_lowering_shardy(\n   if np.prod([mesh.shape[a] for a in manual_axes]) == 1:\n     # No need for a `ManualComputationOp` if all manual axes are size 1.\n     with _extend_axis_env(mesh, manual_axes), config._check_vma(check_vma):\n-      args = (*ctx.dim_var_values, *tokens, *in_nodes)\n       out_nodes, tokens_out = mlir.jaxpr_subcomp(\n           sub_ctx, jaxpr, ctx.name_stack,\n-          mlir.TokenSet(zip(ctx.tokens_in.effects(), in_nodes[:num_tokens])),\n-        (), *args[num_tokens:],\n+          mlir.TokenSet(zip(ctx.tokens_in.effects(), tokens)),\n+          (), *in_nodes,\n           dim_var_values=ctx.dim_var_values)\n-      num_tokens = len(tokens_out.effects())\n-      tokens_out = tokens_out.update_tokens(mlir.TokenSet(zip(\n-          ctx.tokens_in.effects(), out_nodes[:num_tokens])))\n       ctx.set_tokens_out(tokens_out)\n-    return out_nodes[num_tokens:]\n+    return out_nodes\n \n   in_shardings = list(map(\n       partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\ndiff --git a/tests/python_callback_test.py b/tests/python_callback_test.py\nindex 9a3b26530044..26664faa6faf 100644\n--- a/tests/python_callback_test.py\n+++ b/tests/python_callback_test.py\n@@ -1363,9 +1363,18 @@ def f_base(i, x):\n     jax.effects_barrier()\n     self.assertEqual(_collected, expected)\n \n-  def test_can_shard_io_callback_manually(self):\n+  @parameterized.named_parameters(\n+    dict(testcase_name='multi_device',\n+         single_device=False),\n+    dict(testcase_name='single_device',\n+         single_device=True)\n+  )\n+  def test_can_shard_io_callback_manually(self, single_device: bool):\n \n-    mesh = Mesh(np.array(jax.devices()), axis_names=('x',))\n+    devices = jax.devices()\n+    if single_device:\n+      devices = devices[:1]\n+    mesh = Mesh(np.array(devices), axis_names=('x',))\n \n     spec = jax.sharding.PartitionSpec('x')\n     sharding = jax.sharding.NamedSharding(mesh, spec)\n\n```",
        "from_id": [
            "bartchr808",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Ignore singleton tiling dims when constructing nested shape\n\nThey don't contribute anything, but they can trigger some NotImplemented errors\ndown the line.\n\nPiperOrigin-RevId: 760579238",
        "output": "```diff\nCommit: 8f7f3e10be5755b2b3296830ba024dbb381462d0\nDate: 2025-05-19T12:15:10Z\nURL: https://github.com/jax-ml/jax/commit/8f7f3e10be5755b2b3296830ba024dbb381462d0\nFiles changed: 2\nAdditions: +21, Deletions: -0\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex 62c5903f475f..acbcf7c2cda1 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -2166,10 +2166,12 @@ def transfer_tiled2(\n       raise ValueError()\n     nested_ref_shape = tuple(\n         (ref_ty.shape[i], ref_ty.shape[i + ref_logical_rank])\n+        if ref_ty.shape[i + ref_logical_rank] != 1 else (ref_ty.shape[i],)\n         for i in range(ref_logical_rank)\n     )\n     nested_ref_strides = tuple(\n         (ref_strides[i], ref_strides[i + ref_logical_rank])\n+        if ref_ty.shape[i + ref_logical_rank] != 1 else (ref_strides[i],)\n         for i in range(ref_logical_rank)\n     )\n     tiled_nested_shape, tiled_nested_strides = tiling.tile_nested_shape_strides(\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 08b424731f19..39bd8aa77331 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -1469,6 +1469,25 @@ def kernel(ctx, src, dst, smem):\n     y = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)(x)\n     np.testing.assert_array_equal(y, x)\n \n+  def test_tma_with_1d_tiling(self):\n+    swizzle = 128\n+    dtype = jnp.float16\n+    shape = (64, 128)\n+    tiling = (1, swizzle // jnp.dtype(dtype).itemsize)\n+    def kernel(ctx, dst, smem):\n+      iota_tensor(*shape, dtype=dtype).store_tiled(smem, swizzle=swizzle)\n+      ctx.async_copy(\n+          src_ref=smem,\n+          dst_ref=dst,\n+          swizzle=swizzle,\n+          gmem_transform=mgpu.TileTransform(tiling),\n+      )\n+      ctx.await_async_copy(0)\n+    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n+    smem = jax.ShapeDtypeStruct(utils.tile_shape(shape, tiling), dtype)\n+    y = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), x, smem)()\n+    np.testing.assert_array_equal(y, x)\n+\n   @parameterized.named_parameters(\n       (\n           f\"_{''.join(map(str, collective_dims))}={collective_size}{'_' + ''.join(map(str, noncollective_dims)) if noncollective_dims else ''}\",\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add support for fp8 types in WGMMA\n\nPiperOrigin-RevId: 760551961",
        "output": "```diff\nCommit: 097e755b22400f1d6ea633610b70e03d311d2e09\nDate: 2025-05-19T10:38:52Z\nURL: https://github.com/jax-ml/jax/commit/097e755b22400f1d6ea633610b70e03d311d2e09\nFiles changed: 3\nAdditions: +40, Deletions: -13\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex 89a58e4788f5..c4f670527a0c 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -195,7 +195,7 @@ def mma(\n           f\" type f32 or f16, but got: {d.dtype}\"\n       )\n   else:\n-    raise NotImplementedError(f\"Unsupported element type: {element_type}\", type(element_type))\n+    raise NotImplementedError(f\"Unsupported element type: {element_type}\")\n \n   # Step 2. Decide on the instruction shapes we'll use. Note that with swizzles,\n   # instructions must be issued in groups of the same width as the swizzle.\ndiff --git a/jax/experimental/mosaic/gpu/wgmma.py b/jax/experimental/mosaic/gpu/wgmma.py\nindex 8baa16d8a7e9..3637778c371b 100644\n--- a/jax/experimental/mosaic/gpu/wgmma.py\n+++ b/jax/experimental/mosaic/gpu/wgmma.py\n@@ -85,10 +85,11 @@ def tree_unflatten(cls, aux, value):\n \n def _supported_wgmma_types(dtype, abtype) -> bool:\n   input_types_are = lambda ty: ty.isinstance(abtype)\n+  f16_acc_types = (ir.F16Type, ir.Float8E5M2Type, ir.Float8E4M3FNType)\n   if ir.F32Type.isinstance(dtype):\n-    return any(input_types_are(ty) for ty in (ir.FloatTF32Type, ir.BF16Type, ir.F16Type))\n+    return any(input_types_are(ty) for ty in (ir.FloatTF32Type, ir.BF16Type, *f16_acc_types))\n   elif ir.F16Type.isinstance(dtype):\n-    return input_types_are(ir.F16Type)\n+    return any(input_types_are(ty) for ty in f16_acc_types)\n   else:\n     return False\n \n@@ -187,8 +188,12 @@ def take_regs(n):\n   b_desc_reg, use_out_reg = take_regs(2)\n   imm_regs = \", \".join(take_regs(num_imm_regs))  # Immediate regs (scale, ...).\n   assert next(reg_count) == len(reg_constraints_list)\n-  el_ty = element_type\n   k_instr = 32 // bytewidth(element_type)\n+  el_ty = str(element_type)\n+  if ir.Float8E5M2Type.isinstance(element_type):\n+    el_ty = \"e5m2\"\n+  elif ir.Float8E4M3FNType.isinstance(element_type):\n+    el_ty = \"e4m3\"\n   wgmma_instr = (\n       f\"wgmma.mma_async.sync.aligned.m64n{n}k{k_instr}.{out_ty}.{el_ty}.{el_ty} \"\n       f\"{acc_reg_vector}, {a_regs}, {b_desc_reg}, p, {imm_regs};\"\n@@ -291,18 +296,24 @@ def wgmma(\n         f\"Accumulator shape mismatch: expected {(m, n)}, got {acc.value.shape}\"\n     )\n   f32 = ir.F32Type.get()\n+  f16 = ir.F16Type.get()\n   if element_type == f32 or element_type == ir.BF16Type.get():\n     if acc.value.mlir_dtype != f32:\n       raise ValueError(\n           f\"WGMMA with element type {element_type} only supports accumulators\"\n           f\" of type f32, but got: {acc.value.mlir_dtype}\"\n       )\n-  elif element_type == ir.F16Type.get():\n-    if acc.value.mlir_dtype != element_type and acc.value.mlir_dtype != f32:\n+  elif any(\n+      t.isinstance(element_type)\n+      for t in {ir.F16Type, ir.Float8E5M2Type, ir.Float8E4M3FNType}\n+  ):\n+    if acc.value.mlir_dtype != f16 and acc.value.mlir_dtype != f32:\n       raise ValueError(\n-          \"WGMMA with element type f16 only supports accumulators of type f32\"\n-          f\" or f16, but got: {acc.value.mlir_dtype}\"\n+          f\"WGMMA with element type {element_type} only supports accumulators \"\n+          f\"of type f32 or f16, but got: {acc.value.mlir_dtype}\"\n       )\n+  else:\n+    raise NotImplementedError(f\"Unsupported element type: {element_type}\")\n \n   # Step 2. Decide on the instruction shapes we'll use. Note that with swizzles,\n   # instructions must be issued in groups of the same width as the swizzle.\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 62f377f031a0..08b424731f19 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -653,7 +653,13 @@ def setUp(self):\n   @parameterized.product(\n       lhs_transpose=(False, True),\n       rhs_transpose=(False, True),\n-      in_mlir_dtype_cls=(ir.F16Type, ir.BF16Type, ir.F32Type),\n+      in_mlir_dtype_cls=(\n+          ir.F16Type,\n+          ir.BF16Type,\n+          ir.F32Type,\n+          ir.Float8E5M2Type,\n+          ir.Float8E4M3FNType,\n+      ),\n       m=(64, 128, 192),\n       n=(64, 128, 192),\n       k_steps=(1, 2),\n@@ -675,8 +681,8 @@ def test_wgmma_basic(\n       rhs_tiling_kind,\n       lhs_tiling_kind,\n   ):\n-    if jax_out_dtype == jnp.float16 and in_mlir_dtype_cls is not ir.F16Type:\n-      self.skipTest(\"Only f16 input is supported for f16 output.\")\n+    if jax_out_dtype == jnp.float16 and in_mlir_dtype_cls in {ir.F32Type, ir.BF16Type}:\n+      self.skipTest(f\"{in_mlir_dtype_cls.get()} does not support f16 output.\")\n     if swizzle != 128 and lhs_transpose and lhs_tiling_kind == \"large\":\n       self.skipTest(\"Transpose only supported in 128B swizzled WGMMA\")\n     if rhs_tiling_kind == \"small+no_transpose\" and not rhs_transpose:\n@@ -686,10 +692,10 @@ def test_wgmma_basic(\n \n     in_mlir_dtype = in_mlir_dtype_cls.get()\n     out_mlir_dtype = utils.dtype_to_ir_type(jax_out_dtype)\n+    if (lhs_transpose or not rhs_transpose) and bytewidth(in_mlir_dtype) != 2:\n+      self.skipTest(\"Transpose only supported in 16-bit WGMMA\")\n     if ir.F32Type.isinstance(in_mlir_dtype):  # We actually use tf32 instead\n       in_jax_dtype = jnp.float32\n-      if lhs_transpose or not rhs_transpose:\n-        self.skipTest(\"Transpose only supported in 16-bit WGMMA\")\n       exponent_bits, mantissa_bits = 8, 10  # Use tf32\n     elif bytewidth(in_mlir_dtype) == 2:\n       if n % 64 != 0:\n@@ -702,10 +708,18 @@ def test_wgmma_basic(\n         exponent_bits, mantissa_bits = 8, 7\n       else:\n         raise NotImplementedError(in_mlir_dtype)\n+    elif in_mlir_dtype_cls == ir.Float8E5M2Type:\n+      in_jax_dtype = jnp.float8_e5m2\n+      exponent_bits, mantissa_bits = 5, 2\n+    elif in_mlir_dtype_cls == ir.Float8E4M3FNType:\n+      in_jax_dtype = jnp.float8_e4m3fn\n+      exponent_bits, mantissa_bits = 4, 3\n     else:\n       raise NotImplementedError(in_mlir_dtype)\n     nk_tile = swizzle // bytewidth(in_mlir_dtype)\n     k = nk_tile * k_steps\n+    if n % nk_tile:\n+      self.skipTest(\"tiling does not divide N\")\n     assert m % 64 == 0 and n % nk_tile == 0\n \n     small_rhs_tile = rhs_tiling_kind != \"large\"\n@@ -781,6 +795,8 @@ def quantize(x):\n     x32, y32 = x.astype(np.float32), y.astype(np.float32)\n     ref = (x32.T if lhs_transpose else x32) @ (y32.T if rhs_transpose else y32)\n     atol = 2e-2 if jax_out_dtype == jnp.float16 else 5e-6\n+    if utils.bitwidth(in_mlir_dtype) == 8:\n+      atol = 3e-2\n     np.testing.assert_allclose(z, ref, atol=atol)\n \n   # TODO(apaszke): Add support for f32\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Added support for unrolling to `lax.fori_loop` lowering\n\nWe currently require that `unroll` divides `length` for simplicity. This\nrestriction can be lifted later if/when necessary.\n\nPiperOrigin-RevId: 760540276",
        "output": "```diff\nCommit: 88105e90e03dc52055a57f2d84628bb563a053e9\nDate: 2025-05-19T09:56:21Z\nURL: https://github.com/jax-ml/jax/commit/88105e90e03dc52055a57f2d84628bb563a053e9\nFiles changed: 2\nAdditions: +55, Deletions: -23\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex b611ea4c17f9..73623de43e39 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -2370,11 +2370,11 @@ def _lower_jaxpr_to_for_loop(\n     ctx: LoweringRuleContext,\n     jaxpr: jax_core.Jaxpr,\n     start: ir.Value,\n-    length: ir.Value | int,\n+    length: int | ir.Value,\n     consts,\n     *args,\n     has_loop_index: bool,\n-    unroll: bool = False,\n+    unroll: int | None = None,\n ):\n   _consts_avals, arg_avals = util.split_list(ctx.avals_in, [len(consts)])\n   arg_avals = arg_avals[has_loop_index:]\n@@ -2395,22 +2395,42 @@ def as_values(vals, avals):\n     return [v if a else _ensure(v, av) for a, v, av in zip(is_acc, vals, avals)]\n \n   def loop(loop_index, body_args):\n-    if has_loop_index:\n-      loop_index = arith_dialect.addi(loop_index, start)\n-      jaxpr_args = [*consts, loop_index, *body_args]\n-    else:\n-      jaxpr_args = [*consts, *body_args]\n-    outs = lower_jaxpr_to_mosaic_gpu(\n-        ctx.module_ctx, ctx.launch_ctx, jaxpr, jaxpr_args\n-    )\n+    outs = body_args\n+    if unroll is not None:\n+      loop_index = arith_dialect.muli(\n+          loop_index, _ir_constant(unroll, start.type)\n+      )\n+    loop_index = arith_dialect.addi(loop_index, start)\n+    for step in range(unroll or 1):\n+      if has_loop_index:\n+        loop_index = arith_dialect.addi(\n+            loop_index, _ir_constant(step, start.type)\n+        )\n+        jaxpr_args = [*consts, loop_index, *outs]\n+      else:\n+        jaxpr_args = [*consts, *outs]\n+      outs = lower_jaxpr_to_mosaic_gpu(\n+          ctx.module_ctx, ctx.launch_ctx, jaxpr, jaxpr_args\n+      )\n     return as_values(outs, out_avals)\n \n-  if unroll:\n-    assert isinstance(length, int)\n-    outs = as_values(args, arg_avals)\n-    for i in range(length):\n-      outs = loop(_ir_constant(i, start.type), outs)\n-    return outs\n+  if unroll is not None:\n+    if not isinstance(length, int):\n+      raise NotImplementedError(\n+          \"``length`` must be an integer when ``unroll` is specified, got\"\n+          f\" {length}\"\n+      )\n+    if length % unroll:\n+      # TODO(slebedev): Emit an epilogue taking care of the remaining steps.\n+      raise NotImplementedError(\n+          f\"``unroll`` must divide ``length``, got {unroll=} and {length=}\"\n+      )\n+    if unroll == length:\n+      # Special-case: the loop is fully unrolled.\n+      return loop(_ir_constant(0, start.type), as_values(args, arg_avals))\n+    return mgpu.fori(\n+        _ir_constant(length // unroll, start.type), as_values(args, arg_avals)\n+    )(loop).results\n   else:\n     if not isinstance(length, ir.Value):\n       length = _ir_constant(length, start.type)\n@@ -2432,11 +2452,7 @@ def _scan_lowering_rule(\n     _split_transpose: bool,\n ):\n   # Can only handle fori_loop-like scans.\n-  if (\n-      (num_extensive := len(args) - num_consts - num_carry)\n-      or reverse\n-      or not (unroll == 1 or unroll == length)\n-  ):\n+  if (num_extensive := len(args) - num_consts - num_carry) or reverse:\n     raise NotImplementedError\n   del linear, num_extensive, reverse\n \n@@ -2465,7 +2481,7 @@ def _scan_lowering_rule(\n       consts,\n       *args,\n       has_loop_index=has_loop_index,\n-      unroll=unroll == length,\n+      unroll=unroll,\n   )\n   if has_loop_index:\n     # Need to return the final loop index value if the outer scan expects\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex dc35f03843f7..d71593dc9078 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1147,11 +1147,27 @@ def test_fori_loop_array(self, force_while):\n     )\n     def kernel(x_ref, o_ref):\n       # Equivalent to x_ref[...] + 2 + 3.\n-      o_ref[...] = _fori_loop(force_while, 2, 4, lambda i, x: x + i, x_ref[...])\n+      o_ref[...] = _fori_loop(\n+          force_while, 2, 4, lambda i, x: x + i, x_ref[...]\n+      )\n \n     x = jnp.arange(256, dtype=jnp.int32)\n     np.testing.assert_array_equal(kernel(x), x + 2 + 3)\n \n+  @parameterized.product(unroll=[1, 2])\n+  def test_fori_loop_array_unrolled(self, unroll):\n+    @functools.partial(\n+        self.pallas_call, out_shape=jax.ShapeDtypeStruct([256], jnp.int32)\n+    )\n+    def kernel(x_ref, o_ref):\n+      # Equivalent to x_ref[...] + 2 + 3 + 4 + 5.\n+      o_ref[...] = lax.fori_loop(\n+          2, 6, lambda i, x: x + i, x_ref[...], unroll=unroll\n+      )\n+\n+    x = jnp.arange(256, dtype=jnp.int32)\n+    np.testing.assert_array_equal(kernel(x), x + 2 + 3 + 4 + 5)\n+\n   @parameterized.product(force_while=[False, True])\n   def test_fori_loop_scalar(self, force_while):\n     @functools.partial(\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] Generalized `empty_like` to accept a pytree of shapes/dtypes\n\nPiperOrigin-RevId: 760529300",
        "output": "```diff\nCommit: 0a3e8dc9b7dc636b4deba44adc8ca8174d2f562b\nDate: 2025-05-19T09:17:02Z\nURL: https://github.com/jax-ml/jax/commit/0a3e8dc9b7dc636b4deba44adc8ca8174d2f562b\nFiles changed: 2\nAdditions: +30, Deletions: -42\ndiff --git a/jax/_src/pallas/helpers.py b/jax/_src/pallas/helpers.py\nindex 684101e47e9e..6b274c0b6cce 100644\n--- a/jax/_src/pallas/helpers.py\n+++ b/jax/_src/pallas/helpers.py\n@@ -13,10 +13,7 @@\n # limitations under the License.\n \"\"\"Pallas helper functions.\"\"\"\n \n-from typing import Any, Protocol\n-\n import jax\n-import jax.numpy as jnp\n from jax._src.pallas import pallas_call\n from jax._src.pallas import core as pl_core\n \n@@ -24,39 +21,42 @@\n @jax.named_call\n def empty(\n     shape: tuple[int, ...],\n-    dtype: jnp.dtype,\n+    dtype: jax.typing.DTypeLike,\n     *,\n-    memory_space: Any = None,\n-    interpret: Any = False,\n+    memory_space: object | None = None,\n+    interpret: bool = False,\n+    backend: pl_core.Backend | None = None,\n ):\n-  def _empty_kernel(_):\n-    # No-op to leave the out_ref uninitialized\n-    pass\n+  return empty_like(\n+      jax.ShapeDtypeStruct(shape, dtype),\n+      memory_space=memory_space,\n+      interpret=interpret,\n+      backend=backend,\n+  )\n+\n \n+@jax.named_call\n+def empty_like(\n+    x: object,\n+    *,\n+    memory_space: object | None = None,\n+    interpret: bool = False,\n+    backend: pl_core.Backend | None = None,\n+):\n   if memory_space is None:\n-    kernel_memory_space = pl_core.MemorySpace.ANY\n-    memory_space = jax.ShapeDtypeStruct\n-  else:\n-    kernel_memory_space = memory_space\n+    memory_space = pl_core.MemorySpace.ANY\n   return pallas_call.pallas_call(\n-      _empty_kernel,\n-      in_specs=[],\n-      out_specs=pl_core.BlockSpec(memory_space=kernel_memory_space),\n-      out_shape=memory_space(shape, dtype),\n+      # No-op to leave the out_ref uninitialized\n+      lambda *_: None,\n+      out_specs=jax.tree.map(\n+          lambda _: pl_core.BlockSpec(memory_space=memory_space), x\n+      ),\n+      out_shape=x,\n       interpret=interpret,\n+      backend=backend,\n   )()\n \n \n-class ArrayLike(Protocol):\n-  shape: tuple[int, ...]\n-  dtype: jnp.dtype\n-\n-\n-def empty_like(\n-    x: ArrayLike, *, memory_space: Any = None, interpret: Any = False):\n-  return empty(x.shape, x.dtype, memory_space=memory_space, interpret=interpret)\n-\n-\n def when(condition):\n   def _wrapped(f):\n     if isinstance(condition, bool):\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 4a5cfbf3517f..3d43e2c0ab61 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -33,7 +33,7 @@\n from jax._src import tree_util\n from jax._src.lib.mlir.dialects import arith as arith_dialect\n from jax._src.pallas import core as pallas_core\n-from jax._src.pallas import pallas_call\n+from jax._src.pallas import helpers as pallas_helpers\n from jax._src.pallas import primitives as pallas_primitives\n import jax._src.pallas.utils as pallas_utils\n from jax._src.state import discharge as state_discharge\n@@ -175,7 +175,7 @@ def kernel(\n     out_shape: object,\n     *,\n     scratch_shapes: pallas_core.ScratchShapeTree = (),\n-    compiler_params: object | None = None,\n+    compiler_params: pallas_core.CompilerParams | None = None,\n     **mesh_kwargs: object,\n ):\n   if unwrap_out := not isinstance(out_shape, (tuple, list)):\n@@ -195,24 +195,12 @@ def cmap_body():\n           mesh, compiler_params=compiler_params\n       )(cmap_body)\n     _, outs = state_discharge.run_state(stateful)(\n-        (operands, empty_like(out_shape))\n+        (operands, pallas_helpers.empty_like(out_shape, backend=\"mosaic_gpu\"))\n     )\n     return outs[0] if unwrap_out else outs\n   return wrapper\n \n \n-def empty_like(shape):\n-  return pallas_call.pallas_call(\n-      lambda *_: None,\n-      out_shape=shape,\n-      out_specs=jax.tree.map(\n-          lambda _: pallas_core.BlockSpec(memory_space=GPUMemorySpace.GMEM),\n-          shape,\n-      ),\n-      backend=\"mosaic_gpu\",\n-  )()\n-\n-\n def _is_known_divisible(value, divisor, fuel=10) -> bool:\n   \"\"\"Returns True if the value is statically known to be divisible by the divisor.\"\"\"\n   if divisor == 1:\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28804 from mattjj:hijax\n\nPiperOrigin-RevId: 759960246",
        "output": "```diff\nCommit: 6cb11d87657b630d9a87d3ae7c2f1ff4644eaefe\nDate: 2025-05-17T11:25:00Z\nURL: https://github.com/jax-ml/jax/commit/6cb11d87657b630d9a87d3ae7c2f1ff4644eaefe\nFiles changed: 6\nAdditions: +511, Deletions: -314\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex ab97c7ff1c2c..e49173c3df45 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,7 +88,7 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info', '_is_high', '_mut_types']\n+               '_effects', '_debug_info', '_is_high', '_final_typechange_env']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -97,7 +97,7 @@ class Jaxpr:\n   _effects: Effects\n   _debug_info: DebugInfo\n   _is_high: bool\n-  _mut_types: dict[Var, Any]\n+  _final_typechange_env: dict[Var, Any]\n \n   @property\n   def constvars(self) -> list[Var]:\n@@ -128,8 +128,8 @@ def is_high(self) -> bool:\n     return self._is_high\n \n   @property\n-  def mut_types(self) -> dict[Var, Any]:\n-    return self._mut_types\n+  def final_typechange_env(self) -> dict[Var, Any]:\n+    return self._final_typechange_env\n \n   def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                outvars: Sequence[Atom], eqns: Sequence[JaxprEqn],\n@@ -139,7 +139,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n                is_high: bool = False,\n-               mut_types: dict | None = None,\n+               final_typechange_env: dict | None = None,\n                ):\n     \"\"\"\n     Args:\n@@ -165,7 +165,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n     self._is_high = is_high\n-    self._mut_types = mut_types or {}\n+    self._final_typechange_env = final_typechange_env or {}\n \n   def __str__(self):\n     return str(self.pretty_print())\n@@ -193,7 +193,8 @@ def replace(self, **kwargs):\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n         is_high=kwargs.pop(\"is_high\", self.is_high),\n-        mut_types=kwargs.pop(\"mut_types\", self.mut_types),\n+        final_typechange_env=kwargs.pop(\"final_typechange_env\",\n+                                        self.final_typechange_env),\n     )\n     if kwargs:\n       raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 9e875f43d831..f77db5443a86 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -1183,13 +1183,13 @@ def has_effects(effects) -> bool:\n   known_effects = make_jaxpr_effects(jaxpr.constvars, ins_known_and_ref_res,\n                                      known_outvars, known_eqns)\n   known_mut, staged_mut, ins_known_ = {}, {}, set(ins_known)  # type: ignore\n-  for v, t in jaxpr.mut_types.items():\n+  for v, t in jaxpr.final_typechange_env.items():\n     [staged_mut, known_mut][v in ins_known_][v] = t\n \n   # TODO(mattjj,necula): debug info should be updated here\n   jaxpr_known = jaxpr.replace(\n       invars=ins_known_and_ref_res, outvars=known_outvars,\n-      eqns=known_eqns, effects=known_effects, mut_types=known_mut)\n+      eqns=known_eqns, effects=known_effects, final_typechange_env=known_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_known)\n \n   _, ins_staged = partition_list(in_inst, jaxpr.invars)\n@@ -1200,7 +1200,7 @@ def has_effects(effects) -> bool:\n   # TODO(mattjj,necula): debug info should be updated here\n   jaxpr_staged = jaxpr.replace(\n       invars=staged_invars, outvars=outs_staged, eqns=staged_eqns,\n-      effects=staged_effects, mut_types=staged_mut)\n+      effects=staged_effects, final_typechange_env=staged_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_staged)\n \n   return (jaxpr_known, jaxpr_staged, out_unknowns, out_inst, len(residuals),\n@@ -1713,6 +1713,7 @@ class JaxprStackFrame:\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n   is_high: bool\n+  final_typechange_env: dict\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1728,6 +1729,7 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_vars = []\n     self.debug_info = debug_info\n     self.is_high = False\n+    self.final_typechange_env = {}\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1753,9 +1755,8 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n-    mut_types = {v: v.aval for v in invars if v.aval.mutable} if self.is_high else {}\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info, self.is_high, mut_types)\n+                  debug_info, self.is_high, self.final_typechange_env)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1872,6 +1873,8 @@ def new_arg(self, aval, source_info: SourceInfo):\n     self.frame.tracers.append(tracer)\n     self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n     self.frame.invars.append(var)\n+    if aval.mutable:\n+      self.frame.final_typechange_env[var] = aval\n     return tracer\n \n   def new_const(self, c, source_info: SourceInfo):\n@@ -2692,7 +2695,7 @@ def lower_traceable(jaxpr, *lo_args):\n   assert (problem := next(lo_args_, None)) is None\n   hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.mut_types.items()\n+  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.final_typechange_env.items()\n               for lo_val in ty.get(hi_args[in_idx[v]])]\n   lo_outs = [lo_val for t, hi_val in zip(jaxpr.out_avals, hi_outs)\n              for lo_val in t.lower_val(hi_val)]\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 5edd74fe74ef..10e7e697e706 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -1597,21 +1597,18 @@ def _is_high(jaxpr, **_) -> bool:\n   return jaxpr.jaxpr.is_high\n pjit_p.is_high = _is_high  # type: ignore\n \n-def _to_lojax(*hi_args, jaxpr, out_shardings, out_layouts, **params):\n-  num_mut = [len(ty.lo_ty()) for ty in jaxpr.jaxpr.mut_types.values()]\n-  out_shardings = (UNSPECIFIED,) * sum(num_mut) + out_shardings\n-  out_layouts = (None,) * sum(num_mut) + out_layouts\n+def _to_lojax( *hi_args, jaxpr, **params):\n+  params, num_mutants = _lojax_expand_params(jaxpr, **params)\n \n   lo_args = [lo_val for t, hi_val in zip(jaxpr.in_avals, hi_args)\n              for lo_val in t.lower_val(hi_val)]\n   lo_jaxpr = pe.lower_jaxpr(jaxpr)\n-  all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, out_shardings=out_shardings,\n-                         out_layouts=out_layouts, **params)\n-  out_mut, lo_outs = split_list(all_outs, [sum(num_mut)])\n+  all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, **params)\n+  out_mut, lo_outs = split_list(all_outs, [num_mutants])\n \n   out_mut_ = iter(out_mut)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  for var, ty in jaxpr.jaxpr.mut_types.items():\n+  for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n     ty.set(hi_args[in_idx[var]], *it.islice(out_mut_, len(ty.lo_ty())))\n   assert next(out_mut_, None) is None\n \n@@ -1623,6 +1620,31 @@ def _to_lojax(*hi_args, jaxpr, out_shardings, out_layouts, **params):\n   return hi_outs\n pjit_p.to_lojax = _to_lojax\n \n+def _lojax_expand_params(\n+    hi_jaxpr, *, donated_invars, in_shardings, in_layouts, out_shardings,\n+    out_layouts, **params):\n+  # some pjit params match the length of hi_jaxpr.invars/outvars, so when\n+  # lowering we must expand them to match their number of lojax types\n+  def expand(hi_tys, xs):\n+    return tuple(y for hi, x in zip(hi_tys, xs) for y in (x,) * len(hi.lo_ty()))\n+  donated_invars = expand(hi_jaxpr.in_avals , donated_invars)\n+  in_shardings   = expand(hi_jaxpr.in_avals , in_shardings  )\n+  in_layouts     = expand(hi_jaxpr.in_avals , in_layouts    )\n+  out_shardings  = expand(hi_jaxpr.out_avals, out_shardings )\n+  out_layouts    = expand(hi_jaxpr.out_avals, out_layouts   )\n+\n+  # also, the lo_jaxpr has pure outputs corresponding to mutable hi_jaxpr types\n+  num_mutants = sum(len(hi_ty.lo_ty()) for hi_ty in\n+                    hi_jaxpr.jaxpr.final_typechange_env.values())\n+  out_shardings = (UNSPECIFIED,) * num_mutants + out_shardings\n+  out_layouts = (None,) * num_mutants + out_layouts\n+\n+  new_params = dict(params, donated_invars=donated_invars,\n+                    in_shardings=in_shardings, in_layouts=in_layouts,\n+                    out_shardings=out_shardings, out_layouts=out_layouts)\n+  return new_params, num_mutants\n+\n+\n def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n   # If device or backend is set, return the default layout. This is because you\n   # can pass arrays on cpu (with untiled layouts) to jit with backend='tpu'\ndiff --git a/tests/BUILD b/tests/BUILD\nindex c51d40715d15..c417a63404a9 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -1875,6 +1875,17 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"hijax_test\",\n+    srcs = [\"hijax_test.py\"],\n+    deps = [\n+        \"//jax:experimental\",\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n+)\n+\n jax_multiplatform_test(\n     name = \"colocated_python_test\",\n     srcs = [\"colocated_python_test.py\"],\ndiff --git a/tests/attrs_test.py b/tests/attrs_test.py\nindex b6cef7fec4dc..60a3753a7ba5 100644\n--- a/tests/attrs_test.py\n+++ b/tests/attrs_test.py\n@@ -15,9 +15,7 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n-from functools import partial\n import itertools as it\n-import unittest\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -27,10 +25,6 @@\n import jax.numpy as jnp\n \n from jax._src import config\n-from jax._src import core\n-from jax._src import dtypes\n-from jax._src.interpreters import ad\n-from jax._src.interpreters import partial_eval as pe\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip, safe_map\n \n@@ -1332,292 +1326,5 @@ def f(lst1, lst2):\n       f(b, b)\n \n \n-class HiPrimitive(core.Primitive):\n-  def __init__(self, name):\n-    self.name = name\n-    ad.primitive_jvps[self] = self.jvp\n-    ad.primitive_transposes[self] = self.transpose\n-    pe.custom_staging_rules[self] = self.staging\n-\n-  def staging(self, trace, *args, **kwargs):\n-    trace.frame.is_high = True\n-    return trace.default_process_primitive(self, args, kwargs)\n-\n-  def is_high(self, **params):\n-    return True\n-\n-  def abstract_eval(self, *arg_avals, **params):\n-    assert False, \"must override\"\n-\n-  def to_lojax(self, *lotypes_wrapped_in_hitypes, **params):\n-    assert False, \"must override\"\n-\n-  def jvp(self, primals, tangents, **params):\n-    assert False, \"must override\"\n-\n-  def transpose(self, *args, **params):\n-    assert False  # TODO\n-\n-\n-class HijaxTest(jtu.JaxTestCase):\n-\n-  def test_custom_types_and_primitive(self):\n-    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n-\n-    @dataclass(frozen=True)\n-    class MyArray:\n-      arr: jax.Array  # always f32\n-\n-    @dataclass(frozen=True)\n-    class MyTy(core.AbstractValue):\n-      mutable = False\n-\n-      def to_tangent_aval(self):\n-        return MyTy()\n-      def str_short(self, short_dtypes=False):\n-        return 'MyTy'\n-      def lo_ty(self):\n-        return [core.ShapedArray((), jnp.dtype('float32'))]\n-      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n-        return [hi_val.arr]\n-      def raise_val(self, val) -> MyArray:\n-        return MyArray(val)\n-\n-      def __eq__(self, other): return isinstance(other, MyTy)\n-\n-      def vspace_zero(self):\n-        return MyArray(jnp.zeros((), 'float32'))\n-      def vspace_add(self, x, y):\n-        return add(x, y)\n-\n-      def strip_weak_type(self): return self\n-      def normalize(self): return self\n-    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n-\n-    class ToMy(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, lo_aval):\n-        return MyTy(), set()\n-\n-      def to_lojax(_, lo):\n-        return MyArray(lo)\n-\n-      def jvp(_, primals, tangents):\n-        x, x_dot = *primals, *tangents\n-        return to(x), to(x_dot)\n-\n-      def transpose(self, out_bar, _):\n-        return from_(out_bar),\n-\n-    class FromMy(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, hi_aval):\n-        return hi_aval.lo_ty()[0], set()\n-\n-      def to_lojax(_, hi):\n-        return hi.arr\n-\n-      def jvp(_, primals, tangents):\n-        x, x_dot = *primals, *tangents\n-        return from_(x), from_(x_dot)\n-\n-      def transpose(self, out_bar, _):\n-        return to(out_bar),\n-\n-    def to(x): return to_p.bind(x)\n-    to_p = ToMy('to_my')\n-\n-    def from_(x): return from_p.bind(x)\n-    from_p = FromMy('from_my')\n-\n-    def mul(x, y): return mul_p.bind(x, y)\n-    def add(x, y): return add_p.bind(x, y)\n-\n-    class MyMul(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, hi_x, hi_y):\n-        if hi_x != hi_y: raise Exception\n-        return hi_x, set()\n-\n-      def to_lojax(_, hi_x, hi_y):\n-        return MyArray(hi_x.arr * hi_y.arr)\n-\n-      def jvp(_, primals, tangents):\n-        (x, y), (x_dot, y_dot) = primals, tangents\n-        return mul(x, y), add(mul(x, y_dot), mul(x_dot, y))\n-\n-      def transpose(self, out_bar, x, y):\n-        assert ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)\n-        if ad.is_undefined_primal(x):\n-          return mul(out_bar, y), None\n-        else:\n-          return None, mul(x, out_bar)\n-\n-    class MyAdd(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, hi_x, hi_y):\n-        if hi_x != hi_y: raise Exception\n-        return hi_x, set()\n-\n-      def to_lojax(_, hi_x, hi_y):\n-        return MyArray(hi_x.arr + hi_y.arr)\n-\n-      def jvp(_, primals, tangents):\n-        assert False  # TODO\n-\n-      def transpose(self, out_bar, x, y):\n-        return out_bar, out_bar\n-\n-    mul_p = MyMul('my_mul')\n-    add_p = MyAdd('my_add')\n-\n-\n-    @jax.jit\n-    def f(x):\n-      return to(from_(x))\n-\n-    # test basic to/from jit\n-    a = MyArray(jnp.ones(()))\n-    b = f(a)  # don't crash\n-    self.assertIsInstance(b, MyArray)\n-    self.assertAllClose(b.arr, jnp.ones(()))\n-\n-    # test basic to/from autodiff\n-    b, b_dot = jax.jvp(f, (a,), (a,))\n-    self.assertIsInstance(b, MyArray)\n-    self.assertIsInstance(b_dot, MyArray)\n-\n-    # test mul jit and backward pass\n-\n-    @jax.jit\n-    def f(x):\n-      return mul(x, x)\n-\n-    b, f_vjp = jax.vjp(f, a)\n-    self.assertIn('MyTy', str(f_vjp))\n-    a_grad, = f_vjp(b)\n-    self.assertIsInstance(a_grad, MyArray)\n-    self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n-\n-  def test_box_autodiff(self):\n-    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n-    class BoxTy(core.AbstractValue):\n-      mutable = True\n-\n-      def to_tangent_aval(self):\n-        # NOTE not really used, for some reason we had to write it anyway\n-        return core.ShapedArray((), dtypes.float0)\n-\n-      def str_short(self, short_dtypes=False):\n-        return 'BoxTy'\n-\n-      def lower_val(self, box):\n-        return [box._val]\n-\n-      def raise_val(self, val):\n-        return Box(val)  # we're gonna mutate this\n-\n-      def lo_ty(self):\n-        return [core.ShapedArray((), jnp.dtype('float32'))]\n-\n-      def get(self, box):\n-        return [box._val]\n-\n-      def set(self, box, val):\n-        box._val = val\n-\n-    class Box:\n-      def __init__(self, val):\n-        self._val = val\n-      ty = BoxTy()\n-    core.pytype_aval_mappings[Box] = lambda b: b.ty\n-\n-\n-    class BoxSet(HiPrimitive):\n-      multiple_results = True\n-      def is_high(self) -> bool: return True\n-\n-      def abstract_eval(*_, **__):\n-        return [], set()\n-\n-      def to_lojax(_, box, val):\n-        box._val = val\n-        return []\n-\n-      def jvp(_, primals, tangents):\n-        assert False  # TODO\n-\n-      def transpose(_, *args):\n-        assert False  # TODO\n-    box_set_p = BoxSet('box_set')\n-\n-    class BoxGet(HiPrimitive):\n-      def is_high(self) -> bool: return True\n-\n-      def abstract_eval(*_, **__):\n-        return jnp.dtype('float32'), set()\n-\n-      def to_lojax(_, box):\n-        return box._val\n-\n-      def jvp(_, primals, tangents):\n-        assert False  # TODO\n-\n-      def transpose(_, *args):\n-        assert False  # TODO\n-    box_get_p = BoxGet('box_get')\n-\n-    class StashTangents(HiPrimitive):\n-      def is_high(self):\n-        return True\n-\n-      def abstract_eval(_, box_aval, x_aval):\n-        del box_aval\n-        return x_aval, set()\n-\n-      def to_lojax(_, box, x):\n-        assert False  # TODO\n-\n-      def jvp(_, primals, tangents):\n-        box, x = primals\n-        _, x_dot = tangents\n-        box_set(box, x_dot)\n-        return x, x_dot\n-\n-      def transpose(self, *args):\n-        assert False  # TODO\n-    stash_tangents_p = StashTangents('stash_tangents')\n-\n-    def box_set(box, val):\n-      box_set_p.bind(box, val)\n-\n-    def box_get(box):\n-      return box_get_p.bind(box)\n-\n-    def stash_tangents(box, x):\n-      return stash_tangents_p.bind(box, x)\n-\n-    @jax.jit\n-    def f(box, x):\n-      box_set(box, x)\n-\n-    box = Box(0.0)\n-    f(box, 1.)\n-    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n-\n-    @jax.jit\n-    def f(box, x):\n-      x = stash_tangents(box, x)\n-      return x\n-\n-    box = Box(0.0)\n-    jax.jvp(partial(f, box), (3.,), (5.,))\n-    self.assertAllClose(box_get(box), 5.0, check_dtypes=False)\n-\n-\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\ndiff --git a/tests/hijax_test.py b/tests/hijax_test.py\nnew file mode 100644\nindex 000000000000..21034d164d28\n--- /dev/null\n+++ b/tests/hijax_test.py\n@@ -0,0 +1,453 @@\n+# Copyright 2024 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+from functools import partial\n+import itertools as it\n+import unittest\n+\n+from absl.testing import absltest\n+\n+import jax\n+import jax.numpy as jnp\n+\n+from jax._src import config\n+from jax._src import core\n+from jax._src import dtypes\n+from jax._src.interpreters import ad\n+from jax._src.interpreters import partial_eval as pe\n+from jax._src import test_util as jtu\n+from jax._src.util import safe_zip, safe_map\n+\n+config.parse_flags_with_absl()\n+\n+map, unsafe_map = safe_map, map\n+zip, unsafe_zip = safe_zip, zip\n+\n+\n+# TODO(mattjj,dougalm): move HiPrimitive, Box, etc out of tests and into library\n+class HiPrimitive(core.Primitive):\n+  def __init__(self, name):\n+    self.name = name\n+    ad.primitive_jvps[self] = self.jvp\n+    ad.primitive_transposes[self] = self.transpose\n+    pe.custom_staging_rules[self] = self.staging\n+\n+  def staging(self, trace, *args, **kwargs):\n+    trace.frame.is_high = True\n+    return trace.default_process_primitive(self, args, kwargs)\n+\n+  def is_high(self, **params):\n+    return True\n+\n+  def abstract_eval(self, *arg_avals, **params):\n+    assert False, \"must override\"\n+\n+  def to_lojax(self, *lotypes_wrapped_in_hitypes, **params):\n+    assert False, \"must override\"\n+\n+  def jvp(self, primals, tangents, **params):\n+    assert False, \"must override\"\n+\n+  def transpose(self, *args, **params):\n+    assert False  # TODO\n+\n+\n+class BoxTy(core.AbstractValue):\n+  mutable = True\n+\n+  def __init__(self, leaf_avals, treedef):\n+    self._leaf_avals = leaf_avals  # hijax avals\n+    self._treedef = treedef\n+\n+  # aval interface: hashability and str_short\n+  def __hash__(self):\n+    return hash((self._leaf_avals, self._treedef))\n+\n+  def __eq__(self, other):\n+    return (isinstance(other, BoxTy) and self._leaf_avals == other._leaf_avals\n+            and self._treedef == other._treedef)\n+\n+  def str_short(self, short_dtypes=False):\n+    return 'BoxTy'\n+\n+  # hijax interface: lower val, raise val, and low type\n+  def lo_ty(self):\n+    return [lo_aval for hi_aval in self._leaf_avals for lo_aval in hi_aval.lo_ty()]\n+\n+  def lower_val(self, box):\n+    leaf_vals, treedef = jax.tree.flatten(box._val)\n+    assert treedef == self._treedef\n+    return [lo_val for hi_aval, hi_val in zip(self._leaf_avals, leaf_vals)\n+            for lo_val in hi_aval.lower_val(hi_val)]\n+\n+  def raise_val(self, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in self._leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    return Box(jax.tree.unflatten(self._treedef, hi_vals))  # will be mutated\n+\n+  # mutable interface: get/set\n+  def get(self, box):\n+    leaf_vals, treedef = jax.tree.flatten(box._val)\n+    assert treedef == self._treedef\n+    return [lo_val for hi_ty, hi_val in zip(self._leaf_avals, leaf_vals)\n+            for lo_val in hi_ty.lower_val(hi_val)]\n+\n+  def set(self, box, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in self._leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    box._val = jax.tree.unflatten(self._treedef, hi_vals)\n+\n+  # TODO placeholder thing\n+  def to_tangent_aval(self):\n+    return core.ShapedArray((), dtypes.float0)  # TODO revise placeholder\n+\n+class Box:  # noqa: F811\n+  def __init__(self, val):\n+    self._val = val\n+\n+  @property\n+  def ty(self):\n+    leaves, treedef = jax.tree.flatten(self._val)\n+    leaf_avals = tuple(map(core.typeof, leaves))\n+    return BoxTy(leaf_avals, treedef)\n+core.pytype_aval_mappings[Box] = lambda b: b.ty\n+\n+\n+class BoxSet(HiPrimitive):\n+  multiple_results = True\n+\n+  def is_high(self, *, treedef) -> bool: return True\n+\n+  def staging(self, trace, box, *leaves, treedef):\n+    super().staging(trace, box, *leaves, treedef=treedef)\n+    avals = tuple(t.aval for t in leaves)\n+    trace.frame.final_typechange_env[trace.getvar(box)] = BoxTy(avals, treedef)\n+\n+  def abstract_eval(self, box_ty, *leaf_avals, treedef):\n+    return [], set()  # TODO better typechecking...\n+\n+  def to_lojax(_, box, *leaves, treedef):\n+    box._val = jax.tree.unflatten(treedef, leaves)\n+    return []\n+\n+  def jvp(_, primals, tangents, *, treedef):\n+    assert False  # TODO\n+\n+  def transpose(_, *args, treedef):\n+    assert False  # TODO\n+box_set_p = BoxSet('box_set')\n+\n+def box_set(box, val):\n+  leaves, treedef = jax.tree.flatten(val)\n+  box_set_p.bind(box, *leaves, treedef=treedef)\n+\n+\n+class BoxGet(HiPrimitive):\n+  multiple_results = True\n+\n+  def is_high(self) -> bool: return True\n+\n+  def abstract_eval(self, box_ty):\n+    return box_ty._leaf_avals, set()\n+\n+  def to_lojax(_, box):\n+    return jax.tree.leaves(box._val)\n+\n+  def jvp(_, primals, tangents):\n+    assert False  # TODO\n+\n+  def transpose(_, *args):\n+    assert False  # TODO\n+box_get_p = BoxGet('box_get')\n+\n+def box_get(box):\n+  leaf_vals = box_get_p.bind(box)\n+  return jax.tree.unflatten(core.typeof(box)._treedef, leaf_vals)\n+\n+\n+class HijaxTest(jtu.JaxTestCase):\n+\n+  def test_custom_types_and_primitive(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+\n+    @dataclass(frozen=True)\n+    class MyArray:\n+      arr: jax.Array  # always f32\n+\n+    @dataclass(frozen=True)\n+    class MyTy(core.AbstractValue):\n+      mutable = False\n+\n+      def to_tangent_aval(self):\n+        return MyTy()\n+      def str_short(self, short_dtypes=False):\n+        return 'MyTy'\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n+        return [hi_val.arr]\n+      def raise_val(self, val) -> MyArray:\n+        return MyArray(val)\n+\n+      def __eq__(self, other): return isinstance(other, MyTy)\n+\n+      def vspace_zero(self):\n+        return MyArray(jnp.zeros((), 'float32'))\n+      def vspace_add(self, x, y):\n+        return add(x, y)\n+    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n+\n+    class ToMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, lo_aval):\n+        return MyTy(), set()\n+\n+      def to_lojax(_, lo):\n+        return MyArray(lo)\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return to(x), to(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return from_(out_bar),\n+\n+    class FromMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_aval):\n+        return hi_aval.lo_ty()[0], set()\n+\n+      def to_lojax(_, hi):\n+        return hi.arr\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return from_(x), from_(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return to(out_bar),\n+\n+    def to(x): return to_p.bind(x)\n+    to_p = ToMy('to_my')\n+\n+    def from_(x): return from_p.bind(x)\n+    from_p = FromMy('from_my')\n+\n+    def mul(x, y): return mul_p.bind(x, y)\n+    def add(x, y): return add_p.bind(x, y)\n+\n+    class MyMul(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr * hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        (x, y), (x_dot, y_dot) = primals, tangents\n+        return mul(x, y), add(mul(x, y_dot), mul(x_dot, y))\n+\n+      def transpose(self, out_bar, x, y):\n+        assert ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)\n+        if ad.is_undefined_primal(x):\n+          return mul(out_bar, y), None\n+        else:\n+          return None, mul(x, out_bar)\n+\n+    class MyAdd(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr + hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(self, out_bar, x, y):\n+        return out_bar, out_bar\n+\n+    mul_p = MyMul('my_mul')\n+    add_p = MyAdd('my_add')\n+\n+\n+    @jax.jit\n+    def f(x):\n+      return to(from_(x))\n+\n+    # test basic to/from jit\n+    a = MyArray(jnp.ones(()))\n+    b = f(a)  # don't crash\n+    self.assertIsInstance(b, MyArray)\n+    self.assertAllClose(b.arr, jnp.ones(()))\n+\n+    # test basic to/from autodiff\n+    b, b_dot = jax.jvp(f, (a,), (a,))\n+    self.assertIsInstance(b, MyArray)\n+    self.assertIsInstance(b_dot, MyArray)\n+\n+    # test mul jit and backward pass\n+\n+    @jax.jit\n+    def f(x):\n+      return mul(x, x)\n+\n+    b, f_vjp = jax.vjp(f, a)\n+    self.assertIn('MyTy', str(f_vjp))\n+    a_grad, = f_vjp(b)\n+    self.assertIsInstance(a_grad, MyArray)\n+    self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n+\n+  def test_box_autodiff(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+\n+    class StashTangents(HiPrimitive):\n+      def is_high(self):\n+        return True\n+\n+      def abstract_eval(_, box_aval, x_aval):\n+        del box_aval\n+        return x_aval, set()\n+\n+      def to_lojax(_, box, x):\n+        assert False  # TODO\n+\n+      def jvp(_, primals, tangents):\n+        box, x = primals\n+        _, x_dot = tangents\n+        box_set(box, x_dot)\n+        return x, x_dot\n+\n+      def transpose(self, *args):\n+        assert False  # TODO\n+    stash_tangents_p = StashTangents('stash_tangents')\n+\n+    def stash_tangents(box, x):\n+      return stash_tangents_p.bind(box, x)\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    box = Box(0.0)\n+    f(box, 1.)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+\n+    @jax.jit\n+    def f(box, x):\n+      x = stash_tangents(box, x)\n+      return x\n+\n+    box = Box(0.0)\n+    jax.jvp(partial(f, box), (3.,), (5.,))\n+    self.assertAllClose(box_get(box), 5.0, check_dtypes=False)\n+\n+  def test_type_changing_box(self):\n+    box = Box(jnp.arange(1))\n+    box_set(box, jnp.arange(2))\n+    self.assertLen(box._val, 2)\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    f(box, jnp.arange(3))\n+    self.assertLen(box._val, 3)\n+    f(box, jnp.arange(4))\n+    self.assertLen(box._val, 4)\n+\n+  def test_pytree_box(self):\n+    box = Box(None)\n+\n+    @jax.jit\n+    def f(box, x):\n+      assert tracing_ok\n+      val = box_get(box)\n+      if val is None:\n+        box_set(box, x)\n+      else:\n+        box_set(box, [x, x])\n+\n+    tracing_ok = True\n+    f(box, 1.0)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+    f(box, 2.0)\n+    self.assertAllClose(box_get(box), [2.0, 2.0], check_dtypes=False)\n+    f(box, 3.0)\n+    self.assertAllClose(box_get(box), [3.0, 3.0], check_dtypes=False)\n+    tracing_ok = False\n+    f(box, 4.0)\n+    self.assertAllClose(box_get(box), [4.0, 4.0], check_dtypes=False)\n+\n+  def test_pytree_of_hijaxtypes_box(self):\n+\n+    @dataclass(frozen=True)\n+    class MyArray:\n+      arr: jax.Array  # always f32\n+\n+    @dataclass(frozen=True)\n+    class MyTy(core.AbstractValue):\n+      mutable = False\n+\n+      def to_tangent_aval(self):\n+        return MyTy()\n+      def str_short(self, short_dtypes=False):\n+        return 'MyTy'\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n+        return [hi_val.arr]\n+      def raise_val(self, val) -> MyArray:\n+        return MyArray(val)\n+\n+      def __eq__(self, other): return isinstance(other, MyTy)\n+\n+    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n+\n+    box = Box([MyArray(jnp.float32(1)),\n+               MyArray(jnp.float32(2))])\n+\n+    @jax.jit\n+    def f(box):\n+      a, b = box_get(box)\n+      box_set(box, [b, a])\n+\n+    f(box)\n+    val = box_get(box)\n+    self.assertIsInstance(val, list)\n+    self.assertLen(val, 2)\n+    b_, a_ = val\n+    self.assertIsInstance(a_, MyArray)\n+    self.assertIsInstance(b_, MyArray)\n+    self.assertAllClose(a_.arr, 1, check_dtypes=False)\n+    self.assertAllClose(b_.arr, 2, check_dtypes=False)\n+\n+\n+if __name__ == '__main__':\n+  absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Added a test case to guard JAX ad, jax2tf in jax.lax.scan.\n\nPiperOrigin-RevId: 759808188",
        "output": "```diff\nCommit: 4097840c58599971cde03c8d5044b7336bcad085\nDate: 2025-05-17T00:06:18Z\nURL: https://github.com/jax-ml/jax/commit/4097840c58599971cde03c8d5044b7336bcad085\nFiles changed: 1\nAdditions: +48, Deletions: -0\ndiff --git a/jax/experimental/jax2tf/tests/jax2tf_test.py b/jax/experimental/jax2tf/tests/jax2tf_test.py\nindex 3052b532cb97..db608adc3dde 100644\n--- a/jax/experimental/jax2tf/tests/jax2tf_test.py\n+++ b/jax/experimental/jax2tf/tests/jax2tf_test.py\n@@ -1694,6 +1694,54 @@ def f_jax(x):\n                                 \"Unsupported precision in dot_general\"):\n       jax2tf.convert(f_jax, native_serialization=False)(x)\n \n+  def test_jvp_through_loop(self):\n+    # Context: b/388929258\n+\n+    num_actions = 512\n+\n+    def tf_preprocessor(features):\n+      features[\"num_c_actions\"] = tf.constant(256, tf.int32)\n+      return features\n+\n+    def postprocessor(prob, features):\n+      actions = jnp.arange(num_actions, dtype=jnp.int32)\n+      r = actions // features[\"num_c_actions\"]\n+      c = actions - r * features[\"num_c_actions\"]\n+      rr = jnp.array([0.12, 0.3])[r] * prob\n+      rc = (jnp.arange(256) * 0.7)[c] * prob\n+      return rr, rc\n+\n+    def loop_step(features, params):\n+      features = jax2tf.call_tf(tf_preprocessor)(features)\n+      odds = features[\"f1\"] @ params[\"w1\"] + features[\"f2\"] @ params[\"w2\"]\n+      prob = jax.nn.sigmoid(odds)\n+      rr, rc = postprocessor(prob, features)\n+      new_f1 = jnp.mean(rr, keepdims=True)\n+      new_f2 = jnp.mean(rc, keepdims=True)\n+      return new_f1, new_f2\n+\n+    def loop(init_features, params):\n+      def body(carry, unused_x):\n+        f1, f2 = carry\n+        return loop_step({\"f1\": f1, \"f2\": f2}, params), None\n+\n+      (rr, rc), _ = jax.lax.scan(\n+          body, (init_features[\"f1\"], init_features[\"f2\"]), length=10\n+      )\n+      return rr, rc\n+\n+    def loss(features, params):\n+      rr, rc = loop(features, params)\n+      return jnp.mean((rr - rc) ** 2)\n+\n+    jax.grad(loss, argnums=(1,))(\n+        {\"f1\": jnp.array([0.5]), \"f2\": jnp.array([0.7])},\n+        {\n+            \"w1\": jnp.ones((1, num_actions)) * 0.01,\n+            \"w2\": jnp.ones((1, num_actions)) * 0.01,\n+        },\n+    )\n+\n \n @jtu.with_config(jax_enable_custom_prng=True)\n class Jax2tfWithCustomPRNGTest(tf_test_util.JaxToTfTestCase):\n\n```",
        "from_id": [
            "brills",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[hijax] type-changing boxes with pytree contents",
        "output": "```diff\nCommit: 1f592543337c8e542e5ebb801c89b083928f6009\nDate: 2025-05-16T22:37:44Z\nURL: https://github.com/jax-ml/jax/commit/1f592543337c8e542e5ebb801c89b083928f6009\nFiles changed: 6\nAdditions: +512, Deletions: -314\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex ab97c7ff1c2c..e49173c3df45 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,7 +88,7 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info', '_is_high', '_mut_types']\n+               '_effects', '_debug_info', '_is_high', '_final_typechange_env']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -97,7 +97,7 @@ class Jaxpr:\n   _effects: Effects\n   _debug_info: DebugInfo\n   _is_high: bool\n-  _mut_types: dict[Var, Any]\n+  _final_typechange_env: dict[Var, Any]\n \n   @property\n   def constvars(self) -> list[Var]:\n@@ -128,8 +128,8 @@ def is_high(self) -> bool:\n     return self._is_high\n \n   @property\n-  def mut_types(self) -> dict[Var, Any]:\n-    return self._mut_types\n+  def final_typechange_env(self) -> dict[Var, Any]:\n+    return self._final_typechange_env\n \n   def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                outvars: Sequence[Atom], eqns: Sequence[JaxprEqn],\n@@ -139,7 +139,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n                is_high: bool = False,\n-               mut_types: dict | None = None,\n+               final_typechange_env: dict | None = None,\n                ):\n     \"\"\"\n     Args:\n@@ -165,7 +165,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n     self._is_high = is_high\n-    self._mut_types = mut_types or {}\n+    self._final_typechange_env = final_typechange_env or {}\n \n   def __str__(self):\n     return str(self.pretty_print())\n@@ -193,7 +193,8 @@ def replace(self, **kwargs):\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n         is_high=kwargs.pop(\"is_high\", self.is_high),\n-        mut_types=kwargs.pop(\"mut_types\", self.mut_types),\n+        final_typechange_env=kwargs.pop(\"final_typechange_env\",\n+                                        self.final_typechange_env),\n     )\n     if kwargs:\n       raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 9e875f43d831..f77db5443a86 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -1183,13 +1183,13 @@ def has_effects(effects) -> bool:\n   known_effects = make_jaxpr_effects(jaxpr.constvars, ins_known_and_ref_res,\n                                      known_outvars, known_eqns)\n   known_mut, staged_mut, ins_known_ = {}, {}, set(ins_known)  # type: ignore\n-  for v, t in jaxpr.mut_types.items():\n+  for v, t in jaxpr.final_typechange_env.items():\n     [staged_mut, known_mut][v in ins_known_][v] = t\n \n   # TODO(mattjj,necula): debug info should be updated here\n   jaxpr_known = jaxpr.replace(\n       invars=ins_known_and_ref_res, outvars=known_outvars,\n-      eqns=known_eqns, effects=known_effects, mut_types=known_mut)\n+      eqns=known_eqns, effects=known_effects, final_typechange_env=known_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_known)\n \n   _, ins_staged = partition_list(in_inst, jaxpr.invars)\n@@ -1200,7 +1200,7 @@ def has_effects(effects) -> bool:\n   # TODO(mattjj,necula): debug info should be updated here\n   jaxpr_staged = jaxpr.replace(\n       invars=staged_invars, outvars=outs_staged, eqns=staged_eqns,\n-      effects=staged_effects, mut_types=staged_mut)\n+      effects=staged_effects, final_typechange_env=staged_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_staged)\n \n   return (jaxpr_known, jaxpr_staged, out_unknowns, out_inst, len(residuals),\n@@ -1713,6 +1713,7 @@ class JaxprStackFrame:\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n   is_high: bool\n+  final_typechange_env: dict\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1728,6 +1729,7 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_vars = []\n     self.debug_info = debug_info\n     self.is_high = False\n+    self.final_typechange_env = {}\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1753,9 +1755,8 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n-    mut_types = {v: v.aval for v in invars if v.aval.mutable} if self.is_high else {}\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info, self.is_high, mut_types)\n+                  debug_info, self.is_high, self.final_typechange_env)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1872,6 +1873,8 @@ def new_arg(self, aval, source_info: SourceInfo):\n     self.frame.tracers.append(tracer)\n     self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n     self.frame.invars.append(var)\n+    if aval.mutable:\n+      self.frame.final_typechange_env[var] = aval\n     return tracer\n \n   def new_const(self, c, source_info: SourceInfo):\n@@ -2692,7 +2695,7 @@ def lower_traceable(jaxpr, *lo_args):\n   assert (problem := next(lo_args_, None)) is None\n   hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.mut_types.items()\n+  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.final_typechange_env.items()\n               for lo_val in ty.get(hi_args[in_idx[v]])]\n   lo_outs = [lo_val for t, hi_val in zip(jaxpr.out_avals, hi_outs)\n              for lo_val in t.lower_val(hi_val)]\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 5edd74fe74ef..10e7e697e706 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -1597,21 +1597,18 @@ def _is_high(jaxpr, **_) -> bool:\n   return jaxpr.jaxpr.is_high\n pjit_p.is_high = _is_high  # type: ignore\n \n-def _to_lojax(*hi_args, jaxpr, out_shardings, out_layouts, **params):\n-  num_mut = [len(ty.lo_ty()) for ty in jaxpr.jaxpr.mut_types.values()]\n-  out_shardings = (UNSPECIFIED,) * sum(num_mut) + out_shardings\n-  out_layouts = (None,) * sum(num_mut) + out_layouts\n+def _to_lojax( *hi_args, jaxpr, **params):\n+  params, num_mutants = _lojax_expand_params(jaxpr, **params)\n \n   lo_args = [lo_val for t, hi_val in zip(jaxpr.in_avals, hi_args)\n              for lo_val in t.lower_val(hi_val)]\n   lo_jaxpr = pe.lower_jaxpr(jaxpr)\n-  all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, out_shardings=out_shardings,\n-                         out_layouts=out_layouts, **params)\n-  out_mut, lo_outs = split_list(all_outs, [sum(num_mut)])\n+  all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, **params)\n+  out_mut, lo_outs = split_list(all_outs, [num_mutants])\n \n   out_mut_ = iter(out_mut)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  for var, ty in jaxpr.jaxpr.mut_types.items():\n+  for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n     ty.set(hi_args[in_idx[var]], *it.islice(out_mut_, len(ty.lo_ty())))\n   assert next(out_mut_, None) is None\n \n@@ -1623,6 +1620,31 @@ def _to_lojax(*hi_args, jaxpr, out_shardings, out_layouts, **params):\n   return hi_outs\n pjit_p.to_lojax = _to_lojax\n \n+def _lojax_expand_params(\n+    hi_jaxpr, *, donated_invars, in_shardings, in_layouts, out_shardings,\n+    out_layouts, **params):\n+  # some pjit params match the length of hi_jaxpr.invars/outvars, so when\n+  # lowering we must expand them to match their number of lojax types\n+  def expand(hi_tys, xs):\n+    return tuple(y for hi, x in zip(hi_tys, xs) for y in (x,) * len(hi.lo_ty()))\n+  donated_invars = expand(hi_jaxpr.in_avals , donated_invars)\n+  in_shardings   = expand(hi_jaxpr.in_avals , in_shardings  )\n+  in_layouts     = expand(hi_jaxpr.in_avals , in_layouts    )\n+  out_shardings  = expand(hi_jaxpr.out_avals, out_shardings )\n+  out_layouts    = expand(hi_jaxpr.out_avals, out_layouts   )\n+\n+  # also, the lo_jaxpr has pure outputs corresponding to mutable hi_jaxpr types\n+  num_mutants = sum(len(hi_ty.lo_ty()) for hi_ty in\n+                    hi_jaxpr.jaxpr.final_typechange_env.values())\n+  out_shardings = (UNSPECIFIED,) * num_mutants + out_shardings\n+  out_layouts = (None,) * num_mutants + out_layouts\n+\n+  new_params = dict(params, donated_invars=donated_invars,\n+                    in_shardings=in_shardings, in_layouts=in_layouts,\n+                    out_shardings=out_shardings, out_layouts=out_layouts)\n+  return new_params, num_mutants\n+\n+\n def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n   # If device or backend is set, return the default layout. This is because you\n   # can pass arrays on cpu (with untiled layouts) to jit with backend='tpu'\ndiff --git a/tests/BUILD b/tests/BUILD\nindex c51d40715d15..35695c75c79f 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -1875,6 +1875,18 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"hijax_test\",\n+    srcs = [\"hijax_test.py\"],\n+    deps = [\n+        \"//jax:experimental\",\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n+)\n+\n+\n jax_multiplatform_test(\n     name = \"colocated_python_test\",\n     srcs = [\"colocated_python_test.py\"],\ndiff --git a/tests/attrs_test.py b/tests/attrs_test.py\nindex b6cef7fec4dc..60a3753a7ba5 100644\n--- a/tests/attrs_test.py\n+++ b/tests/attrs_test.py\n@@ -15,9 +15,7 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n-from functools import partial\n import itertools as it\n-import unittest\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -27,10 +25,6 @@\n import jax.numpy as jnp\n \n from jax._src import config\n-from jax._src import core\n-from jax._src import dtypes\n-from jax._src.interpreters import ad\n-from jax._src.interpreters import partial_eval as pe\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip, safe_map\n \n@@ -1332,292 +1326,5 @@ def f(lst1, lst2):\n       f(b, b)\n \n \n-class HiPrimitive(core.Primitive):\n-  def __init__(self, name):\n-    self.name = name\n-    ad.primitive_jvps[self] = self.jvp\n-    ad.primitive_transposes[self] = self.transpose\n-    pe.custom_staging_rules[self] = self.staging\n-\n-  def staging(self, trace, *args, **kwargs):\n-    trace.frame.is_high = True\n-    return trace.default_process_primitive(self, args, kwargs)\n-\n-  def is_high(self, **params):\n-    return True\n-\n-  def abstract_eval(self, *arg_avals, **params):\n-    assert False, \"must override\"\n-\n-  def to_lojax(self, *lotypes_wrapped_in_hitypes, **params):\n-    assert False, \"must override\"\n-\n-  def jvp(self, primals, tangents, **params):\n-    assert False, \"must override\"\n-\n-  def transpose(self, *args, **params):\n-    assert False  # TODO\n-\n-\n-class HijaxTest(jtu.JaxTestCase):\n-\n-  def test_custom_types_and_primitive(self):\n-    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n-\n-    @dataclass(frozen=True)\n-    class MyArray:\n-      arr: jax.Array  # always f32\n-\n-    @dataclass(frozen=True)\n-    class MyTy(core.AbstractValue):\n-      mutable = False\n-\n-      def to_tangent_aval(self):\n-        return MyTy()\n-      def str_short(self, short_dtypes=False):\n-        return 'MyTy'\n-      def lo_ty(self):\n-        return [core.ShapedArray((), jnp.dtype('float32'))]\n-      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n-        return [hi_val.arr]\n-      def raise_val(self, val) -> MyArray:\n-        return MyArray(val)\n-\n-      def __eq__(self, other): return isinstance(other, MyTy)\n-\n-      def vspace_zero(self):\n-        return MyArray(jnp.zeros((), 'float32'))\n-      def vspace_add(self, x, y):\n-        return add(x, y)\n-\n-      def strip_weak_type(self): return self\n-      def normalize(self): return self\n-    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n-\n-    class ToMy(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, lo_aval):\n-        return MyTy(), set()\n-\n-      def to_lojax(_, lo):\n-        return MyArray(lo)\n-\n-      def jvp(_, primals, tangents):\n-        x, x_dot = *primals, *tangents\n-        return to(x), to(x_dot)\n-\n-      def transpose(self, out_bar, _):\n-        return from_(out_bar),\n-\n-    class FromMy(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, hi_aval):\n-        return hi_aval.lo_ty()[0], set()\n-\n-      def to_lojax(_, hi):\n-        return hi.arr\n-\n-      def jvp(_, primals, tangents):\n-        x, x_dot = *primals, *tangents\n-        return from_(x), from_(x_dot)\n-\n-      def transpose(self, out_bar, _):\n-        return to(out_bar),\n-\n-    def to(x): return to_p.bind(x)\n-    to_p = ToMy('to_my')\n-\n-    def from_(x): return from_p.bind(x)\n-    from_p = FromMy('from_my')\n-\n-    def mul(x, y): return mul_p.bind(x, y)\n-    def add(x, y): return add_p.bind(x, y)\n-\n-    class MyMul(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, hi_x, hi_y):\n-        if hi_x != hi_y: raise Exception\n-        return hi_x, set()\n-\n-      def to_lojax(_, hi_x, hi_y):\n-        return MyArray(hi_x.arr * hi_y.arr)\n-\n-      def jvp(_, primals, tangents):\n-        (x, y), (x_dot, y_dot) = primals, tangents\n-        return mul(x, y), add(mul(x, y_dot), mul(x_dot, y))\n-\n-      def transpose(self, out_bar, x, y):\n-        assert ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)\n-        if ad.is_undefined_primal(x):\n-          return mul(out_bar, y), None\n-        else:\n-          return None, mul(x, out_bar)\n-\n-    class MyAdd(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, hi_x, hi_y):\n-        if hi_x != hi_y: raise Exception\n-        return hi_x, set()\n-\n-      def to_lojax(_, hi_x, hi_y):\n-        return MyArray(hi_x.arr + hi_y.arr)\n-\n-      def jvp(_, primals, tangents):\n-        assert False  # TODO\n-\n-      def transpose(self, out_bar, x, y):\n-        return out_bar, out_bar\n-\n-    mul_p = MyMul('my_mul')\n-    add_p = MyAdd('my_add')\n-\n-\n-    @jax.jit\n-    def f(x):\n-      return to(from_(x))\n-\n-    # test basic to/from jit\n-    a = MyArray(jnp.ones(()))\n-    b = f(a)  # don't crash\n-    self.assertIsInstance(b, MyArray)\n-    self.assertAllClose(b.arr, jnp.ones(()))\n-\n-    # test basic to/from autodiff\n-    b, b_dot = jax.jvp(f, (a,), (a,))\n-    self.assertIsInstance(b, MyArray)\n-    self.assertIsInstance(b_dot, MyArray)\n-\n-    # test mul jit and backward pass\n-\n-    @jax.jit\n-    def f(x):\n-      return mul(x, x)\n-\n-    b, f_vjp = jax.vjp(f, a)\n-    self.assertIn('MyTy', str(f_vjp))\n-    a_grad, = f_vjp(b)\n-    self.assertIsInstance(a_grad, MyArray)\n-    self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n-\n-  def test_box_autodiff(self):\n-    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n-    class BoxTy(core.AbstractValue):\n-      mutable = True\n-\n-      def to_tangent_aval(self):\n-        # NOTE not really used, for some reason we had to write it anyway\n-        return core.ShapedArray((), dtypes.float0)\n-\n-      def str_short(self, short_dtypes=False):\n-        return 'BoxTy'\n-\n-      def lower_val(self, box):\n-        return [box._val]\n-\n-      def raise_val(self, val):\n-        return Box(val)  # we're gonna mutate this\n-\n-      def lo_ty(self):\n-        return [core.ShapedArray((), jnp.dtype('float32'))]\n-\n-      def get(self, box):\n-        return [box._val]\n-\n-      def set(self, box, val):\n-        box._val = val\n-\n-    class Box:\n-      def __init__(self, val):\n-        self._val = val\n-      ty = BoxTy()\n-    core.pytype_aval_mappings[Box] = lambda b: b.ty\n-\n-\n-    class BoxSet(HiPrimitive):\n-      multiple_results = True\n-      def is_high(self) -> bool: return True\n-\n-      def abstract_eval(*_, **__):\n-        return [], set()\n-\n-      def to_lojax(_, box, val):\n-        box._val = val\n-        return []\n-\n-      def jvp(_, primals, tangents):\n-        assert False  # TODO\n-\n-      def transpose(_, *args):\n-        assert False  # TODO\n-    box_set_p = BoxSet('box_set')\n-\n-    class BoxGet(HiPrimitive):\n-      def is_high(self) -> bool: return True\n-\n-      def abstract_eval(*_, **__):\n-        return jnp.dtype('float32'), set()\n-\n-      def to_lojax(_, box):\n-        return box._val\n-\n-      def jvp(_, primals, tangents):\n-        assert False  # TODO\n-\n-      def transpose(_, *args):\n-        assert False  # TODO\n-    box_get_p = BoxGet('box_get')\n-\n-    class StashTangents(HiPrimitive):\n-      def is_high(self):\n-        return True\n-\n-      def abstract_eval(_, box_aval, x_aval):\n-        del box_aval\n-        return x_aval, set()\n-\n-      def to_lojax(_, box, x):\n-        assert False  # TODO\n-\n-      def jvp(_, primals, tangents):\n-        box, x = primals\n-        _, x_dot = tangents\n-        box_set(box, x_dot)\n-        return x, x_dot\n-\n-      def transpose(self, *args):\n-        assert False  # TODO\n-    stash_tangents_p = StashTangents('stash_tangents')\n-\n-    def box_set(box, val):\n-      box_set_p.bind(box, val)\n-\n-    def box_get(box):\n-      return box_get_p.bind(box)\n-\n-    def stash_tangents(box, x):\n-      return stash_tangents_p.bind(box, x)\n-\n-    @jax.jit\n-    def f(box, x):\n-      box_set(box, x)\n-\n-    box = Box(0.0)\n-    f(box, 1.)\n-    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n-\n-    @jax.jit\n-    def f(box, x):\n-      x = stash_tangents(box, x)\n-      return x\n-\n-    box = Box(0.0)\n-    jax.jvp(partial(f, box), (3.,), (5.,))\n-    self.assertAllClose(box_get(box), 5.0, check_dtypes=False)\n-\n-\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\ndiff --git a/tests/hijax_test.py b/tests/hijax_test.py\nnew file mode 100644\nindex 000000000000..21034d164d28\n--- /dev/null\n+++ b/tests/hijax_test.py\n@@ -0,0 +1,453 @@\n+# Copyright 2024 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+from functools import partial\n+import itertools as it\n+import unittest\n+\n+from absl.testing import absltest\n+\n+import jax\n+import jax.numpy as jnp\n+\n+from jax._src import config\n+from jax._src import core\n+from jax._src import dtypes\n+from jax._src.interpreters import ad\n+from jax._src.interpreters import partial_eval as pe\n+from jax._src import test_util as jtu\n+from jax._src.util import safe_zip, safe_map\n+\n+config.parse_flags_with_absl()\n+\n+map, unsafe_map = safe_map, map\n+zip, unsafe_zip = safe_zip, zip\n+\n+\n+# TODO(mattjj,dougalm): move HiPrimitive, Box, etc out of tests and into library\n+class HiPrimitive(core.Primitive):\n+  def __init__(self, name):\n+    self.name = name\n+    ad.primitive_jvps[self] = self.jvp\n+    ad.primitive_transposes[self] = self.transpose\n+    pe.custom_staging_rules[self] = self.staging\n+\n+  def staging(self, trace, *args, **kwargs):\n+    trace.frame.is_high = True\n+    return trace.default_process_primitive(self, args, kwargs)\n+\n+  def is_high(self, **params):\n+    return True\n+\n+  def abstract_eval(self, *arg_avals, **params):\n+    assert False, \"must override\"\n+\n+  def to_lojax(self, *lotypes_wrapped_in_hitypes, **params):\n+    assert False, \"must override\"\n+\n+  def jvp(self, primals, tangents, **params):\n+    assert False, \"must override\"\n+\n+  def transpose(self, *args, **params):\n+    assert False  # TODO\n+\n+\n+class BoxTy(core.AbstractValue):\n+  mutable = True\n+\n+  def __init__(self, leaf_avals, treedef):\n+    self._leaf_avals = leaf_avals  # hijax avals\n+    self._treedef = treedef\n+\n+  # aval interface: hashability and str_short\n+  def __hash__(self):\n+    return hash((self._leaf_avals, self._treedef))\n+\n+  def __eq__(self, other):\n+    return (isinstance(other, BoxTy) and self._leaf_avals == other._leaf_avals\n+            and self._treedef == other._treedef)\n+\n+  def str_short(self, short_dtypes=False):\n+    return 'BoxTy'\n+\n+  # hijax interface: lower val, raise val, and low type\n+  def lo_ty(self):\n+    return [lo_aval for hi_aval in self._leaf_avals for lo_aval in hi_aval.lo_ty()]\n+\n+  def lower_val(self, box):\n+    leaf_vals, treedef = jax.tree.flatten(box._val)\n+    assert treedef == self._treedef\n+    return [lo_val for hi_aval, hi_val in zip(self._leaf_avals, leaf_vals)\n+            for lo_val in hi_aval.lower_val(hi_val)]\n+\n+  def raise_val(self, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in self._leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    return Box(jax.tree.unflatten(self._treedef, hi_vals))  # will be mutated\n+\n+  # mutable interface: get/set\n+  def get(self, box):\n+    leaf_vals, treedef = jax.tree.flatten(box._val)\n+    assert treedef == self._treedef\n+    return [lo_val for hi_ty, hi_val in zip(self._leaf_avals, leaf_vals)\n+            for lo_val in hi_ty.lower_val(hi_val)]\n+\n+  def set(self, box, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in self._leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    box._val = jax.tree.unflatten(self._treedef, hi_vals)\n+\n+  # TODO placeholder thing\n+  def to_tangent_aval(self):\n+    return core.ShapedArray((), dtypes.float0)  # TODO revise placeholder\n+\n+class Box:  # noqa: F811\n+  def __init__(self, val):\n+    self._val = val\n+\n+  @property\n+  def ty(self):\n+    leaves, treedef = jax.tree.flatten(self._val)\n+    leaf_avals = tuple(map(core.typeof, leaves))\n+    return BoxTy(leaf_avals, treedef)\n+core.pytype_aval_mappings[Box] = lambda b: b.ty\n+\n+\n+class BoxSet(HiPrimitive):\n+  multiple_results = True\n+\n+  def is_high(self, *, treedef) -> bool: return True\n+\n+  def staging(self, trace, box, *leaves, treedef):\n+    super().staging(trace, box, *leaves, treedef=treedef)\n+    avals = tuple(t.aval for t in leaves)\n+    trace.frame.final_typechange_env[trace.getvar(box)] = BoxTy(avals, treedef)\n+\n+  def abstract_eval(self, box_ty, *leaf_avals, treedef):\n+    return [], set()  # TODO better typechecking...\n+\n+  def to_lojax(_, box, *leaves, treedef):\n+    box._val = jax.tree.unflatten(treedef, leaves)\n+    return []\n+\n+  def jvp(_, primals, tangents, *, treedef):\n+    assert False  # TODO\n+\n+  def transpose(_, *args, treedef):\n+    assert False  # TODO\n+box_set_p = BoxSet('box_set')\n+\n+def box_set(box, val):\n+  leaves, treedef = jax.tree.flatten(val)\n+  box_set_p.bind(box, *leaves, treedef=treedef)\n+\n+\n+class BoxGet(HiPrimitive):\n+  multiple_results = True\n+\n+  def is_high(self) -> bool: return True\n+\n+  def abstract_eval(self, box_ty):\n+    return box_ty._leaf_avals, set()\n+\n+  def to_lojax(_, box):\n+    return jax.tree.leaves(box._val)\n+\n+  def jvp(_, primals, tangents):\n+    assert False  # TODO\n+\n+  def transpose(_, *args):\n+    assert False  # TODO\n+box_get_p = BoxGet('box_get')\n+\n+def box_get(box):\n+  leaf_vals = box_get_p.bind(box)\n+  return jax.tree.unflatten(core.typeof(box)._treedef, leaf_vals)\n+\n+\n+class HijaxTest(jtu.JaxTestCase):\n+\n+  def test_custom_types_and_primitive(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+\n+    @dataclass(frozen=True)\n+    class MyArray:\n+      arr: jax.Array  # always f32\n+\n+    @dataclass(frozen=True)\n+    class MyTy(core.AbstractValue):\n+      mutable = False\n+\n+      def to_tangent_aval(self):\n+        return MyTy()\n+      def str_short(self, short_dtypes=False):\n+        return 'MyTy'\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n+        return [hi_val.arr]\n+      def raise_val(self, val) -> MyArray:\n+        return MyArray(val)\n+\n+      def __eq__(self, other): return isinstance(other, MyTy)\n+\n+      def vspace_zero(self):\n+        return MyArray(jnp.zeros((), 'float32'))\n+      def vspace_add(self, x, y):\n+        return add(x, y)\n+    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n+\n+    class ToMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, lo_aval):\n+        return MyTy(), set()\n+\n+      def to_lojax(_, lo):\n+        return MyArray(lo)\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return to(x), to(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return from_(out_bar),\n+\n+    class FromMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_aval):\n+        return hi_aval.lo_ty()[0], set()\n+\n+      def to_lojax(_, hi):\n+        return hi.arr\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return from_(x), from_(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return to(out_bar),\n+\n+    def to(x): return to_p.bind(x)\n+    to_p = ToMy('to_my')\n+\n+    def from_(x): return from_p.bind(x)\n+    from_p = FromMy('from_my')\n+\n+    def mul(x, y): return mul_p.bind(x, y)\n+    def add(x, y): return add_p.bind(x, y)\n+\n+    class MyMul(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr * hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        (x, y), (x_dot, y_dot) = primals, tangents\n+        return mul(x, y), add(mul(x, y_dot), mul(x_dot, y))\n+\n+      def transpose(self, out_bar, x, y):\n+        assert ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)\n+        if ad.is_undefined_primal(x):\n+          return mul(out_bar, y), None\n+        else:\n+          return None, mul(x, out_bar)\n+\n+    class MyAdd(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr + hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(self, out_bar, x, y):\n+        return out_bar, out_bar\n+\n+    mul_p = MyMul('my_mul')\n+    add_p = MyAdd('my_add')\n+\n+\n+    @jax.jit\n+    def f(x):\n+      return to(from_(x))\n+\n+    # test basic to/from jit\n+    a = MyArray(jnp.ones(()))\n+    b = f(a)  # don't crash\n+    self.assertIsInstance(b, MyArray)\n+    self.assertAllClose(b.arr, jnp.ones(()))\n+\n+    # test basic to/from autodiff\n+    b, b_dot = jax.jvp(f, (a,), (a,))\n+    self.assertIsInstance(b, MyArray)\n+    self.assertIsInstance(b_dot, MyArray)\n+\n+    # test mul jit and backward pass\n+\n+    @jax.jit\n+    def f(x):\n+      return mul(x, x)\n+\n+    b, f_vjp = jax.vjp(f, a)\n+    self.assertIn('MyTy', str(f_vjp))\n+    a_grad, = f_vjp(b)\n+    self.assertIsInstance(a_grad, MyArray)\n+    self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n+\n+  def test_box_autodiff(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+\n+    class StashTangents(HiPrimitive):\n+      def is_high(self):\n+        return True\n+\n+      def abstract_eval(_, box_aval, x_aval):\n+        del box_aval\n+        return x_aval, set()\n+\n+      def to_lojax(_, box, x):\n+        assert False  # TODO\n+\n+      def jvp(_, primals, tangents):\n+        box, x = primals\n+        _, x_dot = tangents\n+        box_set(box, x_dot)\n+        return x, x_dot\n+\n+      def transpose(self, *args):\n+        assert False  # TODO\n+    stash_tangents_p = StashTangents('stash_tangents')\n+\n+    def stash_tangents(box, x):\n+      return stash_tangents_p.bind(box, x)\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    box = Box(0.0)\n+    f(box, 1.)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+\n+    @jax.jit\n+    def f(box, x):\n+      x = stash_tangents(box, x)\n+      return x\n+\n+    box = Box(0.0)\n+    jax.jvp(partial(f, box), (3.,), (5.,))\n+    self.assertAllClose(box_get(box), 5.0, check_dtypes=False)\n+\n+  def test_type_changing_box(self):\n+    box = Box(jnp.arange(1))\n+    box_set(box, jnp.arange(2))\n+    self.assertLen(box._val, 2)\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    f(box, jnp.arange(3))\n+    self.assertLen(box._val, 3)\n+    f(box, jnp.arange(4))\n+    self.assertLen(box._val, 4)\n+\n+  def test_pytree_box(self):\n+    box = Box(None)\n+\n+    @jax.jit\n+    def f(box, x):\n+      assert tracing_ok\n+      val = box_get(box)\n+      if val is None:\n+        box_set(box, x)\n+      else:\n+        box_set(box, [x, x])\n+\n+    tracing_ok = True\n+    f(box, 1.0)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+    f(box, 2.0)\n+    self.assertAllClose(box_get(box), [2.0, 2.0], check_dtypes=False)\n+    f(box, 3.0)\n+    self.assertAllClose(box_get(box), [3.0, 3.0], check_dtypes=False)\n+    tracing_ok = False\n+    f(box, 4.0)\n+    self.assertAllClose(box_get(box), [4.0, 4.0], check_dtypes=False)\n+\n+  def test_pytree_of_hijaxtypes_box(self):\n+\n+    @dataclass(frozen=True)\n+    class MyArray:\n+      arr: jax.Array  # always f32\n+\n+    @dataclass(frozen=True)\n+    class MyTy(core.AbstractValue):\n+      mutable = False\n+\n+      def to_tangent_aval(self):\n+        return MyTy()\n+      def str_short(self, short_dtypes=False):\n+        return 'MyTy'\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n+        return [hi_val.arr]\n+      def raise_val(self, val) -> MyArray:\n+        return MyArray(val)\n+\n+      def __eq__(self, other): return isinstance(other, MyTy)\n+\n+    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n+\n+    box = Box([MyArray(jnp.float32(1)),\n+               MyArray(jnp.float32(2))])\n+\n+    @jax.jit\n+    def f(box):\n+      a, b = box_get(box)\n+      box_set(box, [b, a])\n+\n+    f(box)\n+    val = box_get(box)\n+    self.assertIsInstance(val, list)\n+    self.assertLen(val, 2)\n+    b_, a_ = val\n+    self.assertIsInstance(a_, MyArray)\n+    self.assertIsInstance(b_, MyArray)\n+    self.assertAllClose(a_.arr, 1, check_dtypes=False)\n+    self.assertAllClose(b_.arr, 2, check_dtypes=False)\n+\n+\n+if __name__ == '__main__':\n+  absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "Keep the serialized version of `BufferAssignmentProto` in `CompiledMemoryStats` to reduce its overheads\n\nMost users of `CompiledMemoryStats` do not use this field. So it is cheaper in terms of both CPU and RAM to keep it as a serialized string rather than a proto. If this continues to become a problem, we can consider inventing a separate executable API for buffer assignment.\n\nPiperOrigin-RevId: 759735092",
        "output": "```diff\nCommit: 7412adec21c534f8e4bcc627552f28d162decc86\nDate: 2025-05-16T20:31:22Z\nURL: https://github.com/jax-ml/jax/commit/7412adec21c534f8e4bcc627552f28d162decc86\nFiles changed: 1\nAdditions: +4, Deletions: -1\ndiff --git a/jaxlib/xla.cc b/jaxlib/xla.cc\nindex 4020e061b3f4..3412766de6bd 100644\n--- a/jaxlib/xla.cc\n+++ b/jaxlib/xla.cc\n@@ -490,7 +490,10 @@ NB_MODULE(_jax, m) {\n               &CompiledMemoryStats::host_temp_size_in_bytes)\n       .def_prop_ro(\"serialized_buffer_assignment_proto\",\n                    [](const CompiledMemoryStats& cms) -> nb::bytes {\n-#if JAX_IFRT_VERSION_NUMBER >= 7\n+#if JAX_IFRT_VERSION_NUMBER >= 9\n+                     const std::string& s = cms.serialized_buffer_assignment;\n+                     return nb::bytes(s.data(), s.size());\n+#elif JAX_IFRT_VERSION_NUMBER >= 7\n                      if (cms.buffer_assignment.has_value()) {\n                        std::string s =\n                            cms.buffer_assignment->SerializeAsString();\n\n```",
        "from_id": [
            "junwhanahn",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic TPU][NFC] Consolidate `getIntConst`.\n\nPiperOrigin-RevId: 759728429",
        "output": "```diff\nCommit: d47fc8d928905516a0d4dbfda6f70060068c779d\nDate: 2025-05-16T20:11:07Z\nURL: https://github.com/jax-ml/jax/commit/d47fc8d928905516a0d4dbfda6f70060068c779d\nFiles changed: 2\nAdditions: +25, Deletions: -36\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 656be0e677b0..ba1dfc95c66c 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -209,23 +209,18 @@ bool incrementIndex(const MutableArrayRef<int64_t> idx,\n   return false;\n }\n \n-FailureOr<int64_t> getIntConst(Value v, bool silent = false) {\n-  if (auto constant_op = v.getDefiningOp<arith::ConstantOp>()) {\n-    if (auto integer_attr = dyn_cast<IntegerAttr>(constant_op.getValue())) {\n-      return integer_attr.getValue().getSExtValue();\n-    }\n-  }\n-  if (silent) {\n-    return failure();\n+FailureOr<int64_t> expectIntConst(Value v) {\n+  if (auto cst = getIntConst(v)) {\n+    return cst.value();\n   }\n   return emitError(v.getLoc(), \"Expected an integer constant\");\n }\n \n-FailureOr<SmallVector<int64_t>> getIntConstsFromOperandRange(\n-    ValueRange vals, bool silent = false) {\n+FailureOr<SmallVector<int64_t>> expectIntConstsFromOperandRange(\n+    ValueRange vals) {\n   SmallVector<int64_t> res(vals.size());\n   for (int i = 0; i < vals.size(); ++i) {\n-    FAILUREOR_ASSIGN_OR_RETURN(res[i], getIntConst(vals[i], silent));\n+    FAILUREOR_ASSIGN_OR_RETURN(res[i], expectIntConst(vals[i]));\n   }\n   return res;\n }\n@@ -265,7 +260,7 @@ FailureOr<std::pair<Value, SmallVector<int64_t>>> sliceRef(\n   Value c0 = nullptr;\n   SmallVector<int64_t> indices_within_slice(indices.size() - tiling.size(), 0);\n   for (auto tiled_idx : indices.take_back(tiling.size())) {\n-    if (auto cst = getIntConst(tiled_idx, /*silent=*/true); succeeded(cst)) {\n+    if (auto cst = getIntConst(tiled_idx)) {\n       indices_within_slice.push_back(*cst);\n       if (!c0) {\n         c0 = builder.create<arith::ConstantOp>(i32,\n@@ -1548,7 +1543,7 @@ LogicalResult tpu_load_rule(RewriteContext &ctx, Operation &op,\n   }\n   FAILUREOR_ASSIGN_OR_RETURN(\n       const SmallVector<int64_t> indices,\n-      getIntConstsFromOperandRange(load_op.getIndices()));\n+      expectIntConstsFromOperandRange(load_op.getIndices()));\n   TPU_ASSERT_EQ_OP(indices.size(), 2);\n   if (indices[1] % ctx.target_shape[1] != 0) {\n     return op.emitOpError(\"Not implemented: Lane index is not a multiple of \")\n@@ -1606,8 +1601,8 @@ LogicalResult strided_op_rule_impl(RewriteContext &ctx, Operation &op,\n   if (strides[rank - 1] != 1) {\n     return op.emitOpError(\"Not Implemented: Stride on last dim is not 1\");\n   }\n-  auto last_idx = getIntConst(indices[rank - 1], /*silent=*/true);\n-  if (failed(last_idx)) {\n+  auto last_idx = getIntConst(indices[rank - 1]);\n+  if (!last_idx.has_value()) {\n     return op.emitOpError(\"Not Implemented: Dynamic index on last dim\");\n   } else if (last_idx.value() != 0) {\n     return op.emitOpError(\"Not Implemented: Index on last dim is not 0\");\n@@ -1975,7 +1970,7 @@ LogicalResult tpu_store_rule(RewriteContext &ctx, Operation &op,\n   tpu::StoreOp store_op = cast<tpu::StoreOp>(op);\n   FAILUREOR_ASSIGN_OR_RETURN(\n       const SmallVector<int64_t> indices,\n-      getIntConstsFromOperandRange(store_op.getIndices()));\n+      expectIntConstsFromOperandRange(store_op.getIndices()));\n   TPU_ASSERT_EQ_OP(indices.size(), 2);\n   if (indices[1] % ctx.target_shape[1] != 0) {\n     return op.emitOpError(\"Not implemented: Lane index is not a multiple of \")\n@@ -2143,15 +2138,14 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n     return op.emitOpError(\"Not implemented: unsupported layout for input\");\n   }\n   LayoutOffsets expected_offsets_out = layout_in.offsets();\n-  auto shift = getIntConst(amount, /*silent=*/true);\n-  const bool has_static_shift = succeeded(shift);\n+  auto shift = getIntConst(amount);\n   int rotated_tiled_dim = op.getDimension() - (op.getType().getRank() - 2);\n   bool has_padding_along_rotation =\n       (rotated_tiled_dim == 0 || rotated_tiled_dim == 1) &&\n       op.getType().getShape()[op.getDimension()] %\n               layout.tiling()[rotated_tiled_dim] !=\n           0;\n-  if (has_static_shift && has_padding_along_rotation) {\n+  if (shift.has_value() && has_padding_along_rotation) {\n     // We checked above that there are no implicit dims.\n     const int64_t dim_size = op.getType().getShape()[op.getDimension()];\n     // TODO(b/337384645): Currently we assume {0, 0} offsets in the input\n@@ -2173,7 +2167,7 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n   // TODO(b/411170715): Allow sublane rotation once the bug is fixed.\n   // TODO(b/337384645): Support non-zero stride.\n   if (has_padding_along_rotation &&\n-      (!has_static_shift ||\n+      (!shift.has_value() ||\n        (rotated_tiled_dim == 0 ||\n         (rotated_tiled_dim == 1 && op.getStride().value_or(0) != 0)))) {\n     return op.emitOpError(\"Not implemented: unsupported unaligned shape\");\n@@ -2200,19 +2194,19 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n         builder.getIntegerAttr(builder.getIndexType(), d));\n   };\n   auto modI = [&](const Value &v, unsigned d) -> Value {\n-    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+    if (auto cst = getIntConst(v)) {\n       return mlirI32Const(cst.value() % d);\n     }\n     return builder.create<arith::RemUIOp>(v, mlirI32Const(d));\n   };\n   auto divI = [&](const Value &v, unsigned d) -> Value {\n-    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+    if (auto cst = getIntConst(v)) {\n       return mlirI32Const(cst.value() / d);\n     }\n     return builder.create<arith::DivUIOp>(v, mlirI32Const(d));\n   };\n   auto addI = [&](const Value &v, unsigned d) -> Value {\n-    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+    if (auto cst = getIntConst(v)) {\n       return mlirI32Const(cst.value() + d);\n     }\n     return builder.create<arith::AddIOp>(v, mlirI32Const(d));\n@@ -2239,8 +2233,7 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n   auto getVmaskByPaddingEnd = [&](Value padding, int dim, int stride = 0) {\n     CHECK(dim == 0 || dim == 1);\n     Value padding_vreg;\n-    if (auto padding_cst = getIntConst(padding, /*silent=*/true);\n-        succeeded(padding_cst)) {\n+    if (auto padding_cst = getIntConst(padding)) {\n       CHECK_GE(padding_cst.value(), 0);\n       CHECK_LE(padding_cst.value(), ctx.target_shape[dim]);\n       padding_vreg = builder.create<arith::ConstantOp>(DenseElementsAttr::get(\n@@ -2269,8 +2262,7 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n   // and blend the data from contiguous vregs to emulate circular rotation.\n   auto rotateOnTilingDim = [&](const xla::Array<Value> &vregs,\n                                const Value &shift, int axis, int stride = 0) {\n-    if (auto shift_cst = getIntConst(shift, /*silent=*/true);\n-        succeeded(shift_cst)) {\n+    if (auto shift_cst = getIntConst(shift)) {\n       if (shift_cst.value() == 0 && stride == 0) {\n         return vregs;\n       }\n@@ -2395,8 +2387,7 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n     CHECK((tiling_dim != 1 && stride == 0) || (tiling_dim == 1 && stride >= 0));\n     SmallVector<xla::Array<Value>, 4> chunks;\n     // Handle rotation with static shift.\n-    if (auto shift_cst = getIntConst(shift, /*silent=*/true);\n-        succeeded(shift_cst)) {\n+    if (auto shift_cst = getIntConst(shift)) {\n       int64_t static_shift = shift_cst.value();\n       if (has_padding_along_rotation) {\n         return lazyRotate(vregs, static_shift, axis);\n@@ -2519,8 +2510,7 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n                                  vty.getDimSize(dim));\n         // After applying stride, we expect all shifts in a vreg are less or\n         // equal to the vreg's lane count for now.\n-        if (auto base_amount_cst = getIntConst(base_amount, /*silent=*/true);\n-            succeeded(base_amount_cst)) {\n+        if (auto base_amount_cst = getIntConst(base_amount)) {\n           int64_t static_base_amount = base_amount_cst.value();\n           auto max_shift_in_vreg = static_base_amount % ctx.target_shape[1] +\n                                    (ctx.target_shape[0] - 1) * stride;\n@@ -3163,7 +3153,7 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n   bool must_support_unaligned_dynamic_index = false;\n   if (load_op.getIndices().size() > 1) {\n     auto second_minor_idx = load_op.getIndices().take_back(2)[0];\n-    if (failed(getIntConst(second_minor_idx, /*silent=*/true)) &&\n+    if (!getIntConst(second_minor_idx).has_value() &&\n         !isGuaranteedDivisible(second_minor_idx, memref_tiling[0])) {\n       must_support_unaligned_dynamic_index = true;\n     }\n@@ -3196,7 +3186,7 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n   }\n \n   auto add_idx = [&](const Value &v, int64_t d) -> Value {\n-    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+    if (auto cst = getIntConst(v)) {\n       return IdxConst(cst.value() + d, builder, op.getLoc());\n     }\n     return builder.create<arith::AddIOp>(v, IdxConst(d, builder, op.getLoc()));\n@@ -4476,7 +4466,7 @@ LogicalResult vector_store_impl(RewriteContext &ctx, Op store_op,\n   bool must_support_unaligned_dynamic_index = false;\n   if (store_op.getIndices().size() > 1) {\n     auto second_minor_idx = store_op.getIndices().take_back(2)[0];\n-    if (failed(getIntConst(second_minor_idx, /*silent=*/true)) &&\n+    if (!getIntConst(second_minor_idx).has_value() &&\n         !isGuaranteedDivisible(second_minor_idx, memref_tiling[0])) {\n       must_support_unaligned_dynamic_index = true;\n     }\n@@ -4507,7 +4497,7 @@ LogicalResult vector_store_impl(RewriteContext &ctx, Op store_op,\n   }\n \n   auto add_idx = [&](const Value &v, int64_t d) -> Value {\n-    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+    if (auto cst = getIntConst(v)) {\n       return IdxConst(cst.value() + d, builder, op.getLoc());\n     }\n     return builder.create<arith::AddIOp>(v, IdxConst(d, builder, op.getLoc()));\ndiff --git a/jaxlib/mosaic/dialect/tpu/util.h b/jaxlib/mosaic/dialect/tpu/util.h\nindex 000cb4411e62..ac83d95b715e 100644\n--- a/jaxlib/mosaic/dialect/tpu/util.h\n+++ b/jaxlib/mosaic/dialect/tpu/util.h\n@@ -283,7 +283,6 @@ inline arith::ConstantOp I32Const(int32_t value, ArrayRef<int64_t> shape,\n                builder.getIntegerAttr(builder.getI32Type(), value)));\n }\n \n-// TODO(jevinjiang): consolidate this with getIntConst in apply-vector-layout.\n std::optional<int64_t> getIntConst(Value v);\n }  // namespace mlir::tpu\n \n\n```",
        "from_id": [
            "bythew3i",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28778 from jakevdp:mypy-version-bump\n\nPiperOrigin-RevId: 759668066",
        "output": "```diff\nCommit: 993deb852f531539d0b6376248039971a0af8c7c\nDate: 2025-05-16T17:32:30Z\nURL: https://github.com/jax-ml/jax/commit/993deb852f531539d0b6376248039971a0af8c7c\nFiles changed: 3\nAdditions: +4, Deletions: -4\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex a6697076404f..46deb8eb4879 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -36,7 +36,7 @@ repos:\n   - id: ruff\n \n - repo: https://github.com/pre-commit/mirrors-mypy\n-  rev: 'bbc3dc1f890007061f18f17e2334f216ea9e5df7'  # frozen: v1.14.1\n+  rev: 'f40886d54c729f533f864ed6ce584e920feb0af7'  # frozen: v1.15.0\n   hooks:\n   - id: mypy\n     files: (jax/|tests/typing_test\\.py)\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex aa9bfe9529ce..f26b4222b23b 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -264,7 +264,7 @@ def _get_batched_exception(self) -> BatchedError | None:\n       cur_effect = None\n       for error_effect, code in self._code.items():\n         if self._pred[error_effect][idx]:   # type: ignore\n-          if min_code is None or code[idx] < min_code:\n+          if min_code is None or code[idx] < min_code:  # type: ignore[index]\n             min_code = code[idx]   # type: ignore\n             cur_effect = error_effect\n \ndiff --git a/jax/_src/export/_export.py b/jax/_src/export/_export.py\nindex d5a328bb8e05..c0ca1e108590 100644\n--- a/jax/_src/export/_export.py\n+++ b/jax/_src/export/_export.py\n@@ -760,8 +760,8 @@ def export_sharding(s: LoweringSharding,\n         elif cur_mesh.shape_tuple != sharding.mesh.shape_tuple:\n           raise ValueError(\n               \"Mesh for all inputs/outputs should be equal. Got one mesh \"\n-              f\"{cur_mesh} on an array {cur_arg._aval} at \"\n-              f\"{shape_poly.args_kwargs_path_to_str(cur_k_path)} and another mesh: \"\n+              f\"{cur_mesh} on an array {cur_arg._aval} at \"  # type: ignore[union-attr]\n+              f\"{shape_poly.args_kwargs_path_to_str(cur_k_path)} and another mesh: \"  # type: ignore[arg-type]\n               f\"{sharding.mesh}' on a tensor {arg._aval} at \"\n               f\"{shape_poly.args_kwargs_path_to_str(k_path)}\")\n     if cur_mesh and isinstance(cur_mesh, mesh_lib.Mesh):\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Skip tests that need stdout capture when using pytest\n\nThe prints all go to the stdout captured by pytest instead of being\nintercepted by `jtu`.\n\nPiperOrigin-RevId: 759656514",
        "output": "```diff\nCommit: abda2872ce6562807012ad927534d06b8a515f9c\nDate: 2025-05-16T17:04:09Z\nURL: https://github.com/jax-ml/jax/commit/abda2872ce6562807012ad927534d06b8a515f9c\nFiles changed: 2\nAdditions: +6, Deletions: -0\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 03ded0ac446c..62f377f031a0 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -20,6 +20,7 @@\n import itertools\n import math\n import operator\n+import sys\n import re\n import unittest\n \n@@ -236,6 +237,8 @@ def setUp(self):\n \n   @contextlib.contextmanager\n   def capture_stdout(self):\n+    if \"pytest\" in sys.modules:\n+      self.skipTest(\"pytest interacts badly with GPU stdout capture\")\n     if mosaic_gpu_lib is None:\n       raise ValueError(\"Running tests but missing Mosaic GPU extension\")\n     with jtu.capture_stdout() as stdout:\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 40539463e007..dc35f03843f7 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -19,6 +19,7 @@\n import operator\n import os\n import re\n+import sys\n import tempfile\n from typing import ClassVar\n \n@@ -106,6 +107,8 @@ def pallas_call(self, *args, **kwargs):\n \n   @contextlib.contextmanager\n   def capture_stdout(self):\n+    if \"pytest\" in sys.modules:\n+      self.skipTest(\"pytest interacts badly with GPU stdout capture\")\n     if mosaic_gpu_lib is None:\n       raise ValueError(\"Running tests but missing Mosaic GPU extension\")\n     with jtu.capture_stdout() as stdout:\n\n```",
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas/Mosaic GPU] Fix `get_p` lowering to handle `RefUnion`s correctly.\n\nIn the case of transformed refs, it's possible that a transform ends up\nchanging the dtype of the reference considered. This typically happens when\nextracting an aliased ref out of a `RefUnion`, but it could also happen if\nwe were to handle `RefBitcast`s. As a result, querying the dtype of refs by\nquerying their `aval` is not a safe operation.\n\nPiperOrigin-RevId: 759624846",
        "output": "```diff\nCommit: 997978143b57e9e6a237cd16c596e4e74424c90b\nDate: 2025-05-16T15:28:43Z\nURL: https://github.com/jax-ml/jax/commit/997978143b57e9e6a237cd16c596e4e74424c90b\nFiles changed: 3\nAdditions: +70, Deletions: -27\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 9ead4f16c1a6..b611ea4c17f9 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -812,7 +812,6 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n         mesh=jax_mesh,\n     )\n     del runtime_smem, grouped_barriers, runtime_barriers\n-\n     _ = lower_jaxpr_to_mosaic_gpu(\n         module_ctx, launch_ctx, jaxpr, buffers_gmem, consts\n     )\n@@ -1288,8 +1287,7 @@ def _get_lowering_rule(ctx: LoweringRuleContext, x_ref, *leaves, tree):\n \n   if not isinstance(x_ref, ir.Value) and ir.MemRefType.isinstance(x_ref):\n     raise TypeError(f\"Can only load from references (got {x_ref}).\")\n-\n-  x_aval = ctx.avals_in[0]\n+  dtype = ctx.avals_out[0].dtype\n \n   transforms = jax.tree.unflatten(tree, leaves)\n   x_smem, transforms = _handle_transforms(\n@@ -1300,21 +1298,21 @@ def _get_lowering_rule(ctx: LoweringRuleContext, x_ref, *leaves, tree):\n     case (gpu_core.UnswizzleRef(swizzle), gpu_core.UntileRef(tiling)):\n       if tiling != (\n           8,\n-          (swizzle * 8) // pallas_utils.dtype_bitwidth(x_aval.dtype),\n+          (swizzle * 8) // pallas_utils.dtype_bitwidth(dtype),\n       ):\n         raise NotImplementedError(\"Tiling does not fit swizzle\")\n       return mgpu.FragmentedArray.load_tiled(\n-          x_smem, is_signed=mgpu_utils.is_signed(x_aval.dtype), swizzle=swizzle\n+          x_smem, is_signed=mgpu_utils.is_signed(dtype), swizzle=swizzle\n       )\n     case ():\n       # Handle scalar indexing.\n       if not ctx.avals_out[0].shape:\n-        is_signed = mgpu_utils.is_signed(x_aval.dtype)\n+        is_signed = mgpu_utils.is_signed(dtype)\n         val = memref_dialect.load(x_smem, [])\n         return mgpu.FragmentedArray.splat(val, shape=(), is_signed=is_signed)\n \n       return mgpu.FragmentedArray.load_strided(\n-          x_smem, is_signed=mgpu_utils.is_signed(x_aval.dtype)\n+          x_smem, is_signed=mgpu_utils.is_signed(dtype)\n       )\n     case _:\n       raise NotImplementedError(f\"Unsupported transforms: {transforms}\")\n@@ -1325,12 +1323,11 @@ def _get_lowering_rule_wg(ctx: LoweringRuleContext, x_smem, *leaves, tree):\n   if not isinstance(x_smem, ir.Value) and ir.MemRefType.isinstance(x_smem):\n     raise TypeError(f\"Can only load from references (got {x_smem}).\")\n \n-  x_aval = ctx.avals_in[0]\n-\n   transforms = jax.tree.unflatten(tree, leaves)\n   x_smem, transforms = _handle_transforms(\n       ctx, x_smem, transforms, allow_peer_refs=True\n   )\n+  mlir_dtype = ir.MemRefType(x_smem.type).element_type\n \n   if transforms:\n     raise NotImplementedError(\n@@ -1338,7 +1335,7 @@ def _get_lowering_rule_wg(ctx: LoweringRuleContext, x_smem, *leaves, tree):\n     )\n \n   shape = ctx.avals_out[0].shape\n-  ty = ir.VectorType.get(shape, mgpu_utils.dtype_to_ir_type(x_aval.dtype))\n+  ty = ir.VectorType.get(shape, mlir_dtype)\n   if shape:\n     zero_index = arith_dialect.constant(ir.IndexType.get(), 0)\n     indices = [zero_index for _ in range(len(shape))]\n@@ -1374,7 +1371,8 @@ def _swap_lowering_rule(\n   transforms = jax.tree.unflatten(tree, leaves)\n   transposed_value = value.layout == mgpu.WGMMA_TRANSPOSED_LAYOUT\n   x_smem, transforms = _handle_transforms(\n-      ctx, x_ref, transforms, handle_transposes=not transposed_value, allow_peer_refs=True\n+      ctx, x_ref, transforms, handle_transposes=not transposed_value,\n+      allow_peer_refs=True\n   )\n   mgpu.warpgroup_barrier()  # Make sure reads have completed before we write.\n   match transforms:\n@@ -1437,16 +1435,15 @@ def _swap_lowering_rule_wg(\n   if not ir.MemRefType.isinstance(x_smem.type):\n     raise TypeError(f\"Can only store to references (got {x_smem}).\")\n \n-  x_aval = ctx.avals_in[0]\n-\n   transforms = jax.tree.unflatten(tree, leaves)\n-  x_smem, transforms = _handle_transforms(ctx, x_smem, transforms, allow_peer_refs=True)\n+  x_smem, transforms = _handle_transforms(\n+      ctx, x_smem, transforms, allow_peer_refs=True)\n   if transforms:\n     raise NotImplementedError(\n         \"Transforms are not yet implemented for warpgroup semantics\"\n     )\n-\n-  ty = ir.VectorType.get(shape, mgpu_utils.dtype_to_ir_type(x_aval.dtype))\n+  x_mlir_dtype = ir.MemRefType(x_smem.type).element_type\n+  ty = ir.VectorType.get(shape, x_mlir_dtype)\n   if shape:\n     zero_index = arith_dialect.constant(ir.IndexType.get(), 0)\n     indices = [zero_index for _ in range(len(shape))]\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex f199b7b245c6..0e9319972949 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -85,7 +85,7 @@ def _load_p_lowering_rule(\n   if not isinstance(x_ref, ir.Value) or not ir.MemRefType.isinstance(x_ref.type):\n     raise TypeError(f\"Can only load from references (got {x_ref}).\")\n \n-  x_aval = ctx.avals_in[0]\n+  out_aval = ctx.avals_out[0]\n \n   transforms = jax.tree.unflatten(args_tree, leaves)\n   x_ref, transforms = lowering._handle_transforms(ctx, x_ref, transforms)\n@@ -93,10 +93,10 @@ def _load_p_lowering_rule(\n   if layout is not None:\n     layout = layout.to_mgpu()\n \n-  is_signed = mgpu_utils.is_signed(x_aval.dtype)\n+  is_signed = mgpu_utils.is_signed(out_aval.dtype)\n   match transforms:\n     case (gpu_core.UnswizzleRef(swizzle), gpu_core.UntileRef(tiling)):\n-      if tiling != (8, swizzle // x_aval.dtype.itemsize):\n+      if tiling != (8, swizzle // out_aval.dtype.itemsize):\n         raise NotImplementedError(\"Tiling does not fit swizzle\")\n       return mgpu.FragmentedArray.load_tiled(\n           x_ref,\n@@ -106,8 +106,8 @@ def _load_p_lowering_rule(\n       )\n     case ():\n       # Handle scalar indexing.\n-      if not ctx.avals_out[0].shape:\n-        is_signed = mgpu_utils.is_signed(x_aval.dtype)\n+      if not out_aval.shape:\n+        is_signed = mgpu_utils.is_signed(out_aval.dtype)\n         val = memref_dialect.load(x_ref, [])\n         return mgpu.FragmentedArray.splat(\n             val, shape=(), layout=layout, is_signed=is_signed\n@@ -259,7 +259,9 @@ def _copy_smem_to_gmem_lowering(\n   )\n   src_transforms = src_transforms_treedef.unflatten(flat_src_transforms)\n   dst_transforms = dst_transforms_treedef.unflatten(flat_dst_transforms)\n-  src, src_transforms = lowering._handle_transforms(ctx, src, src_transforms, handle_transposes=False)\n+  src, src_transforms = lowering._handle_transforms(\n+      ctx, src, src_transforms, handle_transposes=False\n+  )\n   copy_params = _extract_gmem_copy_params(dst_transforms) | _extract_smem_copy_params(src_transforms)\n   if ctx.module_ctx.lowering_semantics == mgpu.LoweringSemantics.Lane:\n     ctx.launch_ctx.async_copy(\n@@ -475,7 +477,9 @@ def _copy_gmem_to_smem_lowering(\n   )\n   src_transforms = src_transforms_treedef.unflatten(flat_src_transforms)\n   dst_transforms = dst_transforms_treedef.unflatten(flat_dst_transforms)\n-  dst, dst_transforms = lowering._handle_transforms(ctx, dst, dst_transforms, handle_transposes=False)\n+  dst, dst_transforms = lowering._handle_transforms(\n+      ctx, dst, dst_transforms, handle_transposes=False\n+  )\n   copy_params = _extract_smem_copy_params(dst_transforms) | _extract_gmem_copy_params(src_transforms)\n   barrier_indexer = _extract_barrier_indexer(\n       barrier_transforms_treedef.unflatten(flat_barrier_transforms)\n@@ -921,7 +925,6 @@ def _wgmma_lowering(\n     a_transforms_tree,\n     b_transforms_tree,\n ):\n-  _, a_aval, *_ = ctx.avals_in\n   lhs_swizzle: int | None = None\n   if a_transforms_tree is not None:\n     a_transforms_leaves, b_transforms_leaves = util.split_list(\n@@ -942,7 +945,8 @@ def _wgmma_lowering(\n         lhs_transpose = True\n       case _:\n         raise ValueError(f\"WGMMA lhs has unsupported transforms: {a_transforms}.\")\n-    swizzle_elems = lhs_swizzle // a_aval.dtype.itemsize\n+    a_mlir_dtype = ir.MemRefType(a.type).element_type\n+    swizzle_elems = lhs_swizzle // mgpu_utils.bytewidth(a_mlir_dtype)\n     if tiling != (8, swizzle_elems):\n       raise NotImplementedError(\"WGMMA lhs tiling does not fit swizzle\")\n   else:\n@@ -991,7 +995,8 @@ def _wgmma_lowering(\n       raise ValueError(f\"WGMMA rhs has unsupported transforms: {b_transforms}.\")\n \n   if lhs_swizzle is not None:\n-    swizzle_elems = rhs_swizzle // a_aval.dtype.itemsize\n+    b_mlir_dtype = ir.MemRefType(b.type).element_type\n+    swizzle_elems = rhs_swizzle // mgpu_utils.bytewidth(b_mlir_dtype)\n     if rhs_swizzle != lhs_swizzle:\n       raise NotImplementedError(\"WGMMA rhs swizzle must match lhs swizzle\")\n     if rhs_tiling != (8, swizzle_elems):\n@@ -1917,7 +1922,9 @@ def _inline_mgpu_lowering_rule(\n       assert transforms is None\n       continue\n     assert isinstance(aval, pallas_core.AbstractMemoryRef)\n-    a, user_transforms = lowering._handle_transforms(ctx, a, transforms, handle_transposes=False)\n+    a, user_transforms = lowering._handle_transforms(\n+        ctx, a, transforms, handle_transposes=False\n+    )\n     # Transforms that do not originate from a MemoryRefTransform are\n     # applied implicitly (eg by emit-pipeline) and therefore we do not\n     # expect the user to pass them to the type. The transforms not\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 9d259097f90a..40539463e007 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1671,6 +1671,45 @@ def unpack_i4_as_i8(x):\n     test_as_i8 = jax.lax.convert_element_type(kernel(x), new_dtype=jnp.int8)\n     np.testing.assert_array_equal(test_as_i8[:256], unpack_i4_as_i8(x))\n \n+  def test_smem_aliasing_works_for_quantization(self):\n+    self.skip_if_wg_semantics()\n+    shape = (64, 256)\n+    large_ty, small_ty = jnp.bfloat16, jnp.uint4\n+    large_swizzle = plgpu.SwizzleTransform(64 * jnp.finfo(large_ty).bits // 8)\n+    small_swizzle = plgpu.SwizzleTransform(64 * jnp.iinfo(small_ty).bits // 8)\n+    tiling = plgpu.TilingTransform((8, 64))\n+\n+    def kernel(x_gmem, o_gmem):\n+      return pl.run_scoped(\n+          functools.partial(scoped_kernel, x_gmem, o_gmem),\n+          plgpu.RefUnion(\n+              plgpu.SMEM(shape, large_ty, transforms=(tiling, large_swizzle)),\n+              plgpu.SMEM(shape, small_ty, transforms=(tiling, small_swizzle))\n+          ),\n+          plgpu.Barrier(1, num_barriers=1),\n+      )\n+\n+    def scoped_kernel(x_gmem, o_gmem, aliased_ref, barrier):\n+      ref_large_ty, ref_small_ty = aliased_ref\n+      plgpu.copy_gmem_to_smem(x_gmem, ref_small_ty, barrier=barrier)\n+      plgpu.barrier_wait(barrier)\n+      ref_large_ty[...] = ref_small_ty[...].astype(ref_large_ty.dtype) * 3\n+      plgpu.commit_smem()\n+      plgpu.copy_smem_to_gmem(ref_large_ty, o_gmem)\n+      plgpu.wait_smem_to_gmem(0, wait_read_only=True)\n+\n+    kernel_fn = self.pallas_call(\n+        kernel,\n+        in_specs=[pl.BlockSpec(memory_space=plgpu.GMEM)],\n+        out_specs=pl.BlockSpec(memory_space=plgpu.GMEM),\n+        out_shape=jax.ShapeDtypeStruct(shape, large_ty),\n+        grid=(1, 1),\n+    )\n+    key = jax.random.key(42)\n+    x = jax.random.randint(key, shape, 0, 4).astype(small_ty)\n+    expected = x * 3\n+    np.testing.assert_array_equal(kernel_fn(x), expected)\n+\n   def test_assigning_to_ref_union_raises(self):\n     @functools.partial(\n         self.pallas_call,\n\n```",
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Strip leading zeros from ML_WHEEL_GIT_HASH.\n\nThey end up being stripped by setuptools, which leads to a mismatch\nbetween expected and actual wheel names, which is fatal, as Bazel is\nexpecting only a particular name, not to mention other issues.\n\nhttps://peps.python.org/pep-0440/\n\nPiperOrigin-RevId: 759610274",
        "output": "```diff\nCommit: 6523226e701ee08ed9e65f28c74dd01c8a49728f\nDate: 2025-05-16T14:41:54Z\nURL: https://github.com/jax-ml/jax/commit/6523226e701ee08ed9e65f28c74dd01c8a49728f\nFiles changed: 1\nAdditions: +4, Deletions: -1\ndiff --git a/build/build.py b/build/build.py\nindex 4a7c745ce9d9..b65f7a49dd8f 100755\n--- a/build/build.py\n+++ b/build/build.py\n@@ -637,7 +637,10 @@ async def main():\n       if \"ML_WHEEL_BUILD_DATE\" in option:\n         wheel_build_date = option.split(\"=\")[-1].replace(\"-\", \"\")\n       if \"ML_WHEEL_GIT_HASH\" in option:\n-        wheel_git_hash = option.split(\"=\")[-1][:9]\n+        # Strip leading zeros as they end up being stripped by setuptools,\n+        # which leads to a mismatch between expected and actual wheel names\n+        # https://peps.python.org/pep-0440/\n+        wheel_git_hash = option.split(\"=\")[-1][:9].lstrip('0')\n \n   with open(\".jax_configure.bazelrc\", \"w\") as f:\n     jax_configure_options = utils.get_jax_configure_bazel_options(wheel_build_command_base.get_command_as_list(), args.use_new_wheel_build_rule)\n\n```",
        "from_id": [
            "belitskiy",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[mosaic_gpu] Added support for attaching source information to the PTX\n\nThe implementation currently forces O=0 due to a suspected bug in the NVPTX\nbackend.\n\nTo get source information\n\n* Set MOSAIC_GPU_LINE_INFO=1\n* Run with --jax_include_full_tracebacks_in_locations=true\n\nPiperOrigin-RevId: 759608368",
        "output": "```diff\nCommit: 73be65e1bc35f754db3b994f7cf4cd2556269646\nDate: 2025-05-16T14:35:24Z\nURL: https://github.com/jax-ml/jax/commit/73be65e1bc35f754db3b994f7cf4cd2556269646\nFiles changed: 2\nAdditions: +25, Deletions: -7\ndiff --git a/jaxlib/mosaic/gpu/BUILD b/jaxlib/mosaic/gpu/BUILD\nindex 115d0c47cc52..66f13bdac7f5 100644\n--- a/jaxlib/mosaic/gpu/BUILD\n+++ b/jaxlib/mosaic/gpu/BUILD\n@@ -149,8 +149,6 @@ cc_library(\n         \":nvshmem\",\n         \":passes\",\n         \":target\",\n-        \"//jaxlib/cuda:cuda_vendor\",\n-        \"//jaxlib/mosaic/dialect/gpu:mosaic_gpu\",\n         \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/cleanup\",\n@@ -200,11 +198,16 @@ cc_library(\n         \"@llvm-project//mlir:UBToLLVM\",\n         \"@llvm-project//mlir:VectorDialect\",\n         \"@llvm-project//mlir:VectorToLLVM\",\n-        \"@tsl//tsl/profiler/lib:traceme\",\n+        \"//jaxlib/cuda:cuda_vendor\",\n+        \"//jaxlib/mosaic/dialect/gpu:mosaic_gpu\",\n         \"@xla//xla/ffi\",\n         \"@xla//xla/ffi:ffi_api\",\n         \"@xla//xla/service:custom_call_status\",\n         \"@xla//xla/service:custom_call_target_registry\",\n+        \"@tsl//tsl/profiler/lib:traceme\",\n+        # TODO(slebedev): Remove once enable-line-info is merged into the upstream\n+        # ensure-debug-info-scope-on-llvm-func pass in MLIR.\n+        \"@triton//:TritonLLVMIR\",\n     ],\n     alwayslink = True,\n )\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 7c93d54aff9e..27175c3773e6 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -100,6 +100,7 @@ limitations under the License.\n #include \"xla/service/custom_call_status.h\"\n #include \"xla/service/custom_call_target_registry.h\"\n #include \"tsl/profiler/lib/traceme.h\"\n+#include \"triton/Target/LLVMIR/Passes.h\"\n \n namespace {\n \n@@ -174,8 +175,10 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n     mosaic::gpu::registerConvertGpuToLLVMPass();\n     mosaic::gpu::registerByvalInsertionPass();\n     mlir::arith::registerArithExpandOpsPass();\n+    mlir::registerLLVMDIScopePass();\n     return true;\n   });\n+  bool emit_line_info = getenv(\"MOSAIC_GPU_LINE_INFO\") != nullptr;\n   return mlir::parsePassPipeline(absl::StrCat(\n       R\"(\n       builtin.module(\n@@ -188,23 +191,35 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n         convert-scf-to-cf,\n         convert-nvvm-to-llvm,\n         expand-strided-metadata,\n-        nvvm-attach-target{O=3 chip=)\",\n-      sm, R\"( fast=false features=+)\", ptx_isa,\n+        nvvm-attach-target{)\",\n+      // TODO(slebedev): Always use O=3 once\n+      // https://github.com/llvm/llvm-project/pull/140146 is merged.\n+      emit_line_info ? \"O=0\" : \"O=3\", \" chip=\", sm, \" fast=false features=+\",\n+      ptx_isa,\n       R\"( ftz=false  module= triple=nvptx64-nvidia-cuda},\n         lower-affine,\n         convert-arith-to-llvm{index-bitwidth=0},\n         convert-index-to-llvm{index-bitwidth=64},\n         canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true},\n         cse,\n-        gpu.module(strip-debuginfo),\n+        )\",\n+      emit_line_info ? \"\" : \"gpu.module(strip-debuginfo),\",\n+      R\"(\n         gpu.module(convert-gpu-to-nvvm{has-redux=false index-bitwidth=64 use-bare-ptr-memref-call-conv=false}),\n         gpu.module(canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}),\n         gpu.module(cse),\n         gpu.module(mosaic-byval-insertion),\n         gpu.module(reconcile-unrealized-casts),\n         mosaic-convert-gpu-to-llvm,\n+        )\",\n+      // TODO(slebedev): Switch to the ensure-debug-info-scope-on-llvm-func\n+      // pass in MLIR once Triton upstreams its changes.\n+      emit_line_info ? \"enable-line-info,\" : \"\",\n+      R\"(\n         gpu-module-to-binary{format=)\" +\n-      mlir::gpu::stringifyCompilationTarget(target).str() + (!nvshmem_path.empty() ? R\"( l=)\" + nvshmem_path : \"\")  + R\"(},\n+          mlir::gpu::stringifyCompilationTarget(target).str() +\n+          (!nvshmem_path.empty() ? R\"( l=)\" + nvshmem_path : \"\") +\n+          (emit_line_info ? \"  opts=-lineinfo\" : \"\") + R\"(},\n         convert-math-to-llvm{approximate-log1p=true},\n         canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true},\n         cse,\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[JAX] Add ticks around input to clearify in the error message. Currently it looks like this.\n\n```\nValueError: Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:\n    * `in_specs` is a tuple of length 1 but inputs is a tuple of length 4, so the lengths do not match\n\n```\n\nPiperOrigin-RevId: 759499528",
        "output": "```diff\nCommit: 8f919a17f7f19555a911cc76ce4c9722a5228208\nDate: 2025-05-16T08:09:07Z\nURL: https://github.com/jax-ml/jax/commit/8f919a17f7f19555a911cc76ce4c9722a5228208\nFiles changed: 2\nAdditions: +3, Deletions: -3\ndiff --git a/jax/_src/pallas/core.py b/jax/_src/pallas/core.py\nindex f68393a7de54..709bb4640241 100644\n--- a/jax/_src/pallas/core.py\n+++ b/jax/_src/pallas/core.py\n@@ -1077,7 +1077,7 @@ def get_grid_mapping(\n     if in_specs_tree != in_tree:\n       raise ValueError(\n           pytreedef_mismatch_err_msg(\"`in_specs`\", in_specs_tree,\n-                                     \"inputs\", in_tree))\n+                                     \"`inputs`\", in_tree))\n   else:\n     flat_in_specs = [no_block_spec] * len(in_avals)\n \ndiff --git a/tests/pallas/pallas_test.py b/tests/pallas/pallas_test.py\nindex 725b3adb4388..1114153b16c2 100644\n--- a/tests/pallas/pallas_test.py\n+++ b/tests/pallas/pallas_test.py\n@@ -1081,10 +1081,10 @@ def test_pallas_call_in_specs_mismatch_inputs(self):\n                                    pl.BlockSpec((4,), lambda: 0)])\n     with self.assertRaisesRegex(\n         ValueError,\n-        re.compile(\"Pytree for `in_specs` and inputs do not match. \"\n+        re.compile(\"Pytree for `in_specs` and `inputs` do not match. \"\n                    \"There are 1 mismatches, including:\"\n                    \".* at \\\\[1\\\\], `in_specs` is a pytree leaf but \"\n-                   \"inputs is a.*\", re.DOTALL)):\n+                   \"`inputs` is a.*\", re.DOTALL)):\n       f(a, dict(a=a))\n \n   def test_pallas_call_index_map_wrong_number_of_arguments(self):\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28781 from mattjj:hijax\n\nPiperOrigin-RevId: 759336328",
        "output": "```diff\nCommit: 5b2f399c6befde3b23210613fe1c021da8886c50\nDate: 2025-05-15T22:55:27Z\nURL: https://github.com/jax-ml/jax/commit/5b2f399c6befde3b23210613fe1c021da8886c50\nFiles changed: 7\nAdditions: +434, Deletions: -35\ndiff --git a/jax/_src/ad_util.py b/jax/_src/ad_util.py\nindex 8cfd7b214338..4e9616e48375 100644\n--- a/jax/_src/ad_util.py\n+++ b/jax/_src/ad_util.py\n@@ -31,6 +31,9 @@\n map = safe_map\n \n def add_jaxvals(x: ArrayLike, y: ArrayLike) -> Array:\n+  ty = core.typeof(x)\n+  if hasattr(ty, 'vspace_add'):  # TODO(mattjj,dougalm): revise away hasattr\n+    return ty.vspace_add(x, y)\n   x, y = core.standard_insert_pvary(x, y)\n   return add_jaxvals_p.bind(x, y)\n \n@@ -48,6 +51,8 @@ def add_abstract(x, y):\n   return x\n \n def zeros_like_aval(aval: core.AbstractValue) -> Array:\n+  if hasattr(aval, 'vspace_zero'):  # TODO(mattjj,dougalm): revise away hasattr\n+    return aval.vspace_zero()\n   return aval_zeros_likers[type(aval)](aval)\n aval_zeros_likers: dict[type, Callable[[Any], Array]] = {}\n \ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 154ff5132a39..33802e494304 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -540,17 +540,18 @@ def _check_input_dtype_revderiv(name, holomorphic, allow_int, x):\n     if not dtypes.issubdtype(aval.dtype, np.complexfloating):\n       raise TypeError(f\"{name} with holomorphic=True requires inputs with complex dtype, \"\n                       f\"but got {aval.dtype.name}.\")\n-  if (dtypes.issubdtype(aval.dtype, dtypes.extended) or\n-      dtypes.issubdtype(aval.dtype, np.integer) or\n-      dtypes.issubdtype(aval.dtype, np.bool_)):\n-    if not allow_int:\n-      raise TypeError(f\"{name} requires real- or complex-valued inputs (input dtype \"\n-                      f\"that is a sub-dtype of np.inexact), but got {aval.dtype.name}. \"\n-                      \"If you want to use Boolean- or integer-valued inputs, use vjp \"\n-                      \"or set allow_int to True.\")\n-  elif not dtypes.issubdtype(aval.dtype, np.inexact):\n-    raise TypeError(f\"{name} requires numerical-valued inputs (input dtype that is a \"\n-                    f\"sub-dtype of np.bool_ or np.number), but got {aval.dtype.name}.\")\n+  if isinstance(aval, ShapedArray):\n+    if (dtypes.issubdtype(aval.dtype, dtypes.extended) or\n+        dtypes.issubdtype(aval.dtype, np.integer) or\n+        dtypes.issubdtype(aval.dtype, np.bool_)):\n+      if not allow_int:\n+        raise TypeError(f\"{name} requires real- or complex-valued inputs (input dtype \"\n+                        f\"that is a sub-dtype of np.inexact), but got {aval.dtype.name}. \"\n+                        \"If you want to use Boolean- or integer-valued inputs, use vjp \"\n+                        \"or set allow_int to True.\")\n+    elif not dtypes.issubdtype(aval.dtype, np.inexact):\n+      raise TypeError(f\"{name} requires numerical-valued inputs (input dtype that is a \"\n+                      f\"sub-dtype of np.bool_ or np.number), but got {aval.dtype.name}.\")\n _check_input_dtype_grad = partial(_check_input_dtype_revderiv, \"grad\")\n \n def _check_output_dtype_revderiv(name, holomorphic, x):\n@@ -1873,6 +1874,7 @@ def _jvp(fun: lu.WrappedFun, primals, tangents, has_aux=False):\n                     f\"structure; primals have tree structure {tree_def} whereas tangents have \"\n                     f\"tree structure {tree_def_2}.\")\n   for p, t in zip(ps_flat, ts_flat):\n+    if not isinstance(core.typeof(p), ShapedArray): continue\n     if core.primal_dtype_to_tangent_dtype(_dtype(p)) != _dtype(t):\n       raise TypeError(\"primal and tangent arguments to jax.jvp do not match; \"\n                       \"dtypes must be equal, or in case of int/bool primal dtype \"\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex c730e1c289ae..ab97c7ff1c2c 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,7 +88,7 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info']\n+               '_effects', '_debug_info', '_is_high', '_mut_types']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -96,6 +96,8 @@ class Jaxpr:\n   _eqns: list[JaxprEqn]\n   _effects: Effects\n   _debug_info: DebugInfo\n+  _is_high: bool\n+  _mut_types: dict[Var, Any]\n \n   @property\n   def constvars(self) -> list[Var]:\n@@ -121,6 +123,14 @@ def effects(self) -> Effects:\n   def debug_info(self) -> DebugInfo:\n     return self._debug_info\n \n+  @property\n+  def is_high(self) -> bool:\n+    return self._is_high\n+\n+  @property\n+  def mut_types(self) -> dict[Var, Any]:\n+    return self._mut_types\n+\n   def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                outvars: Sequence[Atom], eqns: Sequence[JaxprEqn],\n                effects: Effects = no_effects,\n@@ -128,6 +138,8 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # compatibility we have to allow calls when the debug_info\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n+               is_high: bool = False,\n+               mut_types: dict | None = None,\n                ):\n     \"\"\"\n     Args:\n@@ -152,6 +164,8 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # TODO(necula): re-enable these safety checks\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n+    self._is_high = is_high\n+    self._mut_types = mut_types or {}\n \n   def __str__(self):\n     return str(self.pretty_print())\n@@ -178,6 +192,8 @@ def replace(self, **kwargs):\n         eqns=kwargs.pop(\"eqns\", self.eqns),\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n+        is_high=kwargs.pop(\"is_high\", self.is_high),\n+        mut_types=kwargs.pop(\"mut_types\", self.mut_types),\n     )\n     if kwargs:\n       raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n@@ -517,7 +533,7 @@ def _true_bind(self, *args, **params):\n     for arg in args:\n       if isinstance(arg, Tracer) and not arg._trace.is_valid():\n         raise escaped_tracer_error(arg)\n-    # TODO: figure out how to handle function arguments\n+    # TODO: figure out how to handle function arguments for this assert\n     # assert (not config.enable_checks.value or\n     #         all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n \n@@ -525,6 +541,10 @@ def _true_bind(self, *args, **params):\n     # is called frequently and it's slightly faster to avoid using a context\n     # manager object.\n     prev_trace = trace_ctx.trace\n+\n+    if self.is_high(**params) and prev_trace.requires_low:\n+      return self.to_lojax(*args, **params)  # type: ignore\n+\n     trace_ctx.set_trace(eval_trace)\n     try:\n       return self.bind_with_trace(prev_trace, args, params)\n@@ -561,6 +581,9 @@ def abstract_eval(self, *args, **params):\n   def get_bind_params(self, params):\n     return [], params\n \n+  def is_high(self, **params) -> bool:\n+    return False\n+\n \n def _effect_free_abstract_eval(abstract_eval):\n   def abstract_eval_(*args, **kwargs):\n@@ -627,12 +650,13 @@ def check_avals_context_mesh(avals, prim_name):\n TracerType = TypeVar('TracerType', bound='Tracer')\n \n class Trace(Generic[TracerType]):\n-  __slots__ = (\"__weakref__\", \"_invalidated\", \"_weakref\")\n+  __slots__ = (\"__weakref__\", \"_invalidated\", \"_weakref\", \"requires_low\")\n \n   def __init__(self):\n     self._invalidated = False\n     # We frequently need a weakref to a trace, so let's precompute one.\n     self._weakref = weakref.ref(self)\n+    self.requires_low = True\n \n   def process_primitive(self, primitive, tracers, params):\n     raise NotImplementedError(\"must override\")\n@@ -1445,6 +1469,8 @@ def definitely_equal(x, y):\n \n class AbstractValue:\n   __slots__: list[str] = []\n+  is_high = False\n+  mutable = False\n \n   def to_tangent_aval(self):\n     raise NotImplementedError(\"must override\")\n@@ -1948,6 +1974,10 @@ def __init__(self, shape, dtype, weak_type=False, *, sharding=None,\n     self.sharding = get_sharding(sharding, self.shape)\n     self.vma = get_vma(vma, self.sharding.mesh)\n \n+  def lower_val(self, val): return [val]\n+  def raise_val(self, val): return val\n+  def lo_ty(self): return [self]\n+\n   def update(self, shape=None, dtype=None, weak_type=None, **kwargs):\n     if shape is None:\n       shape = self.shape\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 45705382efa0..090022c9b6a4 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -73,6 +73,7 @@ def jvp(fun: lu.WrappedFun, has_aux=False, instantiate=True,\n def jvpfun(f: Callable, instantiate, transform_stack, primals, tangents):\n   tag = core.TraceTag()\n   tangents = [Zero.from_primal_value(t) if not isinstance(t, Zero)\n+              and isinstance(core.typeof(t), core.ShapedArray)\n               and dtype(t) == float0 else t for t in tangents]\n   ctx = (source_info_util.transform_name_stack('jvp') if transform_stack\n          else contextlib.nullcontext())\n@@ -475,6 +476,7 @@ def __init__(self, parent_trace, tag):\n     super().__init__()\n     self.tag = tag\n     self.parent_trace = parent_trace\n+    self.requires_low = False\n \n   def to_primal_tangent_pair(self, val):\n     if isinstance(val, JVPTracer) and val._trace.tag is self.tag:\n@@ -606,7 +608,8 @@ def process_custom_transpose(self, prim, call, tracers, **params):\n     return map(partial(maybe_jvp_tracer, self), ps_out, ts_out)\n \n def maybe_jvp_tracer(trace, primal, tangent):\n-  if type(tangent) is Zero or dtype(tangent) == float0:\n+  if (type(tangent) is Zero or\n+      core.typeof(tangent) is core.ShapedArray and dtype(tangent) == float0):\n     return primal\n   else:\n     return JVPTracer(trace, primal, tangent)\n@@ -641,6 +644,7 @@ def _primal_tangent_shapes_match(primal, tangent):\n   if type(tangent) is not Zero:\n     primal_aval = get_aval(primal).strip_weak_type()\n     tangent_aval = get_aval(tangent).strip_weak_type()\n+    if not isinstance(primal_aval, core.ShapedArray): return  # TODO(mattjj,dougalm)\n     assert core.definitely_equal_shape(primal_aval.shape, tangent_aval.shape), (primal_aval.shape, tangent_aval.shape)\n     expected_tangent_dtype = core.primal_dtype_to_tangent_dtype(primal_aval.dtype)\n     assert expected_tangent_dtype == tangent_aval.dtype, (expected_tangent_dtype, tangent_aval.dtype)\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 5866b0c5f8eb..9e875f43d831 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -155,6 +155,7 @@ def __init__(self, parent_trace:Trace, name_stack: source_info_util.NameStack, t\n     self.name_stack = name_stack\n     self.tag = tag\n     self.parent_trace = parent_trace\n+    self.requires_low = False\n \n   def to_jaxpr_tracer(self, x):\n     if isinstance(x, JaxprTracer) and x._trace.tag is self.tag:\n@@ -899,9 +900,8 @@ def convert_envvars_to_constvars(jaxpr: Jaxpr, num_env_vars: int) -> Jaxpr:\n     raise NotImplementedError\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   env_vars, invars = split_list(jaxpr.invars, [num_env_vars])\n-  converted_jaxpr = Jaxpr(constvars=jaxpr.constvars + env_vars,\n-                          invars=invars, outvars=jaxpr.outvars, eqns=jaxpr.eqns,\n-                          effects=jaxpr.effects, debug_info=jaxpr.debug_info)\n+  converted_jaxpr = jaxpr.replace(constvars=jaxpr.constvars + env_vars,\n+                                  invars=invars)\n   config.enable_checks.value and core.check_jaxpr(converted_jaxpr)\n   return converted_jaxpr\n \n@@ -1173,6 +1173,7 @@ def has_effects(effects) -> bool:\n   out_unknowns = map(op.or_, out_unknowns, ensure_out_unknowns)\n   out_inst     = map(op.or_, out_inst,     ensure_out_inst)\n \n+\n   ins_known, _ = partition_list(in_unknowns, jaxpr.invars)\n   outs_known, _ = partition_list(out_unknowns, jaxpr.outvars)\n   ref_res_is_input = [r in ins_known for r in residual_refs]\n@@ -1181,8 +1182,14 @@ def has_effects(effects) -> bool:\n   known_outvars = [*outs_known, *residuals]\n   known_effects = make_jaxpr_effects(jaxpr.constvars, ins_known_and_ref_res,\n                                      known_outvars, known_eqns)\n-  jaxpr_known = Jaxpr(jaxpr.constvars, ins_known_and_ref_res, known_outvars,\n-                      known_eqns, known_effects, jaxpr.debug_info)\n+  known_mut, staged_mut, ins_known_ = {}, {}, set(ins_known)  # type: ignore\n+  for v, t in jaxpr.mut_types.items():\n+    [staged_mut, known_mut][v in ins_known_][v] = t\n+\n+  # TODO(mattjj,necula): debug info should be updated here\n+  jaxpr_known = jaxpr.replace(\n+      invars=ins_known_and_ref_res, outvars=known_outvars,\n+      eqns=known_eqns, effects=known_effects, mut_types=known_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_known)\n \n   _, ins_staged = partition_list(in_inst, jaxpr.invars)\n@@ -1190,9 +1197,10 @@ def has_effects(effects) -> bool:\n   staged_invars = [*residuals, *non_input_res_refs, *ins_staged]\n   staged_effects = make_jaxpr_effects(jaxpr.constvars, staged_invars,\n                                       outs_staged, staged_eqns)\n-  jaxpr_staged = Jaxpr(jaxpr.constvars, staged_invars,\n-                       outs_staged, staged_eqns, staged_effects,\n-                       jaxpr.debug_info)\n+  # TODO(mattjj,necula): debug info should be updated here\n+  jaxpr_staged = jaxpr.replace(\n+      invars=staged_invars, outvars=outs_staged, eqns=staged_eqns,\n+      effects=staged_effects, mut_types=staged_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_staged)\n \n   return (jaxpr_known, jaxpr_staged, out_unknowns, out_inst, len(residuals),\n@@ -1483,7 +1491,8 @@ def write(x: Atom, b: bool) -> None:\n       jaxpr.debug_info.traced_for, jaxpr.debug_info.func_src_info,\n       jaxpr.debug_info.filter_arg_names(used_inputs),\n       jaxpr.debug_info.filter_result_paths(used_outputs))\n-  new_jaxpr = Jaxpr(jaxpr.constvars, invars, outvars, eqns, jaxpr_effects, dbg)\n+  new_jaxpr = jaxpr.replace(invars=invars, outvars=outvars, eqns=eqns,\n+                            effects=jaxpr_effects, debug_info=dbg)\n   config.enable_checks.value and core.check_jaxpr(new_jaxpr)\n \n   return new_jaxpr, used_inputs\n@@ -1561,9 +1570,8 @@ def _move_binders_to_front(closed_jaxpr: ClosedJaxpr, to_move: tuple[bool, ...]\n   new_invars = _move_to_front(invars, to_move)\n   new_effs = _renumber_effects(\n       (*constvars, *new_invars), (*constvars, *invars), closed_jaxpr.jaxpr.effects)\n-  new_jaxpr = Jaxpr(constvars, new_invars, closed_jaxpr.jaxpr.outvars,\n-                    closed_jaxpr.jaxpr.eqns, new_effs,\n-                    closed_jaxpr.jaxpr.debug_info)\n+  new_jaxpr = closed_jaxpr.jaxpr.replace(\n+      constvars=constvars, invars=new_invars, effects=new_effs)\n   new_closed_jaxpr = core.ClosedJaxpr(new_jaxpr, closed_jaxpr.consts)\n   return new_closed_jaxpr\n \n@@ -1704,6 +1712,7 @@ class JaxprStackFrame:\n   attrs_inits: list\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n+  is_high: bool\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1718,6 +1727,7 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_inits = []\n     self.attrs_vars = []\n     self.debug_info = debug_info\n+    self.is_high = False\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1743,8 +1753,9 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n+    mut_types = {v: v.aval for v in invars if v.aval.mutable} if self.is_high else {}\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info)\n+                  debug_info, self.is_high, mut_types)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1831,8 +1842,9 @@ def vars(atom: Atom) -> list[Var]:\n class DynamicJaxprTrace(core.Trace):\n   __slots__ = (\"frame\", \"tag\")\n \n-  def __init__(self, debug_info: core.DebugInfo):\n+  def __init__(self, debug_info: core.DebugInfo, lower=False):\n     super().__init__()\n+    self.requires_low = lower\n     self.frame = JaxprStackFrame(debug_info)\n \n   def invalidate(self):\n@@ -2193,10 +2205,11 @@ def trace_to_jaxpr_dynamic(\n     in_avals: Sequence[AbstractValue],\n     *,\n     keep_inputs: list[bool] | None = None,\n+    lower: bool = False,\n ) -> tuple[Jaxpr, list[AbstractValue], list[Any],\n            list[tuple[PyTreeDef, PyTreeDef, tuple[Any, str, AttrKind]]]]:\n   keep_inputs = [True] * len(in_avals) if keep_inputs is None else keep_inputs\n-  trace = DynamicJaxprTrace(fun.debug_info)\n+  trace = DynamicJaxprTrace(fun.debug_info, lower=lower)\n   with core.ensure_no_leaks(trace), source_info_util.reset_name_stack():\n     source_info = source_info_util.current()\n     in_tracers = _input_type_to_tracers(\n@@ -2418,8 +2431,7 @@ def _add_implicit_outputs(jaxpr: Jaxpr) -> tuple[Jaxpr, OutputType]:\n   kept_outs = [False] * len(impl_outvars) + [True] * len(expl_outvars)\n   out_type = tuple(zip(out_avals, kept_outs))\n \n-  new_jaxpr = Jaxpr(jaxpr.constvars, jaxpr.invars, outvars, jaxpr.eqns,\n-                    jaxpr.effects, jaxpr.debug_info)\n+  new_jaxpr = jaxpr.replace(outvars=outvars)\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   return new_jaxpr, out_type\n \n@@ -2663,3 +2675,25 @@ def _linearize_of_pmap_hack(f: lu.WrappedFun, jaxpr, consts) -> tuple[Jaxpr, lis\n     _, jaxpr = f.f.closure\n     return convert_constvars_jaxpr(jaxpr), []\n   return jaxpr, consts\n+\n+\n+@weakref_lru_cache\n+def lower_jaxpr(hi_jaxpr):\n+  in_avals = [lo_ty for t in hi_jaxpr.in_avals for lo_ty in t.lo_ty()]\n+  f = lu.wrap_init(partial(lower_traceable, hi_jaxpr),\n+                   debug_info=hi_jaxpr.jaxpr.debug_info)\n+  lo_jaxpr, _, consts, () = trace_to_jaxpr_dynamic(f, in_avals, lower=True)\n+  return core.ClosedJaxpr(lo_jaxpr, consts)\n+\n+def lower_traceable(jaxpr, *lo_args):\n+  lo_args_ = iter(lo_args)\n+  hi_args = [t.raise_val(*it.islice(lo_args_, len(t.lo_ty())))\n+             for t in jaxpr.in_avals]\n+  assert (problem := next(lo_args_, None)) is None\n+  hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n+  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n+  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.mut_types.items()\n+              for lo_val in ty.get(hi_args[in_idx[v]])]\n+  lo_outs = [lo_val for t, hi_val in zip(jaxpr.out_avals, hi_outs)\n+             for lo_val in t.lower_val(hi_val)]\n+  return mut_outs + lo_outs\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex ca77b659a08d..5edd74fe74ef 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -20,7 +20,7 @@\n import dataclasses\n from functools import partial\n import inspect\n-import itertools\n+import itertools as it\n import logging\n import weakref\n from typing import NamedTuple, Any, Union, cast\n@@ -188,7 +188,8 @@ def _python_pjit_helper(fun: Callable, jit_info: PjitInfo, *args, **kwargs):\n     args_flat = [*init_states, *args_flat]\n \n   try:\n-    if core.trace_state_clean() and not config.debug_key_reuse.value:\n+    if (core.trace_state_clean() and not config.debug_key_reuse.value\n+        and not p.params['jaxpr'].jaxpr.is_high):\n       args_flat = map(core.full_lower, args_flat)\n       core.check_eval_args(args_flat)\n       out_flat, compiled, profiler = _pjit_call_impl_python(*args_flat, **p.params)\n@@ -1592,6 +1593,36 @@ def check_aval_layout_compatibility(\n pjit_p.multiple_results = True\n pjit_p.skip_canonicalization = True\n \n+def _is_high(jaxpr, **_) -> bool:\n+  return jaxpr.jaxpr.is_high\n+pjit_p.is_high = _is_high  # type: ignore\n+\n+def _to_lojax(*hi_args, jaxpr, out_shardings, out_layouts, **params):\n+  num_mut = [len(ty.lo_ty()) for ty in jaxpr.jaxpr.mut_types.values()]\n+  out_shardings = (UNSPECIFIED,) * sum(num_mut) + out_shardings\n+  out_layouts = (None,) * sum(num_mut) + out_layouts\n+\n+  lo_args = [lo_val for t, hi_val in zip(jaxpr.in_avals, hi_args)\n+             for lo_val in t.lower_val(hi_val)]\n+  lo_jaxpr = pe.lower_jaxpr(jaxpr)\n+  all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, out_shardings=out_shardings,\n+                         out_layouts=out_layouts, **params)\n+  out_mut, lo_outs = split_list(all_outs, [sum(num_mut)])\n+\n+  out_mut_ = iter(out_mut)\n+  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n+  for var, ty in jaxpr.jaxpr.mut_types.items():\n+    ty.set(hi_args[in_idx[var]], *it.islice(out_mut_, len(ty.lo_ty())))\n+  assert next(out_mut_, None) is None\n+\n+  lo_outs_ = iter(lo_outs)\n+  hi_outs = [t.raise_val(*it.islice(lo_outs_, len(t.lo_ty())))\n+             for t in jaxpr.out_avals]\n+  assert next(lo_outs_, None) is None\n+\n+  return hi_outs\n+pjit_p.to_lojax = _to_lojax\n+\n def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n   # If device or backend is set, return the default layout. This is because you\n   # can pass arrays on cpu (with untiled layouts) to jit with backend='tpu'\n@@ -3233,7 +3264,7 @@ def _flatten_boxes(dbg, args, kwargs):\n     return args, kwargs, []\n   box_data = []\n   id_first_occurrences = {}\n-  idxs = itertools.count()\n+  idxs = it.count()\n   def visit(x):\n     i = next(idxs)\n     if (isinstance(x, (Box, List)) and\ndiff --git a/tests/attrs_test.py b/tests/attrs_test.py\nindex 60a3753a7ba5..b6cef7fec4dc 100644\n--- a/tests/attrs_test.py\n+++ b/tests/attrs_test.py\n@@ -15,7 +15,9 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n+from functools import partial\n import itertools as it\n+import unittest\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -25,6 +27,10 @@\n import jax.numpy as jnp\n \n from jax._src import config\n+from jax._src import core\n+from jax._src import dtypes\n+from jax._src.interpreters import ad\n+from jax._src.interpreters import partial_eval as pe\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip, safe_map\n \n@@ -1326,5 +1332,292 @@ def f(lst1, lst2):\n       f(b, b)\n \n \n+class HiPrimitive(core.Primitive):\n+  def __init__(self, name):\n+    self.name = name\n+    ad.primitive_jvps[self] = self.jvp\n+    ad.primitive_transposes[self] = self.transpose\n+    pe.custom_staging_rules[self] = self.staging\n+\n+  def staging(self, trace, *args, **kwargs):\n+    trace.frame.is_high = True\n+    return trace.default_process_primitive(self, args, kwargs)\n+\n+  def is_high(self, **params):\n+    return True\n+\n+  def abstract_eval(self, *arg_avals, **params):\n+    assert False, \"must override\"\n+\n+  def to_lojax(self, *lotypes_wrapped_in_hitypes, **params):\n+    assert False, \"must override\"\n+\n+  def jvp(self, primals, tangents, **params):\n+    assert False, \"must override\"\n+\n+  def transpose(self, *args, **params):\n+    assert False  # TODO\n+\n+\n+class HijaxTest(jtu.JaxTestCase):\n+\n+  def test_custom_types_and_primitive(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+\n+    @dataclass(frozen=True)\n+    class MyArray:\n+      arr: jax.Array  # always f32\n+\n+    @dataclass(frozen=True)\n+    class MyTy(core.AbstractValue):\n+      mutable = False\n+\n+      def to_tangent_aval(self):\n+        return MyTy()\n+      def str_short(self, short_dtypes=False):\n+        return 'MyTy'\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n+        return [hi_val.arr]\n+      def raise_val(self, val) -> MyArray:\n+        return MyArray(val)\n+\n+      def __eq__(self, other): return isinstance(other, MyTy)\n+\n+      def vspace_zero(self):\n+        return MyArray(jnp.zeros((), 'float32'))\n+      def vspace_add(self, x, y):\n+        return add(x, y)\n+\n+      def strip_weak_type(self): return self\n+      def normalize(self): return self\n+    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n+\n+    class ToMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, lo_aval):\n+        return MyTy(), set()\n+\n+      def to_lojax(_, lo):\n+        return MyArray(lo)\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return to(x), to(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return from_(out_bar),\n+\n+    class FromMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_aval):\n+        return hi_aval.lo_ty()[0], set()\n+\n+      def to_lojax(_, hi):\n+        return hi.arr\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return from_(x), from_(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return to(out_bar),\n+\n+    def to(x): return to_p.bind(x)\n+    to_p = ToMy('to_my')\n+\n+    def from_(x): return from_p.bind(x)\n+    from_p = FromMy('from_my')\n+\n+    def mul(x, y): return mul_p.bind(x, y)\n+    def add(x, y): return add_p.bind(x, y)\n+\n+    class MyMul(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr * hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        (x, y), (x_dot, y_dot) = primals, tangents\n+        return mul(x, y), add(mul(x, y_dot), mul(x_dot, y))\n+\n+      def transpose(self, out_bar, x, y):\n+        assert ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)\n+        if ad.is_undefined_primal(x):\n+          return mul(out_bar, y), None\n+        else:\n+          return None, mul(x, out_bar)\n+\n+    class MyAdd(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr + hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(self, out_bar, x, y):\n+        return out_bar, out_bar\n+\n+    mul_p = MyMul('my_mul')\n+    add_p = MyAdd('my_add')\n+\n+\n+    @jax.jit\n+    def f(x):\n+      return to(from_(x))\n+\n+    # test basic to/from jit\n+    a = MyArray(jnp.ones(()))\n+    b = f(a)  # don't crash\n+    self.assertIsInstance(b, MyArray)\n+    self.assertAllClose(b.arr, jnp.ones(()))\n+\n+    # test basic to/from autodiff\n+    b, b_dot = jax.jvp(f, (a,), (a,))\n+    self.assertIsInstance(b, MyArray)\n+    self.assertIsInstance(b_dot, MyArray)\n+\n+    # test mul jit and backward pass\n+\n+    @jax.jit\n+    def f(x):\n+      return mul(x, x)\n+\n+    b, f_vjp = jax.vjp(f, a)\n+    self.assertIn('MyTy', str(f_vjp))\n+    a_grad, = f_vjp(b)\n+    self.assertIsInstance(a_grad, MyArray)\n+    self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n+\n+  def test_box_autodiff(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+    class BoxTy(core.AbstractValue):\n+      mutable = True\n+\n+      def to_tangent_aval(self):\n+        # NOTE not really used, for some reason we had to write it anyway\n+        return core.ShapedArray((), dtypes.float0)\n+\n+      def str_short(self, short_dtypes=False):\n+        return 'BoxTy'\n+\n+      def lower_val(self, box):\n+        return [box._val]\n+\n+      def raise_val(self, val):\n+        return Box(val)  # we're gonna mutate this\n+\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+\n+      def get(self, box):\n+        return [box._val]\n+\n+      def set(self, box, val):\n+        box._val = val\n+\n+    class Box:\n+      def __init__(self, val):\n+        self._val = val\n+      ty = BoxTy()\n+    core.pytype_aval_mappings[Box] = lambda b: b.ty\n+\n+\n+    class BoxSet(HiPrimitive):\n+      multiple_results = True\n+      def is_high(self) -> bool: return True\n+\n+      def abstract_eval(*_, **__):\n+        return [], set()\n+\n+      def to_lojax(_, box, val):\n+        box._val = val\n+        return []\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(_, *args):\n+        assert False  # TODO\n+    box_set_p = BoxSet('box_set')\n+\n+    class BoxGet(HiPrimitive):\n+      def is_high(self) -> bool: return True\n+\n+      def abstract_eval(*_, **__):\n+        return jnp.dtype('float32'), set()\n+\n+      def to_lojax(_, box):\n+        return box._val\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(_, *args):\n+        assert False  # TODO\n+    box_get_p = BoxGet('box_get')\n+\n+    class StashTangents(HiPrimitive):\n+      def is_high(self):\n+        return True\n+\n+      def abstract_eval(_, box_aval, x_aval):\n+        del box_aval\n+        return x_aval, set()\n+\n+      def to_lojax(_, box, x):\n+        assert False  # TODO\n+\n+      def jvp(_, primals, tangents):\n+        box, x = primals\n+        _, x_dot = tangents\n+        box_set(box, x_dot)\n+        return x, x_dot\n+\n+      def transpose(self, *args):\n+        assert False  # TODO\n+    stash_tangents_p = StashTangents('stash_tangents')\n+\n+    def box_set(box, val):\n+      box_set_p.bind(box, val)\n+\n+    def box_get(box):\n+      return box_get_p.bind(box)\n+\n+    def stash_tangents(box, x):\n+      return stash_tangents_p.bind(box, x)\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    box = Box(0.0)\n+    f(box, 1.)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+\n+    @jax.jit\n+    def f(box, x):\n+      x = stash_tangents(box, x)\n+      return x\n+\n+    box = Box(0.0)\n+    jax.jvp(partial(f, box), (3.,), (5.,))\n+    self.assertAllClose(box_get(box), 5.0, check_dtypes=False)\n+\n+\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[hijax] landing prototype pieces\n\nshouldn't affect existing behaviors, or trace time\n\nThe main implementation ideas:\n* each Trace is tagged with a `requires_low: bool`\n* each Jaxpr\n  * is tagged with an `is_high: bool`, default False but set True while tracing\n    if any hijax primitives are encountered\n  * includes an `mut_types: dict[Var, HijaxType]` indicating final types for\n    type-changing mutable hijax types\n* each AbstractValue is tagged by a `mutable: bool` which is read to populate\n  `mut_types`\n* each Primitive\n  * has an `is_high(**params) -> bool` method (depends on params for HOPs)\n  * has a `to_lojax(*args, **params)` method taking and returning\n    hijaxtypes-wrapping-lowtracers\n* in `Primitive.bind`, we check if `prim.is_high(**params) and\n  trace.requires_low`, and if so we call `prim.to_lojax`\n\nCo-authored-by: Dougal Maclaurin <dougalm@google.com>",
        "output": "```diff\nCommit: a043de0e7ee4cfe3c793b16b43d4626efdc55fc1\nDate: 2025-05-15T22:08:48Z\nURL: https://github.com/jax-ml/jax/commit/a043de0e7ee4cfe3c793b16b43d4626efdc55fc1\nFiles changed: 7\nAdditions: +434, Deletions: -35\ndiff --git a/jax/_src/ad_util.py b/jax/_src/ad_util.py\nindex 8cfd7b214338..4e9616e48375 100644\n--- a/jax/_src/ad_util.py\n+++ b/jax/_src/ad_util.py\n@@ -31,6 +31,9 @@\n map = safe_map\n \n def add_jaxvals(x: ArrayLike, y: ArrayLike) -> Array:\n+  ty = core.typeof(x)\n+  if hasattr(ty, 'vspace_add'):  # TODO(mattjj,dougalm): revise away hasattr\n+    return ty.vspace_add(x, y)\n   x, y = core.standard_insert_pvary(x, y)\n   return add_jaxvals_p.bind(x, y)\n \n@@ -48,6 +51,8 @@ def add_abstract(x, y):\n   return x\n \n def zeros_like_aval(aval: core.AbstractValue) -> Array:\n+  if hasattr(aval, 'vspace_zero'):  # TODO(mattjj,dougalm): revise away hasattr\n+    return aval.vspace_zero()\n   return aval_zeros_likers[type(aval)](aval)\n aval_zeros_likers: dict[type, Callable[[Any], Array]] = {}\n \ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 154ff5132a39..33802e494304 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -540,17 +540,18 @@ def _check_input_dtype_revderiv(name, holomorphic, allow_int, x):\n     if not dtypes.issubdtype(aval.dtype, np.complexfloating):\n       raise TypeError(f\"{name} with holomorphic=True requires inputs with complex dtype, \"\n                       f\"but got {aval.dtype.name}.\")\n-  if (dtypes.issubdtype(aval.dtype, dtypes.extended) or\n-      dtypes.issubdtype(aval.dtype, np.integer) or\n-      dtypes.issubdtype(aval.dtype, np.bool_)):\n-    if not allow_int:\n-      raise TypeError(f\"{name} requires real- or complex-valued inputs (input dtype \"\n-                      f\"that is a sub-dtype of np.inexact), but got {aval.dtype.name}. \"\n-                      \"If you want to use Boolean- or integer-valued inputs, use vjp \"\n-                      \"or set allow_int to True.\")\n-  elif not dtypes.issubdtype(aval.dtype, np.inexact):\n-    raise TypeError(f\"{name} requires numerical-valued inputs (input dtype that is a \"\n-                    f\"sub-dtype of np.bool_ or np.number), but got {aval.dtype.name}.\")\n+  if isinstance(aval, ShapedArray):\n+    if (dtypes.issubdtype(aval.dtype, dtypes.extended) or\n+        dtypes.issubdtype(aval.dtype, np.integer) or\n+        dtypes.issubdtype(aval.dtype, np.bool_)):\n+      if not allow_int:\n+        raise TypeError(f\"{name} requires real- or complex-valued inputs (input dtype \"\n+                        f\"that is a sub-dtype of np.inexact), but got {aval.dtype.name}. \"\n+                        \"If you want to use Boolean- or integer-valued inputs, use vjp \"\n+                        \"or set allow_int to True.\")\n+    elif not dtypes.issubdtype(aval.dtype, np.inexact):\n+      raise TypeError(f\"{name} requires numerical-valued inputs (input dtype that is a \"\n+                      f\"sub-dtype of np.bool_ or np.number), but got {aval.dtype.name}.\")\n _check_input_dtype_grad = partial(_check_input_dtype_revderiv, \"grad\")\n \n def _check_output_dtype_revderiv(name, holomorphic, x):\n@@ -1873,6 +1874,7 @@ def _jvp(fun: lu.WrappedFun, primals, tangents, has_aux=False):\n                     f\"structure; primals have tree structure {tree_def} whereas tangents have \"\n                     f\"tree structure {tree_def_2}.\")\n   for p, t in zip(ps_flat, ts_flat):\n+    if not isinstance(core.typeof(p), ShapedArray): continue\n     if core.primal_dtype_to_tangent_dtype(_dtype(p)) != _dtype(t):\n       raise TypeError(\"primal and tangent arguments to jax.jvp do not match; \"\n                       \"dtypes must be equal, or in case of int/bool primal dtype \"\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex e004263abe71..12e2763d7202 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,7 +88,7 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info']\n+               '_effects', '_debug_info', '_is_high', '_mut_types']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -96,6 +96,8 @@ class Jaxpr:\n   _eqns: list[JaxprEqn]\n   _effects: Effects\n   _debug_info: DebugInfo\n+  _is_high: bool\n+  _mut_types: dict[Var, Any]\n \n   @property\n   def constvars(self) -> list[Var]:\n@@ -121,6 +123,14 @@ def effects(self) -> Effects:\n   def debug_info(self) -> DebugInfo:\n     return self._debug_info\n \n+  @property\n+  def is_high(self) -> bool:\n+    return self._is_high\n+\n+  @property\n+  def mut_types(self) -> dict[Var, Any]:\n+    return self._mut_types\n+\n   def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                outvars: Sequence[Atom], eqns: Sequence[JaxprEqn],\n                effects: Effects = no_effects,\n@@ -128,6 +138,8 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # compatibility we have to allow calls when the debug_info\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n+               is_high: bool = False,\n+               mut_types: dict | None = None,\n                ):\n     \"\"\"\n     Args:\n@@ -152,6 +164,8 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # TODO(necula): re-enable these safety checks\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n+    self._is_high = is_high\n+    self._mut_types = mut_types or {}\n \n   def __str__(self):\n     return str(self.pretty_print())\n@@ -178,6 +192,8 @@ def replace(self, **kwargs):\n         eqns=kwargs.pop(\"eqns\", self.eqns),\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n+        is_high=kwargs.pop(\"is_high\", self.is_high),\n+        mut_types=kwargs.pop(\"mut_types\", self.mut_types),\n     )\n     if kwargs:\n       raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n@@ -517,7 +533,7 @@ def _true_bind(self, *args, **params):\n     for arg in args:\n       if isinstance(arg, Tracer) and not arg._trace.is_valid():\n         raise escaped_tracer_error(arg)\n-    # TODO: figure out how to handle function arguments\n+    # TODO: figure out how to handle function arguments for this assert\n     # assert (not config.enable_checks.value or\n     #         all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n \n@@ -525,6 +541,10 @@ def _true_bind(self, *args, **params):\n     # is called frequently and it's slightly faster to avoid using a context\n     # manager object.\n     prev_trace = trace_ctx.trace\n+\n+    if self.is_high(**params) and prev_trace.requires_low:\n+      return self.to_lojax(*args, **params)  # type: ignore\n+\n     trace_ctx.set_trace(eval_trace)\n     try:\n       return self.bind_with_trace(prev_trace, args, params)\n@@ -561,6 +581,9 @@ def abstract_eval(self, *args, **params):\n   def get_bind_params(self, params):\n     return [], params\n \n+  def is_high(self, **params) -> bool:\n+    return False\n+\n \n def _effect_free_abstract_eval(abstract_eval):\n   def abstract_eval_(*args, **kwargs):\n@@ -627,12 +650,13 @@ def check_avals_context_mesh(avals, prim_name):\n TracerType = TypeVar('TracerType', bound='Tracer')\n \n class Trace(Generic[TracerType]):\n-  __slots__ = (\"__weakref__\", \"_invalidated\", \"_weakref\")\n+  __slots__ = (\"__weakref__\", \"_invalidated\", \"_weakref\", \"requires_low\")\n \n   def __init__(self):\n     self._invalidated = False\n     # We frequently need a weakref to a trace, so let's precompute one.\n     self._weakref = weakref.ref(self)\n+    self.requires_low = True\n \n   def process_primitive(self, primitive, tracers, params):\n     raise NotImplementedError(\"must override\")\n@@ -1445,6 +1469,8 @@ def definitely_equal(x, y):\n \n class AbstractValue:\n   __slots__: list[str] = []\n+  is_high = False\n+  mutable = False\n \n   def to_tangent_aval(self):\n     raise NotImplementedError(\"must override\")\n@@ -1948,6 +1974,10 @@ def __init__(self, shape, dtype, weak_type=False, *, sharding=None,\n     self.sharding = get_sharding(sharding, self.shape)\n     self.vma = get_vma(vma, self.sharding.mesh)\n \n+  def lower_val(self, val): return [val]\n+  def raise_val(self, val): return val\n+  def lo_ty(self): return [self]\n+\n   def update(self, shape=None, dtype=None, weak_type=None, **kwargs):\n     if shape is None:\n       shape = self.shape\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 45705382efa0..090022c9b6a4 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -73,6 +73,7 @@ def jvp(fun: lu.WrappedFun, has_aux=False, instantiate=True,\n def jvpfun(f: Callable, instantiate, transform_stack, primals, tangents):\n   tag = core.TraceTag()\n   tangents = [Zero.from_primal_value(t) if not isinstance(t, Zero)\n+              and isinstance(core.typeof(t), core.ShapedArray)\n               and dtype(t) == float0 else t for t in tangents]\n   ctx = (source_info_util.transform_name_stack('jvp') if transform_stack\n          else contextlib.nullcontext())\n@@ -475,6 +476,7 @@ def __init__(self, parent_trace, tag):\n     super().__init__()\n     self.tag = tag\n     self.parent_trace = parent_trace\n+    self.requires_low = False\n \n   def to_primal_tangent_pair(self, val):\n     if isinstance(val, JVPTracer) and val._trace.tag is self.tag:\n@@ -606,7 +608,8 @@ def process_custom_transpose(self, prim, call, tracers, **params):\n     return map(partial(maybe_jvp_tracer, self), ps_out, ts_out)\n \n def maybe_jvp_tracer(trace, primal, tangent):\n-  if type(tangent) is Zero or dtype(tangent) == float0:\n+  if (type(tangent) is Zero or\n+      core.typeof(tangent) is core.ShapedArray and dtype(tangent) == float0):\n     return primal\n   else:\n     return JVPTracer(trace, primal, tangent)\n@@ -641,6 +644,7 @@ def _primal_tangent_shapes_match(primal, tangent):\n   if type(tangent) is not Zero:\n     primal_aval = get_aval(primal).strip_weak_type()\n     tangent_aval = get_aval(tangent).strip_weak_type()\n+    if not isinstance(primal_aval, core.ShapedArray): return  # TODO(mattjj,dougalm)\n     assert core.definitely_equal_shape(primal_aval.shape, tangent_aval.shape), (primal_aval.shape, tangent_aval.shape)\n     expected_tangent_dtype = core.primal_dtype_to_tangent_dtype(primal_aval.dtype)\n     assert expected_tangent_dtype == tangent_aval.dtype, (expected_tangent_dtype, tangent_aval.dtype)\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 5866b0c5f8eb..9e875f43d831 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -155,6 +155,7 @@ def __init__(self, parent_trace:Trace, name_stack: source_info_util.NameStack, t\n     self.name_stack = name_stack\n     self.tag = tag\n     self.parent_trace = parent_trace\n+    self.requires_low = False\n \n   def to_jaxpr_tracer(self, x):\n     if isinstance(x, JaxprTracer) and x._trace.tag is self.tag:\n@@ -899,9 +900,8 @@ def convert_envvars_to_constvars(jaxpr: Jaxpr, num_env_vars: int) -> Jaxpr:\n     raise NotImplementedError\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   env_vars, invars = split_list(jaxpr.invars, [num_env_vars])\n-  converted_jaxpr = Jaxpr(constvars=jaxpr.constvars + env_vars,\n-                          invars=invars, outvars=jaxpr.outvars, eqns=jaxpr.eqns,\n-                          effects=jaxpr.effects, debug_info=jaxpr.debug_info)\n+  converted_jaxpr = jaxpr.replace(constvars=jaxpr.constvars + env_vars,\n+                                  invars=invars)\n   config.enable_checks.value and core.check_jaxpr(converted_jaxpr)\n   return converted_jaxpr\n \n@@ -1173,6 +1173,7 @@ def has_effects(effects) -> bool:\n   out_unknowns = map(op.or_, out_unknowns, ensure_out_unknowns)\n   out_inst     = map(op.or_, out_inst,     ensure_out_inst)\n \n+\n   ins_known, _ = partition_list(in_unknowns, jaxpr.invars)\n   outs_known, _ = partition_list(out_unknowns, jaxpr.outvars)\n   ref_res_is_input = [r in ins_known for r in residual_refs]\n@@ -1181,8 +1182,14 @@ def has_effects(effects) -> bool:\n   known_outvars = [*outs_known, *residuals]\n   known_effects = make_jaxpr_effects(jaxpr.constvars, ins_known_and_ref_res,\n                                      known_outvars, known_eqns)\n-  jaxpr_known = Jaxpr(jaxpr.constvars, ins_known_and_ref_res, known_outvars,\n-                      known_eqns, known_effects, jaxpr.debug_info)\n+  known_mut, staged_mut, ins_known_ = {}, {}, set(ins_known)  # type: ignore\n+  for v, t in jaxpr.mut_types.items():\n+    [staged_mut, known_mut][v in ins_known_][v] = t\n+\n+  # TODO(mattjj,necula): debug info should be updated here\n+  jaxpr_known = jaxpr.replace(\n+      invars=ins_known_and_ref_res, outvars=known_outvars,\n+      eqns=known_eqns, effects=known_effects, mut_types=known_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_known)\n \n   _, ins_staged = partition_list(in_inst, jaxpr.invars)\n@@ -1190,9 +1197,10 @@ def has_effects(effects) -> bool:\n   staged_invars = [*residuals, *non_input_res_refs, *ins_staged]\n   staged_effects = make_jaxpr_effects(jaxpr.constvars, staged_invars,\n                                       outs_staged, staged_eqns)\n-  jaxpr_staged = Jaxpr(jaxpr.constvars, staged_invars,\n-                       outs_staged, staged_eqns, staged_effects,\n-                       jaxpr.debug_info)\n+  # TODO(mattjj,necula): debug info should be updated here\n+  jaxpr_staged = jaxpr.replace(\n+      invars=staged_invars, outvars=outs_staged, eqns=staged_eqns,\n+      effects=staged_effects, mut_types=staged_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_staged)\n \n   return (jaxpr_known, jaxpr_staged, out_unknowns, out_inst, len(residuals),\n@@ -1483,7 +1491,8 @@ def write(x: Atom, b: bool) -> None:\n       jaxpr.debug_info.traced_for, jaxpr.debug_info.func_src_info,\n       jaxpr.debug_info.filter_arg_names(used_inputs),\n       jaxpr.debug_info.filter_result_paths(used_outputs))\n-  new_jaxpr = Jaxpr(jaxpr.constvars, invars, outvars, eqns, jaxpr_effects, dbg)\n+  new_jaxpr = jaxpr.replace(invars=invars, outvars=outvars, eqns=eqns,\n+                            effects=jaxpr_effects, debug_info=dbg)\n   config.enable_checks.value and core.check_jaxpr(new_jaxpr)\n \n   return new_jaxpr, used_inputs\n@@ -1561,9 +1570,8 @@ def _move_binders_to_front(closed_jaxpr: ClosedJaxpr, to_move: tuple[bool, ...]\n   new_invars = _move_to_front(invars, to_move)\n   new_effs = _renumber_effects(\n       (*constvars, *new_invars), (*constvars, *invars), closed_jaxpr.jaxpr.effects)\n-  new_jaxpr = Jaxpr(constvars, new_invars, closed_jaxpr.jaxpr.outvars,\n-                    closed_jaxpr.jaxpr.eqns, new_effs,\n-                    closed_jaxpr.jaxpr.debug_info)\n+  new_jaxpr = closed_jaxpr.jaxpr.replace(\n+      constvars=constvars, invars=new_invars, effects=new_effs)\n   new_closed_jaxpr = core.ClosedJaxpr(new_jaxpr, closed_jaxpr.consts)\n   return new_closed_jaxpr\n \n@@ -1704,6 +1712,7 @@ class JaxprStackFrame:\n   attrs_inits: list\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n+  is_high: bool\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1718,6 +1727,7 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_inits = []\n     self.attrs_vars = []\n     self.debug_info = debug_info\n+    self.is_high = False\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1743,8 +1753,9 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n+    mut_types = {v: v.aval for v in invars if v.aval.mutable} if self.is_high else {}\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info)\n+                  debug_info, self.is_high, mut_types)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1831,8 +1842,9 @@ def vars(atom: Atom) -> list[Var]:\n class DynamicJaxprTrace(core.Trace):\n   __slots__ = (\"frame\", \"tag\")\n \n-  def __init__(self, debug_info: core.DebugInfo):\n+  def __init__(self, debug_info: core.DebugInfo, lower=False):\n     super().__init__()\n+    self.requires_low = lower\n     self.frame = JaxprStackFrame(debug_info)\n \n   def invalidate(self):\n@@ -2193,10 +2205,11 @@ def trace_to_jaxpr_dynamic(\n     in_avals: Sequence[AbstractValue],\n     *,\n     keep_inputs: list[bool] | None = None,\n+    lower: bool = False,\n ) -> tuple[Jaxpr, list[AbstractValue], list[Any],\n            list[tuple[PyTreeDef, PyTreeDef, tuple[Any, str, AttrKind]]]]:\n   keep_inputs = [True] * len(in_avals) if keep_inputs is None else keep_inputs\n-  trace = DynamicJaxprTrace(fun.debug_info)\n+  trace = DynamicJaxprTrace(fun.debug_info, lower=lower)\n   with core.ensure_no_leaks(trace), source_info_util.reset_name_stack():\n     source_info = source_info_util.current()\n     in_tracers = _input_type_to_tracers(\n@@ -2418,8 +2431,7 @@ def _add_implicit_outputs(jaxpr: Jaxpr) -> tuple[Jaxpr, OutputType]:\n   kept_outs = [False] * len(impl_outvars) + [True] * len(expl_outvars)\n   out_type = tuple(zip(out_avals, kept_outs))\n \n-  new_jaxpr = Jaxpr(jaxpr.constvars, jaxpr.invars, outvars, jaxpr.eqns,\n-                    jaxpr.effects, jaxpr.debug_info)\n+  new_jaxpr = jaxpr.replace(outvars=outvars)\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   return new_jaxpr, out_type\n \n@@ -2663,3 +2675,25 @@ def _linearize_of_pmap_hack(f: lu.WrappedFun, jaxpr, consts) -> tuple[Jaxpr, lis\n     _, jaxpr = f.f.closure\n     return convert_constvars_jaxpr(jaxpr), []\n   return jaxpr, consts\n+\n+\n+@weakref_lru_cache\n+def lower_jaxpr(hi_jaxpr):\n+  in_avals = [lo_ty for t in hi_jaxpr.in_avals for lo_ty in t.lo_ty()]\n+  f = lu.wrap_init(partial(lower_traceable, hi_jaxpr),\n+                   debug_info=hi_jaxpr.jaxpr.debug_info)\n+  lo_jaxpr, _, consts, () = trace_to_jaxpr_dynamic(f, in_avals, lower=True)\n+  return core.ClosedJaxpr(lo_jaxpr, consts)\n+\n+def lower_traceable(jaxpr, *lo_args):\n+  lo_args_ = iter(lo_args)\n+  hi_args = [t.raise_val(*it.islice(lo_args_, len(t.lo_ty())))\n+             for t in jaxpr.in_avals]\n+  assert (problem := next(lo_args_, None)) is None\n+  hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n+  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n+  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.mut_types.items()\n+              for lo_val in ty.get(hi_args[in_idx[v]])]\n+  lo_outs = [lo_val for t, hi_val in zip(jaxpr.out_avals, hi_outs)\n+             for lo_val in t.lower_val(hi_val)]\n+  return mut_outs + lo_outs\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex ca77b659a08d..5edd74fe74ef 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -20,7 +20,7 @@\n import dataclasses\n from functools import partial\n import inspect\n-import itertools\n+import itertools as it\n import logging\n import weakref\n from typing import NamedTuple, Any, Union, cast\n@@ -188,7 +188,8 @@ def _python_pjit_helper(fun: Callable, jit_info: PjitInfo, *args, **kwargs):\n     args_flat = [*init_states, *args_flat]\n \n   try:\n-    if core.trace_state_clean() and not config.debug_key_reuse.value:\n+    if (core.trace_state_clean() and not config.debug_key_reuse.value\n+        and not p.params['jaxpr'].jaxpr.is_high):\n       args_flat = map(core.full_lower, args_flat)\n       core.check_eval_args(args_flat)\n       out_flat, compiled, profiler = _pjit_call_impl_python(*args_flat, **p.params)\n@@ -1592,6 +1593,36 @@ def check_aval_layout_compatibility(\n pjit_p.multiple_results = True\n pjit_p.skip_canonicalization = True\n \n+def _is_high(jaxpr, **_) -> bool:\n+  return jaxpr.jaxpr.is_high\n+pjit_p.is_high = _is_high  # type: ignore\n+\n+def _to_lojax(*hi_args, jaxpr, out_shardings, out_layouts, **params):\n+  num_mut = [len(ty.lo_ty()) for ty in jaxpr.jaxpr.mut_types.values()]\n+  out_shardings = (UNSPECIFIED,) * sum(num_mut) + out_shardings\n+  out_layouts = (None,) * sum(num_mut) + out_layouts\n+\n+  lo_args = [lo_val for t, hi_val in zip(jaxpr.in_avals, hi_args)\n+             for lo_val in t.lower_val(hi_val)]\n+  lo_jaxpr = pe.lower_jaxpr(jaxpr)\n+  all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, out_shardings=out_shardings,\n+                         out_layouts=out_layouts, **params)\n+  out_mut, lo_outs = split_list(all_outs, [sum(num_mut)])\n+\n+  out_mut_ = iter(out_mut)\n+  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n+  for var, ty in jaxpr.jaxpr.mut_types.items():\n+    ty.set(hi_args[in_idx[var]], *it.islice(out_mut_, len(ty.lo_ty())))\n+  assert next(out_mut_, None) is None\n+\n+  lo_outs_ = iter(lo_outs)\n+  hi_outs = [t.raise_val(*it.islice(lo_outs_, len(t.lo_ty())))\n+             for t in jaxpr.out_avals]\n+  assert next(lo_outs_, None) is None\n+\n+  return hi_outs\n+pjit_p.to_lojax = _to_lojax\n+\n def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n   # If device or backend is set, return the default layout. This is because you\n   # can pass arrays on cpu (with untiled layouts) to jit with backend='tpu'\n@@ -3233,7 +3264,7 @@ def _flatten_boxes(dbg, args, kwargs):\n     return args, kwargs, []\n   box_data = []\n   id_first_occurrences = {}\n-  idxs = itertools.count()\n+  idxs = it.count()\n   def visit(x):\n     i = next(idxs)\n     if (isinstance(x, (Box, List)) and\ndiff --git a/tests/attrs_test.py b/tests/attrs_test.py\nindex 60a3753a7ba5..b6cef7fec4dc 100644\n--- a/tests/attrs_test.py\n+++ b/tests/attrs_test.py\n@@ -15,7 +15,9 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n+from functools import partial\n import itertools as it\n+import unittest\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -25,6 +27,10 @@\n import jax.numpy as jnp\n \n from jax._src import config\n+from jax._src import core\n+from jax._src import dtypes\n+from jax._src.interpreters import ad\n+from jax._src.interpreters import partial_eval as pe\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip, safe_map\n \n@@ -1326,5 +1332,292 @@ def f(lst1, lst2):\n       f(b, b)\n \n \n+class HiPrimitive(core.Primitive):\n+  def __init__(self, name):\n+    self.name = name\n+    ad.primitive_jvps[self] = self.jvp\n+    ad.primitive_transposes[self] = self.transpose\n+    pe.custom_staging_rules[self] = self.staging\n+\n+  def staging(self, trace, *args, **kwargs):\n+    trace.frame.is_high = True\n+    return trace.default_process_primitive(self, args, kwargs)\n+\n+  def is_high(self, **params):\n+    return True\n+\n+  def abstract_eval(self, *arg_avals, **params):\n+    assert False, \"must override\"\n+\n+  def to_lojax(self, *lotypes_wrapped_in_hitypes, **params):\n+    assert False, \"must override\"\n+\n+  def jvp(self, primals, tangents, **params):\n+    assert False, \"must override\"\n+\n+  def transpose(self, *args, **params):\n+    assert False  # TODO\n+\n+\n+class HijaxTest(jtu.JaxTestCase):\n+\n+  def test_custom_types_and_primitive(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+\n+    @dataclass(frozen=True)\n+    class MyArray:\n+      arr: jax.Array  # always f32\n+\n+    @dataclass(frozen=True)\n+    class MyTy(core.AbstractValue):\n+      mutable = False\n+\n+      def to_tangent_aval(self):\n+        return MyTy()\n+      def str_short(self, short_dtypes=False):\n+        return 'MyTy'\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n+        return [hi_val.arr]\n+      def raise_val(self, val) -> MyArray:\n+        return MyArray(val)\n+\n+      def __eq__(self, other): return isinstance(other, MyTy)\n+\n+      def vspace_zero(self):\n+        return MyArray(jnp.zeros((), 'float32'))\n+      def vspace_add(self, x, y):\n+        return add(x, y)\n+\n+      def strip_weak_type(self): return self\n+      def normalize(self): return self\n+    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n+\n+    class ToMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, lo_aval):\n+        return MyTy(), set()\n+\n+      def to_lojax(_, lo):\n+        return MyArray(lo)\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return to(x), to(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return from_(out_bar),\n+\n+    class FromMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_aval):\n+        return hi_aval.lo_ty()[0], set()\n+\n+      def to_lojax(_, hi):\n+        return hi.arr\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return from_(x), from_(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return to(out_bar),\n+\n+    def to(x): return to_p.bind(x)\n+    to_p = ToMy('to_my')\n+\n+    def from_(x): return from_p.bind(x)\n+    from_p = FromMy('from_my')\n+\n+    def mul(x, y): return mul_p.bind(x, y)\n+    def add(x, y): return add_p.bind(x, y)\n+\n+    class MyMul(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr * hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        (x, y), (x_dot, y_dot) = primals, tangents\n+        return mul(x, y), add(mul(x, y_dot), mul(x_dot, y))\n+\n+      def transpose(self, out_bar, x, y):\n+        assert ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)\n+        if ad.is_undefined_primal(x):\n+          return mul(out_bar, y), None\n+        else:\n+          return None, mul(x, out_bar)\n+\n+    class MyAdd(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr + hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(self, out_bar, x, y):\n+        return out_bar, out_bar\n+\n+    mul_p = MyMul('my_mul')\n+    add_p = MyAdd('my_add')\n+\n+\n+    @jax.jit\n+    def f(x):\n+      return to(from_(x))\n+\n+    # test basic to/from jit\n+    a = MyArray(jnp.ones(()))\n+    b = f(a)  # don't crash\n+    self.assertIsInstance(b, MyArray)\n+    self.assertAllClose(b.arr, jnp.ones(()))\n+\n+    # test basic to/from autodiff\n+    b, b_dot = jax.jvp(f, (a,), (a,))\n+    self.assertIsInstance(b, MyArray)\n+    self.assertIsInstance(b_dot, MyArray)\n+\n+    # test mul jit and backward pass\n+\n+    @jax.jit\n+    def f(x):\n+      return mul(x, x)\n+\n+    b, f_vjp = jax.vjp(f, a)\n+    self.assertIn('MyTy', str(f_vjp))\n+    a_grad, = f_vjp(b)\n+    self.assertIsInstance(a_grad, MyArray)\n+    self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n+\n+  def test_box_autodiff(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+    class BoxTy(core.AbstractValue):\n+      mutable = True\n+\n+      def to_tangent_aval(self):\n+        # NOTE not really used, for some reason we had to write it anyway\n+        return core.ShapedArray((), dtypes.float0)\n+\n+      def str_short(self, short_dtypes=False):\n+        return 'BoxTy'\n+\n+      def lower_val(self, box):\n+        return [box._val]\n+\n+      def raise_val(self, val):\n+        return Box(val)  # we're gonna mutate this\n+\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+\n+      def get(self, box):\n+        return [box._val]\n+\n+      def set(self, box, val):\n+        box._val = val\n+\n+    class Box:\n+      def __init__(self, val):\n+        self._val = val\n+      ty = BoxTy()\n+    core.pytype_aval_mappings[Box] = lambda b: b.ty\n+\n+\n+    class BoxSet(HiPrimitive):\n+      multiple_results = True\n+      def is_high(self) -> bool: return True\n+\n+      def abstract_eval(*_, **__):\n+        return [], set()\n+\n+      def to_lojax(_, box, val):\n+        box._val = val\n+        return []\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(_, *args):\n+        assert False  # TODO\n+    box_set_p = BoxSet('box_set')\n+\n+    class BoxGet(HiPrimitive):\n+      def is_high(self) -> bool: return True\n+\n+      def abstract_eval(*_, **__):\n+        return jnp.dtype('float32'), set()\n+\n+      def to_lojax(_, box):\n+        return box._val\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(_, *args):\n+        assert False  # TODO\n+    box_get_p = BoxGet('box_get')\n+\n+    class StashTangents(HiPrimitive):\n+      def is_high(self):\n+        return True\n+\n+      def abstract_eval(_, box_aval, x_aval):\n+        del box_aval\n+        return x_aval, set()\n+\n+      def to_lojax(_, box, x):\n+        assert False  # TODO\n+\n+      def jvp(_, primals, tangents):\n+        box, x = primals\n+        _, x_dot = tangents\n+        box_set(box, x_dot)\n+        return x, x_dot\n+\n+      def transpose(self, *args):\n+        assert False  # TODO\n+    stash_tangents_p = StashTangents('stash_tangents')\n+\n+    def box_set(box, val):\n+      box_set_p.bind(box, val)\n+\n+    def box_get(box):\n+      return box_get_p.bind(box)\n+\n+    def stash_tangents(box, x):\n+      return stash_tangents_p.bind(box, x)\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    box = Box(0.0)\n+    f(box, 1.)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+\n+    @jax.jit\n+    def f(box, x):\n+      x = stash_tangents(box, x)\n+      return x\n+\n+    box = Box(0.0)\n+    jax.jvp(partial(f, box), (3.,), (5.,))\n+    self.assertAllClose(box_get(box), 5.0, check_dtypes=False)\n+\n+\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "[pre-commit] update mypy to v1.15.0",
        "output": "```diff\nCommit: 8ff60c3422e3ac93e5c2de31e4f7ea7fbd90ba92\nDate: 2025-05-15T21:32:00Z\nURL: https://github.com/jax-ml/jax/commit/8ff60c3422e3ac93e5c2de31e4f7ea7fbd90ba92\nFiles changed: 3\nAdditions: +4, Deletions: -4\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex a6697076404f..46deb8eb4879 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -36,7 +36,7 @@ repos:\n   - id: ruff\n \n - repo: https://github.com/pre-commit/mirrors-mypy\n-  rev: 'bbc3dc1f890007061f18f17e2334f216ea9e5df7'  # frozen: v1.14.1\n+  rev: 'f40886d54c729f533f864ed6ce584e920feb0af7'  # frozen: v1.15.0\n   hooks:\n   - id: mypy\n     files: (jax/|tests/typing_test\\.py)\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex aa9bfe9529ce..f26b4222b23b 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -264,7 +264,7 @@ def _get_batched_exception(self) -> BatchedError | None:\n       cur_effect = None\n       for error_effect, code in self._code.items():\n         if self._pred[error_effect][idx]:   # type: ignore\n-          if min_code is None or code[idx] < min_code:\n+          if min_code is None or code[idx] < min_code:  # type: ignore[index]\n             min_code = code[idx]   # type: ignore\n             cur_effect = error_effect\n \ndiff --git a/jax/_src/export/_export.py b/jax/_src/export/_export.py\nindex d5a328bb8e05..c0ca1e108590 100644\n--- a/jax/_src/export/_export.py\n+++ b/jax/_src/export/_export.py\n@@ -760,8 +760,8 @@ def export_sharding(s: LoweringSharding,\n         elif cur_mesh.shape_tuple != sharding.mesh.shape_tuple:\n           raise ValueError(\n               \"Mesh for all inputs/outputs should be equal. Got one mesh \"\n-              f\"{cur_mesh} on an array {cur_arg._aval} at \"\n-              f\"{shape_poly.args_kwargs_path_to_str(cur_k_path)} and another mesh: \"\n+              f\"{cur_mesh} on an array {cur_arg._aval} at \"  # type: ignore[union-attr]\n+              f\"{shape_poly.args_kwargs_path_to_str(cur_k_path)} and another mesh: \"  # type: ignore[arg-type]\n               f\"{sharding.mesh}' on a tensor {arg._aval} at \"\n               f\"{shape_poly.args_kwargs_path_to_str(k_path)}\")\n     if cur_mesh and isinstance(cur_mesh, mesh_lib.Mesh):\n\n```",
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Merge pull request #28721 from jakevdp:fix-typestubs\n\nPiperOrigin-RevId: 759294851",
        "output": "```diff\nCommit: 3c55db3db8c27fe7d0d289192fe1088c598b936f\nDate: 2025-05-15T21:12:26Z\nURL: https://github.com/jax-ml/jax/commit/3c55db3db8c27fe7d0d289192fe1088c598b936f\nFiles changed: 1\nAdditions: +31, Deletions: -32\ndiff --git a/jax/numpy/__init__.pyi b/jax/numpy/__init__.pyi\nindex c52ce2628cda..4db407861f34 100644\n--- a/jax/numpy/__init__.pyi\n+++ b/jax/numpy/__init__.pyi\n@@ -253,7 +253,8 @@ def broadcast_shapes(*shapes: Sequence[int]) -> tuple[int, ...]: ...\n def broadcast_shapes(*shapes: Sequence[int | _core.Tracer]\n                      ) -> tuple[int | _core.Tracer, ...]: ...\n \n-def broadcast_to(array: ArrayLike, shape: DimSize | Shape) -> Array: ...\n+def broadcast_to(array: ArrayLike, shape: DimSize | Shape, *,\n+                 out_sharding: NamedSharding | P | None = None) -> Array: ...\n c_: _CClass\n can_cast = _np.can_cast\n def cbrt(x: ArrayLike, /) -> Array: ...\n@@ -267,6 +268,7 @@ def clip(\n     /,\n     min: ArrayLike | None = ...,\n     max: ArrayLike | None = ...,\n+    *,\n     a: ArrayLike | DeprecatedArg | None = ...,\n     a_min: ArrayLike | DeprecatedArg | None = ...,\n     a_max: ArrayLike | DeprecatedArg | None = ...\n@@ -278,7 +280,7 @@ complex128: Any\n complex64: Any\n complex_: Any\n complexfloating = _np.complexfloating\n-def compress(condition: ArrayLike, a: ArrayLike, axis: int | None = ...,\n+def compress(condition: ArrayLike, a: ArrayLike, axis: int | None = ..., *,\n              size: int | None = ..., fill_value: ArrayLike = ..., out: None = ...) -> Array: ...\n def concat(arrays: Sequence[ArrayLike], /, *, axis: int | None = 0) -> Array: ...\n def concatenate(\n@@ -314,9 +316,9 @@ def cross(\n     axis: int | None = ...,\n ) -> Array: ...\n csingle: Any\n-def cumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def cumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n             out: None = ...) -> Array: ...\n-def cumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def cumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n            out: None = ...) -> Array: ...\n def cumulative_prod(x: ArrayLike, /, *, axis: int | None = ...,\n                     dtype: DTypeLike | None = ...,\n@@ -371,7 +373,6 @@ def einsum(\n     optimize: str | builtins.bool | list[tuple[int, ...]] = ...,\n     precision: PrecisionLike = ...,\n     preferred_element_type: DTypeLike | None = ...,\n-    _use_xeinsum: builtins.bool = False,\n     _dot_general: Callable[..., Array] = ...,\n     out_sharding: NamedSharding | P | None = ...,\n ) -> Array: ...\n@@ -385,7 +386,6 @@ def einsum(\n     optimize: str | builtins.bool | list[tuple[int, ...]] = ...,\n     precision: PrecisionLike = ...,\n     preferred_element_type: DTypeLike | None = ...,\n-    _use_xeinsum: builtins.bool = False,\n     _dot_general: Callable[..., Array] = ...,\n     out_sharding: NamedSharding | P | None = ...,\n ) -> Array: ...\n@@ -397,7 +397,6 @@ def einsum(\n     optimize: str | builtins.bool | list[tuple[int, ...]] = ...,\n     precision: PrecisionLike = ...,\n     preferred_element_type: DTypeLike | None = ...,\n-    _use_xeinsum: builtins.bool = ...,\n     _dot_general: Callable[..., Array] = ...,\n     out_sharding: NamedSharding | P | None = ...,\n ) -> Array: ...\n@@ -422,7 +421,7 @@ def einsum_path(\n     optimize: str | builtins.bool | list[tuple[int, ...]] =  ...,\n ) -> tuple[list[tuple[int, ...]], Any]: ...\n \n-def empty(shape: Any, dtype: DTypeLike | None = ...,\n+def empty(shape: Any, dtype: DTypeLike | None = ..., *,\n           device: _Device | _Sharding | None = ...) -> Array: ...\n def empty_like(prototype: ArrayLike | DuckTypedArray,\n                dtype: DTypeLike | None = ...,\n@@ -579,17 +578,17 @@ def intersect1d(ar1: ArrayLike, ar2: ArrayLike, assume_unique: builtins.bool = .\n def invert(x: ArrayLike, /) -> Array: ...\n def isclose(a: ArrayLike, b: ArrayLike, rtol: ArrayLike = ...,\n             atol: ArrayLike = ..., equal_nan: builtins.bool = ...) -> Array: ...\n-def iscomplex(m: ArrayLike) -> Array: ...\n+def iscomplex(x: ArrayLike) -> Array: ...\n def iscomplexobj(x: Any) -> builtins.bool: ...\n def isdtype(dtype: DTypeLike, kind: DType | str | tuple[DType | str, ...]) -> builtins.bool: ...\n def isfinite(x: ArrayLike, /) -> Array: ...\n-def isin(element: ArrayLike, test_elements: ArrayLike,\n-         assume_unique: builtins.bool = ..., invert: builtins.bool = ..., method: str = ...) -> Array: ...\n+def isin(element: ArrayLike, test_elements: ArrayLike, assume_unique: builtins.bool = ...,\n+         invert: builtins.bool = ..., *, method: str = ...) -> Array: ...\n def isinf(x: ArrayLike, /) -> Array: ...\n def isnan(x: ArrayLike, /) -> Array: ...\n def isneginf(x: ArrayLike, /) -> Array: ...\n def isposinf(x: ArrayLike, /) -> Array: ...\n-def isreal(m: ArrayLike) -> Array: ...\n+def isreal(x: ArrayLike) -> Array: ...\n def isrealobj(x: Any) -> builtins.bool: ...\n def isscalar(element: Any) -> builtins.bool: ...\n def issubdtype(arg1: DTypeLike, arg2: DTypeLike) -> builtins.bool: ...\n@@ -644,7 +643,7 @@ def logspace(start: ArrayLike, stop: ArrayLike, num: int = ...,\n              endpoint: builtins.bool = ..., base: ArrayLike = ...,\n              dtype: DTypeLike | None = ..., axis: int = ...) -> Array: ...\n def mask_indices(\n-    n: int, mask_func: Callable, k: int = ...\n+    n: int, mask_func: Callable, k: int = ..., *, size: int | None = ...\n ) -> tuple[Array, ...]: ...\n def matmul(\n     a: ArrayLike, b: ArrayLike, *, precision: PrecisionLike = ...,\n@@ -655,7 +654,7 @@ def max(a: ArrayLike, axis: _Axis = ..., out: None = ...,\n         keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n         where: ArrayLike | None = ...) -> Array: ...\n def maximum(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n-def mean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def mean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n          out: None = ..., keepdims: builtins.bool = ..., *,\n          where: ArrayLike | None = ...) -> Array: ...\n def median(a: ArrayLike, axis: int | tuple[int, ...] | None = ...,\n@@ -689,14 +688,14 @@ def nanargmin(\n     out: None = ...,\n     keepdims: builtins.bool | None = ...,\n ) -> Array: ...\n-def nancumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def nancumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n                out: None = ...) -> Array: ...\n-def nancumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def nancumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n                out: None = ...) -> Array: ...\n def nanmax(a: ArrayLike, axis: _Axis = ..., out: None = ...,\n            keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n            where: ArrayLike | None = ...) -> Array: ...\n-def nanmean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nanmean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n             out: None = ...,\n             keepdims: builtins.bool = ...,\n             where: ArrayLike | None = ...) -> Array: ...\n@@ -710,21 +709,21 @@ def nanpercentile(a: ArrayLike, q: ArrayLike,\n                   axis: int | tuple[int, ...] | None = ...,\n                   out: None = ..., overwrite_input: builtins.bool = ..., method: str = ...,\n                   keepdims: builtins.bool = ..., *, interpolation: DeprecatedArg | str = ...) -> Array: ...\n-def nanprod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nanprod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n             out: None = ...,\n             keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n             where: ArrayLike | None = ...) -> Array: ...\n def nanquantile(a: ArrayLike, q: ArrayLike, axis: int | tuple[int, ...] | None = ...,\n                 out: None = ..., overwrite_input: builtins.bool = ..., method: str = ...,\n                 keepdims: builtins.bool = ..., *, interpolation: DeprecatedArg | str = ...) -> Array: ...\n-def nanstd(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ..., out: None = ...,\n-           ddof: int = ..., keepdims: builtins.bool = ...,\n+def nanstd(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n+           out: None = ..., ddof: int = ..., keepdims: builtins.bool = ...,\n            where: ArrayLike | None = ...) -> Array: ...\n-def nansum(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nansum(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n            out: None = ..., keepdims: builtins.bool = ...,\n            initial: ArrayLike | None = ...,\n            where: ArrayLike | None = ...) -> Array: ...\n-def nanvar(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nanvar(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n            out: None = ...,\n            ddof: int = 0, keepdims: builtins.bool = False,\n            where: ArrayLike | None = ...) -> Array: ...\n@@ -740,7 +739,7 @@ def not_equal(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n number = _np.number\n object_ = _np.object_\n ogrid: _Ogrid\n-def ones(shape: Any, dtype: DTypeLike | None = ...,\n+def ones(shape: Any, dtype: DTypeLike | None = ..., *,\n          device: _Device | _Sharding | None = ...) -> Array: ...\n def ones_like(a: ArrayLike | DuckTypedArray,\n               dtype: DTypeLike | None = ...,\n@@ -782,7 +781,7 @@ def positive(x: ArrayLike, /) -> Array: ...\n def pow(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n def power(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n printoptions = _np.printoptions\n-def prod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def prod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n          out: None = ..., keepdims: builtins.bool = ...,\n          initial: ArrayLike | None = ..., where: ArrayLike | None = ...,\n          promote_integers: builtins.bool = ...) -> Array: ...\n@@ -805,7 +804,6 @@ def ravel_multi_index(multi_index: Sequence[ArrayLike], dims: Sequence[int],\n                       mode: str = ..., order: str = ...) -> Array: ...\n def real(x: ArrayLike, /) -> Array: ...\n def reciprocal(x: ArrayLike, /) -> Array: ...\n-register_jax_array_methods: Any\n def remainder(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n def repeat(a: ArrayLike, repeats: ArrayLike, axis: int | None = ..., *,\n            total_repeat_length: int | None = ...,\n@@ -844,7 +842,8 @@ def setdiff1d(\n     size: int | None = ...,\n     fill_value: ArrayLike | None = ...,\n ) -> Array: ...\n-def setxor1d(ar1: ArrayLike, ar2: ArrayLike, assume_unique: builtins.bool = ...) -> Array: ...\n+def setxor1d(ar1: ArrayLike, ar2: ArrayLike, assume_unique: builtins.bool = ..., *,\n+             size: int | None = ..., fill_value: ArrayLike | None = ...) -> Array: ...\n def shape(a: ArrayLike | SupportsShape) -> tuple[int, ...]: ...\n def sign(x: ArrayLike, /) -> Array: ...\n def signbit(x: ArrayLike, /) -> Array: ...\n@@ -882,14 +881,14 @@ def stack(\n     out: None = ...,\n     dtype: DTypeLike | None = ...,\n ) -> Array: ...\n-def std(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def std(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n         out: None = ..., ddof: int = ..., keepdims: builtins.bool = ..., *,\n         where: ArrayLike | None = ..., correction: int | float | None = ...) -> Array: ...\n subtract: BinaryUfunc\n def sum(\n     a: ArrayLike,\n     axis: _Axis = ...,\n-    dtype: DTypeLike = ...,\n+    dtype: DTypeLike | None = ...,\n     out: None = ...,\n     keepdims: builtins.bool = ...,\n     initial: ArrayLike | None = ...,\n@@ -927,7 +926,7 @@ def transpose(a: ArrayLike, axes: Sequence[int] | None = ...) -> Array: ...\n def trapezoid(y: ArrayLike, x: ArrayLike | None = None, dx: ArrayLike = ...,\n               axis: int = ...) -> Array: ...\n def tri(\n-    N: int, M: int | None = ..., k: int = ..., dtype: DTypeLike = ...\n+    N: int, M: int | None = ..., k: int = ..., dtype: DTypeLike | None = ...\n ) -> Array: ...\n def tril(m: ArrayLike, k: int = ...) -> Array: ...\n def tril_indices(\n@@ -970,7 +969,7 @@ class _UniqueInverseResult(NamedTuple):\n def unique(ar: ArrayLike, return_index: builtins.bool = ..., return_inverse: builtins.bool = ...,\n            return_counts: builtins.bool = ..., axis: int | None = ...,\n            *, equal_nan: builtins.bool = ..., size: int | None = ...,\n-           fill_value: ArrayLike | None = ...\n+           fill_value: ArrayLike | None = ..., sorted: bool = ...,\n ): ...\n def unique_all(x: ArrayLike, /, *, size: int | None = ...,\n                fill_value: ArrayLike | None = ...) -> _UniqueAllResult: ...\n@@ -994,7 +993,7 @@ def unwrap(p: ArrayLike, discont: ArrayLike | None = ...,\n def vander(\n     x: ArrayLike, N: int | None = ..., increasing: builtins.bool = ...\n ) -> Array: ...\n-def var(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def var(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n         out: None = ..., ddof: int = ..., keepdims: builtins.bool = ..., *,\n         where: ArrayLike | None = ..., correction: int | float | None = ...) -> Array: ...\n def vdot(\n@@ -1029,7 +1028,7 @@ def where(condition: ArrayLike, x: ArrayLike | None = ...,\n           fill_value: None | ArrayLike | tuple[ArrayLike, ...] = ...\n           ) -> Array | tuple[Array, ...]: ...\n \n-def zeros(shape: Any, dtype: DTypeLike | None = ...,\n+def zeros(shape: Any, dtype: DTypeLike | None = ..., *,\n           device: _Device | _Sharding | None = ...) -> Array: ...\n def zeros_like(a: ArrayLike | DuckTypedArray,\n                dtype: DTypeLike | None = ...,\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28317 from yhtang:pr-add-k8s-docs\n\nPiperOrigin-RevId: 759252455",
        "output": "```diff\nCommit: 72f540eb7f3d0355eb43114a8770e93727860259\nDate: 2025-05-15T19:25:23Z\nURL: https://github.com/jax-ml/jax/commit/72f540eb7f3d0355eb43114a8770e93727860259\nFiles changed: 3\nAdditions: +89, Deletions: -2\ndiff --git a/.github/workflows/k8s.yaml b/.github/workflows/k8s.yaml\nindex 470a899a187e..5756b1afbbd2 100644\n--- a/.github/workflows/k8s.yaml\n+++ b/.github/workflows/k8s.yaml\n@@ -1,4 +1,4 @@\n-name: Distributed run using K8s Jobset\n+name: Multi-process run using K8s\n \n on:\n   push:\ndiff --git a/docs/multi_process.md b/docs/multi_process.md\nindex 8ecc51cc2557..f8c2566ca872 100644\n--- a/docs/multi_process.md\n+++ b/docs/multi_process.md\n@@ -307,6 +307,83 @@ what it prints:\n \n Woohoo, look at all those TPU cores!\n \n+### Kubernetes Example\n+\n+Running multi-controller JAX on a Kubernetes cluster is almost identical in spirit to the GPU and TPU examples above: every pod runs the same Python program, JAX discovers its peers, and the cluster behaves like one giant machine.\n+\n+1. **Container image** - start from a JAX-enabled image, e.g. one of the public JAX AI images on Google Artifact Registry ([TPU][google-artifact-tpu] / [GPU][google-artifact-gpu]) or NVIDIA ([NGC][nvidia-ngc] / [JAX-Toolbox][nvidia-jax-toolbox]).\n+\n+2. **Workload type** - use either a [JobSet][k8s-jobset] or an [indexed Job][k8s-indexed-job]. Each replica corresponds to one JAX process.\n+\n+3. **Service Account** - JAX needs permission to list the pods that belong to the job so that processes discover their peers. A minimal RBAC setup is provided in [examples/k8s/svc-acct.yaml][rbac-svc-acct].\n+\n+Below is a [minimal JobSet][minimal-jobset] that launches two replicas. Replace the placeholders - \n+image, GPU count, and any private registry secrets - with values that match your environment.\n+\n+```yaml\n+apiVersion: jobset.x-k8s.io/v1alpha2\n+kind: JobSet\n+metadata:\n+  name: jaxjob\n+spec:\n+  replicatedJobs:\n+  - name: workers\n+    template:\n+      spec:\n+        parallelism: 2\n+        completions: 2\n+        backoffLimit: 0\n+        template:\n+          spec:\n+            serviceAccountName: jax-job-sa  # kubectl apply -f svc-acct.yaml\n+            restartPolicy: Never\n+            imagePullSecrets:\n+              # https://k8s.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n+            - name: null\n+            containers:\n+            - name: main\n+              image: null  # e.g. ghcr.io/nvidia/jax:jax\n+              imagePullPolicy: Always\n+              resources:\n+                limits:\n+                  cpu: 1\n+                  # https://k8s.io/docs/tasks/manage-gpus/scheduling-gpus/\n+                  nvidia.com/gpu: null\n+              command: \n+                - python\n+              args:\n+                - -c\n+                - |\n+                  import jax\n+                  jax.distributed.initialize()\n+                  print(jax.devices())\n+                  print(jax.local_devices())\n+                  assert jax.process_count() > 1\n+                  assert len(jax.devices()) > len(jax.local_devices())\n+```\n+\n+Apply the manifest and watch the pods complete:\n+\n+```bash\n+$ kubectl apply -f example.yaml\n+$ kubectl get pods -l jobset.sigs.k8s.io/jobset-name=jaxjob\n+NAME                       READY   STATUS      RESTARTS   AGE\n+jaxjob-workers-0-0-xpx8l   0/1     Completed   0          8m32s\n+jaxjob-workers-0-1-ddkq8   0/1     Completed   0          8m32s\n+```\n+\n+When the job finishes, inspect the logs to confirm that every process saw all accelerators:\n+\n+```bash\n+$ kubectl logs -l jobset.sigs.k8s.io/jobset-name=jaxjob\n+[CudaDevice(id=0), CudaDevice(id=1)]\n+[CudaDevice(id=0)]\n+[CudaDevice(id=0), CudaDevice(id=1)]\n+[CudaDevice(id=1)]\n+```\n+\n+Every pod should have the same set of global devices and a different set of local devices. At this point, you can replace the inline script with your real JAX program.\n+\n Once the processes are set up, we can start building global {class}`jax.Array`s\n and running computations. The remaining Python code examples in this tutorial\n are meant to be run on all processes simultaneously, after running\n@@ -580,3 +657,11 @@ assert (np.all(\n [distributed_arrays]: https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html\n [gpu_machines]: https://cloud.google.com/compute/docs/gpus\n [unified_sharding]: https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html\n+[google-artifact-tpu]: https://console.cloud.google.com/artifacts/docker/cloud-tpu-images/us/jax-ai-image/tpu\n+[google-artifact-gpu]: https://console.cloud.google.com/artifacts/docker/deeplearning-images/us-central1/jax-ai-image/gpu\n+[nvidia-ngc]: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/jax\n+[nvidia-jax-toolbox]: https://github.com/NVIDIA/JAX-Toolbox\n+[k8s-jobset]: https://github.com/kubernetes-sigs/jobset\n+[k8s-indexed-job]: https://kubernetes.io/docs/concepts/workloads/controllers/job/#parallel-jobs\n+[rbac-svc-acct]: https://github.com/jax-ml/jax/blob/main/examples/k8s/svc-acct.yaml\n+[minimal-jobset]: https://github.com/jax-ml/jax/blob/main/examples/k8s/example.yaml\ndiff --git a/jax/_src/clusters/k8s_cluster.py b/jax/_src/clusters/k8s_cluster.py\nindex b40b39cade34..af1b7c020eed 100644\n--- a/jax/_src/clusters/k8s_cluster.py\n+++ b/jax/_src/clusters/k8s_cluster.py\n@@ -107,7 +107,9 @@ def _handle_api_exception(cls):\n           \"this job does not have the permission for pod introspection. Please \"\n           \"either grant the default SA permission to read pod info, or create a \"\n           \"dedicated service account with the permission and associated with \"\n-          \"the job. For more details, see <PLACERHOLDER_LINK>.\",\n+          \"the job. For an example on setting up the service account, see the \"\n+          \"example/k8s directory in the JAX repo. For more details, please refer to \"\n+          \"https://docs.jax.dev/en/latest/multi_process.html#kubernetes-example\",\n           width=80\n         ))\n       raise RuntimeError('\\n'.join(err_msg)) from e\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28757 from mattjj:custom-vjp-aval-mismatch-extra\n\nPiperOrigin-RevId: 759203972",
        "output": "```diff\nCommit: 053326344be62a35071f41bd45e9a3e9318d1f0b\nDate: 2025-05-15T17:30:45Z\nURL: https://github.com/jax-ml/jax/commit/053326344be62a35071f41bd45e9a3e9318d1f0b\nFiles changed: 5\nAdditions: +27, Deletions: -26\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex e004263abe71..c730e1c289ae 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -2777,6 +2777,26 @@ def typematch(t1: AbstractValue, t2: AbstractValue) -> bool:\n   else:\n     return False\n \n+def aval_mismatch_extra(a1: AbstractValue, a2: AbstractValue) -> str:\n+  assert not typematch(a1, a2)\n+  if isinstance(a1, ShapedArray) and isinstance(a2, ShapedArray):\n+    mismatches = []\n+    if a1.dtype != a2.dtype:\n+      mismatches.append('the dtypes do not match')\n+    if a1.shape != a2.shape:\n+      mismatches.append('the shapes do not match')\n+    if a1.vma != a2.vma:\n+      mismatches.append('the varying manual axes do not match')\n+    # TODO(yashkatariya,mattjj): add check for sharding-in-types mismatch\n+\n+    if len(mismatches) == 0:\n+      return ''\n+    elif len(mismatches) == 1:\n+      return ', so ' + mismatches[0]\n+    else:\n+      return ', so ' + ', '.join(mismatches[:-1]) + ', and ' + mismatches[-1]\n+  return ''\n+\n class JaxprTypeError(TypeError): pass\n \n custom_typechecks: dict[Primitive, Callable] = {}\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex 7b81c4e86889..9b28595e1835 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -917,7 +917,8 @@ def append(x, d):\n                \"shape/dtypes as the args tuple of the primal function, but at \"\n                f\"output{keystr(kp)} the bwd rule produced an output of \"\n                f\"shape/dtype {a_.str_short()} corresponding \"\n-               f\"to an input of shape/dtype {a.str_short()}.\")\n+               f\"to an input of shape/dtype {a.str_short()}\"\n+               f\"{core.aval_mismatch_extra(a, a_)}\")\n         raise ValueError(msg)\n       results.append(ct)\n   return results\ndiff --git a/jax/_src/lax/control_flow/common.py b/jax/_src/lax/control_flow/common.py\nindex 87dbcd8d3f32..b75cbf6ac708 100644\n--- a/jax/_src/lax/control_flow/common.py\n+++ b/jax/_src/lax/control_flow/common.py\n@@ -260,23 +260,3 @@ def _show_diff(array1, array2):\n def _avals_short(avals):\n   to_str = lambda aval: getattr(aval, 'str_short', partial(str, aval))()\n   return ' '.join(map(to_str, avals))\n-\n-def _aval_mismatch_extra(a1: core.AbstractValue, a2: core.AbstractValue) -> str:\n-  assert not core.typematch(a1, a2)\n-  if isinstance(a1, core.ShapedArray) and isinstance(a2, core.ShapedArray):\n-    mismatches = []\n-    if a1.dtype != a2.dtype:\n-      mismatches.append('the dtypes do not match')\n-    if a1.shape != a2.shape:\n-      mismatches.append('the shapes do not match')\n-    if a1.vma != a2.vma:\n-      mismatches.append('the varying manual axes do not match')\n-    # TODO(yashkatariya,mattjj): add check for sharding-in-types mismatch\n-\n-    if len(mismatches) == 0:\n-      return ''\n-    elif len(mismatches) == 1:\n-      return ', so ' + mismatches[0]\n-    else:\n-      return ', so ' + ', '.join(mismatches[:-1]) + ', and ' + mismatches[-1]\n-  return ''\ndiff --git a/jax/_src/lax/control_flow/conditionals.py b/jax/_src/lax/control_flow/conditionals.py\nindex d875989921d0..741636c47e31 100644\n--- a/jax/_src/lax/control_flow/conditionals.py\n+++ b/jax/_src/lax/control_flow/conditionals.py\n@@ -53,8 +53,8 @@\n import numpy as np\n \n from jax._src.lax.control_flow.common import (\n-    _avals_short, _typecheck_param, _aval_mismatch_extra,\n-    _initial_style_jaxprs_with_common_consts, _make_closed_jaxpr, _prune_zeros)\n+    _avals_short, _typecheck_param, _initial_style_jaxprs_with_common_consts,\n+    _make_closed_jaxpr, _prune_zeros)\n \n map, unsafe_map = safe_map, map\n \n@@ -351,7 +351,7 @@ def _check_branch_outputs(\n   if not all(map(core.typematch, out_avals1, out_avals2)):\n     diffs = [f'the output of {name1}{component(p)} has type {a1.str_short()}'\n              f' but the corresponding output of {name2} has type '\n-             f'{a2.str_short()}{_aval_mismatch_extra(a1, a2)}'\n+             f'{a2.str_short()}{core.aval_mismatch_extra(a1, a2)}'\n              for p, a1, a2 in zip(paths, out_avals1, out_avals2)\n              if not core.typematch(a1, a2)]\n     if len(diffs) == 0:\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex c85a23b6b199..7efe3294fdca 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -51,7 +51,7 @@\n from jax._src.lax.control_flow.common import (\n     _avals_short, _initial_style_jaxpr,\n     _initial_style_jaxpr_attrs, _make_closed_jaxpr_attrs, _prune_zeros,\n-    _typecheck_param, _aval_mismatch_extra)\n+    _typecheck_param)\n from jax._src.lax.other import logaddexp\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n@@ -478,7 +478,7 @@ def _check_carry_type(name, body_fun, in_carry, out_carry_tree, out_avals):\n   if not all(_map(core.typematch, in_avals, out_avals)):\n     diffs = [f'{component(path)} has type {in_aval.str_short()}'\n              ' but the corresponding output carry component has type '\n-             f'{out_aval.str_short()}{_aval_mismatch_extra(in_aval, out_aval)}'\n+             f'{out_aval.str_short()}{core.aval_mismatch_extra(in_aval, out_aval)}'\n              for path, in_aval, out_aval in zip(paths, in_avals, out_avals)\n              if not core.typematch(in_aval, out_aval)]\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "improve error message with when custom_vjp bwd rule produces wrong shape/dtype",
        "output": "```diff\nCommit: 0984dc8bbcb9406a86111c700cb7cbbb3faedbe8\nDate: 2025-05-15T16:46:32Z\nURL: https://github.com/jax-ml/jax/commit/0984dc8bbcb9406a86111c700cb7cbbb3faedbe8\nFiles changed: 5\nAdditions: +27, Deletions: -26\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex e004263abe71..c730e1c289ae 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -2777,6 +2777,26 @@ def typematch(t1: AbstractValue, t2: AbstractValue) -> bool:\n   else:\n     return False\n \n+def aval_mismatch_extra(a1: AbstractValue, a2: AbstractValue) -> str:\n+  assert not typematch(a1, a2)\n+  if isinstance(a1, ShapedArray) and isinstance(a2, ShapedArray):\n+    mismatches = []\n+    if a1.dtype != a2.dtype:\n+      mismatches.append('the dtypes do not match')\n+    if a1.shape != a2.shape:\n+      mismatches.append('the shapes do not match')\n+    if a1.vma != a2.vma:\n+      mismatches.append('the varying manual axes do not match')\n+    # TODO(yashkatariya,mattjj): add check for sharding-in-types mismatch\n+\n+    if len(mismatches) == 0:\n+      return ''\n+    elif len(mismatches) == 1:\n+      return ', so ' + mismatches[0]\n+    else:\n+      return ', so ' + ', '.join(mismatches[:-1]) + ', and ' + mismatches[-1]\n+  return ''\n+\n class JaxprTypeError(TypeError): pass\n \n custom_typechecks: dict[Primitive, Callable] = {}\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex 7b81c4e86889..9b28595e1835 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -917,7 +917,8 @@ def append(x, d):\n                \"shape/dtypes as the args tuple of the primal function, but at \"\n                f\"output{keystr(kp)} the bwd rule produced an output of \"\n                f\"shape/dtype {a_.str_short()} corresponding \"\n-               f\"to an input of shape/dtype {a.str_short()}.\")\n+               f\"to an input of shape/dtype {a.str_short()}\"\n+               f\"{core.aval_mismatch_extra(a, a_)}\")\n         raise ValueError(msg)\n       results.append(ct)\n   return results\ndiff --git a/jax/_src/lax/control_flow/common.py b/jax/_src/lax/control_flow/common.py\nindex 87dbcd8d3f32..b75cbf6ac708 100644\n--- a/jax/_src/lax/control_flow/common.py\n+++ b/jax/_src/lax/control_flow/common.py\n@@ -260,23 +260,3 @@ def _show_diff(array1, array2):\n def _avals_short(avals):\n   to_str = lambda aval: getattr(aval, 'str_short', partial(str, aval))()\n   return ' '.join(map(to_str, avals))\n-\n-def _aval_mismatch_extra(a1: core.AbstractValue, a2: core.AbstractValue) -> str:\n-  assert not core.typematch(a1, a2)\n-  if isinstance(a1, core.ShapedArray) and isinstance(a2, core.ShapedArray):\n-    mismatches = []\n-    if a1.dtype != a2.dtype:\n-      mismatches.append('the dtypes do not match')\n-    if a1.shape != a2.shape:\n-      mismatches.append('the shapes do not match')\n-    if a1.vma != a2.vma:\n-      mismatches.append('the varying manual axes do not match')\n-    # TODO(yashkatariya,mattjj): add check for sharding-in-types mismatch\n-\n-    if len(mismatches) == 0:\n-      return ''\n-    elif len(mismatches) == 1:\n-      return ', so ' + mismatches[0]\n-    else:\n-      return ', so ' + ', '.join(mismatches[:-1]) + ', and ' + mismatches[-1]\n-  return ''\ndiff --git a/jax/_src/lax/control_flow/conditionals.py b/jax/_src/lax/control_flow/conditionals.py\nindex d875989921d0..741636c47e31 100644\n--- a/jax/_src/lax/control_flow/conditionals.py\n+++ b/jax/_src/lax/control_flow/conditionals.py\n@@ -53,8 +53,8 @@\n import numpy as np\n \n from jax._src.lax.control_flow.common import (\n-    _avals_short, _typecheck_param, _aval_mismatch_extra,\n-    _initial_style_jaxprs_with_common_consts, _make_closed_jaxpr, _prune_zeros)\n+    _avals_short, _typecheck_param, _initial_style_jaxprs_with_common_consts,\n+    _make_closed_jaxpr, _prune_zeros)\n \n map, unsafe_map = safe_map, map\n \n@@ -351,7 +351,7 @@ def _check_branch_outputs(\n   if not all(map(core.typematch, out_avals1, out_avals2)):\n     diffs = [f'the output of {name1}{component(p)} has type {a1.str_short()}'\n              f' but the corresponding output of {name2} has type '\n-             f'{a2.str_short()}{_aval_mismatch_extra(a1, a2)}'\n+             f'{a2.str_short()}{core.aval_mismatch_extra(a1, a2)}'\n              for p, a1, a2 in zip(paths, out_avals1, out_avals2)\n              if not core.typematch(a1, a2)]\n     if len(diffs) == 0:\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex c85a23b6b199..7efe3294fdca 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -51,7 +51,7 @@\n from jax._src.lax.control_flow.common import (\n     _avals_short, _initial_style_jaxpr,\n     _initial_style_jaxpr_attrs, _make_closed_jaxpr_attrs, _prune_zeros,\n-    _typecheck_param, _aval_mismatch_extra)\n+    _typecheck_param)\n from jax._src.lax.other import logaddexp\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n@@ -478,7 +478,7 @@ def _check_carry_type(name, body_fun, in_carry, out_carry_tree, out_avals):\n   if not all(_map(core.typematch, in_avals, out_avals)):\n     diffs = [f'{component(path)} has type {in_aval.str_short()}'\n              ' but the corresponding output carry component has type '\n-             f'{out_aval.str_short()}{_aval_mismatch_extra(in_aval, out_aval)}'\n+             f'{out_aval.str_short()}{core.aval_mismatch_extra(in_aval, out_aval)}'\n              for path, in_aval, out_aval in zip(paths, in_avals, out_avals)\n              if not core.typematch(in_aval, out_aval)]\n \n\n```",
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "#sdy Fix incorrect sharding on a token during a callback.\n\nThe \"add a token\" part of the `callback` primitive's MLIR lowering was incorrectly adding a ranked sharding by using the sharding of a ranked tensor. So instead create an unranked sharding explicitly\n\nPiperOrigin-RevId: 759135477",
        "output": "```diff\nCommit: afdf51d797da6b851b80d40fda856040e75c641e\nDate: 2025-05-15T14:15:14Z\nURL: https://github.com/jax-ml/jax/commit/afdf51d797da6b851b80d40fda856040e75c641e\nFiles changed: 1\nAdditions: +13, Deletions: -10\ndiff --git a/jax/_src/callback.py b/jax/_src/callback.py\nindex 7fcccac14950..5b5ec593a550 100644\n--- a/jax/_src/callback.py\n+++ b/jax/_src/callback.py\n@@ -40,7 +40,7 @@\n from jax._src.lib import xla_client as xc\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n-from jax._src.sharding_impls import SdyArray, SdyArrayList, SingleDeviceSharding\n+from jax._src.sharding_impls import SdyArray, SdyArrayList, SdyDim, SingleDeviceSharding\n from jax._src.typing import DeprecatedArg\n import numpy as np\n \n@@ -157,11 +157,11 @@ def _callback_op_sharding(\n       ndim = 0\n       if avals_out and isinstance(avals_out[0], core.ShapedArray):\n         ndim = avals_out[0].ndim\n-      op_sharding = sharding_impls.SdyArrayList([\n-          sharding_impls.SdyArray(\n+      op_sharding = SdyArrayList([\n+          SdyArray(\n               mesh_shape=(),\n               dim_shardings=[\n-                  sharding_impls.SdyDim(axes=[], is_open=False)\n+                  SdyDim(axes=[], is_open=False)\n               ] * ndim,\n               logical_device_ids=())])\n     else:\n@@ -199,8 +199,8 @@ def _callback_op_sharding(\n       # number of result ops. If there are no result ops, we need 1 shardy\n       # annotation.\n       num_sdy_shardings = max(1, len(avals_out))\n-      op_sharding = sharding_impls.SdyArrayList(num_sdy_shardings * [\n-          sharding_impls.SdyArray(\n+      op_sharding = SdyArrayList(num_sdy_shardings * [\n+          SdyArray(\n               mesh_shape=(),\n               dim_shardings=[],\n               logical_device_ids=(device_index,))])\n@@ -838,14 +838,17 @@ def _wrapped_callback(token, *args):  # type: ignore  # pylint: disable=function\n         config.use_shardy_partitioner.value\n         and sharding is not None\n         and len(ctx.avals_out) > 0\n-        and isinstance(sharding, sharding_impls.SdyArrayList)\n+        and isinstance(sharding, SdyArrayList)\n     ):\n       # Add a sharding annotation for the token if we have at least one\n       # output. Otherwise, the single shardy annotation required of all ops\n       # (even those without any results) can annotate the token.\n-      sharding = sharding_impls.SdyArrayList(\n-          [*sharding.shardings, sharding.shardings[-1]]\n-      )\n+      sharding = SdyArrayList([\n+          SdyArray(\n+              mesh_shape=(),\n+              dim_shardings=[],\n+              logical_device_ids=()),\n+          *sharding.shardings])\n     ctx = dataclasses.replace(\n         ctx,\n         avals_in=[core.abstract_token, *ctx.avals_in],\n\n```",
        "from_id": [
            "bartchr808",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "#sdy Properly handle token types in JAX and `ManualComputationOp`.\n\nWe weren't handling them correctly meaning you couldn't use a `shard_map`/`ManualComputationOp` which has callbacks inside.\n\nPiperOrigin-RevId: 759072597",
        "output": "```diff\nCommit: 0a0368bb2add564122617bb68caf731942d41279\nDate: 2025-05-15T10:37:53Z\nURL: https://github.com/jax-ml/jax/commit/0a0368bb2add564122617bb68caf731942d41279\nFiles changed: 6\nAdditions: +84, Deletions: -27\ndiff --git a/jax/_src/callback.py b/jax/_src/callback.py\nindex bc233b634f3c..7fcccac14950 100644\n--- a/jax/_src/callback.py\n+++ b/jax/_src/callback.py\n@@ -154,13 +154,15 @@ def _callback_op_sharding(\n           \" computations\"\n       )\n     if config.use_shardy_partitioner.value:\n-      assert len(avals_out) == 1\n+      ndim = 0\n+      if avals_out and isinstance(avals_out[0], core.ShapedArray):\n+        ndim = avals_out[0].ndim\n       op_sharding = sharding_impls.SdyArrayList([\n           sharding_impls.SdyArray(\n               mesh_shape=(),\n               dim_shardings=[\n                   sharding_impls.SdyDim(axes=[], is_open=False)\n-              ] * avals_out[0].ndim,\n+              ] * ndim,\n               logical_device_ids=())])\n     else:\n       op_sharding = xc.OpSharding()  # type: ignore[assignment]\ndiff --git a/jax/_src/debugging.py b/jax/_src/debugging.py\nindex c2febf752b92..e931a6edb9b3 100644\n--- a/jax/_src/debugging.py\n+++ b/jax/_src/debugging.py\n@@ -164,14 +164,16 @@ def debug_callback_lowering(ctx, *args, effect, partitioned, callback, **params)\n       # If we have fully manual sharding during lowering, that means the JAX\n       # program has per-device semantics, so we run the callback on each device.\n       if config.use_shardy_partitioner.value:\n-        assert len(ctx.avals_out) == 1\n+        ndim = 0\n+        if ctx.avals_out and isinstance(ctx.avals_out[0], core.ShapedArray):\n+          ndim = ctx.avals_out[0].ndim\n         sharding = sharding_impls.SdyArrayList([\n             sharding_impls.SdyArray(\n                 mesh_shape=(),\n                 dim_shardings=[\n                     sharding_impls.SdyDim(axes=[], is_open=False)\n-                ] * ctx.avals_out[0].ndim,\n-                logical_device_ids=())])\n+                ] * ndim,\n+                logical_device_ids=(0,))])\n       else:\n         sharding = xc.OpSharding()\n         sharding.type = xc.OpSharding.Type.MANUAL\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex abcc2ca0acf1..ac529a667d04 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -44,7 +44,7 @@\n                            get_abstract_mesh, get_concrete_mesh)\n from jax._src.api import _shared_code_pmap, _prepare_pmap\n from jax._src.lib.mlir import ir\n-from jax._src.lib.mlir.dialects import sdy\n+from jax._src.lib.mlir.dialects import hlo, sdy\n from jax._src.util import (HashableFunction, HashablePartial, unzip2,\n                            as_hashable_function, memoize, partition_list,\n                            merge_lists, split_list, subs_list2)\n@@ -786,6 +786,13 @@ def _shardy_shard_map_sharding(\n   return sdy_sharding\n \n \n+def _shardy_shard_map_token_sharding(\n+    ctx: mlir.LoweringRuleContext, mesh\n+  ) -> ir.Attribute:\n+  ns = _make_scoped_manual_sharding(ctx, mesh, {})\n+  return ns._to_sdy_sharding(0)\n+\n+\n def _shard_map_lowering_shardy(\n     ctx, in_nodes, jaxpr, mesh, in_names, out_names, manual_axes, check_vma):\n   axis_ctx = ctx.module_context.axis_context\n@@ -799,36 +806,70 @@ def _shard_map_lowering_shardy(\n   new_axis_context = sharding_impls.SPMDAxisContext(mesh, manual_axes)\n   sub_ctx = ctx.module_context.replace(axis_context=new_axis_context)\n \n+  tokens = [ctx.tokens_in.get(eff) for eff in ctx.tokens_in.effects()]\n+  num_tokens = len(tokens)\n   manual_axes = order_wrt_mesh(mesh, shardy_manual_axes)\n   if np.prod([mesh.shape[a] for a in manual_axes]) == 1:\n     # No need for a `ManualComputationOp` if all manual axes are size 1.\n     with _extend_axis_env(mesh, manual_axes), config._check_vma(check_vma):\n-      out_nodes, _ = mlir.jaxpr_subcomp(\n-          sub_ctx, jaxpr, ctx.name_stack, mlir.TokenSet(), (), *in_nodes,\n+      args = (*ctx.dim_var_values, *tokens, *in_nodes)\n+      out_nodes, tokens_out = mlir.jaxpr_subcomp(\n+          sub_ctx, jaxpr, ctx.name_stack,\n+          mlir.TokenSet(zip(ctx.tokens_in.effects(), in_nodes[:num_tokens])),\n+        (), *args[num_tokens:],\n           dim_var_values=ctx.dim_var_values)\n-    return out_nodes\n+      num_tokens = len(tokens_out.effects())\n+      tokens_out = tokens_out.update_tokens(mlir.TokenSet(zip(\n+          ctx.tokens_in.effects(), out_nodes[:num_tokens])))\n+      ctx.set_tokens_out(tokens_out)\n+    return out_nodes[num_tokens:]\n \n-  in_shardings = sharding_impls.SdyArrayList(map(\n+  in_shardings = list(map(\n       partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n-      in_names, ctx.avals_in)).build()\n-  out_shardings = sharding_impls.SdyArrayList(map(\n+      in_names, ctx.avals_in))\n+  num_dim_vars = len(ctx.dim_var_values)\n+  in_shardings = ([_shardy_shard_map_token_sharding(ctx, mesh)]\n+                  * (num_tokens + num_dim_vars) + in_shardings)\n+  in_shardings = sharding_impls.SdyArrayList(in_shardings).build()\n+\n+  out_shardings = list(map(\n       partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n-      out_names, ctx.avals_out)).build()\n-  output_types = map(mlir.aval_to_ir_type, ctx.avals_out)\n+      out_names, ctx.avals_out))\n+  out_shardings = [\n+      _shardy_shard_map_token_sharding(ctx, mesh)] * num_tokens + out_shardings\n+  out_shardings = sharding_impls.SdyArrayList(out_shardings).build()\n+\n+  output_types = ([hlo.TokenType.get()] * num_tokens +\n+                  list(map(mlir.aval_to_ir_type, ctx.avals_out)))\n+\n+  args = (*ctx.dim_var_values, *tokens, *in_nodes)\n   manual_computation_op = sdy.ManualComputationOp(\n-      output_types, in_nodes, in_shardings, out_shardings,\n+      output_types,\n+      mlir.flatten_ir_values(args),\n+      in_shardings, out_shardings,\n       sdy.ManualAxesAttr.get(\n           ir.ArrayAttr.get([ir.StringAttr.get(i) for i in manual_axes])))\n   block = ir.Block.create_at_start(\n-      manual_computation_op.body, map(mlir.aval_to_ir_type, in_avals_))\n+      manual_computation_op.body,\n+      (*(i if isinstance(i, ir.Type) else i.type for i in ctx.dim_var_values),\n+       *([hlo.TokenType.get()] * num_tokens),\n+       *map(mlir.aval_to_ir_type, in_avals_)))\n   with (ir.InsertionPoint(block), _extend_axis_env(mesh, manual_axes),\n         config._check_vma(check_vma)):\n-    out_nodes_, _ = mlir.jaxpr_subcomp(\n-        sub_ctx, jaxpr, ctx.name_stack, mlir.TokenSet(), (), *block.arguments,\n+    out_nodes_, tokens_out = mlir.jaxpr_subcomp(\n+        sub_ctx, jaxpr, ctx.name_stack,\n+        mlir.TokenSet(zip(\n+            ctx.tokens_in.effects(), block.arguments[:num_tokens])),\n+        (), *block.arguments[num_tokens+num_dim_vars:],\n         dim_var_values=ctx.dim_var_values)\n-    sdy.ReturnOp([ir.Value(x) for x in out_nodes_])\n+    sdy.ReturnOp([ir.Value(x) for x in (*[v for _, v in tokens_out.items()],\n+                                        *out_nodes_)])\n+    num_tokens = len(tokens_out.effects())\n+    tokens_out = tokens_out.update_tokens(mlir.TokenSet(zip(\n+        ctx.tokens_in.effects(), manual_computation_op.results[:num_tokens])))\n+    ctx.set_tokens_out(tokens_out)\n \n-  return manual_computation_op.results\n+  return manual_computation_op.results[num_tokens:]\n \n \n def _shard_map_lowering(ctx, *in_nodes, jaxpr, mesh, in_names, out_names,\n@@ -846,7 +887,8 @@ def _shard_map_lowering(ctx, *in_nodes, jaxpr, mesh, in_names, out_names,\n   with _extend_axis_env(mesh, manual_axes), config._check_vma(check_vma):\n     out_nodes_, tokens_out = mlir.call_lowering(\n         \"shmap_body\", ctx.name_stack, jaxpr, None, sub_ctx, in_avals_,\n-        out_avals_, ctx.tokens_in, *in_nodes_, dim_var_values=ctx.dim_var_values,\n+        out_avals_, ctx.tokens_in, *in_nodes_,\n+        dim_var_values=ctx.dim_var_values,\n         arg_names=map(_pspec_mhlo_attrs, in_names, in_avals_),\n         result_names=map(_pspec_mhlo_attrs, out_names, out_avals_))\n   ctx.set_tokens_out(tokens_out)\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex d1c8ec7f050d..7d87705e20bd 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -4485,8 +4485,6 @@ def test_in_out_shardings_unconstrained_error(self):\n               in_shardings=NamedSharding(mesh, P(P.UNCONSTRAINED, 'x')))\n \n   def test_empty_io_callback_under_shard_map(self):\n-    if config.use_shardy_partitioner.value:\n-      self.skipTest(\"TODO(b/384938613): Failing under shardy.\")\n     mesh = jtu.create_mesh((4,), 'i')\n \n     def empty_callback(x):\ndiff --git a/tests/python_callback_test.py b/tests/python_callback_test.py\nindex 9f7336548d12..9a3b26530044 100644\n--- a/tests/python_callback_test.py\n+++ b/tests/python_callback_test.py\n@@ -1364,8 +1364,6 @@ def f_base(i, x):\n     self.assertEqual(_collected, expected)\n \n   def test_can_shard_io_callback_manually(self):\n-    if config.use_shardy_partitioner.value:\n-      self.skipTest(\"TODO(b/384938613): Failing under shardy.\")\n \n     mesh = Mesh(np.array(jax.devices()), axis_names=('x',))\n \ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 00d437aadb08..2fdc846a356b 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -869,8 +869,6 @@ def test_shmap_abstract_mesh_errors(self):\n   @jtu.run_on_devices('cpu', 'gpu', 'tpu')\n   @jtu.thread_unsafe_test()\n   def test_debug_print_jit(self, jit):\n-    if config.use_shardy_partitioner.value:\n-      self.skipTest('TODO(b/384938613): Failing under shardy')\n     mesh = Mesh(jax.devices(), ('i',))\n \n     @partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P('i'))\n@@ -892,6 +890,23 @@ def f(x):\n     for i in range(len(jax.devices())):\n       self.assertIn(f'instance {i} has value', output())\n \n+  @jtu.run_on_devices('cpu', 'gpu', 'tpu')\n+  @jtu.thread_unsafe_test()\n+  def test_debug_print_jit_partial_auto(self):\n+    mesh = jtu.create_mesh((2,2), ('x', 'y'))\n+\n+    @partial(shard_map, mesh=mesh, in_specs=P('x'), out_specs=P('x'),\n+             axis_names=frozenset({'x'}))\n+    def f(x):\n+      idx = jax.lax.axis_index('x')\n+      jax.debug.print(\"instance {i} has value x={x}\", i=idx, x=x)\n+      y = jnp.cos(x)\n+      return y\n+\n+    f = jax.jit(f)\n+    x = jnp.arange(2 * len(jax.devices()))\n+    f(x)  # don't crash!\n+\n   def test_debug_print_eager(self):\n     mesh = Mesh(jax.devices(), ('i',))\n \n\n```",
        "from_id": [
            "bartchr808",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] Do not emit verbose lowering errors by default\n\nThe errors are too verbose and mostly not very useful.\n\nPiperOrigin-RevId: 759025165",
        "output": "```diff\nCommit: 7cbdc3c2defe2bb1fdec55b7157a0d2b43fcd27b\nDate: 2025-05-15T07:55:23Z\nURL: https://github.com/jax-ml/jax/commit/7cbdc3c2defe2bb1fdec55b7157a0d2b43fcd27b\nFiles changed: 2\nAdditions: +1, Deletions: -5\ndiff --git a/jax/_src/pallas/pallas_call.py b/jax/_src/pallas/pallas_call.py\nindex def8efd472c6..2d27bd3cc485 100644\n--- a/jax/_src/pallas/pallas_call.py\n+++ b/jax/_src/pallas/pallas_call.py\n@@ -1235,7 +1235,7 @@ def _trace_kernel_to_jaxpr(\n \n _PALLAS_VERBOSE_ERRORS = config.bool_flag(\n     \"jax_pallas_verbose_errors\",\n-    default=config.bool_env(\"JAX_PALLAS_VERBOSE_ERRORS\", True),\n+    default=config.bool_env(\"JAX_PALLAS_VERBOSE_ERRORS\", False),\n     help=(\n         \"If True, print verbose error messages for Pallas kernels.\"\n     ),\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 21ab7ea1a482..109af4213b81 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -162,7 +162,6 @@ jax_multiplatform_test(\n     ],\n     env = {\n         \"JAX_PALLAS_USE_MOSAIC_GPU\": \"1\",\n-        \"JAX_PALLAS_VERBOSE_ERRORS\": \"0\",\n     },\n     shard_count = 16,\n     tags = [\n@@ -239,9 +238,6 @@ jax_multiplatform_test(\n         \"gpu_h100_x32\",\n         \"gpu_h100\",\n     ],\n-    env = {\n-        \"JAX_PALLAS_VERBOSE_ERRORS\": \"0\",\n-    },\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_mosaic_gpu\",  # build_cleaner: keep\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix CI build failure on Mac.\n\nWe must not depend on the nvidia_nvshmem_cu12 pip package directly since it does not exist on Windows and Mac platforms.\n\nPiperOrigin-RevId: 758917499",
        "output": "```diff\nCommit: ec72f173cf98d95d1537b1f3b9f6720e2a032203\nDate: 2025-05-15T01:37:44Z\nURL: https://github.com/jax-ml/jax/commit/ec72f173cf98d95d1537b1f3b9f6720e2a032203\nFiles changed: 2\nAdditions: +3, Deletions: -2\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 3f73a4b9e68f..4018bff873bd 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -887,7 +887,6 @@ pytype_strict_library(\n py_library_providing_imports_info(\n     name = \"mosaic_gpu\",\n     srcs = glob([\"experimental/mosaic/gpu/*.py\"]),\n-    data = py_deps(\"libnvshmem_device\"),\n     visibility = [\n         \":mosaic_gpu_users\",\n     ],\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex 1d4e24720c2e..a8dc67eb3804 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -100,7 +100,6 @@ _py_deps = {\n     \"tensorstore\": get_optional_dep(\"@pypi//tensorstore\"),\n     \"torch\": [],\n     \"zstandard\": get_zstandard(),\n-    \"libnvshmem_device\": [\"@pypi//nvidia_nvshmem_cu12\"],\n }\n \n def all_py_deps(excluded = []):\n@@ -188,14 +187,17 @@ def _gpu_test_deps():\n             \"//jaxlib/cuda:gpu_only_test_deps\",\n             \"//jaxlib/rocm:gpu_only_test_deps\",\n             \"//jax_plugins:gpu_plugin_only_test_deps\",\n+            \"@pypi//nvidia_nvshmem_cu12\",\n         ],\n         \"//jax:config_build_jaxlib_false\": [\n             \"@pypi//jax_cuda12_plugin\",\n             \"@pypi//jax_cuda12_pjrt\",\n+            \"@pypi//nvidia_nvshmem_cu12\",\n         ],\n         \"//jax:config_build_jaxlib_wheel\": [\n             \"//jaxlib/tools:jax_cuda_plugin_py_import\",\n             \"//jaxlib/tools:jax_cuda_pjrt_py_import\",\n+            \"@pypi//nvidia_nvshmem_cu12\",\n         ],\n     })\n \n\n```",
        "from_id": [
            "hawkinsp",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28755 from hawkinsp:plugins\n\nPiperOrigin-RevId: 758915292",
        "output": "```diff\nCommit: 0f91c40a54069784fc9e1742f72d4a0dbeb6fcf5\nDate: 2025-05-15T01:30:25Z\nURL: https://github.com/jax-ml/jax/commit/0f91c40a54069784fc9e1742f72d4a0dbeb6fcf5\nFiles changed: 3\nAdditions: +150, Deletions: -136\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex a0c30132c169..9fd4e50304d0 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -21,6 +21,8 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n     given its name.\n \n * Changes\n+  * Additional checking for the versions of CUDA package dependencies was\n+    reenabled, having been accidentally disabled in a previous release.\n   * JAX nightly packages are now published to artifact registry. To install\n     these packages, see the [JAX installation guide](https://docs.jax.dev/en/latest/installation.html#jax-nightly-installation).\n   * `jax.sharding.PartitionSpec` no longer inherits from a tuple.\ndiff --git a/jax/_src/xla_bridge.py b/jax/_src/xla_bridge.py\nindex 72a16d5fbe5c..ce0c36fdcca4 100644\n--- a/jax/_src/xla_bridge.py\n+++ b/jax/_src/xla_bridge.py\n@@ -31,7 +31,6 @@\n import pkgutil\n import platform as py_platform\n import threading\n-import traceback\n from typing import Any, Sequence, Union\n import warnings\n \n@@ -311,141 +310,6 @@ def _check_cuda_compute_capability(devices_to_check):\n       )\n \n \n-def _check_cuda_versions(raise_on_first_error: bool = False,\n-                         debug: bool = False):\n-  assert cuda_versions is not None\n-  results: list[dict[str, Any]] = []\n-\n-  def _make_msg(name: str,\n-                runtime_version: int,\n-                build_version: int,\n-                min_supported: int,\n-                debug_msg: bool = False):\n-    if debug_msg:\n-      return (f\"Package: {name}\\n\"\n-              f\"Version JAX was built against: {build_version}\\n\"\n-              f\"Minimum supported: {min_supported}\\n\"\n-              f\"Installed version: {runtime_version}\")\n-    if min_supported:\n-      req_str = (f\"The local installation version must be no lower than \"\n-                 f\"{min_supported}.\")\n-    else:\n-      req_str = (\"The local installation must be the same version as \"\n-                 \"the version against which JAX was built.\")\n-    msg = (f\"Outdated {name} installation found.\\n\"\n-           f\"Version JAX was built against: {build_version}\\n\"\n-           f\"Minimum supported: {min_supported}\\n\"\n-           f\"Installed version: {runtime_version}\\n\"\n-           f\"{req_str}\")\n-    return msg\n-\n-  def _version_check(name: str,\n-                     get_version,\n-                     get_build_version,\n-                     scale_for_comparison: int = 1,\n-                     min_supported_version: int = 0):\n-    \"\"\"Checks the runtime CUDA component version against the JAX one.\n-\n-    Args:\n-      name: Of the CUDA component.\n-      get_version: A function to get the local runtime version of the component.\n-      get_build_version: A function to get the build version of the component.\n-      scale_for_comparison: For rounding down a version to ignore patch/minor.\n-      min_supported_version: An absolute minimum version required. Must be\n-        passed without rounding down.\n-\n-    Raises:\n-      RuntimeError: If the component is not found, or is of unsupported version,\n-        and if raising the error is not deferred till later.\n-    \"\"\"\n-\n-    build_version = get_build_version()\n-    try:\n-      version = get_version()\n-    except Exception as e:\n-      err_msg = f\"Unable to load {name}. Is it installed?\"\n-      if raise_on_first_error:\n-        raise RuntimeError(err_msg) from e\n-      err_msg += f\"\\n{traceback.format_exc()}\"\n-      results.append({\"name\": name, \"installed\": False, \"msg\": err_msg})\n-      return\n-\n-    if not min_supported_version:\n-      min_supported_version = build_version // scale_for_comparison\n-    passed = min_supported_version <= version\n-\n-    if not passed or debug:\n-      msg = _make_msg(name=name,\n-                      runtime_version=version,\n-                      build_version=build_version,\n-                      min_supported=min_supported_version,\n-                      debug_msg=passed)\n-      if not passed and raise_on_first_error:\n-        raise RuntimeError(msg)\n-      else:\n-        record = {\"name\": name,\n-                  \"installed\": True,\n-                  \"msg\": msg,\n-                  \"passed\": passed,\n-                  \"build_version\": build_version,\n-                  \"version\": version,\n-                  \"minimum_supported\": min_supported_version}\n-        results.append(record)\n-\n-  _version_check(\"CUDA\", cuda_versions.cuda_runtime_get_version,\n-                 cuda_versions.cuda_runtime_build_version,\n-                 scale_for_comparison=10,\n-                 min_supported_version=12010)\n-  _version_check(\n-      \"cuDNN\",\n-      cuda_versions.cudnn_get_version,\n-      cuda_versions.cudnn_build_version,\n-      # NVIDIA promise both backwards and forwards compatibility for cuDNN patch\n-      # versions:\n-      # https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#api-compat\n-      scale_for_comparison=100,\n-      min_supported_version=9100\n-  )\n-  _version_check(\"cuFFT\", cuda_versions.cufft_get_version,\n-                 cuda_versions.cufft_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100)\n-  _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n-                 cuda_versions.cusolver_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=11400)\n-  _version_check(\"cuPTI\", cuda_versions.cupti_get_version,\n-                 cuda_versions.cupti_build_version,\n-                 min_supported_version=18)\n-  _version_check(\"cuBLAS\", cuda_versions.cublas_get_version,\n-                 cuda_versions.cublas_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=120100)\n-  _version_check(\"cuSPARSE\", cuda_versions.cusparse_get_version,\n-                 cuda_versions.cusparse_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=12100)\n-\n-  errors = []\n-  debug_results = []\n-  for result in results:\n-    message: str = result['msg']\n-    if not result['installed'] or not result['passed']:\n-      errors.append(message)\n-    else:\n-      debug_results.append(message)\n-\n-  join_str = f'\\n{\"-\" * 50}\\n'\n-  if debug_results:\n-    print(f'CUDA components status (debug):\\n'\n-          f'{join_str.join(debug_results)}')\n-  if errors:\n-    raise RuntimeError(f'Unable to use CUDA because of the '\n-                       f'following issues with CUDA components:\\n'\n-                       f'{join_str.join(errors)}')\n \n def get_num_nodes_from_gpu_topology(topology: str) -> int:\n     try:\ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 1be29326c95f..9df7fc69ff1a 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -17,6 +17,8 @@\n import logging\n import os\n import pathlib\n+import traceback\n+from typing import Any\n \n from jax._src.lib import triton\n from jax._src.lib import xla_client\n@@ -29,8 +31,12 @@\n     cuda_plugin_extension = importlib.import_module(\n         f'{pkg_name}.cuda_plugin_extension'\n     )\n+    cuda_versions = importlib.import_module(\n+        f'{pkg_name}._versions'\n+    )\n   except ImportError:\n     cuda_plugin_extension = None\n+    cuda_versions = None\n   else:\n     break\n \n@@ -76,11 +82,153 @@ def _get_library_path():\n   return None\n \n \n+def _check_cuda_versions(raise_on_first_error: bool = False,\n+                         debug: bool = False):\n+  assert cuda_versions is not None\n+  results: list[dict[str, Any]] = []\n+\n+  def _make_msg(name: str,\n+                runtime_version: int,\n+                build_version: int,\n+                min_supported: int,\n+                debug_msg: bool = False):\n+    if debug_msg:\n+      return (f\"Package: {name}\\n\"\n+              f\"Version JAX was built against: {build_version}\\n\"\n+              f\"Minimum supported: {min_supported}\\n\"\n+              f\"Installed version: {runtime_version}\")\n+    if min_supported:\n+      req_str = (f\"The local installation version must be no lower than \"\n+                 f\"{min_supported}.\")\n+    else:\n+      req_str = (\"The local installation must be the same version as \"\n+                 \"the version against which JAX was built.\")\n+    msg = (f\"Outdated {name} installation found.\\n\"\n+           f\"Version JAX was built against: {build_version}\\n\"\n+           f\"Minimum supported: {min_supported}\\n\"\n+           f\"Installed version: {runtime_version}\\n\"\n+           f\"{req_str}\")\n+    return msg\n+\n+  def _version_check(name: str,\n+                     get_version,\n+                     get_build_version,\n+                     scale_for_comparison: int = 1,\n+                     min_supported_version: int = 0):\n+    \"\"\"Checks the runtime CUDA component version against the JAX one.\n+\n+    Args:\n+      name: Of the CUDA component.\n+      get_version: A function to get the local runtime version of the component.\n+      get_build_version: A function to get the build version of the component.\n+      scale_for_comparison: For rounding down a version to ignore patch/minor.\n+      min_supported_version: An absolute minimum version required. Must be\n+        passed without rounding down.\n+\n+    Raises:\n+      RuntimeError: If the component is not found, or is of unsupported version,\n+        and if raising the error is not deferred till later.\n+    \"\"\"\n+\n+    build_version = get_build_version()\n+    try:\n+      version = get_version()\n+    except Exception as e:\n+      err_msg = f\"Unable to load {name}. Is it installed?\"\n+      if raise_on_first_error:\n+        raise RuntimeError(err_msg) from e\n+      err_msg += f\"\\n{traceback.format_exc()}\"\n+      results.append({\"name\": name, \"installed\": False, \"msg\": err_msg})\n+      return\n+\n+    if not min_supported_version:\n+      min_supported_version = build_version // scale_for_comparison\n+    passed = min_supported_version <= version\n+\n+    if not passed or debug:\n+      msg = _make_msg(name=name,\n+                      runtime_version=version,\n+                      build_version=build_version,\n+                      min_supported=min_supported_version,\n+                      debug_msg=passed)\n+      if not passed and raise_on_first_error:\n+        raise RuntimeError(msg)\n+      else:\n+        record = {\"name\": name,\n+                  \"installed\": True,\n+                  \"msg\": msg,\n+                  \"passed\": passed,\n+                  \"build_version\": build_version,\n+                  \"version\": version,\n+                  \"minimum_supported\": min_supported_version}\n+        results.append(record)\n+\n+  _version_check(\"CUDA\", cuda_versions.cuda_runtime_get_version,\n+                 cuda_versions.cuda_runtime_build_version,\n+                 scale_for_comparison=10,\n+                 min_supported_version=12010)\n+  _version_check(\n+      \"cuDNN\",\n+      cuda_versions.cudnn_get_version,\n+      cuda_versions.cudnn_build_version,\n+      # NVIDIA promise both backwards and forwards compatibility for cuDNN patch\n+      # versions:\n+      # https://docs.nvidia.com/deeplearning/cudnn/backend/latest/developer/forward-compatibility.html#cudnn-api-compatibility\n+      scale_for_comparison=100,\n+  )\n+  _version_check(\"cuFFT\", cuda_versions.cufft_get_version,\n+                 cuda_versions.cufft_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100)\n+  _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n+                 cuda_versions.cusolver_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100,\n+                 min_supported_version=11400)\n+  _version_check(\"cuPTI\", cuda_versions.cupti_get_version,\n+                 cuda_versions.cupti_build_version,\n+                 min_supported_version=18)\n+  _version_check(\"cuBLAS\", cuda_versions.cublas_get_version,\n+                 cuda_versions.cublas_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100,\n+                 min_supported_version=120100)\n+  _version_check(\"cuSPARSE\", cuda_versions.cusparse_get_version,\n+                 cuda_versions.cusparse_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100,\n+                 min_supported_version=12100)\n+\n+  errors = []\n+  debug_results = []\n+  for result in results:\n+    message: str = result['msg']\n+    if not result['installed'] or not result['passed']:\n+      errors.append(message)\n+    else:\n+      debug_results.append(message)\n+\n+  join_str = f'\\n{\"-\" * 50}\\n'\n+  if debug_results:\n+    print(f'CUDA components status (debug):\\n'\n+          f'{join_str.join(debug_results)}')\n+  if errors:\n+    raise RuntimeError(f'Unable to use CUDA because of the '\n+                       f'following issues with CUDA components:\\n'\n+                       f'{join_str.join(errors)}')\n+\n+\n def initialize():\n   path = _get_library_path()\n   if path is None:\n     return\n \n+  if not os.getenv(\"JAX_SKIP_CUDA_CONSTRAINTS_CHECK\"):\n+    _check_cuda_versions(raise_on_first_error=True)\n+  else:\n+    print('Skipped CUDA versions constraints check due to the '\n+          'JAX_SKIP_CUDA_CONSTRAINTS_CHECK env var being set.')\n+\n   options = xla_client.generate_pjrt_gpu_plugin_options()\n   c_api = xb.register_plugin(\n       'cuda', priority=500, library_path=str(path), options=options\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[JAX] Fix unhashable slice in api_test\n\n`slice` is not hashable before Python 3.12. This change mitigates it by\nconverting it into a hash value.\n\nPiperOrigin-RevId: 758905560",
        "output": "```diff\nCommit: 9a1535e210dcb29f2b0f7494a773ed9aee5b9049\nDate: 2025-05-15T00:58:02Z\nURL: https://github.com/jax-ml/jax/commit/9a1535e210dcb29f2b0f7494a773ed9aee5b9049\nFiles changed: 1\nAdditions: +5, Deletions: -2\ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 1fd192a52525..3fe3d6fa7514 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -54,6 +54,7 @@\n from jax._src import xla_bridge\n from jax._src import debugging\n from jax._src import pjit as pjit_lib\n+from jax._src import sharding_impls\n from jax._src.ad_checkpoint import saved_residuals\n from jax._src.interpreters import ad as ad_internal\n from jax._src.interpreters import mlir\n@@ -2047,10 +2048,12 @@ def test_internal_device_put_assembled(self):\n     per_device_arrs = {\n         # Use uncommitted arrays that are not aligned with the destination\n         # sharding so that we trigger `BatchedDevicePut`.\n-        index: jnp.array(arr[index])\n+        sharding_impls.hashed_index(index): jnp.array(arr[index])\n         for _, index in sharding.devices_indices_map(arr.shape).items()\n     }\n-    data_callback = lambda index: per_device_arrs[index]\n+    data_callback = lambda index: per_device_arrs[\n+        sharding_impls.hashed_index(index)\n+    ]\n     with jtu.count_internal_device_puts() as counts:\n       jax.make_array_from_callback(arr.shape, sharding, data_callback)\n     self.assertEqual(\n\n```",
        "from_id": [
            "hyeontaek",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add numpy and absl/testing dep to custom_api_test. Fixes https://github.com/jax-ml/jax/actions/runs/15031061909/job/42243435305\n\nPiperOrigin-RevId: 758898284",
        "output": "```diff\nCommit: bf4fda96a936aab5adc6430763419c3bdb3d9495\nDate: 2025-05-15T00:32:41Z\nURL: https://github.com/jax-ml/jax/commit/bf4fda96a936aab5adc6430763419c3bdb3d9495\nFiles changed: 2\nAdditions: +9, Deletions: -2\ndiff --git a/tests/BUILD b/tests/BUILD\nindex e70d4593e8fc..1d1f3c28b239 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -50,7 +50,10 @@ jax_multiplatform_test(\n     shard_count = 10,\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 49a05ee487f0..21ab7ea1a482 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -427,7 +427,11 @@ jax_multiplatform_test(\n         \"//jax:extend\",\n         \"//jax:pallas_mosaic_gpu\",\n         \"//jax:test_multiprocess\",\n-    ] + py_deps(\"portpicker\"),\n+    ] + py_deps([\n+        \"portpicker\",\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reenable CUDA version checks from Python.\n\nThese had been accidentally broken at some point in the plugin\nswitchover..",
        "output": "```diff\nCommit: 011639cf3621c52c00ffc1a24abf7f4dacf19966\nDate: 2025-05-14T21:35:56Z\nURL: https://github.com/jax-ml/jax/commit/011639cf3621c52c00ffc1a24abf7f4dacf19966\nFiles changed: 3\nAdditions: +150, Deletions: -136\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex a0c30132c169..9fd4e50304d0 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -21,6 +21,8 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n     given its name.\n \n * Changes\n+  * Additional checking for the versions of CUDA package dependencies was\n+    reenabled, having been accidentally disabled in a previous release.\n   * JAX nightly packages are now published to artifact registry. To install\n     these packages, see the [JAX installation guide](https://docs.jax.dev/en/latest/installation.html#jax-nightly-installation).\n   * `jax.sharding.PartitionSpec` no longer inherits from a tuple.\ndiff --git a/jax/_src/xla_bridge.py b/jax/_src/xla_bridge.py\nindex 72a16d5fbe5c..ce0c36fdcca4 100644\n--- a/jax/_src/xla_bridge.py\n+++ b/jax/_src/xla_bridge.py\n@@ -31,7 +31,6 @@\n import pkgutil\n import platform as py_platform\n import threading\n-import traceback\n from typing import Any, Sequence, Union\n import warnings\n \n@@ -311,141 +310,6 @@ def _check_cuda_compute_capability(devices_to_check):\n       )\n \n \n-def _check_cuda_versions(raise_on_first_error: bool = False,\n-                         debug: bool = False):\n-  assert cuda_versions is not None\n-  results: list[dict[str, Any]] = []\n-\n-  def _make_msg(name: str,\n-                runtime_version: int,\n-                build_version: int,\n-                min_supported: int,\n-                debug_msg: bool = False):\n-    if debug_msg:\n-      return (f\"Package: {name}\\n\"\n-              f\"Version JAX was built against: {build_version}\\n\"\n-              f\"Minimum supported: {min_supported}\\n\"\n-              f\"Installed version: {runtime_version}\")\n-    if min_supported:\n-      req_str = (f\"The local installation version must be no lower than \"\n-                 f\"{min_supported}.\")\n-    else:\n-      req_str = (\"The local installation must be the same version as \"\n-                 \"the version against which JAX was built.\")\n-    msg = (f\"Outdated {name} installation found.\\n\"\n-           f\"Version JAX was built against: {build_version}\\n\"\n-           f\"Minimum supported: {min_supported}\\n\"\n-           f\"Installed version: {runtime_version}\\n\"\n-           f\"{req_str}\")\n-    return msg\n-\n-  def _version_check(name: str,\n-                     get_version,\n-                     get_build_version,\n-                     scale_for_comparison: int = 1,\n-                     min_supported_version: int = 0):\n-    \"\"\"Checks the runtime CUDA component version against the JAX one.\n-\n-    Args:\n-      name: Of the CUDA component.\n-      get_version: A function to get the local runtime version of the component.\n-      get_build_version: A function to get the build version of the component.\n-      scale_for_comparison: For rounding down a version to ignore patch/minor.\n-      min_supported_version: An absolute minimum version required. Must be\n-        passed without rounding down.\n-\n-    Raises:\n-      RuntimeError: If the component is not found, or is of unsupported version,\n-        and if raising the error is not deferred till later.\n-    \"\"\"\n-\n-    build_version = get_build_version()\n-    try:\n-      version = get_version()\n-    except Exception as e:\n-      err_msg = f\"Unable to load {name}. Is it installed?\"\n-      if raise_on_first_error:\n-        raise RuntimeError(err_msg) from e\n-      err_msg += f\"\\n{traceback.format_exc()}\"\n-      results.append({\"name\": name, \"installed\": False, \"msg\": err_msg})\n-      return\n-\n-    if not min_supported_version:\n-      min_supported_version = build_version // scale_for_comparison\n-    passed = min_supported_version <= version\n-\n-    if not passed or debug:\n-      msg = _make_msg(name=name,\n-                      runtime_version=version,\n-                      build_version=build_version,\n-                      min_supported=min_supported_version,\n-                      debug_msg=passed)\n-      if not passed and raise_on_first_error:\n-        raise RuntimeError(msg)\n-      else:\n-        record = {\"name\": name,\n-                  \"installed\": True,\n-                  \"msg\": msg,\n-                  \"passed\": passed,\n-                  \"build_version\": build_version,\n-                  \"version\": version,\n-                  \"minimum_supported\": min_supported_version}\n-        results.append(record)\n-\n-  _version_check(\"CUDA\", cuda_versions.cuda_runtime_get_version,\n-                 cuda_versions.cuda_runtime_build_version,\n-                 scale_for_comparison=10,\n-                 min_supported_version=12010)\n-  _version_check(\n-      \"cuDNN\",\n-      cuda_versions.cudnn_get_version,\n-      cuda_versions.cudnn_build_version,\n-      # NVIDIA promise both backwards and forwards compatibility for cuDNN patch\n-      # versions:\n-      # https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#api-compat\n-      scale_for_comparison=100,\n-      min_supported_version=9100\n-  )\n-  _version_check(\"cuFFT\", cuda_versions.cufft_get_version,\n-                 cuda_versions.cufft_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100)\n-  _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n-                 cuda_versions.cusolver_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=11400)\n-  _version_check(\"cuPTI\", cuda_versions.cupti_get_version,\n-                 cuda_versions.cupti_build_version,\n-                 min_supported_version=18)\n-  _version_check(\"cuBLAS\", cuda_versions.cublas_get_version,\n-                 cuda_versions.cublas_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=120100)\n-  _version_check(\"cuSPARSE\", cuda_versions.cusparse_get_version,\n-                 cuda_versions.cusparse_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=12100)\n-\n-  errors = []\n-  debug_results = []\n-  for result in results:\n-    message: str = result['msg']\n-    if not result['installed'] or not result['passed']:\n-      errors.append(message)\n-    else:\n-      debug_results.append(message)\n-\n-  join_str = f'\\n{\"-\" * 50}\\n'\n-  if debug_results:\n-    print(f'CUDA components status (debug):\\n'\n-          f'{join_str.join(debug_results)}')\n-  if errors:\n-    raise RuntimeError(f'Unable to use CUDA because of the '\n-                       f'following issues with CUDA components:\\n'\n-                       f'{join_str.join(errors)}')\n \n def get_num_nodes_from_gpu_topology(topology: str) -> int:\n     try:\ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 1be29326c95f..9df7fc69ff1a 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -17,6 +17,8 @@\n import logging\n import os\n import pathlib\n+import traceback\n+from typing import Any\n \n from jax._src.lib import triton\n from jax._src.lib import xla_client\n@@ -29,8 +31,12 @@\n     cuda_plugin_extension = importlib.import_module(\n         f'{pkg_name}.cuda_plugin_extension'\n     )\n+    cuda_versions = importlib.import_module(\n+        f'{pkg_name}._versions'\n+    )\n   except ImportError:\n     cuda_plugin_extension = None\n+    cuda_versions = None\n   else:\n     break\n \n@@ -76,11 +82,153 @@ def _get_library_path():\n   return None\n \n \n+def _check_cuda_versions(raise_on_first_error: bool = False,\n+                         debug: bool = False):\n+  assert cuda_versions is not None\n+  results: list[dict[str, Any]] = []\n+\n+  def _make_msg(name: str,\n+                runtime_version: int,\n+                build_version: int,\n+                min_supported: int,\n+                debug_msg: bool = False):\n+    if debug_msg:\n+      return (f\"Package: {name}\\n\"\n+              f\"Version JAX was built against: {build_version}\\n\"\n+              f\"Minimum supported: {min_supported}\\n\"\n+              f\"Installed version: {runtime_version}\")\n+    if min_supported:\n+      req_str = (f\"The local installation version must be no lower than \"\n+                 f\"{min_supported}.\")\n+    else:\n+      req_str = (\"The local installation must be the same version as \"\n+                 \"the version against which JAX was built.\")\n+    msg = (f\"Outdated {name} installation found.\\n\"\n+           f\"Version JAX was built against: {build_version}\\n\"\n+           f\"Minimum supported: {min_supported}\\n\"\n+           f\"Installed version: {runtime_version}\\n\"\n+           f\"{req_str}\")\n+    return msg\n+\n+  def _version_check(name: str,\n+                     get_version,\n+                     get_build_version,\n+                     scale_for_comparison: int = 1,\n+                     min_supported_version: int = 0):\n+    \"\"\"Checks the runtime CUDA component version against the JAX one.\n+\n+    Args:\n+      name: Of the CUDA component.\n+      get_version: A function to get the local runtime version of the component.\n+      get_build_version: A function to get the build version of the component.\n+      scale_for_comparison: For rounding down a version to ignore patch/minor.\n+      min_supported_version: An absolute minimum version required. Must be\n+        passed without rounding down.\n+\n+    Raises:\n+      RuntimeError: If the component is not found, or is of unsupported version,\n+        and if raising the error is not deferred till later.\n+    \"\"\"\n+\n+    build_version = get_build_version()\n+    try:\n+      version = get_version()\n+    except Exception as e:\n+      err_msg = f\"Unable to load {name}. Is it installed?\"\n+      if raise_on_first_error:\n+        raise RuntimeError(err_msg) from e\n+      err_msg += f\"\\n{traceback.format_exc()}\"\n+      results.append({\"name\": name, \"installed\": False, \"msg\": err_msg})\n+      return\n+\n+    if not min_supported_version:\n+      min_supported_version = build_version // scale_for_comparison\n+    passed = min_supported_version <= version\n+\n+    if not passed or debug:\n+      msg = _make_msg(name=name,\n+                      runtime_version=version,\n+                      build_version=build_version,\n+                      min_supported=min_supported_version,\n+                      debug_msg=passed)\n+      if not passed and raise_on_first_error:\n+        raise RuntimeError(msg)\n+      else:\n+        record = {\"name\": name,\n+                  \"installed\": True,\n+                  \"msg\": msg,\n+                  \"passed\": passed,\n+                  \"build_version\": build_version,\n+                  \"version\": version,\n+                  \"minimum_supported\": min_supported_version}\n+        results.append(record)\n+\n+  _version_check(\"CUDA\", cuda_versions.cuda_runtime_get_version,\n+                 cuda_versions.cuda_runtime_build_version,\n+                 scale_for_comparison=10,\n+                 min_supported_version=12010)\n+  _version_check(\n+      \"cuDNN\",\n+      cuda_versions.cudnn_get_version,\n+      cuda_versions.cudnn_build_version,\n+      # NVIDIA promise both backwards and forwards compatibility for cuDNN patch\n+      # versions:\n+      # https://docs.nvidia.com/deeplearning/cudnn/backend/latest/developer/forward-compatibility.html#cudnn-api-compatibility\n+      scale_for_comparison=100,\n+  )\n+  _version_check(\"cuFFT\", cuda_versions.cufft_get_version,\n+                 cuda_versions.cufft_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100)\n+  _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n+                 cuda_versions.cusolver_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100,\n+                 min_supported_version=11400)\n+  _version_check(\"cuPTI\", cuda_versions.cupti_get_version,\n+                 cuda_versions.cupti_build_version,\n+                 min_supported_version=18)\n+  _version_check(\"cuBLAS\", cuda_versions.cublas_get_version,\n+                 cuda_versions.cublas_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100,\n+                 min_supported_version=120100)\n+  _version_check(\"cuSPARSE\", cuda_versions.cusparse_get_version,\n+                 cuda_versions.cusparse_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100,\n+                 min_supported_version=12100)\n+\n+  errors = []\n+  debug_results = []\n+  for result in results:\n+    message: str = result['msg']\n+    if not result['installed'] or not result['passed']:\n+      errors.append(message)\n+    else:\n+      debug_results.append(message)\n+\n+  join_str = f'\\n{\"-\" * 50}\\n'\n+  if debug_results:\n+    print(f'CUDA components status (debug):\\n'\n+          f'{join_str.join(debug_results)}')\n+  if errors:\n+    raise RuntimeError(f'Unable to use CUDA because of the '\n+                       f'following issues with CUDA components:\\n'\n+                       f'{join_str.join(errors)}')\n+\n+\n def initialize():\n   path = _get_library_path()\n   if path is None:\n     return\n \n+  if not os.getenv(\"JAX_SKIP_CUDA_CONSTRAINTS_CHECK\"):\n+    _check_cuda_versions(raise_on_first_error=True)\n+  else:\n+    print('Skipped CUDA versions constraints check due to the '\n+          'JAX_SKIP_CUDA_CONSTRAINTS_CHECK env var being set.')\n+\n   options = xla_client.generate_pjrt_gpu_plugin_options()\n   c_api = xb.register_plugin(\n       'cuda', priority=500, library_path=str(path), options=options\n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Merge pull request #28753 from hawkinsp:plugins\n\nPiperOrigin-RevId: 758833461",
        "output": "```diff\nCommit: 7e8fa0d888319edb76194ebf393d014bc0d88330\nDate: 2025-05-14T21:24:35Z\nURL: https://github.com/jax-ml/jax/commit/7e8fa0d888319edb76194ebf393d014bc0d88330\nFiles changed: 4\nAdditions: +48, Deletions: -113\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 6afcdf2892f4..3f73a4b9e68f 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -101,17 +101,22 @@ string_flag(\n )\n \n config_setting(\n-    name = \"disable_jaxlib_and_jax_build\",\n+    name = \"config_build_jax_true\",\n+    flag_values = {\n+        \":build_jax\": \"true\",\n+    },\n+)\n+\n+config_setting(\n+    name = \"config_build_jax_false\",\n     flag_values = {\n-        \":build_jaxlib\": \"false\",\n         \":build_jax\": \"false\",\n     },\n )\n \n config_setting(\n-    name = \"enable_jaxlib_and_jax_py_import\",\n+    name = \"config_build_jax_wheel\",\n     flag_values = {\n-        \":build_jaxlib\": \"wheel\",\n         \":build_jax\": \"wheel\",\n     },\n )\ndiff --git a/jax_plugins/cuda/BUILD.bazel b/jax_plugins/cuda/BUILD.bazel\nindex c3c20f536cff..7070bf6bc495 100644\n--- a/jax_plugins/cuda/BUILD.bazel\n+++ b/jax_plugins/cuda/BUILD.bazel\n@@ -14,7 +14,6 @@\n \n load(\n     \"//jaxlib:jax.bzl\",\n-    \"if_windows\",\n     \"py_library_providing_imports_info\",\n     \"pytype_library\",\n )\n@@ -58,35 +57,3 @@ py_library_providing_imports_info(\n     data = [\":pjrt_c_api_gpu_plugin.so\"],\n     lib_rule = pytype_library,\n )\n-\n-config_setting(\n-    name = \"disable_jaxlib_for_cpu_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"false\",\n-        \"@local_config_cuda//:enable_cuda\": \"False\",\n-    },\n-)\n-\n-config_setting(\n-    name = \"disable_jaxlib_for_cuda12_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"false\",\n-        \"@local_config_cuda//:enable_cuda\": \"True\",\n-    },\n-)\n-\n-config_setting(\n-    name = \"enable_py_import_for_cpu_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"wheel\",\n-        \"@local_config_cuda//:enable_cuda\": \"False\",\n-    },\n-)\n-\n-config_setting(\n-    name = \"enable_py_import_for_cuda12_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"wheel\",\n-        \"@local_config_cuda//:enable_cuda\": \"True\",\n-    },\n-)\ndiff --git a/jax_plugins/rocm/BUILD.bazel b/jax_plugins/rocm/BUILD.bazel\nindex 15e9e627830e..7ee0726e7960 100644\n--- a/jax_plugins/rocm/BUILD.bazel\n+++ b/jax_plugins/rocm/BUILD.bazel\n@@ -16,7 +16,6 @@ licenses([\"notice\"])\n \n load(\n   \"//jaxlib:jax.bzl\",\n-  \"if_windows\",\n   \"py_library_providing_imports_info\",\n   \"pytype_library\",\n )\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex a48c44f406f2..1d4e24720c2e 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -167,70 +167,36 @@ def if_building_jaxlib(\n       if_building: the source code targets to depend on in case we don't depend on the jaxlib wheels\n       if_not_building: the wheels to depend on if we are not depending directly on //jaxlib.\n     \"\"\"\n-\n     return select({\n         \"//jax:config_build_jaxlib_true\": if_building,\n         \"//jax:config_build_jaxlib_false\": if_not_building,\n         \"//jax:config_build_jaxlib_wheel\": [],\n     })\n \n-def _get_test_deps(deps, backend_independent):\n-    \"\"\"Returns the test deps for the given backend.\n-\n-    Args:\n-      deps: the full list of test dependencies\n-      backend_independent: whether the test is backend independent\n+def _cpu_test_deps():\n+    \"\"\"Returns the test depencies needed for a CPU-only JAX test.\"\"\"\n+    return select({\n+        \"//jax:config_build_jaxlib_true\": [],\n+        \"//jax:config_build_jaxlib_false\": [\"@pypi//jaxlib\"],\n+        \"//jax:config_build_jaxlib_wheel\": [\"//jaxlib/tools:jaxlib_py_import\"],\n+    })\n \n-    Returns:\n-      A list of test deps for the given backend.\n-        For CPU builds:\n-          If --//jax:build_jaxlib=true, returns pypi test deps.\n-          If --//jax:build_jaxlib=false, returns jaxlib pypi wheel dep and pypi test deps.\n-          If --//jax:build_jaxlib=wheel, returns jaxlib py_import dep and pypi test deps.\n-        For GPU builds:\n-          If --//jax:build_jaxlib=true, returns pypi test deps and gpu build deps.\n-          If --//jax:build_jaxlib=false, returns jaxlib, jax-cuda-plugin,\n-            jax-cuda-pjrt pypi wheel deps and pypi test deps.\n-          If --//jax:build_jaxlib=wheel, returns jaxlib,\n-            jax-cuda-plugin, jax-cuda-pjrt py_import deps and pypi test deps.\n-    \"\"\"\n-    gpu_build_deps = [\n-        \"//jaxlib/cuda:gpu_only_test_deps\",\n-        \"//jaxlib/rocm:gpu_only_test_deps\",\n-        \"//jax_plugins:gpu_plugin_only_test_deps\",\n-    ]\n-    pypi_test_deps = [d for d in deps if d.startswith(\"@pypi//\")]\n-\n-    gpu_py_imports = [\n-        \"//jaxlib/tools:jaxlib_py_import\",\n-        \"//jaxlib/tools:jax_cuda_plugin_py_import\",\n-        \"//jaxlib/tools:jax_cuda_pjrt_py_import\",\n-    ] + pypi_test_deps\n-    cpu_py_imports = [\n-        \"//jaxlib/tools:jaxlib_py_import\",\n-    ] + pypi_test_deps\n-    jaxlib_pypi_wheel_deps = [\n-        \"@pypi//jaxlib\",\n-    ] + pypi_test_deps\n-\n-    if backend_independent:\n-        test_deps = pypi_test_deps\n-        gpu_pypi_wheel_deps = jaxlib_pypi_wheel_deps\n-        gpu_py_import_deps = cpu_py_imports\n-    else:\n-        test_deps = gpu_build_deps + pypi_test_deps\n-        gpu_pypi_wheel_deps = jaxlib_pypi_wheel_deps + [\n+def _gpu_test_deps():\n+    \"\"\"Returns the additional dependencies needed for a GPU test.\"\"\"\n+    return select({\n+        \"//jax:config_build_jaxlib_true\": [\n+            \"//jaxlib/cuda:gpu_only_test_deps\",\n+            \"//jaxlib/rocm:gpu_only_test_deps\",\n+            \"//jax_plugins:gpu_plugin_only_test_deps\",\n+        ],\n+        \"//jax:config_build_jaxlib_false\": [\n             \"@pypi//jax_cuda12_plugin\",\n             \"@pypi//jax_cuda12_pjrt\",\n-        ]\n-        gpu_py_import_deps = gpu_py_imports\n-\n-    return select({\n-        \"//jax:config_build_jaxlib_true\": test_deps,\n-        \"//jax_plugins/cuda:disable_jaxlib_for_cpu_build\": jaxlib_pypi_wheel_deps,\n-        \"//jax_plugins/cuda:disable_jaxlib_for_cuda12_build\": gpu_pypi_wheel_deps,\n-        \"//jax_plugins/cuda:enable_py_import_for_cpu_build\": cpu_py_imports,\n-        \"//jax_plugins/cuda:enable_py_import_for_cuda12_build\": gpu_py_import_deps,\n+        ],\n+        \"//jax:config_build_jaxlib_wheel\": [\n+            \"//jaxlib/tools:jax_cuda_plugin_py_import\",\n+            \"//jaxlib/tools:jax_cuda_pjrt_py_import\",\n+        ],\n     })\n \n def _get_jax_test_deps(deps):\n@@ -246,28 +212,23 @@ def _get_jax_test_deps(deps):\n       If --//jax:build_jax=false, returns jax pypi wheel dep and transitive pypi test deps.\n       If --//jax:build_jax=wheel, returns jax py_import dep and transitive pypi test deps.\n     \"\"\"\n-    jax_build_deps = [d for d in deps if not d.startswith(\"@pypi//\")]\n+    non_pypi_deps = [d for d in deps if not d.startswith(\"@pypi//\")]\n \n     # A lot of tests don't have explicit dependencies on scipy, ml_dtypes, etc. But the tests\n     # transitively depends on them via //jax. So we need to make sure that these dependencies are\n     # included in the test when JAX is built from source.\n-    jax_transitive_pypi_test_deps = {k: \"true\" for k in py_deps([\n+    pypi_deps = depset([d for d in deps if d.startswith(\"@pypi//\")])\n+    pypi_deps = depset(py_deps([\n         \"ml_dtypes\",\n         \"scipy\",\n         \"opt_einsum\",\n         \"flatbuffers\",\n-    ])}\n+    ]), transitive = [pypi_deps]).to_list()\n \n-    # Remove the pypi deps that are already provided by _get_test_deps().\n-    for d in deps:\n-        if d.startswith(\"@pypi//\") and jax_transitive_pypi_test_deps.get(d):\n-            jax_transitive_pypi_test_deps.pop(d)\n-    return select({\n-        \"//jax:disable_jaxlib_and_jax_build\": [\"//:jax_wheel_with_internal_test_util\"] +\n-                                              jax_transitive_pypi_test_deps.keys(),\n-        \"//jax:enable_jaxlib_and_jax_py_import\": [\"//:jax_py_import\"] +\n-                                                 jax_transitive_pypi_test_deps.keys(),\n-        \"//conditions:default\": jax_build_deps + jax_transitive_pypi_test_deps.keys(),\n+    return pypi_deps + select({\n+        \"//jax:config_build_jax_false\": [\"//:jax_wheel_with_internal_test_util\"],\n+        \"//jax:config_build_jax_wheel\": [\"//:jax_py_import\"],\n+        \"//jax:config_build_jax_true\": non_pypi_deps,\n     })\n \n # buildifier: disable=function-docstring\n@@ -316,18 +277,21 @@ def jax_multiplatform_test(\n         test_tags = list(tags) + [\"jax_test_%s\" % backend] + backend_tags.get(backend, [])\n         if enable_backends != None and backend not in enable_backends and not any([config.startswith(backend) for config in enable_configs]):\n             test_tags.append(\"manual\")\n+        test_deps = _cpu_test_deps() + _get_jax_test_deps([\n+            \"//jax\",\n+            \"//jax:test_util\",\n+        ] + deps)\n         if backend == \"gpu\":\n+            test_deps += _gpu_test_deps()\n             test_tags += tf_cuda_tests_tags()\n+        elif backend == \"tpu\":\n+            test_deps += [\"@pypi//libtpu\"]\n         native.py_test(\n             name = name + \"_\" + backend,\n             srcs = srcs,\n             args = test_args,\n             env = env,\n-            deps = _get_test_deps(deps, backend_independent = False) +\n-                   _get_jax_test_deps([\n-                       \"//jax\",\n-                       \"//jax:test_util\",\n-                   ] + deps),\n+            deps = test_deps,\n             data = data,\n             shard_count = test_shards,\n             tags = test_tags,\n@@ -620,13 +584,13 @@ def jax_py_test(\n     env = dict(env)\n     env.setdefault(\"PYTHONWARNINGS\", \"error\")\n     deps = kwargs.get(\"deps\", [])\n-    test_deps = _get_test_deps(deps, backend_independent = True) + _get_jax_test_deps(deps)\n+    test_deps = _cpu_test_deps() + _get_jax_test_deps(deps)\n     kwargs[\"deps\"] = test_deps\n     py_test(name = name, env = env, **kwargs)\n \n def pytype_test(name, **kwargs):\n     deps = kwargs.get(\"deps\", [])\n-    test_deps = _get_test_deps(deps, backend_independent = True) + _get_jax_test_deps(deps)\n+    test_deps = _cpu_test_deps() + _get_jax_test_deps(deps)\n     kwargs[\"deps\"] = test_deps\n     native.py_test(name = name, **kwargs)\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add use_raw_buffers which allows switching the implementation to\nhold references to raw buffers instead of PjRtBuffers.\n\nThis fixes an issue where the buffers can be deleted before\nthe transfer is complete, but introduces another problem where\nif they are donated it will now silently read from donated arrays.\n\nOnce the underlying runtime exposes usage holds properly, this\nnew codepath should take a usage hold and the old pjrtbuffer\npath should be removed.\n\nPiperOrigin-RevId: 758819621",
        "output": "```diff\nCommit: 5a6957fa75db31c9f63df8788f12f47b0d060f44\nDate: 2025-05-14T20:51:03Z\nURL: https://github.com/jax-ml/jax/commit/5a6957fa75db31c9f63df8788f12f47b0d060f44\nFiles changed: 2\nAdditions: +57, Deletions: -8\ndiff --git a/jaxlib/BUILD b/jaxlib/BUILD\nindex add6dbd7d92a..7047ddc3edd6 100644\n--- a/jaxlib/BUILD\n+++ b/jaxlib/BUILD\n@@ -1033,6 +1033,7 @@ cc_library(\n         \"@xla//xla/pjrt:status_casters\",\n         \"@xla//xla/python:nb_numpy\",\n         \"@xla//xla/python:types\",\n+        \"@xla//xla/python:version\",\n         \"@xla//xla/python/ifrt\",\n         \"@xla//xla/python/pjrt_ifrt\",\n         \"@xla//xla/python/pjrt_ifrt:pjrt_dtype\",\ndiff --git a/jaxlib/py_socket_transfer.cc b/jaxlib/py_socket_transfer.cc\nindex 114e3c14874d..fde63df8da47 100644\n--- a/jaxlib/py_socket_transfer.cc\n+++ b/jaxlib/py_socket_transfer.cc\n@@ -60,6 +60,7 @@ limitations under the License.\n #include \"xla/python/transfer/streaming_ifrt.h\"\n #include \"xla/python/transfer/transfer_socket.pb.h\"\n #include \"xla/python/types.h\"\n+#include \"xla/python/version.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -109,6 +110,7 @@ absl::StatusOr<xla::PjRtMemorySpace*> MemorySpaceFromSharding(\n   }\n }\n \n+#if JAX_IFRT_VERSION_NUMBER < 8\n class IfrtArrayEntry : public PullTable::Entry {\n  public:\n   struct BufferRef {\n@@ -153,10 +155,48 @@ class IfrtArrayEntry : public PullTable::Entry {\n   std::shared_ptr<PremappedCopierState> state_;\n   size_t xfer_size_;\n };\n+#endif\n \n-absl::StatusOr<tsl::RCReference<IfrtArrayEntry>> CreatePullEntry(\n+absl::StatusOr<tsl::RCReference<PullTable::Entry>> CreatePullEntry(\n     const std::vector<xla::ifrt::ArrayRef>& arrs,\n-    std::shared_ptr<PremappedCopierState> state, size_t xfer_size) {\n+    std::shared_ptr<PremappedCopierState> state, size_t xfer_size,\n+    bool use_raw_buffers) {\n+#if JAX_IFRT_VERSION_NUMBER >= 8\n+  if (use_raw_buffers) {\n+    std::vector<RawBufferEntry::BufferRef> refs;\n+    for (auto& arr : arrs) {\n+      auto* pjrt_arr = llvm::dyn_cast_or_null<xla::ifrt::PjRtArray>(arr.get());\n+      if (pjrt_arr == nullptr) {\n+        return absl::InvalidArgumentError(\n+            \"Cannot remote transfer non-pjrt arrays.\");\n+      }\n+      for (auto& pjrt_buf : pjrt_arr->pjrt_buffers()) {\n+        TF_ASSIGN_OR_RETURN(size_t buf_size,\n+                            pjrt_buf->GetOnDeviceSizeInBytes());\n+        TF_ASSIGN_OR_RETURN(\n+            auto raw_buffer,\n+            xla::PjRtRawBuffer::CreateRawAliasOfBuffer(pjrt_buf.get()));\n+        refs.push_back(\n+            {pjrt_buf->GetReadyFuture(), std::move(raw_buffer), buf_size});\n+      }\n+    }\n+    return tsl::MakeRef<RawBufferEntry>(std::move(refs), state, xfer_size);\n+  }\n+\n+  std::vector<PjRtBufferEntry::BufferRef> refs;\n+  for (auto& arr : arrs) {\n+    auto* pjrt_arr = llvm::dyn_cast_or_null<xla::ifrt::PjRtArray>(arr.get());\n+    if (pjrt_arr == nullptr) {\n+      return absl::InvalidArgumentError(\n+          \"Cannot remote transfer non-pjrt arrays.\");\n+    }\n+    for (auto& pjrt_buf : pjrt_arr->pjrt_buffers()) {\n+      TF_ASSIGN_OR_RETURN(size_t buf_size, pjrt_buf->GetOnDeviceSizeInBytes());\n+      refs.push_back({pjrt_buf, buf_size});\n+    }\n+  }\n+  return tsl::MakeRef<PjRtBufferEntry>(std::move(refs), state, xfer_size);\n+#else\n   std::vector<IfrtArrayEntry::BufferRef> refs;\n   for (auto& arr : arrs) {\n     auto* pjrt_arr = llvm::dyn_cast_or_null<xla::ifrt::PjRtArray>(arr.get());\n@@ -170,6 +210,7 @@ absl::StatusOr<tsl::RCReference<IfrtArrayEntry>> CreatePullEntry(\n     }\n   }\n   return tsl::MakeRef<IfrtArrayEntry>(std::move(refs), state, xfer_size);\n+#endif\n }\n \n class PyTransferServerConnection {\n@@ -195,7 +236,8 @@ class PyTransferServer {\n   absl::Status Start(xla::ifrt::Client* client, size_t max_num_parallel_copies,\n                      size_t xfer_size, const SocketAddress& addr,\n                      const std::vector<SocketAddress>& transport_addresses,\n-                     bool supports_pinned_allocator) {\n+                     bool supports_pinned_allocator, bool use_raw_buffers) {\n+    use_raw_buffers_ = use_raw_buffers;\n     std::shared_ptr<BulkTransportFactory> factory;\n     if (transport_addresses.empty()) {\n       factory = BulkTransportFactory::CreateLocal();\n@@ -235,8 +277,9 @@ class PyTransferServer {\n   }\n \n   void AwaitPull(uint64_t uuid, const std::vector<xla::ifrt::ArrayRef>& arrs) {\n-    server_->AwaitPull(uuid, xla::ValueOrThrow(CreatePullEntry(\n-                                 arrs, premapped_copier_, xfer_size_)));\n+    server_->AwaitPull(\n+        uuid, xla::ValueOrThrow(CreatePullEntry(arrs, premapped_copier_,\n+                                                xfer_size_, use_raw_buffers_)));\n   }\n \n   size_t xfer_size() { return xfer_size_; }\n@@ -249,6 +292,7 @@ class PyTransferServer {\n   std::shared_ptr<SocketServer> server_;\n   std::shared_ptr<PremappedCopierState> premapped_copier_;\n   size_t xfer_size_;\n+  bool use_raw_buffers_ = false;\n };\n \n absl::StatusOr<xla::ifrt::ArraySpec> ArraySpecFromShapeDtypeStruct(\n@@ -394,7 +438,8 @@ void RegisterTransferServerTypes(nanobind::module_& m) {\n       [](xla::nb_class_ptr<xla::PyClient> py_client, std::string address,\n          std::vector<std::string> transport_addresses_str,\n          size_t max_num_parallel_copies, size_t transfer_size,\n-         bool supports_pinned_allocator) -> PyTransferServer {\n+         bool supports_pinned_allocator,\n+         bool use_raw_buffers) -> PyTransferServer {\n         PyTransferServer result;\n         std::vector<SocketAddress> transport_addresses;\n         transport_addresses.reserve(transport_addresses_str.size());\n@@ -405,7 +450,7 @@ void RegisterTransferServerTypes(nanobind::module_& m) {\n         xla::ThrowIfError(result.Start(\n             py_client->ifrt_client(), max_num_parallel_copies, transfer_size,\n             xla::ValueOrThrow(SocketAddress::Parse(address)),\n-            transport_addresses, supports_pinned_allocator));\n+            transport_addresses, supports_pinned_allocator, use_raw_buffers));\n         return result;\n       },\n       nb::arg(\"client\"), nb::arg(\"address\") = SocketAddress().ToString(),\n@@ -413,7 +458,10 @@ void RegisterTransferServerTypes(nanobind::module_& m) {\n       nb::arg(\"max_num_parallel_copies\") = 8,\n       nb::arg(\"transfer_size\") = 256 * 1024 * 1024,\n       // Dual pinning not confirmed to be supported.\n-      nb::arg(\"supports_pinned_allocator\") = false);\n+      nb::arg(\"supports_pinned_allocator\") = false,\n+      // Technically unsafe (because a future donation won't wait for the\n+      // transfer to complete).\n+      nb::arg(\"use_raw_buffers\") = false);\n }\n \n }  // namespace aux\n\n```",
        "from_id": [
            "pschuh",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Simplify the Bazel logic used to add dependencies to tests.\n\nA lot of this logic was confusing phrased as conditions over both CPU\nand GPU build flags. But we can decompose it:\n* dependencies we add for CPU tests, and\n* additional dependencies we add for GPU tests.\n\nWhile we are here, also add the necessary pypi dependency for TPU tests.",
        "output": "```diff\nCommit: 4102cf09ec9b30651f12c22f8537f007d6f7e129\nDate: 2025-05-14T20:41:32Z\nURL: https://github.com/jax-ml/jax/commit/4102cf09ec9b30651f12c22f8537f007d6f7e129\nFiles changed: 4\nAdditions: +48, Deletions: -113\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 6afcdf2892f4..3f73a4b9e68f 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -101,17 +101,22 @@ string_flag(\n )\n \n config_setting(\n-    name = \"disable_jaxlib_and_jax_build\",\n+    name = \"config_build_jax_true\",\n+    flag_values = {\n+        \":build_jax\": \"true\",\n+    },\n+)\n+\n+config_setting(\n+    name = \"config_build_jax_false\",\n     flag_values = {\n-        \":build_jaxlib\": \"false\",\n         \":build_jax\": \"false\",\n     },\n )\n \n config_setting(\n-    name = \"enable_jaxlib_and_jax_py_import\",\n+    name = \"config_build_jax_wheel\",\n     flag_values = {\n-        \":build_jaxlib\": \"wheel\",\n         \":build_jax\": \"wheel\",\n     },\n )\ndiff --git a/jax_plugins/cuda/BUILD.bazel b/jax_plugins/cuda/BUILD.bazel\nindex c3c20f536cff..7070bf6bc495 100644\n--- a/jax_plugins/cuda/BUILD.bazel\n+++ b/jax_plugins/cuda/BUILD.bazel\n@@ -14,7 +14,6 @@\n \n load(\n     \"//jaxlib:jax.bzl\",\n-    \"if_windows\",\n     \"py_library_providing_imports_info\",\n     \"pytype_library\",\n )\n@@ -58,35 +57,3 @@ py_library_providing_imports_info(\n     data = [\":pjrt_c_api_gpu_plugin.so\"],\n     lib_rule = pytype_library,\n )\n-\n-config_setting(\n-    name = \"disable_jaxlib_for_cpu_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"false\",\n-        \"@local_config_cuda//:enable_cuda\": \"False\",\n-    },\n-)\n-\n-config_setting(\n-    name = \"disable_jaxlib_for_cuda12_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"false\",\n-        \"@local_config_cuda//:enable_cuda\": \"True\",\n-    },\n-)\n-\n-config_setting(\n-    name = \"enable_py_import_for_cpu_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"wheel\",\n-        \"@local_config_cuda//:enable_cuda\": \"False\",\n-    },\n-)\n-\n-config_setting(\n-    name = \"enable_py_import_for_cuda12_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"wheel\",\n-        \"@local_config_cuda//:enable_cuda\": \"True\",\n-    },\n-)\ndiff --git a/jax_plugins/rocm/BUILD.bazel b/jax_plugins/rocm/BUILD.bazel\nindex 15e9e627830e..7ee0726e7960 100644\n--- a/jax_plugins/rocm/BUILD.bazel\n+++ b/jax_plugins/rocm/BUILD.bazel\n@@ -16,7 +16,6 @@ licenses([\"notice\"])\n \n load(\n   \"//jaxlib:jax.bzl\",\n-  \"if_windows\",\n   \"py_library_providing_imports_info\",\n   \"pytype_library\",\n )\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex a48c44f406f2..1d4e24720c2e 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -167,70 +167,36 @@ def if_building_jaxlib(\n       if_building: the source code targets to depend on in case we don't depend on the jaxlib wheels\n       if_not_building: the wheels to depend on if we are not depending directly on //jaxlib.\n     \"\"\"\n-\n     return select({\n         \"//jax:config_build_jaxlib_true\": if_building,\n         \"//jax:config_build_jaxlib_false\": if_not_building,\n         \"//jax:config_build_jaxlib_wheel\": [],\n     })\n \n-def _get_test_deps(deps, backend_independent):\n-    \"\"\"Returns the test deps for the given backend.\n-\n-    Args:\n-      deps: the full list of test dependencies\n-      backend_independent: whether the test is backend independent\n+def _cpu_test_deps():\n+    \"\"\"Returns the test depencies needed for a CPU-only JAX test.\"\"\"\n+    return select({\n+        \"//jax:config_build_jaxlib_true\": [],\n+        \"//jax:config_build_jaxlib_false\": [\"@pypi//jaxlib\"],\n+        \"//jax:config_build_jaxlib_wheel\": [\"//jaxlib/tools:jaxlib_py_import\"],\n+    })\n \n-    Returns:\n-      A list of test deps for the given backend.\n-        For CPU builds:\n-          If --//jax:build_jaxlib=true, returns pypi test deps.\n-          If --//jax:build_jaxlib=false, returns jaxlib pypi wheel dep and pypi test deps.\n-          If --//jax:build_jaxlib=wheel, returns jaxlib py_import dep and pypi test deps.\n-        For GPU builds:\n-          If --//jax:build_jaxlib=true, returns pypi test deps and gpu build deps.\n-          If --//jax:build_jaxlib=false, returns jaxlib, jax-cuda-plugin,\n-            jax-cuda-pjrt pypi wheel deps and pypi test deps.\n-          If --//jax:build_jaxlib=wheel, returns jaxlib,\n-            jax-cuda-plugin, jax-cuda-pjrt py_import deps and pypi test deps.\n-    \"\"\"\n-    gpu_build_deps = [\n-        \"//jaxlib/cuda:gpu_only_test_deps\",\n-        \"//jaxlib/rocm:gpu_only_test_deps\",\n-        \"//jax_plugins:gpu_plugin_only_test_deps\",\n-    ]\n-    pypi_test_deps = [d for d in deps if d.startswith(\"@pypi//\")]\n-\n-    gpu_py_imports = [\n-        \"//jaxlib/tools:jaxlib_py_import\",\n-        \"//jaxlib/tools:jax_cuda_plugin_py_import\",\n-        \"//jaxlib/tools:jax_cuda_pjrt_py_import\",\n-    ] + pypi_test_deps\n-    cpu_py_imports = [\n-        \"//jaxlib/tools:jaxlib_py_import\",\n-    ] + pypi_test_deps\n-    jaxlib_pypi_wheel_deps = [\n-        \"@pypi//jaxlib\",\n-    ] + pypi_test_deps\n-\n-    if backend_independent:\n-        test_deps = pypi_test_deps\n-        gpu_pypi_wheel_deps = jaxlib_pypi_wheel_deps\n-        gpu_py_import_deps = cpu_py_imports\n-    else:\n-        test_deps = gpu_build_deps + pypi_test_deps\n-        gpu_pypi_wheel_deps = jaxlib_pypi_wheel_deps + [\n+def _gpu_test_deps():\n+    \"\"\"Returns the additional dependencies needed for a GPU test.\"\"\"\n+    return select({\n+        \"//jax:config_build_jaxlib_true\": [\n+            \"//jaxlib/cuda:gpu_only_test_deps\",\n+            \"//jaxlib/rocm:gpu_only_test_deps\",\n+            \"//jax_plugins:gpu_plugin_only_test_deps\",\n+        ],\n+        \"//jax:config_build_jaxlib_false\": [\n             \"@pypi//jax_cuda12_plugin\",\n             \"@pypi//jax_cuda12_pjrt\",\n-        ]\n-        gpu_py_import_deps = gpu_py_imports\n-\n-    return select({\n-        \"//jax:config_build_jaxlib_true\": test_deps,\n-        \"//jax_plugins/cuda:disable_jaxlib_for_cpu_build\": jaxlib_pypi_wheel_deps,\n-        \"//jax_plugins/cuda:disable_jaxlib_for_cuda12_build\": gpu_pypi_wheel_deps,\n-        \"//jax_plugins/cuda:enable_py_import_for_cpu_build\": cpu_py_imports,\n-        \"//jax_plugins/cuda:enable_py_import_for_cuda12_build\": gpu_py_import_deps,\n+        ],\n+        \"//jax:config_build_jaxlib_wheel\": [\n+            \"//jaxlib/tools:jax_cuda_plugin_py_import\",\n+            \"//jaxlib/tools:jax_cuda_pjrt_py_import\",\n+        ],\n     })\n \n def _get_jax_test_deps(deps):\n@@ -246,28 +212,23 @@ def _get_jax_test_deps(deps):\n       If --//jax:build_jax=false, returns jax pypi wheel dep and transitive pypi test deps.\n       If --//jax:build_jax=wheel, returns jax py_import dep and transitive pypi test deps.\n     \"\"\"\n-    jax_build_deps = [d for d in deps if not d.startswith(\"@pypi//\")]\n+    non_pypi_deps = [d for d in deps if not d.startswith(\"@pypi//\")]\n \n     # A lot of tests don't have explicit dependencies on scipy, ml_dtypes, etc. But the tests\n     # transitively depends on them via //jax. So we need to make sure that these dependencies are\n     # included in the test when JAX is built from source.\n-    jax_transitive_pypi_test_deps = {k: \"true\" for k in py_deps([\n+    pypi_deps = depset([d for d in deps if d.startswith(\"@pypi//\")])\n+    pypi_deps = depset(py_deps([\n         \"ml_dtypes\",\n         \"scipy\",\n         \"opt_einsum\",\n         \"flatbuffers\",\n-    ])}\n+    ]), transitive = [pypi_deps]).to_list()\n \n-    # Remove the pypi deps that are already provided by _get_test_deps().\n-    for d in deps:\n-        if d.startswith(\"@pypi//\") and jax_transitive_pypi_test_deps.get(d):\n-            jax_transitive_pypi_test_deps.pop(d)\n-    return select({\n-        \"//jax:disable_jaxlib_and_jax_build\": [\"//:jax_wheel_with_internal_test_util\"] +\n-                                              jax_transitive_pypi_test_deps.keys(),\n-        \"//jax:enable_jaxlib_and_jax_py_import\": [\"//:jax_py_import\"] +\n-                                                 jax_transitive_pypi_test_deps.keys(),\n-        \"//conditions:default\": jax_build_deps + jax_transitive_pypi_test_deps.keys(),\n+    return pypi_deps + select({\n+        \"//jax:config_build_jax_false\": [\"//:jax_wheel_with_internal_test_util\"],\n+        \"//jax:config_build_jax_wheel\": [\"//:jax_py_import\"],\n+        \"//jax:config_build_jax_true\": non_pypi_deps,\n     })\n \n # buildifier: disable=function-docstring\n@@ -316,18 +277,21 @@ def jax_multiplatform_test(\n         test_tags = list(tags) + [\"jax_test_%s\" % backend] + backend_tags.get(backend, [])\n         if enable_backends != None and backend not in enable_backends and not any([config.startswith(backend) for config in enable_configs]):\n             test_tags.append(\"manual\")\n+        test_deps = _cpu_test_deps() + _get_jax_test_deps([\n+            \"//jax\",\n+            \"//jax:test_util\",\n+        ] + deps)\n         if backend == \"gpu\":\n+            test_deps += _gpu_test_deps()\n             test_tags += tf_cuda_tests_tags()\n+        elif backend == \"tpu\":\n+            test_deps += [\"@pypi//libtpu\"]\n         native.py_test(\n             name = name + \"_\" + backend,\n             srcs = srcs,\n             args = test_args,\n             env = env,\n-            deps = _get_test_deps(deps, backend_independent = False) +\n-                   _get_jax_test_deps([\n-                       \"//jax\",\n-                       \"//jax:test_util\",\n-                   ] + deps),\n+            deps = test_deps,\n             data = data,\n             shard_count = test_shards,\n             tags = test_tags,\n@@ -620,13 +584,13 @@ def jax_py_test(\n     env = dict(env)\n     env.setdefault(\"PYTHONWARNINGS\", \"error\")\n     deps = kwargs.get(\"deps\", [])\n-    test_deps = _get_test_deps(deps, backend_independent = True) + _get_jax_test_deps(deps)\n+    test_deps = _cpu_test_deps() + _get_jax_test_deps(deps)\n     kwargs[\"deps\"] = test_deps\n     py_test(name = name, env = env, **kwargs)\n \n def pytype_test(name, **kwargs):\n     deps = kwargs.get(\"deps\", [])\n-    test_deps = _get_test_deps(deps, backend_independent = True) + _get_jax_test_deps(deps)\n+    test_deps = _cpu_test_deps() + _get_jax_test_deps(deps)\n     kwargs[\"deps\"] = test_deps\n     native.py_test(name = name, **kwargs)\n \n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Merge pull request #28748 from dfm:scan-lin\n\nPiperOrigin-RevId: 758806795",
        "output": "```diff\nCommit: 43e3385f297287f358ec46a5b99b6a2d2f046b54\nDate: 2025-05-14T20:18:51Z\nURL: https://github.com/jax-ml/jax/commit/43e3385f297287f358ec46a5b99b6a2d2f046b54\nFiles changed: 2\nAdditions: +116, Deletions: -0\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 05e9c010dc51..c85a23b6b199 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -683,6 +683,108 @@ def _scan_jvp(primals, tangents, reverse, length, jaxpr, num_consts, num_carry,\n                   for p, nz in zip(primals_out, nonzeros_out)]\n   return primals_out, tangents_out\n \n+def _scan_linearization(nzs, *primals_in, reverse: bool, length: int,\n+                        num_consts: int, num_carry: int,\n+                        jaxpr: core.ClosedJaxpr, linear: Sequence[bool],\n+                        unroll: int, _split_transpose: bool):\n+  const_nz, init_nz, xs_nz = split_list(nzs, [num_consts, num_carry])\n+  carry_nz = init_nz\n+  for _ in range(1 + num_carry):\n+    nzs = const_nz + carry_nz + xs_nz\n+    primal_jaxpr, num_res, nzs_out, tangent_jaxpr = ad.linearize_jaxpr(jaxpr, nzs)\n+    carry_nz_out = nzs_out[:num_carry]\n+    if carry_nz_out == carry_nz:\n+      break\n+    else:\n+      carry_nz = _map(operator.or_, carry_nz, carry_nz_out)\n+  else:\n+    assert False, \"Fixpoint not reached\"\n+\n+  # The linearize_jaxpr function produces primal_jaxpr with num_res residuals\n+  # output at the front, and tangent_jaxpr with num_res residuals input at the\n+  # back. We could move all the residuals to the back and treat them as\n+  # extensive outputs, but this would be wasteful for residuals that are\n+  # loop invariant, or forwarded extensive inputs.\n+\n+  # First, for residuals that are forwarded constants, we move those to the\n+  # front in the tangent_jaxpr to treat them as intensive inputs.\n+  in_fwd = pe._jaxpr_forwarding(primal_jaxpr.jaxpr)\n+  primal_jaxpr, tangent_jaxpr, intensive_res, in_fwd = _const_to_intensive_res_forwarding(\n+      primal_jaxpr, tangent_jaxpr, num_res, num_consts, primals_in, in_fwd)\n+  num_intensive_res = len(intensive_res)\n+  num_res -= num_intensive_res\n+\n+  # After pruning the intensive residuals, the rest get moved to the back and\n+  # handled as extensive outputs from the primal.\n+  num_out = len(nzs_out)\n+  primal_jaxpr = pe.move_outvars_to_back(\n+      primal_jaxpr, [True] * num_res + [False] * num_out)\n+  in_fwd = in_fwd[num_res:] + in_fwd[:num_res]\n+\n+  # Then, any residuals or other extensive outputs that are forwarded extensive\n+  # inputs, we remove them from the primal jaxpr, and manually forward them.\n+  in_fwd = [in_idx if out_idx >= num_carry and in_idx is not None and\n+            in_idx >= num_consts + num_carry else None\n+            for out_idx, in_idx in enumerate(in_fwd)]\n+  primal_jaxpr = pe.prune_closed_jaxpr_outputs(primal_jaxpr,\n+                                               [i is None for i in in_fwd])\n+\n+  out = scan_p.bind(*primals_in, jaxpr=primal_jaxpr, reverse=reverse,\n+                    length=length, num_consts=num_consts, num_carry=num_carry,\n+                    linear=linear, unroll=unroll, _split_transpose=_split_transpose)\n+  out_ = iter(out)\n+  all_out = [next(out_) if f is None else _maybe_put(primals_in[f]) for f in in_fwd]\n+  assert next(out_, None) is None\n+  primals_out, extensive_res = split_list(all_out, [len(all_out) - num_res])\n+  res = [*intensive_res, *extensive_res]\n+\n+  def tangent_fun(res, *tangents):\n+    intensive_res, extensive_res = split_list(res, [num_intensive_res])\n+    nz_tangents = [ad.instantiate_zeros(x) for nz, x in zip(nzs, tangents) if nz]\n+    tangent_linear = (\n+        (False,) * len(intensive_res) +\n+        (True,) * len(nz_tangents) +\n+        (False,) * len(extensive_res)\n+    )\n+    tangent_num_consts = len(intensive_res) + sum(nzs[:num_consts])\n+    tangent_num_carry = sum(nzs[num_consts:num_consts + num_carry])\n+    nz_tangents_out = scan_p.bind(*intensive_res, *nz_tangents, *extensive_res,\n+                                  jaxpr=tangent_jaxpr,\n+                                  reverse=reverse, length=length,\n+                                  num_consts=tangent_num_consts,\n+                                  num_carry=tangent_num_carry,\n+                                  linear=tangent_linear, unroll=unroll,\n+                                  _split_transpose=_split_transpose)\n+    tangent_avals_out = [v.aval.to_tangent_aval() for v in jaxpr.jaxpr.outvars]\n+    nz_tangents_out_ = iter(nz_tangents_out)\n+    tangents_out = [next(nz_tangents_out_) if nz else ad.Zero(aval)\n+                    for aval, nz in zip(tangent_avals_out, nzs_out)]\n+    assert next(nz_tangents_out_, None) is None\n+    return tangents_out\n+\n+  return primals_out, nzs_out, res, tangent_fun\n+\n+def _const_to_intensive_res_forwarding(\n+    primal_jaxpr: core.ClosedJaxpr,\n+    tangent_jaxpr: core.ClosedJaxpr,\n+    num_res: int,\n+    num_consts: int,\n+    primals_in: Sequence[Any],\n+    in_fwd: list[int | None]\n+) -> tuple[core.ClosedJaxpr, core.ClosedJaxpr, list[Any], list[int | None]]:\n+  const_to_res = [in_idx if in_idx is not None and in_idx < num_consts else None\n+                  for in_idx in in_fwd[:num_res]]\n+  new_in_fwd = [f for c, f in zip(const_to_res, in_fwd[:num_res]) if c is None]\n+  new_in_fwd += in_fwd[num_res:]\n+  intensive_res = [primals_in[f] for f in const_to_res if f is not None]\n+  num_out = len(primal_jaxpr.out_avals) - num_res\n+  primal_jaxpr = pe.prune_closed_jaxpr_outputs(\n+      primal_jaxpr, [i is None for i in const_to_res] + [True] * num_out)\n+  num_nz = len(tangent_jaxpr.in_avals) - num_res\n+  tangent_jaxpr = pe.move_binders_to_front(\n+      tangent_jaxpr, [False] * num_nz + [i is not None for i in const_to_res])\n+  return primal_jaxpr, tangent_jaxpr, intensive_res, new_in_fwd\n+\n def _scan_partial_eval(trace, *tracers, reverse: bool,\n                        length: int, num_consts: int, num_carry: int,\n                        jaxpr: core.ClosedJaxpr, linear: Sequence[bool],\n@@ -1385,6 +1487,7 @@ def arrange_jaxpr_args_for_wrapped(args):\n scan_p.def_effectful_abstract_eval(_scan_abstract_eval)\n ad.primitive_jvps[scan_p] = _scan_jvp\n ad.primitive_transposes[scan_p] = _scan_transpose\n+ad.primitive_linearizations[scan_p] = _scan_linearization\n pe.custom_partial_eval_rules[scan_p] = _scan_partial_eval\n xla.register_initial_style_primitive(scan_p)\n mlir.register_lowering(scan_p,\ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex d32d761ee1fa..54dff47fea32 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -28,6 +28,7 @@\n \n import jax\n from jax._src import core\n+from jax._src import config\n from jax import dtypes\n from jax import lax\n from jax import random\n@@ -3298,6 +3299,18 @@ def body_fun(c, _):\n     outs_ref = body_fun(body_fun(init_vals, [x[0] for x in xs])[0], [x[1] for x in xs])[0]\n     self.assertAllClose(outs, outs_ref, check_dtypes=False)\n \n+  def test_scan_diff_of_print(self):\n+    # ref: https://github.com/jax-ml/jax/issues/28738\n+    def f(c, _):\n+      jax.debug.print(\"c = {c}\", c=c, ordered=True)\n+      return c + 1, None\n+    def g(x):\n+      return jax.lax.scan(f, x, length=2)[0]\n+    with config.use_direct_linearize(True):\n+      jaxpr = jax.make_jaxpr(jax.value_and_grad(g))(1.0)\n+    eqn_jaxpr = jaxpr.eqns[0].params[\"jaxpr\"]\n+    self.assertIn(\"debug_callback\", [e.primitive.name for e in eqn_jaxpr.eqns])\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28751 from hawkinsp:plugins\n\nPiperOrigin-RevId: 758801504",
        "output": "```diff\nCommit: db99793ace5034ceaf93c840a2764376a247a1a1\nDate: 2025-05-14T20:07:06Z\nURL: https://github.com/jax-ml/jax/commit/db99793ace5034ceaf93c840a2764376a247a1a1\nFiles changed: 8\nAdditions: +52, Deletions: -41\ndiff --git a/jax_plugins/cuda/BUILD.bazel b/jax_plugins/cuda/BUILD.bazel\nindex 6566cfc62b0c..c3c20f536cff 100644\n--- a/jax_plugins/cuda/BUILD.bazel\n+++ b/jax_plugins/cuda/BUILD.bazel\n@@ -34,15 +34,28 @@ exports_files([\n     \"setup.py\",\n ])\n \n+cc_binary(\n+    name = \"pjrt_c_api_gpu_plugin.so\",\n+    linkopts = [\n+        \"-Wl,--version-script,$(location :gpu_version_script.lds)\",\n+        \"-Wl,--no-undefined\",\n+    ],\n+    linkshared = True,\n+    deps = [\n+        \":gpu_version_script.lds\",\n+        \"//jaxlib/mosaic/gpu:custom_call\",\n+        \"@xla//xla/pjrt/c:pjrt_c_api_gpu\",\n+        \"@xla//xla/service:gpu_plugin\",\n+        \"@xla//xla/stream_executor:cuda_platform\",\n+    ],\n+)\n+\n py_library_providing_imports_info(\n     name = \"cuda_plugin\",\n     srcs = [\n         \"__init__.py\",\n     ],\n-    data = if_windows(\n-        [\"@xla//xla/pjrt/c/pjrt_c_api_gpu_plugin.pyd\"],\n-        [\"//jaxlib/tools:pjrt_c_api_gpu_plugin.so\"],\n-    ),\n+    data = [\":pjrt_c_api_gpu_plugin.so\"],\n     lib_rule = pytype_library,\n )\n \ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 4891fbeb3332..1be29326c95f 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -51,7 +51,7 @@ def _get_library_path():\n     runfiles_dir = os.getenv('RUNFILES_DIR', None)\n     if runfiles_dir:\n       local_path = os.path.join(\n-          runfiles_dir, '__main__/jaxlib/tools/pjrt_c_api_gpu_plugin.so'\n+          runfiles_dir, '__main__/jax_plugins/cuda/pjrt_c_api_gpu_plugin.so'\n       )\n \n   if os.path.exists(local_path):\ndiff --git a/jaxlib/tools/gpu_version_script.lds b/jax_plugins/cuda/gpu_version_script.lds\nsimilarity index 100%\nrename from jaxlib/tools/gpu_version_script.lds\nrename to jax_plugins/cuda/gpu_version_script.lds\ndiff --git a/jax_plugins/rocm/BUILD.bazel b/jax_plugins/rocm/BUILD.bazel\nindex 6e265bcd18cf..15e9e627830e 100644\n--- a/jax_plugins/rocm/BUILD.bazel\n+++ b/jax_plugins/rocm/BUILD.bazel\n@@ -34,14 +34,26 @@ exports_files([\n     \"setup.py\",\n ])\n \n+cc_binary(\n+    name = \"pjrt_c_api_gpu_plugin.so\",\n+    linkopts = [\n+        \"-Wl,--version-script,$(location :gpu_version_script.lds)\",\n+        \"-Wl,--no-undefined\",\n+    ],\n+    linkshared = True,\n+    deps = [\n+        \":gpu_version_script.lds\",\n+        \"@xla//xla/pjrt/c:pjrt_c_api_gpu\",\n+        \"@xla//xla/service:gpu_plugin\",\n+        \"@xla//xla/stream_executor:rocm_platform\",\n+    ],\n+)\n+\n py_library_providing_imports_info(\n     name = \"rocm_plugin\",\n     srcs = [\n         \"__init__.py\",\n     ],\n-    data = if_windows(\n-        [\"@xla//xla/pjrt/c/pjrt_c_api_gpu_plugin.pyd\"],\n-        [\"@xla//xla/pjrt/c:pjrt_c_api_gpu_plugin.so\"],\n-    ),\n+    data = [\":pjrt_c_api_gpu_plugin.so\"],\n     lib_rule = pytype_library,\n )\ndiff --git a/jax_plugins/rocm/__init__.py b/jax_plugins/rocm/__init__.py\nindex 0b1b077acfcd..cf2a625fa783 100644\n--- a/jax_plugins/rocm/__init__.py\n+++ b/jax_plugins/rocm/__init__.py\n@@ -51,7 +51,7 @@ def _get_library_path():\n     runfiles_dir = os.getenv('RUNFILES_DIR', None)\n     if runfiles_dir:\n       local_path = pathlib.Path(\n-          os.path.join(runfiles_dir, 'xla/xla/pjrt/c/pjrt_c_api_gpu_plugin.so')\n+          os.path.join(runfiles_dir, '__main__/jax_plugins/rocm/pjrt_c_api_gpu_plugin.so')\n       )\n \n   if local_path.exists():\ndiff --git a/jax_plugins/rocm/gpu_version_script.lds b/jax_plugins/rocm/gpu_version_script.lds\nnew file mode 100644\nindex 000000000000..cbac4549bde3\n--- /dev/null\n+++ b/jax_plugins/rocm/gpu_version_script.lds\n@@ -0,0 +1,9 @@\n+VERS_1.0 {\n+  global:\n+    extern \"C\" {\n+      GetPjrtApi;\n+    };\n+\n+  local:\n+    *;\n+};\ndiff --git a/jaxlib/tools/BUILD.bazel b/jaxlib/tools/BUILD.bazel\nindex 219096836ffc..22bae26a4420 100644\n--- a/jaxlib/tools/BUILD.bazel\n+++ b/jaxlib/tools/BUILD.bazel\n@@ -64,10 +64,10 @@ py_binary(\n         \"LICENSE.txt\",\n         \"//jaxlib\",\n         \"//jaxlib:README.md\",\n+        \"//jaxlib:_jax\",\n         \"//jaxlib:jaxlib_binaries\",\n         \"//jaxlib:setup.py\",\n         \"//jaxlib:xla_client.py\",\n-        \"//jaxlib:_jax\",\n         \"@xla//xla/ffi/api:api.h\",\n         \"@xla//xla/ffi/api:c_api.h\",\n         \"@xla//xla/ffi/api:ffi.h\",\n@@ -90,35 +90,15 @@ jax_py_test(\n     ],\n )\n \n-cc_binary(\n-    name = \"pjrt_c_api_gpu_plugin.so\",\n-    linkopts = [\n-        \"-Wl,--version-script,$(location :gpu_version_script.lds)\",\n-        \"-Wl,--no-undefined\",\n-    ],\n-    linkshared = True,\n-    deps = [\n-        \":gpu_version_script.lds\",\n-        \"@xla//xla/pjrt/c:pjrt_c_api_gpu\",\n-        \"@xla//xla/pjrt/c:pjrt_c_api_gpu_version_script.lds\",\n-        \"@xla//xla/service:gpu_plugin\",\n-    ] + if_cuda([\n-        \"//jaxlib/mosaic/gpu:custom_call\",\n-        \"@xla//xla/stream_executor:cuda_platform\",\n-    ]) + if_rocm([\n-        \"@xla//xla/stream_executor:rocm_platform\",\n-    ]),\n-)\n-\n py_binary(\n     name = \"build_gpu_plugin_wheel\",\n     srcs = [\"build_gpu_plugin_wheel.py\"],\n     data = [\n         \"LICENSE.txt\",\n-        \":pjrt_c_api_gpu_plugin.so\",\n     ] + if_cuda([\n         \"//jaxlib:version\",\n         \"//jaxlib/cuda:cuda_gpu_support\",\n+        \"//jax_plugins/cuda:pjrt_c_api_gpu_plugin.so\",\n         \"//jax_plugins/cuda:pyproject.toml\",\n         \"//jax_plugins/cuda:setup.py\",\n         \"//jax_plugins/cuda:__init__.py\",\n@@ -126,6 +106,7 @@ py_binary(\n     ]) + if_rocm([\n         \"//jaxlib:version\",\n         \"//jaxlib/rocm:rocm_gpu_support\",\n+        \"//jax_plugins/rocm:pjrt_c_api_gpu_plugin.so\",\n         \"//jax_plugins/rocm:pyproject.toml\",\n         \"//jax_plugins/rocm:setup.py\",\n         \"//jax_plugins/rocm:__init__.py\",\n@@ -387,10 +368,6 @@ jax_wheel(\n )\n \n # JAX PJRT wheel targets.\n-pytype_strict_library(\n-    name = \"pjrt_c_api_gpu_plugin_so\",\n-    data = [\":pjrt_c_api_gpu_plugin.so\"],\n-)\n \n py_binary(\n     name = \"build_gpu_plugin_wheel_tool\",\n@@ -407,12 +384,12 @@ py_binary(\n \n wheel_sources(\n     name = \"jax_pjrt_sources\",\n-    data_srcs = [\n-        \":pjrt_c_api_gpu_plugin_so\",\n-    ] + if_cuda([\n+    data_srcs = if_cuda([\n+        \"//jax_plugins/cuda:cuda_plugin\",\n         \"//jaxlib/cuda:cuda_gpu_support\",\n         \"@local_config_cuda//cuda:cuda-nvvm\",\n     ]) + if_rocm([\n+        \"//jax_plugins/rocm:rocm_plugin\",\n         \"//jaxlib/rocm:rocm_gpu_support\",\n     ]),\n     py_srcs = [\ndiff --git a/jaxlib/tools/build_gpu_plugin_wheel.py b/jaxlib/tools/build_gpu_plugin_wheel.py\nindex 337bedab4591..68e08d89338e 100644\n--- a/jaxlib/tools/build_gpu_plugin_wheel.py\n+++ b/jaxlib/tools/build_gpu_plugin_wheel.py\n@@ -120,7 +120,7 @@ def prepare_cuda_plugin_wheel(\n       ],\n   )\n   copy_files(\n-      f\"{source_file_prefix}jaxlib/tools/pjrt_c_api_gpu_plugin.so\",\n+      f\"{source_file_prefix}jax_plugins/cuda/pjrt_c_api_gpu_plugin.so\",\n       dst_dir=plugin_dir,\n       dst_filename=\"xla_cuda_plugin.so\",\n   )\n@@ -158,7 +158,7 @@ def prepare_rocm_plugin_wheel(\n       ],\n   )\n   copy_files(\n-      f\"{source_file_prefix}jaxlib/tools/pjrt_c_api_gpu_plugin.so\",\n+      f\"{source_file_prefix}jax_plugins/rocm/pjrt_c_api_gpu_plugin.so\",\n       dst_dir=plugin_dir,\n       dst_filename=\"xla_rocm_plugin.so\",\n   )\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add a linearization rule for scan.",
        "output": "```diff\nCommit: f0e00a6658709067951446868894ea300b365c8b\nDate: 2025-05-14T19:33:09Z\nURL: https://github.com/jax-ml/jax/commit/f0e00a6658709067951446868894ea300b365c8b\nFiles changed: 2\nAdditions: +116, Deletions: -0\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 05e9c010dc51..c85a23b6b199 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -683,6 +683,108 @@ def _scan_jvp(primals, tangents, reverse, length, jaxpr, num_consts, num_carry,\n                   for p, nz in zip(primals_out, nonzeros_out)]\n   return primals_out, tangents_out\n \n+def _scan_linearization(nzs, *primals_in, reverse: bool, length: int,\n+                        num_consts: int, num_carry: int,\n+                        jaxpr: core.ClosedJaxpr, linear: Sequence[bool],\n+                        unroll: int, _split_transpose: bool):\n+  const_nz, init_nz, xs_nz = split_list(nzs, [num_consts, num_carry])\n+  carry_nz = init_nz\n+  for _ in range(1 + num_carry):\n+    nzs = const_nz + carry_nz + xs_nz\n+    primal_jaxpr, num_res, nzs_out, tangent_jaxpr = ad.linearize_jaxpr(jaxpr, nzs)\n+    carry_nz_out = nzs_out[:num_carry]\n+    if carry_nz_out == carry_nz:\n+      break\n+    else:\n+      carry_nz = _map(operator.or_, carry_nz, carry_nz_out)\n+  else:\n+    assert False, \"Fixpoint not reached\"\n+\n+  # The linearize_jaxpr function produces primal_jaxpr with num_res residuals\n+  # output at the front, and tangent_jaxpr with num_res residuals input at the\n+  # back. We could move all the residuals to the back and treat them as\n+  # extensive outputs, but this would be wasteful for residuals that are\n+  # loop invariant, or forwarded extensive inputs.\n+\n+  # First, for residuals that are forwarded constants, we move those to the\n+  # front in the tangent_jaxpr to treat them as intensive inputs.\n+  in_fwd = pe._jaxpr_forwarding(primal_jaxpr.jaxpr)\n+  primal_jaxpr, tangent_jaxpr, intensive_res, in_fwd = _const_to_intensive_res_forwarding(\n+      primal_jaxpr, tangent_jaxpr, num_res, num_consts, primals_in, in_fwd)\n+  num_intensive_res = len(intensive_res)\n+  num_res -= num_intensive_res\n+\n+  # After pruning the intensive residuals, the rest get moved to the back and\n+  # handled as extensive outputs from the primal.\n+  num_out = len(nzs_out)\n+  primal_jaxpr = pe.move_outvars_to_back(\n+      primal_jaxpr, [True] * num_res + [False] * num_out)\n+  in_fwd = in_fwd[num_res:] + in_fwd[:num_res]\n+\n+  # Then, any residuals or other extensive outputs that are forwarded extensive\n+  # inputs, we remove them from the primal jaxpr, and manually forward them.\n+  in_fwd = [in_idx if out_idx >= num_carry and in_idx is not None and\n+            in_idx >= num_consts + num_carry else None\n+            for out_idx, in_idx in enumerate(in_fwd)]\n+  primal_jaxpr = pe.prune_closed_jaxpr_outputs(primal_jaxpr,\n+                                               [i is None for i in in_fwd])\n+\n+  out = scan_p.bind(*primals_in, jaxpr=primal_jaxpr, reverse=reverse,\n+                    length=length, num_consts=num_consts, num_carry=num_carry,\n+                    linear=linear, unroll=unroll, _split_transpose=_split_transpose)\n+  out_ = iter(out)\n+  all_out = [next(out_) if f is None else _maybe_put(primals_in[f]) for f in in_fwd]\n+  assert next(out_, None) is None\n+  primals_out, extensive_res = split_list(all_out, [len(all_out) - num_res])\n+  res = [*intensive_res, *extensive_res]\n+\n+  def tangent_fun(res, *tangents):\n+    intensive_res, extensive_res = split_list(res, [num_intensive_res])\n+    nz_tangents = [ad.instantiate_zeros(x) for nz, x in zip(nzs, tangents) if nz]\n+    tangent_linear = (\n+        (False,) * len(intensive_res) +\n+        (True,) * len(nz_tangents) +\n+        (False,) * len(extensive_res)\n+    )\n+    tangent_num_consts = len(intensive_res) + sum(nzs[:num_consts])\n+    tangent_num_carry = sum(nzs[num_consts:num_consts + num_carry])\n+    nz_tangents_out = scan_p.bind(*intensive_res, *nz_tangents, *extensive_res,\n+                                  jaxpr=tangent_jaxpr,\n+                                  reverse=reverse, length=length,\n+                                  num_consts=tangent_num_consts,\n+                                  num_carry=tangent_num_carry,\n+                                  linear=tangent_linear, unroll=unroll,\n+                                  _split_transpose=_split_transpose)\n+    tangent_avals_out = [v.aval.to_tangent_aval() for v in jaxpr.jaxpr.outvars]\n+    nz_tangents_out_ = iter(nz_tangents_out)\n+    tangents_out = [next(nz_tangents_out_) if nz else ad.Zero(aval)\n+                    for aval, nz in zip(tangent_avals_out, nzs_out)]\n+    assert next(nz_tangents_out_, None) is None\n+    return tangents_out\n+\n+  return primals_out, nzs_out, res, tangent_fun\n+\n+def _const_to_intensive_res_forwarding(\n+    primal_jaxpr: core.ClosedJaxpr,\n+    tangent_jaxpr: core.ClosedJaxpr,\n+    num_res: int,\n+    num_consts: int,\n+    primals_in: Sequence[Any],\n+    in_fwd: list[int | None]\n+) -> tuple[core.ClosedJaxpr, core.ClosedJaxpr, list[Any], list[int | None]]:\n+  const_to_res = [in_idx if in_idx is not None and in_idx < num_consts else None\n+                  for in_idx in in_fwd[:num_res]]\n+  new_in_fwd = [f for c, f in zip(const_to_res, in_fwd[:num_res]) if c is None]\n+  new_in_fwd += in_fwd[num_res:]\n+  intensive_res = [primals_in[f] for f in const_to_res if f is not None]\n+  num_out = len(primal_jaxpr.out_avals) - num_res\n+  primal_jaxpr = pe.prune_closed_jaxpr_outputs(\n+      primal_jaxpr, [i is None for i in const_to_res] + [True] * num_out)\n+  num_nz = len(tangent_jaxpr.in_avals) - num_res\n+  tangent_jaxpr = pe.move_binders_to_front(\n+      tangent_jaxpr, [False] * num_nz + [i is not None for i in const_to_res])\n+  return primal_jaxpr, tangent_jaxpr, intensive_res, new_in_fwd\n+\n def _scan_partial_eval(trace, *tracers, reverse: bool,\n                        length: int, num_consts: int, num_carry: int,\n                        jaxpr: core.ClosedJaxpr, linear: Sequence[bool],\n@@ -1385,6 +1487,7 @@ def arrange_jaxpr_args_for_wrapped(args):\n scan_p.def_effectful_abstract_eval(_scan_abstract_eval)\n ad.primitive_jvps[scan_p] = _scan_jvp\n ad.primitive_transposes[scan_p] = _scan_transpose\n+ad.primitive_linearizations[scan_p] = _scan_linearization\n pe.custom_partial_eval_rules[scan_p] = _scan_partial_eval\n xla.register_initial_style_primitive(scan_p)\n mlir.register_lowering(scan_p,\ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex d32d761ee1fa..54dff47fea32 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -28,6 +28,7 @@\n \n import jax\n from jax._src import core\n+from jax._src import config\n from jax import dtypes\n from jax import lax\n from jax import random\n@@ -3298,6 +3299,18 @@ def body_fun(c, _):\n     outs_ref = body_fun(body_fun(init_vals, [x[0] for x in xs])[0], [x[1] for x in xs])[0]\n     self.assertAllClose(outs, outs_ref, check_dtypes=False)\n \n+  def test_scan_diff_of_print(self):\n+    # ref: https://github.com/jax-ml/jax/issues/28738\n+    def f(c, _):\n+      jax.debug.print(\"c = {c}\", c=c, ordered=True)\n+      return c + 1, None\n+    def g(x):\n+      return jax.lax.scan(f, x, length=2)[0]\n+    with config.use_direct_linearize(True):\n+      jaxpr = jax.make_jaxpr(jax.value_and_grad(g))(1.0)\n+    eqn_jaxpr = jaxpr.eqns[0].params[\"jaxpr\"]\n+    self.assertIn(\"debug_callback\", [e.primitive.name for e in eqn_jaxpr.eqns])\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```",
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Move definition of pjrt_c_api_gpu_plugin.so into jax_plugins/{cuda,rocm}\n\nThis is simpler and should work better for a bazel submodule.",
        "output": "```diff\nCommit: 183425cf05da44cb5496db3046409af812747e5d\nDate: 2025-05-14T19:30:52Z\nURL: https://github.com/jax-ml/jax/commit/183425cf05da44cb5496db3046409af812747e5d\nFiles changed: 8\nAdditions: +52, Deletions: -41\ndiff --git a/jax_plugins/cuda/BUILD.bazel b/jax_plugins/cuda/BUILD.bazel\nindex 6566cfc62b0c..c3c20f536cff 100644\n--- a/jax_plugins/cuda/BUILD.bazel\n+++ b/jax_plugins/cuda/BUILD.bazel\n@@ -34,15 +34,28 @@ exports_files([\n     \"setup.py\",\n ])\n \n+cc_binary(\n+    name = \"pjrt_c_api_gpu_plugin.so\",\n+    linkopts = [\n+        \"-Wl,--version-script,$(location :gpu_version_script.lds)\",\n+        \"-Wl,--no-undefined\",\n+    ],\n+    linkshared = True,\n+    deps = [\n+        \":gpu_version_script.lds\",\n+        \"//jaxlib/mosaic/gpu:custom_call\",\n+        \"@xla//xla/pjrt/c:pjrt_c_api_gpu\",\n+        \"@xla//xla/service:gpu_plugin\",\n+        \"@xla//xla/stream_executor:cuda_platform\",\n+    ],\n+)\n+\n py_library_providing_imports_info(\n     name = \"cuda_plugin\",\n     srcs = [\n         \"__init__.py\",\n     ],\n-    data = if_windows(\n-        [\"@xla//xla/pjrt/c/pjrt_c_api_gpu_plugin.pyd\"],\n-        [\"//jaxlib/tools:pjrt_c_api_gpu_plugin.so\"],\n-    ),\n+    data = [\":pjrt_c_api_gpu_plugin.so\"],\n     lib_rule = pytype_library,\n )\n \ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 4891fbeb3332..1be29326c95f 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -51,7 +51,7 @@ def _get_library_path():\n     runfiles_dir = os.getenv('RUNFILES_DIR', None)\n     if runfiles_dir:\n       local_path = os.path.join(\n-          runfiles_dir, '__main__/jaxlib/tools/pjrt_c_api_gpu_plugin.so'\n+          runfiles_dir, '__main__/jax_plugins/cuda/pjrt_c_api_gpu_plugin.so'\n       )\n \n   if os.path.exists(local_path):\ndiff --git a/jaxlib/tools/gpu_version_script.lds b/jax_plugins/cuda/gpu_version_script.lds\nsimilarity index 100%\nrename from jaxlib/tools/gpu_version_script.lds\nrename to jax_plugins/cuda/gpu_version_script.lds\ndiff --git a/jax_plugins/rocm/BUILD.bazel b/jax_plugins/rocm/BUILD.bazel\nindex 6e265bcd18cf..15e9e627830e 100644\n--- a/jax_plugins/rocm/BUILD.bazel\n+++ b/jax_plugins/rocm/BUILD.bazel\n@@ -34,14 +34,26 @@ exports_files([\n     \"setup.py\",\n ])\n \n+cc_binary(\n+    name = \"pjrt_c_api_gpu_plugin.so\",\n+    linkopts = [\n+        \"-Wl,--version-script,$(location :gpu_version_script.lds)\",\n+        \"-Wl,--no-undefined\",\n+    ],\n+    linkshared = True,\n+    deps = [\n+        \":gpu_version_script.lds\",\n+        \"@xla//xla/pjrt/c:pjrt_c_api_gpu\",\n+        \"@xla//xla/service:gpu_plugin\",\n+        \"@xla//xla/stream_executor:rocm_platform\",\n+    ],\n+)\n+\n py_library_providing_imports_info(\n     name = \"rocm_plugin\",\n     srcs = [\n         \"__init__.py\",\n     ],\n-    data = if_windows(\n-        [\"@xla//xla/pjrt/c/pjrt_c_api_gpu_plugin.pyd\"],\n-        [\"@xla//xla/pjrt/c:pjrt_c_api_gpu_plugin.so\"],\n-    ),\n+    data = [\":pjrt_c_api_gpu_plugin.so\"],\n     lib_rule = pytype_library,\n )\ndiff --git a/jax_plugins/rocm/__init__.py b/jax_plugins/rocm/__init__.py\nindex 0b1b077acfcd..cf2a625fa783 100644\n--- a/jax_plugins/rocm/__init__.py\n+++ b/jax_plugins/rocm/__init__.py\n@@ -51,7 +51,7 @@ def _get_library_path():\n     runfiles_dir = os.getenv('RUNFILES_DIR', None)\n     if runfiles_dir:\n       local_path = pathlib.Path(\n-          os.path.join(runfiles_dir, 'xla/xla/pjrt/c/pjrt_c_api_gpu_plugin.so')\n+          os.path.join(runfiles_dir, '__main__/jax_plugins/rocm/pjrt_c_api_gpu_plugin.so')\n       )\n \n   if local_path.exists():\ndiff --git a/jax_plugins/rocm/gpu_version_script.lds b/jax_plugins/rocm/gpu_version_script.lds\nnew file mode 100644\nindex 000000000000..cbac4549bde3\n--- /dev/null\n+++ b/jax_plugins/rocm/gpu_version_script.lds\n@@ -0,0 +1,9 @@\n+VERS_1.0 {\n+  global:\n+    extern \"C\" {\n+      GetPjrtApi;\n+    };\n+\n+  local:\n+    *;\n+};\ndiff --git a/jaxlib/tools/BUILD.bazel b/jaxlib/tools/BUILD.bazel\nindex 219096836ffc..22bae26a4420 100644\n--- a/jaxlib/tools/BUILD.bazel\n+++ b/jaxlib/tools/BUILD.bazel\n@@ -64,10 +64,10 @@ py_binary(\n         \"LICENSE.txt\",\n         \"//jaxlib\",\n         \"//jaxlib:README.md\",\n+        \"//jaxlib:_jax\",\n         \"//jaxlib:jaxlib_binaries\",\n         \"//jaxlib:setup.py\",\n         \"//jaxlib:xla_client.py\",\n-        \"//jaxlib:_jax\",\n         \"@xla//xla/ffi/api:api.h\",\n         \"@xla//xla/ffi/api:c_api.h\",\n         \"@xla//xla/ffi/api:ffi.h\",\n@@ -90,35 +90,15 @@ jax_py_test(\n     ],\n )\n \n-cc_binary(\n-    name = \"pjrt_c_api_gpu_plugin.so\",\n-    linkopts = [\n-        \"-Wl,--version-script,$(location :gpu_version_script.lds)\",\n-        \"-Wl,--no-undefined\",\n-    ],\n-    linkshared = True,\n-    deps = [\n-        \":gpu_version_script.lds\",\n-        \"@xla//xla/pjrt/c:pjrt_c_api_gpu\",\n-        \"@xla//xla/pjrt/c:pjrt_c_api_gpu_version_script.lds\",\n-        \"@xla//xla/service:gpu_plugin\",\n-    ] + if_cuda([\n-        \"//jaxlib/mosaic/gpu:custom_call\",\n-        \"@xla//xla/stream_executor:cuda_platform\",\n-    ]) + if_rocm([\n-        \"@xla//xla/stream_executor:rocm_platform\",\n-    ]),\n-)\n-\n py_binary(\n     name = \"build_gpu_plugin_wheel\",\n     srcs = [\"build_gpu_plugin_wheel.py\"],\n     data = [\n         \"LICENSE.txt\",\n-        \":pjrt_c_api_gpu_plugin.so\",\n     ] + if_cuda([\n         \"//jaxlib:version\",\n         \"//jaxlib/cuda:cuda_gpu_support\",\n+        \"//jax_plugins/cuda:pjrt_c_api_gpu_plugin.so\",\n         \"//jax_plugins/cuda:pyproject.toml\",\n         \"//jax_plugins/cuda:setup.py\",\n         \"//jax_plugins/cuda:__init__.py\",\n@@ -126,6 +106,7 @@ py_binary(\n     ]) + if_rocm([\n         \"//jaxlib:version\",\n         \"//jaxlib/rocm:rocm_gpu_support\",\n+        \"//jax_plugins/rocm:pjrt_c_api_gpu_plugin.so\",\n         \"//jax_plugins/rocm:pyproject.toml\",\n         \"//jax_plugins/rocm:setup.py\",\n         \"//jax_plugins/rocm:__init__.py\",\n@@ -387,10 +368,6 @@ jax_wheel(\n )\n \n # JAX PJRT wheel targets.\n-pytype_strict_library(\n-    name = \"pjrt_c_api_gpu_plugin_so\",\n-    data = [\":pjrt_c_api_gpu_plugin.so\"],\n-)\n \n py_binary(\n     name = \"build_gpu_plugin_wheel_tool\",\n@@ -407,12 +384,12 @@ py_binary(\n \n wheel_sources(\n     name = \"jax_pjrt_sources\",\n-    data_srcs = [\n-        \":pjrt_c_api_gpu_plugin_so\",\n-    ] + if_cuda([\n+    data_srcs = if_cuda([\n+        \"//jax_plugins/cuda:cuda_plugin\",\n         \"//jaxlib/cuda:cuda_gpu_support\",\n         \"@local_config_cuda//cuda:cuda-nvvm\",\n     ]) + if_rocm([\n+        \"//jax_plugins/rocm:rocm_plugin\",\n         \"//jaxlib/rocm:rocm_gpu_support\",\n     ]),\n     py_srcs = [\ndiff --git a/jaxlib/tools/build_gpu_plugin_wheel.py b/jaxlib/tools/build_gpu_plugin_wheel.py\nindex 337bedab4591..68e08d89338e 100644\n--- a/jaxlib/tools/build_gpu_plugin_wheel.py\n+++ b/jaxlib/tools/build_gpu_plugin_wheel.py\n@@ -120,7 +120,7 @@ def prepare_cuda_plugin_wheel(\n       ],\n   )\n   copy_files(\n-      f\"{source_file_prefix}jaxlib/tools/pjrt_c_api_gpu_plugin.so\",\n+      f\"{source_file_prefix}jax_plugins/cuda/pjrt_c_api_gpu_plugin.so\",\n       dst_dir=plugin_dir,\n       dst_filename=\"xla_cuda_plugin.so\",\n   )\n@@ -158,7 +158,7 @@ def prepare_rocm_plugin_wheel(\n       ],\n   )\n   copy_files(\n-      f\"{source_file_prefix}jaxlib/tools/pjrt_c_api_gpu_plugin.so\",\n+      f\"{source_file_prefix}jax_plugins/rocm/pjrt_c_api_gpu_plugin.so\",\n       dst_dir=plugin_dir,\n       dst_filename=\"xla_rocm_plugin.so\",\n   )\n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Merge pull request #28747 from jakevdp:canon-dtype-jax-array\n\nPiperOrigin-RevId: 758785125",
        "output": "```diff\nCommit: 5d628e0fd38d2064c9c476da8ab72fa3e24bf15f\nDate: 2025-05-14T19:20:31Z\nURL: https://github.com/jax-ml/jax/commit/5d628e0fd38d2064c9c476da8ab72fa3e24bf15f\nFiles changed: 2\nAdditions: +8, Deletions: -0\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 5f1cf1670729..6afcdf2892f4 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -1180,6 +1180,7 @@ pytype_strict_library(\n         \":abstract_arrays\",\n         \":config\",\n         \":core\",\n+        \":deprecations\",\n         \":dtypes\",\n         \":sharding_impls\",\n         \":source_info_util\",\ndiff --git a/jax/_src/interpreters/xla.py b/jax/_src/interpreters/xla.py\nindex 7fbb22923e0f..73a57f935f5d 100644\n--- a/jax/_src/interpreters/xla.py\n+++ b/jax/_src/interpreters/xla.py\n@@ -23,6 +23,7 @@\n import numpy as np\n \n from jax._src import core\n+from jax._src import deprecations\n from jax._src import dtypes\n from jax._src.abstract_arrays import numpy_scalar_types\n from jax._src.util import safe_zip, safe_map\n@@ -100,6 +101,12 @@ def canonicalize_dtype(x):\n     handler = canonicalize_dtype_handlers.get(typ)\n     if handler: return handler(x)\n   if hasattr(x, '__jax_array__'):\n+    deprecations.warn(\n+      'jax-abstract-dunder-array',\n+      ('Triggering of __jax_array__() during abstractification is deprecated.'\n+       ' To avoid this error, either explicitly convert your object using'\n+       ' jax.numpy.array(), or register your object as a pytree.'),\n+      stacklevel=6)\n     return canonicalize_dtype(x.__jax_array__())\n   raise InvalidInputException(\n       f\"Argument '{x}' of type {type(x)} is not a valid JAX type.\")\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28750 from hawkinsp:requ\n\nPiperOrigin-RevId: 758780857",
        "output": "```diff\nCommit: c8b920bab4465cb148276a49839e8a30b6aabc92\nDate: 2025-05-14T19:09:29Z\nURL: https://github.com/jax-ml/jax/commit/c8b920bab4465cb148276a49839e8a30b6aabc92\nFiles changed: 2\nAdditions: +8, Deletions: -0\ndiff --git a/WORKSPACE b/WORKSPACE\nindex 903085714e65..f389afe2263f 100644\n--- a/WORKSPACE\n+++ b/WORKSPACE\n@@ -14,6 +14,8 @@ python_init_repositories(\n     default_python_version = \"system\",\n     local_wheel_dist_folder = \"../dist\",\n     local_wheel_inclusion_list = [\n+        \"ml_dtypes*\",\n+        \"ml-dtypes*\",\n         \"numpy*\",\n         \"scipy*\",\n         \"jax-*\",\ndiff --git a/build/requirements_lock_3_14_ft.txt b/build/requirements_lock_3_14_ft.txt\nindex e50305f4fa48..6eedf149f5fa 100644\n--- a/build/requirements_lock_3_14_ft.txt\n+++ b/build/requirements_lock_3_14_ft.txt\n@@ -19,3 +19,9 @@ flatbuffers==24.12.23\n ml-dtypes==0.5.1\n \n opt-einsum==3.4.0\n+\n+build==1.2.2.post1\n+setuptools==80.0.0\n+wheel==0.45.1\n+pyproject-hooks==1.2.0\n+packaging==25.0\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add requirements needed for building wheels under Python 3.14t.\n\nAlso allow ml_dtypes as a local wheel override.",
        "output": "```diff\nCommit: e968cb6b04d31ac7a045f119de027ee17fc16e1e\nDate: 2025-05-14T18:36:57Z\nURL: https://github.com/jax-ml/jax/commit/e968cb6b04d31ac7a045f119de027ee17fc16e1e\nFiles changed: 2\nAdditions: +8, Deletions: -0\ndiff --git a/WORKSPACE b/WORKSPACE\nindex 903085714e65..f389afe2263f 100644\n--- a/WORKSPACE\n+++ b/WORKSPACE\n@@ -14,6 +14,8 @@ python_init_repositories(\n     default_python_version = \"system\",\n     local_wheel_dist_folder = \"../dist\",\n     local_wheel_inclusion_list = [\n+        \"ml_dtypes*\",\n+        \"ml-dtypes*\",\n         \"numpy*\",\n         \"scipy*\",\n         \"jax-*\",\ndiff --git a/build/requirements_lock_3_14_ft.txt b/build/requirements_lock_3_14_ft.txt\nindex e50305f4fa48..6eedf149f5fa 100644\n--- a/build/requirements_lock_3_14_ft.txt\n+++ b/build/requirements_lock_3_14_ft.txt\n@@ -19,3 +19,9 @@ flatbuffers==24.12.23\n ml-dtypes==0.5.1\n \n opt-einsum==3.4.0\n+\n+build==1.2.2.post1\n+setuptools==80.0.0\n+wheel==0.45.1\n+pyproject-hooks==1.2.0\n+packaging==25.0\n\n```",
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Warn when __jax_array__ is seen in xla dtype canonicalization",
        "output": "```diff\nCommit: a5361315fd874cf4d5e278e20a9ab2de521d8df4\nDate: 2025-05-14T18:08:48Z\nURL: https://github.com/jax-ml/jax/commit/a5361315fd874cf4d5e278e20a9ab2de521d8df4\nFiles changed: 2\nAdditions: +8, Deletions: -0\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 5f1cf1670729..6afcdf2892f4 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -1180,6 +1180,7 @@ pytype_strict_library(\n         \":abstract_arrays\",\n         \":config\",\n         \":core\",\n+        \":deprecations\",\n         \":dtypes\",\n         \":sharding_impls\",\n         \":source_info_util\",\ndiff --git a/jax/_src/interpreters/xla.py b/jax/_src/interpreters/xla.py\nindex 7fbb22923e0f..73a57f935f5d 100644\n--- a/jax/_src/interpreters/xla.py\n+++ b/jax/_src/interpreters/xla.py\n@@ -23,6 +23,7 @@\n import numpy as np\n \n from jax._src import core\n+from jax._src import deprecations\n from jax._src import dtypes\n from jax._src.abstract_arrays import numpy_scalar_types\n from jax._src.util import safe_zip, safe_map\n@@ -100,6 +101,12 @@ def canonicalize_dtype(x):\n     handler = canonicalize_dtype_handlers.get(typ)\n     if handler: return handler(x)\n   if hasattr(x, '__jax_array__'):\n+    deprecations.warn(\n+      'jax-abstract-dunder-array',\n+      ('Triggering of __jax_array__() during abstractification is deprecated.'\n+       ' To avoid this error, either explicitly convert your object using'\n+       ' jax.numpy.array(), or register your object as a pytree.'),\n+      stacklevel=6)\n     return canonicalize_dtype(x.__jax_array__())\n   raise InvalidInputException(\n       f\"Argument '{x}' of type {type(x)} is not a valid JAX type.\")\n\n```",
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Partial in `repeats` if `total_repeat_length is None` since repeats is expected to be a constant.\n\nThis fixes an error in explicit sharding mode where we were converting repeats to a Tracer instead of it being a concrete value.\n\nPiperOrigin-RevId: 758743044",
        "output": "```diff\nCommit: 5abc510b434884639b9c3f48ba379172d443dba6\nDate: 2025-05-14T17:36:30Z\nURL: https://github.com/jax-ml/jax/commit/5abc510b434884639b9c3f48ba379172d443dba6\nFiles changed: 2\nAdditions: +22, Deletions: -8\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex 266aad4954ba..0bd287dadd51 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -6698,9 +6698,8 @@ def repeat(a: ArrayLike, repeats: ArrayLike, axis: int | None = None, *,\n            [3, 3, 4, 4, 4, 4, 4]], dtype=int32)\n   \"\"\"\n   if out_sharding is not None:\n-    return auto_axes(\n-        partial(_repeat, axis=axis, total_repeat_length=total_repeat_length),\n-        out_sharding=out_sharding)(a, repeats)\n+    return _auto_repeat(_repeat, a, repeats, axis, total_repeat_length,\n+                        out_sharding)\n   ctx_mesh = get_abstract_mesh()\n   if ctx_mesh._are_all_axes_explicit:\n     aval = core.typeof(a)\n@@ -6710,17 +6709,26 @@ def repeat(a: ArrayLike, repeats: ArrayLike, axis: int | None = None, *,\n     assert axis is not None and aval.sharding.spec[axis] is None\n     out_sharding = (NamedSharding(ctx_mesh, P())\n                     if aval.sharding.mesh.empty else aval.sharding)\n-    return auto_axes(\n-        partial(_repeat, axis=axis, total_repeat_length=total_repeat_length),\n-        out_sharding=out_sharding)(a, repeats)\n+    return _auto_repeat(_repeat, a, repeats, axis, total_repeat_length,\n+                        out_sharding)\n   try:\n-    return _repeat(a, repeats, axis=axis,\n+    return _repeat(a, repeats=repeats, axis=axis,\n                    total_repeat_length=total_repeat_length)\n   except core.ShardingTypeError as e:\n     raise ValueError(\n         \"Please pass sharding to `jnp.repeat` via `out_sharding` parameter.\")\n \n-def _repeat(a: ArrayLike, repeats: ArrayLike, *, axis: int | None = None,\n+def _auto_repeat(fun, a, repeats, axis, total_repeat_length, out_sharding):\n+  if total_repeat_length is None:\n+    return auto_axes(partial(fun, repeats=repeats, axis=axis,\n+                             total_repeat_length=total_repeat_length),\n+                     out_sharding=out_sharding)(a)\n+  else:\n+    return auto_axes(\n+        partial(fun, axis=axis, total_repeat_length=total_repeat_length),\n+        out_sharding=out_sharding)(a, repeats=repeats)\n+\n+def _repeat(a: ArrayLike, *, repeats: ArrayLike, axis: int | None = None,\n             total_repeat_length: int | None = None) -> Array:\n   if core.is_dim(repeats):\n     util.check_arraylike(\"repeat\", a)\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex e374b0b15a7b..d1c8ec7f050d 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7617,6 +7617,12 @@ def test_jnp_repeat(self, mesh):\n     out = jnp.repeat(a, np.array((2,2,2,2)) - 1, axis=0, out_sharding=P('x'))\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('x', None)))\n \n+    a = jax.device_put(jnp.eye(16).reshape(16, 16), P('x'))\n+    @jax.jit\n+    def f(x):\n+      return jnp.repeat(x, 3, axis=-1)\n+    f(a)\n+\n   @jtu.with_explicit_mesh((2,), ('x',))\n   def test_scatter_gather(self, mesh):\n     x = np.random.uniform(size=(mesh.size * 2, 3))\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28732 from vfdev-5:remove-tsan-suppresions\n\nPiperOrigin-RevId: 758726514",
        "output": "```diff\nCommit: bf0f6eee628a9a3912112c9835ce34ff504a58f5\nDate: 2025-05-14T17:02:20Z\nURL: https://github.com/jax-ml/jax/commit/bf0f6eee628a9a3912112c9835ce34ff504a58f5\nFiles changed: 2\nAdditions: +0, Deletions: -8\ndiff --git a/.github/workflows/tsan-suppressions_3.13.txt b/.github/workflows/tsan-suppressions_3.13.txt\nindex e82699036e92..aec94dfef004 100644\n--- a/.github/workflows/tsan-suppressions_3.13.txt\n+++ b/.github/workflows/tsan-suppressions_3.13.txt\n@@ -40,7 +40,3 @@ race:gemm_oncopy\n # https://github.com/python/cpython/issues/132245\n race:split_keys_entry_added\n race_top:dict_dict_merge\n-\n-# https://github.com/python/cpython/issues/132013\n-# Fixed on 3.14 and not backported to 3.13\n-race_top:frozenset_hash\n\\ No newline at end of file\ndiff --git a/.github/workflows/tsan-suppressions_3.14.txt b/.github/workflows/tsan-suppressions_3.14.txt\nindex ec4d81c987d0..ec5102502a2b 100644\n--- a/.github/workflows/tsan-suppressions_3.14.txt\n+++ b/.github/workflows/tsan-suppressions_3.14.txt\n@@ -18,7 +18,3 @@ race:dscal_k_\n race:scal_k_\n race:gemm_beta\n race:gemm_oncopy\n-\n-# https://github.com/python/cpython/issues/132214\n-# Should be fixed\n-# race_top:update_one_slot\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28635 from ROCm:skip-csr-mat-tests-rocm63\n\nPiperOrigin-RevId: 758726482",
        "output": "```diff\nCommit: d1a1a5530cd0fa22311da905748592495551482c\nDate: 2025-05-14T16:58:00Z\nURL: https://github.com/jax-ml/jax/commit/d1a1a5530cd0fa22311da905748592495551482c\nFiles changed: 1\nAdditions: +27, Deletions: -0\ndiff --git a/tests/sparse_test.py b/tests/sparse_test.py\nindex 71437fd0e028..97a156f9f6f5 100644\n--- a/tests/sparse_test.py\n+++ b/tests/sparse_test.py\n@@ -16,6 +16,8 @@\n from functools import partial\n import itertools\n import math\n+import os\n+from pathlib import Path\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -42,6 +44,15 @@\n import numpy as np\n import scipy.sparse\n \n+def get_rocm_version():\n+  rocm_path = os.environ.get(\"ROCM_PATH\", \"/opt/rocm\")\n+  version_path = Path(rocm_path) / \".info\" / \"version\"\n+  if not version_path.exists():\n+    raise FileNotFoundError(f\"Expected ROCm version file at {version_path}\")\n+  version_str = version_path.read_text().strip()\n+  major, minor, *_ = version_str.split(\".\")\n+  return int(major), int(minor)\n+\n jax.config.parse_flags_with_absl()\n \n all_dtypes = jtu.dtypes.integer + jtu.dtypes.floating + jtu.dtypes.complex\n@@ -208,6 +219,14 @@ def test_csr_fromdense(self, shape, dtype):\n     transpose=[True, False],\n   )\n   def test_csr_matvec(self, shape, dtype, transpose):\n+    if (\n+        jtu.is_device_rocm() and\n+        get_rocm_version() < (6, 4) and\n+        dtype in (jtu.dtypes.floating + jtu.dtypes.complex)\n+    ):\n+      # TODO: Remove this check when ROCm 6.4+ is the minimum supported version\n+      self.skipTest(\"ROCm <6.4 bug: NaN propagation when beta==0 (fixed in ROCm 6.4.0)\")\n+\n     op = lambda M: M.T if transpose else M\n \n     v_rng = jtu.rand_default(self.rng())\n@@ -228,6 +247,14 @@ def test_csr_matvec(self, shape, dtype, transpose):\n       transpose=[True, False],\n   )\n   def test_csr_matmat(self, shape, dtype, transpose):\n+    if (\n+        jtu.is_device_rocm() and\n+        get_rocm_version() < (6, 4) and\n+        dtype in (jtu.dtypes.floating + jtu.dtypes.complex)\n+    ):\n+      # TODO: Remove this check when ROCm 6.4+ is the minimum supported version\n+      self.skipTest(\"ROCm <6.4 bug: NaN propagation when beta==0 (fixed in ROCm 6.4.0)\")\n+\n     op = lambda M: M.T if transpose else M\n \n     B_rng = jtu.rand_default(self.rng())\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Lower unreduced to shardy. GSPMD doesn't support unreduced.\n\nPiperOrigin-RevId: 758719692",
        "output": "```diff\nCommit: 0b70c2323df506811fb4d5290df72ca5ee328c55\nDate: 2025-05-14T16:41:11Z\nURL: https://github.com/jax-ml/jax/commit/0b70c2323df506811fb4d5290df72ca5ee328c55\nFiles changed: 8\nAdditions: +64, Deletions: -45\ndiff --git a/jax/_src/callback.py b/jax/_src/callback.py\nindex 06b36ce5c880..bc233b634f3c 100644\n--- a/jax/_src/callback.py\n+++ b/jax/_src/callback.py\n@@ -158,7 +158,7 @@ def _callback_op_sharding(\n       op_sharding = sharding_impls.SdyArrayList([\n           sharding_impls.SdyArray(\n               mesh_shape=(),\n-              dimension_shardings=[\n+              dim_shardings=[\n                   sharding_impls.SdyDim(axes=[], is_open=False)\n               ] * avals_out[0].ndim,\n               logical_device_ids=())])\n@@ -200,7 +200,7 @@ def _callback_op_sharding(\n       op_sharding = sharding_impls.SdyArrayList(num_sdy_shardings * [\n           sharding_impls.SdyArray(\n               mesh_shape=(),\n-              dimension_shardings=[],\n+              dim_shardings=[],\n               logical_device_ids=(device_index,))])\n     else:\n       op_sharding = xc.OpSharding()  # type: ignore[assignment]\n@@ -610,7 +610,7 @@ def send_to_host(\n       assert len(sharding.shardings) >= 1\n       sharding = SdyArrayList([\n           SdyArray(\n-              mesh_shape=(), dimension_shardings=[],\n+              mesh_shape=(), dim_shardings=[],\n               logical_device_ids=sharding.shardings[0].logical_device_ids)])\n     mlir.set_sharding(send_op, sharding)\n   return send_op.result\n@@ -645,7 +645,7 @@ def receive_from_host(\n       sharding = SdyArrayList([\n           sharding.shardings[0],\n           SdyArray(\n-              mesh_shape=(), dimension_shardings=[],\n+              mesh_shape=(), dim_shardings=[],\n               logical_device_ids=sharding.shardings[0].logical_device_ids)])\n     mlir.set_sharding(recv_op, sharding)\n   # Token should be at the end of the results\ndiff --git a/jax/_src/debugging.py b/jax/_src/debugging.py\nindex 63abcbef331e..c2febf752b92 100644\n--- a/jax/_src/debugging.py\n+++ b/jax/_src/debugging.py\n@@ -168,7 +168,7 @@ def debug_callback_lowering(ctx, *args, effect, partitioned, callback, **params)\n         sharding = sharding_impls.SdyArrayList([\n             sharding_impls.SdyArray(\n                 mesh_shape=(),\n-                dimension_shardings=[\n+                dim_shardings=[\n                     sharding_impls.SdyDim(axes=[], is_open=False)\n                 ] * ctx.avals_out[0].ndim,\n                 logical_device_ids=())])\n@@ -184,7 +184,7 @@ def debug_callback_lowering(ctx, *args, effect, partitioned, callback, **params)\n     if config.use_shardy_partitioner.value:\n       sharding = sharding_impls.SdyArrayList([\n           sharding_impls.SdyArray(\n-              mesh_shape=(), dimension_shardings=[], logical_device_ids=(0,))])\n+              mesh_shape=(), dim_shardings=[], logical_device_ids=(0,))])\n     else:\n       sharding = xc.OpSharding()\n       sharding.type = xc.OpSharding.Type.MAXIMAL\ndiff --git a/jax/_src/interpreters/mlir.py b/jax/_src/interpreters/mlir.py\nindex 94418f0b958b..0256057b8b09 100644\n--- a/jax/_src/interpreters/mlir.py\n+++ b/jax/_src/interpreters/mlir.py\n@@ -1841,7 +1841,7 @@ def replicate_trailing_dims(ctx, val: ir.Value, aval) -> ir.Value:\n     physical_ndim = core.physical_aval(aval).ndim\n     s = SdyArray(\n         mesh_shape=None,\n-        dimension_shardings=[\n+        dim_shardings=[\n             sharding_impls.SdyDim(axes=[], is_open=i < aval.ndim)\n             for i in range(physical_ndim)\n         ])\ndiff --git a/jax/_src/named_sharding.py b/jax/_src/named_sharding.py\nindex 45ae1c124a22..faf0b2a9f2b2 100644\n--- a/jax/_src/named_sharding.py\n+++ b/jax/_src/named_sharding.py\n@@ -44,7 +44,8 @@ def __init__(self, mesh: mesh_lib.Mesh):\n   def _to_sdy_sharding(self, ndim: int) -> SdyArray:\n     dim_shardings = [SdyDim(axes=[], is_open=True)\n                      for _ in range(ndim)]\n-    return SdyArray(self.mesh.shape_tuple, dim_shardings)\n+    return SdyArray(mesh_shape=self.mesh.shape_tuple,\n+                    dim_shardings=dim_shardings)\n \n class UnspecifiedValue:\n   def __repr__(self):\n@@ -244,8 +245,10 @@ def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n       else:\n         dim_spec = dim_spec if isinstance(dim_spec, tuple) else (dim_spec,)\n         dim_shardings[i].axes = dim_spec\n-    return SdyArray(self.mesh.shape_tuple, dim_shardings,\n-                    self._logical_device_ids)\n+    return SdyArray(mesh_shape=self.mesh.shape_tuple,\n+                    dim_shardings=dim_shardings,\n+                    logical_device_ids=self._logical_device_ids,\n+                    unreduced_axes=self.spec.unreduced)\n \n NamedSharding.__module__ = 'jax.sharding'\n \n@@ -285,13 +288,21 @@ def _custom_repr(self):\n     priority_repr = '' if self.priority is None else f'p{self.priority}'\n     return f'{{{axes_repr}{open_repr}}}{priority_repr}'\n \n+def _get_axes(axes, mesh_shape):\n+  if not axes:\n+    return ()\n+  assert mesh_shape is not None\n+  # Sort wrt mesh axis names so order is deterministic and doesn't hang in\n+  # McJAX.\n+  return tuple(n for n, _ in mesh_shape if n in axes)\n \n-@dataclasses.dataclass\n+@dataclasses.dataclass(kw_only=True)\n class SdyArray:\n   mesh_shape: tuple[tuple[str, int], ...] | None\n-  dimension_shardings: Sequence[SdyDim]\n+  dim_shardings: Sequence[SdyDim]\n   logical_device_ids: tuple[int, ...] | None = None\n   replicated_axes: tuple[str, ...] = ()\n+  unreduced_axes: tuple[str, ...] = ()\n \n   def build(self) -> sdy.TensorShardingAttr:\n     if self.mesh_shape is None:\n@@ -302,14 +313,18 @@ def build(self) -> sdy.TensorShardingAttr:\n       mesh_attr = sdy.MeshAttr.get(\n           [sdy.MeshAxisAttr.get(name, size) for name, size in self.mesh_shape],\n           ldi)\n+\n+    replicated_axes = _get_axes(self.replicated_axes, self.mesh_shape)\n+    unreduced_axes = _get_axes(self.unreduced_axes, self.mesh_shape)\n     return sdy.TensorShardingAttr.get(\n         mesh_attr,\n-        [dim_sharding.build() for dim_sharding in self.dimension_shardings],\n-        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in self.replicated_axes])\n+        [dim_sharding.build() for dim_sharding in self.dim_shardings],\n+        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes],\n+        unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in unreduced_axes])\n \n   def __repr__(self):\n     dim_sharding_repr = ', '.join(\n-        d._custom_repr() for d in self.dimension_shardings)\n+        d._custom_repr() for d in self.dim_shardings)\n     device_id_repr = (f', device_ids={self.logical_device_ids}'\n                       if self.logical_device_ids is not None else '')\n     rar = (f', replicated_axes={self.replicated_axes}'\n@@ -317,6 +332,26 @@ def __repr__(self):\n     return f\"SdyArray([{dim_sharding_repr}]{device_id_repr}{rar})\"\n \n \n+# TODO(yashkatariya): Upstream this into `_to_sdy_sharding` maybe with an extra\n+# parameter to it `_to_sdy_sharding(self, ndim, modify_wrt_axis_types=False)`\n+def modify_sdy_sharding_wrt_axis_types(sdy_sharding: SdyArray, mesh):\n+  if mesh._any_axis_auto:\n+    dim_shardings, used_axes = [], []  # type: ignore\n+    for d in sdy_sharding.dim_shardings:\n+      # TODO(yashkatariya): Maybe if any mesh axis is auto, mark all axes as open?\n+      dim_shardings.append(SdyDim(axes=[], is_open=True)\n+                           if not d.axes and not d.is_open else d)\n+      used_axes.extend(d.axes)\n+    remaining_axes = set(mesh.axis_names) - set(used_axes)\n+    replicated_axes = tuple(r for r in remaining_axes\n+                            if mesh._name_to_type[r] == mesh_lib.AxisType.Explicit)\n+    return SdyArray(mesh_shape=sdy_sharding.mesh_shape,\n+                    dim_shardings=dim_shardings,\n+                    logical_device_ids=sdy_sharding.logical_device_ids,\n+                    replicated_axes=replicated_axes)\n+  return sdy_sharding\n+\n+\n @cache(max_size=4096, trace_context_in_key=False)\n def named_sharding_to_xla_hlo_sharding(\n     self, num_dimensions: int) -> xc.HloSharding:\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 3eb46da890f2..abcc2ca0acf1 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -781,7 +781,7 @@ def _shardy_shard_map_sharding(\n     aval_in = core.physical_aval(aval_in)\n   sdy_sharding = ns._to_sdy_sharding(aval_in.ndim)\n   if len(manual_axes) < len(mesh.axis_names):\n-    for dim_sharding in sdy_sharding.dimension_shardings:\n+    for dim_sharding in sdy_sharding.dim_shardings:\n       dim_sharding.is_open = True\n   return sdy_sharding\n \ndiff --git a/jax/_src/sharding_impls.py b/jax/_src/sharding_impls.py\nindex f7f0ebd2cc26..982af82c5c4d 100644\n--- a/jax/_src/sharding_impls.py\n+++ b/jax/_src/sharding_impls.py\n@@ -38,7 +38,8 @@\n     SdyArray, SdyDim, UnspecifiedValue, AUTO,\n     _check_unique_resources, NamedSharding, UNSPECIFIED,\n     ArrayMapping, ArrayMappingOrAutoOrUnspecified, get_array_mapping,\n-    array_mapping_to_axis_resources, named_sharding_to_xla_hlo_sharding)\n+    array_mapping_to_axis_resources, named_sharding_to_xla_hlo_sharding,\n+    modify_sdy_sharding_wrt_axis_types)\n from jax._src.op_shardings import (\n     are_op_shardings_equal, get_num_ways_dim_sharded, is_op_sharding_replicated)\n from jax._src.partition_spec import PartitionSpec\n@@ -95,27 +96,6 @@ def build(self) -> sdy.TensorShardingPerValueAttr:\n         [sharding.build() for sharding in self.shardings])\n \n \n-# TODO(yashkatariya): Upstream this into `_to_sdy_sharding` maybe with an extra\n-# parameter to it `_to_sdy_sharding(self, ndim, modify_wrt_axis_types=False)`\n-def modify_sdy_sharding_wrt_axis_types(sdy_sharding: SdyArray, mesh):\n-  if mesh._any_axis_auto:\n-    dim_shardings, used_axes = [], []  # type: ignore\n-    for d in sdy_sharding.dimension_shardings:\n-      # TODO(yashkatariya): Maybe if any mesh axis is auto, mark all axes as open?\n-      dim_shardings.append(SdyDim(axes=[], is_open=True)\n-                           if not d.axes and not d.is_open else d)\n-      used_axes.extend(d.axes)\n-    remaining_axes = set(mesh.axis_names) - set(used_axes)\n-    # Sort wrt mesh axis names so order is deterministic and doesn't hang in\n-    # McJAX.\n-    remaining_axes = [n for n in mesh.axis_names if n in remaining_axes]\n-    replicated_axes = tuple(r for r in remaining_axes\n-                            if mesh._name_to_type[r] == mesh_lib.AxisType.Explicit)\n-    return SdyArray(sdy_sharding.mesh_shape, dim_shardings,\n-                    sdy_sharding.logical_device_ids, replicated_axes)\n-  return sdy_sharding\n-\n-\n replicated_hlo_sharding = xc.HloSharding.replicate()\n \n \n@@ -188,7 +168,7 @@ def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n   def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n     sdy_dim_sharding = [SdyDim(axes=[], is_open=False)\n                         for _ in range(num_dimensions)]\n-    return SdyArray(None, sdy_dim_sharding)\n+    return SdyArray(mesh_shape=None, dim_shardings=sdy_dim_sharding)\n \n   @property\n   def is_fully_replicated(self) -> bool:\ndiff --git a/tests/array_test.py b/tests/array_test.py\nindex 0bab37c07bff..1691c3acc749 100644\n--- a/tests/array_test.py\n+++ b/tests/array_test.py\n@@ -1477,8 +1477,8 @@ def test_long_axis_names(self):\n     self.assertEqual(\n         sdy_sharding,\n         SdyArray(\n-            mesh.shape_tuple,\n-            [SdyDim(\n+            mesh_shape=mesh.shape_tuple,\n+            dim_shardings=[SdyDim(\n              ('sequence', 'data'), False),\n              SdyDim(('model',), False),\n              SdyDim([], False)]))\n@@ -1497,8 +1497,8 @@ def test_unconstrained(self):\n     self.assertEqual(\n         sdy_sharding,\n         SdyArray(\n-            mesh.shape_tuple,\n-            [SdyDim([], False),\n+            mesh_shape=mesh.shape_tuple,\n+            dim_shardings=[SdyDim([], False),\n              SdyDim([], True),\n              SdyDim(('x',), False)]))\n     with ir.Context() as ctx:\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 9b658dd8c604..e374b0b15a7b 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7735,6 +7735,7 @@ def f(x):\n     f(arr)  # doesn't crash\n     jax.jit(f)(arr)  # doesn't crash\n \n+  @config.use_shardy_partitioner(True)\n   @jtu.with_explicit_mesh((2, 2), ('x', 'y'))\n   def test_unreduced_basic(self, mesh):\n     np_inp = np.arange(16).reshape(8, 2)\n@@ -7758,7 +7759,10 @@ def f(x, y, a, b):\n       self.assertEqual(out.aval.sharding.spec, P('x', None))\n       return out\n \n-    f.trace(x, y, a, b)  # doesn't crash\n+    traced = f.trace(x, y, a, b)\n+    lowered_text = traced.lower().as_text()\n+    self.assertIn('unreduced={\"y\"}', lowered_text)\n+    self.assertTrue(lowered_text.count('unreduced={\"y\"}') == 3)\n \n   @jtu.with_explicit_mesh((2, 2, 1), ('x', 'y', 'z'))\n   def test_dot_general_unreduced_error(self, mesh):\n@@ -8595,7 +8599,7 @@ def f(x, y):\n   def test_array_sharding_repr_with_priority(self):\n     sharding = sharding_impls.SdyArray(\n         mesh_shape=(('data', 4), ('model', 8), ('expert', 2)),\n-        dimension_shardings=[\n+        dim_shardings=[\n             sharding_impls.SdyDim(axes=['data', 'expert'], is_open=False),\n             sharding_impls.SdyDim(axes=['model'], is_open=True, priority=2)])\n     self.assertEqual(repr(sharding), \"SdyArray([{'data', 'expert'}, {'model', ?}p2])\")\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Skip CSR matmat/matvec float tests on ROCm <6.4 (NaN issue with beta==0).\n\nCherry-picked from ROCm fork (commit 5dbfa9d1bcb3629658c2ec9addf45ac389f17305).\n\nAdded TODOs to remove this check when ROCm 6.4+ is the minimum supported version.",
        "output": "```diff\nCommit: 0af3e8205307db0e12259930de0ce63d50f18f2e\nDate: 2025-05-14T15:03:11Z\nURL: https://github.com/jax-ml/jax/commit/0af3e8205307db0e12259930de0ce63d50f18f2e\nFiles changed: 1\nAdditions: +27, Deletions: -0\ndiff --git a/tests/sparse_test.py b/tests/sparse_test.py\nindex 71437fd0e028..97a156f9f6f5 100644\n--- a/tests/sparse_test.py\n+++ b/tests/sparse_test.py\n@@ -16,6 +16,8 @@\n from functools import partial\n import itertools\n import math\n+import os\n+from pathlib import Path\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -42,6 +44,15 @@\n import numpy as np\n import scipy.sparse\n \n+def get_rocm_version():\n+  rocm_path = os.environ.get(\"ROCM_PATH\", \"/opt/rocm\")\n+  version_path = Path(rocm_path) / \".info\" / \"version\"\n+  if not version_path.exists():\n+    raise FileNotFoundError(f\"Expected ROCm version file at {version_path}\")\n+  version_str = version_path.read_text().strip()\n+  major, minor, *_ = version_str.split(\".\")\n+  return int(major), int(minor)\n+\n jax.config.parse_flags_with_absl()\n \n all_dtypes = jtu.dtypes.integer + jtu.dtypes.floating + jtu.dtypes.complex\n@@ -208,6 +219,14 @@ def test_csr_fromdense(self, shape, dtype):\n     transpose=[True, False],\n   )\n   def test_csr_matvec(self, shape, dtype, transpose):\n+    if (\n+        jtu.is_device_rocm() and\n+        get_rocm_version() < (6, 4) and\n+        dtype in (jtu.dtypes.floating + jtu.dtypes.complex)\n+    ):\n+      # TODO: Remove this check when ROCm 6.4+ is the minimum supported version\n+      self.skipTest(\"ROCm <6.4 bug: NaN propagation when beta==0 (fixed in ROCm 6.4.0)\")\n+\n     op = lambda M: M.T if transpose else M\n \n     v_rng = jtu.rand_default(self.rng())\n@@ -228,6 +247,14 @@ def test_csr_matvec(self, shape, dtype, transpose):\n       transpose=[True, False],\n   )\n   def test_csr_matmat(self, shape, dtype, transpose):\n+    if (\n+        jtu.is_device_rocm() and\n+        get_rocm_version() < (6, 4) and\n+        dtype in (jtu.dtypes.floating + jtu.dtypes.complex)\n+    ):\n+      # TODO: Remove this check when ROCm 6.4+ is the minimum supported version\n+      self.skipTest(\"ROCm <6.4 bug: NaN propagation when beta==0 (fixed in ROCm 6.4.0)\")\n+\n     op = lambda M: M.T if transpose else M\n \n     B_rng = jtu.rand_default(self.rng())\n\n```",
        "from_id": [
            "psanal35"
        ]
    },
    {
        "text_input": "Rename `SdyArraySharding -> SdyArray` and `SdyDimSharding -> SdyDim` since these are not `Sharding` from a JAX POV.\n\nPiperOrigin-RevId: 758679923",
        "output": "```diff\nCommit: 3142bc3b464dec165008257c62d93afa4e97d919\nDate: 2025-05-14T14:46:24Z\nURL: https://github.com/jax-ml/jax/commit/3142bc3b464dec165008257c62d93afa4e97d919\nFiles changed: 8\nAdditions: +78, Deletions: -78\ndiff --git a/jax/_src/callback.py b/jax/_src/callback.py\nindex d23389af16eb..06b36ce5c880 100644\n--- a/jax/_src/callback.py\n+++ b/jax/_src/callback.py\n@@ -40,7 +40,7 @@\n from jax._src.lib import xla_client as xc\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n-from jax._src.sharding_impls import SdyArraySharding, SdyArrayShardingList, SingleDeviceSharding\n+from jax._src.sharding_impls import SdyArray, SdyArrayList, SingleDeviceSharding\n from jax._src.typing import DeprecatedArg\n import numpy as np\n \n@@ -155,11 +155,11 @@ def _callback_op_sharding(\n       )\n     if config.use_shardy_partitioner.value:\n       assert len(avals_out) == 1\n-      op_sharding = sharding_impls.SdyArrayShardingList([\n-          sharding_impls.SdyArraySharding(\n+      op_sharding = sharding_impls.SdyArrayList([\n+          sharding_impls.SdyArray(\n               mesh_shape=(),\n               dimension_shardings=[\n-                  sharding_impls.SdyDimSharding(axes=[], is_open=False)\n+                  sharding_impls.SdyDim(axes=[], is_open=False)\n               ] * avals_out[0].ndim,\n               logical_device_ids=())])\n     else:\n@@ -197,8 +197,8 @@ def _callback_op_sharding(\n       # number of result ops. If there are no result ops, we need 1 shardy\n       # annotation.\n       num_sdy_shardings = max(1, len(avals_out))\n-      op_sharding = sharding_impls.SdyArrayShardingList(num_sdy_shardings * [\n-          sharding_impls.SdyArraySharding(\n+      op_sharding = sharding_impls.SdyArrayList(num_sdy_shardings * [\n+          sharding_impls.SdyArray(\n               mesh_shape=(),\n               dimension_shardings=[],\n               logical_device_ids=(device_index,))])\n@@ -590,7 +590,7 @@ def send_to_host(\n     operand: Any,\n     name: str,\n     *,\n-    sharding: SdyArrayShardingList | xc.OpSharding | None = None,\n+    sharding: SdyArrayList | xc.OpSharding | None = None,\n ) -> ir.Value:\n   channel_handle = hlo.ChannelHandle.get(channel, mlir.SEND_TO_HOST_TYPE)\n   send_op = hlo.SendOp([operand], token, channel_handle,\n@@ -606,10 +606,10 @@ def send_to_host(\n       # we need to create an equivalent sharding with no dimensions. If there\n       # are multiple shardings, just grab the first one since all these\n       # shardings should be the same.\n-      assert isinstance(sharding, SdyArrayShardingList)\n+      assert isinstance(sharding, SdyArrayList)\n       assert len(sharding.shardings) >= 1\n-      sharding = SdyArrayShardingList([\n-          SdyArraySharding(\n+      sharding = SdyArrayList([\n+          SdyArray(\n               mesh_shape=(), dimension_shardings=[],\n               logical_device_ids=sharding.shardings[0].logical_device_ids)])\n     mlir.set_sharding(send_op, sharding)\n@@ -622,7 +622,7 @@ def receive_from_host(\n     out_aval: core.ShapedArray,\n     name: str,\n     *,\n-    sharding: SdyArrayShardingList | xc.OpSharding | None = None,\n+    sharding: SdyArrayList | xc.OpSharding | None = None,\n ) -> tuple[ir.Value, ir.Value]:\n   channel_handle = hlo.ChannelHandle.get(channel, mlir.RECV_FROM_HOST_TYPE)\n   recv_op = hlo.RecvOp([mlir.aval_to_ir_type(out_aval),\n@@ -634,7 +634,7 @@ def receive_from_host(\n           _xla_host_transfer_rendezvous=ir.StringAttr.get(str(name))))\n   if sharding is not None:\n     if config.use_shardy_partitioner.value:\n-      assert isinstance(sharding, SdyArrayShardingList)\n+      assert isinstance(sharding, SdyArrayList)\n       assert len(sharding.shardings) >= 1\n        # `RecvOp`'s last argument is a `TokenType`. Since Shardy requires the\n       # number of shardings to match the number of results, but JAX only sees\n@@ -642,9 +642,9 @@ def receive_from_host(\n       # Note that even if a function returns N results, we will end up with N\n       # `RecvOp`s, so we only need to get the first sharding. All shardings are\n       # the same anyways, operating on the same single device ID.\n-      sharding = SdyArrayShardingList([\n+      sharding = SdyArrayList([\n           sharding.shardings[0],\n-          SdyArraySharding(\n+          SdyArray(\n               mesh_shape=(), dimension_shardings=[],\n               logical_device_ids=sharding.shardings[0].logical_device_ids)])\n     mlir.set_sharding(recv_op, sharding)\n@@ -683,7 +683,7 @@ def _emit_tpu_python_callback(\n     result_avals: Sequence[core.ShapedArray],\n     result_shapes: Sequence[xc.Shape],\n     *,\n-    sharding: SdyArrayShardingList | xc.OpSharding | None = None,\n+    sharding: SdyArrayList | xc.OpSharding | None = None,\n ) -> tuple[Sequence[ir.Value], Any]:\n   token = token or hlo.create_token()\n   _wrapped_callback = callback\n@@ -738,7 +738,7 @@ def emit_python_callback(\n     *,\n     has_side_effect: bool,\n     partitioned: bool = False,\n-    sharding: SdyArrayShardingList | xc.OpSharding | None = None,\n+    sharding: SdyArrayList | xc.OpSharding | None = None,\n ) -> tuple[Sequence[mlir.IrValues], Any, Any]:\n   \"\"\"Emits MLIR that calls back to a provided Python function.\n \n@@ -836,12 +836,12 @@ def _wrapped_callback(token, *args):  # type: ignore  # pylint: disable=function\n         config.use_shardy_partitioner.value\n         and sharding is not None\n         and len(ctx.avals_out) > 0\n-        and isinstance(sharding, sharding_impls.SdyArrayShardingList)\n+        and isinstance(sharding, sharding_impls.SdyArrayList)\n     ):\n       # Add a sharding annotation for the token if we have at least one\n       # output. Otherwise, the single shardy annotation required of all ops\n       # (even those without any results) can annotate the token.\n-      sharding = sharding_impls.SdyArrayShardingList(\n+      sharding = sharding_impls.SdyArrayList(\n           [*sharding.shardings, sharding.shardings[-1]]\n       )\n     ctx = dataclasses.replace(\ndiff --git a/jax/_src/debugging.py b/jax/_src/debugging.py\nindex 29cbb01511e9..63abcbef331e 100644\n--- a/jax/_src/debugging.py\n+++ b/jax/_src/debugging.py\n@@ -165,11 +165,11 @@ def debug_callback_lowering(ctx, *args, effect, partitioned, callback, **params)\n       # program has per-device semantics, so we run the callback on each device.\n       if config.use_shardy_partitioner.value:\n         assert len(ctx.avals_out) == 1\n-        sharding = sharding_impls.SdyArrayShardingList([\n-            sharding_impls.SdyArraySharding(\n+        sharding = sharding_impls.SdyArrayList([\n+            sharding_impls.SdyArray(\n                 mesh_shape=(),\n                 dimension_shardings=[\n-                    sharding_impls.SdyDimSharding(axes=[], is_open=False)\n+                    sharding_impls.SdyDim(axes=[], is_open=False)\n                 ] * ctx.avals_out[0].ndim,\n                 logical_device_ids=())])\n       else:\n@@ -182,8 +182,8 @@ def debug_callback_lowering(ctx, *args, effect, partitioned, callback, **params)\n     # program has bulk array semantics, so we run the callback with a MAXIMAL\n     # sharding and hence execute it only once on the full logical value).\n     if config.use_shardy_partitioner.value:\n-      sharding = sharding_impls.SdyArrayShardingList([\n-          sharding_impls.SdyArraySharding(\n+      sharding = sharding_impls.SdyArrayList([\n+          sharding_impls.SdyArray(\n               mesh_shape=(), dimension_shardings=[], logical_device_ids=(0,))])\n     else:\n       sharding = xc.OpSharding()\ndiff --git a/jax/_src/interpreters/mlir.py b/jax/_src/interpreters/mlir.py\nindex f6ef5787ccbf..94418f0b958b 100644\n--- a/jax/_src/interpreters/mlir.py\n+++ b/jax/_src/interpreters/mlir.py\n@@ -57,7 +57,7 @@\n from jax._src.partition_spec import PartitionSpec\n from jax._src.sharding import Sharding as JSharding\n from jax._src.sharding_impls import ( AUTO, NamedSharding,\n-                                     SdyArraySharding, SdyArrayShardingList,\n+                                     SdyArray, SdyArrayList,\n                                      modify_sdy_sharding_wrt_axis_types)\n from jax._src.state.types import AbstractRef\n from jax._src.util import foreach\n@@ -1034,7 +1034,7 @@ def add_manual_axes(axis_ctx: sharding_impls.SPMDAxisContext, sharding, ndim):\n def _to_physical_op_sharding(\n     ctx: ModuleContext,\n     aval: core.AbstractValue, sharding: JSharding | AUTO | None,\n-) -> xc.OpSharding | SdyArraySharding | None:\n+) -> xc.OpSharding | SdyArray | None:\n   if sharding is None:\n     return None\n   if all_unconstrained(sharding, aval):\n@@ -1839,10 +1839,10 @@ def replicate_trailing_dims(ctx, val: ir.Value, aval) -> ir.Value:\n   assert isinstance(aval, (core.ShapedArray, core.DShapedArray))\n   if config.use_shardy_partitioner.value:\n     physical_ndim = core.physical_aval(aval).ndim\n-    s = SdyArraySharding(\n+    s = SdyArray(\n         mesh_shape=None,\n         dimension_shardings=[\n-            sharding_impls.SdyDimSharding(axes=[], is_open=i < aval.ndim)\n+            sharding_impls.SdyDim(axes=[], is_open=i < aval.ndim)\n             for i in range(physical_ndim)\n         ])\n     return wrap_with_sharding_op(ctx, val, aval, s)\n@@ -2665,7 +2665,7 @@ def _wrap_with_spmd_op(name: str,\n                        ctx: LoweringRuleContext,\n                        x: ir.Value,\n                        aval_out: core.AbstractValue,\n-                       sharding: xc.OpSharding | SdyArraySharding,\n+                       sharding: xc.OpSharding | SdyArray,\n                        unspecified_dims: set[int] | None = None,\n                        has_side_effect: bool = False,\n                        allow_shardy_lowering: bool = False):\n@@ -2730,7 +2730,7 @@ def lower_with_sharding_in_types(ctx, op, aval, sharding_proto=None):\n     return wrap_with_sharding_op(ctx, op, aval, proto, unspecified_dims)\n \n \n-def set_sharding(op, sharding: xc.OpSharding | SdyArraySharding | SdyArrayShardingList):\n+def set_sharding(op, sharding: xc.OpSharding | SdyArray | SdyArrayList):\n   if config.use_shardy_partitioner.value:\n     op.attributes[\"sdy.sharding\"] = get_sharding_attr(sharding)\n   else:\n@@ -2738,7 +2738,7 @@ def set_sharding(op, sharding: xc.OpSharding | SdyArraySharding | SdyArrayShardi\n \n \n def get_sharding_attr(\n-    sharding: xc.OpSharding | SdyArraySharding | SdyArrayShardingList\n+    sharding: xc.OpSharding | SdyArray | SdyArrayList\n ) -> ir.Attribute:\n   if config.use_shardy_partitioner.value:\n     return sharding.build()  # type: ignore\ndiff --git a/jax/_src/named_sharding.py b/jax/_src/named_sharding.py\nindex 3dfcbd29fc96..45ae1c124a22 100644\n--- a/jax/_src/named_sharding.py\n+++ b/jax/_src/named_sharding.py\n@@ -41,10 +41,10 @@ class AUTO:\n   def __init__(self, mesh: mesh_lib.Mesh):\n     self.mesh = mesh\n \n-  def _to_sdy_sharding(self, ndim: int) -> SdyArraySharding:\n-    dim_shardings = [SdyDimSharding(axes=[], is_open=True)\n+  def _to_sdy_sharding(self, ndim: int) -> SdyArray:\n+    dim_shardings = [SdyDim(axes=[], is_open=True)\n                      for _ in range(ndim)]\n-    return SdyArraySharding(self.mesh.shape_tuple, dim_shardings)\n+    return SdyArray(self.mesh.shape_tuple, dim_shardings)\n \n class UnspecifiedValue:\n   def __repr__(self):\n@@ -232,8 +232,8 @@ def with_spec(self, spec: PartitionSpec | Sequence[Any]) -> NamedSharding:\n   def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n     return named_sharding_to_xla_hlo_sharding(self, num_dimensions)\n \n-  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArraySharding:\n-    dim_shardings = [SdyDimSharding(axes=[], is_open=False)\n+  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n+    dim_shardings = [SdyDim(axes=[], is_open=False)\n                      for _ in range(num_dimensions)]\n     for i, dim_spec in enumerate(self.spec):\n       if dim_spec is PartitionSpec.UNCONSTRAINED:\n@@ -244,8 +244,8 @@ def _to_sdy_sharding(self, num_dimensions: int) -> SdyArraySharding:\n       else:\n         dim_spec = dim_spec if isinstance(dim_spec, tuple) else (dim_spec,)\n         dim_shardings[i].axes = dim_spec\n-    return SdyArraySharding(self.mesh.shape_tuple, dim_shardings,\n-                            self._logical_device_ids)\n+    return SdyArray(self.mesh.shape_tuple, dim_shardings,\n+                    self._logical_device_ids)\n \n NamedSharding.__module__ = 'jax.sharding'\n \n@@ -264,7 +264,7 @@ def get_array_mapping(\n   return d\n \n @dataclasses.dataclass\n-class SdyDimSharding:\n+class SdyDim:\n   axes: Sequence[str]\n   is_open: bool\n   priority: int | None = None\n@@ -275,7 +275,7 @@ def build(self) -> sdy.DimensionShardingAttr:\n         is_closed=not self.is_open, priority=self.priority)\n \n   def __repr__(self):\n-    return f'SdyDimSharding({self._custom_repr()})'\n+    return f'SdyDim({self._custom_repr()})'\n \n   def _custom_repr(self):\n     axes_repr = ', '.join(f\"'{a}'\" for a in self.axes)\n@@ -287,9 +287,9 @@ def _custom_repr(self):\n \n \n @dataclasses.dataclass\n-class SdyArraySharding:\n+class SdyArray:\n   mesh_shape: tuple[tuple[str, int], ...] | None\n-  dimension_shardings: Sequence[SdyDimSharding]\n+  dimension_shardings: Sequence[SdyDim]\n   logical_device_ids: tuple[int, ...] | None = None\n   replicated_axes: tuple[str, ...] = ()\n \n@@ -314,7 +314,7 @@ def __repr__(self):\n                       if self.logical_device_ids is not None else '')\n     rar = (f', replicated_axes={self.replicated_axes}'\n            if self.replicated_axes else '')\n-    return f\"SdyArraySharding([{dim_sharding_repr}]{device_id_repr}{rar})\"\n+    return f\"SdyArray([{dim_sharding_repr}]{device_id_repr}{rar})\"\n \n \n @cache(max_size=4096, trace_context_in_key=False)\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex b772a3de239e..3eb46da890f2 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -773,7 +773,7 @@ def _valid_repeats(mesh: Mesh, vma: Set[AxisName], names: AxisNames) -> bool:\n \n def _shardy_shard_map_sharding(\n     ctx: mlir.LoweringRuleContext, mesh, manual_axes, names, aval_in\n-) -> sharding_impls.SdyArraySharding:\n+) -> sharding_impls.SdyArray:\n   axes = {name: i for i, ns in names.items() for name in ns}\n   ns = _make_scoped_manual_sharding(ctx, mesh, axes)\n   if dtypes.issubdtype(aval_in.dtype, dtypes.extended):\n@@ -808,10 +808,10 @@ def _shard_map_lowering_shardy(\n           dim_var_values=ctx.dim_var_values)\n     return out_nodes\n \n-  in_shardings = sharding_impls.SdyArrayShardingList(map(\n+  in_shardings = sharding_impls.SdyArrayList(map(\n       partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n       in_names, ctx.avals_in)).build()\n-  out_shardings = sharding_impls.SdyArrayShardingList(map(\n+  out_shardings = sharding_impls.SdyArrayList(map(\n       partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n       out_names, ctx.avals_out)).build()\n   output_types = map(mlir.aval_to_ir_type, ctx.avals_out)\ndiff --git a/jax/_src/sharding_impls.py b/jax/_src/sharding_impls.py\nindex 2394e9e18f38..f7f0ebd2cc26 100644\n--- a/jax/_src/sharding_impls.py\n+++ b/jax/_src/sharding_impls.py\n@@ -35,7 +35,7 @@\n from jax._src.lib import xla_client as xc\n from jax._src.lib.mlir.dialects import sdy\n from jax._src.named_sharding import (  # noqa: F401\n-    SdyArraySharding, SdyDimSharding, UnspecifiedValue, AUTO,\n+    SdyArray, SdyDim, UnspecifiedValue, AUTO,\n     _check_unique_resources, NamedSharding, UNSPECIFIED,\n     ArrayMapping, ArrayMappingOrAutoOrUnspecified, get_array_mapping,\n     array_mapping_to_axis_resources, named_sharding_to_xla_hlo_sharding)\n@@ -87,8 +87,8 @@ def device_replica_id_map(sharding, global_shape: Shape) -> Mapping[Device, int]\n \n \n @dataclasses.dataclass\n-class SdyArrayShardingList:\n-  shardings: Sequence[SdyArraySharding]\n+class SdyArrayList:\n+  shardings: Sequence[SdyArray]\n \n   def build(self) -> sdy.TensorShardingPerValueAttr:\n     return sdy.TensorShardingPerValueAttr.get(\n@@ -97,12 +97,12 @@ def build(self) -> sdy.TensorShardingPerValueAttr:\n \n # TODO(yashkatariya): Upstream this into `_to_sdy_sharding` maybe with an extra\n # parameter to it `_to_sdy_sharding(self, ndim, modify_wrt_axis_types=False)`\n-def modify_sdy_sharding_wrt_axis_types(sdy_sharding: SdyArraySharding, mesh):\n+def modify_sdy_sharding_wrt_axis_types(sdy_sharding: SdyArray, mesh):\n   if mesh._any_axis_auto:\n     dim_shardings, used_axes = [], []  # type: ignore\n     for d in sdy_sharding.dimension_shardings:\n       # TODO(yashkatariya): Maybe if any mesh axis is auto, mark all axes as open?\n-      dim_shardings.append(SdyDimSharding(axes=[], is_open=True)\n+      dim_shardings.append(SdyDim(axes=[], is_open=True)\n                            if not d.axes and not d.is_open else d)\n       used_axes.extend(d.axes)\n     remaining_axes = set(mesh.axis_names) - set(used_axes)\n@@ -111,8 +111,8 @@ def modify_sdy_sharding_wrt_axis_types(sdy_sharding: SdyArraySharding, mesh):\n     remaining_axes = [n for n in mesh.axis_names if n in remaining_axes]\n     replicated_axes = tuple(r for r in remaining_axes\n                             if mesh._name_to_type[r] == mesh_lib.AxisType.Explicit)\n-    return SdyArraySharding(sdy_sharding.mesh_shape, dim_shardings,\n-                            sdy_sharding.logical_device_ids, replicated_axes)\n+    return SdyArray(sdy_sharding.mesh_shape, dim_shardings,\n+                    sdy_sharding.logical_device_ids, replicated_axes)\n   return sdy_sharding\n \n \n@@ -185,10 +185,10 @@ def _device_assignment(self) -> XLADeviceAssignment:\n   def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n     return replicated_hlo_sharding\n \n-  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArraySharding:\n-    sdy_dim_sharding = [SdyDimSharding(axes=[], is_open=False)\n+  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n+    sdy_dim_sharding = [SdyDim(axes=[], is_open=False)\n                         for _ in range(num_dimensions)]\n-    return SdyArraySharding(None, sdy_dim_sharding)\n+    return SdyArray(None, sdy_dim_sharding)\n \n   @property\n   def is_fully_replicated(self) -> bool:\n@@ -330,8 +330,8 @@ def with_memory_kind(self, kind: str):\n   def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n     raise NotImplementedError(\"pmap doesn't use OpSharding.\")\n \n-  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArraySharding:\n-    raise NotImplementedError(\"pmap doesn't use SdyArraySharding.\")\n+  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n+    raise NotImplementedError(\"pmap doesn't use SdyArray.\")\n \n   @functools.cached_property\n   def is_fully_replicated(self) -> bool:\n@@ -540,9 +540,9 @@ def _device_assignment(self) -> XLADeviceAssignment:\n   def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n     return _positional_sharding_to_xla_hlo_sharding(self, num_dimensions)\n \n-  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArraySharding:\n+  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n     raise NotImplementedError(\n-        \"PositionalSharding can't be converted to an SdyArraySharding.\")\n+        \"PositionalSharding can't be converted to an SdyArray.\")\n \n   @functools.cached_property\n   def is_fully_addressable(self) -> bool:\n@@ -657,9 +657,9 @@ def _device_assignment(self) -> XLADeviceAssignment:\n   def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n     return self._hlo_sharding\n \n-  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArraySharding:\n+  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n     raise NotImplementedError(\n-        \"GSPMDSharding can't be converted to SdyArraySharding.\")\n+        \"GSPMDSharding can't be converted to SdyArray.\")\n \n   @functools.cached_property\n   def is_fully_replicated(self) -> bool:\ndiff --git a/tests/array_test.py b/tests/array_test.py\nindex b951f7f6b4cd..0bab37c07bff 100644\n--- a/tests/array_test.py\n+++ b/tests/array_test.py\n@@ -35,8 +35,8 @@\n from jax._src.sharding import common_devices_indices_map\n from jax._src.sharding_impls import (\n     _op_sharding_to_pos_sharding, pmap_sharding_devices_indices_map,\n-    NamedSharding, GSPMDSharding, PositionalSharding, SdyDimSharding,\n-    SdyArraySharding)\n+    NamedSharding, GSPMDSharding, PositionalSharding, SdyDim,\n+    SdyArray)\n from jax.experimental.pjit import pjit\n from jax.experimental import multihost_utils\n from jax.sharding import PartitionSpec as P\n@@ -1476,12 +1476,12 @@ def test_long_axis_names(self):\n     sdy_sharding = s._to_sdy_sharding(3)\n     self.assertEqual(\n         sdy_sharding,\n-        SdyArraySharding(\n+        SdyArray(\n             mesh.shape_tuple,\n-            [SdyDimSharding(\n+            [SdyDim(\n              ('sequence', 'data'), False),\n-             SdyDimSharding(('model',), False),\n-             SdyDimSharding([], False)]))\n+             SdyDim(('model',), False),\n+             SdyDim([], False)]))\n     with ir.Context() as ctx:\n       dialects.sdy.register_dialect(ctx)\n       self.assertEqual(\n@@ -1496,11 +1496,11 @@ def test_unconstrained(self):\n     sdy_sharding = s._to_sdy_sharding(3)\n     self.assertEqual(\n         sdy_sharding,\n-        SdyArraySharding(\n+        SdyArray(\n             mesh.shape_tuple,\n-            [SdyDimSharding([], False),\n-             SdyDimSharding([], True),\n-             SdyDimSharding(('x',), False)]))\n+            [SdyDim([], False),\n+             SdyDim([], True),\n+             SdyDim(('x',), False)]))\n     with ir.Context() as ctx:\n       dialects.sdy.register_dialect(ctx)\n       self.assertEqual(\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 22a4d4f70f8c..9b658dd8c604 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -8593,26 +8593,26 @@ def f(x, y):\n     self.assertIn('sdy.mesh @mesh = <[\"x\"=8]>', lowered_str)\n \n   def test_array_sharding_repr_with_priority(self):\n-    sharding = sharding_impls.SdyArraySharding(\n+    sharding = sharding_impls.SdyArray(\n         mesh_shape=(('data', 4), ('model', 8), ('expert', 2)),\n         dimension_shardings=[\n-            sharding_impls.SdyDimSharding(axes=['data', 'expert'], is_open=False),\n-            sharding_impls.SdyDimSharding(axes=['model'], is_open=True, priority=2)])\n-    self.assertEqual(repr(sharding), \"SdyArraySharding([{'data', 'expert'}, {'model', ?}p2])\")\n+            sharding_impls.SdyDim(axes=['data', 'expert'], is_open=False),\n+            sharding_impls.SdyDim(axes=['model'], is_open=True, priority=2)])\n+    self.assertEqual(repr(sharding), \"SdyArray([{'data', 'expert'}, {'model', ?}p2])\")\n \n   def test_array_sharding_repr_with_logical_ids(self):\n     abstract_mesh = jax.sharding.AbstractMesh((4, 8, 2), ('x', 'y', 'z'))\n     ns = NamedSharding(abstract_mesh, P(('x', 'y'), 'z', P.UNCONSTRAINED, None),\n                        _logical_device_ids=[4, 5, 6, 7, 0, 1, 2, 3])\n     self.assertEqual(repr(ns._to_sdy_sharding(4)),\n-                     \"SdyArraySharding([{'x', 'y'}, {'z'}, {?}, {}], \"\n+                     \"SdyArray([{'x', 'y'}, {'z'}, {?}, {}], \"\n                      \"device_ids=[4, 5, 6, 7, 0, 1, 2, 3])\")\n \n   def test_dimension_sharding_repr(self):\n-    dim_sharding = sharding_impls.SdyDimSharding(\n+    dim_sharding = sharding_impls.SdyDim(\n         axes=['data', 'model'], is_open=True, priority=2)\n     self.assertEqual(repr(dim_sharding),\n-                     \"SdyDimSharding({'data', 'model', ?}p2)\")\n+                     \"SdyDim({'data', 'model', ?}p2)\")\n \n   def test_tensor_dialect(self):\n     # While this doesn't emit any `mlir::TensorDialect` ops, some pass in the\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Replace gsutil command with gcloud storage commands.\n\nGCP recommends using `gcloud storage` instead of `gsutil`\n\nhttps://cloud.google.com/storage/docs/gsutil\n\nPiperOrigin-RevId: 758672654",
        "output": "```diff\nCommit: abc79f3d590360a48245d051c62fdc23351a90d6\nDate: 2025-05-14T14:23:53Z\nURL: https://github.com/jax-ml/jax/commit/abc79f3d590360a48245d051c62fdc23351a90d6\nFiles changed: 6\nAdditions: +21, Deletions: -21\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 72878ad7aacb..677d8d869a22 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -84,12 +84,12 @@ jobs:\n         continue-on-error: true\n         run: |\n           mkdir -p $(pwd)/dist\n-          gsutil -m cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n+          gcloud storage cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n \n           if [[ ${{ inputs.jaxlib-version }} == \"head\" ]]; then\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*plugin*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*pjrt*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*plugin*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*pjrt*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n           elif [[ ${{ inputs.jaxlib-version }} == \"pypi_latest\" ]]; then\n             PYTHON=python${{ inputs.python }}\n             $PYTHON -m pip download jaxlib jax-cuda12-pjrt jax-cuda12-plugin --dest $(pwd)/dist/\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 37a791784506..1b534ee3b6fc 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -136,13 +136,13 @@ jobs:\n       - name: Upload artifacts to a GCS bucket (non-Windows runs)\n         if: >-\n           ${{ inputs.upload_artifacts_to_gcs && !contains(inputs.runner, 'windows-x86') }}\n-        run:  gsutil -m cp -r \"$(pwd)/dist/*.whl\" \"${{ inputs.gcs_upload_uri }}\"/\n+        run:  gcloud storage cp -r \"$(pwd)/dist/*.whl\" \"${{ inputs.gcs_upload_uri }}\"/\n       # Set shell to cmd to avoid path errors when using gcloud commands on Windows\n       - name: Upload artifacts to a GCS bucket (Windows runs)\n         if: >-\n           ${{ inputs.upload_artifacts_to_gcs &&  contains(inputs.runner, 'windows-x86') }}\n         shell: cmd\n-        run:  gsutil -m cp -r \"dist/*.whl\" \"${{ inputs.gcs_upload_uri }}\"/\n+        run:  gcloud storage cp -r \"dist/*.whl\" \"${{ inputs.gcs_upload_uri }}\"/\n       - name: Store the GCS upload URI as an output\n         id: store-gcs-upload-uri\n         if: ${{ inputs.upload_artifacts_to_gcs }}\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex bdce2b684803..fc4633110667 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -96,12 +96,12 @@ jobs:\n         if: ${{ !contains(inputs.runner, 'windows-x86') }}\n         run: |\n           mkdir -p $(pwd)/dist\n-          gsutil -m cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n+          gcloud storage cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n \n           if [[ \"${{ inputs.download-jax-only-from-gcs }}\" == \"1\" ]]; then\n             echo \"JAX only release. Only downloading the jax wheel from the release bucket.\"\n           else\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n           fi\n       - name: Download wheels from GCS (Windows runs)\n         id: download-wheel-artifacts-w\n@@ -113,14 +113,14 @@ jobs:\n         shell: cmd\n         run: |\n           mkdir dist\n-          @REM Use `call` so that we can run sequential gsutil commands on Windows\n+          @REM Use `call` so that we can run sequential gcloud storage commands on Windows\n           @REM See https://github.com/GoogleCloudPlatform/gsutil/issues/233#issuecomment-196150652\n-          call gsutil -m cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl dist/\n+          call gcloud storage cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl dist/\n \n           if \"${{ inputs.download-jax-only-from-gcs }}\"==\"1\" (\n             echo \"JAX only release. Only downloading the jax wheel from the release bucket.\"\n           ) else (\n-            call gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*%PYTHON_MAJOR_MINOR%*%OS%*%ARCH%*.whl\" dist/\n+            call gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*%PYTHON_MAJOR_MINOR%*%OS%*%ARCH%*.whl\" dist/\n           )\n       - name: Skip the test run if the wheel artifacts were not downloaded successfully\n         if: steps.download-wheel-artifacts-nw.outcome == 'failure' || steps.download-wheel-artifacts-w.outcome == 'failure'\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 4df752310ace..af034ab09991 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -93,7 +93,7 @@ jobs:\n         continue-on-error: true\n         run: |\n           mkdir -p $(pwd)/dist\n-          gsutil -m cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n+          gcloud storage cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n \n           # Do not download the jaxlib and CUDA plugin artifacts if we are testing a jax only\n           # release.\n@@ -104,9 +104,9 @@ jobs:\n             # required dependency of jax so that gets installed automatically.\n             echo \"JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI=jax_cuda_pypi\">> $GITHUB_ENV\n           else\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*plugin*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*pjrt*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*plugin*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*pjrt*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n           fi\n       - name: Skip the test run if the wheel artifacts were not downloaded successfully\n         if: steps.download-wheel-artifacts.outcome == 'failure'\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 55a0b4cc1a5f..2d4d2925bd2f 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -114,11 +114,11 @@ jobs:\n         continue-on-error: true\n         run: |\n           mkdir -p $(pwd)/dist\n-          gsutil -m cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n+          gcloud storage cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n           if [[ \"${{ inputs.download-jax-only-from-gcs }}\" == \"1\" ]]; then\n             echo \"JAX only release. Only downloading the jax wheel from the release bucket.\"\n           else\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n           fi\n       - name: Skip the test run if the wheel artifacts were not downloaded successfully\n         if: steps.download-wheel-artifacts.outcome == 'failure'\ndiff --git a/.github/workflows/wheel_tests_nightly_release.yml b/.github/workflows/wheel_tests_nightly_release.yml\nindex 8d597b84f735..3e616a894d13 100644\n--- a/.github/workflows/wheel_tests_nightly_release.yml\n+++ b/.github/workflows/wheel_tests_nightly_release.yml\n@@ -171,15 +171,15 @@ jobs:\n           python_major_minor=$(echo \"${python_major_minor//-nogil/t}\" | tr -d '.')\n           python_major_minor=\"cp${python_major_minor%t}-cp${python_major_minor}-\"\n \n-          gsutil -m cp -r \"${final_gcs_download_uri}\"/jax*py3*none*any.whl $(pwd)/dist/\n+          gcloud storage cp -r \"${final_gcs_download_uri}\"/jax*py3*none*any.whl $(pwd)/dist/\n \n           jax_wheel=$(ls dist/jax*py3*none*any.whl 2>/dev/null)\n           echo \"JAX_WHEEL=$jax_wheel\" >> $GITHUB_ENV\n \n           if [[ \"${{ inputs.download-jax-only-from-gcs }}\" != \"1\" ]]; then\n-            gsutil -m cp -r \"${final_gcs_download_uri}/jaxlib*${python_major_minor}*linux*x86_64*.whl\" $(pwd)/dist/\n-            gsutil -m cp -r \"${final_gcs_download_uri}/jax*cuda*plugin*${python_major_minor}*linux*x86_64*.whl\" $(pwd)/dist/\n-            gsutil -m cp -r \"${final_gcs_download_uri}/jax*cuda*pjrt*linux*x86_64*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${final_gcs_download_uri}/jaxlib*${python_major_minor}*linux*x86_64*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${final_gcs_download_uri}/jax*cuda*plugin*${python_major_minor}*linux*x86_64*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${final_gcs_download_uri}/jax*cuda*pjrt*linux*x86_64*.whl\" $(pwd)/dist/\n \n             jaxlib_wheel=$(ls dist/jaxlib*${python_major_minor}*linux*x86_64*.whl 2>/dev/null)\n             jax_cuda_plugin_wheel=$(ls dist/jax*cuda*plugin*${python_major_minor}*linux*x86_64*.whl 2>/dev/null)\n\n```",
        "from_id": [
            "nitins17",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas/Mosaic GPU] Optimize the construction of output buffers for `plgpu.kernel`.\n\nPreviously to this change, we would zero initialize all buffers---which was a\nbig overhead. Instead, we now generate an empty custom call in order to only\ngenerate an allocation.\n\nPiperOrigin-RevId: 758659576",
        "output": "```diff\nCommit: 7f5b6e7d02656d3b5f116fe557fcdc0c365f88ee\nDate: 2025-05-14T13:42:49Z\nURL: https://github.com/jax-ml/jax/commit/7f5b6e7d02656d3b5f116fe557fcdc0c365f88ee\nFiles changed: 1\nAdditions: +14, Deletions: -1\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 21c78720e812..e4fe1a7035a7 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -33,6 +33,7 @@\n from jax._src import tree_util\n from jax._src.lib.mlir.dialects import arith as arith_dialect\n from jax._src.pallas import core as pallas_core\n+from jax._src.pallas import pallas_call\n from jax._src.pallas import primitives as pallas_primitives\n import jax._src.pallas.utils as pallas_utils\n from jax._src.state import discharge as state_discharge\n@@ -194,12 +195,24 @@ def cmap_body():\n           mesh, compiler_params=compiler_params\n       )(cmap_body)\n     _, outs = state_discharge.run_state(stateful)(\n-        (operands, jax.tree.map(jnp.zeros_like, out_shape))\n+        (operands, empty_like(out_shape))\n     )\n     return outs[0] if unwrap_out else outs\n   return wrapper\n \n \n+def empty_like(shape):\n+  return pallas_call.pallas_call(\n+      lambda *_: None,\n+      out_shape=shape,\n+      out_specs=jax.tree.map(\n+          lambda _: pallas_core.BlockSpec(memory_space=GPUMemorySpace.GMEM),\n+          shape,\n+      ),\n+      backend=\"mosaic_gpu\",\n+  )()\n+\n+\n def _is_known_divisible(value, divisor, fuel=10) -> bool:\n   \"\"\"Returns True if the value is statically known to be divisible by the divisor.\"\"\"\n   if fuel < 0:\n\n```",
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Removed few tsan cpython suppressions as fixed",
        "output": "```diff\nCommit: 08a0485a49378a8ba688cc06a9da925cafdb4c42\nDate: 2025-05-14T11:47:35Z\nURL: https://github.com/jax-ml/jax/commit/08a0485a49378a8ba688cc06a9da925cafdb4c42\nFiles changed: 2\nAdditions: +0, Deletions: -8\ndiff --git a/.github/workflows/tsan-suppressions_3.13.txt b/.github/workflows/tsan-suppressions_3.13.txt\nindex e82699036e92..aec94dfef004 100644\n--- a/.github/workflows/tsan-suppressions_3.13.txt\n+++ b/.github/workflows/tsan-suppressions_3.13.txt\n@@ -40,7 +40,3 @@ race:gemm_oncopy\n # https://github.com/python/cpython/issues/132245\n race:split_keys_entry_added\n race_top:dict_dict_merge\n-\n-# https://github.com/python/cpython/issues/132013\n-# Fixed on 3.14 and not backported to 3.13\n-race_top:frozenset_hash\n\\ No newline at end of file\ndiff --git a/.github/workflows/tsan-suppressions_3.14.txt b/.github/workflows/tsan-suppressions_3.14.txt\nindex ec4d81c987d0..ec5102502a2b 100644\n--- a/.github/workflows/tsan-suppressions_3.14.txt\n+++ b/.github/workflows/tsan-suppressions_3.14.txt\n@@ -18,7 +18,3 @@ race:dscal_k_\n race:scal_k_\n race:gemm_beta\n race:gemm_oncopy\n-\n-# https://github.com/python/cpython/issues/132214\n-# Should be fixed\n-# race_top:update_one_slot\n\n```",
        "from_id": [
            "vfdev-5",
            "noreply@github.com"
        ]
    },
    {
        "text_input": "Simplify if_building_jaxlib macro.\n\nI don't believe the GPU case of this macro ever matters, so this can be a condition strictly about how we're building jaxlib. No behavioral changes intended.\n\nPiperOrigin-RevId: 758624138",
        "output": "```diff\nCommit: 67e1d5c2c8baf4face571e637deddd363cd864b9\nDate: 2025-05-14T11:38:27Z\nURL: https://github.com/jax-ml/jax/commit/67e1d5c2c8baf4face571e637deddd363cd864b9\nFiles changed: 2\nAdditions: +30, Deletions: -28\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 16bc9de6935e..5f1cf1670729 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -64,12 +64,26 @@ string_flag(\n )\n \n config_setting(\n-    name = \"enable_jaxlib_build\",\n+    name = \"config_build_jaxlib_true\",\n     flag_values = {\n         \":build_jaxlib\": \"true\",\n     },\n )\n \n+config_setting(\n+    name = \"config_build_jaxlib_false\",\n+    flag_values = {\n+        \":build_jaxlib\": \"false\",\n+    },\n+)\n+\n+config_setting(\n+    name = \"config_build_jaxlib_wheel\",\n+    flag_values = {\n+        \":build_jaxlib\": \"wheel\",\n+    },\n+)\n+\n # The flag controls whether jax should be built by Bazel.\n # If \":build_jax=true\", then jax will be built.\n # If \":build_jax=false\", then jax is not built. It is assumed that the pre-built jax wheel\n@@ -212,7 +226,6 @@ py_library(\n             \":jax\",\n         ],\n         if_not_building = [],\n-        if_not_building_for_cpu = [],\n     ) + py_deps(\"numpy\"),\n )\n \n@@ -229,7 +242,6 @@ py_library(\n             \"//jax/_src/lib\",\n         ],\n         if_not_building = [],\n-        if_not_building_for_cpu = [],\n     ) + py_deps(\"numpy\"),\n )\n \n@@ -243,7 +255,6 @@ py_library(\n             \":test_util\",\n         ],\n         if_not_building = [],\n-        if_not_building_for_cpu = [],\n     ),\n )\n \n@@ -259,7 +270,6 @@ py_library(\n             \":test_util\",\n         ],\n         if_not_building = [],\n-        if_not_building_for_cpu = [],\n     ) + py_deps(\"numpy\"),\n )\n \ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex e739b681a029..a48c44f406f2 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -158,28 +158,20 @@ def if_building_jaxlib(\n         if_building,\n         if_not_building = [\n             \"@pypi//jaxlib\",\n-            \"@pypi//jax_cuda12_plugin\",\n-            \"@pypi//jax_cuda12_pjrt\",\n-        ],\n-        if_not_building_for_cpu = [\n-            \"@pypi//jaxlib\",\n         ]):\n-    \"\"\"Adds jaxlib and jaxlib cuda plugin wheels as dependencies instead of depending on sources.\n+    \"\"\"Adds jaxlib wheels as dependencies instead of depending on sources.\n \n     This allows us to test prebuilt versions of jaxlib wheels against the rest of the JAX codebase.\n \n     Args:\n       if_building: the source code targets to depend on in case we don't depend on the jaxlib wheels\n-      if_not_building: the wheels to depend on including gpu-specific plugins in case of\n-                       gpu-enabled builds\n-      if_not_building_for_cpu: the wheels to depend on in case of cpu-only builds\n+      if_not_building: the wheels to depend on if we are not depending directly on //jaxlib.\n     \"\"\"\n \n     return select({\n-        \"//jax:enable_jaxlib_build\": if_building,\n-        \"//jax_plugins/cuda:disable_jaxlib_for_cpu_build\": if_not_building_for_cpu,\n-        \"//jax_plugins/cuda:disable_jaxlib_for_cuda12_build\": if_not_building,\n-        \"//conditions:default\": [],\n+        \"//jax:config_build_jaxlib_true\": if_building,\n+        \"//jax:config_build_jaxlib_false\": if_not_building,\n+        \"//jax:config_build_jaxlib_wheel\": [],\n     })\n \n def _get_test_deps(deps, backend_independent):\n@@ -192,14 +184,14 @@ def _get_test_deps(deps, backend_independent):\n     Returns:\n       A list of test deps for the given backend.\n         For CPU builds:\n-          If --//jax:enable_jaxlib_build=true, returns pypi test deps.\n-          If --//jax:enable_jaxlib_build=false, returns jaxlib pypi wheel dep and pypi test deps.\n-          If --//jax:enable_jaxlib_build=wheel, returns jaxlib py_import dep and pypi test deps.\n+          If --//jax:build_jaxlib=true, returns pypi test deps.\n+          If --//jax:build_jaxlib=false, returns jaxlib pypi wheel dep and pypi test deps.\n+          If --//jax:build_jaxlib=wheel, returns jaxlib py_import dep and pypi test deps.\n         For GPU builds:\n-          If --//jax:enable_jaxlib_build=true, returns pypi test deps and gpu build deps.\n-          If --//jax:enable_jaxlib_build=false, returns jaxlib, jax-cuda-plugin,\n+          If --//jax:build_jaxlib=true, returns pypi test deps and gpu build deps.\n+          If --//jax:build_jaxlib=false, returns jaxlib, jax-cuda-plugin,\n             jax-cuda-pjrt pypi wheel deps and pypi test deps.\n-          If --//jax:enable_jaxlib_build=wheel, returns jaxlib,\n+          If --//jax:build_jaxlib=wheel, returns jaxlib,\n             jax-cuda-plugin, jax-cuda-pjrt py_import deps and pypi test deps.\n     \"\"\"\n     gpu_build_deps = [\n@@ -234,7 +226,7 @@ def _get_test_deps(deps, backend_independent):\n         gpu_py_import_deps = gpu_py_imports\n \n     return select({\n-        \"//jax:enable_jaxlib_build\": test_deps,\n+        \"//jax:config_build_jaxlib_true\": test_deps,\n         \"//jax_plugins/cuda:disable_jaxlib_for_cpu_build\": jaxlib_pypi_wheel_deps,\n         \"//jax_plugins/cuda:disable_jaxlib_for_cuda12_build\": gpu_pypi_wheel_deps,\n         \"//jax_plugins/cuda:enable_py_import_for_cpu_build\": cpu_py_imports,\n@@ -250,9 +242,9 @@ def _get_jax_test_deps(deps):\n     Returns:\n       A list of jax test deps.\n \n-      If --//jax:enable_jax_build=true, returns jax build deps.\n-      If --//jax:enable_jax_build=false, returns jax pypi wheel dep and transitive pypi test deps.\n-      If --//jax:enable_jax_build=wheel, returns jax py_import dep and transitive pypi test deps.\n+      If --//jax:build_jax=true, returns jax build deps.\n+      If --//jax:build_jax=false, returns jax pypi wheel dep and transitive pypi test deps.\n+      If --//jax:build_jax=wheel, returns jax py_import dep and transitive pypi test deps.\n     \"\"\"\n     jax_build_deps = [d for d in deps if not d.startswith(\"@pypi//\")]\n \n\n```",
        "from_id": [
            "hawkinsp",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Prepare to make DmaCopyChunk movable.\n\nPiperOrigin-RevId: 758436699",
        "output": "```diff\nCommit: 9b2043e4f2b434d640a9460fc730a7b7300d9d82\nDate: 2025-05-14T00:16:38Z\nURL: https://github.com/jax-ml/jax/commit/9b2043e4f2b434d640a9460fc730a7b7300d9d82\nFiles changed: 1\nAdditions: +3, Deletions: -3\ndiff --git a/jaxlib/py_socket_transfer.cc b/jaxlib/py_socket_transfer.cc\nindex ed2a4f4c204a..114e3c14874d 100644\n--- a/jaxlib/py_socket_transfer.cc\n+++ b/jaxlib/py_socket_transfer.cc\n@@ -132,9 +132,9 @@ class IfrtArrayEntry : public PullTable::Entry {\n             std::min(xfer_size_, arrs_[bid].buf_size - i * xfer_size_));\n         bool is_largest = blob.size + blob.offset == arrs_[bid].buf_size;\n         state_->ScheduleCopy(\n-            blob, [req_id, state, copier_state = state_, is_largest](\n-                      PremappedCopierState* copier_state_ptr, void* buf,\n-                      const DmaCopyChunk& chunk) {\n+            std::move(blob), [req_id, state, copier_state = state_, is_largest](\n+                                 PremappedCopierState* copier_state_ptr,\n+                                 void* buf, const DmaCopyChunk& chunk) {\n               state->Send(\n                   req_id, buf, chunk.offset, chunk.size, is_largest,\n                   [copier_state, buf]() { copier_state->ReturnBuffer(buf); });\n\n```",
        "from_id": [
            "pschuh",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Global find/replace in jax for s/divisble/divisible.\n\nSpotted this in an error message, then saw there were a few other places with this typo.\n\nPiperOrigin-RevId: 758416484",
        "output": "```diff\nCommit: e2c4a7f53fb95ae30ef2ff74fcb7107e05a94aa9\nDate: 2025-05-13T23:16:50Z\nURL: https://github.com/jax-ml/jax/commit/e2c4a7f53fb95ae30ef2ff74fcb7107e05a94aa9\nFiles changed: 3\nAdditions: +5, Deletions: -4\ndiff --git a/jax/_src/lax/slicing.py b/jax/_src/lax/slicing.py\nindex 9f4645dca975..ad8a2cf0b315 100644\n--- a/jax/_src/lax/slicing.py\n+++ b/jax/_src/lax/slicing.py\n@@ -1353,9 +1353,10 @@ def _get_sharding_for_varying_out_shape(out_shape, operand, name):\n     if (op_sh != out_sh and op_spec is not None and\n         out_sh % _get_sub_spec_size(mesh, op_spec) != 0):\n       raise core.ShardingTypeError(\n-          f\"{name} on sharded dims where out dim ({out_sh}) is not divisble by\"\n+          f\"{name} on sharded dims where out dim ({out_sh}) is not divisible by\"\n           f\" mesh axes ({_get_sub_spec_size(mesh, op_spec)}) with spec\"\n-          f\" ({op_spec}) is not implemented.\")\n+          f\" ({op_spec}) is not implemented.\"\n+      )\n   # TODO(yashkatariya): Returning operand.sharding as is may or may not move\n   # data. So think about how to avoid it which might include creating a new\n   # mesh? For example:\ndiff --git a/jaxlib/mosaic/gpu/runtime.cc b/jaxlib/mosaic/gpu/runtime.cc\nindex cb48a20dc3d5..da7b0159d7b2 100644\n--- a/jaxlib/mosaic/gpu/runtime.cc\n+++ b/jaxlib/mosaic/gpu/runtime.cc\n@@ -115,7 +115,7 @@ void mosaic_gpu_init_tma_desc(CUtensorMap *tma_desc, void *base_addr,\n     if (tma_stride_i % 16 != 0 || tma_stride_i >= static_cast<cuuint64_t>(1)\n                                                       << 40) {\n       fprintf(stderr,\n-              \"Byte strides must be divisble by 16 and less than 2**40, but \"\n+              \"Byte strides must be divisible by 16 and less than 2**40, but \"\n               \"got %ld (item stride = %ld, item size = %ld) at index %ld\\n\",\n               tma_stride_i, strides[rank - 1], elem_bytewidth, rank - i - 2);\n       abort();\ndiff --git a/tests/pallas/pallas_jumble_test.py b/tests/pallas/pallas_jumble_test.py\nindex 509ef08a987f..0a2994a84a8f 100644\n--- a/tests/pallas/pallas_jumble_test.py\n+++ b/tests/pallas/pallas_jumble_test.py\n@@ -354,7 +354,7 @@ def invoke_kernel(x):\n \n     with self.assertRaisesRegex(\n         ValueError,\n-        \"Ragged input shape must be evenly divisble by the grid\"  # noqa: W605\n+        \"Ragged input shape must be evenly divisible by the grid\"  # noqa: W605\n         \" size at the ragged dimension 2\",\n     ):\n       jax.vmap(\n\n```",
        "from_id": [
            "jkr26",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Deprecate the no-op custom_jvp_call_jaxpr_p import stub.\n\nThe `custom_jvp_call_jaxpr_p` primitive has not been used for a long time, and the existing object is just an import stub. Let's try to clean up some @mattjj TODOs!\n\nSadly, since this lives in the public API, I think we need to do a full deprecation cycle, so let's at least get that started!\n\nPiperOrigin-RevId: 758415957",
        "output": "```diff\nCommit: c7a4e34bea51921b1d359be37ef0b5d59de944ab\nDate: 2025-05-13T23:14:55Z\nURL: https://github.com/jax-ml/jax/commit/c7a4e34bea51921b1d359be37ef0b5d59de944ab\nFiles changed: 3\nAdditions: +24, Deletions: -2\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 518e854b5bb1..a0c30132c169 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -27,6 +27,10 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n   * `jax.ShapeDtypeStruct` is immutable now. Please use `.update` method to\n     update your `ShapeDtypeStruct` instead of doing in-place updates.\n \n+* Deprecations\n+  * `jax.custom_derivatives.custom_jvp_call_jaxpr_p` is deprecated, and will be\n+    removed in JAX v0.7.0.\n+\n ## JAX 0.6.0 (April 16, 2025)\n \n * Breaking changes\ndiff --git a/jax/custom_derivatives.py b/jax/custom_derivatives.py\nindex b768b687dfad..6674046dd8e8 100644\n--- a/jax/custom_derivatives.py\n+++ b/jax/custom_derivatives.py\n@@ -23,7 +23,7 @@\n   custom_gradient as custom_gradient,\n   custom_jvp as custom_jvp,\n   custom_jvp_call_p as custom_jvp_call_p,\n-  custom_jvp_call_jaxpr_p as custom_jvp_call_jaxpr_p,\n+  custom_jvp_call_jaxpr_p as _custom_jvp_call_jaxpr_p,\n   custom_vjp as custom_vjp,\n   custom_vjp_call_p as custom_vjp_call_p,\n   custom_vjp_primal_tree_values as custom_vjp_primal_tree_values,\n@@ -36,3 +36,22 @@\n   SymbolicZero as SymbolicZero,\n   zero_from_primal as zero_from_primal\n )\n+\n+_deprecations = {\n+    # Added May 12, 2025\n+    \"custom_jvp_call_jaxpr_p\": (\n+      (\"jax.custom_derivatives.custom_jvp_call_jaxpr_p is deprecated, use \"\n+       \"jax.extend.core.primitives.custom_jvp_call_p instead.\"),\n+      _custom_jvp_call_jaxpr_p,\n+    ),\n+}\n+\n+import typing\n+if typing.TYPE_CHECKING:\n+  custom_jvp_call_jaxpr_p = _custom_jvp_call_jaxpr_p\n+else:\n+  from jax._src.deprecations import deprecation_getattr as _deprecation_getattr\n+  __getattr__ = _deprecation_getattr(__name__, _deprecations)\n+  del _deprecation_getattr\n+del typing\n+del _custom_jvp_call_jaxpr_p\ndiff --git a/jax/extend/core/primitives.py b/jax/extend/core/primitives.py\nindex 30350dace637..515dd3e11dcf 100644\n--- a/jax/extend/core/primitives.py\n+++ b/jax/extend/core/primitives.py\n@@ -24,7 +24,6 @@\n \n from jax._src.custom_derivatives import (\n   custom_jvp_call_p as custom_jvp_call_p,\n-  custom_jvp_call_jaxpr_p as custom_jvp_call_jaxpr_p,\n   custom_vjp_call_p as custom_vjp_call_p,\n )\n \n\n```",
        "from_id": [
            "dfm",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28722 from jakevdp:jaxlib-api-deprecation\n\nPiperOrigin-RevId: 758401882",
        "output": "```diff\nCommit: e6b47535adae06e51efdb0c7f289478450229727\nDate: 2025-05-13T22:36:46Z\nURL: https://github.com/jax-ml/jax/commit/e6b47535adae06e51efdb0c7f289478450229727\nFiles changed: 1\nAdditions: +5, Deletions: -0\ndiff --git a/docs/api_compatibility.md b/docs/api_compatibility.md\nindex 9dca1fc08f50..dda86e2e5d31 100644\n--- a/docs/api_compatibility.md\n+++ b/docs/api_compatibility.md\n@@ -59,6 +59,11 @@ Any API or import path prefixed with an underscore is explicitly private,\n and may change without warning between JAX releases. We are working to move\n all private APIs into `jax._src` to make these expectations more clear.\n \n+### jaxlib\n+Any import path in the `jaxlib` package is considered private, and may change\n+without warning between releases. Some APIs defined in `jaxlib` have public\n+aliases in the `jax` package.\n+\n ### Legacy internal APIs\n In addition, there are several legacy modules that currently expose some\n private APIs without an underscore, including:\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic TPU] Fold reshape (..., M, N, 128) -> (..., M, N * 128) with any tiling and dtype to load.\n\nNow we should support decent number of reshape that match with this pattern. And it is much more efficient to fold reshape (retiling + re-pack) into load.\n\nTake bf16(256, 8, 128) -> bf16(256, 8 * 128) as example, this cl emits 148 bundles and compared to before 630 bundles - about 4.2x speedup.\n\nPiperOrigin-RevId: 758398675",
        "output": "```diff\nCommit: 7887298da20d00af7db58a524beeeebdef4f776a\nDate: 2025-05-13T22:28:02Z\nURL: https://github.com/jax-ml/jax/commit/7887298da20d00af7db58a524beeeebdef4f776a\nFiles changed: 6\nAdditions: +277, Deletions: -15\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu.td b/jaxlib/mosaic/dialect/tpu/tpu.td\nindex 226fc6285192..29ce9c84de07 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu.td\n+++ b/jaxlib/mosaic/dialect/tpu/tpu.td\n@@ -1006,6 +1006,8 @@ def CanonicalizeMosaicPass : Pass<\"tpu-canonicalize-mosaic\", \"::mlir::func::Func\n   let options = [\n     Option<\"hardware_generation\", \"hardware-generation\", \"int\", /*default=*/\"-1\", \"\">,\n     Option<\"compatibility_mode\", \"compatibility-mode\", \"bool\", /*default=*/\"1\", \"\">,\n+    Option<\"lane_count\", \"lane-count\", \"int\", /*default=*/\"128\", \"\">,\n+    Option<\"sublane_count\", \"sublane-count\", \"int\", /*default=*/\"8\", \"\">,\n   ];\n }\n \ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu_dialect.h b/jaxlib/mosaic/dialect/tpu/tpu_dialect.h\nindex 2afaf08f29ed..798386b92744 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu_dialect.h\n+++ b/jaxlib/mosaic/dialect/tpu/tpu_dialect.h\n@@ -74,7 +74,8 @@ std::unique_ptr<OperationPass<func::FuncOp>> createInferMemRefLayoutPass(\n     const TpuTilingFlags &tpu_tiling_flags = {});\n \n std::unique_ptr<OperationPass<func::FuncOp>> createCanonicalizeMosaicPass(\n-    int hardware_generation = -1, bool compatibility_mode = true);\n+    int hardware_generation = -1, bool compatibility_mode = true,\n+    std::array<int64_t, 2> target_shape = {8, 128});\n \n std::unique_ptr<OperationPass<func::FuncOp>> createInferVectorLayoutPass(\n     int hardware_generation = -1,\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\nindex 71e48539f4dd..645e6d615722 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n@@ -14,6 +14,7 @@ limitations under the License.\n ==============================================================================*/\n \n #include <algorithm>\n+#include <array>\n #include <cstdint>\n #include <functional>\n #include <limits>\n@@ -66,6 +67,8 @@ struct CanonicalizeContext {\n   bool compatibility_mode;\n \n   int hardware_generation;\n+\n+  std::array<int64_t, 2> target_shape;\n };\n \n bool need_elementwise_canonicalization(const CanonicalizeContext &ctx,\n@@ -753,6 +756,149 @@ LogicalResult canonicalize_vector_transpose(const CanonicalizeContext &ctx,\n   return success();\n }\n \n+LogicalResult canonicalize_reshape(const CanonicalizeContext &ctx,\n+                                   Operation &raw_op) {\n+  auto op = cast<vector::ShapeCastOp>(raw_op);\n+  // We can canonicalize some reshape(load(x)) -> strided load + ALU ops.\n+  auto src = op.getSource();\n+  auto src_ty = src.getType();\n+  auto tgt_ty = op.getType();\n+  if (auto load_op = src.getDefiningOp<vector::LoadOp>()) {\n+    // Pattern match (..., M, N, 128) -> (..., M, N * 128).\n+    // This reshape can be folded into the load for any dtype and tiling\n+    // as long as the minormost dim is 128 and N is aligned to packing. The\n+    // pseudo code is:\n+    // ```\n+    // src_ref: (M, N, 128) with src_ty\n+    //\n+    // def load_to_reshape(src_ref):\n+    //   b_ref = src_ref.bitcast(i32) # i32[M, N / packing, 128]\n+    //   r_ref = b_ref.reshape(M * N / packing, 128)\n+    //   chunks = []\n+    //   for i in range(N / packing):\n+    //     v = r_ref[i::N / packing, :] # i32[M, 128]\n+    //     for j in range(packing):\n+    //       chunk = v >> (j * bitwidth)\n+    //       chunks.append(chunk)\n+    //   res = concat(chunks, axis=-1) # i32[M, N * 128]\n+    //   # int_src_ty refers to int type with the same bitwidth as src_ty.\n+    //   res = res.astype(int_src_ty) # Trigger i32 -> int_src_ty packing.\n+    //   return bitcast(res, src_ty) # src_ty[M, N * 128]\n+    // ```\n+    // TODO(jevinjiang): we can extend this to support folding more dims to last\n+    // dim not just last 2 dims.\n+    auto bitwidth = src_ty.getElementTypeBitWidth();\n+    auto packing = 32 / bitwidth;\n+    if (packing <= 0) {\n+      return op.emitOpError(\"Unsupported bitwidth = \") << bitwidth;\n+    }\n+    // Memref bitcast is not supported if HW generation is below 4. We don't\n+    // return failure because we will rely on vector reshape.\n+    if ((ctx.hardware_generation < 4 && packing > 1) ||\n+        (ctx.hardware_generation == 4 && packing > 2)) {\n+      return success();\n+    }\n+    auto ref = load_op.getBase();\n+    auto indices = load_op.getIndices();\n+    auto ref_shape = ref.getType().getShape();\n+    auto src_shape = src_ty.getShape();\n+    auto tgt_shape = tgt_ty.getShape();\n+    int ref_rank = ref_shape.size();\n+    int src_rank = src_shape.size();\n+    int tgt_rank = tgt_shape.size();\n+    if (ref_rank != src_rank) {\n+      return op.emitOpError(\"Loaded vector rank and memref rank mismatch\");\n+    }\n+    // Check the memref's eligibility.\n+    if (!isContiguousMemref(ref) || ref_rank <= 2 ||\n+        // TODO(jevinjiang): add support for partial load on last 2 dims where\n+        // last 2 indices are not necessarily 0 or load shape is not full.\n+        getIntConst(indices[ref_rank - 1]) != 0 ||\n+        getIntConst(indices[ref_rank - 2]) != 0 ||\n+        ref_shape[ref_rank - 1] != src_shape[src_rank - 1] ||\n+        ref_shape[ref_rank - 2] != src_shape[src_rank - 2]) {\n+      return success();\n+    }\n+    // Check the reshape's eligibility.\n+    if (src_rank != tgt_rank + 1 || src_shape[src_rank - 2] % packing != 0 ||\n+        src_shape[src_rank - 1] != ctx.target_shape[1] ||\n+        src_shape[src_rank - 2] * src_shape[src_rank - 1] !=\n+            tgt_shape[tgt_rank - 1]) {\n+      return success();\n+    }\n+    // At this point, the pattern is matched.\n+    ImplicitLocOpBuilder builder(op->getLoc(), op.getOperation());\n+    auto loc = op.getLoc();\n+    // First, we bitcast and reshape src ref from (..., M, N, 128) to\n+    // i32(..., M * N / packing, 128).\n+    SmallVector<int64_t> bitcast_shape(ref_shape);\n+    // TODO(jevinjiang): once we have memref pad op, we can use ceiling\n+    // division to ref_shape[ref_rank - 2] and packing to get sublane_cnt.\n+    CHECK_EQ(ref_shape[ref_rank - 2] % packing, 0);\n+    auto i32_2nd_minor_size = ref_shape[ref_rank - 2] / packing;\n+    bitcast_shape[ref_rank - 2] = i32_2nd_minor_size;\n+    auto i32_ref = builder.create<tpu::MemRefBitcastOp>(\n+        MemRefType::get(bitcast_shape, builder.getI32Type()), ref);\n+\n+    SmallVector<int64_t> reshape_shape(ref_shape.begin(),\n+                                       ref_shape.begin() + tgt_rank);\n+    reshape_shape[tgt_rank - 1] = ctx.target_shape[1];\n+    reshape_shape[tgt_rank - 2] = ref_shape[ref_rank - 3] * i32_2nd_minor_size;\n+    auto reshape_ref = builder.create<tpu::MemRefReshapeOp>(\n+        MemRefType::get(reshape_shape, builder.getI32Type()), i32_ref);\n+\n+    // We also need to transform the indices while transforming the memref.\n+    SmallVector<Value> new_indices(indices.begin(), indices.begin() + tgt_rank);\n+    new_indices[tgt_rank - 1] = IdxConst(0, builder, loc);\n+    new_indices[tgt_rank - 2] = builder.create<arith::MulIOp>(\n+        builder.getIndexType(), indices[ref_rank - 3],\n+        IdxConst(i32_2nd_minor_size, builder, loc));\n+    // Then, we strided load the bitcasted ref by stride (N / packing).\n+    int stride = i32_2nd_minor_size;\n+    // Expect to hold src_shape[src_rank - 2] number of chunks which have the\n+    // shape (..., src_shape[src_rank - 3], 128) and wait to be concatenated\n+    // along the last dim.\n+    SmallVector<Value> chunks(src_shape[src_rank - 2]);\n+    SmallVector<int64_t> chunk_shape(tgt_shape);\n+    chunk_shape[tgt_rank - 1] = ctx.target_shape[1];\n+    SmallVector<int32_t> strides(tgt_rank, 1);\n+    strides[tgt_rank - 2] = stride;\n+    auto tgt_2nd_minor_idx = new_indices[tgt_rank - 2];\n+    for (int i = 0; i < stride; ++i) {\n+      new_indices[tgt_rank - 2] = builder.create<arith::AddIOp>(\n+          builder.getIndexType(), tgt_2nd_minor_idx, IdxConst(i, builder, loc));\n+      auto chunk = builder.create<tpu::StridedLoadOp>(\n+          VectorType::get(chunk_shape, builder.getI32Type()), reshape_ref,\n+          new_indices, strides);\n+      for (int j = 0; j < packing; ++j) {\n+        int idx = i * packing + j;\n+        chunks[idx] = builder.create<arith::ShRUIOp>(\n+            chunk.getType(), chunk,\n+            I32Const(j * bitwidth, chunk_shape, builder, loc));\n+      }\n+    }\n+    // Concatenate the chunks along the last dim to get i32(..., M, N * 128).\n+    CHECK_GT(chunks.size(), 0);\n+    Value i32_tgt = chunks[0];\n+    if (chunks.size() > 1) {\n+      i32_tgt = builder.create<tpu::ConcatenateOp>(\n+          VectorType::get(tgt_shape, builder.getI32Type()), chunks,\n+          /*dimension=*/tgt_rank - 1);\n+    }\n+    Value tgt = i32_tgt;\n+    // Convert to target dtype.\n+    if (packing > 1) {\n+      tgt = builder.create<arith::TruncIOp>(\n+          VectorType::get(tgt_shape, builder.getIntegerType(bitwidth)),\n+          i32_tgt);\n+    }\n+    tgt = builder.create<arith::BitcastOp>(tgt_ty, tgt);\n+    op.replaceAllUsesWith(tgt);\n+    op.erase();\n+  }\n+  return success();\n+}\n+\n using canonicalize_rule_type =\n     std::function<LogicalResult(const CanonicalizeContext &ctx, Operation &op)>;\n \n@@ -764,6 +910,7 @@ const llvm::StringMap<canonicalize_rule_type> &rules() {\n       {vector::MultiDimReductionOp::getOperationName(),\n        canonicalize_multi_dim_reduction},\n       {vector::TransposeOp::getOperationName(), canonicalize_vector_transpose},\n+      {vector::ShapeCastOp::getOperationName(), canonicalize_reshape},\n       {arith::SelectOp::getOperationName(), canonicalize_select},\n       {arith::FPToSIOp::getOperationName(), canonicalize_fptosi},\n       {arith::SIToFPOp::getOperationName(), canonicalize_sitofp},\n@@ -808,12 +955,15 @@ bool need_elementwise_canonicalization(const CanonicalizeContext &ctx,\n \n class MosaicCanonicalizer {\n  public:\n-  MosaicCanonicalizer(int hardware_generation, bool compatibility_mode)\n+  MosaicCanonicalizer(int hardware_generation, bool compatibility_mode,\n+                      std::array<int64_t, 2> target_shape)\n       : hardware_generation_(hardware_generation),\n-        compatibility_mode_(compatibility_mode) {}\n+        compatibility_mode_(compatibility_mode),\n+        target_shape_(target_shape) {}\n \n   int hardware_generation_;\n   bool compatibility_mode_;\n+  std::array<int64_t, 2> target_shape_;\n \n   LogicalResult canonicalize(func::FuncOp op) {\n     if (!op.getBody().hasOneBlock()) {\n@@ -834,7 +984,8 @@ class MosaicCanonicalizer {\n   }\n \n   LogicalResult canonicalizeOp(Operation &any_op) {\n-    CanonicalizeContext ctx({compatibility_mode_, hardware_generation_});\n+    CanonicalizeContext ctx(\n+        {compatibility_mode_, hardware_generation_, target_shape_});\n     // We must iterate over the op first, because canonicalization can cause\n     // us to .erase() an op, and accessing getRegions on it after is not sound.\n     // Invariant - top level ops with regions may never be invalidated.\n@@ -859,14 +1010,18 @@ class MosaicCanonicalizer {\n \n struct CanonicalizeMosaicPass\n     : public impl::CanonicalizeMosaicPassBase<CanonicalizeMosaicPass> {\n-  CanonicalizeMosaicPass(int hardware_generation_p, bool compatibility_mode_p)\n+  CanonicalizeMosaicPass(int hardware_generation_p, bool compatibility_mode_p,\n+                         std::array<int64_t, 2> target_shape)\n       : compatibility_mode_(compatibility_mode_p) {\n     this->hardware_generation = hardware_generation_p;\n+    this->sublane_count = target_shape[0];\n+    this->lane_count = target_shape[1];\n   }\n \n   void runOnOperation() override {\n     func::FuncOp func = getOperation();\n-    MosaicCanonicalizer vlc(hardware_generation, compatibility_mode_);\n+    MosaicCanonicalizer vlc(hardware_generation, compatibility_mode_,\n+                            {sublane_count, lane_count});\n     if (vlc.canonicalize(func).failed()) {\n       signalPassFailure();\n     }\n@@ -878,9 +1033,10 @@ struct CanonicalizeMosaicPass\n }  // namespace\n \n std::unique_ptr<OperationPass<func::FuncOp>> createCanonicalizeMosaicPass(\n-    int hardware_generation, bool compatibility_mode) {\n-  return std::make_unique<CanonicalizeMosaicPass>(hardware_generation,\n-                                                  compatibility_mode);\n+    int hardware_generation, bool compatibility_mode,\n+    std::array<int64_t, 2> target_shape) {\n+  return std::make_unique<CanonicalizeMosaicPass>(\n+      hardware_generation, compatibility_mode, target_shape);\n }\n \n }  // namespace mlir::tpu\ndiff --git a/jaxlib/mosaic/dialect/tpu/util.cc b/jaxlib/mosaic/dialect/tpu/util.cc\nindex bb42c678bbf6..b562f81ad534 100644\n--- a/jaxlib/mosaic/dialect/tpu/util.cc\n+++ b/jaxlib/mosaic/dialect/tpu/util.cc\n@@ -27,7 +27,9 @@ limitations under the License.\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/Support/MathExtras.h\"\n #include \"llvm/Support/raw_ostream.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/Value.h\"\n@@ -159,6 +161,17 @@ bool canReinterpretToUntiledMemref(TypedValue<MemRefType> tiled_memref,\n          *(tiled_layout.getTileStrides().end() - 2) == 1;\n }\n \n+bool isContiguousMemref(TypedValue<MemRefType> memref) {\n+  auto memref_ty = getMemRefType(memref);\n+  if (auto tiled_layout =\n+          dyn_cast<tpu::TiledLayoutAttr>(memref_ty.getLayout())) {\n+    auto contiguous_tile_strides = ComputeTileStrides(\n+        memref_ty, tiled_layout.getTiles().front().dimensions());\n+    return contiguous_tile_strides == tiled_layout.getTileStrides();\n+  }\n+  return true;\n+}\n+\n bool HasMemorySpace(MemRefType ty, tpu::MemorySpace space) {\n   auto memory_space =\n       dyn_cast_or_null<tpu::MemorySpaceAttr>(ty.getMemorySpace());\n@@ -278,4 +291,14 @@ void setLayout(Operation *op, ArrayRef<Layout> in, ArrayRef<Layout> out) {\n   setInLayout(op, in);\n   setOutLayout(op, out);\n }\n+\n+std::optional<int64_t> getIntConst(Value v) {\n+  if (auto const_op = v.getDefiningOp<arith::ConstantOp>()) {\n+    if (auto cst_attr = dyn_cast<IntegerAttr>(const_op.getValue())) {\n+      return cst_attr.getValue().getSExtValue();\n+    }\n+  }\n+  return std::nullopt;\n+}\n+\n }  // namespace mlir::tpu\ndiff --git a/jaxlib/mosaic/dialect/tpu/util.h b/jaxlib/mosaic/dialect/tpu/util.h\nindex b9aea1b087dc..000cb4411e62 100644\n--- a/jaxlib/mosaic/dialect/tpu/util.h\n+++ b/jaxlib/mosaic/dialect/tpu/util.h\n@@ -195,11 +195,6 @@ ArrayRef<std::remove_const_t<T>> toArrayRef(absl::Span<T> span) {\n   return ArrayRef<std::remove_const_t<T>>(span.data(), span.size());\n }\n \n-inline arith::ConstantOp IdxConst(int64_t idx, OpBuilder &builder,\n-                                  Location loc) {\n-  return builder.create<arith::ConstantOp>(loc, builder.getIndexType(),\n-                                           builder.getIndexAttr(idx));\n-}\n \n // Debug only util.\n template <typename T>\n@@ -242,6 +237,8 @@ bool canReinterpretToUntiledMemref(TypedValue<MemRefType> tiled_memref,\n                                    const std::array<int64_t, 2> &target_shape,\n                                    bool allow_minormost_padding = false);\n \n+bool isContiguousMemref(TypedValue<MemRefType> memref);\n+\n // Determines whether the given MemRefType has the given memory space.\n bool HasMemorySpace(MemRefType ty, tpu::MemorySpace space);\n \n@@ -264,6 +261,30 @@ void setLayout(Operation *op, Layout in, Layout out);\n void setLayout(Operation *op, ArrayRef<Layout> in, Layout out);\n void setLayout(Operation *op, Layout in, ArrayRef<Layout> out);\n void setLayout(Operation *op, ArrayRef<Layout> in, ArrayRef<Layout> out);\n+\n+// Helper functions to create constants.\n+inline arith::ConstantOp IdxConst(int64_t idx, OpBuilder &builder,\n+                                  Location loc) {\n+  return builder.create<arith::ConstantOp>(loc, builder.getIndexType(),\n+                                           builder.getIndexAttr(idx));\n+}\n+\n+inline arith::ConstantOp I32Const(int32_t value, OpBuilder &builder,\n+                                  Location loc) {\n+  return builder.create<arith::ConstantOp>(loc, builder.getI32Type(),\n+                                           builder.getI32IntegerAttr(value));\n+}\n+\n+inline arith::ConstantOp I32Const(int32_t value, ArrayRef<int64_t> shape,\n+                                  OpBuilder &builder, Location loc) {\n+  return builder.create<arith::ConstantOp>(\n+      loc, DenseElementsAttr::get(\n+               VectorType::get(shape, builder.getI32Type()),\n+               builder.getIntegerAttr(builder.getI32Type(), value)));\n+}\n+\n+// TODO(jevinjiang): consolidate this with getIntConst in apply-vector-layout.\n+std::optional<int64_t> getIntConst(Value v);\n }  // namespace mlir::tpu\n \n #endif  // THIRD_PARTY_PY_JAX_JAXLIB_MOSAIC_DIALECT_TPU_UTIL_H_\ndiff --git a/tests/pallas/tpu_ops_test.py b/tests/pallas/tpu_ops_test.py\nindex 1fb0bc24701b..de87126ebd3f 100644\n--- a/tests/pallas/tpu_ops_test.py\n+++ b/tests/pallas/tpu_ops_test.py\n@@ -38,16 +38,36 @@\n jax.config.parse_flags_with_absl()\n jtu.setup_hypothesis(max_examples=100)\n \n-_JAX_DTYPES = (\n+_JAX_DTYPES_NO_BOOL = (\n     jnp.float32,\n     jnp.bfloat16,\n     jnp.int32,\n     jnp.int16,\n     jnp.int8,\n+    jnp.int4,\n+    jnp.float8_e5m2,\n+)\n+\n+_JAX_DTYPES = (\n+    *_JAX_DTYPES_NO_BOOL,\n     jnp.bool_,\n )\n \n \n+def rand(\n+    shape: tuple[int, ...], dtype: np.dtype | jnp.dtype, seed: int = 1234\n+) -> np.ndarray:\n+  \"\"\"A helper function to generate random data for testing.\"\"\"\n+  rng = np.random.Generator(np.random.Philox(counter=0, key=seed))\n+  if jnp.issubdtype(dtype, jnp.floating):\n+    return rng.normal(size=shape).astype(dtype)\n+  if jnp.issubdtype(dtype, jnp.integer):\n+    return rng.integers(\n+        jnp.iinfo(dtype).min, jnp.iinfo(dtype).max, shape, dtype=np.int32\n+    ).astype(dtype)\n+  raise NotImplementedError(f\"Unsupported random data generation for {dtype=}\")\n+\n+\n class PallasBaseTest(jtu.JaxTestCase):\n   INTERPRET = False\n \n@@ -511,6 +531,45 @@ def kernel(src, tgt):\n         output[tuple(slice(0, d) for d in src_shape)], x\n     )\n \n+  # TODO(jevinjiang): we need to support strided load for bool.\n+  @parameterized.product(dtype=_JAX_DTYPES_NO_BOOL)\n+  @hp.given(\n+      slice_start=hps.integers(0, 3),\n+      slice_size=hps.integers(1, 3),\n+      m=hps.integers(1, 32),\n+      # Need to make sure the 2nd minor has no padding.\n+      n=hps.sampled_from([1, 2, 4, 8, 16, 24, 32]),\n+  )\n+  @hp.settings(max_examples=20)  # 20 examples for each dtype.\n+  def test_load_to_reshape(self, dtype, slice_start, slice_size, m, n):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 15):\n+      self.skipTest(\"Requires libtpu built after 2025-05-15\")\n+    bitwidth = pallas_utils.dtype_bitwidth(dtype)\n+    if jtu.get_tpu_version() < 4 and bitwidth != 32:\n+      self.skipTest(\"Requires TPUv4+ for non-32-bit types\")\n+    if jtu.get_tpu_version() == 4 and bitwidth <= 8:\n+      self.skipTest(\"Int8 is not supported on this target\")\n+    packing = 32 // bitwidth\n+    n *= packing\n+    slices = (\n+        slice(slice_start, slice_start + slice_size),\n+        slice(slice_start, slice_start + m),\n+        slice(None),\n+        slice(None),\n+    )\n+    inp_shape = (8, 64, n, 128)\n+    out_shape = (slice_size, m, n * 128)\n+\n+    def kernel(inp_ref, out_ref):\n+      inp = inp_ref[slices]\n+      out_ref[...] = inp.reshape(out_shape)\n+\n+    inp = rand(inp_shape, dtype, seed=1234)\n+    run = pl.pallas_call(kernel, jax.ShapeDtypeStruct(out_shape, dtype))\n+    output = run(inp)\n+    expected = inp[slices].reshape(out_shape)\n+    np.testing.assert_array_equal(output, expected)\n+\n \n @jtu.thread_unsafe_test_class()  # hypothesis is not thread safe\n class OpsInterpretTest(OpsTest):\n\n```",
        "from_id": [
            "bythew3i",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28661 from jakevdp:jax-array-jep\n\nPiperOrigin-RevId: 758381022",
        "output": "```diff\nCommit: 0c10daedea6e7e0fc4f164e8fe515b7f1c379bb8\nDate: 2025-05-13T21:38:49Z\nURL: https://github.com/jax-ml/jax/commit/0c10daedea6e7e0fc4f164e8fe515b7f1c379bb8\nFiles changed: 2\nAdditions: +215, Deletions: -0\ndiff --git a/docs/jep/28661-jax-array-protocol.md b/docs/jep/28661-jax-array-protocol.md\nnew file mode 100644\nindex 000000000000..e05d69d2822d\n--- /dev/null\n+++ b/docs/jep/28661-jax-array-protocol.md\n@@ -0,0 +1,214 @@\n+# JEP 28661: Supporting the `__jax_array__` protocol\n+\n+[@jakevdp](http://github.com/jakevdp), *May 2025*\n+\n+An occasional user request is for the ability to define custom array-like objects that\n+work with jax APIs. JAX currently has a partial implementation of a mechanism that does\n+this via a `__jax_array__` method defined on the custom object. This was never intended\n+to be a load-bearing public API (see the discussion at {jax-issue}`#4725`), but has\n+become essential to packages like Keras and flax, which explicitly document the ability\n+to use their custom array objects with jax functions. This JEP proposes a design for\n+full, documented support of the `__jax_array__` protocol.\n+\n+## Levels of array extensibility\n+Requests for extensibility of JAX arrays come in a few flavors:\n+\n+### Level 1 Extensibility: polymorphic inputs\n+What Ill call \"Level 1\" extensibility is the desire that JAX APIs accept polymorphic inputs.\n+That is, a user desires behavior like this:\n+\n+```python\n+class CustomArray:\n+  data: numpy.ndarray\n+  ...\n+\n+x = CustomArray(np.arange(5))\n+result = jnp.sin(x)  # Converts `x` to JAX array and returns a JAX array\n+```\n+\n+Under this extensibility model, JAX functions would accept CustomArray objects as inputs,\n+implicitly converting them to `jax.Array` objects for the sake of computation.\n+This is similar to the functionality offered by NumPy via the `__array__` method, and in\n+JAX (in many but not all cases) via the `__jax_array__` method.\n+\n+This is the mode of extensibility that has been requested by the maintainers of `flax.nnx`\n+and others. The current implementation is also used by JAX internally for the case of\n+symbolic dimensions.\n+\n+### Level 2 extensibility: polymorphic outputs\n+What Ill call \"Level 2\" extensibility is the desire that JAX APIs should not only accept\n+polymorphic inputs, but also wrap outputs to match the class of the input.\n+That is, a user desires behavior like this:\n+\n+```python\n+class CustomArray:\n+  data: numpy.ndarray\n+  ...\n+\n+x = CustomArray(np.arange(5))\n+result = jnp.sin(x)  # returns a new CustomArray\n+```\n+\n+Under this extensibility model, JAX functions would not only accept custom objects\n+as inputs, but have some protocol to determine how to correctly re-wrap outputs with\n+the same class. In NumPy, this sort of functionality is offered in varying degrees by\n+the special `__array_ufunc__`, `__array_wrap__`, and `__array_function__` protocols,\n+which allow user-defined objects to customize how NumPy API functions operate on\n+arbitrary inputs and map input types to outputs.\n+JAX does not currently have any equivalent to these interfaces in NumPy.\n+\n+This is the mode of extensibility that has been requested by the maintainers of `keras`,\n+among others.\n+\n+### Level 3 extensibility: subclassing `Array`\n+\n+What Ill call \"Level 3\" extensibility is the desire that the JAX array object itself\n+could be subclassable. NumPy provides some APIs that allow this\n+(see [Subclassing ndarray](https://numpy.org/devdocs/user/basics.subclassing.html)) but\n+this sort of approach would take some extra thought in JAX due to the need for\n+representing array objects abstractly via tracing.\n+\n+This mode of extensibility has occasionally been requested by users who want to add\n+special metadata to JAX arrays, such as units of measurement.\n+\n+## Synopsis\n+\n+For the sake of this proposal, we will stick with the simplest, level 1 extensibility\n+model. The proposed interface is the one currently non-uniformly supported by a number\n+of JAX APIs, the `__jax_array__` method. Its usage looks something like this:\n+\n+```python\n+import jax\n+import jax.numpy as jnp\n+import numpy as np\n+\n+class CustomArray:\n+  data: np.ndarray\n+\n+  def __init__(self, data: np.ndarray):\n+    self.data = data\n+\n+  def __jax_array__(self) -> jax.Array:\n+    return jnp.asarray(self.data)\n+\n+arr = CustomArray(np.arange(5))\n+result = jnp.multiply(arr, 2)\n+print(repr(result))\n+# Array([0, 2, 4, 6, 8], dtype=int32)\n+```\n+\n+We may revisit other extensibility levels in the future.\n+\n+## Design challenges\n+\n+JAX presents some interesting design challenges related to this kind of extensibility,\n+which have not been fully explored previously. Well discuss them in turn here:\n+\n+### Priority of `__jax_array__` vs. PyTree flattening\n+JAX already has a supported mechanism for registering custom objects, namely pytree\n+registration (see [Extending pytrees](https://docs.jax.dev/en/latest/pytrees.html#extending-pytrees)).\n+If we also support __jax_array__, which one should take precedence?\n+\n+To put this more concretely, what should be the result of this code?\n+\n+```python\n+@jax.jit\n+def f(x):\n+  print(\"is JAX array:\", isinstance(x, jax.Array))\n+\n+f(CustomArray(...))\n+```\n+\n+If we choose to prioritize `__jax_array__` at the JIT boundary, then the output of this\n+function would be:\n+```\n+is JAX array: True\n+```\n+That is, at the JIT boundary, the `CustomArray` object would be converted into a\n+`__jax_array__`, and its shape and dtype would be used to construct a standard JAX\n+tracer for the function.\n+\n+If we choose to prioritize pytree flattening at the JIT boundary, then the output of\n+this function would be:\n+```\n+type(x)=CustomArray\n+```\n+That is, at the JIT boundary, the `CustomArray` object is flattened, and then unflattened\n+before being passed to the JIT-compiled function for tracing. If `CustomArray` has been\n+registered as a pytree, it will generally contain traced arrays as its attributes, and\n+when x is passed to any JAX API that supports `__jax_array__`, these traced attributes\n+will be converted to a single traced array according to the logic specified in the method.\n+\n+There are deeper consequences here for how other transformations like vmap and grad work\n+when encountering custom objects: for example, if we prioritize pytree flattening, vmap\n+would operate over the dimensions of the flattened contents of the custom object, while\n+if we prioritize `__jax_array__`, vmap would operate over the converted array dimensions.\n+\n+This also has consequences when it comes to JIT invariance: consider a function like this:\n+```python\n+def f(x):\n+  if isinstance(x, CustomArray):\n+    return x.custom_method()\n+  else:\n+    # do something else\n+    ...\n+\n+result1 = f(x)\n+result2 = jax.jit(f)(x)\n+```\n+If `jit` consumes `x` via pytree flattening, the results should agree for a well-specified\n+flattening rule. If `jit` consumes `x` via `__jax_array__`, the results will differ because\n+`x` is no longer a CustomArray within the JIT-compiled version of the function.\n+\n+#### Synopsis\n+As of JAX v0.6.0, transformations prioritize `__jax_array__` when it is available. This status\n+quo can lead to confusion around lack of JIT invariance, and the current implementation in practice\n+leads to subtle bugs in the case of automatic differentiation, where the forward and backward pass\n+do not treat inputs consistently.\n+\n+Because the pytree extensibility mechanism already exists for the case of customizing\n+transformations, it seems most straightforward if transformations act only via this\n+mechanism: that is, **we propose to remove `__jax_array__` parsing during abstractification.**\n+This approach will preserve object identity through transformations, and give the user the\n+most possible flexibility. If the user wants to opt-in to array conversion semantics, that\n+is always possible by explicitly casting their input via jnp.asarray, which will trigger the \n+`__jax_array__` protocol.\n+\n+### Which APIs should support `__jax_array__`?\n+JAX has a number of different levels of API, from the level of explicit primitive binding\n+(e.g. `jax.lax.add_p.bind(x, y)`) to the `jax.lax` APIs (e.g. `jax.lax.add(x, y)`) to the\n+`jax.numpy` APIs (e.g. `jax.numpy.add(x, y)`). Which of these API categories should handle\n+implicit conversion via `__jax_array__`?\n+\n+In order to limit the scope of the change and the required testing, I propose that `__jax_array__`\n+only be explicitly supported in `jax.numpy` APIs: after all, it is inspired by the` __array__`\n+protocol which is supported by the NumPy package. We could always expand this in the future to\n+`jax.lax` APIs if needed.\n+\n+This is in line with the current state of the package, where `__jax_array__` handling is mainly\n+within the input validation utilities used by `jax.numpy` APIs.\n+\n+## Implementation\n+With these design choices in mind, we plan to implement this as follows:\n+\n+- **Adding runtime support to `jax.numpy`**: This is likely the easiest part, as most\n+  `jax.numpy` functions use a common internal utility (`ensure_arraylike`) to validate\n+  inputs and convert them to array. This utility already supports `__jax_array__`, and\n+  so most jax.numpy APIs are already compliant.\n+- **Adding test coverage**:  To ensure compliance across the APIs, we should add a new\n+  test scaffold that calls every `jax.numpy` API with custom inputs and validates correct\n+  behavior.\n+- **Deprecating `__jax_array__` during abstractification**: Currently JAX's abstractification\n+  pass, used in `jit` and other transformations, does parse the `__jax_array__` protocol,\n+  and this is not the behavior we want long-term. We need to deprecate this behavior, and\n+  ensure that downstream packages that rely on it can move toward pytree registration or\n+  explicit array conversion where necessary.\n+- **Adding type annotations**: the type interface for jax.numpy functions is in\n+  `jax/numpy/__init__.pyi`, and well need to change each input type from `ArrayLike` to\n+  `ArrayLike | SupportsJAXArray`, where the latter is a protocol with a `__jax_array__`\n+  method. We cannot add this directly to the `ArrayLike` definition, because `ArrayLike`\n+  is used in contexts where `__jax_array__` should not be supported.\n+- **Documentation**: once the above support is added, we should add a documentation section\n+  on array extensibility that outlines exactly what to expect regarding the `__jax_array__`\n+  protocol, with examples of how it can be used in conjunction with pytree registration\n+  in order to effectively work with user-defined types.\ndiff --git a/docs/jep/index.rst b/docs/jep/index.rst\nindex 1c4ecbb3411f..2ba85a5f4a8d 100644\n--- a/docs/jep/index.rst\n+++ b/docs/jep/index.rst\n@@ -52,6 +52,7 @@ Then create a pull request that adds a file named\n   17111: Efficient transposition of `shard_map` (and other maps) <17111-shmap-transpose>\n   18137: Scope of JAX NumPy & SciPy Wrappers <18137-numpy-scipy-scope>\n   25516: Effort-based versioning <25516-effver>\n+  28661: Supporting the `__jax_array__` protocol <28661-jax-array-protocol>\n \n \n Several early JEPs were converted in hindsight from other documentation,\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Avoid duplication of Bazel dependencies between //jax/_src/lib and //jaxlib.\n\nPut all dependencies on //jaxlib and make //jax/_src/lib a pure forwarding rule.\n\nPiperOrigin-RevId: 758378774",
        "output": "```diff\nCommit: b2549761e1cdee37b4d990bb28e97fdab7a99fe7\nDate: 2025-05-13T21:33:37Z\nURL: https://github.com/jax-ml/jax/commit/b2549761e1cdee37b4d990bb28e97fdab7a99fe7\nFiles changed: 2\nAdditions: +35, Deletions: -46\ndiff --git a/jax/_src/lib/BUILD b/jax/_src/lib/BUILD\nindex 4bbc861432aa..e0b5ea607501 100644\n--- a/jax/_src/lib/BUILD\n+++ b/jax/_src/lib/BUILD\n@@ -40,30 +40,5 @@ py_library_providing_imports_info(\n         \"//jax:version\",\n     ] + if_building_jaxlib([\n         \"//jaxlib\",\n-        \"//jaxlib/mosaic/python:gpu_dialect\",\n-        \"//jaxlib/mosaic/python:tpu_dialect\",\n-        \"//jaxlib:cpu_feature_guard\",\n-        \"//jaxlib:utils\",\n-        \"//jaxlib:weakref_lru_cache\",\n-        \"//jaxlib:xla_client\",\n-        \"//jaxlib:_jax\",\n-        \"//jaxlib/triton\",\n-        \"//jaxlib/mlir/_mlir_libs:register_jax_dialects\",\n-        \"//jaxlib/mlir:arithmetic_dialect\",\n-        \"//jaxlib/mlir:builtin_dialect\",\n-        \"//jaxlib/mlir:chlo_dialect\",\n-        \"//jaxlib/mlir:control_flow_dialect\",\n-        \"//jaxlib/mlir:func_dialect\",\n-        \"//jaxlib/mlir:ir\",\n-        \"//jaxlib/mlir:math_dialect\",\n-        \"//jaxlib/mlir:memref_dialect\",\n-        \"//jaxlib/mlir:mhlo_dialect\",\n-        \"//jaxlib/mlir:pass_manager\",\n-        \"//jaxlib/mlir:scf_dialect\",\n-        \"//jaxlib/mlir:sdy_dialect\",\n-        \"//jaxlib/mlir:sparse_tensor_dialect\",\n-        \"//jaxlib/mlir:stablehlo_dialect\",\n-        \"//jaxlib/mlir:vector_dialect\",\n-        \"@xla//xla/python:_profiler\",\n     ]),\n )\ndiff --git a/jaxlib/BUILD b/jaxlib/BUILD\nindex e0fb2699a25e..add6dbd7d92a 100644\n--- a/jaxlib/BUILD\n+++ b/jaxlib/BUILD\n@@ -22,7 +22,6 @@ load(\n     \"nanobind_extension\",\n     \"proto_library\",\n     \"py_deps\",\n-    \"py_library_providing_imports_info\",\n     \"py_strict_test\",\n     \"pytype_library\",\n     \"pytype_strict_library\",\n@@ -49,33 +48,17 @@ package_group(\n     ],\n )\n \n-py_library_providing_imports_info(\n+pytype_strict_library(\n     name = \"jaxlib\",\n-    srcs = [\n-        \"cpu_sparse.py\",\n-        \"gpu_common_utils.py\",\n-        \"gpu_linalg.py\",\n-        \"gpu_prng.py\",\n-        \"gpu_rnn.py\",\n-        \"gpu_solver.py\",\n-        \"gpu_sparse.py\",\n-        \"gpu_triton.py\",\n-        \"hlo_helpers.py\",\n-        \"init.py\",\n-        \"lapack.py\",\n-        \"plugin_support.py\",\n-        \"xla_client.py\",\n-        \":version\",\n-    ],\n     data = [\":ffi_headers\"],\n-    lib_rule = pytype_library,\n     deps = [\n+        \":_jax\",\n         \":cpu_feature_guard\",\n         \":jax\",\n+        \":jaxlib_files\",\n         \":utils\",\n         \":weakref_lru_cache\",\n-        \"//jaxlib:_jax\",\n-        \"//jaxlib:xla_client\",\n+        \":xla_client\",\n         \"//jaxlib/cpu:_lapack\",\n         \"//jaxlib/cpu:_sparse\",\n         \"//jaxlib/mlir\",\n@@ -98,8 +81,39 @@ py_library_providing_imports_info(\n         \"//jaxlib/mlir:sparse_tensor_dialect\",\n         \"//jaxlib/mlir:stablehlo_dialect\",\n         \"//jaxlib/mlir:vector_dialect\",\n+        \"//jaxlib/mlir/_mlir_libs:register_jax_dialects\",\n         \"//jaxlib/mosaic\",\n+        \"//jaxlib/mosaic/python:gpu_dialect\",\n+        \"//jaxlib/mosaic/python:tpu_dialect\",\n         \"//jaxlib/triton\",\n+        \"@xla//xla/python:_profiler\",\n+    ],\n+)\n+\n+pytype_library(\n+    name = \"jaxlib_files\",\n+    srcs = [\n+        \"cpu_sparse.py\",\n+        \"gpu_common_utils.py\",\n+        \"gpu_linalg.py\",\n+        \"gpu_prng.py\",\n+        \"gpu_rnn.py\",\n+        \"gpu_solver.py\",\n+        \"gpu_sparse.py\",\n+        \"gpu_triton.py\",\n+        \"hlo_helpers.py\",\n+        \"init.py\",\n+        \"lapack.py\",\n+        \"plugin_support.py\",\n+        \"xla_client.py\",\n+        \":version\",\n+    ],\n+    deps = [\n+        \":_jax\",\n+        \"//jaxlib/cpu:_lapack\",\n+        \"//jaxlib/cpu:_sparse\",\n+        \"//jaxlib/mlir:ir\",\n+        \"//jaxlib/mlir:stablehlo_dialect\",\n     ],\n )\n \n\n```",
        "from_id": [
            "hawkinsp",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[doc] mention jaxlib in the API compatibility doc",
        "output": "```diff\nCommit: 6971e125a904ec6af6336c823154f0eda2d7f86a\nDate: 2025-05-13T21:18:03Z\nURL: https://github.com/jax-ml/jax/commit/6971e125a904ec6af6336c823154f0eda2d7f86a\nFiles changed: 1\nAdditions: +5, Deletions: -0\ndiff --git a/docs/api_compatibility.md b/docs/api_compatibility.md\nindex 9dca1fc08f50..dda86e2e5d31 100644\n--- a/docs/api_compatibility.md\n+++ b/docs/api_compatibility.md\n@@ -59,6 +59,11 @@ Any API or import path prefixed with an underscore is explicitly private,\n and may change without warning between JAX releases. We are working to move\n all private APIs into `jax._src` to make these expectations more clear.\n \n+### jaxlib\n+Any import path in the `jaxlib` package is considered private, and may change\n+without warning between releases. Some APIs defined in `jaxlib` have public\n+aliases in the `jax` package.\n+\n ### Legacy internal APIs\n In addition, there are several legacy modules that currently expose some\n private APIs without an underscore, including:\n\n```",
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "jax.numpy: make type stubs consistent with runtime",
        "output": "```diff\nCommit: b5a595571bb21ae5e8a93f9f0150cf289be4f423\nDate: 2025-05-13T21:07:33Z\nURL: https://github.com/jax-ml/jax/commit/b5a595571bb21ae5e8a93f9f0150cf289be4f423\nFiles changed: 1\nAdditions: +31, Deletions: -32\ndiff --git a/jax/numpy/__init__.pyi b/jax/numpy/__init__.pyi\nindex c52ce2628cda..4db407861f34 100644\n--- a/jax/numpy/__init__.pyi\n+++ b/jax/numpy/__init__.pyi\n@@ -253,7 +253,8 @@ def broadcast_shapes(*shapes: Sequence[int]) -> tuple[int, ...]: ...\n def broadcast_shapes(*shapes: Sequence[int | _core.Tracer]\n                      ) -> tuple[int | _core.Tracer, ...]: ...\n \n-def broadcast_to(array: ArrayLike, shape: DimSize | Shape) -> Array: ...\n+def broadcast_to(array: ArrayLike, shape: DimSize | Shape, *,\n+                 out_sharding: NamedSharding | P | None = None) -> Array: ...\n c_: _CClass\n can_cast = _np.can_cast\n def cbrt(x: ArrayLike, /) -> Array: ...\n@@ -267,6 +268,7 @@ def clip(\n     /,\n     min: ArrayLike | None = ...,\n     max: ArrayLike | None = ...,\n+    *,\n     a: ArrayLike | DeprecatedArg | None = ...,\n     a_min: ArrayLike | DeprecatedArg | None = ...,\n     a_max: ArrayLike | DeprecatedArg | None = ...\n@@ -278,7 +280,7 @@ complex128: Any\n complex64: Any\n complex_: Any\n complexfloating = _np.complexfloating\n-def compress(condition: ArrayLike, a: ArrayLike, axis: int | None = ...,\n+def compress(condition: ArrayLike, a: ArrayLike, axis: int | None = ..., *,\n              size: int | None = ..., fill_value: ArrayLike = ..., out: None = ...) -> Array: ...\n def concat(arrays: Sequence[ArrayLike], /, *, axis: int | None = 0) -> Array: ...\n def concatenate(\n@@ -314,9 +316,9 @@ def cross(\n     axis: int | None = ...,\n ) -> Array: ...\n csingle: Any\n-def cumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def cumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n             out: None = ...) -> Array: ...\n-def cumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def cumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n            out: None = ...) -> Array: ...\n def cumulative_prod(x: ArrayLike, /, *, axis: int | None = ...,\n                     dtype: DTypeLike | None = ...,\n@@ -371,7 +373,6 @@ def einsum(\n     optimize: str | builtins.bool | list[tuple[int, ...]] = ...,\n     precision: PrecisionLike = ...,\n     preferred_element_type: DTypeLike | None = ...,\n-    _use_xeinsum: builtins.bool = False,\n     _dot_general: Callable[..., Array] = ...,\n     out_sharding: NamedSharding | P | None = ...,\n ) -> Array: ...\n@@ -385,7 +386,6 @@ def einsum(\n     optimize: str | builtins.bool | list[tuple[int, ...]] = ...,\n     precision: PrecisionLike = ...,\n     preferred_element_type: DTypeLike | None = ...,\n-    _use_xeinsum: builtins.bool = False,\n     _dot_general: Callable[..., Array] = ...,\n     out_sharding: NamedSharding | P | None = ...,\n ) -> Array: ...\n@@ -397,7 +397,6 @@ def einsum(\n     optimize: str | builtins.bool | list[tuple[int, ...]] = ...,\n     precision: PrecisionLike = ...,\n     preferred_element_type: DTypeLike | None = ...,\n-    _use_xeinsum: builtins.bool = ...,\n     _dot_general: Callable[..., Array] = ...,\n     out_sharding: NamedSharding | P | None = ...,\n ) -> Array: ...\n@@ -422,7 +421,7 @@ def einsum_path(\n     optimize: str | builtins.bool | list[tuple[int, ...]] =  ...,\n ) -> tuple[list[tuple[int, ...]], Any]: ...\n \n-def empty(shape: Any, dtype: DTypeLike | None = ...,\n+def empty(shape: Any, dtype: DTypeLike | None = ..., *,\n           device: _Device | _Sharding | None = ...) -> Array: ...\n def empty_like(prototype: ArrayLike | DuckTypedArray,\n                dtype: DTypeLike | None = ...,\n@@ -579,17 +578,17 @@ def intersect1d(ar1: ArrayLike, ar2: ArrayLike, assume_unique: builtins.bool = .\n def invert(x: ArrayLike, /) -> Array: ...\n def isclose(a: ArrayLike, b: ArrayLike, rtol: ArrayLike = ...,\n             atol: ArrayLike = ..., equal_nan: builtins.bool = ...) -> Array: ...\n-def iscomplex(m: ArrayLike) -> Array: ...\n+def iscomplex(x: ArrayLike) -> Array: ...\n def iscomplexobj(x: Any) -> builtins.bool: ...\n def isdtype(dtype: DTypeLike, kind: DType | str | tuple[DType | str, ...]) -> builtins.bool: ...\n def isfinite(x: ArrayLike, /) -> Array: ...\n-def isin(element: ArrayLike, test_elements: ArrayLike,\n-         assume_unique: builtins.bool = ..., invert: builtins.bool = ..., method: str = ...) -> Array: ...\n+def isin(element: ArrayLike, test_elements: ArrayLike, assume_unique: builtins.bool = ...,\n+         invert: builtins.bool = ..., *, method: str = ...) -> Array: ...\n def isinf(x: ArrayLike, /) -> Array: ...\n def isnan(x: ArrayLike, /) -> Array: ...\n def isneginf(x: ArrayLike, /) -> Array: ...\n def isposinf(x: ArrayLike, /) -> Array: ...\n-def isreal(m: ArrayLike) -> Array: ...\n+def isreal(x: ArrayLike) -> Array: ...\n def isrealobj(x: Any) -> builtins.bool: ...\n def isscalar(element: Any) -> builtins.bool: ...\n def issubdtype(arg1: DTypeLike, arg2: DTypeLike) -> builtins.bool: ...\n@@ -644,7 +643,7 @@ def logspace(start: ArrayLike, stop: ArrayLike, num: int = ...,\n              endpoint: builtins.bool = ..., base: ArrayLike = ...,\n              dtype: DTypeLike | None = ..., axis: int = ...) -> Array: ...\n def mask_indices(\n-    n: int, mask_func: Callable, k: int = ...\n+    n: int, mask_func: Callable, k: int = ..., *, size: int | None = ...\n ) -> tuple[Array, ...]: ...\n def matmul(\n     a: ArrayLike, b: ArrayLike, *, precision: PrecisionLike = ...,\n@@ -655,7 +654,7 @@ def max(a: ArrayLike, axis: _Axis = ..., out: None = ...,\n         keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n         where: ArrayLike | None = ...) -> Array: ...\n def maximum(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n-def mean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def mean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n          out: None = ..., keepdims: builtins.bool = ..., *,\n          where: ArrayLike | None = ...) -> Array: ...\n def median(a: ArrayLike, axis: int | tuple[int, ...] | None = ...,\n@@ -689,14 +688,14 @@ def nanargmin(\n     out: None = ...,\n     keepdims: builtins.bool | None = ...,\n ) -> Array: ...\n-def nancumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def nancumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n                out: None = ...) -> Array: ...\n-def nancumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def nancumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n                out: None = ...) -> Array: ...\n def nanmax(a: ArrayLike, axis: _Axis = ..., out: None = ...,\n            keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n            where: ArrayLike | None = ...) -> Array: ...\n-def nanmean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nanmean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n             out: None = ...,\n             keepdims: builtins.bool = ...,\n             where: ArrayLike | None = ...) -> Array: ...\n@@ -710,21 +709,21 @@ def nanpercentile(a: ArrayLike, q: ArrayLike,\n                   axis: int | tuple[int, ...] | None = ...,\n                   out: None = ..., overwrite_input: builtins.bool = ..., method: str = ...,\n                   keepdims: builtins.bool = ..., *, interpolation: DeprecatedArg | str = ...) -> Array: ...\n-def nanprod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nanprod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n             out: None = ...,\n             keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n             where: ArrayLike | None = ...) -> Array: ...\n def nanquantile(a: ArrayLike, q: ArrayLike, axis: int | tuple[int, ...] | None = ...,\n                 out: None = ..., overwrite_input: builtins.bool = ..., method: str = ...,\n                 keepdims: builtins.bool = ..., *, interpolation: DeprecatedArg | str = ...) -> Array: ...\n-def nanstd(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ..., out: None = ...,\n-           ddof: int = ..., keepdims: builtins.bool = ...,\n+def nanstd(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n+           out: None = ..., ddof: int = ..., keepdims: builtins.bool = ...,\n            where: ArrayLike | None = ...) -> Array: ...\n-def nansum(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nansum(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n            out: None = ..., keepdims: builtins.bool = ...,\n            initial: ArrayLike | None = ...,\n            where: ArrayLike | None = ...) -> Array: ...\n-def nanvar(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nanvar(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n            out: None = ...,\n            ddof: int = 0, keepdims: builtins.bool = False,\n            where: ArrayLike | None = ...) -> Array: ...\n@@ -740,7 +739,7 @@ def not_equal(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n number = _np.number\n object_ = _np.object_\n ogrid: _Ogrid\n-def ones(shape: Any, dtype: DTypeLike | None = ...,\n+def ones(shape: Any, dtype: DTypeLike | None = ..., *,\n          device: _Device | _Sharding | None = ...) -> Array: ...\n def ones_like(a: ArrayLike | DuckTypedArray,\n               dtype: DTypeLike | None = ...,\n@@ -782,7 +781,7 @@ def positive(x: ArrayLike, /) -> Array: ...\n def pow(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n def power(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n printoptions = _np.printoptions\n-def prod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def prod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n          out: None = ..., keepdims: builtins.bool = ...,\n          initial: ArrayLike | None = ..., where: ArrayLike | None = ...,\n          promote_integers: builtins.bool = ...) -> Array: ...\n@@ -805,7 +804,6 @@ def ravel_multi_index(multi_index: Sequence[ArrayLike], dims: Sequence[int],\n                       mode: str = ..., order: str = ...) -> Array: ...\n def real(x: ArrayLike, /) -> Array: ...\n def reciprocal(x: ArrayLike, /) -> Array: ...\n-register_jax_array_methods: Any\n def remainder(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n def repeat(a: ArrayLike, repeats: ArrayLike, axis: int | None = ..., *,\n            total_repeat_length: int | None = ...,\n@@ -844,7 +842,8 @@ def setdiff1d(\n     size: int | None = ...,\n     fill_value: ArrayLike | None = ...,\n ) -> Array: ...\n-def setxor1d(ar1: ArrayLike, ar2: ArrayLike, assume_unique: builtins.bool = ...) -> Array: ...\n+def setxor1d(ar1: ArrayLike, ar2: ArrayLike, assume_unique: builtins.bool = ..., *,\n+             size: int | None = ..., fill_value: ArrayLike | None = ...) -> Array: ...\n def shape(a: ArrayLike | SupportsShape) -> tuple[int, ...]: ...\n def sign(x: ArrayLike, /) -> Array: ...\n def signbit(x: ArrayLike, /) -> Array: ...\n@@ -882,14 +881,14 @@ def stack(\n     out: None = ...,\n     dtype: DTypeLike | None = ...,\n ) -> Array: ...\n-def std(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def std(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n         out: None = ..., ddof: int = ..., keepdims: builtins.bool = ..., *,\n         where: ArrayLike | None = ..., correction: int | float | None = ...) -> Array: ...\n subtract: BinaryUfunc\n def sum(\n     a: ArrayLike,\n     axis: _Axis = ...,\n-    dtype: DTypeLike = ...,\n+    dtype: DTypeLike | None = ...,\n     out: None = ...,\n     keepdims: builtins.bool = ...,\n     initial: ArrayLike | None = ...,\n@@ -927,7 +926,7 @@ def transpose(a: ArrayLike, axes: Sequence[int] | None = ...) -> Array: ...\n def trapezoid(y: ArrayLike, x: ArrayLike | None = None, dx: ArrayLike = ...,\n               axis: int = ...) -> Array: ...\n def tri(\n-    N: int, M: int | None = ..., k: int = ..., dtype: DTypeLike = ...\n+    N: int, M: int | None = ..., k: int = ..., dtype: DTypeLike | None = ...\n ) -> Array: ...\n def tril(m: ArrayLike, k: int = ...) -> Array: ...\n def tril_indices(\n@@ -970,7 +969,7 @@ class _UniqueInverseResult(NamedTuple):\n def unique(ar: ArrayLike, return_index: builtins.bool = ..., return_inverse: builtins.bool = ...,\n            return_counts: builtins.bool = ..., axis: int | None = ...,\n            *, equal_nan: builtins.bool = ..., size: int | None = ...,\n-           fill_value: ArrayLike | None = ...\n+           fill_value: ArrayLike | None = ..., sorted: bool = ...,\n ): ...\n def unique_all(x: ArrayLike, /, *, size: int | None = ...,\n                fill_value: ArrayLike | None = ...) -> _UniqueAllResult: ...\n@@ -994,7 +993,7 @@ def unwrap(p: ArrayLike, discont: ArrayLike | None = ...,\n def vander(\n     x: ArrayLike, N: int | None = ..., increasing: builtins.bool = ...\n ) -> Array: ...\n-def var(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def var(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n         out: None = ..., ddof: int = ..., keepdims: builtins.bool = ..., *,\n         where: ArrayLike | None = ..., correction: int | float | None = ...) -> Array: ...\n def vdot(\n@@ -1029,7 +1028,7 @@ def where(condition: ArrayLike, x: ArrayLike | None = ...,\n           fill_value: None | ArrayLike | tuple[ArrayLike, ...] = ...\n           ) -> Array | tuple[Array, ...]: ...\n \n-def zeros(shape: Any, dtype: DTypeLike | None = ...,\n+def zeros(shape: Any, dtype: DTypeLike | None = ..., *,\n           device: _Device | _Sharding | None = ...) -> Array: ...\n def zeros_like(a: ArrayLike | DuckTypedArray,\n                dtype: DTypeLike | None = ...,\n\n```",
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Merge pull request #28355 from jakevdp:jax-array-abstract\n\nPiperOrigin-RevId: 758360297",
        "output": "```diff\nCommit: e55dadd29a818ffedf079598bb787a5688f66a06\nDate: 2025-05-13T20:48:27Z\nURL: https://github.com/jax-ml/jax/commit/e55dadd29a818ffedf079598bb787a5688f66a06\nFiles changed: 5\nAdditions: +41, Deletions: -2\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex da9efdb0eacc..e004263abe71 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -34,6 +34,7 @@\n \n import numpy as np\n \n+from jax._src import deprecations\n from jax._src import dtypes\n from jax._src import config\n from jax._src import effects\n@@ -1554,6 +1555,12 @@ def shaped_abstractify(x):\n   if isinstance(x, AbstractValue):\n     return x\n   if hasattr(x, '__jax_array__'):\n+    deprecations.warn(\n+      'jax-abstract-dunder-array',\n+      ('Triggering of __jax_array__() during abstractification is deprecated.'\n+       ' To avoid this error, either explicitly convert your object using'\n+       ' jax.numpy.array(), or register your object as a pytree.'),\n+      stacklevel=6)\n     return shaped_abstractify(x.__jax_array__())\n   if hasattr(x, 'dtype'):\n     aval = ShapedArray(np.shape(x), x.dtype,\n@@ -1578,6 +1585,12 @@ def get_aval(x):\n     if (aval_fn := pytype_aval_mappings.get(t)):\n       return aval_fn(x)\n   if hasattr(x, '__jax_array__'):\n+    deprecations.warn(\n+      'jax-abstract-dunder-array',\n+      ('Triggering of __jax_array__() during abstractification is deprecated.'\n+       ' To avoid this error, either explicitly convert your object using'\n+       ' jax.numpy.array(), or register your object as a pytree.'),\n+      stacklevel=6)\n     return get_aval(x.__jax_array__())\n   raise TypeError(f\"Argument '{x}' of type '{typ}' is not a valid JAX type\")\n \ndiff --git a/jax/_src/deprecations.py b/jax/_src/deprecations.py\nindex 329491b1e8a8..4e5e22745658 100644\n--- a/jax/_src/deprecations.py\n+++ b/jax/_src/deprecations.py\n@@ -135,3 +135,4 @@ def warn(deprecation_id: str, message: str, stacklevel: int) -> None:\n register('jax-numpy-trimzeros-not-1d-array')\n register('jax-scipy-special-sph-harm')\n register('jax-jit-positional-args')\n+register('jax-abstract-dunder-array')\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex 2a1ec7227439..266aad4954ba 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -1237,7 +1237,7 @@ def permute_dims(a: ArrayLike, /, axes: tuple[int, ...]) -> Array:\n            [2, 5],\n            [3, 6]], dtype=int32)\n   \"\"\"\n-  util.check_arraylike(\"permute_dims\", a)\n+  a = util.ensure_arraylike(\"permute_dims\", a)\n   return lax.transpose(a, axes)\n \n \ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 5f775b46fb16..1fd192a52525 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -4036,6 +4036,9 @@ def test_default_device(self):\n   def test_dunder_jax_array(self):\n     # https://github.com/jax-ml/jax/pull/4725\n \n+    @partial(jax.tree_util.register_dataclass,\n+             data_fields=['jax_val'],\n+             meta_fields=[])\n     class AlexArray:\n       def __init__(self, jax_val):\n         self.jax_val = jax_val\n@@ -4045,10 +4048,16 @@ def __jax_array__(self):\n       shape = property(lambda self: self.jax_val.shape)\n \n     x = AlexArray(jnp.array([1., 2., 3.]))\n+\n+    y = jax.jit(lambda x: x)(x)\n+    self.assertIsInstance(x, AlexArray)\n+    self.assertArraysEqual(jnp.asarray(x), jnp.asarray(y))\n+\n     y = jnp.sin(x)\n     self.assertAllClose(y, jnp.sin(jnp.array([1., 2., 3.])))\n     y = api.grad(api.jit(lambda x: jnp.sin(x).sum()))(x)\n-    self.assertAllClose(y, jnp.cos(jnp.array([1., 2., 3.])))\n+    self.assertIsInstance(y, AlexArray)\n+    self.assertAllClose(jnp.asarray(y), jnp.cos(jnp.array([1., 2., 3.])))\n \n     x = AlexArray(jnp.array([[1., 2., 3.]]))\n     y = api.pmap(jnp.sin)(x)\n@@ -4066,6 +4075,19 @@ def __jax_array__(self):\n     a2 = jnp.array(((x, x), [x, x]))\n     self.assertAllClose(np.array(((1, 1), (1, 1))), a2)\n \n+  def test_dunder_jax_array_warnings(self):\n+    class AlexArray:\n+      def __init__(self, jax_val):\n+        self.jax_val = jax_val\n+      def __jax_array__(self):\n+        return self.jax_val\n+\n+    f = jax.jit(lambda x: x)\n+    a = AlexArray(jnp.arange(4))\n+    msg = r\"Triggering of __jax_array__\\(\\) during abstractification is deprecated.\"\n+    with self.assertDeprecationWarnsOrRaises('jax-abstract-dunder-array', msg):\n+      f(a)\n+\n   @jtu.thread_unsafe_test()  # count_jit_tracing_cache_miss() isn't thread-safe\n   def test_eval_shape_weak_type(self):\n     # https://github.com/jax-ml/jax/issues/23302\ndiff --git a/tests/array_extensibility_test.py b/tests/array_extensibility_test.py\nindex 91e2a1d9cf6d..36726659c2f9 100644\n--- a/tests/array_extensibility_test.py\n+++ b/tests/array_extensibility_test.py\n@@ -29,6 +29,9 @@\n config.parse_flags_with_absl()\n \n \n+@functools.partial(jax.tree_util.register_dataclass,\n+                   data_fields=['x'],\n+                   meta_fields=[])\n class JaxArrayWrapper:\n   \"\"\"Class that provides a __jax_array__ method.\"\"\"\n   x: ArrayLike\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add direct `pypi` dependencies to the JAX test targets.\n\nPiperOrigin-RevId: 758330650",
        "output": "```diff\nCommit: 725d0f64addd67b242691012854a582d2f14ce6c\nDate: 2025-05-13T19:34:09Z\nURL: https://github.com/jax-ml/jax/commit/725d0f64addd67b242691012854a582d2f14ce6c\nFiles changed: 4\nAdditions: +674, Deletions: -124\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex a8fe2b50344b..e739b681a029 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -256,18 +256,13 @@ def _get_jax_test_deps(deps):\n     \"\"\"\n     jax_build_deps = [d for d in deps if not d.startswith(\"@pypi//\")]\n \n-    # A lot of tests don't have explicit dependencies on absl/testing, numpy, etc. But the tests\n+    # A lot of tests don't have explicit dependencies on scipy, ml_dtypes, etc. But the tests\n     # transitively depends on them via //jax. So we need to make sure that these dependencies are\n     # included in the test when JAX is built from source.\n-    # TODO(ybaturina): Add individual dependencies for each test and remove this block.\n     jax_transitive_pypi_test_deps = {k: \"true\" for k in py_deps([\n-        \"absl/testing\",\n-        \"numpy\",\n         \"ml_dtypes\",\n         \"scipy\",\n         \"opt_einsum\",\n-        \"hypothesis\",\n-        \"cloudpickle\",\n         \"flatbuffers\",\n     ])}\n \ndiff --git a/tests/BUILD b/tests/BUILD\nindex 4c6369bee5de..e70d4593e8fc 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -38,7 +38,10 @@ jax_multiplatform_test(\n     shard_count = 10,\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -61,12 +64,16 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu_ops\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"device_test\",\n     srcs = [\"device_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -75,12 +82,16 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"absl/testing\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"api_util_test\",\n     srcs = [\"api_util_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -98,7 +109,10 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"absl/testing\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -112,7 +126,11 @@ jax_multiplatform_test(\n         \"gpu_h100x2\",\n     ],\n     tags = [\"multiaccelerator\"],\n-    deps = py_deps(\"tensorflow_core\"),\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"tensorflow_core\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -121,6 +139,10 @@ jax_multiplatform_test(\n     shard_count = {\n         \"gpu\": 5,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -132,7 +154,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental_buffer_callback\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -151,11 +176,19 @@ jax_multiplatform_test(\n         \"cpu\": 5,\n         \"gpu\": 10,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"debug_nans_test\",\n     srcs = [\"debug_nans_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -164,14 +197,20 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"portpicker\"),\n+    ] + py_deps([\n+        \"portpicker\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"distributed_test\",\n     srcs = [\"distributed_test.py\"],\n     enable_backends = [\"gpu\"],\n-    deps = py_deps(\"portpicker\"),\n+    deps = py_deps([\n+        \"portpicker\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_py_test(\n@@ -184,12 +223,19 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"portpicker\"),\n+    ] + py_deps([\n+        \"portpicker\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"dtypes_test\",\n     srcs = [\"dtypes_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -199,12 +245,13 @@ jax_multiplatform_test(\n     enable_configs = [\n         \"cpu\",\n     ],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n     name = \"extend_test\",\n     srcs = [\"extend_test.py\"],\n-    deps = [\"//jax:extend\"],\n+    deps = [\"//jax:extend\"] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -214,7 +261,10 @@ jax_multiplatform_test(\n         \"gpu_h100x2\",\n     ],\n     # TODO(dfm): Remove after removal of jex.ffi imports.\n-    deps = [\"//jax:extend\"],\n+    deps = [\"//jax:extend\"] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -231,11 +281,19 @@ jax_multiplatform_test(\n         \"cpu\": 20,\n         \"gpu\": 10,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"generated_fun_test\",\n     srcs = [\"generated_fun_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -246,6 +304,7 @@ jax_multiplatform_test(\n         \"XLA_PYTHON_CLIENT_PREALLOCATE\": \"0\",\n     },\n     main = \"gpu_memory_flags_test.py\",\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -255,6 +314,7 @@ jax_multiplatform_test(\n     env = {\n         \"XLA_PYTHON_CLIENT_PREALLOCATE\": \"1\",\n     },\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -269,7 +329,12 @@ jax_multiplatform_test(\n     },\n     deps = [\n         \"//jax:experimental_sparse\",\n-    ] + py_deps(\"matplotlib\"),\n+    ] + py_deps([\n+        \"matplotlib\",\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -280,6 +345,11 @@ jax_multiplatform_test(\n         \"gpu\": 10,\n         \"tpu\": 15,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -288,7 +358,7 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -306,7 +376,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -329,7 +402,10 @@ jax_multiplatform_test(\n     tags = [\"multiaccelerator\"],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -345,7 +421,10 @@ jax_multiplatform_test(\n     tags = [\"multiaccelerator\"],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -359,7 +438,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -375,7 +457,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -390,7 +475,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -406,7 +494,7 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -422,7 +510,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:experimental\",\n         \"//jax:internal_test_util\",\n-    ],\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -431,7 +522,10 @@ jax_multiplatform_test(\n     tags = [\"multiaccelerator\"],\n     deps = [\n         \"//jax:experimental\",\n-    ] + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -443,18 +537,31 @@ jax_multiplatform_test(\n         \"tpu\": 8,\n     },\n     tags = [\"noasan\"],  # Linking TF causes a linker OOM.\n-    deps = py_deps(\"pil\") + py_deps(\"tensorflow_core\"),\n+    deps = py_deps([\n+        \"pil\",\n+        \"tensorflow_core\",\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"infeed_test\",\n     srcs = [\"infeed_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"jax_jit_test\",\n     srcs = [\"jax_jit_test.py\"],\n     main = \"jax_jit_test.py\",\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -464,7 +571,10 @@ jax_py_test(\n         \"//jax:test_util\",\n         \"//jax/experimental/jax2tf\",\n         \"//jax/tools:jax_to_ir\",\n-    ] + py_deps(\"tensorflow_core\"),\n+    ] + py_deps([\n+        \"tensorflow_core\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_py_test(\n@@ -474,7 +584,7 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:jaxpr_util\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -487,7 +597,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:jet\",\n         \"//jax:stax\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -498,16 +611,28 @@ jax_multiplatform_test(\n         \"gpu\": 30,\n         \"tpu\": 20,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"custom_root_test\",\n     srcs = [\"custom_root_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"custom_linear_solve_test\",\n     srcs = [\"custom_linear_solve_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -526,6 +651,10 @@ jax_multiplatform_test(\n         \"noasan\",  # Test times out on all backends\n         \"test_cpu_thunks\",\n     ],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -536,6 +665,10 @@ jax_multiplatform_test(\n         \"gpu\": 30,\n         \"tpu\": 40,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -546,6 +679,10 @@ jax_multiplatform_test(\n         \"gpu\": 20,\n         \"tpu\": 20,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -556,11 +693,19 @@ jax_multiplatform_test(\n         \"gpu\": 10,\n         \"tpu\": 10,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"lax_numpy_einsum_test\",\n     srcs = [\"lax_numpy_einsum_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -571,11 +716,19 @@ jax_multiplatform_test(\n         \"gpu\": 5,\n         \"tpu\": 5,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"lax_numpy_vectorize_test\",\n     srcs = [\"lax_numpy_vectorize_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -586,7 +739,11 @@ jax_multiplatform_test(\n         \"gpu\": 20,\n         \"tpu\": 8,\n     },\n-    deps = py_deps(\"numpy\") + py_deps(\"scipy\") + py_deps(\"absl/testing\"),\n+    deps = py_deps([\n+        \"numpy\",\n+        \"scipy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -600,6 +757,11 @@ jax_multiplatform_test(\n         \"gpu\": 5,\n         \"tpu\": 5,\n     },\n+    deps = py_deps([\n+        \"numpy\",\n+        \"scipy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -617,7 +779,11 @@ jax_multiplatform_test(\n         \"tpu\": 20,\n     },\n     tags = [\"noasan\"],  # Times out under asan.\n-    deps = py_deps(\"numpy\") + py_deps(\"scipy\") + py_deps(\"absl/testing\"),\n+    deps = py_deps([\n+        \"numpy\",\n+        \"scipy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -630,7 +796,10 @@ jax_multiplatform_test(\n     },\n     deps = [\n         \"//jax:internal_test_util\",\n-    ] + py_deps(\"numpy\") + py_deps(\"scipy\") + py_deps(\"absl/testing\"),\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -648,7 +817,11 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:internal_test_util\",\n         \"//jax:lax_reference\",\n-    ] + py_deps(\"numpy\") + py_deps(\"mpmath\"),\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+        \"mpmath\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -659,7 +832,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:internal_test_util\",\n         \"//jax:lax_reference\",\n-    ] + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -670,6 +846,10 @@ jax_multiplatform_test(\n         \"gpu\": 30,\n         \"tpu\": 20,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -680,7 +860,10 @@ jax_multiplatform_test(\n         \"gpu\": 40,\n         \"tpu\": 40,\n     },\n-    deps = [\"//jax:internal_test_util\"] + py_deps(\"numpy\") + py_deps(\"absl/testing\"),\n+    deps = [\"//jax:internal_test_util\"] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -691,7 +874,10 @@ jax_multiplatform_test(\n         \"gpu\": 40,\n         \"tpu\": 40,\n     },\n-    deps = [\"//jax:internal_test_util\"] + py_deps(\"numpy\") + py_deps(\"absl/testing\"),\n+    deps = [\"//jax:internal_test_util\"] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_py_test(\n@@ -702,7 +888,7 @@ jax_py_test(\n     deps = [\n         \"//jax:internal_test_util\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -713,7 +899,7 @@ jax_py_test(\n     deps = [\n         \"//jax:internal_test_util\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -733,6 +919,11 @@ jax_multiplatform_test(\n         \"gpu\": 40,\n         \"tpu\": 40,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -753,24 +944,36 @@ jax_multiplatform_test(\n     tags = [\n         \"multiaccelerator\",\n     ],\n+    deps = py_deps([\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"magma_linalg_test\",\n     srcs = [\"magma_linalg_test.py\"],\n     enable_backends = [\"gpu\"],\n-    deps = py_deps(\"magma\"),\n+    deps = py_deps([\n+        \"magma\",\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"cholesky_update_test\",\n     srcs = [\"cholesky_update_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"metadata_test\",\n     srcs = [\"metadata_test.py\"],\n     enable_backends = [\"cpu\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -779,7 +982,7 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -789,12 +992,17 @@ jax_multiplatform_test(\n         \"tpu_v3_x4\",\n         \"gpu_h100x2\",\n     ],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"multi_device_test\",\n     srcs = [\"multi_device_test.py\"],\n     enable_backends = [\"cpu\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -813,12 +1021,20 @@ jax_multiplatform_test(\n         \"tpu\": 10,\n         \"gpu\": 10,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"optimizers_test\",\n     srcs = [\"optimizers_test.py\"],\n-    deps = [\"//jax:optimizers\"],\n+    deps = [\"//jax:optimizers\"] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -826,7 +1042,11 @@ jax_multiplatform_test(\n     srcs = [\"pickle_test.py\"],\n     deps = [\n         \"//jax:experimental\",\n-    ] + py_deps(\"cloudpickle\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"cloudpickle\",\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -850,7 +1070,10 @@ jax_multiplatform_test(\n     tags = [\"multiaccelerator\"],\n     deps = [\n         \"//jax:internal_test_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -868,12 +1091,21 @@ jax_multiplatform_test(\n     # in this case there's not a good place to do it, see b/197635968#comment19\n     # for details.\n     tags = [\"nomsan\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"heap_profiler_test\",\n     srcs = [\"heap_profiler_test.py\"],\n     enable_backends = [\"cpu\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -892,7 +1124,7 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:profiler\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -907,7 +1139,10 @@ jax_multiplatform_test(\n         \"nomsan\",  # TODO(b/355237462): msan false-positives in torch?\n         \"not_build:arm\",\n     ],\n-    deps = py_deps(\"torch\"),\n+    deps = py_deps([\n+        \"torch\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -921,11 +1156,19 @@ jax_multiplatform_test(\n         ],\n     },\n     shard_count = 8,\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"random_test\",\n     srcs = [\"random_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -951,6 +1194,11 @@ jax_multiplatform_test(\n         \"tpu\": 40,\n     },\n     tags = [\"noasan\"],  # Times out\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n # TODO(b/199564969): remove once we always enable_custom_prng\n@@ -959,6 +1207,7 @@ jax_multiplatform_test(\n     srcs = [\"random_test.py\"],\n     args = [\"--jax_enable_custom_prng=true\"],\n     main = \"random_test.py\",\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -972,21 +1221,41 @@ jax_multiplatform_test(\n         ],  # Times out on TPU with asan/tsan/msan.\n     },\n     shard_count = 12,\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"scipy_interpolate_test\",\n     srcs = [\"scipy_interpolate_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"scipy_ndimage_test\",\n     srcs = [\"scipy_ndimage_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"scipy_optimize_test\",\n     srcs = [\"scipy_optimize_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1012,6 +1281,11 @@ jax_multiplatform_test(\n         \"gpu\": 40,\n         \"tpu\": 50,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1021,7 +1295,11 @@ jax_multiplatform_test(\n         \"cpu\": 4,\n         \"gpu\": 4,\n     },\n-    deps = py_deps(\"scipy\"),\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1039,6 +1317,11 @@ jax_multiplatform_test(\n         \"noasan\",\n         \"notsan\",\n     ],  # Times out\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1071,7 +1354,11 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:experimental_sparse\",\n         \"//jax:sparse_test_util\",\n-    ] + py_deps(\"scipy\"),\n+    ] + py_deps([\n+        \"scipy\",\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1108,7 +1395,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:experimental_sparse\",\n         \"//jax:sparse_test_util\",\n-    ] + py_deps(\"scipy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1133,12 +1423,16 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:experimental_sparse\",\n         \"//jax:sparse_test_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"stack_test\",\n     srcs = [\"stack_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1149,33 +1443,50 @@ jax_multiplatform_test(\n         \"gpu\": 2,\n         \"tpu\": 4,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"error_check_test\",\n     srcs = [\"error_check_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n     name = \"jax_numpy_error_test\",\n     srcs = [\"jax_numpy_error_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n     name = \"stax_test\",\n     srcs = [\"stax_test.py\"],\n-    deps = [\"//jax:stax\"],\n+    deps = [\"//jax:stax\"] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"linear_search_test\",\n     srcs = [\"third_party/scipy/line_search_test.py\"],\n     main = \"third_party/scipy/line_search_test.py\",\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"blocked_sampler_test\",\n     srcs = [\"blocked_sampler_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1184,7 +1495,11 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"cloudpickle\",\n+    ]),\n )\n \n pytype_test(\n@@ -1193,7 +1508,11 @@ pytype_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+        \"//jax:typing\",\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1202,7 +1521,7 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1211,7 +1530,7 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1230,7 +1549,9 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:compiler\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"absl/logging\"),\n+    ] + py_deps([\n+        \"absl/logging\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1240,7 +1561,10 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:lru_cache\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"filelock\"),\n+    ] + py_deps([\n+        \"filelock\",\n+        \"absl/logging\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1249,7 +1573,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:compilation_cache_internal\",\n         \"//jax:compiler\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1258,7 +1585,7 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:cache_key\",\n         \"//jax:compiler\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1267,18 +1594,27 @@ jax_multiplatform_test(\n     shard_count = {\n         \"cpu\": 10,\n     },\n-    deps = [\"//jax:ode\"],\n+    deps = [\"//jax:ode\"] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"key_reuse_test\",\n     srcs = [\"key_reuse_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"roofline_test\",\n     srcs = [\"roofline_test.py\"],\n     enable_backends = [\"cpu\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1286,7 +1622,10 @@ jax_multiplatform_test(\n     srcs = [\"x64_context_test.py\"],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1297,6 +1636,10 @@ jax_multiplatform_test(\n         \"gpu\": 5,\n         \"tpu\": 10,\n     },\n+    deps = py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1306,17 +1649,26 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:mesh_utils\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"transfer_guard_test\",\n     srcs = [\"transfer_guard_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"cloudpickle\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"garbage_collection_guard_test\",\n     srcs = [\"garbage_collection_guard_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1342,6 +1694,10 @@ jax_multiplatform_test(\n         \"tpu_v4_x4\",\n     ],\n     tags = [\"multiaccelerator\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1356,6 +1712,10 @@ jax_multiplatform_test(\n         \"gpu_h100_shardy\",\n         \"tpu_v3_x4_shardy\",\n     ],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1374,7 +1734,10 @@ jax_multiplatform_test(\n     tags = [\"multiaccelerator\"],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1390,6 +1753,10 @@ jax_multiplatform_test(\n         \"tpu_v3_x4\",\n         \"tpu_v4_x4\",\n     ],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1411,12 +1778,20 @@ jax_multiplatform_test(\n         \"gpu\": 2,\n         \"tpu\": 2,\n     },\n-    deps = py_deps(\"hypothesis\"),\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"mutable_array_test\",\n     srcs = [\"mutable_array_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1425,6 +1800,10 @@ jax_multiplatform_test(\n     shard_count = {\n         \"tpu\": 20,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1445,7 +1824,7 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1469,12 +1848,16 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:experimental\",\n         \"//jax:tree_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"clear_backends_test\",\n     srcs = [\"clear_backends_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1482,7 +1865,10 @@ jax_multiplatform_test(\n     srcs = [\"attrs_test.py\"],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1491,7 +1877,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:experimental_colocated_python\",\n         \"//jax/extend:ifrt_programs\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1504,7 +1893,10 @@ jax_multiplatform_test(\n     shard_count = 15,\n     deps = [\n         \"//jax:rnn\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1514,7 +1906,7 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:mosaic\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1523,7 +1915,7 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1532,12 +1924,13 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n     name = \"logging_test\",\n     srcs = [\"logging_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1550,6 +1943,10 @@ jax_multiplatform_test(\n         \"tpu_v3_x4\",\n     ],\n     tags = [],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1574,7 +1971,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:internal_test_harnesses\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1596,7 +1996,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:internal_test_harnesses\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1610,7 +2013,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:internal_export_back_compat_test_data\",\n         \"//jax:internal_export_back_compat_test_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1618,12 +2024,16 @@ jax_multiplatform_test(\n     srcs = [\"fused_attention_stablehlo_test.py\"],\n     enable_backends = [\"gpu\"],\n     tags = [\"multiaccelerator\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"xla_metadata_test\",\n     srcs = [\"xla_metadata_test.py\"],\n-    deps = [\"//jax:experimental\"],\n+    deps = [\"//jax:experimental\"] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1637,7 +2047,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1646,7 +2059,7 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1656,7 +2069,7 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:source_mapper\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1665,12 +2078,19 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"string_array_test\",\n     srcs = [\"string_array_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1682,6 +2102,7 @@ jax_multiplatform_test(\n         \"gpu_h100\",\n     ],\n     tags = [\"multiaccelerator\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1691,6 +2112,10 @@ jax_multiplatform_test(\n     shard_count = {\n         \"gpu\": 4,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1700,7 +2125,7 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:experimental\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n exports_files(\ndiff --git a/tests/mosaic/BUILD b/tests/mosaic/BUILD\nindex 24acb1b9a3f2..75e1df335f6f 100644\n--- a/tests/mosaic/BUILD\n+++ b/tests/mosaic/BUILD\n@@ -41,7 +41,11 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:mosaic_gpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\") + py_deps(\"hypothesis\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -53,7 +57,10 @@ jax_multiplatform_test(\n     tags = [\"multiaccelerator\"],\n     deps = [\n         \"//jax:mosaic_gpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -83,7 +90,10 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:mosaic_gpu\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"absl/testing\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -95,7 +105,11 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:mosaic_gpu\",\n         \"//jax/experimental/mosaic/gpu/examples:matmul\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\") + py_deps(\"hypothesis\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -110,7 +124,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:mosaic_gpu\",\n-    ] + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 3769da27a1eb..49a05ee487f0 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -54,7 +54,10 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu_ops\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -68,7 +71,10 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu_ops\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -88,7 +94,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -124,7 +133,11 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"hypothesis\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"hypothesis\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -162,7 +175,11 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_mosaic_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"hypothesis\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"hypothesis\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -182,7 +199,11 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_tpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"hypothesis\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"hypothesis\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -202,7 +223,10 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu_ops\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -221,7 +245,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_mosaic_gpu\",  # build_cleaner: keep\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -240,7 +267,7 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_mosaic_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu_ops\",  # build_cleaner: keep\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -253,7 +280,10 @@ jax_py_test(\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu\",  # build_cleaner: keep\n         \"//jax:test_util\",\n-    ] + jax_gpu_support_deps,\n+    ] + jax_gpu_support_deps + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -273,7 +303,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu\",  # build_cleaner: keep\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -294,7 +327,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu\",  # build_cleaner: keep\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -307,7 +343,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_tpu\",\n         \"//jax/_src/pallas/mosaic:random\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -321,7 +360,11 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\") + py_deps(\"hypothesis\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -360,7 +403,10 @@ jax_multiplatform_test(\n         \"//jax:extend\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -398,7 +444,11 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"hypothesis\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"hypothesis\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -415,7 +465,10 @@ jax_multiplatform_test(\n         \"//jax:extend\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -436,7 +489,11 @@ jax_multiplatform_test(\n         \"//jax:extend\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"hypothesis\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -449,7 +506,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:pallas_tpu\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -464,7 +524,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:extend\",\n         \"//jax:pallas_tpu\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -481,7 +544,10 @@ jax_multiplatform_test(\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n         \"//jax/_src/pallas/mosaic:random\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -494,7 +560,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_tpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -507,7 +576,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_tpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -525,7 +597,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -543,7 +618,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -560,7 +638,11 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\") + py_deps(\"hypothesis\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -575,7 +657,10 @@ jax_multiplatform_test(\n         \"//jax:extend\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n # This test doesn't need a TPU; it only tests numpy-using helpers.\n@@ -588,7 +673,11 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:pallas_tpu_ops\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\") + py_deps(\"hypothesis\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -606,7 +695,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_gpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -629,7 +721,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_gpu\",\n         \"//jax:pallas_gpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -647,7 +742,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_gpu\",\n         \"//jax:pallas_gpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -663,7 +761,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_mosaic_gpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -680,7 +781,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_experimental_gpu_ops\",\n         \"//jax:pallas_mosaic_gpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -701,7 +805,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax/_src/pallas/fuser\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -722,7 +829,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_fuser\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -758,5 +868,8 @@ jax_multiplatform_test(\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n         \"//jax/_src/pallas/fuser\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n\n```",
        "from_id": [
            "hawkinsp",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Use DmaCopyChunk::Make because directly assigning the struct\nfully constrains the implementation.\n\nPiperOrigin-RevId: 758330465",
        "output": "```diff\nCommit: 0e4f213e9c84b1f59a9d1be84c63cd1b5e4dfd2b\nDate: 2025-05-13T19:32:09Z\nURL: https://github.com/jax-ml/jax/commit/0e4f213e9c84b1f59a9d1be84c63cd1b5e4dfd2b\nFiles changed: 1\nAdditions: +3, Deletions: -6\ndiff --git a/jaxlib/py_socket_transfer.cc b/jaxlib/py_socket_transfer.cc\nindex 491e90d778cf..ed2a4f4c204a 100644\n--- a/jaxlib/py_socket_transfer.cc\n+++ b/jaxlib/py_socket_transfer.cc\n@@ -127,12 +127,9 @@ class IfrtArrayEntry : public PullTable::Entry {\n       auto req_id = base_req_id;\n       ++base_req_id;\n       for (size_t i = 0; i * xfer_size_ < arrs_[bid].buf_size; ++i) {\n-        DmaCopyChunk blob;\n-        blob.arr = std::move(arrs_[bid].arr);\n-        blob.buffer = arrs_[bid].buffer;\n-        blob.buffer_id = bid;\n-        blob.offset = i * xfer_size_;\n-        blob.size = std::min(xfer_size_, arrs_[bid].buf_size - blob.offset);\n+        DmaCopyChunk blob = DmaCopyChunk::Make(\n+            std::move(arrs_[bid].arr), arrs_[bid].buffer, bid, i * xfer_size_,\n+            std::min(xfer_size_, arrs_[bid].buf_size - i * xfer_size_));\n         bool is_largest = blob.size + blob.offset == arrs_[bid].buf_size;\n         state_->ScheduleCopy(\n             blob, [req_id, state, copier_state = state_, is_largest](\n\n```",
        "from_id": [
            "pschuh",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[JAX] Make fully replicated sharding to avoid materializing the same host buffers\n\nThis change recognizes fully replicated shardings (based on JAX sharding), and\nmaterialize only one host buffer. This saves the cost of repeatedly\nmaterializing the same host buffer for multiple devices, and makes it\nstreamlined to create a multi-device IFRT array.\n\nClean up `JAX_IFRT_VERSION_NUMBER < 2` since we are well past it.\n\nPiperOrigin-RevId: 758320113",
        "output": "```diff\nCommit: ef1b3e9231a929ed509628c080e48d516a5e173d\nDate: 2025-05-13T19:07:40Z\nURL: https://github.com/jax-ml/jax/commit/ef1b3e9231a929ed509628c080e48d516a5e173d\nFiles changed: 7\nAdditions: +210, Deletions: -28\ndiff --git a/jax/_src/test_util.py b/jax/_src/test_util.py\nindex c584ffefa4f2..bb1ef6595ec3 100644\n--- a/jax/_src/test_util.py\n+++ b/jax/_src/test_util.py\n@@ -54,6 +54,7 @@\n from jax._src import mesh as mesh_lib\n from jax._src.cloud_tpu_init import running_in_cloud_tpu_vm\n from jax._src.interpreters import mlir\n+from jax._src.lib import jaxlib_extension_version\n from jax._src.lib.mlir.dialects import hlo\n from jax._src.numpy.util import promote_dtypes, promote_dtypes_inexact\n from jax._src.public_test_util import (  # noqa: F401\n@@ -354,6 +355,20 @@ def assert_num_jit_and_pmap_compilations(times):\n     raise AssertionError(f\"Expected exactly {times} XLA compilations, \"\n                          f\"but executed {count()}\")\n \n+@contextmanager\n+def count_internal_device_puts():\n+  if jaxlib_extension_version >= 341:\n+    before = jax._src.lib._jax.get_internal_device_put_info()\n+  counts = {}\n+  try:\n+    yield lambda: counts\n+  finally:\n+    if jaxlib_extension_version >= 341:\n+      after = jax._src.lib._jax.get_internal_device_put_info()\n+      for k, v in after.items():\n+        diff = v - before.get(k, 0)\n+        if diff != 0:\n+          counts[k] = diff\n \n def jaxlib_version() -> tuple[int, ...]:\n   return _jaxlib.version\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 6f4f952be9c3..c9c25e172161 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -989,3 +989,5 @@ def approx_top_k_reduction_output_size(\n     aggregate_to_topk: bool | None = ...,\n     input_size_override: int | None = ...,\n ) -> tuple[int, int]: ...\n+\n+def get_internal_device_put_info() -> dict[str, int]: ...\ndiff --git a/jaxlib/py_values.cc b/jaxlib/py_values.cc\nindex 81f6523d3e14..6ea5c272eea3 100644\n--- a/jaxlib/py_values.cc\n+++ b/jaxlib/py_values.cc\n@@ -25,6 +25,7 @@ limitations under the License.\n #include <optional>\n #include <string>\n #include <type_traits>\n+#include <unordered_map>\n #include <utility>\n #include <variant>\n #include <vector>\n@@ -78,6 +79,12 @@ namespace xla {\n \n namespace {\n \n+// Gets the thread-local instance.\n+static DevicePutInfo& GetDevicePutInfo() {\n+  thread_local DevicePutInfo device_put_info;\n+  return device_put_info;\n+}\n+\n // Prepared data for creating a single shard of an array. Holds a single-device\n // IFRT array or a host buffer.\n struct Shard {\n@@ -147,6 +154,27 @@ using DevicePutHandler = std::function<absl::StatusOr<ShardFn>(\n     nb::handle obj, ifrt::Client* client, ifrt::Device* to_device,\n     ifrt::MemoryKind to_memory_kind, const DevicePutOptions& options)>;\n \n+// Shared logic that makes an IFRT array (either single-device or multi-device)\n+// from a fully-replicated `shard` that is created from a host buffer (not from\n+// an existing IFRT array). `shard` will be consumed.\n+//\n+// `user_context` will be used for a new IFRT array created.\n+//\n+// Expected to be called without holding GIL.\n+absl::StatusOr<tsl::RCReference<ifrt::Array>>\n+MakeIfrtArrayFromFullyReplicatedShard(\n+    ifrt::Client* ifrt_client, ifrt::ShardingRef ifrt_sharding, Shard& shard,\n+    tsl::RCReference<ifrt::UserContext> user_context) {\n+  auto host_buffer_shard = std::get<ifrt::Client::HostBuffer>(\n+      std::move(shard.ifrt_array_or_host_buffer));\n+  return ifrt_client->MakeArrayFromHostBuffer(\n+      host_buffer_shard.data, host_buffer_shard.dtype,\n+      std::move(host_buffer_shard.shape),\n+      std::move(host_buffer_shard.byte_strides), std::move(ifrt_sharding),\n+      shard.host_buffer_semantics, std::move(host_buffer_shard.on_done),\n+      std::move(user_context));\n+}\n+\n // Shared logic that makes a single-device IFRT array from a `shard`. `shard`\n // will be consumed.\n //\n@@ -161,18 +189,11 @@ absl::StatusOr<ifrt::ArrayRef> MakeSingleDeviceIfrtArrayFromShard(\n   if (auto* ifrt_array =\n           std::get_if<ifrt::ArrayRef>(&shard.ifrt_array_or_host_buffer)) {\n     return std::move(*ifrt_array);\n-  } else {\n-    auto host_buffer_shard = std::get<ifrt::Client::HostBuffer>(\n-        std::move(shard.ifrt_array_or_host_buffer));\n-    ifrt::ShardingRef ifrt_sharding =\n-        ifrt::SingleDeviceSharding::Create(ifrt_device, ifrt_memory_kind);\n-    return ifrt_client->MakeArrayFromHostBuffer(\n-        host_buffer_shard.data, host_buffer_shard.dtype,\n-        std::move(host_buffer_shard.shape),\n-        std::move(host_buffer_shard.byte_strides), std::move(ifrt_sharding),\n-        shard.host_buffer_semantics, std::move(host_buffer_shard.on_done),\n-        std::move(user_context));\n   }\n+  ifrt::ShardingRef ifrt_sharding =\n+      ifrt::SingleDeviceSharding::Create(ifrt_device, ifrt_memory_kind);\n+  return MakeIfrtArrayFromFullyReplicatedShard(\n+      ifrt_client, std::move(ifrt_sharding), shard, std::move(user_context));\n }\n \n // Makes an IFRT Array from `shards` using a batched array creation API (fast\n@@ -587,10 +608,12 @@ absl::StatusOr<ShardFn> MakeShardFn(nb::handle arg, ifrt::Client* client,\n                                     ifrt::Device* to_device,\n                                     ifrt::MemoryKind to_memory_kind,\n                                     const DevicePutOptions& options) {\n-  using PyObjectDeviceHandlerMap = absl::flat_hash_map<PyObject*, DevicePutHandler>;\n+  using PyObjectDeviceHandlerMap =\n+      absl::flat_hash_map<PyObject*, DevicePutHandler>;\n \n-  auto init_fn = [](){\n-    std::unique_ptr<PyObjectDeviceHandlerMap> p = std::make_unique<PyObjectDeviceHandlerMap>();\n+  auto init_fn = []() {\n+    std::unique_ptr<PyObjectDeviceHandlerMap> p =\n+        std::make_unique<PyObjectDeviceHandlerMap>();\n \n     const NumpyScalarTypes& dtypes = GetNumpyScalarTypes();\n     // Python scalar types.\n@@ -660,7 +683,8 @@ absl::StatusOr<ShardFn> MakeShardFn(nb::handle arg, ifrt::Client* client,\n     (*p)[dtypes.np_intc.ptr()] = HandleNumpyScalar<int32_t>;\n     return p;\n   };\n-  const PyObjectDeviceHandlerMap& handlers = xla::SafeStaticInit<PyObjectDeviceHandlerMap>(init_fn);\n+  const PyObjectDeviceHandlerMap& handlers =\n+      xla::SafeStaticInit<PyObjectDeviceHandlerMap>(init_fn);\n \n   if (arg.type().ptr() == PyArray::type().ptr()) {\n     auto array = nb::borrow<PyArray>(arg);\n@@ -895,6 +919,7 @@ absl::StatusOr<DevicePutResult> DevicePutWithDevice(\n     ifrt::Device* ifrt_device, ifrt::MemoryKind ifrt_memory_kind,\n     const DevicePutOptions& options) {\n   tsl::profiler::TraceMe traceme(\"DevicePut\");\n+  ++GetDevicePutInfo().device_put_with_device;\n \n   if (!ifrt_device->IsAddressable()) {\n     return InvalidArgument(\"Cannot copy array to non-addressable device: %s\",\n@@ -924,6 +949,7 @@ absl::StatusOr<DevicePutResult> DevicePutWithSharding(\n     absl::Span<const int64_t> shape, nanobind::handle sharding,\n     const DevicePutOptions& options) {\n   tsl::profiler::TraceMe traceme(\"DevicePutWithSharding\");\n+  ++GetDevicePutInfo().device_put_with_sharding;\n \n   TF_ASSIGN_OR_RETURN(ifrt::DeviceListRef ifrt_device_list,\n                       GetIfrtDeviceList(sharding));\n@@ -973,12 +999,19 @@ absl::StatusOr<DevicePutResult> DevicePutWithSharding(\n   }\n \n   ifrt::ShardingRef ifrt_sharding;\n+  bool is_fully_replicated;\n   if (is_pmap_sharding) {\n     CHECK(!shard_fns.empty());\n     // IFRT Sharding will be determined once we discover the shard shape.\n+    is_fully_replicated = false;\n   } else {\n     TF_ASSIGN_OR_RETURN(ifrt_sharding,\n                         GetIfrtHloSharding(sharding, ifrt_shape));\n+    // Fully-replicated shardings enable additional optimizations of using a\n+    // single host buffer.\n+    // TODO(hyeontaek): Enable a similar optimization for partially replicated\n+    // cases to reduce the number of host buffers to obtain.\n+    is_fully_replicated = ifrt_sharding->IsFullyReplicated();\n   }\n   tsl::RCReference<ifrt::UserContext> ifrt_user_context =\n       ifrt_client->CreateUserContext();\n@@ -988,12 +1021,6 @@ absl::StatusOr<DevicePutResult> DevicePutWithSharding(\n   // Whether to build an IFRT array from host buffers as a single batch. We do\n   // not batch any shard is already an IFRT array.\n   bool should_batch = true;\n-#if JAX_IFRT_VERSION_NUMBER < 2\n-  // PjRt-IFRT would fail `xla::ifrt::Client::MakeArrayFromHostBuffer()` invoked\n-  // by `xla::ifrt::ClientMakeArraysFromHostBufferShards()` for a fully\n-  // replicated sharding if the sharding has any non-addressable device.\n-  should_batch = false;\n-#endif\n \n   std::vector<Shard> shards;\n   shards.reserve(shard_fns.size());\n@@ -1004,7 +1031,15 @@ absl::StatusOr<DevicePutResult> DevicePutWithSharding(\n       should_batch = false;\n     }\n     shards.push_back(std::move(shard));\n+    if (should_batch && is_fully_replicated) {\n+      // We need only one host buffer for a fully-replicated array.\n+      break;\n+    }\n   }\n+  // While we have finished calling `shard_fns`, we cannot destroy them until we\n+  // make a call to IFRT array creation. Destroying `shard_fns` would release\n+  // host buffers prematurely and can cause the array creation API to see\n+  // garbage data.\n \n   // TODO(emilyaf): Remove the following and just use ifrt_dtype when tokens are\n   // supported.\n@@ -1021,12 +1056,22 @@ absl::StatusOr<DevicePutResult> DevicePutWithSharding(\n \n   ifrt::ArrayRef ifrt_array;\n   if (should_batch) {\n-    TF_ASSIGN_OR_RETURN(ifrt_array,\n-                        MakeIfrtArrayFromShardsInBatch(\n-                            ifrt_client, ifrt_dtype, std::move(ifrt_shape),\n-                            std::move(ifrt_sharding), absl::MakeSpan(shards),\n-                            std::move(ifrt_user_context)));\n+    if (is_fully_replicated && shards.size() == 1) {\n+      ++GetDevicePutInfo().device_put_fully_replicated;\n+      TF_ASSIGN_OR_RETURN(\n+          ifrt_array, MakeIfrtArrayFromFullyReplicatedShard(\n+                          ifrt_client, std::move(ifrt_sharding), shards.front(),\n+                          std::move(ifrt_user_context)));\n+    } else {\n+      ++GetDevicePutInfo().device_put_batched;\n+      TF_ASSIGN_OR_RETURN(ifrt_array,\n+                          MakeIfrtArrayFromShardsInBatch(\n+                              ifrt_client, ifrt_dtype, std::move(ifrt_shape),\n+                              std::move(ifrt_sharding), absl::MakeSpan(shards),\n+                              std::move(ifrt_user_context)));\n+    }\n   } else {\n+    ++GetDevicePutInfo().device_put_assembled;\n     TF_ASSIGN_OR_RETURN(\n         ifrt_array, MakeIfrtArrayFromShardsWithAssembly(\n                         ifrt_client, ifrt_dtype, std::move(ifrt_shape),\n@@ -1038,4 +1083,15 @@ absl::StatusOr<DevicePutResult> DevicePutWithSharding(\n   return DevicePutResult(std::move(ifrt_array), weak_type);\n }\n \n+std::unordered_map<std::string, int64_t> DevicePutInfo::GetInfo() {\n+  const DevicePutInfo& info = GetDevicePutInfo();\n+  return std::unordered_map<std::string, int64_t>({\n+      {\"device_put_with_device\", info.device_put_with_device},\n+      {\"device_put_with_sharding\", info.device_put_with_sharding},\n+      {\"device_put_fully_replicated\", info.device_put_fully_replicated},\n+      {\"device_put_batched\", info.device_put_batched},\n+      {\"device_put_assembled\", info.device_put_assembled},\n+  });\n+}\n+\n }  // namespace xla\ndiff --git a/jaxlib/py_values.h b/jaxlib/py_values.h\nindex 64a83aa66ab9..d74cf9668a99 100644\n--- a/jaxlib/py_values.h\n+++ b/jaxlib/py_values.h\n@@ -21,6 +21,7 @@ limitations under the License.\n #include <cstdint>\n #include <string>\n #include <tuple>\n+#include <unordered_map>\n #include <utility>\n \n #include \"absl/container/inlined_vector.h\"\n@@ -32,7 +33,6 @@ limitations under the License.\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/nb_numpy.h\"\n-#include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n@@ -136,6 +136,26 @@ H AbslHashValue(H h, const xla::PyArgSignature& s) {\n   return h;\n }\n \n+// Tracks the number of DevicePut calls and subcases. For testing.\n+struct DevicePutInfo {\n+  // DevicePutWithDevice call count.\n+  int device_put_with_device = 0;\n+\n+  // DevicePutWithSharding call count.\n+  int device_put_with_sharding = 0;\n+\n+  // DevicePutWithSharding with a fully replicated sharding.\n+  int device_put_fully_replicated = 0;\n+  // DevicePutWithSharding that made a batched array creation call.\n+  int device_put_batched = 0;\n+  // DevicePutWithSharding that made per-shard creation calls followed by an\n+  // assembly call.\n+  int device_put_assembled = 0;\n+\n+  // Returns a map of the counters for the current thread.\n+  static std::unordered_map<std::string, int64_t> GetInfo();\n+};\n+\n }  // namespace xla\n \n #endif  // JAXLIB_PY_VALUES_H_\ndiff --git a/jaxlib/xla.cc b/jaxlib/xla.cc\nindex adf6f3c98297..4020e061b3f4 100644\n--- a/jaxlib/xla.cc\n+++ b/jaxlib/xla.cc\n@@ -45,6 +45,7 @@ limitations under the License.\n #include \"nanobind/stl/string.h\"  // IWYU pragma: keep\n #include \"nanobind/stl/string_view.h\"  // IWYU pragma: keep\n #include \"nanobind/stl/unique_ptr.h\"  // IWYU pragma: keep\n+#include \"nanobind/stl/unordered_map.h\"  // IWYU pragma: keep\n #include \"nanobind/stl/variant.h\"  // IWYU pragma: keep\n #include \"nanobind/stl/vector.h\"  // IWYU pragma: keep\n #include \"jaxlib/ffi.h\"\n@@ -975,6 +976,9 @@ NB_MODULE(_jax, m) {\n         nb::arg(\"recall_target\"), nb::arg(\"aggregate_to_topk\") = true,\n         nb::arg(\"input_size_override\") = -1);\n \n+  m.def(\"get_internal_device_put_info\",\n+        []() { return DevicePutInfo::GetInfo(); });\n+\n }  // NOLINT(readability/fn_size)\n \n }  // namespace xla\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 6aaae11c139d..69e168de9c2d 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 340\n+_version = 341\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 15966c678d87..5f775b46fb16 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -60,6 +60,7 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.compilation_cache import is_persistent_cache_enabled\n from jax._src.lib import _jax\n+from jax._src.lib import jaxlib_extension_version\n import jax._src.util as jax_util\n from jax.ad_checkpoint import checkpoint_name, checkpoint as new_checkpoint\n from jax.errors import (UnexpectedTracerError, TracerIntegerConversionError,\n@@ -1972,6 +1973,90 @@ def test_device_put_sharding_mismatched_tree_different_leaf_count(self):\n     ):\n       jax.device_put((x, y, z), device=(s1, s2))\n \n+  def test_internal_device_put_with_device(self):\n+    if jaxlib_extension_version < 341:\n+      raise unittest.SkipTest(\n+          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n+          \" DevicePut calls\")\n+\n+    # Hitting the cache for a single-device jitted execution while using a numpy\n+    # array calls internal `DevicePutWithDevice`.\n+    f = jax.jit(lambda x: x + 1)\n+    f(np.arange(8))\n+\n+    with jtu.count_internal_device_puts() as counts:\n+      f(np.arange(8))\n+    self.assertEqual(counts(), {\"device_put_with_device\": 1})\n+\n+  def test_internal_device_put_fully_replicated(self):\n+    if jaxlib_extension_version < 341:\n+      raise unittest.SkipTest(\n+          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n+          \" DevicePut calls\")\n+    if jax.device_count() < 2:\n+      raise unittest.SkipTest(\"Test requires >= 2 devices\")\n+\n+    # Creating an array from a numpy array with a fully-replicated sharding\n+    # calls internal `DevicePutWithSharding`, taking the fully-replicated sub\n+    # case.\n+    mesh = jax.sharding.Mesh(np.array(jax.devices()[:2]), \"x\")\n+    sharding = jax.NamedSharding(mesh, P())\n+\n+    with jtu.count_internal_device_puts() as counts:\n+      jax.device_put(np.arange(8), sharding)\n+    self.assertEqual(\n+        counts(),\n+        {\"device_put_with_sharding\": 1, \"device_put_fully_replicated\": 1},\n+    )\n+\n+  def test_internal_device_put_batched(self):\n+    if jaxlib_extension_version < 341:\n+      raise unittest.SkipTest(\n+          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n+          \" DevicePut calls\")\n+    if jax.device_count() < 2:\n+      raise unittest.SkipTest(\"Test requires >= 2 devices\")\n+\n+    # Creating an array from a numpy array with a non-fully-replicated sharding\n+    # calls internal `DevicePutWithSharding`, performing batched creation of a\n+    # multi-shard array.\n+    mesh = jax.sharding.Mesh(np.array(jax.devices()[:2]), \"x\")\n+    sharding = jax.NamedSharding(mesh, P(\"x\"))\n+\n+    with jtu.count_internal_device_puts() as counts:\n+      jax.device_put(np.arange(8), sharding)\n+    self.assertEqual(\n+        counts(), {\"device_put_with_sharding\": 1, \"device_put_batched\": 1}\n+    )\n+\n+  def test_internal_device_put_assembled(self):\n+    if jaxlib_extension_version < 341:\n+      raise unittest.SkipTest(\n+          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n+          \" DevicePut calls\")\n+    if jax.device_count() < 2:\n+      raise unittest.SkipTest(\"Test requires >= 2 devices\")\n+\n+    # Creating an array from per-device JAX arrays calls internal\n+    # `DevicePutWithSharding`, performing per-shard array adoption followed by\n+    # assembly.\n+    mesh = jax.sharding.Mesh(np.array(jax.devices()[:2]), \"x\")\n+    sharding = jax.NamedSharding(mesh, P(\"x\"))\n+\n+    arr = np.arange(8)\n+    per_device_arrs = {\n+        # Use uncommitted arrays that are not aligned with the destination\n+        # sharding so that we trigger `BatchedDevicePut`.\n+        index: jnp.array(arr[index])\n+        for _, index in sharding.devices_indices_map(arr.shape).items()\n+    }\n+    data_callback = lambda index: per_device_arrs[index]\n+    with jtu.count_internal_device_puts() as counts:\n+      jax.make_array_from_callback(arr.shape, sharding, data_callback)\n+    self.assertEqual(\n+        counts(), {\"device_put_with_sharding\": 1, \"device_put_assembled\": 1}\n+    )\n+\n   def test_device_put_custom_type_not_accepting_none_leaves(self):\n \n     class CustomNode(list):\n\n```",
        "from_id": [
            "hyeontaek",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28715 from dfm:custom-vjp-pp\n\nPiperOrigin-RevId: 758306384",
        "output": "```diff\nCommit: d6608988b3470e283c7a4a61739df23cb9d6b2a9\nDate: 2025-05-13T18:35:56Z\nURL: https://github.com/jax-ml/jax/commit/d6608988b3470e283c7a4a61739df23cb9d6b2a9\nFiles changed: 2\nAdditions: +46, Deletions: -0\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex dcd893f44123..7b81c4e86889 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -1046,6 +1046,23 @@ def dce_bwd(*args):\n   return list(used_ins), new_eqn\n pe.dce_rules[custom_vjp_call_p] = _custom_vjp_call_dce\n \n+\n+def _custom_vjp_call_pp_rule(eqn: core.JaxprEqn,\n+                             context: core.JaxprPpContext,\n+                             settings: core.JaxprPpSettings) -> core.pp.Doc:\n+  params = dict(eqn.params)\n+  if not params[\"num_consts\"]:\n+    params.pop(\"num_consts\")\n+  params.pop(\"out_trees\")\n+  params[\"fwd\"] = params.pop(\"fwd_jaxpr_thunk\").debug_info.func_name\n+  params[\"bwd\"] = params.pop(\"bwd\").debug_info.func_name\n+  names = sorted(params)\n+  params[\"name\"] = params[\"call_jaxpr\"].jaxpr.debug_info.func_name\n+  return core._pp_eqn(eqn.replace(params=params), context, settings,\n+                      params=[\"name\"] + names)\n+\n+core.pp_eqn_rules[custom_vjp_call_p] = _custom_vjp_call_pp_rule\n+\n batching.primitive_batchers[ad.custom_lin_p] = ad.raise_custom_vjp_error_on_jvp\n mlir.register_lowering(ad.custom_lin_p, ad.raise_custom_vjp_error_on_jvp)\n \ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 73dc2fbefcaa..9d10b40c6030 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3117,6 +3117,35 @@ def f_bwd(res, cts):\n     ):\n       f(0.5, 0.1, z=1.0)\n \n+  def test_pretty_print(self):\n+    @jax.custom_vjp\n+    def f(x):\n+      return x + 1\n+\n+    def f_fwd(x):\n+      return f(x), ()\n+\n+    def f_bwd(_, g):\n+      return g\n+    f.defvjp(f_fwd, f_bwd)\n+\n+    x = jnp.array([4.2], dtype=jnp.float32)\n+    jaxpr = jax.make_jaxpr(f)(x)\n+    actual = jaxpr.pretty_print(use_color=False)\n+    expected = textwrap.dedent(\n+        \"\"\"\n+        { lambda ; a:f32[1]. let\n+            b:f32[1] = custom_vjp_call[\n+              name=f\n+              bwd=f_bwd\n+              call_jaxpr={ lambda ; c:f32[1]. let d:f32[1] = add c 1.0:f32[] in (d,) }\n+              fwd=f_fwd\n+              symbolic_zeros=False\n+            ] a\n+          in (b,) }\n+        \"\"\").strip()\n+    self.assertEqual(actual, expected)\n+\n \n def transpose_unary(f, x_example):\n   def transposed(y):\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas] Allow f8 casting tests on TPUv5-.\n\nPiperOrigin-RevId: 758287085",
        "output": "```diff\nCommit: 78f89b8bb4742fd27ba0aa155b4f6f31c0a1d8db\nDate: 2025-05-13T17:52:03Z\nURL: https://github.com/jax-ml/jax/commit/78f89b8bb4742fd27ba0aa155b4f6f31c0a1d8db\nFiles changed: 1\nAdditions: +16, Deletions: -4\ndiff --git a/tests/pallas/ops_test.py b/tests/pallas/ops_test.py\nindex 61ebc19e018f..9bb6d31d15e1 100644\n--- a/tests/pallas/ops_test.py\n+++ b/tests/pallas/ops_test.py\n@@ -594,10 +594,16 @@ def kernel(x_ref, y_ref):\n   def test_cast_from_32bit(self, from_dtype, to_dtype, data):\n     sut_is_mosaic_gpu = jtu.test_device_matches([\"gpu\"]) and use_mosaic_gpu\n     if to_dtype in {\"float8_e4m3b11fnuz\", \"float8_e5m2\", \"float8_e4m3fn\"}:\n-      if not jtu.test_device_matches([\"tpu\"]) or jtu.get_tpu_version() < 5:\n+      if not jtu.test_device_matches([\"tpu\"]):\n         self.skipTest(\"Not supported on this hardware\")\n-      if not jtu.if_cloud_tpu_at_least(2025, 3, 8):\n+      if jtu.get_tpu_version() >= 5 and not jtu.if_cloud_tpu_at_least(\n+          2025, 3, 8\n+      ):\n         self.skipTest(\"Test requires libtpu from 2025/3/8 or later\")\n+      if jtu.get_tpu_version() < 5 and not jtu.if_cloud_tpu_at_least(\n+          2025, 5, 15\n+      ):\n+        self.skipTest(\"Test requires libtpu from 2025/5/15 or later\")\n     if from_dtype in {\"int2\", \"uint2\"} or to_dtype in {\"int2\", \"uint2\"}:\n       if jtu.test_device_matches([\"tpu\"]) and not jtu.if_cloud_tpu_at_least(\n           2025, 4, 1\n@@ -721,10 +727,16 @@ def test_cast_from_sub_32bit(self, from_dtype, to_dtype, randomize):\n         \"float8_e5m2\",\n         \"float8_e4m3fn\",\n     } or to_dtype in {\"float8_e4m3b11fnuz\", \"float8_e5m2\", \"float8_e4m3fn\"}:\n-      if not jtu.test_device_matches([\"tpu\"]) or jtu.get_tpu_version() < 5:\n+      if not jtu.test_device_matches([\"tpu\"]):\n         self.skipTest(\"Not supported on this hardware\")\n-      if not jtu.if_cloud_tpu_at_least(2025, 3, 9):\n+      if jtu.get_tpu_version() >= 5 and not jtu.if_cloud_tpu_at_least(\n+          2025, 3, 9\n+      ):\n         self.skipTest(\"Test requires libtpu from 2025/3/9 or later\")\n+      if jtu.get_tpu_version() < 5 and not jtu.if_cloud_tpu_at_least(\n+          2025, 5, 15\n+      ):\n+        self.skipTest(\"Test requires libtpu from 2025/5/15 or later\")\n     if from_dtype == \"int2\" and to_dtype == \"bool\":\n       self.skipTest(\n           \"TODO(b/343490729): XLA compare(s2, s2) yields wrong results\"\n\n```",
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add a pretty printing rule for custom_vjp.",
        "output": "```diff\nCommit: 71692fcbedf162c586ec8847d34938151ffa1f79\nDate: 2025-05-13T17:51:31Z\nURL: https://github.com/jax-ml/jax/commit/71692fcbedf162c586ec8847d34938151ffa1f79\nFiles changed: 2\nAdditions: +46, Deletions: -0\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex dcd893f44123..7b81c4e86889 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -1046,6 +1046,23 @@ def dce_bwd(*args):\n   return list(used_ins), new_eqn\n pe.dce_rules[custom_vjp_call_p] = _custom_vjp_call_dce\n \n+\n+def _custom_vjp_call_pp_rule(eqn: core.JaxprEqn,\n+                             context: core.JaxprPpContext,\n+                             settings: core.JaxprPpSettings) -> core.pp.Doc:\n+  params = dict(eqn.params)\n+  if not params[\"num_consts\"]:\n+    params.pop(\"num_consts\")\n+  params.pop(\"out_trees\")\n+  params[\"fwd\"] = params.pop(\"fwd_jaxpr_thunk\").debug_info.func_name\n+  params[\"bwd\"] = params.pop(\"bwd\").debug_info.func_name\n+  names = sorted(params)\n+  params[\"name\"] = params[\"call_jaxpr\"].jaxpr.debug_info.func_name\n+  return core._pp_eqn(eqn.replace(params=params), context, settings,\n+                      params=[\"name\"] + names)\n+\n+core.pp_eqn_rules[custom_vjp_call_p] = _custom_vjp_call_pp_rule\n+\n batching.primitive_batchers[ad.custom_lin_p] = ad.raise_custom_vjp_error_on_jvp\n mlir.register_lowering(ad.custom_lin_p, ad.raise_custom_vjp_error_on_jvp)\n \ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 73dc2fbefcaa..9d10b40c6030 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3117,6 +3117,35 @@ def f_bwd(res, cts):\n     ):\n       f(0.5, 0.1, z=1.0)\n \n+  def test_pretty_print(self):\n+    @jax.custom_vjp\n+    def f(x):\n+      return x + 1\n+\n+    def f_fwd(x):\n+      return f(x), ()\n+\n+    def f_bwd(_, g):\n+      return g\n+    f.defvjp(f_fwd, f_bwd)\n+\n+    x = jnp.array([4.2], dtype=jnp.float32)\n+    jaxpr = jax.make_jaxpr(f)(x)\n+    actual = jaxpr.pretty_print(use_color=False)\n+    expected = textwrap.dedent(\n+        \"\"\"\n+        { lambda ; a:f32[1]. let\n+            b:f32[1] = custom_vjp_call[\n+              name=f\n+              bwd=f_bwd\n+              call_jaxpr={ lambda ; c:f32[1]. let d:f32[1] = add c 1.0:f32[] in (d,) }\n+              fwd=f_fwd\n+              symbolic_zeros=False\n+            ] a\n+          in (b,) }\n+        \"\"\").strip()\n+    self.assertEqual(actual, expected)\n+\n \n def transpose_unary(f, x_example):\n   def transposed(y):\n\n```",
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Merge pull request #28711 from dfm:custom-vjp-symb-zeros\n\nPiperOrigin-RevId: 758272898",
        "output": "```diff\nCommit: 8060ca2e8dd12172b96f4fedcc42e953e7f20d0a\nDate: 2025-05-13T17:20:08Z\nURL: https://github.com/jax-ml/jax/commit/8060ca2e8dd12172b96f4fedcc42e953e7f20d0a\nFiles changed: 1\nAdditions: +11, Deletions: -9\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 435e9027f5b3..45705382efa0 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -565,12 +565,12 @@ def process_custom_vjp_call(self, prim, fun, fwd, bwd, tracers, out_trees,\n     _, res_tree = out_trees()\n     res, primals_out = split_list(res_and_primals_out, [res_tree.num_leaves])\n     avals_out = [core.get_aval(x).to_tangent_aval() for x in primals_out]\n-    # TODO(frostig,mattjj): avoid instantiating zeros when we don't have to!\n+    in_zeros = [type(t) is Zero for t in tangents_in]\n+    nz_tangents_in = [t for z, t in zip(in_zeros, tangents_in) if not z]\n     with core.set_current_trace(self.parent_trace):\n-      tangents_in = map(instantiate_zeros, tangents_in)\n       tangents_out = custom_lin_p.bind(\n-        *res, *tangents_in, num_res=res_tree.num_leaves, bwd=bwd,\n-        out_avals=avals_out, symbolic_zeros=symbolic_zeros)\n+          *res, *nz_tangents_in, num_res=res_tree.num_leaves, bwd=bwd,\n+          out_avals=avals_out, symbolic_zeros=symbolic_zeros, in_zeros=in_zeros)\n     return map(partial(maybe_jvp_tracer, self), primals_out, tangents_out)\n \n   def process_custom_transpose(self, prim, call, tracers, **params):\n@@ -734,11 +734,12 @@ def process_custom_vjp_call(self, prim, fun, fwd,\n     res, primals_out = split_list(res_and_primals_out, [res_tree.num_leaves])\n     avals_out = [core.get_aval(x).to_tangent_aval() for x in primals_out]\n \n-    tangents_in_zeros = map(instantiate_zeros, tangents_in)\n+    in_zeros = [type(t) is Zero for t in tangents_in]\n+    nz_tangents_in = [t for z, t in zip(in_zeros, tangents_in) if not z]\n     with core.set_current_trace(self.tangent_trace):\n       tangents_out = custom_lin_p.bind(\n-        *res, *tangents_in_zeros, num_res=res_tree.num_leaves, bwd=bwd,\n-        out_avals=avals_out, symbolic_zeros=symbolic_zeros)\n+          *res, *nz_tangents_in, num_res=res_tree.num_leaves, bwd=bwd,\n+          out_avals=avals_out, symbolic_zeros=symbolic_zeros, in_zeros=in_zeros)\n     tangent_nzs_out = [type(t) is not Zero for t in tangents_out]\n     return map(partial(maybe_linearize_tracer, self), primals_out, tangent_nzs_out, tangents_out)\n \n@@ -1223,7 +1224,7 @@ def raise_custom_vjp_error_on_jvp(*_, **__):\n \n def _custom_lin_transpose(cts_out, *invals, num_res,\n                           bwd: lu.WrappedFun, out_avals,\n-                          symbolic_zeros):\n+                          symbolic_zeros, in_zeros):\n   res, _ = split_list(invals, [num_res])\n   if symbolic_zeros:\n     cts_out = map(replace_internal_symbolic_zeros, cts_out)\n@@ -1231,7 +1232,8 @@ def _custom_lin_transpose(cts_out, *invals, num_res,\n     cts_out = map(instantiate_zeros, cts_out)\n   cts_in = bwd.call_wrapped(*res, *cts_out)\n   cts_in = map(replace_rule_output_symbolic_zeros, cts_in)\n-  return [None] * num_res + list(cts_in)\n+  nz_cts_in, _ = partition_list(in_zeros, cts_in)\n+  return [None] * num_res + nz_cts_in\n primitive_transposes[custom_lin_p] = _custom_lin_transpose\n \n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Don't instantiate zeros passed to custom_lin_p.",
        "output": "```diff\nCommit: df66c2fdc538a5b0d8e7d052a96ceaa6258a9da5\nDate: 2025-05-13T16:57:47Z\nURL: https://github.com/jax-ml/jax/commit/df66c2fdc538a5b0d8e7d052a96ceaa6258a9da5\nFiles changed: 1\nAdditions: +11, Deletions: -9\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 435e9027f5b3..45705382efa0 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -565,12 +565,12 @@ def process_custom_vjp_call(self, prim, fun, fwd, bwd, tracers, out_trees,\n     _, res_tree = out_trees()\n     res, primals_out = split_list(res_and_primals_out, [res_tree.num_leaves])\n     avals_out = [core.get_aval(x).to_tangent_aval() for x in primals_out]\n-    # TODO(frostig,mattjj): avoid instantiating zeros when we don't have to!\n+    in_zeros = [type(t) is Zero for t in tangents_in]\n+    nz_tangents_in = [t for z, t in zip(in_zeros, tangents_in) if not z]\n     with core.set_current_trace(self.parent_trace):\n-      tangents_in = map(instantiate_zeros, tangents_in)\n       tangents_out = custom_lin_p.bind(\n-        *res, *tangents_in, num_res=res_tree.num_leaves, bwd=bwd,\n-        out_avals=avals_out, symbolic_zeros=symbolic_zeros)\n+          *res, *nz_tangents_in, num_res=res_tree.num_leaves, bwd=bwd,\n+          out_avals=avals_out, symbolic_zeros=symbolic_zeros, in_zeros=in_zeros)\n     return map(partial(maybe_jvp_tracer, self), primals_out, tangents_out)\n \n   def process_custom_transpose(self, prim, call, tracers, **params):\n@@ -734,11 +734,12 @@ def process_custom_vjp_call(self, prim, fun, fwd,\n     res, primals_out = split_list(res_and_primals_out, [res_tree.num_leaves])\n     avals_out = [core.get_aval(x).to_tangent_aval() for x in primals_out]\n \n-    tangents_in_zeros = map(instantiate_zeros, tangents_in)\n+    in_zeros = [type(t) is Zero for t in tangents_in]\n+    nz_tangents_in = [t for z, t in zip(in_zeros, tangents_in) if not z]\n     with core.set_current_trace(self.tangent_trace):\n       tangents_out = custom_lin_p.bind(\n-        *res, *tangents_in_zeros, num_res=res_tree.num_leaves, bwd=bwd,\n-        out_avals=avals_out, symbolic_zeros=symbolic_zeros)\n+          *res, *nz_tangents_in, num_res=res_tree.num_leaves, bwd=bwd,\n+          out_avals=avals_out, symbolic_zeros=symbolic_zeros, in_zeros=in_zeros)\n     tangent_nzs_out = [type(t) is not Zero for t in tangents_out]\n     return map(partial(maybe_linearize_tracer, self), primals_out, tangent_nzs_out, tangents_out)\n \n@@ -1223,7 +1224,7 @@ def raise_custom_vjp_error_on_jvp(*_, **__):\n \n def _custom_lin_transpose(cts_out, *invals, num_res,\n                           bwd: lu.WrappedFun, out_avals,\n-                          symbolic_zeros):\n+                          symbolic_zeros, in_zeros):\n   res, _ = split_list(invals, [num_res])\n   if symbolic_zeros:\n     cts_out = map(replace_internal_symbolic_zeros, cts_out)\n@@ -1231,7 +1232,8 @@ def _custom_lin_transpose(cts_out, *invals, num_res,\n     cts_out = map(instantiate_zeros, cts_out)\n   cts_in = bwd.call_wrapped(*res, *cts_out)\n   cts_in = map(replace_rule_output_symbolic_zeros, cts_in)\n-  return [None] * num_res + list(cts_in)\n+  nz_cts_in, _ = partition_list(in_zeros, cts_in)\n+  return [None] * num_res + nz_cts_in\n primitive_transposes[custom_lin_p] = _custom_lin_transpose\n \n \n\n```",
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "[Mosaic] Make `tpu.relayout` an explicit operation, merge in existing behavior, stop calling relayout() in apply.\n\nThis change should reduce complexity and make it easier to see what happened in a graph.\n\nNote - there are still cases where certain relayout() calls are not ops yet, those will be migrated in the future. Specifically, see the note in the CL around force_relayout.\n\nAdded helper methods to generate full like vectors.\n\nFollowup for subsequent CLs: Simplify Relayout rule in future CLs, maybe break up into smaller sub relayouts with nice names.\n\nFollowup for subsequent CLs: Unify transpose in here\n\nPiperOrigin-RevId: 758240646",
        "output": "```diff\nCommit: 123022cae08d83c4d53ac77481b5c2391f003794\nDate: 2025-05-13T15:56:29Z\nURL: https://github.com/jax-ml/jax/commit/123022cae08d83c4d53ac77481b5c2391f003794\nFiles changed: 2\nAdditions: +40, Deletions: -31\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex d625e8bf4d6f..656be0e677b0 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -6811,6 +6811,7 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n   FAILUREOR_ASSIGN_OR_RETURN(\n       xla::Array<Value> src_tiles,\n       disassemble(builder, src, v, target_shape, /*use_implicit_shape=*/true));\n+\n   if (is_mask_pack) {\n     std::vector<int64_t> vmsks_shape(src_tiles.dimensions().begin(),\n                                      src_tiles.dimensions().end());\n@@ -6855,6 +6856,7 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n   auto assemble_with_mask_check = [&](xla::Array<Value> &tiles,\n                                       bool use_implicit_shape = false) {\n \n+\n     if (is_mask) {\n       auto zeros_tile = builder.create<arith::ConstantOp>(\n           tiles.begin()->getLoc(),\n@@ -6985,34 +6987,18 @@ LogicalResult tpu_relayout_rule(RewriteContext &ctx, Operation &op,\n \n   auto in_layout_array_attr =\n       tpu_relayout_op->getAttrOfType<ArrayAttr>(\"in_layout\");\n-  if (!in_layout_array_attr || in_layout_array_attr.empty()) {\n-    return tpu_relayout_op.emitOpError(\n-        \"missing or empty 'in_layout' attribute\");\n-  }\n   auto src_vla = dyn_cast<tpu::VectorLayoutAttr>(in_layout_array_attr[0]);\n-  if (!src_vla) {\n-    return tpu_relayout_op.emitOpError(\n-        \"'in_layout' attribute is not a VectorLayoutAttr\");\n-  }\n   VectorLayout src_layout = src_vla.getLayout().value();\n \n   auto out_layout_array_attr =\n       tpu_relayout_op->getAttrOfType<ArrayAttr>(\"out_layout\");\n-  if (!out_layout_array_attr || out_layout_array_attr.empty()) {\n-    return tpu_relayout_op.emitOpError(\n-        \"missing or empty 'out_layout' attribute\");\n-  }\n   auto dst_vla = dyn_cast<tpu::VectorLayoutAttr>(out_layout_array_attr[0]);\n-  if (!dst_vla) {\n-    return tpu_relayout_op.emitOpError(\n-        \"'out_layout' attribute is not a VectorLayoutAttr\");\n-  }\n   VectorLayout dst_layout = dst_vla.getLayout().value();\n \n   if (src_layout == dst_layout) {\n-    tpu_relayout_op.replaceAllUsesWith(tpu_relayout_op.getInput());\n-    tpu_relayout_op.erase();\n-    return success();\n+    return op.emitError(\n+        \"Source and destination layouts are the same - did you forget to run \"\n+        \"relayout-insertion-pass?\");\n   }\n \n   OpBuilder builder(&op);\n@@ -7079,9 +7065,6 @@ const llvm::StringMap<rule_type> &rules() {\n   return *rules;\n }\n \n-// TODO(apaszke): Implement a debug mode that inserts additional assertions.\n-// For example, we should verify that ops that were supposed to generate\n-// replicated outputs satisfy that requirement.\n LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n   // When an operation does not have any operands, the layout_in tuple is empty.\n   // If one of the operands is not of vector type, the corresponding entry in\n@@ -7117,14 +7100,11 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n                                  getOutLayouts(*def_op, ctx.target_shape));\n       const Layout lo = def_layouts[res_idx];\n       TPU_ASSERT_OP(lo.has_value());\n-      if (*lo == *li) {\n-        continue;\n+      if (*lo != *li) {\n+        return op.emitError(\n+            \"Invariant violation: Input layout does not match output layout - \"\n+            \"did you forget to run relayout-insertion?\");\n       }\n-      OpBuilder builder(&op);\n-      FAILUREOR_ASSIGN_OR_RETURN(\n-          Value new_v, relayout(ctx, builder, vector_operand, /*src=*/*lo,\n-                                /*dst=*/*li));\n-      op.setOperand(idx, new_v);\n     }\n   }\n \n@@ -7132,7 +7112,8 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n   // support for offsets outside of the first tile. When support is more broad,\n   // any op without support should check it within their own rule.\n   if (!isa<arith::TruncFOp, arith::TruncIOp, vector::BroadcastOp,\n-           vector::ExtractStridedSliceOp, vector::ShapeCastOp>(op)) {\n+           vector::ExtractStridedSliceOp, vector::ShapeCastOp, tpu::RelayoutOp>(\n+          op)) {\n     for (const Layout &layout : layouts_in) {\n       if (layout && layout->offsets()[1].has_value() &&\n           layout->offsets()[1].value() >= layout->tiling()[1]) {\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/relayout_insertion.cc b/jaxlib/mosaic/dialect/tpu/transforms/relayout_insertion.cc\nindex 6ddf8bd5ce66..178b97876b49 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/relayout_insertion.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/relayout_insertion.cc\n@@ -119,7 +119,26 @@ FailureOr<TypedValue<VectorType>> relayout(\n               dst_bitwidth_layout);\n     return cast<TypedValue<VectorType>>(cmp_op.getResult());\n   }\n-  return v;\n+  // Fall through to generic relayout.\n+  auto relayout_op =\n+      builder.create<tpu::RelayoutOp>(v.getLoc(), v.getType(), v);\n+  setLayout(relayout_op, src, dst);\n+\n+  return cast<TypedValue<VectorType>>(relayout_op.getResult());\n+}\n+\n+LogicalResult insertRelayout(Operation &op, int hardware_generation,\n+                             std::array<int64_t, 2> target_shape);\n+\n+LogicalResult insertRelayoutBlock(Block &block, int hardware_generation,\n+                                  const std::array<int64_t, 2> target_shape) {\n+  // We'll be modifying the block, so use early increment.\n+  for (Operation &op : make_early_inc_range(block)) {\n+    if (failed(insertRelayout(op, hardware_generation, target_shape))) {\n+      return failure();\n+    }\n+  }\n+  return success();\n }\n \n // TODO(jevinjiang): make relayout to an op so we don't need decide when to\n@@ -167,6 +186,15 @@ LogicalResult insertRelayout(Operation &op, int hardware_generation,\n                               /*dst=*/*li, hardware_generation, target_shape));\n     op.setOperand(idx, new_v);\n   }\n+\n+  for (auto &region : op.getRegions()) {\n+    for (auto &block : region.getBlocks()) {\n+      if (failed(\n+              insertRelayoutBlock(block, hardware_generation, target_shape))) {\n+        return failure();\n+      }\n+    }\n+  }\n   return success();\n }\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28589 from dfm:consolidate-custom-vjp-primitives\n\nPiperOrigin-RevId: 758235138",
        "output": "```diff\nCommit: 1ad9eae3a28d2254500a4aa52e0af7d649a939ee\nDate: 2025-05-13T15:41:13Z\nURL: https://github.com/jax-ml/jax/commit/1ad9eae3a28d2254500a4aa52e0af7d649a939ee\nFiles changed: 8\nAdditions: +111, Deletions: -159\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex 144cbaf5cd21..aa9bfe9529ce 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -1079,17 +1079,17 @@ def jvp(*xs):\n     return [*primal_errs, *out_primals, *tangent_errs, *out_tangents]\n   return lu.wrap_init(jvp, debug_info=jvp_jaxpr_fun.debug_info)\n \n-def custom_vjp_call_jaxpr_rule(in_err, enabled_errors, *in_vals,\n-                               fun_jaxpr: core.ClosedJaxpr,\n-                               fwd_jaxpr_thunk, num_consts,\n-                               bwd: lu.WrappedFun, out_trees,\n-                               symbolic_zeros: bool):\n+def custom_vjp_call_rule(in_err, enabled_errors, *in_vals,\n+                         call_jaxpr: core.ClosedJaxpr,\n+                         fwd_jaxpr_thunk, num_consts,\n+                         bwd: lu.WrappedFun, out_trees,\n+                         symbolic_zeros: bool):\n   err_vals, err_tree = jtu.tree_flatten(in_err)\n   num_errs = err_tree.num_leaves\n   checkified_fun = lu.wrap_init(\n-      functools.partial(checkify_jaxpr_flat, fun_jaxpr.jaxpr,\n-                        fun_jaxpr.consts, enabled_errors, err_tree),\n-      debug_info=fun_jaxpr.jaxpr.debug_info)\n+      functools.partial(checkify_jaxpr_flat, call_jaxpr.jaxpr,\n+                        call_jaxpr.consts, enabled_errors, err_tree),\n+      debug_info=call_jaxpr.jaxpr.debug_info)\n   checkified_fun, fun_metadata = _flatten_and_get_error_metadata_thunk(\n       checkified_fun)\n \n@@ -1097,13 +1097,13 @@ def checkified_fwd(*args):\n     # TODO(lenamartens, sharadmv): why not checkify here?\n     xs, zeros = args[::2], args[1::2]\n     xs, zeros = xs[num_errs:], zeros[num_errs:]\n-    fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk(*zeros)\n+    fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk.call_wrapped(*zeros)\n     xs_without_consts = xs[num_consts:]\n     return core.eval_jaxpr(fwd_jaxpr, fwd_consts, *xs_without_consts)\n \n   # TODO(necula): the fwd result_paths are not quite the same as fun_jaxpr\n   checkified_fwd_wrapped = lu.wrap_init(checkified_fwd,\n-                                        debug_info=fun_jaxpr.jaxpr.debug_info)\n+                                        debug_info=fwd_jaxpr_thunk.debug_info)\n   bwd_ = lu.wrap_init(lambda *args: (*(None,)*num_errs, *bwd.call_wrapped(*args)),\n                       debug_info=bwd.debug_info)\n   checkified_fwd_wrapped, fwd_out_tree = flatten_fun_output(checkified_fwd_wrapped)\n@@ -1118,7 +1118,7 @@ def checkified_fwd(*args):\n   else:\n     out_err, out_vals = in_err, all_outs\n   return out_err, out_vals\n-error_checks[custom_derivatives.custom_vjp_call_jaxpr_p] = custom_vjp_call_jaxpr_rule\n+error_checks[custom_derivatives.custom_vjp_call_p] = custom_vjp_call_rule\n \n \n def check_discharge_rule(error, enabled_errors, *args, err_tree, debug):\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex dc8fc90e3d1f..dcd893f44123 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -425,16 +425,14 @@ def _custom_jvp_call_typecheck(_, *in_avals, call_jaxpr, jvp_jaxpr_fun,\n   return call_jaxpr.out_avals, call_jaxpr.effects\n core.custom_typechecks[custom_jvp_call_p] = _custom_jvp_call_typecheck\n \n-def _custom_jvp_call_mlir_translation(ctx, *args, call_jaxpr, jvp_jaxpr_fun,\n-                                      num_consts, symbolic_zeros):\n-  del jvp_jaxpr_fun, num_consts, symbolic_zeros\n+def _custom_jvp_vjp_call_lowering(ctx, *args, call_jaxpr, **_):\n   consts = mlir._ir_consts(call_jaxpr.consts)\n   out, tokens = mlir.jaxpr_subcomp(ctx.module_context, call_jaxpr.jaxpr,\n                                    ctx.name_stack, ctx.tokens_in, consts,\n                                    *args, dim_var_values=ctx.dim_var_values)\n   ctx.set_tokens_out(tokens)\n   return out\n-mlir.register_lowering(custom_jvp_call_p, _custom_jvp_call_mlir_translation)\n+mlir.register_lowering(custom_jvp_call_p, _custom_jvp_vjp_call_lowering)\n \n # If a (multi)linear function is defined with a custom jvp, then\n # custom_jvp_call_ can appear in jaxprs to be transposed. Since it's already\n@@ -936,8 +934,8 @@ def _temporary_dtype_exception(a, a_) -> bool:\n def _temporary_shape_exception(a, a_) -> bool:\n   return config.custom_vjp_disable_shape_check.value\n \n-class CustomVJPCallPrimitive(core.CallPrimitive):\n-  initial_style: core.Primitive\n+class CustomVJPCallPrimitive(core.Primitive):\n+  multiple_results = True\n \n   def bind(self, *args, **params):\n     return self._true_bind(*args, **params)\n@@ -946,107 +944,70 @@ def bind_with_trace(self, trace, args, params):\n     fun, fwd, bwd, tracers = args[0], args[1], args[2], args[3:]\n     return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n \n-custom_vjp_call_p = CustomVJPCallPrimitive('custom_vjp_call')\n+  def impl(self, fun, fwd, bwd, *args):\n+    raise NotImplementedError\n+\n+  def get_bind_params(self, params):\n+    new_params = dict(params)\n+    call_jaxpr: core.ClosedJaxpr = new_params.pop('call_jaxpr')\n+    num_consts: int = new_params.pop('num_consts')\n+    fwd_jaxpr_thunk = new_params.pop('fwd_jaxpr_thunk')\n+    fun = lu.wrap_init(core.jaxpr_as_fun(call_jaxpr),\n+                       debug_info=call_jaxpr.jaxpr.debug_info)\n+    fwd = lift_fwd(num_consts, fwd_jaxpr_thunk)\n+    const_avals, _ = split_list(call_jaxpr.in_avals, [num_consts])\n+    bwd = _handle_consts_in_bwd(new_params.pop('bwd'), const_avals)\n+    return [fun, fwd, bwd], new_params\n+\n+def lift_fwd(num_consts: int, fwd_jaxpr_thunk: lu.WrappedFun) -> lu.WrappedFun:\n+  def fwd(*args):\n+    vals, zeros = args[::2], args[1::2]\n+    assert len(vals) == len(zeros)\n+    _, primals = split_list(vals, [num_consts])\n+    const_zeros, in_zeros = split_list(zeros, [num_consts])\n+    if any(const_zeros):\n+      raise ad.CustomVJPException()\n+    fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk.call_wrapped(*in_zeros)\n+    return core.eval_jaxpr(fwd_jaxpr, fwd_consts, *primals)\n+  return lu.wrap_init(fwd, debug_info=fwd_jaxpr_thunk.debug_info)\n \n-def _custom_vjp_call_jaxpr_impl(*args, fun_jaxpr, **_):\n-  return core.jaxpr_as_fun(fun_jaxpr)(*args)\n+@lu.transformation2\n+def _handle_consts_in_bwd(f, const_avals, *args):\n+  return [Zero(a) for a in const_avals] + list(f(*args))\n \n-def _custom_vjp_call_jaxpr_abstract_eval(*_, fun_jaxpr, **__):\n-  disallowed_effects = effects.custom_derivatives_allowed_effects.filter_not_in(fun_jaxpr.effects)\n+custom_vjp_call_p = CustomVJPCallPrimitive('custom_vjp_call')\n+mlir.register_lowering(custom_vjp_call_p, _custom_jvp_vjp_call_lowering)\n+\n+def _custom_vjp_call_typecheck(_, *in_avals, call_jaxpr, **kwargs):\n+  del in_avals, kwargs\n+  disallowed_effects = effects.custom_derivatives_allowed_effects.filter_not_in(\n+      call_jaxpr.effects)\n   if disallowed_effects:\n     raise NotImplementedError(\n         f'Effects not supported in `custom_vjp`: {disallowed_effects}')\n-  return fun_jaxpr.out_avals, fun_jaxpr.effects\n-\n-custom_vjp_call_jaxpr_p = core.Primitive('custom_vjp_call_jaxpr')\n-custom_vjp_call_jaxpr_p.multiple_results = True\n-custom_vjp_call_jaxpr_p.def_impl(_custom_vjp_call_jaxpr_impl)\n-custom_vjp_call_jaxpr_p.def_effectful_abstract_eval(_custom_vjp_call_jaxpr_abstract_eval)\n-CustomVJPCallPrimitive.initial_style = custom_vjp_call_jaxpr_p\n-\n-mlir.register_lowering(custom_vjp_call_jaxpr_p, mlir.lower_fun(\n-    _custom_vjp_call_jaxpr_impl, multiple_results=True))\n-\n-def _custom_vjp_call_jaxpr_jvp(\n-    primals, tangents, *, fun_jaxpr: core.ClosedJaxpr,\n-    fwd_jaxpr_thunk: Callable[..., tuple[core.Jaxpr, Sequence[Any]]],\n-    num_consts: int, bwd: lu.WrappedFun,\n-    out_trees: Callable[[], Sequence[PyTreeDef]],\n-    symbolic_zeros: bool):\n-  _, args = split_list(primals, [num_consts])\n-  consts_dot, args_dot = split_list(tangents, [num_consts])\n-  if any(type(t) is not Zero for t in consts_dot):\n-    raise ad.CustomVJPException()\n-  zeros = [type(t) is not Zero for t in args_dot]\n-  fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk(*zeros)  # consts can be tracers!\n-  _, res_tree = out_trees()\n-  res_and_primals_out = core.eval_jaxpr(fwd_jaxpr, fwd_consts, *args)\n-  res, primals_out = split_list(res_and_primals_out, [res_tree.num_leaves])\n-  avals_out = [core.get_aval(x).to_tangent_aval() for x in primals_out]\n-  args_dot = map(ad.instantiate_zeros, args_dot)\n-  tangents_out = ad.custom_lin_p.bind(\n-      *res, *args_dot, num_res=res_tree.num_leaves, bwd=bwd,\n-      out_avals=avals_out, symbolic_zeros=symbolic_zeros)\n-  tangents_out = map(lax.tie_p.bind, primals_out, tangents_out)\n-  return primals_out, tangents_out\n-ad.primitive_jvps[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_jvp\n-\n-def _custom_vjp_call_jaxpr_vmap(\n-    axis_data, args, in_dims, *,\n-    fun_jaxpr: core.ClosedJaxpr,\n-    fwd_jaxpr_thunk: Callable[..., tuple[core.Jaxpr, Sequence[Any]]],\n-    num_consts: int, bwd: lu.WrappedFun,\n-    out_trees: Callable, symbolic_zeros: bool):\n-  args = [batching.moveaxis(x, d, 0) if d is not not_mapped and d != 0\n-          else x for x, d in zip(args, in_dims)]\n-  in_batched = [d is not not_mapped for d in in_dims]\n-  _, args_batched = split_list(in_batched, [num_consts])\n-  batched_fun_jaxpr, out_batched = batching.batch_jaxpr(\n-      fun_jaxpr, axis_data, in_batched, False)\n-  out_dims1 = [0 if b else not_mapped for b in out_batched]\n-  out_dims2 = []\n-\n-  @pe._memoize\n-  def batched_fwd_jaxpr_thunk(*zeros):\n-    fwd_jaxpr = core.ClosedJaxpr(*fwd_jaxpr_thunk(*zeros))  # consts can be tracers\n-    batched_fwd_jaxpr, out_batched = batching.batch_jaxpr(\n-        fwd_jaxpr, axis_data, args_batched, False)\n-    out_dims2.append([0 if b else not_mapped for b in out_batched])\n-    return batched_fwd_jaxpr.jaxpr, batched_fwd_jaxpr.consts\n-\n-  fwd_args_batched = [0 if b else not_mapped for b in args_batched]\n-  fwd_out_dims = lambda: out_dims2[0]\n-  tag = core.TraceTag()\n-  batched_bwd = batching.batch_custom_vjp_bwd(\n-    bwd, tag, axis_data, fwd_out_dims, fwd_args_batched)\n-\n-  batched_outs = custom_vjp_call_jaxpr_p.bind(\n-      *args, fun_jaxpr=batched_fun_jaxpr,\n-      fwd_jaxpr_thunk=batched_fwd_jaxpr_thunk, bwd=batched_bwd,\n-      num_consts=num_consts, out_trees=out_trees, symbolic_zeros=symbolic_zeros)\n-  out_dims = out_dims2[0] if out_dims2 else out_dims1\n-  return batched_outs, out_dims\n-batching.fancy_primitive_batchers[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_vmap\n+  return call_jaxpr.out_avals, call_jaxpr.effects\n+core.custom_typechecks[custom_vjp_call_p] = _custom_vjp_call_typecheck\n \n-def _custom_vjp_call_jaxpr_dce(\n+def _custom_vjp_call_dce(\n     used_outs: Sequence[bool], eqn: core.JaxprEqn\n ) -> tuple[list[bool], core.JaxprEqn | None]:\n   if not any(used_outs) and not pe.has_effects(eqn):\n     return [False] * len(eqn.invars), None\n-  fun_jaxpr: core.ClosedJaxpr = eqn.params[\"fun_jaxpr\"]\n+  call_jaxpr: core.ClosedJaxpr = eqn.params[\"call_jaxpr\"]\n   fwd_jaxpr_thunk = eqn.params[\"fwd_jaxpr_thunk\"]\n   bwd: lu.WrappedFun = eqn.params[\"bwd\"]\n   out_trees: Callable[[], Sequence[PyTreeDef]] = eqn.params[\"out_trees\"]\n   symbolic_zeros: bool = eqn.params[\"symbolic_zeros\"]\n-  dce_fun_jaxpr: core.ClosedJaxpr\n+  dce_call_jaxpr: core.ClosedJaxpr\n   used_ins: Sequence[bool]\n-  dce_fun_jaxpr, used_ins = _cached_closed_call_dce_instantiate(\n-      fun_jaxpr, tuple(used_outs))\n+  dce_call_jaxpr, used_ins = _cached_closed_call_dce_instantiate(\n+      call_jaxpr, tuple(used_outs))\n   assert all(used_ins)\n \n+  @partial(lu.wrap_init, debug_info=fwd_jaxpr_thunk.debug_info)\n   @pe._memoize\n   def dce_fwd_jaxpr_thunk(*zeros):\n-    fwd_jaxpr = core.ClosedJaxpr(*fwd_jaxpr_thunk(*zeros))\n+    fwd_jaxpr = core.ClosedJaxpr(*fwd_jaxpr_thunk.call_wrapped(*zeros))\n     _, res_tree = out_trees()\n     num_res = res_tree.num_leaves\n     dce_fwd_jaxpr, _ = _cached_closed_call_dce_instantiate(\n@@ -1058,7 +1019,7 @@ def dce_bwd(*args):\n     res, cts = split_list(args, [res_tree.num_leaves])\n     cts_ = iter(cts)\n     all_cts = []\n-    for used, aval in zip(used_outs, fun_jaxpr.out_avals):\n+    for used, aval in zip(used_outs, call_jaxpr.out_avals):\n       if used:\n         all_cts.append(next(cts_))\n       else:\n@@ -1075,17 +1036,15 @@ def dce_bwd(*args):\n   outvars = [v for used, v in zip(used_outs, eqn.outvars) if used]\n   new_params = dict(\n       eqn.params,\n-      fun_jaxpr=dce_fun_jaxpr,\n+      call_jaxpr=dce_call_jaxpr,\n       fwd_jaxpr_thunk=dce_fwd_jaxpr_thunk,\n       bwd=dce_bwd_wrapped,\n   )\n   new_eqn = pe.new_jaxpr_eqn(\n-      eqn.invars, outvars, eqn.primitive, new_params, dce_fun_jaxpr.effects,\n+      eqn.invars, outvars, eqn.primitive, new_params, dce_call_jaxpr.effects,\n       eqn.source_info, eqn.ctx)\n   return list(used_ins), new_eqn\n-pe.dce_rules[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_dce\n-\n-xla.register_initial_style_primitive(custom_vjp_call_jaxpr_p)\n+pe.dce_rules[custom_vjp_call_p] = _custom_vjp_call_dce\n \n batching.primitive_batchers[ad.custom_lin_p] = ad.raise_custom_vjp_error_on_jvp\n mlir.register_lowering(ad.custom_lin_p, ad.raise_custom_vjp_error_on_jvp)\n@@ -1586,7 +1545,6 @@ def jvp(primals, tangents):\n # TODO(mattjj): remove these stubs, which exist to avoid breaking internal users\n custom_jvp_call_jaxpr_p = core.Primitive(\"custom_jvp_call_jaxpr\")\n \n-\n # The following is a helper for optimizing the behavior of custom_vjp when used\n # under remat. This is really only useful when the `fwd` function to custom_vjp\n # executes a black box kernel. Otherwise, DCE will perform this optimization\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 64226a789cde..5866b0c5f8eb 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -434,49 +434,45 @@ def process_custom_vjp_call(self, prim, f, fwd, bwd, tracers, out_trees, symboli\n     if all(t.is_known() for t in tracers):\n       vals = [t.pval[1] for t in tracers]\n       with core.set_current_trace(self.parent_trace):\n-        return prim.bind(f, fwd, bwd, *vals, out_trees=out_trees, symbolic_zeros=symbolic_zeros)\n-    else:\n-      # TODO(mattjj): remove non-ad users of partial eval, then drop this case.\n-      # We stage out the whole thing, i.e. no nontrivial partial evaluation.\n-      tracers = map(self.instantiate_const_abstracted, tracers)\n-      # Because we instantiate all tracers, in_knowns is all False.\n-      in_knowns, in_avals, () = partition_pvals([t.pval for t in tracers])\n-      f = trace_to_subjaxpr_nounits(f, self, True, f.debug_info)\n-      f, aux = partial_eval_wrapper_nounits(f, (*in_knowns,), (*in_avals,))\n-      with core.set_current_trace(self.parent_trace):\n-        out_flat = prim.bind(f, fwd, bwd, out_trees=out_trees,\n-                             symbolic_zeros=symbolic_zeros)\n-      out_knowns, out_avals, jaxpr, env = aux()\n-      out_consts, res = split_list(out_flat, [len(out_flat)-len(jaxpr.constvars)])\n-      res_tracers = map(self.new_instantiated_const, res)\n-      env_tracers = map(self.to_jaxpr_tracer, env)\n-      out_tracers = [JaxprTracer(self, PartialVal.unknown(a), None)\n-                    for a in out_avals]\n-      closed_jaxpr = core.ClosedJaxpr(convert_constvars_jaxpr(jaxpr), ())\n-\n-      @_memoize\n-      def fwd_jaxpr_thunk(*zeros):\n-        fwd_ = _interleave_fun(fwd, zeros)\n-        fwd_ = trace_to_subjaxpr_nounits(fwd_, self, True, fwd_.debug_info)\n-        fwd_, aux = partial_eval_wrapper_nounits(fwd_, (*in_knowns,), (*in_avals,))\n-        out_flat = fwd_.call_wrapped()\n-        out_knowns, out_avals, jaxpr, env = aux()\n-        _, res = split_list(out_flat, [len(out_flat)-len(jaxpr.constvars)])\n-        converted_jaxpr = convert_envvars_to_constvars(jaxpr, len(env))\n-        return converted_jaxpr, (*res, *env)\n+        return prim.bind(f, fwd, bwd, *vals, out_trees=out_trees,\n+                         symbolic_zeros=symbolic_zeros)\n+\n+    tracers = map(self.instantiate_const, tracers)\n+    in_knowns = (False,) * len(tracers)\n+    in_avals = tuple(t.aval for t in tracers)\n+    f_ = trace_to_subjaxpr_nounits2(f, self.tag, f.debug_info, True)\n+    f_, aux = partial_eval_wrapper_nounits(f_, in_knowns, in_avals)\n+    params = dict(out_trees=out_trees, symbolic_zeros=symbolic_zeros)\n+    res = prim.bind_with_trace(self.parent_trace, (f_, fwd, bwd), params)\n+    out_knowns, out_avals, jaxpr, env = aux()\n+    assert not any(out_knowns)\n+    res_tracers = map(self.instantiate_const, map(self.new_const, res))\n+    env_tracers = map(self.to_jaxpr_tracer, env)\n+    out_tracers = [JaxprTracer(self, PartialVal.unknown(a), None)\n+                   for a in out_avals]\n+    closed_jaxpr = close_jaxpr(convert_constvars_jaxpr(jaxpr))\n+\n+    @partial(lu.wrap_init, debug_info=fwd.debug_info)\n+    @_memoize\n+    def fwd_jaxpr_thunk(*zeros):\n+      fwd_ = _interleave_fun(fwd, zeros)\n+      fwd_jaxpr, _, consts, () = trace_to_jaxpr_dynamic(fwd_, in_avals)\n+      return fwd_jaxpr, consts\n \n     name_stack = self._current_truncated_name_stack()\n     source = source_info_util.current().replace(name_stack=name_stack)\n+    params = dict(\n+        call_jaxpr=closed_jaxpr,\n+        fwd_jaxpr_thunk=fwd_jaxpr_thunk,\n+        num_consts=len(res) + len(env),\n+        bwd=bwd,\n+        out_trees=out_trees,\n+        symbolic_zeros=symbolic_zeros\n+    )\n     eqn = new_eqn_recipe((*res_tracers, *env_tracers, *tracers),\n-                         out_tracers, prim.initial_style,\n-                         dict(fun_jaxpr=closed_jaxpr,\n-                              fwd_jaxpr_thunk=fwd_jaxpr_thunk,\n-                              num_consts=len(res) + len(env),\n-                              bwd=bwd, out_trees=out_trees,\n-                              symbolic_zeros=symbolic_zeros),\n-                         jaxpr.effects, source)\n+                         out_tracers, prim, params, jaxpr.effects, source)\n     for t in out_tracers: t.recipe = eqn\n-    return merge_lists(out_knowns, out_tracers, out_consts)\n+    return out_tracers\n \n def partition_pvals(\n     pvals: list[PartialVal]\n@@ -2050,6 +2046,7 @@ def process_custom_jvp_call(self, prim, fun: lu.WrappedFun,\n     fun_jaxpr, out_avals, consts, () = trace_to_jaxpr_dynamic(fun, in_avals)\n     closed_fun_jaxpr = core.ClosedJaxpr(convert_constvars_jaxpr(fun_jaxpr), ())\n \n+    @partial(lu.wrap_init, debug_info=jvp.debug_info)\n     @_memoize\n     def jvp_jaxpr_thunk(*in_zeros):\n       for store in jvp.stores: store and store.reset()\n@@ -2065,8 +2062,7 @@ def jvp_jaxpr_thunk(*in_zeros):\n     outvars = map(self.makevar, out_tracers)\n     eqn = new_jaxpr_eqn([*constvars, *invars], outvars, prim,\n                         dict(call_jaxpr=closed_fun_jaxpr,\n-                             jvp_jaxpr_fun=lu.wrap_init(jvp_jaxpr_thunk,\n-                                                        debug_info=jvp.debug_info),\n+                             jvp_jaxpr_fun=jvp_jaxpr_thunk,\n                              num_consts=len(consts),\n                              symbolic_zeros=symbolic_zeros),\n                         fun_jaxpr.effects,\n@@ -2086,6 +2082,7 @@ def process_custom_vjp_call(self, prim: core.Primitive,\n     fun_jaxpr, out_avals, consts, _ = trace_to_jaxpr_dynamic(fun, in_avals)\n     closed_fun_jaxpr = core.ClosedJaxpr(convert_constvars_jaxpr(fun_jaxpr), ())\n \n+    @partial(lu.wrap_init, debug_info=fwd.debug_info)\n     @_memoize\n     def fwd_jaxpr_from_zeros(*zeros):\n       for store in fwd.stores: store and store.reset()\n@@ -2098,9 +2095,8 @@ def fwd_jaxpr_from_zeros(*zeros):\n     invars = map(self.getvar, tracers)\n     constvars = map(self.getvar, map(to_jaxpr_tracer, consts))\n     outvars = map(self.makevar, out_tracers)\n-    eqn = new_jaxpr_eqn([*constvars, *invars], outvars,\n-                        prim.initial_style,  # pytype: disable=attribute-error\n-                        dict(fun_jaxpr=closed_fun_jaxpr,\n+    eqn = new_jaxpr_eqn([*constvars, *invars], outvars, prim,\n+                        dict(call_jaxpr=closed_fun_jaxpr,\n                              fwd_jaxpr_thunk=fwd_jaxpr_from_zeros,\n                              num_consts=len(consts),\n                              bwd=bwd, out_trees=out_trees,\ndiff --git a/jax/_src/pallas/cost_estimate.py b/jax/_src/pallas/cost_estimate.py\nindex 3b82d3095f64..93bcf5348b24 100644\n--- a/jax/_src/pallas/cost_estimate.py\n+++ b/jax/_src/pallas/cost_estimate.py\n@@ -238,15 +238,15 @@ def _pjit_cost_rule(ctx, *, jaxpr: jax_core.ClosedJaxpr, **_):\n   )\n register_cost_rule(pjit.pjit_p, _pjit_cost_rule)\n \n-def _custom_vjp_rule(ctx, *, fun_jaxpr: jax_core.ClosedJaxpr, **_):\n+def _custom_vjp_rule(ctx, *, call_jaxpr: jax_core.ClosedJaxpr, **_):\n   del ctx\n-  inner_cost = cost_estimate_jaxpr(fun_jaxpr)\n+  inner_cost = cost_estimate_jaxpr(call_jaxpr)\n   return CostEstimate(\n       flops=inner_cost.flops,\n       transcendentals=inner_cost.transcendentals,\n       bytes_accessed=inner_cost.bytes_accessed,\n   )\n-register_cost_rule(custom_derivatives.custom_vjp_call_jaxpr_p, _custom_vjp_rule)\n+register_cost_rule(custom_derivatives.custom_vjp_call_p, _custom_vjp_rule)\n \n def _run_state_rule(*_, jaxpr: jax_core.Jaxpr, **_2):\n   inner_cost = cost_estimate_jaxpr(pe.close_jaxpr(jaxpr))\ndiff --git a/jax/custom_derivatives.py b/jax/custom_derivatives.py\nindex 3628ae4aaa6e..b768b687dfad 100644\n--- a/jax/custom_derivatives.py\n+++ b/jax/custom_derivatives.py\n@@ -26,7 +26,6 @@\n   custom_jvp_call_jaxpr_p as custom_jvp_call_jaxpr_p,\n   custom_vjp as custom_vjp,\n   custom_vjp_call_p as custom_vjp_call_p,\n-  custom_vjp_call_jaxpr_p as custom_vjp_call_jaxpr_p,\n   custom_vjp_primal_tree_values as custom_vjp_primal_tree_values,\n   CustomVJPPrimal as CustomVJPPrimal,\n   linear_call as linear_call,\ndiff --git a/jax/experimental/jax2tf/jax2tf.py b/jax/experimental/jax2tf/jax2tf.py\nindex 786e021e2ff0..536bf1f201f0 100644\n--- a/jax/experimental/jax2tf/jax2tf.py\n+++ b/jax/experimental/jax2tf/jax2tf.py\n@@ -3461,14 +3461,14 @@ def _custom_jvp_call(*args: TfVal, call_jaxpr: core.ClosedJaxpr,\n tf_impl[custom_derivatives.custom_jvp_call_p] = _custom_jvp_call\n \n \n-def _custom_vjp_call_jaxpr(*args: TfVal, fun_jaxpr: core.ClosedJaxpr,\n-                           **_) -> Sequence[TfVal]:\n+def _custom_vjp_call(*args: TfVal, call_jaxpr: core.ClosedJaxpr,\n+                     **_) -> Sequence[TfVal]:\n   # TODO(necula): ensure that there is no AD transformation in scope\n-  return _interpret_jaxpr(fun_jaxpr, *args, extra_name_stack=\"custom_vjp\",\n+  return _interpret_jaxpr(call_jaxpr, *args, extra_name_stack=\"custom_vjp\",\n                           fresh_constant_cache=False)\n \n \n-tf_impl[custom_derivatives.custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr\n+tf_impl[custom_derivatives.custom_vjp_call_p] = _custom_vjp_call\n \n \n def _custom_lin(*args: TfVal, **_) -> Sequence[TfVal]:\ndiff --git a/jax/extend/core/primitives.py b/jax/extend/core/primitives.py\nindex 60d8cd24a949..30350dace637 100644\n--- a/jax/extend/core/primitives.py\n+++ b/jax/extend/core/primitives.py\n@@ -26,7 +26,6 @@\n   custom_jvp_call_p as custom_jvp_call_p,\n   custom_jvp_call_jaxpr_p as custom_jvp_call_jaxpr_p,\n   custom_vjp_call_p as custom_vjp_call_p,\n-  custom_vjp_call_jaxpr_p as custom_vjp_call_jaxpr_p,\n )\n \n from jax._src.dispatch import device_put_p as device_put_p\ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 72c14634a9c8..73dc2fbefcaa 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3065,7 +3065,7 @@ def check_jaxpr(jaxpr, used_outs, includes, excludes):\n       if not dce_jaxpr.eqns:\n         assert not includes\n         return\n-      call_jaxpr = dce_jaxpr.eqns[0].params[\"fun_jaxpr\"]\n+      call_jaxpr = dce_jaxpr.eqns[0].params[\"call_jaxpr\"]\n       for prim in includes:\n         assert any(eqn.primitive == prim for eqn in call_jaxpr.eqns)\n       for prim in excludes:\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add layout inference and lowering for `scf.WhileOp` and enable tests.\n\nPiperOrigin-RevId: 758205399",
        "output": "```diff\nCommit: 66476a0b4c20753d79b0960bc476167589691e67\nDate: 2025-05-13T14:12:52Z\nURL: https://github.com/jax-ml/jax/commit/66476a0b4c20753d79b0960bc476167589691e67\nFiles changed: 4\nAdditions: +243, Deletions: -62\ndiff --git a/jax/experimental/mosaic/gpu/dialect_lowering.py b/jax/experimental/mosaic/gpu/dialect_lowering.py\nindex c1506bde32ea..320ae32607e9 100644\n--- a/jax/experimental/mosaic/gpu/dialect_lowering.py\n+++ b/jax/experimental/mosaic/gpu/dialect_lowering.py\n@@ -1123,6 +1123,54 @@ def _unflatten_ir_values(\n   return result\n \n \n+def _move_scf_block_to_block_with_flattened_arguments(\n+    ctx: LoweringContext,\n+    old_block: ir.Block,\n+    new_block: ir.Block,\n+    last_op_type: type[ir.OpView],\n+    args_template: Sequence[_VectorTemplate | None],\n+    *new_leading_args: Sequence[ir.Value],\n+) -> Sequence[_VectorTemplate | None]:\n+  \"\"\"Moves the operations from `old_block` to `new_block`.\n+\n+  The input arguments to the block, if any, are flattened using the provided\n+  `args_template`, except for any new_leading_args which are simply prepended\n+  to the flattened arguments and must be part of the template.\n+\n+  The last operation of the old block must be of type `last_op_type` which\n+  is expected to be either a `scf.YieldOp` or a `scf.ConditionOp`. This\n+  operation is recreated with flattened output arguments.\n+  \"\"\"\n+  out_template = None\n+  with ir.InsertionPoint(new_block):\n+    new_carry = _unflatten_ir_values(new_block.arguments[len(new_leading_args):], args_template)\n+    new_args = new_leading_args + tuple(new_carry)\n+    for old_arg, new_arg in zip(old_block.arguments, new_args, strict=True):\n+      old_arg.replace_all_uses_with(new_arg)\n+    for op in [*old_block]:\n+      if not isinstance(op, last_op_type):\n+        mgpu.private_operation_remove_from_parent(op)\n+        mgpu.private_block_append_owned_operation(new_block, op)\n+        ctx.lower_op(op)\n+      else:\n+        assert out_template is None\n+        layouts = (\n+            inference_utils.in_layouts(op)\n+            if inference_utils.has_in_layouts_set(op)\n+            else []\n+        )\n+        if isinstance(op, scf.YieldOp):\n+          flat_operands, out_template = _flatten_ir_values(op.operands, layouts)\n+          scf.yield_(flat_operands)\n+        elif isinstance(op, scf.ConditionOp):\n+          flat_carry, out_template = _flatten_ir_values(op.args, layouts)\n+          scf.condition(op.condition, flat_carry)\n+        else:\n+          raise NotImplementedError(f\"Unsupported op type: {op}\")\n+        op.erase()\n+  assert out_template is not None\n+  return out_template\n+\n @_register_lowering(scf.ForOp)\n def _for_op_lowering_rule(\n     ctx: LoweringContext, for_op: scf.ForOp\n@@ -1145,33 +1193,78 @@ def _for_op_lowering_rule(\n       for_op.step,\n       flat_init_args,\n   )\n-  with ir.InsertionPoint(new_for_op.body):\n-    recreated_carry = _unflatten_ir_values(\n-        new_for_op.body.arguments[1:], args_template\n-    )\n-    ops_to_lower = []\n-    for op in [*for_op.body]:\n-      if op == yield_op:\n-        continue\n-      mgpu.private_operation_remove_from_parent(op)\n-      mgpu.private_block_append_owned_operation(new_for_op.body, op)\n-      ops_to_lower.append(op)\n-    new_args = (new_for_op.induction_variable, *recreated_carry)\n-    for old_carry, new_carry in zip(for_op.body.arguments, new_args, strict=True):\n-      old_carry.replace_all_uses_with(new_carry)\n-\n-  for op in ops_to_lower:\n-    with ir.InsertionPoint(op):\n-      ctx.lower_op(op)\n \n-  with ir.InsertionPoint(new_for_op.body):\n-    flat_operands, _ = _flatten_ir_values(yield_op.operands, in_layouts)\n-    yield_op.erase()\n-    scf.yield_(flat_operands)\n+  _move_scf_block_to_block_with_flattened_arguments(\n+      ctx,\n+      for_op.body,\n+      new_for_op.body,\n+      scf.YieldOp,\n+      args_template,\n+      new_for_op.induction_variable,\n+  )\n \n   return _unflatten_ir_values(new_for_op.results, args_template)\n \n \n+@_register_lowering(scf.WhileOp)\n+def _while_op_lowering_rule(\n+    ctx: LoweringContext, while_op: scf.WhileOp\n+) -> MlirLoweringRuleResult:\n+  if not inference_utils.should_have_layout(while_op):\n+    return _traverse_op_lowering_rule(ctx, while_op)\n+\n+  before_block = while_op.before.blocks[0]\n+  after_block = while_op.after.blocks[0]\n+  condition_op = before_block.operations[len(before_block.operations) - 1]\n+  yield_op = after_block.operations[len(after_block.operations) - 1]\n+\n+  in_layouts = inference_utils.in_layouts(while_op)\n+  out_layouts = inference_utils.out_layouts(while_op)\n+\n+  if in_layouts:\n+    yield_layouts = inference_utils.in_layouts(yield_op)\n+    if in_layouts != yield_layouts:\n+      raise ValueError(\n+          f\"Input layouts {in_layouts} do not match yield layouts\"\n+          f\" {yield_layouts}\"\n+      )\n+\n+  if out_layouts:\n+    condition_layouts = inference_utils.in_layouts(condition_op)\n+    if out_layouts != condition_layouts:\n+      raise ValueError(\n+          f\"Output layouts {out_layouts} do not match condition layouts\"\n+          f\" {condition_layouts}\"\n+      )\n+\n+  flat_inits, inits_template = _flatten_ir_values(while_op.inits, in_layouts)\n+  result_types = _infer_flat_result_types(while_op, out_layouts)\n+  new_while_op = scf.WhileOp(result_types, flat_inits)\n+\n+  # Before block\n+  init_types = [v.type for v in flat_inits]\n+  new_before_block = new_while_op.before.blocks.append(*init_types)\n+  results_template = _move_scf_block_to_block_with_flattened_arguments(\n+      ctx,\n+      before_block,\n+      new_before_block,\n+      scf.ConditionOp,\n+      inits_template,\n+  )\n+\n+  # After block\n+  new_after_block = new_while_op.after.blocks.append(*result_types)\n+  _move_scf_block_to_block_with_flattened_arguments(\n+      ctx,\n+      after_block,\n+      new_after_block,\n+      scf.YieldOp,\n+      results_template,\n+  )\n+\n+  return _unflatten_ir_values(new_while_op.results, results_template)\n+\n+\n def _infer_flat_result_types(\n     op: ir.OpView, out_layouts: Sequence[ir.Attribute]\n ) -> Sequence[ir.Type]:\n@@ -1221,19 +1314,9 @@ def _index_switch_op_lowering_rule(\n   ):\n     [block] = region.blocks\n     new_block = new_region.blocks.append()\n-    with ir.InsertionPoint(new_block):\n-      for op in [*block]:\n-        if not isinstance(op, scf.YieldOp):\n-          mgpu.private_operation_remove_from_parent(op)\n-          mgpu.private_block_append_owned_operation(new_block, op)\n-          ctx.lower_op(op)\n-          continue\n-        if inference_utils.in_layouts(op) != out_layouts:\n-          raise ValueError(\"Layout mismatch\")\n-        flat_results, results_template = _flatten_ir_values(\n-            op.operands, out_layouts\n-        )\n-        scf.yield_(flat_results)\n+    results_template = _move_scf_block_to_block_with_flattened_arguments(\n+        ctx, block, new_block, scf.YieldOp, []\n+    )\n   return _unflatten_ir_values(new_switch_op.results, results_template)\n \n \ndiff --git a/jax/experimental/mosaic/gpu/layout_inference.py b/jax/experimental/mosaic/gpu/layout_inference.py\nindex b39dc933ce9d..c010bf181bce 100644\n--- a/jax/experimental/mosaic/gpu/layout_inference.py\n+++ b/jax/experimental/mosaic/gpu/layout_inference.py\n@@ -336,38 +336,61 @@ def _infer_constant_op_layout(constant_op: arith.ConstantOp) -> OptionalLayouts:\n   return [], [layout]\n \n \n-@partial(_add_layout_inference_rule, scf.YieldOp)\n-def _infer_yield_op_layout(op: scf.YieldOp) -> OptionalLayouts:\n+def _layouts_from_values(values: Sequence[ir.Value]) -> list[ir.Attribute] | None:\n   layouts = []\n-  for result in op.results_:\n-    if not ir.VectorType.isinstance(result.type):\n+  for value in values:\n+    if not ir.VectorType.isinstance(value.type):\n       continue\n-    if (layout := inference_utils.value_layout(result)) is not None:\n+    if (layout := inference_utils.value_layout(value)) is not None:\n       if layouts_lib.is_splat_fragmented_layout(layout):\n         return None\n       layouts.append(layout)\n     else:\n       # Not all layouts could be inferred for vector ops. Return for now.\n       return None\n+  return layouts\n \n+@partial(_add_layout_inference_rule, scf.YieldOp)\n+def _infer_yield_op_layout(op: scf.YieldOp) -> OptionalLayouts:\n+  layouts = _layouts_from_values(op.results_)\n+  if layouts is None:\n+    return None\n   return (layouts, [])\n \n \n+@partial(_add_layout_inference_rule, scf.ConditionOp)\n+def _infer_condition_op_layout(op: scf.ConditionOp) -> OptionalLayouts:\n+  layouts = _layouts_from_values(op.args)\n+  if layouts is None:\n+    return None\n+  return (layouts, [])\n+\n+\n+def _last_op(region: ir.Region, expected_op_type: type[ir.OpView]):\n+  [block] = region.blocks\n+  last_op = block.operations[len(block.operations) - 1]\n+  assert isinstance(last_op, expected_op_type)\n+  return last_op\n+\n+\n+def _infer_from_op(op: ir.OpView) -> list[ir.Attribute] | None:\n+  if not inference_utils.has_in_layouts_set(op):\n+    return None\n+  in_layouts = list(inference_utils.in_layouts(op))\n+  if any(\n+      layouts_lib.is_splat_fragmented_layout(layout)\n+      for layout in in_layouts\n+  ):\n+    return None\n+  return in_layouts\n+\n+\n def _infer_from_yield_ops(op: ir.Operation) -> list[ir.Attribute] | None:\n   candidates = []\n   for region in op.regions:\n-    [block] = region.blocks\n-    yield_op = block.operations[len(block.operations) - 1]\n-    assert isinstance(yield_op, scf.YieldOp)\n-    if not inference_utils.has_in_layouts_set(yield_op):\n-      continue\n-    yield_layouts = inference_utils.in_layouts(yield_op)\n-    if any(\n-        layouts_lib.is_splat_fragmented_layout(layout)\n-        for layout in yield_layouts\n-    ):\n-      continue\n-    candidates.append(yield_layouts)\n+    yield_layouts = _infer_from_op(_last_op(region, scf.YieldOp))\n+    if yield_layouts is not None:\n+      candidates.append(yield_layouts)\n   if not candidates:\n     return None\n   return [_choose_representative_layout(set(c)) for c in zip(*candidates)]\n@@ -382,6 +405,27 @@ def _infer_for_op_layout(op: scf.ForOp) -> OptionalLayouts:\n   return None\n \n \n+@partial(_add_layout_inference_rule, scf.WhileOp)\n+def _infer_while_op_layout(op: scf.WhileOp) -> OptionalLayouts:\n+  # TODO(dasenov): we don't attempt to propagate from outside for the moment.\n+\n+  # Note that the inputs or results do not necessarily contain vector types. If\n+  # there is no vector type, the corresponding layouts (in_layouts or\n+  # out_layouts) should be an empty list.\n+\n+  yield_op = _last_op(op.after, scf.YieldOp)\n+  needs_in_layouts = inference_utils.should_have_layout(yield_op)\n+  in_layouts = _infer_from_op(yield_op) if needs_in_layouts else []\n+\n+  condition_op = _last_op(op.before, scf.ConditionOp)\n+  needs_out_layouts = inference_utils.should_have_layout(condition_op)\n+  out_layouts = _infer_from_op(condition_op) if needs_out_layouts else []\n+\n+  if in_layouts is None or out_layouts is None:\n+    return None\n+  return in_layouts, out_layouts\n+\n+\n @partial(_add_layout_inference_rule, scf.IfOp)\n def _infer_if_op_layout(op: scf.IfOp) -> OptionalLayouts:\n   if layouts := _infer_from_yield_ops(op):\ndiff --git a/tests/mosaic/gpu_layout_inference_test.py b/tests/mosaic/gpu_layout_inference_test.py\nindex 315ae2659ab6..038766542f3b 100644\n--- a/tests/mosaic/gpu_layout_inference_test.py\n+++ b/tests/mosaic/gpu_layout_inference_test.py\n@@ -429,6 +429,59 @@ def body(lower_bound, upper_bound, step, a, b, c):\n     self.assertSequenceEqual(for_op.attributes[\"in_layouts\"], [wgmma_layout])\n     self.assertSequenceEqual(for_op.attributes[\"out_layouts\"], [wgmma_layout])\n \n+  @parameterized.parameters(\n+      ((), None, (), None),\n+      ((64, 32), mgpu.WGMMA_LAYOUT, (), None),\n+      ((), None, (64, 32), mgpu.WGMMA_LAYOUT),\n+      ((64,), mgpu.WGMMA_ROW_LAYOUT, (64, 32), mgpu.WGMMA_LAYOUT),\n+  )\n+  def test_infer_while_op_layouts(\n+      self, init_shape, init_layout, result_shape, result_layout\n+  ):\n+    if init_shape:\n+      in_type = ir.VectorType.get(init_shape, ir.F32Type.get())\n+    else:\n+      in_type = ir.F32Type.get()\n+\n+    if result_shape:\n+      out_type = ir.VectorType.get(result_shape, ir.F32Type.get())\n+    else:\n+      out_type = ir.F32Type.get()\n+\n+    while_op = condition_op = yield_op = None\n+\n+    def body(condition, init, result):\n+      nonlocal while_op, condition_op, yield_op\n+      while_op = scf.WhileOp([out_type], [init])\n+      before_block = while_op.before.blocks.append(init.type)\n+      with ir.InsertionPoint(before_block):\n+        condition_op = scf.ConditionOp(condition, [result])\n+\n+      after_block = while_op.after.blocks.append(out_type)\n+      with ir.InsertionPoint(after_block):\n+        yield_op = scf.YieldOp([init])\n+\n+    with ir.InsertionPoint(self.module.body):\n+      i1 = ir.IntegerType.get_signless(1)\n+      func.FuncOp.from_py_func(i1, in_type, out_type)(body)\n+\n+    [f] = self.module.body.operations\n+    f_layouts = []\n+    if init_layout:\n+      f_layouts.append(layouts.to_layout_attr(init_layout))\n+    if result_layout:\n+      f_layouts.append(layouts.to_layout_attr(result_layout))\n+    if f_layouts:\n+      f.attributes[\"in_layouts\"] = ir.ArrayAttr.get(f_layouts)\n+\n+    mgpu.infer_layout(self.module)\n+\n+    if init_layout or result_layout:\n+      init_layouts = [layouts.to_layout_attr(init_layout)] if init_layout else []\n+      result_layouts = [layouts.to_layout_attr(result_layout)] if result_layout else []\n+      self.assertSequenceEqual(while_op.attributes[\"in_layouts\"], init_layouts)\n+      self.assertSequenceEqual(while_op.attributes[\"out_layouts\"], result_layouts)\n+\n   def test_infer_layout_has_no_layout_for_non_vector_types(self):\n     shape = (32, 4)\n     elt_ty = ir.BF16Type.get()\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 873854266782..9d259097f90a 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1218,8 +1218,6 @@ def body(idx, _):\n     np.testing.assert_array_equal(kernel(x, y), x + y)\n \n   def test_while_loop(self):\n-    self.skip_if_wg_semantics()\n-\n     @functools.partial(\n         self.pallas_call, out_shape=jax.ShapeDtypeStruct([128], jnp.int32)\n     )\n@@ -1242,8 +1240,6 @@ def body(acc):\n     )\n \n   def test_while_loop_layout_mismatch(self):\n-    self.skip_if_wg_semantics()  # while and conditional are not yet supported.\n-\n     @functools.partial(\n         self.pallas_call, out_shape=jax.ShapeDtypeStruct([128], jnp.int32)\n     )\n@@ -1261,8 +1257,17 @@ def body(acc):\n \n       _ = jax.lax.while_loop(cond, body, o_ref[...])\n \n-    with self.assertRaisesRegex(ValueError, \"has layout .*, when it should be\"):\n-      kernel()\n+    if self.LOWERING_SEMANTICS == plgpu.LoweringSemantics.Warpgroup:\n+      with self.assertRaisesRegex(\n+          NotImplementedError,\n+          \"Cannot convert from WGStridedFragLayout.* to TiledLayout\",\n+      ):\n+        kernel()\n+    else:\n+      with self.assertRaisesRegex(\n+          ValueError, \"has layout .*, when it should be\"\n+      ):\n+        kernel()\n \n   def test_cond(self):\n     @functools.partial(\n@@ -1722,10 +1727,6 @@ class PallasCallSm90ATest(PallasSm90ATest):\n \n   @parameterized.parameters(False, True)\n   def test_fori_loop_accumulator(self, force_while):\n-    if force_while:\n-      # Layout inference and lowering for 'while' are not yet implemented for\n-      # warpgroup semantics.\n-      self.skip_if_wg_semantics()\n     if self.LOWERING_SEMANTICS == plgpu.LoweringSemantics.Lane:\n       transforms = (plgpu.TilingTransform((8, 64)), plgpu.SwizzleTransform(128))\n     else:\n\n```",
        "from_id": [
            "dimitar-asenov",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Nuked a debug test added by accident\n\nPiperOrigin-RevId: 758144215",
        "output": "```diff\nCommit: 0d6ad8aee0fe786109e2e37b2b0e92eed712c950\nDate: 2025-05-13T10:39:06Z\nURL: https://github.com/jax-ml/jax/commit/0d6ad8aee0fe786109e2e37b2b0e92eed712c950\nFiles changed: 1\nAdditions: +0, Deletions: -14\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex a5719b01f4ad..873854266782 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -3321,20 +3321,6 @@ def do_wgmma(acc_ref):\n \n     np.testing.assert_allclose(kernel(x, x), x @ x)\n \n-  def test_debug_bug(self):\n-    dtype = jnp.float16\n-    @functools.partial(\n-        self.pallas_call,\n-        out_shape=jax.ShapeDtypeStruct([256], dtype),\n-    )\n-    def kernel(o_ref):\n-      kv_step = jnp.asarray(0)\n-      @pl.when(kv_step < -2)\n-      def dp():\n-        pl.debug_print(\"foo\")\n-      o_ref[...] = jnp.zeros_like(o_ref)\n-    kernel()\n-\n   # TODO(apaszke): Clusters and multicast\n \n \n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update tridiagonal solve kernels on GPU to properly use the FFI.\n\nThis fixes https://github.com/jax-ml/jax/issues/28544 by using the batched algorithms directly when possible. It also adds complex dtype and batch partitioning support to tridiagonal solves on GPU.\n\nPiperOrigin-RevId: 758129745",
        "output": "```diff\nCommit: 91de2e39c161e3f26e8d58ab7071a189903563f8\nDate: 2025-05-13T09:46:51Z\nURL: https://github.com/jax-ml/jax/commit/91de2e39c161e3f26e8d58ab7071a189903563f8\nFiles changed: 11\nAdditions: +270, Deletions: -37\ndiff --git a/jax/_src/lax/linalg.py b/jax/_src/lax/linalg.py\nindex dd86c22432d8..857b115b06d8 100644\n--- a/jax/_src/lax/linalg.py\n+++ b/jax/_src/lax/linalg.py\n@@ -48,7 +48,7 @@\n from jax._src.lib import gpu_solver\n from jax._src.lib import gpu_sparse\n from jax._src.lib import lapack\n-from jax._src.lib import version as jaxlib_version\n+from jax._src.lib import version as jaxlib_version, jaxlib_extension_version\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import chlo\n from jax._src.lib.mlir.dialects import hlo\n@@ -2530,29 +2530,33 @@ def _tridiagonal_solve_shape_rule(dl_shape, d_shape, du_shape, b_shape, **_):\n   return b_shape\n \n def _tridiagonal_solve_gpu_lowering(ctx, dl, d, du, b, *, target_name_prefix):\n-  _, _, _, b_aval = ctx.avals_in\n-  *batch_dims, m, n = b_aval.shape\n-  batch_size = math.prod(batch_dims)\n-\n-  mod = gpu_sparse._cusparse if target_name_prefix == \"cu\" else gpu_sparse._hipsparse\n-  assert mod is not None\n-  opaque = mod.build_gtsv2_descriptor(batch_size, m, n, m)\n-  if b_aval.dtype == np.float32:\n-    buffer_size = mod.gtsv2_f32_buffer_size(m, n, m)\n-    target_name = \"sparse_gtsv2_f32_ffi\"\n-  elif b_aval.dtype == np.float64:\n-    buffer_size = mod.gtsv2_f64_buffer_size(m, n, m)\n-    target_name = \"sparse_gtsv2_f64_ffi\"\n-  else:\n-    raise NotImplementedError(\n-        \"tridiagonal_solve is only implemented for float32 and float64 on GPU.\")\n-\n-  buffer_aval = core.ShapedArray(shape=(buffer_size,), dtype=np.int8)\n-  sub_ctx = ctx.replace(avals_out=[*ctx.avals_out, buffer_aval])\n-  rule = _linalg_ffi_lowering(\n-      f\"{target_name_prefix}{target_name}\", operand_output_aliases={3: 0},\n-      batch_partitionable=False)\n-  return rule(sub_ctx, dl, d, du, b, opaque=opaque)[:1]\n+  if jaxlib_extension_version < 340:\n+    _, _, _, b_aval = ctx.avals_in\n+    *batch_dims, m, n = b_aval.shape\n+    batch_size = math.prod(batch_dims)\n+    mod = gpu_sparse._cusparse if target_name_prefix == \"cu\" else gpu_sparse._hipsparse\n+    assert mod is not None\n+    opaque = mod.build_gtsv2_descriptor(batch_size, m, n, m)\n+    if b_aval.dtype == np.float32:\n+      buffer_size = mod.gtsv2_f32_buffer_size(m, n, m)\n+      target_name = \"sparse_gtsv2_f32_ffi\"\n+    elif b_aval.dtype == np.float64:\n+      buffer_size = mod.gtsv2_f64_buffer_size(m, n, m)\n+      target_name = \"sparse_gtsv2_f64_ffi\"\n+    else:\n+      raise NotImplementedError(\n+          \"tridiagonal_solve is only implemented for float32 and float64 on GPU.\")\n+\n+    buffer_aval = core.ShapedArray(shape=(buffer_size,), dtype=np.int8)\n+    sub_ctx = ctx.replace(avals_out=[*ctx.avals_out, buffer_aval])\n+    rule = _linalg_ffi_lowering(\n+        f\"{target_name_prefix}{target_name}\", operand_output_aliases={3: 0},\n+        batch_partitionable=False)\n+    return rule(sub_ctx, dl, d, du, b, opaque=opaque)[:1]\n+\n+  target_name = f\"{target_name_prefix}sparse_gtsv2_ffi\"\n+  rule = _linalg_ffi_lowering(target_name, operand_output_aliases={3: 0})\n+  return rule(ctx, dl, d, du, b)\n \n def _tridiagonal_solve_cpu_lowering(ctx, dl, d, du, b, **kwargs):\n   del kwargs  # unused\ndiff --git a/jaxlib/cuda/BUILD b/jaxlib/cuda/BUILD\nindex 2cc1476b637e..eabb3157ecca 100644\n--- a/jaxlib/cuda/BUILD\n+++ b/jaxlib/cuda/BUILD\n@@ -259,11 +259,13 @@ cc_library(\n         \":cuda_gpu_kernel_helpers\",\n         \":cuda_vendor\",\n         \":ffi_wrapper\",\n+        \"//jaxlib:ffi_helpers\",\n         \"//jaxlib:kernel_helpers\",\n         \"//jaxlib/gpu:handle_pool\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@xla//xla/ffi/api:ffi\",\ndiff --git a/jaxlib/gpu/sparse.cc b/jaxlib/gpu/sparse.cc\nindex 592c0f454a55..0190ba776de5 100644\n--- a/jaxlib/gpu/sparse.cc\n+++ b/jaxlib/gpu/sparse.cc\n@@ -614,6 +614,8 @@ nb::dict Registrations() {\n       EncapsulateFfiHandler(gtsv2_f32_ffi);\n   dict[JAX_GPU_PREFIX \"sparse_gtsv2_f64_ffi\"] =\n       EncapsulateFfiHandler(gtsv2_f64_ffi);\n+  dict[JAX_GPU_PREFIX \"sparse_gtsv2_ffi\"] = EncapsulateFfiHandler(kGtsv2);\n+\n   // TODO(tomhennigan): Add support for gtsv2 complex 32/64.\n   return dict;\n }\ndiff --git a/jaxlib/gpu/sparse_kernels.cc b/jaxlib/gpu/sparse_kernels.cc\nindex a9c08317e066..363321e3ca8b 100644\n--- a/jaxlib/gpu/sparse_kernels.cc\n+++ b/jaxlib/gpu/sparse_kernels.cc\n@@ -16,20 +16,29 @@ limitations under the License.\n #include \"jaxlib/gpu/sparse_kernels.h\"\n \n #include <cstddef>\n+#include <cstdint>\n #include <cstring>\n #include <string>\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"absl/synchronization/mutex.h\"\n+#include \"jaxlib/ffi_helpers.h\"\n #include \"jaxlib/gpu/ffi_wrapper.h\"\n #include \"jaxlib/gpu/gpu_kernel_helpers.h\"\n #include \"jaxlib/gpu/handle_pool.h\"\n #include \"jaxlib/gpu/vendor.h\"\n #include \"jaxlib/kernel_helpers.h\"\n+#include \"xla/ffi/api/ffi.h\"\n #include \"xla/service/custom_call_status.h\"\n \n+#define JAX_FFI_RETURN_IF_GPU_ERROR(...) \\\n+  FFI_RETURN_IF_ERROR_STATUS(JAX_AS_STATUS(__VA_ARGS__))\n+\n+namespace ffi = ::xla::ffi;\n+\n namespace jax {\n \n template <>\n@@ -641,5 +650,162 @@ void gtsv2_f64(gpuStream_t stream, void** buffers, const char* opaque,\n   }\n }\n \n+template <typename T, typename BufferSizeF, typename KernelF>\n+ffi::Error Gtsv2Impl(BufferSizeF getBufferSize, KernelF kernel, int64_t batch,\n+                     int64_t rows, int64_t cols, gpuStream_t stream,\n+                     ffi::ScratchAllocator& scratch, ffi::AnyBuffer dl,\n+                     ffi::AnyBuffer d, ffi::AnyBuffer du, ffi::AnyBuffer b,\n+                     ffi::Result<ffi::AnyBuffer> out) {\n+  FFI_ASSIGN_OR_RETURN(auto m, MaybeCastNoOverflow<int>(rows));\n+  FFI_ASSIGN_OR_RETURN(auto n, MaybeCastNoOverflow<int>(cols));\n+\n+  FFI_ASSIGN_OR_RETURN(auto handle, SparseHandlePool::Borrow(stream));\n+  size_t buffer_size_in_bytes;\n+  JAX_FFI_RETURN_IF_GPU_ERROR(getBufferSize(handle.get(), m, n, nullptr,\n+                                            nullptr, nullptr, nullptr, m,\n+                                            &buffer_size_in_bytes));\n+  auto maybe_workspace = scratch.Allocate(buffer_size_in_bytes);\n+  if (!maybe_workspace.has_value()) {\n+    return ffi::Error::Internal(\"Unable to allocate workspace for gtsv2\");\n+  }\n+  void* workspace = maybe_workspace.value();\n+\n+  auto dl_data = static_cast<T*>(dl.untyped_data());\n+  auto d_data = static_cast<T*>(d.untyped_data());\n+  auto du_data = static_cast<T*>(du.untyped_data());\n+  auto b_data = static_cast<T*>(b.untyped_data());\n+  auto out_data = static_cast<T*>(out->untyped_data());\n+  if (b_data != out_data) {\n+    JAX_FFI_RETURN_IF_GPU_ERROR(gpuMemcpyAsync(\n+        out_data, b_data, b.size_bytes(), gpuMemcpyDeviceToDevice, stream));\n+  }\n+\n+  for (int64_t i = 0; i < batch; ++i) {\n+    JAX_FFI_RETURN_IF_GPU_ERROR(kernel(handle.get(), m, n, dl_data, d_data,\n+                                       du_data, out_data, m, workspace));\n+    dl_data += m;\n+    d_data += m;\n+    du_data += m;\n+    out_data += m * n;\n+  }\n+  return ffi::Error::Success();\n+}\n+\n+template <typename T, typename BufferSizeF, typename KernelF>\n+ffi::Error Gtsv2BatchedImpl(BufferSizeF getBufferSize, KernelF kernel,\n+                            int64_t batch, int64_t rows, gpuStream_t stream,\n+                            ffi::ScratchAllocator& scratch, ffi::AnyBuffer dl,\n+                            ffi::AnyBuffer d, ffi::AnyBuffer du,\n+                            ffi::AnyBuffer b, ffi::Result<ffi::AnyBuffer> out) {\n+  FFI_ASSIGN_OR_RETURN(auto batch_count, MaybeCastNoOverflow<int>(batch));\n+  FFI_ASSIGN_OR_RETURN(auto m, MaybeCastNoOverflow<int>(rows));\n+\n+  FFI_ASSIGN_OR_RETURN(auto handle, SparseHandlePool::Borrow(stream));\n+  size_t buffer_size_in_bytes;\n+  JAX_FFI_RETURN_IF_GPU_ERROR(getBufferSize(handle.get(), m, nullptr, nullptr,\n+                                            nullptr, nullptr, batch_count, m,\n+                                            &buffer_size_in_bytes));\n+  auto maybe_workspace = scratch.Allocate(buffer_size_in_bytes);\n+  if (!maybe_workspace.has_value()) {\n+    return ffi::Error::Internal(\"Unable to allocate workspace for gtsv2\");\n+  }\n+  void* workspace = maybe_workspace.value();\n+\n+  auto dl_data = static_cast<T*>(dl.untyped_data());\n+  auto d_data = static_cast<T*>(d.untyped_data());\n+  auto du_data = static_cast<T*>(du.untyped_data());\n+  auto b_data = static_cast<T*>(b.untyped_data());\n+  auto out_data = static_cast<T*>(out->untyped_data());\n+  if (b_data != out_data) {\n+    JAX_FFI_RETURN_IF_GPU_ERROR(gpuMemcpyAsync(\n+        out_data, b_data, b.size_bytes(), gpuMemcpyDeviceToDevice, stream));\n+  }\n+\n+  JAX_FFI_RETURN_IF_GPU_ERROR(kernel(handle.get(), m, dl_data, d_data, du_data,\n+                                     out_data, batch_count, m, workspace));\n+  return ffi::Error::Success();\n+}\n+\n+ffi::Error Gtsv2(gpuStream_t stream, ffi::ScratchAllocator scratch,\n+                 ffi::AnyBuffer dl, ffi::AnyBuffer d, ffi::AnyBuffer du,\n+                 ffi::AnyBuffer b, ffi::Result<ffi::AnyBuffer> out) {\n+  auto dataType = dl.element_type();\n+  if (dataType != d.element_type() || dataType != du.element_type() ||\n+      dataType != b.element_type() || dataType != out->element_type()) {\n+    return ffi::Error::InvalidArgument(\n+        \"The inputs and outputs to gtsv2 must have the same element type\");\n+  }\n+  FFI_ASSIGN_OR_RETURN((auto [batch, rows, cols]),\n+                       SplitBatch2D(b.dimensions()));\n+  FFI_RETURN_IF_ERROR(\n+      CheckShape(out->dimensions(), {batch, rows, cols}, \"out\", \"gtsv2\"));\n+  FFI_RETURN_IF_ERROR(\n+      CheckShape(dl.dimensions(), {batch, rows}, \"dl\", \"gtsv2\"));\n+  FFI_RETURN_IF_ERROR(CheckShape(d.dimensions(), {batch, rows}, \"d\", \"gtsv2\"));\n+  FFI_RETURN_IF_ERROR(\n+      CheckShape(du.dimensions(), {batch, rows}, \"du\", \"gtsv2\"));\n+  if (batch > 1 && cols == 1) {\n+    switch (dataType) {\n+      case ffi::F32:\n+        return Gtsv2BatchedImpl<float>(\n+            gpusparseSgtsv2StridedBatch_bufferSizeExt,\n+            gpusparseSgtsv2StridedBatch, batch, rows, stream, scratch, dl, d,\n+            du, b, out);\n+      case ffi::F64:\n+        return Gtsv2BatchedImpl<double>(\n+            gpusparseDgtsv2StridedBatch_bufferSizeExt,\n+            gpusparseDgtsv2StridedBatch, batch, rows, stream, scratch, dl, d,\n+            du, b, out);\n+      case ffi::C64:\n+        return Gtsv2BatchedImpl<gpuComplex>(\n+            gpusparseCgtsv2StridedBatch_bufferSizeExt,\n+            gpusparseCgtsv2StridedBatch, batch, rows, stream, scratch, dl, d,\n+            du, b, out);\n+      case ffi::C128:\n+        return Gtsv2BatchedImpl<gpuDoubleComplex>(\n+            gpusparseZgtsv2StridedBatch_bufferSizeExt,\n+            gpusparseZgtsv2StridedBatch, batch, rows, stream, scratch, dl, d,\n+            du, b, out);\n+      default:\n+        break;\n+    }\n+\n+  } else {\n+    switch (dataType) {\n+      case ffi::F32:\n+        return Gtsv2Impl<float>(gpusparseSgtsv2_bufferSizeExt, gpusparseSgtsv2,\n+                                batch, rows, cols, stream, scratch, dl, d, du,\n+                                b, out);\n+      case ffi::F64:\n+        return Gtsv2Impl<double>(gpusparseDgtsv2_bufferSizeExt, gpusparseDgtsv2,\n+                                 batch, rows, cols, stream, scratch, dl, d, du,\n+                                 b, out);\n+      case ffi::C64:\n+        return Gtsv2Impl<gpuComplex>(gpusparseCgtsv2_bufferSizeExt,\n+                                     gpusparseCgtsv2, batch, rows, cols, stream,\n+                                     scratch, dl, d, du, b, out);\n+      case ffi::C128:\n+        return Gtsv2Impl<gpuDoubleComplex>(gpusparseZgtsv2_bufferSizeExt,\n+                                           gpusparseZgtsv2, batch, rows, cols,\n+                                           stream, scratch, dl, d, du, b, out);\n+      default:\n+        break;\n+    }\n+  }\n+  return ffi::Error::InvalidArgument(absl::StrFormat(\n+      \"Unsupported dtype %s in gtsv2\", absl::FormatStreamed(dataType)));\n+}\n+\n+XLA_FFI_DEFINE_HANDLER_SYMBOL(kGtsv2, Gtsv2,\n+                              ffi::Ffi::Bind()\n+                                  .Ctx<ffi::PlatformStream<gpuStream_t>>()\n+                                  .Ctx<ffi::ScratchAllocator>()\n+                                  .Arg<ffi::AnyBuffer>()  // dl\n+                                  .Arg<ffi::AnyBuffer>()  // d\n+                                  .Arg<ffi::AnyBuffer>()  // du\n+                                  .Arg<ffi::AnyBuffer>()  // b\n+                                  .Ret<ffi::AnyBuffer>()  // out\n+);\n+\n }  // namespace JAX_GPU_NAMESPACE\n }  // namespace jax\ndiff --git a/jaxlib/gpu/sparse_kernels.h b/jaxlib/gpu/sparse_kernels.h\nindex d735c320307c..3b365872f591 100644\n--- a/jaxlib/gpu/sparse_kernels.h\n+++ b/jaxlib/gpu/sparse_kernels.h\n@@ -157,6 +157,7 @@ XLA_FFI_DECLARE_HANDLER_SYMBOL(CooMatvecFfi);\n XLA_FFI_DECLARE_HANDLER_SYMBOL(CooMatmatFfi);\n XLA_FFI_DECLARE_HANDLER_SYMBOL(gtsv2_f32_ffi);\n XLA_FFI_DECLARE_HANDLER_SYMBOL(gtsv2_f64_ffi);\n+XLA_FFI_DECLARE_HANDLER_SYMBOL(kGtsv2);\n \n }  // namespace JAX_GPU_NAMESPACE\n }  // namespace jax\ndiff --git a/jaxlib/gpu/vendor.h b/jaxlib/gpu/vendor.h\nindex 5deb8d4c650a..b96552f81bd1 100644\n--- a/jaxlib/gpu/vendor.h\n+++ b/jaxlib/gpu/vendor.h\n@@ -152,7 +152,8 @@ typedef cusparseDnVecDescr_t gpusparseDnVecDescr_t;\n #define GPUDNN_STATUS_SUCCESS CUDNN_STATUS_SUCCESS\n #define GPUDNN_WGRAD_MODE_ADD CUDNN_WGRAD_MODE_ADD\n #define GPUDNN_RNN_ALGO_STANDARD CUDNN_RNN_ALGO_STANDARD\n-#define GPUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED\n+#define GPUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED \\\n+  CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED\n #define GPUDNN_RNN_PADDED_IO_ENABLED CUDNN_RNN_PADDED_IO_ENABLED\n #define GPUDNN_DEFAULT_MATH CUDNN_DEFAULT_MATH\n #define GPUDNN_FMA_MATH CUDNN_FMA_MATH\n@@ -289,10 +290,28 @@ typedef cusparseDnVecDescr_t gpusparseDnVecDescr_t;\n #define gpusparseSpMM_bufferSize cusparseSpMM_bufferSize\n #define gpusparseSpMV cusparseSpMV\n #define gpusparseSpMV_bufferSize cusparseSpMV_bufferSize\n+\n #define gpusparseSgtsv2 cusparseSgtsv2\n #define gpusparseDgtsv2 cusparseDgtsv2\n+#define gpusparseCgtsv2 cusparseCgtsv2\n+#define gpusparseZgtsv2 cusparseZgtsv2\n #define gpusparseSgtsv2_bufferSizeExt cusparseSgtsv2_bufferSizeExt\n #define gpusparseDgtsv2_bufferSizeExt cusparseDgtsv2_bufferSizeExt\n+#define gpusparseCgtsv2_bufferSizeExt cusparseCgtsv2_bufferSizeExt\n+#define gpusparseZgtsv2_bufferSizeExt cusparseZgtsv2_bufferSizeExt\n+\n+#define gpusparseSgtsv2StridedBatch_bufferSizeExt \\\n+  cusparseSgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseDgtsv2StridedBatch_bufferSizeExt \\\n+  cusparseDgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseCgtsv2StridedBatch_bufferSizeExt \\\n+  cusparseCgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseZgtsv2StridedBatch_bufferSizeExt \\\n+  cusparseZgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseSgtsv2StridedBatch cusparseSgtsv2StridedBatch\n+#define gpusparseDgtsv2StridedBatch cusparseDgtsv2StridedBatch\n+#define gpusparseCgtsv2StridedBatch cusparseCgtsv2StridedBatch\n+#define gpusparseZgtsv2StridedBatch cusparseZgtsv2StridedBatch\n \n #define GPUSPARSE_INDEX_16U CUSPARSE_INDEX_16U\n #define GPUSPARSE_INDEX_32I CUSPARSE_INDEX_32I\n@@ -636,10 +655,28 @@ typedef hipsparseDnVecDescr_t gpusparseDnVecDescr_t;\n #define gpusparseSpMM_bufferSize hipsparseSpMM_bufferSize\n #define gpusparseSpMV hipsparseSpMV\n #define gpusparseSpMV_bufferSize hipsparseSpMV_bufferSize\n+\n #define gpusparseSgtsv2 hipsparseSgtsv2\n #define gpusparseDgtsv2 hipsparseDgtsv2\n+#define gpusparseCgtsv2 hipsparseCgtsv2\n+#define gpusparseZgtsv2 hipsparseZgtsv2\n #define gpusparseSgtsv2_bufferSizeExt hipsparseSgtsv2_bufferSizeExt\n #define gpusparseDgtsv2_bufferSizeExt hipsparseDgtsv2_bufferSizeExt\n+#define gpusparseCgtsv2_bufferSizeExt hipsparseCgtsv2_bufferSizeExt\n+#define gpusparseZgtsv2_bufferSizeExt hipsparseZgtsv2_bufferSizeExt\n+\n+#define gpusparseSgtsv2StridedBatch_bufferSizeExt \\\n+  hipsparseSgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseDgtsv2StridedBatch_bufferSizeExt \\\n+  hipsparseDgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseCgtsv2StridedBatch_bufferSizeExt \\\n+  hipsparseCgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseZgtsv2StridedBatch_bufferSizeExt \\\n+  hipsparseZgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseSgtsv2StridedBatch hipsparseSgtsv2StridedBatch\n+#define gpusparseDgtsv2StridedBatch hipsparseDgtsv2StridedBatch\n+#define gpusparseCgtsv2StridedBatch hipsparseCgtsv2StridedBatch\n+#define gpusparseZgtsv2StridedBatch hipsparseZgtsv2StridedBatch\n \n #define GPUSPARSE_INDEX_16U HIPSPARSE_INDEX_16U\n #define GPUSPARSE_INDEX_32I HIPSPARSE_INDEX_32I\ndiff --git a/jaxlib/gpu_sparse.py b/jaxlib/gpu_sparse.py\nindex af03eb6e6a8a..bf1dc6f64ec1 100644\n--- a/jaxlib/gpu_sparse.py\n+++ b/jaxlib/gpu_sparse.py\n@@ -35,3 +35,12 @@ def registrations() -> dict[str, list[tuple[str, Any, int]]]:\n           for name, value in module.registrations().items()\n       )\n   return registrations  # pytype: disable=bad-return-type\n+\n+def batch_partitionable_targets() -> list[str]:\n+  targets: list[str] = []\n+  for module in [_cusparse, _hipsparse]:\n+    if module:\n+      targets.extend(\n+          name for name in module.registrations() if name.endswith(\"gtsv2_ffi\")\n+      )\n+  return targets\ndiff --git a/jaxlib/rocm/BUILD b/jaxlib/rocm/BUILD\nindex 75406174dd93..d0468d72d1b3 100644\n--- a/jaxlib/rocm/BUILD\n+++ b/jaxlib/rocm/BUILD\n@@ -244,11 +244,13 @@ cc_library(\n         \":ffi_wrapper\",\n         \":hip_gpu_kernel_helpers\",\n         \":hip_vendor\",\n+        \"//jaxlib:ffi_helpers\",\n         \"//jaxlib:kernel_helpers\",\n         \"//jaxlib/gpu:handle_pool\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@local_config_rocm//rocm:hipsparse\",\n         \"@local_config_rocm//rocm:rocm_headers\",\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 449dfa653286..6aaae11c139d 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 339\n+_version = 340\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\ndiff --git a/tests/linalg_sharding_test.py b/tests/linalg_sharding_test.py\nindex 2f190cdc5ad6..e68e94e16494 100644\n--- a/tests/linalg_sharding_test.py\n+++ b/tests/linalg_sharding_test.py\n@@ -22,6 +22,7 @@\n from jax import lax\n from jax._src import config\n from jax._src import test_util as jtu\n+from jax._src.lib import jaxlib_extension_version\n from jax.sharding import PartitionSpec as P\n \n config.parse_flags_with_absl()\n@@ -31,13 +32,8 @@\n complex_types = jtu.dtypes.complex\n \n \n+# These functions are only supported on CPU.\n CPU_ONLY_FUN_AND_SHAPES = [\n-    # The GPU kernel for this function still uses an opaque descriptor to\n-    # encode the input shapes so it is not partitionable.\n-    # TODO(danfm): Update the kernel and enable this test on GPU.\n-    (lax.linalg.tridiagonal_solve, ((6,), (6,), (6,), (6, 4))),\n-\n-    # These functions are only supported on CPU.\n     (lax.linalg.hessenberg, ((6, 6),)),\n     (lax.linalg.schur, ((6, 6),)),\n ]\n@@ -51,6 +47,7 @@\n     (lax.linalg.svd, ((10, 6),)),\n     (lax.linalg.triangular_solve, ((6, 6), (4, 6))),\n     (lax.linalg.tridiagonal, ((6, 6),)),\n+    (lax.linalg.tridiagonal_solve, ((6,), (6,), (6,), (6, 4))),\n ]\n \n ALL_FUN_AND_SHAPES = CPU_ONLY_FUN_AND_SHAPES + CPU_AND_GPU_FUN_AND_SHAPES\n@@ -73,6 +70,11 @@ def get_fun_and_shapes(self, fun_and_shapes, grad=False):\n         self.skipTest(\n             f\"Partitioning {fun_and_shapes[0].__name__} only supported on GPU \"\n             \"when shardy is enabled.\")\n+      if (fun_and_shapes[0] == lax.linalg.tridiagonal_solve and\n+          jaxlib_extension_version < 340):\n+        self.skipTest(\n+            f\"Partitioning {fun_and_shapes[0].__name__} on GPU, requires a \"\n+            \"more recent jaxlib version.\")\n     if not grad:\n       return fun_and_shapes\n \n@@ -178,7 +180,9 @@ def jvp_fun(primals, tangents):\n         (primals_sharded, tangents),\n     ]:\n       _, actual = jvp_fun_jit(*args)\n-      self.assertAllClose(actual, expected, atol={np.float64: 1e-12})\n+      self.assertAllClose(actual, expected, rtol={\n+          np.float32: 1e-4, np.float64: 1e-11, np.complex64: 1e-4,\n+          np.complex128: 1e-11})\n       hlo = jvp_fun_jit.lower(primals_sharded, tangents_sharded).compile()\n       self.assertNotIn(\"all-\", hlo.as_text())\n \n@@ -199,7 +203,9 @@ def test_batch_axis_sharding_vjp(self, fun_and_shapes, dtype):\n     vjp_fun_jit = jax.jit(vjp_fun)\n     expected = vjp_fun(tangents)\n     actual = vjp_fun_jit(tangents_sharded)\n-    self.assertAllClose(actual, expected)\n+    self.assertAllClose(actual, expected, rtol={\n+          np.float32: 1e-4, np.float64: 1e-11, np.complex64: 1e-4,\n+          np.complex128: 1e-11})\n     hlo = vjp_fun_jit.lower(tangents_sharded).compile()\n     self.assertNotIn(\"all-\", hlo.as_text())\n \ndiff --git a/tests/linalg_test.py b/tests/linalg_test.py\nindex 033ca989c8e7..a9f81ec04560 100644\n--- a/tests/linalg_test.py\n+++ b/tests/linalg_test.py\n@@ -33,6 +33,7 @@\n from jax._src.lax import linalg as lax_linalg\n from jax._src import test_util as jtu\n from jax._src import xla_bridge\n+from jax._src.lib import jaxlib_extension_version\n from jax._src.numpy.util import promote_dtypes_inexact\n \n config.parse_flags_with_absl()\n@@ -2202,7 +2203,7 @@ def testSelect(self, dtype):\n   @jtu.sample_product(shape=[(3,), (3, 4), (3, 4, 5)],\n                       dtype=float_types + complex_types)\n   def test_tridiagonal_solve(self, shape, dtype):\n-    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]):\n+    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]) and jaxlib_extension_version < 340:\n       self.skipTest(\"Data type not supported on GPU\")\n     rng = self.rng()\n     d = 1.0 + jtu.rand_positive(rng)(shape, dtype)\n@@ -2217,7 +2218,10 @@ def build_tri(dl, d, du):\n       build_tri = jax.vmap(build_tri)\n \n     a = build_tri(dl, d, du)\n-    self.assertAllClose(a @ x, b, atol=5e-5, rtol=1e-4)\n+    with jax.default_matmul_precision(\"float32\"):\n+      self.assertAllClose(a @ x, b, atol={\n+          np.float32: 1e-3, np.float64: 1e-10, np.complex64: 1e-3,\n+          np.complex128: 1e-10})\n \n   def test_tridiagonal_solve_endpoints(self):\n     # tridagonal_solve shouldn't depend on the endpoints being explicitly zero.\n@@ -2238,7 +2242,7 @@ def test_tridiagonal_solve_endpoints(self):\n \n   @jtu.sample_product(shape=[(3,), (3, 4)], dtype=float_types + complex_types)\n   def test_tridiagonal_solve_grad(self, shape, dtype):\n-    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]):\n+    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]) and jaxlib_extension_version < 340:\n       self.skipTest(\"Data type not supported on GPU\")\n     rng = self.rng()\n     d = 1.0 + jtu.rand_positive(rng)(shape, dtype)\n\n```",
        "from_id": [
            "dfm",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Consolidate initial/final style custom_vjp primitives into one.",
        "output": "```diff\nCommit: 74938be8456050fec5032682f1463ea36983e1de\nDate: 2025-05-13T09:39:08Z\nURL: https://github.com/jax-ml/jax/commit/74938be8456050fec5032682f1463ea36983e1de\nFiles changed: 8\nAdditions: +111, Deletions: -159\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex 144cbaf5cd21..aa9bfe9529ce 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -1079,17 +1079,17 @@ def jvp(*xs):\n     return [*primal_errs, *out_primals, *tangent_errs, *out_tangents]\n   return lu.wrap_init(jvp, debug_info=jvp_jaxpr_fun.debug_info)\n \n-def custom_vjp_call_jaxpr_rule(in_err, enabled_errors, *in_vals,\n-                               fun_jaxpr: core.ClosedJaxpr,\n-                               fwd_jaxpr_thunk, num_consts,\n-                               bwd: lu.WrappedFun, out_trees,\n-                               symbolic_zeros: bool):\n+def custom_vjp_call_rule(in_err, enabled_errors, *in_vals,\n+                         call_jaxpr: core.ClosedJaxpr,\n+                         fwd_jaxpr_thunk, num_consts,\n+                         bwd: lu.WrappedFun, out_trees,\n+                         symbolic_zeros: bool):\n   err_vals, err_tree = jtu.tree_flatten(in_err)\n   num_errs = err_tree.num_leaves\n   checkified_fun = lu.wrap_init(\n-      functools.partial(checkify_jaxpr_flat, fun_jaxpr.jaxpr,\n-                        fun_jaxpr.consts, enabled_errors, err_tree),\n-      debug_info=fun_jaxpr.jaxpr.debug_info)\n+      functools.partial(checkify_jaxpr_flat, call_jaxpr.jaxpr,\n+                        call_jaxpr.consts, enabled_errors, err_tree),\n+      debug_info=call_jaxpr.jaxpr.debug_info)\n   checkified_fun, fun_metadata = _flatten_and_get_error_metadata_thunk(\n       checkified_fun)\n \n@@ -1097,13 +1097,13 @@ def checkified_fwd(*args):\n     # TODO(lenamartens, sharadmv): why not checkify here?\n     xs, zeros = args[::2], args[1::2]\n     xs, zeros = xs[num_errs:], zeros[num_errs:]\n-    fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk(*zeros)\n+    fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk.call_wrapped(*zeros)\n     xs_without_consts = xs[num_consts:]\n     return core.eval_jaxpr(fwd_jaxpr, fwd_consts, *xs_without_consts)\n \n   # TODO(necula): the fwd result_paths are not quite the same as fun_jaxpr\n   checkified_fwd_wrapped = lu.wrap_init(checkified_fwd,\n-                                        debug_info=fun_jaxpr.jaxpr.debug_info)\n+                                        debug_info=fwd_jaxpr_thunk.debug_info)\n   bwd_ = lu.wrap_init(lambda *args: (*(None,)*num_errs, *bwd.call_wrapped(*args)),\n                       debug_info=bwd.debug_info)\n   checkified_fwd_wrapped, fwd_out_tree = flatten_fun_output(checkified_fwd_wrapped)\n@@ -1118,7 +1118,7 @@ def checkified_fwd(*args):\n   else:\n     out_err, out_vals = in_err, all_outs\n   return out_err, out_vals\n-error_checks[custom_derivatives.custom_vjp_call_jaxpr_p] = custom_vjp_call_jaxpr_rule\n+error_checks[custom_derivatives.custom_vjp_call_p] = custom_vjp_call_rule\n \n \n def check_discharge_rule(error, enabled_errors, *args, err_tree, debug):\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex dc8fc90e3d1f..dcd893f44123 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -425,16 +425,14 @@ def _custom_jvp_call_typecheck(_, *in_avals, call_jaxpr, jvp_jaxpr_fun,\n   return call_jaxpr.out_avals, call_jaxpr.effects\n core.custom_typechecks[custom_jvp_call_p] = _custom_jvp_call_typecheck\n \n-def _custom_jvp_call_mlir_translation(ctx, *args, call_jaxpr, jvp_jaxpr_fun,\n-                                      num_consts, symbolic_zeros):\n-  del jvp_jaxpr_fun, num_consts, symbolic_zeros\n+def _custom_jvp_vjp_call_lowering(ctx, *args, call_jaxpr, **_):\n   consts = mlir._ir_consts(call_jaxpr.consts)\n   out, tokens = mlir.jaxpr_subcomp(ctx.module_context, call_jaxpr.jaxpr,\n                                    ctx.name_stack, ctx.tokens_in, consts,\n                                    *args, dim_var_values=ctx.dim_var_values)\n   ctx.set_tokens_out(tokens)\n   return out\n-mlir.register_lowering(custom_jvp_call_p, _custom_jvp_call_mlir_translation)\n+mlir.register_lowering(custom_jvp_call_p, _custom_jvp_vjp_call_lowering)\n \n # If a (multi)linear function is defined with a custom jvp, then\n # custom_jvp_call_ can appear in jaxprs to be transposed. Since it's already\n@@ -936,8 +934,8 @@ def _temporary_dtype_exception(a, a_) -> bool:\n def _temporary_shape_exception(a, a_) -> bool:\n   return config.custom_vjp_disable_shape_check.value\n \n-class CustomVJPCallPrimitive(core.CallPrimitive):\n-  initial_style: core.Primitive\n+class CustomVJPCallPrimitive(core.Primitive):\n+  multiple_results = True\n \n   def bind(self, *args, **params):\n     return self._true_bind(*args, **params)\n@@ -946,107 +944,70 @@ def bind_with_trace(self, trace, args, params):\n     fun, fwd, bwd, tracers = args[0], args[1], args[2], args[3:]\n     return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n \n-custom_vjp_call_p = CustomVJPCallPrimitive('custom_vjp_call')\n+  def impl(self, fun, fwd, bwd, *args):\n+    raise NotImplementedError\n+\n+  def get_bind_params(self, params):\n+    new_params = dict(params)\n+    call_jaxpr: core.ClosedJaxpr = new_params.pop('call_jaxpr')\n+    num_consts: int = new_params.pop('num_consts')\n+    fwd_jaxpr_thunk = new_params.pop('fwd_jaxpr_thunk')\n+    fun = lu.wrap_init(core.jaxpr_as_fun(call_jaxpr),\n+                       debug_info=call_jaxpr.jaxpr.debug_info)\n+    fwd = lift_fwd(num_consts, fwd_jaxpr_thunk)\n+    const_avals, _ = split_list(call_jaxpr.in_avals, [num_consts])\n+    bwd = _handle_consts_in_bwd(new_params.pop('bwd'), const_avals)\n+    return [fun, fwd, bwd], new_params\n+\n+def lift_fwd(num_consts: int, fwd_jaxpr_thunk: lu.WrappedFun) -> lu.WrappedFun:\n+  def fwd(*args):\n+    vals, zeros = args[::2], args[1::2]\n+    assert len(vals) == len(zeros)\n+    _, primals = split_list(vals, [num_consts])\n+    const_zeros, in_zeros = split_list(zeros, [num_consts])\n+    if any(const_zeros):\n+      raise ad.CustomVJPException()\n+    fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk.call_wrapped(*in_zeros)\n+    return core.eval_jaxpr(fwd_jaxpr, fwd_consts, *primals)\n+  return lu.wrap_init(fwd, debug_info=fwd_jaxpr_thunk.debug_info)\n \n-def _custom_vjp_call_jaxpr_impl(*args, fun_jaxpr, **_):\n-  return core.jaxpr_as_fun(fun_jaxpr)(*args)\n+@lu.transformation2\n+def _handle_consts_in_bwd(f, const_avals, *args):\n+  return [Zero(a) for a in const_avals] + list(f(*args))\n \n-def _custom_vjp_call_jaxpr_abstract_eval(*_, fun_jaxpr, **__):\n-  disallowed_effects = effects.custom_derivatives_allowed_effects.filter_not_in(fun_jaxpr.effects)\n+custom_vjp_call_p = CustomVJPCallPrimitive('custom_vjp_call')\n+mlir.register_lowering(custom_vjp_call_p, _custom_jvp_vjp_call_lowering)\n+\n+def _custom_vjp_call_typecheck(_, *in_avals, call_jaxpr, **kwargs):\n+  del in_avals, kwargs\n+  disallowed_effects = effects.custom_derivatives_allowed_effects.filter_not_in(\n+      call_jaxpr.effects)\n   if disallowed_effects:\n     raise NotImplementedError(\n         f'Effects not supported in `custom_vjp`: {disallowed_effects}')\n-  return fun_jaxpr.out_avals, fun_jaxpr.effects\n-\n-custom_vjp_call_jaxpr_p = core.Primitive('custom_vjp_call_jaxpr')\n-custom_vjp_call_jaxpr_p.multiple_results = True\n-custom_vjp_call_jaxpr_p.def_impl(_custom_vjp_call_jaxpr_impl)\n-custom_vjp_call_jaxpr_p.def_effectful_abstract_eval(_custom_vjp_call_jaxpr_abstract_eval)\n-CustomVJPCallPrimitive.initial_style = custom_vjp_call_jaxpr_p\n-\n-mlir.register_lowering(custom_vjp_call_jaxpr_p, mlir.lower_fun(\n-    _custom_vjp_call_jaxpr_impl, multiple_results=True))\n-\n-def _custom_vjp_call_jaxpr_jvp(\n-    primals, tangents, *, fun_jaxpr: core.ClosedJaxpr,\n-    fwd_jaxpr_thunk: Callable[..., tuple[core.Jaxpr, Sequence[Any]]],\n-    num_consts: int, bwd: lu.WrappedFun,\n-    out_trees: Callable[[], Sequence[PyTreeDef]],\n-    symbolic_zeros: bool):\n-  _, args = split_list(primals, [num_consts])\n-  consts_dot, args_dot = split_list(tangents, [num_consts])\n-  if any(type(t) is not Zero for t in consts_dot):\n-    raise ad.CustomVJPException()\n-  zeros = [type(t) is not Zero for t in args_dot]\n-  fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk(*zeros)  # consts can be tracers!\n-  _, res_tree = out_trees()\n-  res_and_primals_out = core.eval_jaxpr(fwd_jaxpr, fwd_consts, *args)\n-  res, primals_out = split_list(res_and_primals_out, [res_tree.num_leaves])\n-  avals_out = [core.get_aval(x).to_tangent_aval() for x in primals_out]\n-  args_dot = map(ad.instantiate_zeros, args_dot)\n-  tangents_out = ad.custom_lin_p.bind(\n-      *res, *args_dot, num_res=res_tree.num_leaves, bwd=bwd,\n-      out_avals=avals_out, symbolic_zeros=symbolic_zeros)\n-  tangents_out = map(lax.tie_p.bind, primals_out, tangents_out)\n-  return primals_out, tangents_out\n-ad.primitive_jvps[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_jvp\n-\n-def _custom_vjp_call_jaxpr_vmap(\n-    axis_data, args, in_dims, *,\n-    fun_jaxpr: core.ClosedJaxpr,\n-    fwd_jaxpr_thunk: Callable[..., tuple[core.Jaxpr, Sequence[Any]]],\n-    num_consts: int, bwd: lu.WrappedFun,\n-    out_trees: Callable, symbolic_zeros: bool):\n-  args = [batching.moveaxis(x, d, 0) if d is not not_mapped and d != 0\n-          else x for x, d in zip(args, in_dims)]\n-  in_batched = [d is not not_mapped for d in in_dims]\n-  _, args_batched = split_list(in_batched, [num_consts])\n-  batched_fun_jaxpr, out_batched = batching.batch_jaxpr(\n-      fun_jaxpr, axis_data, in_batched, False)\n-  out_dims1 = [0 if b else not_mapped for b in out_batched]\n-  out_dims2 = []\n-\n-  @pe._memoize\n-  def batched_fwd_jaxpr_thunk(*zeros):\n-    fwd_jaxpr = core.ClosedJaxpr(*fwd_jaxpr_thunk(*zeros))  # consts can be tracers\n-    batched_fwd_jaxpr, out_batched = batching.batch_jaxpr(\n-        fwd_jaxpr, axis_data, args_batched, False)\n-    out_dims2.append([0 if b else not_mapped for b in out_batched])\n-    return batched_fwd_jaxpr.jaxpr, batched_fwd_jaxpr.consts\n-\n-  fwd_args_batched = [0 if b else not_mapped for b in args_batched]\n-  fwd_out_dims = lambda: out_dims2[0]\n-  tag = core.TraceTag()\n-  batched_bwd = batching.batch_custom_vjp_bwd(\n-    bwd, tag, axis_data, fwd_out_dims, fwd_args_batched)\n-\n-  batched_outs = custom_vjp_call_jaxpr_p.bind(\n-      *args, fun_jaxpr=batched_fun_jaxpr,\n-      fwd_jaxpr_thunk=batched_fwd_jaxpr_thunk, bwd=batched_bwd,\n-      num_consts=num_consts, out_trees=out_trees, symbolic_zeros=symbolic_zeros)\n-  out_dims = out_dims2[0] if out_dims2 else out_dims1\n-  return batched_outs, out_dims\n-batching.fancy_primitive_batchers[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_vmap\n+  return call_jaxpr.out_avals, call_jaxpr.effects\n+core.custom_typechecks[custom_vjp_call_p] = _custom_vjp_call_typecheck\n \n-def _custom_vjp_call_jaxpr_dce(\n+def _custom_vjp_call_dce(\n     used_outs: Sequence[bool], eqn: core.JaxprEqn\n ) -> tuple[list[bool], core.JaxprEqn | None]:\n   if not any(used_outs) and not pe.has_effects(eqn):\n     return [False] * len(eqn.invars), None\n-  fun_jaxpr: core.ClosedJaxpr = eqn.params[\"fun_jaxpr\"]\n+  call_jaxpr: core.ClosedJaxpr = eqn.params[\"call_jaxpr\"]\n   fwd_jaxpr_thunk = eqn.params[\"fwd_jaxpr_thunk\"]\n   bwd: lu.WrappedFun = eqn.params[\"bwd\"]\n   out_trees: Callable[[], Sequence[PyTreeDef]] = eqn.params[\"out_trees\"]\n   symbolic_zeros: bool = eqn.params[\"symbolic_zeros\"]\n-  dce_fun_jaxpr: core.ClosedJaxpr\n+  dce_call_jaxpr: core.ClosedJaxpr\n   used_ins: Sequence[bool]\n-  dce_fun_jaxpr, used_ins = _cached_closed_call_dce_instantiate(\n-      fun_jaxpr, tuple(used_outs))\n+  dce_call_jaxpr, used_ins = _cached_closed_call_dce_instantiate(\n+      call_jaxpr, tuple(used_outs))\n   assert all(used_ins)\n \n+  @partial(lu.wrap_init, debug_info=fwd_jaxpr_thunk.debug_info)\n   @pe._memoize\n   def dce_fwd_jaxpr_thunk(*zeros):\n-    fwd_jaxpr = core.ClosedJaxpr(*fwd_jaxpr_thunk(*zeros))\n+    fwd_jaxpr = core.ClosedJaxpr(*fwd_jaxpr_thunk.call_wrapped(*zeros))\n     _, res_tree = out_trees()\n     num_res = res_tree.num_leaves\n     dce_fwd_jaxpr, _ = _cached_closed_call_dce_instantiate(\n@@ -1058,7 +1019,7 @@ def dce_bwd(*args):\n     res, cts = split_list(args, [res_tree.num_leaves])\n     cts_ = iter(cts)\n     all_cts = []\n-    for used, aval in zip(used_outs, fun_jaxpr.out_avals):\n+    for used, aval in zip(used_outs, call_jaxpr.out_avals):\n       if used:\n         all_cts.append(next(cts_))\n       else:\n@@ -1075,17 +1036,15 @@ def dce_bwd(*args):\n   outvars = [v for used, v in zip(used_outs, eqn.outvars) if used]\n   new_params = dict(\n       eqn.params,\n-      fun_jaxpr=dce_fun_jaxpr,\n+      call_jaxpr=dce_call_jaxpr,\n       fwd_jaxpr_thunk=dce_fwd_jaxpr_thunk,\n       bwd=dce_bwd_wrapped,\n   )\n   new_eqn = pe.new_jaxpr_eqn(\n-      eqn.invars, outvars, eqn.primitive, new_params, dce_fun_jaxpr.effects,\n+      eqn.invars, outvars, eqn.primitive, new_params, dce_call_jaxpr.effects,\n       eqn.source_info, eqn.ctx)\n   return list(used_ins), new_eqn\n-pe.dce_rules[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_dce\n-\n-xla.register_initial_style_primitive(custom_vjp_call_jaxpr_p)\n+pe.dce_rules[custom_vjp_call_p] = _custom_vjp_call_dce\n \n batching.primitive_batchers[ad.custom_lin_p] = ad.raise_custom_vjp_error_on_jvp\n mlir.register_lowering(ad.custom_lin_p, ad.raise_custom_vjp_error_on_jvp)\n@@ -1586,7 +1545,6 @@ def jvp(primals, tangents):\n # TODO(mattjj): remove these stubs, which exist to avoid breaking internal users\n custom_jvp_call_jaxpr_p = core.Primitive(\"custom_jvp_call_jaxpr\")\n \n-\n # The following is a helper for optimizing the behavior of custom_vjp when used\n # under remat. This is really only useful when the `fwd` function to custom_vjp\n # executes a black box kernel. Otherwise, DCE will perform this optimization\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 64226a789cde..5866b0c5f8eb 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -434,49 +434,45 @@ def process_custom_vjp_call(self, prim, f, fwd, bwd, tracers, out_trees, symboli\n     if all(t.is_known() for t in tracers):\n       vals = [t.pval[1] for t in tracers]\n       with core.set_current_trace(self.parent_trace):\n-        return prim.bind(f, fwd, bwd, *vals, out_trees=out_trees, symbolic_zeros=symbolic_zeros)\n-    else:\n-      # TODO(mattjj): remove non-ad users of partial eval, then drop this case.\n-      # We stage out the whole thing, i.e. no nontrivial partial evaluation.\n-      tracers = map(self.instantiate_const_abstracted, tracers)\n-      # Because we instantiate all tracers, in_knowns is all False.\n-      in_knowns, in_avals, () = partition_pvals([t.pval for t in tracers])\n-      f = trace_to_subjaxpr_nounits(f, self, True, f.debug_info)\n-      f, aux = partial_eval_wrapper_nounits(f, (*in_knowns,), (*in_avals,))\n-      with core.set_current_trace(self.parent_trace):\n-        out_flat = prim.bind(f, fwd, bwd, out_trees=out_trees,\n-                             symbolic_zeros=symbolic_zeros)\n-      out_knowns, out_avals, jaxpr, env = aux()\n-      out_consts, res = split_list(out_flat, [len(out_flat)-len(jaxpr.constvars)])\n-      res_tracers = map(self.new_instantiated_const, res)\n-      env_tracers = map(self.to_jaxpr_tracer, env)\n-      out_tracers = [JaxprTracer(self, PartialVal.unknown(a), None)\n-                    for a in out_avals]\n-      closed_jaxpr = core.ClosedJaxpr(convert_constvars_jaxpr(jaxpr), ())\n-\n-      @_memoize\n-      def fwd_jaxpr_thunk(*zeros):\n-        fwd_ = _interleave_fun(fwd, zeros)\n-        fwd_ = trace_to_subjaxpr_nounits(fwd_, self, True, fwd_.debug_info)\n-        fwd_, aux = partial_eval_wrapper_nounits(fwd_, (*in_knowns,), (*in_avals,))\n-        out_flat = fwd_.call_wrapped()\n-        out_knowns, out_avals, jaxpr, env = aux()\n-        _, res = split_list(out_flat, [len(out_flat)-len(jaxpr.constvars)])\n-        converted_jaxpr = convert_envvars_to_constvars(jaxpr, len(env))\n-        return converted_jaxpr, (*res, *env)\n+        return prim.bind(f, fwd, bwd, *vals, out_trees=out_trees,\n+                         symbolic_zeros=symbolic_zeros)\n+\n+    tracers = map(self.instantiate_const, tracers)\n+    in_knowns = (False,) * len(tracers)\n+    in_avals = tuple(t.aval for t in tracers)\n+    f_ = trace_to_subjaxpr_nounits2(f, self.tag, f.debug_info, True)\n+    f_, aux = partial_eval_wrapper_nounits(f_, in_knowns, in_avals)\n+    params = dict(out_trees=out_trees, symbolic_zeros=symbolic_zeros)\n+    res = prim.bind_with_trace(self.parent_trace, (f_, fwd, bwd), params)\n+    out_knowns, out_avals, jaxpr, env = aux()\n+    assert not any(out_knowns)\n+    res_tracers = map(self.instantiate_const, map(self.new_const, res))\n+    env_tracers = map(self.to_jaxpr_tracer, env)\n+    out_tracers = [JaxprTracer(self, PartialVal.unknown(a), None)\n+                   for a in out_avals]\n+    closed_jaxpr = close_jaxpr(convert_constvars_jaxpr(jaxpr))\n+\n+    @partial(lu.wrap_init, debug_info=fwd.debug_info)\n+    @_memoize\n+    def fwd_jaxpr_thunk(*zeros):\n+      fwd_ = _interleave_fun(fwd, zeros)\n+      fwd_jaxpr, _, consts, () = trace_to_jaxpr_dynamic(fwd_, in_avals)\n+      return fwd_jaxpr, consts\n \n     name_stack = self._current_truncated_name_stack()\n     source = source_info_util.current().replace(name_stack=name_stack)\n+    params = dict(\n+        call_jaxpr=closed_jaxpr,\n+        fwd_jaxpr_thunk=fwd_jaxpr_thunk,\n+        num_consts=len(res) + len(env),\n+        bwd=bwd,\n+        out_trees=out_trees,\n+        symbolic_zeros=symbolic_zeros\n+    )\n     eqn = new_eqn_recipe((*res_tracers, *env_tracers, *tracers),\n-                         out_tracers, prim.initial_style,\n-                         dict(fun_jaxpr=closed_jaxpr,\n-                              fwd_jaxpr_thunk=fwd_jaxpr_thunk,\n-                              num_consts=len(res) + len(env),\n-                              bwd=bwd, out_trees=out_trees,\n-                              symbolic_zeros=symbolic_zeros),\n-                         jaxpr.effects, source)\n+                         out_tracers, prim, params, jaxpr.effects, source)\n     for t in out_tracers: t.recipe = eqn\n-    return merge_lists(out_knowns, out_tracers, out_consts)\n+    return out_tracers\n \n def partition_pvals(\n     pvals: list[PartialVal]\n@@ -2050,6 +2046,7 @@ def process_custom_jvp_call(self, prim, fun: lu.WrappedFun,\n     fun_jaxpr, out_avals, consts, () = trace_to_jaxpr_dynamic(fun, in_avals)\n     closed_fun_jaxpr = core.ClosedJaxpr(convert_constvars_jaxpr(fun_jaxpr), ())\n \n+    @partial(lu.wrap_init, debug_info=jvp.debug_info)\n     @_memoize\n     def jvp_jaxpr_thunk(*in_zeros):\n       for store in jvp.stores: store and store.reset()\n@@ -2065,8 +2062,7 @@ def jvp_jaxpr_thunk(*in_zeros):\n     outvars = map(self.makevar, out_tracers)\n     eqn = new_jaxpr_eqn([*constvars, *invars], outvars, prim,\n                         dict(call_jaxpr=closed_fun_jaxpr,\n-                             jvp_jaxpr_fun=lu.wrap_init(jvp_jaxpr_thunk,\n-                                                        debug_info=jvp.debug_info),\n+                             jvp_jaxpr_fun=jvp_jaxpr_thunk,\n                              num_consts=len(consts),\n                              symbolic_zeros=symbolic_zeros),\n                         fun_jaxpr.effects,\n@@ -2086,6 +2082,7 @@ def process_custom_vjp_call(self, prim: core.Primitive,\n     fun_jaxpr, out_avals, consts, _ = trace_to_jaxpr_dynamic(fun, in_avals)\n     closed_fun_jaxpr = core.ClosedJaxpr(convert_constvars_jaxpr(fun_jaxpr), ())\n \n+    @partial(lu.wrap_init, debug_info=fwd.debug_info)\n     @_memoize\n     def fwd_jaxpr_from_zeros(*zeros):\n       for store in fwd.stores: store and store.reset()\n@@ -2098,9 +2095,8 @@ def fwd_jaxpr_from_zeros(*zeros):\n     invars = map(self.getvar, tracers)\n     constvars = map(self.getvar, map(to_jaxpr_tracer, consts))\n     outvars = map(self.makevar, out_tracers)\n-    eqn = new_jaxpr_eqn([*constvars, *invars], outvars,\n-                        prim.initial_style,  # pytype: disable=attribute-error\n-                        dict(fun_jaxpr=closed_fun_jaxpr,\n+    eqn = new_jaxpr_eqn([*constvars, *invars], outvars, prim,\n+                        dict(call_jaxpr=closed_fun_jaxpr,\n                              fwd_jaxpr_thunk=fwd_jaxpr_from_zeros,\n                              num_consts=len(consts),\n                              bwd=bwd, out_trees=out_trees,\ndiff --git a/jax/_src/pallas/cost_estimate.py b/jax/_src/pallas/cost_estimate.py\nindex 3b82d3095f64..93bcf5348b24 100644\n--- a/jax/_src/pallas/cost_estimate.py\n+++ b/jax/_src/pallas/cost_estimate.py\n@@ -238,15 +238,15 @@ def _pjit_cost_rule(ctx, *, jaxpr: jax_core.ClosedJaxpr, **_):\n   )\n register_cost_rule(pjit.pjit_p, _pjit_cost_rule)\n \n-def _custom_vjp_rule(ctx, *, fun_jaxpr: jax_core.ClosedJaxpr, **_):\n+def _custom_vjp_rule(ctx, *, call_jaxpr: jax_core.ClosedJaxpr, **_):\n   del ctx\n-  inner_cost = cost_estimate_jaxpr(fun_jaxpr)\n+  inner_cost = cost_estimate_jaxpr(call_jaxpr)\n   return CostEstimate(\n       flops=inner_cost.flops,\n       transcendentals=inner_cost.transcendentals,\n       bytes_accessed=inner_cost.bytes_accessed,\n   )\n-register_cost_rule(custom_derivatives.custom_vjp_call_jaxpr_p, _custom_vjp_rule)\n+register_cost_rule(custom_derivatives.custom_vjp_call_p, _custom_vjp_rule)\n \n def _run_state_rule(*_, jaxpr: jax_core.Jaxpr, **_2):\n   inner_cost = cost_estimate_jaxpr(pe.close_jaxpr(jaxpr))\ndiff --git a/jax/custom_derivatives.py b/jax/custom_derivatives.py\nindex 3628ae4aaa6e..b768b687dfad 100644\n--- a/jax/custom_derivatives.py\n+++ b/jax/custom_derivatives.py\n@@ -26,7 +26,6 @@\n   custom_jvp_call_jaxpr_p as custom_jvp_call_jaxpr_p,\n   custom_vjp as custom_vjp,\n   custom_vjp_call_p as custom_vjp_call_p,\n-  custom_vjp_call_jaxpr_p as custom_vjp_call_jaxpr_p,\n   custom_vjp_primal_tree_values as custom_vjp_primal_tree_values,\n   CustomVJPPrimal as CustomVJPPrimal,\n   linear_call as linear_call,\ndiff --git a/jax/experimental/jax2tf/jax2tf.py b/jax/experimental/jax2tf/jax2tf.py\nindex 786e021e2ff0..536bf1f201f0 100644\n--- a/jax/experimental/jax2tf/jax2tf.py\n+++ b/jax/experimental/jax2tf/jax2tf.py\n@@ -3461,14 +3461,14 @@ def _custom_jvp_call(*args: TfVal, call_jaxpr: core.ClosedJaxpr,\n tf_impl[custom_derivatives.custom_jvp_call_p] = _custom_jvp_call\n \n \n-def _custom_vjp_call_jaxpr(*args: TfVal, fun_jaxpr: core.ClosedJaxpr,\n-                           **_) -> Sequence[TfVal]:\n+def _custom_vjp_call(*args: TfVal, call_jaxpr: core.ClosedJaxpr,\n+                     **_) -> Sequence[TfVal]:\n   # TODO(necula): ensure that there is no AD transformation in scope\n-  return _interpret_jaxpr(fun_jaxpr, *args, extra_name_stack=\"custom_vjp\",\n+  return _interpret_jaxpr(call_jaxpr, *args, extra_name_stack=\"custom_vjp\",\n                           fresh_constant_cache=False)\n \n \n-tf_impl[custom_derivatives.custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr\n+tf_impl[custom_derivatives.custom_vjp_call_p] = _custom_vjp_call\n \n \n def _custom_lin(*args: TfVal, **_) -> Sequence[TfVal]:\ndiff --git a/jax/extend/core/primitives.py b/jax/extend/core/primitives.py\nindex 60d8cd24a949..30350dace637 100644\n--- a/jax/extend/core/primitives.py\n+++ b/jax/extend/core/primitives.py\n@@ -26,7 +26,6 @@\n   custom_jvp_call_p as custom_jvp_call_p,\n   custom_jvp_call_jaxpr_p as custom_jvp_call_jaxpr_p,\n   custom_vjp_call_p as custom_vjp_call_p,\n-  custom_vjp_call_jaxpr_p as custom_vjp_call_jaxpr_p,\n )\n \n from jax._src.dispatch import device_put_p as device_put_p\ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 72c14634a9c8..73dc2fbefcaa 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3065,7 +3065,7 @@ def check_jaxpr(jaxpr, used_outs, includes, excludes):\n       if not dce_jaxpr.eqns:\n         assert not includes\n         return\n-      call_jaxpr = dce_jaxpr.eqns[0].params[\"fun_jaxpr\"]\n+      call_jaxpr = dce_jaxpr.eqns[0].params[\"call_jaxpr\"]\n       for prim in includes:\n         assert any(eqn.primitive == prim for eqn in call_jaxpr.eqns)\n       for prim in excludes:\n\n```",
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Merge pull request #28607 from gnecula:fix_platform_dependent\n\nPiperOrigin-RevId: 758086717",
        "output": "```diff\nCommit: 97a354dfc67fe8e2230257735b6aab1434e8b15e\nDate: 2025-05-13T07:28:43Z\nURL: https://github.com/jax-ml/jax/commit/97a354dfc67fe8e2230257735b6aab1434e8b15e\nFiles changed: 8\nAdditions: +201, Deletions: -90\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex 5a6456762db7..144cbaf5cd21 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -759,7 +759,8 @@ def jaxpr_to_checkify_jaxpr(\n   out_tree, error_effects = metadata()\n   return checked_jaxpr, out_tree, error_effects\n \n-def cond_error_check(error: Error, enabled_errors, index, *ops, branches):\n+def cond_error_check(error: Error, enabled_errors, index, *ops,\n+                     branches, **params):\n   # Get the error-effects out of all branches so the cond can be called with\n   # a merged error with all these effects.\n   err_vals, err_tree = jtu.tree_flatten(error)\n@@ -780,7 +781,7 @@ def get_error_effects_from_jaxpr(jxpr):\n \n   err_and_outs = lax.cond_p.bind(\n       index, *err_vals, *ops,\n-      branches=tuple(new_branches))\n+      branches=tuple(new_branches), **params)\n \n   # we need to merge metadata across out_trees (a tuple)\n   err0, out = tree_unflatten(out_trees[0], err_and_outs)\ndiff --git a/jax/_src/interpreters/mlir.py b/jax/_src/interpreters/mlir.py\nindex e9deb8d3fff9..f6ef5787ccbf 100644\n--- a/jax/_src/interpreters/mlir.py\n+++ b/jax/_src/interpreters/mlir.py\n@@ -2080,6 +2080,11 @@ def _platforms_for_eqn_ctx(eqn_ctx: core.JaxprEqnContext | None\n     return ('tpu',)\n   return ()\n \n+def _platforms_for_eqn(ctx: LoweringRuleContext) -> tuple[str, ...]:\n+  \"\"\"The lowering platforms for the current eqn\"\"\"\n+  return tuple((_platforms_for_eqn_ctx(ctx.jaxpr_eqn_ctx) or\n+               ctx.platforms or ctx.module_context.platforms))\n+\n \n def lower_per_platform(ctx: LoweringRuleContext,\n                        description: str,\n@@ -2122,8 +2127,7 @@ def lower_per_platform(ctx: LoweringRuleContext,\n    rule_args: the args of the lowering rules.\n    rule_kwargs: the kwargs of the lowering rules.\n   \"\"\"\n-  platforms: Sequence[str] = (_platforms_for_eqn_ctx(ctx.jaxpr_eqn_ctx) or\n-                              ctx.platforms or ctx.module_context.platforms)\n+  platforms: Sequence[str] = _platforms_for_eqn(ctx)\n   # Special case the common case (single-platform lowering)\n   if len(platforms) == 1:\n     rule = platform_rules.get(platforms[0], default_rule)\ndiff --git a/jax/_src/lax/control_flow/__init__.py b/jax/_src/lax/control_flow/__init__.py\nindex f89e4d53a476..44ee94e14ca2 100644\n--- a/jax/_src/lax/control_flow/__init__.py\n+++ b/jax/_src/lax/control_flow/__init__.py\n@@ -34,6 +34,7 @@\n     while_p as while_p,\n )\n from jax._src.lax.control_flow.conditionals import (\n+    BranchesPlatforms as BranchesPlatforms,\n     cond as cond,\n     cond_p as cond_p,\n     switch as switch,\ndiff --git a/jax/_src/lax/control_flow/conditionals.py b/jax/_src/lax/control_flow/conditionals.py\nindex 99fa72421ea1..d875989921d0 100644\n--- a/jax/_src/lax/control_flow/conditionals.py\n+++ b/jax/_src/lax/control_flow/conditionals.py\n@@ -46,6 +46,7 @@\n from jax._src.interpreters import xla\n from jax._src.lax import lax\n from jax._src.traceback_util import api_boundary\n+from jax._src.typing import ArrayLike\n from jax._src.util import safe_map, split_list, partition_list, unzip2\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n@@ -127,9 +128,17 @@ def switch(index, branches, *operands):\n   lo = np.array(0, np.int32)\n   hi = np.array(len(branches) - 1, np.int32)\n   index = lax.clamp(lo, index, hi)\n+  return _switch_internal(index, branches, operands,\n+                          branches_platforms=None)\n \n+\n+def _switch_internal(\n+    index: ArrayLike,\n+    branches: Sequence[Callable],\n+    operands: Sequence[ArrayLike], *,\n+    branches_platforms: BranchesPlatforms | None):\n   if (config.disable_jit.value and core.is_concrete(index)):\n-    return branches[int(index)](*operands)\n+    return branches[int(index)](*operands)  # type: ignore\n \n   dbgs = [api_util.debug_info(\"switch\", branch, operands, {})\n           for branch in branches]\n@@ -159,7 +168,10 @@ def switch(index, branches, *operands):\n     raise NotImplementedError(\n         f'Effects not supported in `switch`: {disallowed_effects}')\n   jaxprs = [replace_jaxpr_effects(jaxpr, joined_effects) for jaxpr in jaxprs]\n-  out = cond_p.bind(index, *consts, *ops, branches=tuple(jaxprs))\n+  params = dict(branches=tuple(jaxprs))\n+  if branches_platforms is not None:\n+    params[\"branches_platforms\"] = branches_platforms\n+  out = cond_p.bind(index, *consts, *ops, **params)\n   out_ = iter(out)\n \n   all_inputs = [*consts, *ops]\n@@ -464,7 +476,7 @@ def _bcast_select_n(pred, *cases):\n     pred = lax.broadcast_in_dim(pred, np.shape(cases[0]), idx)\n   return lax.select_n(pred, *cases)\n \n-def _cond_batching_rule(axis_data, args, dims, branches):\n+def _cond_batching_rule(axis_data, args, dims, *, branches, **params):\n   index, *ops = args\n   index_dim, *op_dims = dims\n   # TODO(sharadmv): clean this up by adding a specific blocklist\n@@ -480,6 +492,9 @@ def _cond_batching_rule(axis_data, args, dims, branches):\n \n \n   if index_dim is not batching.not_mapped:\n+    assert \"branches_platforms\" not in params, (\n+        \"The index of a cond with branches_platforms should be a \"\n+        \"platform_index and should never be mapped\")\n     # Convert to a lax.select. While we could get away with not broadcasting\n     # some operands yet, because all outputs must be broadcast together anyway\n     # for the select we broadcast the input operands for simplicity and leave\n@@ -518,10 +533,11 @@ def _cond_batching_rule(axis_data, args, dims, branches):\n         for jaxpr in branches)\n \n     out_dims = [0 if b else batching.not_mapped for b in out_bat]\n-    out = cond_p.bind(index, *ops, branches=branches_batched)\n+    out = cond_p.bind(index, *ops, branches=branches_batched,\n+                      **params)\n     return out, out_dims\n \n-def _cond_jvp(primals, tangents, branches):\n+def _cond_jvp(primals, tangents, *, branches, **params):\n   nonzeros = [type(t) is not ad_util.Zero for t in tangents]\n \n   index_nz, *ops_nz = nonzeros\n@@ -538,14 +554,15 @@ def _cond_jvp(primals, tangents, branches):\n   _, *ops_dot = tangents\n   ops_dot = _prune_zeros(ops_dot)\n \n-  out = cond_p.bind(index, *ops, *ops_dot, branches=branches_jvp)\n+  out = cond_p.bind(index, *ops, *ops_dot, branches=branches_jvp,\n+                    **params)\n   out_primals, out_tangents = split_list(out, [len(out_nz)])\n   out_tangents_iter = iter(out_tangents)\n   out_tangents = [next(out_tangents_iter) if nz else ad_util.Zero.from_primal_value(p)\n                   for p, nz in zip(out_primals, out_nz)]\n   return out_primals, out_tangents\n \n-def _cond_partial_eval(trace, *tracers, branches):\n+def _cond_partial_eval(trace, *tracers, branches, **params):\n   in_unknowns = [t.pval[0] is not None for t in tracers]\n   index_uk, *ops_uk = in_unknowns\n   if any(isinstance(eff, RefEffect) for branch in branches for eff in\n@@ -556,7 +573,7 @@ def _cond_partial_eval(trace, *tracers, branches):\n   if index_uk:\n     # When the branch index is unknown, we stage out the whole cond.\n     # TODO(mattjj): remove this path when old remat is removed\n-    params = dict(branches=branches)\n+    params = dict(branches=branches, **params)\n     return trace.default_process_primitive(cond_p, tracers, params)\n \n   branches_out_uks = []\n@@ -586,7 +603,8 @@ def _cond_partial_eval(trace, *tracers, branches):\n              for j in branches_known[1:])\n \n   in_consts = [t.pval.get_known() for t in tracers if t.pval.is_known()]\n-  out_consts_res = cond_p.bind(*in_consts, branches=branches_known)\n+  out_consts_res = cond_p.bind(*in_consts, branches=branches_known,\n+                               **params)\n   out_consts, res = split_list(out_consts_res, [len(out_consts_res) - num_res])\n \n   index_tracer = trace.instantiate_const(tracers[0])\n@@ -595,7 +613,7 @@ def _cond_partial_eval(trace, *tracers, branches):\n   res_tracers = map(trace.new_instantiated_const, res)\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(aval), None)\n                  for aval in branches_unknown[0].out_avals]\n-  params = dict(branches=branches_unknown)\n+  params = dict(branches=branches_unknown, **params)\n   name_stack = source_info_util.current_name_stack()[len(trace.name_stack):]\n   source = source_info_util.current().replace(name_stack=name_stack)\n   eqn = pe.new_eqn_recipe(\n@@ -608,6 +626,7 @@ def _cond_partial_eval(trace, *tracers, branches):\n def _cond_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n   index_uk, *ops_uk = unks_in\n   branches = eqn.params['branches']\n+  eqn_rest_params = dict(k_v for k_v in eqn.params.items() if k_v[0] != 'branches')\n \n   # Instantiate all inputs (b/c jaxpr_staged will take all inputs).\n   new_inst = [x for x, inst in zip(eqn.invars, inst_in)\n@@ -664,7 +683,7 @@ def _cond_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n   # Build the known eqn.\n   ins_known, _ = partition_list(unks_in, eqn.invars)  # includes index invar\n   out_binders_known, _ = partition_list(unks_out, eqn.outvars)\n-  params_known = dict(branches=branches_known)\n+  params_known = dict(branches=branches_known, **eqn_rest_params)\n   effects_known = _join_cond_effects(branches_known)\n   eqn_known = pe.new_jaxpr_eqn(\n       ins_known, [*out_binders_known, *res_binders], cond_p, params_known,\n@@ -672,7 +691,7 @@ def _cond_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n \n   # Build the staged eqn.\n   _, out_binders_staged = partition_list(inst_out, eqn.outvars)\n-  params_staged = dict(branches=branches_staged)\n+  params_staged = dict(branches=branches_staged, **eqn_rest_params)\n   effects_staged = _join_cond_effects(branches_staged)\n   eqn_staged = pe.new_jaxpr_eqn(\n       [eqn.invars[0], *res_binders, *eqn.invars[1:]], out_binders_staged,\n@@ -818,7 +837,7 @@ def transposed(*args):\n                                          debug_info=jaxpr.jaxpr.debug_info),\n                             res_avals + jaxpr.out_avals)\n \n-def _cond_transpose(cts, *args, branches):\n+def _cond_transpose(cts, *args, branches, **params):\n   index, *ops = args\n   assert type(index) is not ad.UndefinedPrimal\n   linear = [type(x) is ad.UndefinedPrimal for x in ops]\n@@ -838,7 +857,8 @@ def _cond_transpose(cts, *args, branches):\n   res = ops[:num_res]\n   cts = map(ad.instantiate_zeros, cts)\n \n-  out = cond_p.bind(index, *res, *cts, branches=branches_trans)\n+  out = cond_p.bind(index, *res, *cts, branches=branches_trans,\n+                    **params)\n   assert all(map(core.typecheck, lin_in_avals, out))\n \n   out_iter = iter(out)\n@@ -846,7 +866,8 @@ def _cond_transpose(cts, *args, branches):\n   assert next(out_iter, None) is None\n   return [None] + out\n \n-def _cond_typecheck(bind_time, *in_atoms, branches):\n+def _cond_typecheck(bind_time, *in_atoms, branches, **params):\n+  del params\n   if not bind_time:\n     _, *in_atoms = in_atoms\n   avals = [x.aval for x in in_atoms]\n@@ -900,6 +921,16 @@ def _cond_typecheck(bind_time, *in_atoms, branches):\n       f'called with operands of type {_avals_short(op_avals)}')\n   return jaxpr0.out_avals, joined_effects\n \n+\n+BranchesPlatforms = tuple[tuple[str, ...] | None, ...]\n+# cond_p takes an optional branches_platforms param of type `BranchesPlatforms`\n+# when it is a `platform_dependent` conditional.\n+# In that case, `branches_platforms` is a tuple as long\n+# as `branches` and for each branch it specifies the lowering platforms it\n+# corresponds to. The last element, corresponding to the last branch,\n+# can be `None` to represent a default match-all-lowering-platforms.\n+# The index argument of a `platform_dependent` cond is always a\n+# `platform_index` primitive.\n cond_p = core.Primitive('cond')\n cond_p.multiple_results = True\n cond_p.skip_canonicalization = True\n@@ -915,7 +946,39 @@ def _cond_typecheck(bind_time, *in_atoms, branches):\n pe.dce_rules[cond_p] = _cond_dce_rule\n batching.ragged_prop_rules[cond_p] = batching.ragged_mask_assert_no_op_rule\n \n-def _cond_lowering(ctx, index, *args, branches):\n+def _cond_lowering(ctx, index, *args, branches,\n+                   **params):\n+  if (branches_platforms := params.get(\"branches_platforms\", None)) is not None:\n+    branches_kept: list[core.ClosedJaxpr] = []\n+    index_to_kept_index: dict[int, int] = {}\n+    for p in mlir._platforms_for_eqn(ctx):\n+      # Each `p` must appear in exactly one branches_platforms, or in the\n+      # last default branch. Otherwise, platform_index lowering would have\n+      # failed already.\n+      for b_idx, b_platforms in enumerate(branches_platforms):\n+        if b_platforms is None or p in b_platforms:\n+          if b_idx not in index_to_kept_index:\n+            index_to_kept_index[b_idx] = len(branches_kept)\n+            branches_kept.append(branches[b_idx])\n+          break\n+      else:\n+        assert False, p\n+\n+    # Compute the new index into branches_keep\n+    i32_type = ir.RankedTensorType.get([], mlir.dtype_to_ir_type(dtypes.dtype(np.int32)))\n+    kept_index_case_op = hlo.CaseOp([i32_type],\n+                                    index=index,\n+                                    num_branches=len(branches))\n+    for i in range(len(branches)):\n+      branch = kept_index_case_op.regions[i].blocks.append()\n+      with ir.InsertionPoint(branch):\n+        kept_i = np.int32(index_to_kept_index.get(i, 0))\n+        hlo.return_([mlir.ir_constant(kept_i)])\n+\n+    index = kept_index_case_op\n+    branches = branches_kept\n+    assert branches, \"platform_index lowering should have failed first\"\n+\n   joined_effects = core.join_effects(*(branch.effects for branch in branches))\n   ordered_effects = list(effects.ordered_effects.filter_in(joined_effects))\n   num_tokens = len(ordered_effects)\n@@ -952,7 +1015,8 @@ def _cond_lowering(ctx, index, *args, branches):\n mlir.register_lowering(cond_p, _cond_lowering)\n \n @register_partial_discharge_rule(cond_p)\n-def _cond_state_discharge_rule(should_discharge, in_avals, out_avals, index, *args, branches):\n+def _cond_state_discharge_rule(should_discharge, in_avals, out_avals, index, *args,\n+                               branches, **params):\n   assert not should_discharge[0], \"Can't discharge the index.\"\n   discharged_branches = tuple(\n       discharge_state(branch.jaxpr, (), should_discharge=should_discharge[1:])[0]\n@@ -981,7 +1045,8 @@ def _cond_state_discharge_rule(should_discharge, in_avals, out_avals, index, *ar\n                                   if fwd is None]), ())\n       for branch in discharged_branches\n   )\n-  out_vals_no_fwd = cond_p.bind(index, *args, branches=new_branches)\n+  out_vals_no_fwd = cond_p.bind(index, *args, branches=new_branches,\n+                                **params)\n   out_vals, out_ref_vals_no_fwd = util.split_list(out_vals_no_fwd, [len(out_avals)])\n   # Insert forwarded values into reference outputs\n   ref_val_no_fwd_iter = iter(out_ref_vals_no_fwd)\n@@ -1046,50 +1111,41 @@ def other_platforms_code(*args): ...\n     The value ``per_platform[execution_platform](*args)``.\n   \"\"\"\n   # Join identical branches\n-  platform_branches: list[tuple[list[str], Callable]] = []\n+  branches_platforms_list: list[tuple[list[str], Callable]] = []\n   for pname, pbranch in per_platform.items():\n+    if not callable(pbranch):\n+      raise TypeError(f\"lax.platform_dependent: the '{pname}' branch must \"\n+                      \"be a callable.\")\n     if pname == \"gpu\":\n       raise ValueError(\"Use 'cuda' or 'rocm' for lax.platform_dependent.\")\n-    for ps, b in platform_branches:\n+    for ps, b in branches_platforms_list:\n       if b == pbranch:\n         ps.append(pname)\n         break\n     else:\n-      platform_branches.append(([pname], pbranch))\n-\n-  platforms_lists, branches = util.unzip2(platform_branches)\n-  platform_index = platform_index_p.bind(\n-    platforms=tuple(tuple(ps) for ps in platforms_lists),\n-    has_default=(default is not None))\n+      branches_platforms_list.append(([pname], pbranch))\n \n+  platforms_lists, branches = util.unzip2(branches_platforms_list)\n+  branches_platforms: BranchesPlatforms = tuple(tuple(ps) for ps in platforms_lists)\n   if default is not None:\n+    if not callable(default):\n+      raise TypeError(\"lax.platform_dependent: the 'default' branch must \"\n+                      \"be a callable.\")\n     branches = branches + (default,)\n-  # Use a switch, to get the proper transformation rules for free. Since\n-  # platform index has no dependence on the input data, it won't be vectorized\n-  # under vmap.\n-  # If the switch and the platform_index_p above are in the same compilation\n-  # unit then constant-folding will remove the unnecessary branches. However,\n-  # if we run in eager mode the switch below cannot be constant-folded and\n-  # the compilation may fail if some of the branches contain custom calls not\n-  # recognized on the compilation platform. Detect eager mode and keep only the\n-  # needed branch.\n-  try:\n-    # Note/TODO(mvoz): This actually rarely seems to concretize - we could look into\n-    # core.ensure_compile_time_eval to get better single-branch selection.\n-    platform_index_concrete = core.concrete_or_error(operator.index, platform_index)\n-  except core.ConcretizationTypeError:\n-    return switch(platform_index, branches, *args)\n-  else:\n-    assert 0 <= platform_index_concrete < len(branches)\n-    return branches[platform_index_concrete](*args)\n+    branches_platforms = branches_platforms + (None,)  # type: ignore\n+  platform_index = platform_index_p.bind(platforms=branches_platforms)\n+\n+  if core.is_concrete(platform_index):\n+    return branches[int(platform_index)](*args)\n+  return _switch_internal(platform_index, branches, args,\n+                          branches_platforms=branches_platforms)\n+\n \n # A primitive to compute the index of a platform into a list of platforms.\n # Args:\n-#   platforms: Sequence[Sequence[str]]: a sequence of sequences of platform\n-#     names. If the current lowering platform is in one of the inner sequences\n-#     returns the index of that inner sequence in the outer sequence.\n-#   has_default: if True, and if the lowering platform is not found in\n-#     `platforms` then return `len(platforms)`. Otherwise, raise an error.\n+#   platforms: BranchesPlatforms. If the current lowering\n+#     platform is in one of the inner tuples returns the index of that inner\n+#     tuple in the outer tuple.\n platform_index_p = core.Primitive(\"platform_index\")\n platform_index_p.multiple_results = False\n platform_index_p.def_impl(functools.partial(dispatch.apply_primitive,\n@@ -1101,25 +1157,25 @@ def _platform_index_aval(*_, **__):\n \n def _platform_index_lowering(ctx: mlir.LoweringRuleContext,\n                              *,\n-                             platforms: Sequence[Sequence[str]],\n-                             has_default: bool):\n-  def lower_constant(\n-      ctx: mlir.LoweringRuleContext, *, i: int\n-  ) -> Sequence[ir.Value]:\n+                             platforms: BranchesPlatforms):\n+  def lower_constant(ctx: mlir.LoweringRuleContext, *,\n+                     i: int) -> Sequence[ir.Value]:\n     v = mlir.ir_constant(np.int32(i))\n-    assert isinstance(v, ir.Value), v\n     return [v]\n+\n   platform_rules: dict[str, mlir.LoweringRule] = {}\n+  default_rule = None\n   for i, ps in enumerate(platforms):\n     rule = partial(lower_constant, i=i)\n-    for p in ps:\n-      platform_rules[p] = rule\n+    if ps is None:\n+      default_rule = rule\n+    else:\n+      for p in ps:\n+        platform_rules[p] = rule\n \n-  default_rule = (\n-    partial(lower_constant, i=len(platforms)) if has_default else None)\n   return mlir.lower_per_platform(\n     ctx,\n-    f\"platform_index(platforms={platforms}, has_default={has_default})\",\n+    f\"platform_index(platforms={platforms})\",\n     platform_rules, default_rule, effects.no_effects)\n \n mlir.register_lowering(platform_index_p, _platform_index_lowering)\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 776ac1cb8143..9af3cf1e3c0a 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -47,7 +47,7 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.lax import control_flow\n from jax._src.lax import lax as lax_internal\n-from jax._src.lax.control_flow import for_loop\n+from jax._src.lax.control_flow import for_loop, BranchesPlatforms\n from jax._src.lib import version as jaxlib_version\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import arith\n@@ -3128,7 +3128,7 @@ def _while_lowering_rule(\n \n \n @register_lowering_rule(lax.cond_p)\n-def _cond_lowering_rule(ctx: LoweringRuleContext, *args, branches):\n+def _cond_lowering_rule(ctx: LoweringRuleContext, *args, branches, **params):\n   index, *args = args\n   constant_index = _fold_and_get_constant_value(index)\n \n@@ -3898,17 +3898,13 @@ def _pad(val):\n def _platform_index_lowering(\n     ctx: mlir.LoweringRuleContext,\n     *,\n-    platforms: Sequence[Sequence[str]],\n-    has_default: bool,\n+    platforms: BranchesPlatforms,\n ):\n   for i, ps in enumerate(platforms):\n     # note - slightly odd structure here, as platforms is a seq[seq[str]]\n-    if \"mosaic\" in ps:\n+    if \"mosaic\" in ps or ps is None:\n       return ir_constant(i)\n \n-  if has_default:\n-    return ir_constant(len(platforms))\n-\n   raise NotImplementedError(\n       \"No mosaic or default platform indexing rule found.\"\n   )\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex b501693bf627..9ead4f16c1a6 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -2598,7 +2598,10 @@ def _while_lowering_rule(\n @register_lowering_rule(lax.cond_p,\n   mgpu.LoweringSemantics.Lane, gpu_core.PrimitiveSemantics.Warp)\n @register_lowering_rule(lax.cond_p, mgpu.LoweringSemantics.Warpgroup)\n-def _cond_lowering_rule(ctx: LoweringRuleContext, index, *args, branches):\n+def _cond_lowering_rule(ctx: LoweringRuleContext, index, *args, branches,\n+                        **params):\n+  if params:\n+    raise NotImplementedError(\"platform_dependent cond\")\n   index_aval, *_arg_avals = ctx.avals_in\n \n   def _yielded_values(outs, avals):\ndiff --git a/jax/experimental/jax2tf/jax2tf.py b/jax/experimental/jax2tf/jax2tf.py\nindex 4c2f35a95c57..786e021e2ff0 100644\n--- a/jax/experimental/jax2tf/jax2tf.py\n+++ b/jax/experimental/jax2tf/jax2tf.py\n@@ -3062,8 +3062,11 @@ def update_computation(arg1: TfVal, arg2: TfVal) -> TfVal:\n \n \n def _cond(\n-    index: TfVal, *operands: TfVal, branches: Sequence[core.ClosedJaxpr]\n+    index: TfVal, *operands: TfVal, branches: Sequence[core.ClosedJaxpr],\n+    **params\n ) -> Sequence[TfVal]:\n+  if params:\n+    raise NotImplementedError(\"jax2tf conversion for platform_dependent\")\n   # tf.cond needs lambdas with no arguments.\n   branches_tf = [\n       partial(_interpret_jaxpr, jaxpr, *operands,\ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex 422ef769e392..d32d761ee1fa 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -37,8 +37,10 @@\n from jax.ad_checkpoint import checkpoint as new_checkpoint, checkpoint_policies\n import jax.numpy as jnp  # scan tests use numpy\n import jax.scipy as jsp\n+from jax._src import dispatch\n from jax._src.lax import control_flow as lax_control_flow\n from jax._src.lax.control_flow import for_loop\n+from jax._src.interpreters import batching\n from jax._src.interpreters import mlir\n \n jax.config.parse_flags_with_absl()\n@@ -137,6 +139,36 @@ def scan_reference(f, init, xs):\n     lambda ctx, x: mlir.hlo.CustomCallOp(\n         [x.type], [x],\n         call_target_name=mlir.ir.StringAttr.get(\"__testing_non_existent_custom_call\")).results)\n+batching.primitive_batchers[prim_non_existent_custom_call] = (\n+    lambda batched_args, batch_dims: (prim_non_existent_custom_call.bind(batched_args[0]),\n+                                      batch_dims[0]))\n+\n+# A JAX primitive that triggers error when lowering on unintended platforms\n+prim_with_lowering_error = core.Primitive(\"__testing_prim_with_lowering_error\")\n+prim_with_lowering_error.def_abstract_eval(lambda x_aval, **_: x_aval)\n+def prim_with_lowering_error_lowering(platform: str,\n+                                      ctx: mlir.LoweringRuleContext, x, *,\n+                                      only_on: str):\n+  if platform != only_on:\n+    raise ValueError(f\"prim_with_lowering_error with only_on={only_on} lowered for {platform}\")\n+  return mlir.hlo.SineOp(x).results\n+def prim_with_lowering_error_batch_rule(batched_args, batch_dims, **params):\n+  xs, = batched_args\n+  xs_bdim, = batch_dims\n+  return prim_with_lowering_error.bind(xs, **params), xs_bdim\n+\n+batching.primitive_batchers[prim_with_lowering_error] = prim_with_lowering_error_batch_rule\n+\n+mlir.register_lowering(\n+    prim_with_lowering_error,\n+    partial(prim_with_lowering_error_lowering, \"cpu\"),\n+    platform=\"cpu\")\n+mlir.register_lowering(\n+    prim_with_lowering_error,\n+    partial(prim_with_lowering_error_lowering, \"tpu\"),\n+    platform=\"tpu\")\n+prim_with_lowering_error.def_impl(partial(dispatch.apply_primitive,\n+                                          prim_with_lowering_error))\n \n \n class LaxControlFlowTest(jtu.JaxTestCase):\n@@ -1378,7 +1410,7 @@ def f(x):\n   @parameterized.named_parameters(\n       {\"testcase_name\": f\"_{name}\", \"cond\": cond}\n       for cond, name in COND_IMPLS)\n-  def testCondGrad2(self, cond):\n+  def testCondGrad2(self, cond=cond_with_new_checkpoint):\n     def f_ref(x):\n       z = jnp.array([1., 2.], x.dtype) * x if x[0] < 2 else jnp.sin(x)\n       return z.sum()\n@@ -2905,18 +2937,13 @@ def f(x):\n     x = np.arange(3, dtype=np.float32)\n     lowered = jax.jit(f).lower(x)\n     stablehlo = lowered.as_text()\n-    self.assertIn(\"stablehlo.case\", stablehlo)\n-    self.assertIn(\"stablehlo.sine\", stablehlo)\n-    self.assertIn(\"stablehlo.cosine\", stablehlo)\n-\n-    # The HLO has been canonicalized and contains only the branch we need\n-    hlo = lowered.as_text(\"hlo\")\n+    # The StableHLO contains only the branch we need\n     if jtu.device_under_test() == \"cpu\":\n-      self.assertIn(\" sine\", hlo)\n-      self.assertNotIn(\" cosine\", hlo)\n+      self.assertIn(\"stablehlo.sine\", stablehlo)\n+      self.assertNotIn(\"stablehlo.cosine\", stablehlo)\n     else:\n-      self.assertNotIn(\" sine\", hlo)\n-      self.assertIn(\" cosine\", hlo)\n+      self.assertNotIn(\"stablehlo.sine\", stablehlo)\n+      self.assertIn(\"stablehlo.cosine\", stablehlo)\n \n   def test_platform_dependent_with_non_existent_custom_call(self):\n     if not jtu.test_device_matches([\"cpu\"]):\n@@ -2939,8 +2966,7 @@ def f(x):\n \n     x = np.arange(3, dtype=np.float32)\n     hlo = str(jax.jit(f).lower(x).compiler_ir())\n-    occurrences = re.findall(prim_non_existent_custom_call.name, hlo)\n-    self.assertLen(occurrences, 3)\n+    self.assertNotIn(prim_non_existent_custom_call.name, hlo)\n \n     res_eager = f(x)\n     self.assertAllClose(res_eager, 3. * np.sin(x))\n@@ -2956,6 +2982,26 @@ def f(x):\n     res_grad = jax.grad(f)(1.)\n     self.assertAllClose(res_grad, 3. * np.cos(1.))\n \n+  def test_platform_dependent_with_primitive_with_lowering_error(self):\n+    if not jtu.test_device_matches([\"cpu\", \"tpu\"]):\n+      self.skipTest(\"Only for CPU and TPU\")\n+\n+    def f(x):\n+      return lax.platform_dependent(\n+          x,\n+          # Check that we only lower on the intended platform\n+          cpu=lambda x: prim_with_lowering_error.bind(x, only_on=\"cpu\"),\n+          tpu=lambda x: prim_with_lowering_error.bind(x, only_on=\"tpu\"))\n+\n+    self.assertAllClose(np.sin(1.), f(1.))  # Eager\n+    self.assertAllClose(np.sin(1.), jax.jit(f)(1.))\n+    self.assertAllClose(np.sin(1.), lax.cond(True, f, lambda x: x, 1.))\n+    self.assertAllClose(1., lax.cond(False, f, lambda x: x, 1.))\n+    self.assertAllClose((0., np.sin(np.arange(8.))),\n+                        lax.scan(lambda carry, x: (carry, f(x)),\n+                                 0., np.arange(8.)))\n+    self.assertAllClose(np.sin(np.arange(8.)), jax.vmap(f)(np.arange(8.)))\n+\n   def test_platform_dependent_multiple_identical_branches(self):\n     x = np.arange(3, dtype=np.float32)\n     def f(x):\n@@ -2965,13 +3011,14 @@ def f(x):\n         tpu=jnp.sin,\n         default=lambda x: x)\n     res = f(x)\n+    on_cpu_tpu = jtu.device_under_test() in [\"cpu\", \"tpu\"]\n     self.assertAllClose(\n       res,\n-      np.sin(x) if jtu.device_under_test() in [\"cpu\", \"tpu\"] else x)\n-    # We only lower the common branches once\n+      np.sin(x) if on_cpu_tpu else x)\n+\n     stablehlo = jax.jit(f).lower(x).as_text()\n     sines = re.findall(r\"stablehlo.sine\", stablehlo)\n-    self.assertEqual(1, len(sines))\n+    self.assertEqual(1 if on_cpu_tpu else 0, len(sines))\n \n   def test_platform_dependent_no_default(self):\n     ctx = contextlib.ExitStack()\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add `block_until_ready()` to FAQ code snippet.\n\nPiperOrigin-RevId: 757963427",
        "output": "```diff\nCommit: 189ba3a99959245724105217a72902cc50e38b14\nDate: 2025-05-12T23:44:05Z\nURL: https://github.com/jax-ml/jax/commit/189ba3a99959245724105217a72902cc50e38b14\nFiles changed: 1\nAdditions: +3, Deletions: -2\ndiff --git a/docs/faq.rst b/docs/faq.rst\nindex f5d43d25afb6..25d1d9ffab57 100644\n--- a/docs/faq.rst\n+++ b/docs/faq.rst\n@@ -422,7 +422,6 @@ for comparing JAX versus NumPy, making using of IPython's convenient\n `%time and %timeit magics`_::\n \n     import numpy as np\n-    import jax.numpy as jnp\n     import jax\n \n     def f(x):  # function we're benchmarking (works in both NumPy & JAX)\n@@ -431,7 +430,9 @@ for comparing JAX versus NumPy, making using of IPython's convenient\n     x_np = np.ones((1000, 1000), dtype=np.float32)  # same as JAX default dtype\n     %timeit f(x_np)  # measure NumPy runtime\n \n-    %time x_jax = jax.device_put(x_np)  # measure JAX device transfer time\n+    # measure JAX device transfer time\n+    %time x_jax = jax.device_put(x_np).block_until_ready()\n+\n     f_jit = jax.jit(f)\n     %time f_jit(x_jax).block_until_ready()  # measure JAX compilation time\n     %timeit f_jit(x_jax).block_until_ready()  # measure JAX runtime\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Support pltpu.roll on sublanes when not all lanes are used.\n\nPiperOrigin-RevId: 757942183",
        "output": "```diff\nCommit: e43432128b3be8e4e94a82c3c6cb6a24ca44863d\nDate: 2025-05-12T22:46:43Z\nURL: https://github.com/jax-ml/jax/commit/e43432128b3be8e4e94a82c3c6cb6a24ca44863d\nFiles changed: 3\nAdditions: +157, Deletions: -9\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex b8ba61e7c914..d625e8bf4d6f 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -36,6 +36,7 @@ limitations under the License.\n #include \"llvm/ADT/APInt.h\"\n #include \"llvm/ADT/ArrayRef.h\"\n #include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/SmallVectorExtras.h\"\n #include \"llvm/ADT/StringMap.h\"\n #include \"llvm/ADT/iterator_range.h\"\n@@ -2141,16 +2142,41 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n   if (layout_in != layout) {\n     return op.emitOpError(\"Not implemented: unsupported layout for input\");\n   }\n-  if (layout_out != layout) {\n+  LayoutOffsets expected_offsets_out = layout_in.offsets();\n+  auto shift = getIntConst(amount, /*silent=*/true);\n+  const bool has_static_shift = succeeded(shift);\n+  int rotated_tiled_dim = op.getDimension() - (op.getType().getRank() - 2);\n+  bool has_padding_along_rotation =\n+      (rotated_tiled_dim == 0 || rotated_tiled_dim == 1) &&\n+      op.getType().getShape()[op.getDimension()] %\n+              layout.tiling()[rotated_tiled_dim] !=\n+          0;\n+  if (has_static_shift && has_padding_along_rotation) {\n+    // We checked above that there are no implicit dims.\n+    const int64_t dim_size = op.getType().getShape()[op.getDimension()];\n+    // TODO(b/337384645): Currently we assume {0, 0} offsets in the input\n+    // layout. Relax this assumption.\n+    expected_offsets_out[rotated_tiled_dim] =\n+        (dim_size - (shift.value() % dim_size)) %\n+        layout.tiling()[rotated_tiled_dim];\n+  }\n+  if (layout_out.bitwidth() != layout.bitwidth() ||\n+      layout_out.offsets() != expected_offsets_out ||\n+      layout_out.tiling() != layout.tiling() ||\n+      layout_out.implicit_dim() != layout.implicit_dim()) {\n     return op.emitOpError(\"Not implemented: unsupported layout for output\");\n   }\n   auto vty = op.getResult().getType();\n   if (vty.getRank() < 2) {\n     return op.emitOpError(\"Not implemented: unsupported 1D shape\");\n   }\n-  if (*(vty.getShape().end() - 2) % *(layout.tiling().end() - 2) != 0 ||\n-      *(vty.getShape().end() - 1) % *(layout.tiling().end() - 1) != 0) {\n-    return op.emitOpError(\"Not implemented: unsupported unaliged shape\");\n+  // TODO(b/411170715): Allow sublane rotation once the bug is fixed.\n+  // TODO(b/337384645): Support non-zero stride.\n+  if (has_padding_along_rotation &&\n+      (!has_static_shift ||\n+       (rotated_tiled_dim == 0 ||\n+        (rotated_tiled_dim == 1 && op.getStride().value_or(0) != 0)))) {\n+    return op.emitOpError(\"Not implemented: unsupported unaligned shape\");\n   }\n \n   ImplicitLocOpBuilder builder(op.getLoc(), op.getOperation());\n@@ -2277,6 +2303,88 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n     return concatenate(chunks, axis);\n   };\n \n+  // Applies lazy rotation (see go/pltpu-roll for details).\n+  auto lazyRotate = [&](const xla::Array<Value> &vregs, int64_t shift,\n+                        int axis) {\n+    const int tiling_dim = axis - (vregs.num_dimensions() - 2);\n+    const int64_t tile_size = ctx.target_shape[tiling_dim];\n+    const int64_t input_size = vty.getShape()[axis];\n+    const int64_t normalized_shift = shift % input_size;\n+    const int64_t start_idx = input_size - normalized_shift;\n+    const int64_t start_vreg_idx = start_idx / tile_size;\n+    const int64_t valid_amount = input_size % tile_size;\n+\n+    // We start with the following:\n+    //\n+    // vregs:\n+    // +------+ +------+ +------+\n+    // | 0 | |  1   | | 2 XXX|\n+    // +------+ +------+ +------+\n+    //\n+    // where XXX is the padding and  is the prefix of the same size as the\n+    // padding.\n+\n+    // After concatenation:\n+    //\n+    // concat:\n+    // +------+ +------+ +------+ +------+ +------+ +------+\n+    // | 0 | |  1   | | 2 XXX| | 0 | |  1   | | 2 XXX|\n+    // +------+ +------+ +------+ +------+ +------+ +------+\n+    auto concat = concatenate({vregs, vregs}, axis);\n+    auto chunks = split(concat, axis);\n+    int64_t original_num_chunks = chunks.size() / 2;\n+\n+    Value rotate_amount = mlirI32Const(valid_amount);\n+    SmallVector<Value, 2> low = {mlirIndexConst(0), mlirIndexConst(0)};\n+    low[tiling_dim] = mlirIndexConst(valid_amount);\n+    auto mask = builder.create<tpu::CreateMaskOp>(\n+        VectorType::get(ctx.target_shape, builder.getI1Type()), low,\n+        /*high=*/\n+        ArrayRef<Value>{mlirIndexConst(ctx.target_shape[0]),\n+                        mlirIndexConst(ctx.target_shape[1])});\n+    // overwrite padding in the last vreg with valid data from the first vreg,\n+    // yielding:\n+    //\n+    // +------+ +------+ +------+ +------+ +------+ +------+\n+    // | 0 | |  1   | | 2 XXX| | 0 | |  1   | | 2 |\n+    // +------+ +------+ +------+ +------+ +------+ +------+\n+    chunks.back().Each([&](absl::Span<const int64_t> idxs, Value *v) {\n+      *v = builder.create<arith::SelectOp>(\n+          mask,\n+          builder.create<tpu::DynamicRotateOp>(\n+              res_vreg_ty, chunks.front()(idxs), rotate_amount, tiling_dim,\n+              nullptr, nullptr),\n+          *v);\n+    });\n+    // rotate the vregs starting from the middle vreg and then blend the vregs\n+    // to overwrite the padding, yielding:\n+    //\n+    // +------+ +------+ +---+ +------+ +------+ +------+\n+    // | 0 | |  1   | | 2 | | 0 | |  1   | | 2 |\n+    // +------+ +------+ +---+ +------+ +------+ +------+\n+    for (int64_t i = original_num_chunks; i < chunks.size(); ++i) {\n+      chunks[i].Each([&](absl::Span<const int64_t> idxs, Value *v) {\n+        *v = builder.create<tpu::DynamicRotateOp>(\n+            res_vreg_ty, *v, rotate_amount, tiling_dim, nullptr, nullptr);\n+      });\n+    }\n+    for (int64_t i = original_num_chunks - 1; i < chunks.size() - 1; ++i) {\n+      chunks[i].Each([&](absl::Span<const int64_t> idxs, Value *v) {\n+        *v = builder.create<arith::SelectOp>(mask, chunks[i + 1](idxs), *v);\n+      });\n+    }\n+    SmallVector<int64_t> result_dimensions =\n+        layout_out.tileArrayImplicitShape(vty.getShape(), ctx.target_shape);\n+    // assemble the result\n+    xla::Array<Value> result(result_dimensions);\n+    SmallVector<int64_t> starts(result.num_dimensions(), 0);\n+    for (int64_t i = 0; i < result_dimensions[axis]; ++i) {\n+      starts[axis] = i;\n+      result.UpdateSlice(chunks[i + start_vreg_idx], starts);\n+    }\n+    return result;\n+  };\n+\n   std::function<xla::Array<Value>(const xla::Array<Value> &, Value, int, int)>\n       rotate;\n   rotate = [&](const xla::Array<Value> &vregs, Value shift, int axis,\n@@ -2290,6 +2398,9 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n     if (auto shift_cst = getIntConst(shift, /*silent=*/true);\n         succeeded(shift_cst)) {\n       int64_t static_shift = shift_cst.value();\n+      if (has_padding_along_rotation) {\n+        return lazyRotate(vregs, static_shift, axis);\n+      }\n       if (tiling_dim >= 0) {\n         shift = mlirI32Const(static_shift % ctx.target_shape[tiling_dim]);\n         static_shift /= ctx.target_shape[tiling_dim];\n@@ -2379,7 +2490,9 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n     return result;\n   };\n \n-  xla::Array<Value> out_tiles(in_tiles.dimensions());\n+  SmallVector<int64_t> out_dimensions =\n+      layout_out.tileArrayImplicitShape(vty.getShape(), ctx.target_shape);\n+  xla::Array<Value> out_tiles(out_dimensions);\n   const auto dim = op.getDimension();\n   amount = modI(amount, vty.getDimSize(dim));\n \ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex 2e4c1c9c48a9..f42cfb139a37 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -757,9 +757,28 @@ class VectorLayoutInferer {\n     if (op.getType().getRank() < 2) {\n       NYI(\"Unsupported 1D shape\");\n     }\n+    // TODO(b/337384645): Currently we assume {0, 0} offsets in the input\n+    // layout. Relax this assumption.\n     auto layout = VectorLayout(bitwidth, {0, 0}, nativeTiling(bitwidth),\n                                ImplicitDim::kNone);\n-    setLayout(op, {layout, kNoLayout}, layout);\n+    // Calculate the offsets for the output layout.\n+    LayoutOffsets offsets_out = layout.offsets();\n+    // We assume there are no implicit dims.\n+    int tiling_dim = op.getDimension() - (op.getType().getRank() - 2);\n+    if (auto amount = op.getAmount().getDefiningOp<arith::ConstantOp>();\n+        amount && (tiling_dim == 0 || tiling_dim == 1)) {\n+      if (auto integer_attr = dyn_cast<IntegerAttr>(amount.getValue())) {\n+        const int64_t tile_size = layout.tiling()[tiling_dim];\n+        const int64_t dim_size = op.getType().getShape()[op.getDimension()];\n+        const int64_t shift = integer_attr.getValue().getSExtValue();\n+        if (dim_size % tile_size != 0) {\n+          offsets_out[tiling_dim] = (dim_size - (shift % dim_size)) % tile_size;\n+        }\n+      }\n+    }\n+    auto out_layout = VectorLayout(bitwidth, offsets_out,\n+                                   nativeTiling(bitwidth), ImplicitDim::kNone);\n+    setLayout(op, {layout, kNoLayout}, out_layout);\n     return success();\n   }\n \ndiff --git a/tests/pallas/tpu_pallas_test.py b/tests/pallas/tpu_pallas_test.py\nindex a70aa19bda4d..83f21bca7fc1 100644\n--- a/tests/pallas/tpu_pallas_test.py\n+++ b/tests/pallas/tpu_pallas_test.py\n@@ -2895,9 +2895,9 @@ def kernel(x_ref, out_ref):\n     )(x)\n     np.testing.assert_array_equal(out, state_utils.bitcast(x, jnp.uint32))\n \n-  @only_passes_in_interpret()\n-  def test_roll_partial(self):\n-    \"\"\"b/337384645\"\"\"\n+  def test_roll_partial_with_static_shift(self):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 15):\n+      self.skipTest('Needs a newer libtpu')\n     x = np.arange(8192, dtype=jnp.float32).reshape(128, 64)\n \n     def kernel(x_ref, out_ref):\n@@ -2908,6 +2908,22 @@ def kernel(x_ref, out_ref):\n     )(x)\n     np.testing.assert_array_equal(out, np.roll(x, 3, 1))\n \n+  def test_roll_partial_with_dynamic_shift(self):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 15):\n+      self.skipTest('Needs a newer libtpu')\n+    if self.INTERPRET:\n+      self.skipTest('Test only applies to non-interpret mode.')\n+    x = np.arange(8192, dtype=jnp.float32).reshape(128, 64)\n+\n+    def kernel(x_ref, out_ref):\n+      amount = x_ref[0, 0].astype(jnp.int32)\n+      out_ref[...] = pltpu.roll(x_ref[...], amount, 1)\n+\n+    with self.assertRaisesRegex(Exception, 'unsupported unaligned shape'):\n+      _ = self.pallas_call(\n+          kernel, out_shape=jax.ShapeDtypeStruct((128, 64), jnp.float32)\n+      )(x)\n+\n   @only_passes_in_interpret()\n   def test_retiling1(self):\n     \"\"\"b/352626602\"\"\"\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic] Allowed registering lowering per `pltpu.KernelType`\n\nPiperOrigin-RevId: 757925207",
        "output": "```diff\nCommit: bc3a3f0b24e3cb3c8c329053be12e3311b9ef2ff\nDate: 2025-05-12T22:00:25Z\nURL: https://github.com/jax-ml/jax/commit/bc3a3f0b24e3cb3c8c329053be12e3311b9ef2ff\nFiles changed: 3\nAdditions: +46, Deletions: -19\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 1ea5a048a17e..776ac1cb8143 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -15,7 +15,7 @@\n \"\"\"Module for lowering JAX to Mosaic-compatible MLIR dialects.\"\"\"\n from __future__ import annotations\n \n-from collections.abc import Callable, Sequence\n+from collections.abc import Callable, Collection, Sequence\n import contextlib\n import dataclasses\n import functools\n@@ -168,6 +168,7 @@ class LoweringContext:\n   block_shapes: list[tuple[int | pallas_core.Squeezed, ...]]\n   name_stack: source_info_util.NameStack\n   mesh_context: MeshContext | None\n+  kernel_type: tpu_core.KernelType\n   traceback_caches: mlir.TracebackCaches\n   for_verification: bool\n   forward_compatible: bool\n@@ -324,7 +325,7 @@ def ir_constant(x, mlir_type=None):\n   raise NotImplementedError(x.dtype)\n \n \n-lowering_rules = {}\n+lowering_rules = {kernel_type: {} for kernel_type in tpu_core.KernelType}\n skip_mlir_conversions = set()\n \n \n@@ -332,10 +333,14 @@ def ir_constant(x, mlir_type=None):\n \n \n def register_lowering_rule(\n-    prim: jax_core.Primitive, *, ensure_mlir_values: bool = True\n+    prim: jax_core.Primitive,\n+    *,\n+    kernel_types: Collection[tpu_core.KernelType] = (tpu_core.KernelType.TC,),\n+    ensure_mlir_values: bool = True,\n ) -> Callable[[T], T]:\n   def decorator(rule: T) -> T:\n-    lowering_rules[prim] = rule\n+    for kernel_type in kernel_types:\n+      lowering_rules[kernel_type][prim] = rule\n     if not ensure_mlir_values:\n       skip_mlir_conversions.add(prim)\n     return rule\n@@ -673,6 +678,7 @@ def lower_jaxpr_to_module(\n     jaxpr: jax_core.Jaxpr,\n     *,\n     dimension_semantics: Sequence[tpu_core.DimensionSemantics] | None,\n+    kernel_type: tpu_core.KernelType,\n     mesh: mesh_lib.Mesh | None = None,\n     for_verification: bool = False,\n     dynamic_shape_replacement_enabled: bool = False,\n@@ -724,6 +730,7 @@ def dynamic_shape_replacement_fn(\n       jaxpr,\n       mosaic_grid_mapping=mosaic_grid_mapping,\n       name=\"main\",\n+      kernel_type=kernel_type,\n       for_verification=for_verification,\n       forward_compatible=lowering_context.is_forward_compat(),\n       dynamic_shape_replacement_fn=dynamic_shape_replacement_fn,\n@@ -759,6 +766,7 @@ def dynamic_shape_replacement_fn(\n           bm.block_aval,\n           name=func_name,\n           mosaic_grid_mapping=mosaic_grid_mapping,\n+          kernel_type=kernel_type,\n           for_verification=for_verification,\n           forward_compatible=lowering_context.is_forward_compat(),\n           dynamic_shape_replacement_fn=dynamic_shape_replacement_fn,\n@@ -906,8 +914,9 @@ def lower_jaxpr_to_transform_func(\n     *,\n     name: str,\n     mosaic_grid_mapping: MosaicGridMapping,\n+    kernel_type: tpu_core.KernelType,\n     for_verification: bool,\n-     forward_compatible: bool,\n+    forward_compatible: bool,\n     dynamic_shape_replacement_fn: (\n         Callable[[tuple[jax.DimSize, ...]], tuple[int, ...]] | None\n     ) = None,\n@@ -942,6 +951,7 @@ def body_func(*args):\n         arg_block_shapes,\n         source_info_util.NameStack(),\n         mesh_context=mesh_context,\n+        kernel_type=kernel_type,\n         traceback_caches=mlir.TracebackCaches(),\n         for_verification=for_verification,\n         forward_compatible=forward_compatible,\n@@ -966,11 +976,19 @@ def body_func(*args):\n   return body.func_op\n \n \n+lower_jaxpr_to_func_fns = {}\n+\n+\n+def register_jaxpr_to_func(kernel_type: tpu_core.KernelType):\n+  lower_jaxpr_to_func_fns[kernel_type] = lower_jaxpr_to_func\n+\n+\n def lower_jaxpr_to_func(\n     jaxpr: jax_core.Jaxpr,\n     *,\n     mosaic_grid_mapping: MosaicGridMapping,\n     name: str,\n+    kernel_type: tpu_core.KernelType,\n     for_verification: bool,\n     forward_compatible: bool,\n     dynamic_shape_replacement_fn: (\n@@ -1012,6 +1030,7 @@ def body_func(*args):\n         arg_block_shapes,\n         source_info_util.NameStack(),\n         mesh_context=mesh_context,\n+        kernel_type=kernel_type,\n         traceback_caches=mlir.TracebackCaches(),\n         for_verification=for_verification,\n         forward_compatible=forward_compatible,\n@@ -1119,7 +1138,7 @@ def write_env(var: jax_core.Var, val):\n     loc = mlir._source_info_to_location(ctx, eqn.primitive, source_info)\n     with (source_info_util.user_context(eqn.source_info.traceback), loc,\n           eqn.ctx.manager):\n-      if eqn.primitive in lowering_rules:\n+      if eqn.primitive in lowering_rules[ctx.kernel_type]:\n         if eqn.primitive not in skip_mlir_conversions:\n           invals = [_ensure_mlir_value(x, v.aval)\n                     for x, v in zip(invals, eqn.invars)]\n@@ -1142,7 +1161,7 @@ def write_env(var: jax_core.Var, val):\n           tpu.trace_start(message=name, level=10)\n \n         try:\n-          ans = lowering_rules[eqn.primitive](\n+          ans = lowering_rules[ctx.kernel_type][eqn.primitive](\n               rule_context, *invals, **eqn.params\n           )\n         except LoweringException:\n@@ -1162,9 +1181,10 @@ def write_env(var: jax_core.Var, val):\n           raise new_error from e\n       else:\n         raise NotImplementedError(\n-            \"Unimplemented primitive in Pallas TPU lowering: \"\n-            f\"{eqn.primitive.name}. \"\n-            \"Please file an issue on https://github.com/jax-ml/jax/issues.\")\n+            \"Unimplemented primitive in Pallas TPU lowering for\"\n+            f\" {ctx.kernel_type}: {eqn.primitive.name}. Please file an issue on\"\n+            \" https://github.com/jax-ml/jax/issues.\"\n+        )\n       if eqn.primitive.multiple_results:\n         foreach(write_env, eqn.outvars, ans)\n       else:\n@@ -1889,7 +1909,9 @@ def _broadcast_to_lowering_rule(\n   )\n \n \n-@register_lowering_rule(lax.broadcast_in_dim_p)\n+@register_lowering_rule(\n+    lax.broadcast_in_dim_p, kernel_types=[*tpu_core.KernelType]\n+)\n def _broadcast_in_dim_lowering_rule(\n     ctx: LoweringRuleContext, val, *, shape, broadcast_dimensions, sharding\n ):\n@@ -2139,7 +2161,9 @@ def _convert_helper(x, *, to_dtype):\n   raise NotImplementedError(f\"Unsupported cast: {from_dtype} -> {to_dtype}\")\n \n \n-@register_lowering_rule(lax.convert_element_type_p)\n+@register_lowering_rule(\n+    lax.convert_element_type_p, kernel_types=[*tpu_core.KernelType]\n+)\n def _convert_element_type_lowering_rule(\n     ctx: LoweringRuleContext, x, *, new_dtype, weak_type, sharding\n ):\n@@ -2397,7 +2421,9 @@ def _bcast(x, y, x_aval, y_aval, out_aval):\n   return x, y\n \n \n-@register_lowering_rule(lax.add_p, ensure_mlir_values=False)\n+@register_lowering_rule(\n+    lax.add_p, kernel_types=[*tpu_core.KernelType], ensure_mlir_values=False\n+)\n @register_lowering_rule(ad_util.add_any_p, ensure_mlir_values=False)\n def _add_lowering_rule(ctx: LoweringRuleContext, x, y):\n   x, y = _bcast(x, y, ctx.avals_in[0], ctx.avals_in[1], ctx.avals_out[0])\n@@ -2806,7 +2832,9 @@ def _cmp_lowering_rule(primitive, ctx: LoweringRuleContext, x, y):\n \n \n for prim in [lax.eq_p, lax.ne_p, lax.lt_p, lax.le_p, lax.gt_p, lax.ge_p]:\n-  register_lowering_rule(prim)(functools.partial(_cmp_lowering_rule, prim))\n+  register_lowering_rule(prim, kernel_types=[*tpu_core.KernelType])(\n+      functools.partial(_cmp_lowering_rule, prim)\n+  )\n \n \n @register_lowering_rule(lax.and_p, ensure_mlir_values=False)\n@@ -3530,7 +3558,7 @@ def _dma_wait_lowering_rule(ctx: LoweringRuleContext, *args, tree,\n   return []\n \n \n-@register_lowering_rule(lax.axis_index_p)\n+@register_lowering_rule(lax.axis_index_p, kernel_types=[*tpu_core.KernelType])\n def _axis_index_rule(ctx: LoweringRuleContext, *, axis_name: Hashable):\n   grid_names = ctx.lowering_context.grid_names\n   if grid_names and axis_name in grid_names:\ndiff --git a/jax/_src/pallas/mosaic/pallas_call_registration.py b/jax/_src/pallas/mosaic/pallas_call_registration.py\nindex 5de917d077ce..74253e809a35 100644\n--- a/jax/_src/pallas/mosaic/pallas_call_registration.py\n+++ b/jax/_src/pallas/mosaic/pallas_call_registration.py\n@@ -150,6 +150,7 @@ def lower_module(for_verification: bool):\n           grid_mapping,\n           jaxpr,\n           dimension_semantics=mosaic_params.dimension_semantics,\n+          kernel_type=mosaic_params.kernel_type,\n           mesh=jax_mesh,\n           for_verification=for_verification,\n           dynamic_shape_replacement_enabled=pallas_core.dynamic_shapes_export_enabled(),\ndiff --git a/jax/_src/pallas/mosaic/verification.py b/jax/_src/pallas/mosaic/verification.py\nindex 08ff58770804..f45f36a473e9 100644\n--- a/jax/_src/pallas/mosaic/verification.py\n+++ b/jax/_src/pallas/mosaic/verification.py\n@@ -596,11 +596,10 @@ def _assume_abstract_eval(x, y):\n   assert jax_core.typematch(x, y)\n   return x\n \n+@lowering.register_lowering_rule(assume_p)\n def _assume_lowering(ctx: lowering.LoweringRuleContext, x, y):\n   return y if ctx.lowering_context.for_verification else x\n \n-lowering.lowering_rules[assume_p] = _assume_lowering  # type: ignore\n-\n def assume(normally, *, when_verifying):\n   return assume_p.bind(normally, when_verifying)\n \n@@ -613,6 +612,7 @@ def _pretend_abstract_eval(*_, **params):\n   del params  # Unused.\n   return ()\n \n+@lowering.register_lowering_rule(pretend_p)\n def _pretend_lowering(ctx: lowering.LoweringRuleContext, *flat_args, tree):\n   if ctx.lowering_context.for_verification:\n     (base_read_refs, transforms) = tree_util.tree_unflatten(tree, flat_args)\n@@ -631,8 +631,6 @@ def _pretend_lowering(ctx: lowering.LoweringRuleContext, *flat_args, tree):\n     ir.Operation.create(\"verification.pretend\", operands=read_refs)\n   return ()\n \n-lowering.lowering_rules[pretend_p] = _pretend_lowering  # type: ignore\n-\n def pretend(read_refs):\n   refs, transforms = unzip2(\n       primitives._get_ref_and_transforms(r) for r in read_refs\n\n```",
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Do all mesh checks at shard_map **call time** instead of at construction time.\n\n**Why do this change?**\n\n* If a `smap`/`shard_map` is constructed NOT under a mesh context but called under a mesh context, we will error at construction time. After this change, we won't.\n\n* If a `smap`/`shard_map` is nested but constructed at the top level, it will be bound with the mesh available at construction time instead of at call time. This is not ideal since while nesting one axis at a time, manualness of a mesh changes for the nested shard_map call. So we need to look at the mesh at call time. For example:\n\n```\n@jtu.with_explicit_mesh((2, 2), ('x', 'y'),\n                        axis_types=(AxisType.Explicit, AxisType.Auto))\ndef test_smap_auto_explicit_nest_mesh_call_time(self, mesh):\n  @partial(smap, in_axes=1, out_axes=1, axis_name='x')\n  def g(b):\n    return jnp.sin(b)\n\n  @partial(smap, in_axes=0, out_axes=0, axis_name='y')\n  def f(a):\n    self.assertEqual(a.aval.vma, {'y'})\n    b = a * 2\n    return g(b)\n\n  arr = jax.device_put(np.arange(16).reshape(8, 2), P('y'))\n  jax.jit(f)(arr)  # doesn't crash\n```\n\nIn the above example, before this change, `g` would be bound with the mesh whose axis_types were `Explict, Auto` but since `g` is being used inside a `smap` i.e. it's nested, it needs to be bound with the mesh at call time which would have axis_types `Explicit, Manual` for the computation to be correct.\n\nOne minor point regarding this change is that since `axis_name` or `in_specs/out_specs` refer to mesh axis names, the shard_map would need to be called with the correct mesh. Before this errored out at construction time but now it'll error out at call time.\n\nPiperOrigin-RevId: 757912835",
        "output": "```diff\nCommit: 6fc9e17a9e7d46dad326bd595ad77218fa5389e5\nDate: 2025-05-12T21:26:47Z\nURL: https://github.com/jax-ml/jax/commit/6fc9e17a9e7d46dad326bd595ad77218fa5389e5\nFiles changed: 2\nAdditions: +76, Deletions: -59\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex e9f2d3b6072e..b772a3de239e 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -164,82 +164,30 @@ def smap(f, /, *, in_axes=Infer, out_axes, axis_name: AxisName):\n   if not all(isinstance(l, int) for l in tree_leaves(out_axes)):\n     raise TypeError(\"smap out_axes must be an int, None, or (nested) container \"\n                     f\"with those types as leaves, but got {out_axes}.\")\n-  mesh = get_abstract_mesh()\n-  if mesh.empty:\n-    raise ValueError(\n-        \"The context mesh cannot be empty. Use\"\n-        \" `jax.sharding.use_mesh(mesh)` to enter into a mesh context.\")\n-  if mesh._name_to_type[axis_name] != AxisType.Explicit and in_axes is Infer:\n-    raise TypeError(\n-        f\"in_axes was not specified when {axis_name=} was of type\"\n-        f\" {mesh._name_to_type[axis_name]}.\")\n \n   in_specs = (None if in_axes is Infer else\n               tree_map(partial(_axes_to_pspec, axis_name), in_axes,\n                        is_leaf=lambda x: x is None))\n   out_specs = tree_map(partial(_axes_to_pspec, axis_name), out_axes,\n                        is_leaf=lambda x: x is None)\n-  return shard_map(f, axis_names={axis_name}, in_specs=in_specs,\n-                   out_specs=out_specs)\n+  return _shard_map(f, mesh=None, in_specs=in_specs, out_specs=out_specs,\n+                    axis_names={axis_name}, check_vma=True, _smap=True)\n \n \n def _shard_map(f: Callable, *, mesh: Mesh | AbstractMesh | None,\n                in_specs: Specs, out_specs: Specs | Callable[[], Specs],\n                axis_names: Set[AxisName], check_vma: bool,\n-               _skip_mesh_check: bool = False) -> Callable:\n+               _skip_mesh_check: bool = False, _smap: bool = False) -> Callable:\n   if not callable(f):\n     raise TypeError(\"shard_map requires a callable for its first argument, \"\n                     f\"but got {f} of type {type(f)}.\")\n \n-  if mesh is None:\n-    mesh = get_abstract_mesh()\n-    if mesh.empty:\n-      raise ValueError(\n-          \"The context mesh cannot be empty. Either use\"\n-          \" `jax.sharding.use_mesh(mesh)` to enter into a mesh context or pass\"\n-          \" a mesh to `shard_map` via the `mesh` keyword argument.\")\n-  else:\n-    ctx_mesh = get_abstract_mesh()\n-    if (not _skip_mesh_check and not ctx_mesh.empty and\n-        mesh.abstract_mesh != ctx_mesh):\n-      raise ValueError(\n-          f\"The context mesh {ctx_mesh} should match the mesh passed to\"\n-          f\" shard_map {mesh}\")\n-\n-  if not isinstance(mesh, (Mesh, AbstractMesh)):\n-    raise TypeError(\"shard_map requires a `jax.sharding.Mesh` or a \"\n-                    \"`jax.sharding.AbstractMesh` instance for its \"\n-                    f\"second argument, but got {mesh} of type {type(mesh)}.\")\n-\n-  if not isinstance(axis_names, (frozenset, set)):\n-    raise TypeError(\n-        \"`axis_names` argument of shard_map should be of type `frozenset` or\"\n-        f\" `set`. Got type: {type(axis_names)}\")\n-  if isinstance(axis_names, set):\n-    axis_names = frozenset(axis_names)\n-  if not axis_names:\n-    axis_names = frozenset(mesh.axis_names)\n-  if not axis_names.issubset(mesh.axis_names):\n-    raise ValueError(\n-        f\"jax.shard_map requires axis_names={axis_names} to be a subset of \"\n-        f\"mesh.axis_names={mesh.axis_names}\")\n-\n-  if (in_specs is None and\n-      not all(mesh._name_to_type[a] == AxisType.Explicit for a in axis_names)):\n-    raise TypeError(\n-        \"shard_map in_specs argument must be a pytree of\"\n-        \" `jax.sharding.PartitionSpec` instances, but it was `None` when\"\n-        f\" {axis_names=} are of type\"\n-        f\" {', '.join(str(mesh._name_to_type[a]) for a in axis_names)}\")\n-\n-  if in_specs is not None:\n-    _check_specs(SpecErrorType.input, in_specs, axis_names)\n-  if not callable(out_specs):\n-    _check_specs(SpecErrorType.out, out_specs, axis_names)\n-\n   @util.wraps(f)\n   @traceback_util.api_boundary\n   def wrapped(*args):\n+    nonlocal mesh, axis_names\n+    mesh, axis_names = _shmap_checks(mesh, axis_names, in_specs, out_specs,\n+                                     _skip_mesh_check, _smap)\n     fun = lu.wrap_init(\n         f, debug_info=api_util.debug_info(\"shard_map\", f, args, {}))\n     args_flat, in_tree = tree_flatten(args)\n@@ -305,6 +253,59 @@ def out_names_thunk():\n   return wrapped\n \n \n+def _shmap_checks(mesh, axis_names, in_specs, out_specs, _skip_mesh_check,\n+                  _smap):\n+  if mesh is None:\n+    mesh = get_abstract_mesh()\n+    if mesh.empty:\n+      raise ValueError(\n+          \"The context mesh cannot be empty. Use\"\n+          \" `jax.sharding.use_mesh(mesh)` to enter into a mesh context\")\n+  else:\n+    ctx_mesh = get_abstract_mesh()\n+    if (not _skip_mesh_check and not ctx_mesh.empty and\n+        mesh.abstract_mesh != ctx_mesh):\n+      raise ValueError(\n+          f\"The context mesh {ctx_mesh} should match the mesh passed to\"\n+          f\" shard_map {mesh}\")\n+\n+  if not isinstance(mesh, (Mesh, AbstractMesh)):\n+    raise TypeError(\"shard_map requires a `jax.sharding.Mesh` or a \"\n+                    \"`jax.sharding.AbstractMesh` instance for its \"\n+                    f\"second argument, but got {mesh} of type {type(mesh)}.\")\n+\n+  if not isinstance(axis_names, (frozenset, set)):\n+    raise TypeError(\n+        \"`axis_names` argument of shard_map should be of type `frozenset` or\"\n+        f\" `set`. Got type: {type(axis_names)}\")\n+  if isinstance(axis_names, set):\n+    axis_names = frozenset(axis_names)\n+  if not axis_names:\n+    axis_names = frozenset(mesh.axis_names)\n+  if not axis_names.issubset(mesh.axis_names):\n+    raise ValueError(\n+        f\"jax.shard_map requires axis_names={axis_names} to be a subset of \"\n+        f\"mesh.axis_names={mesh.axis_names}\")\n+\n+  if (in_specs is None and\n+      not all(mesh._name_to_type[a] == AxisType.Explicit for a in axis_names)):\n+    axis_types = ', '.join(str(mesh._name_to_type[a]) for a in axis_names)\n+    if _smap:\n+      msg = (f\"in_axes was not specified when axis_name={axis_names} was of\"\n+             f\" type {axis_types}\")\n+    else:\n+      msg = (\"shard_map in_specs argument must be a pytree of\"\n+             \" `jax.sharding.PartitionSpec` instances, but it was `None` when\"\n+             f\" {axis_names=} are of type {axis_types}\")\n+    raise TypeError(msg)\n+\n+  if in_specs is not None:\n+    _check_specs(SpecErrorType.input, in_specs, axis_names)\n+  if not callable(out_specs):\n+    _check_specs(SpecErrorType.out, out_specs, axis_names)\n+  return mesh, axis_names\n+\n+\n # Internally use AxisNames = dict[int, tuple[AxisName, ...]], not PartitionSpecs\n AxisNames = dict[int, tuple[AxisName, ...]]  # TODO(mattjj): make it hashable\n def _canonicalize_spec(spec: PartitionSpec) -> AxisNames:\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 1abef7b06323..00d437aadb08 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -3218,7 +3218,7 @@ def g(x, y):\n   @jtu.with_explicit_mesh((2,), ('x',), axis_types=(AxisType.Auto,))\n   def test_smap_auto_error(self, mesh):\n     with self.assertRaisesRegex(TypeError, \"in_axes was not specified\"):\n-      smap(lambda x: x * 2, out_axes=0, axis_name='x')\n+      smap(lambda x: x * 2, out_axes=0, axis_name='x')(np.arange(4))\n \n   @jtu.with_explicit_mesh((2, 2), ('x', 'y'),\n                           axis_types=(AxisType.Explicit, AxisType.Auto))\n@@ -3273,6 +3273,22 @@ def f(a):\n     arr = jax.device_put(np.arange(16).reshape(8, 2), P('y'))\n     jax.jit(smap(f, in_axes=0, out_axes=0, axis_name='y'))(arr)  # doesn't crash\n \n+  @jtu.with_explicit_mesh((2, 2), ('x', 'y'),\n+                          axis_types=(AxisType.Explicit, AxisType.Auto))\n+  def test_smap_auto_explicit_nest_mesh_call_time(self, mesh):\n+    @partial(smap, in_axes=1, out_axes=1, axis_name='x')\n+    def g(b):\n+      return jnp.sin(b)\n+\n+    @partial(smap, in_axes=0, out_axes=0, axis_name='y')\n+    def f(a):\n+      self.assertEqual(a.aval.vma, {'y'})\n+      b = a * 2\n+      return g(b)\n+\n+    arr = jax.device_put(np.arange(16).reshape(8, 2), P('y'))\n+    jax.jit(f)(arr)  # doesn't crash\n+\n \n class FunSpec(NamedTuple):\n   name: str\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Default `in_axes` of `smap` to `Infer`. This matches the behavior of `jax.shard_map` where `in_specs` is optional. If the `axis_name` `smap` is going manual over is not of type `Explicit`, we error out and providing `in_axes` is compulsory.\n\nThis also allows us to not expose `Infer` as a public API!\n\nAdded some more tests and fixed some bugs too.\n\nPiperOrigin-RevId: 757905314",
        "output": "```diff\nCommit: 1a3d3e37b2a71a1ebe9c0bdc3ffb4d95ac4e0bd5\nDate: 2025-05-12T21:09:03Z\nURL: https://github.com/jax-ml/jax/commit/1a3d3e37b2a71a1ebe9c0bdc3ffb4d95ac4e0bd5\nFiles changed: 2\nAdditions: +80, Deletions: -12\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 939dbeddf3d7..e9f2d3b6072e 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -146,7 +146,7 @@ def _get_default_infer():\n \n # TODO(yashkatariya): We need a singleton which users can provide to `in_axes`\n # to tell smap to infer in_specs from args when mesh is fully explicit.\n-def smap(f, in_axes, out_axes, axis_name: AxisName):\n+def smap(f, /, *, in_axes=Infer, out_axes, axis_name: AxisName):\n   if isinstance(axis_name, (list, tuple)):\n     raise TypeError(\n         f\"smap axis_name should be a `str` or a `Hashable`, but got {axis_name}\")\n@@ -164,6 +164,15 @@ def smap(f, in_axes, out_axes, axis_name: AxisName):\n   if not all(isinstance(l, int) for l in tree_leaves(out_axes)):\n     raise TypeError(\"smap out_axes must be an int, None, or (nested) container \"\n                     f\"with those types as leaves, but got {out_axes}.\")\n+  mesh = get_abstract_mesh()\n+  if mesh.empty:\n+    raise ValueError(\n+        \"The context mesh cannot be empty. Use\"\n+        \" `jax.sharding.use_mesh(mesh)` to enter into a mesh context.\")\n+  if mesh._name_to_type[axis_name] != AxisType.Explicit and in_axes is Infer:\n+    raise TypeError(\n+        f\"in_axes was not specified when {axis_name=} was of type\"\n+        f\" {mesh._name_to_type[axis_name]}.\")\n \n   in_specs = (None if in_axes is Infer else\n               tree_map(partial(_axes_to_pspec, axis_name), in_axes,\n@@ -215,12 +224,13 @@ def _shard_map(f: Callable, *, mesh: Mesh | AbstractMesh | None,\n         f\"jax.shard_map requires axis_names={axis_names} to be a subset of \"\n         f\"mesh.axis_names={mesh.axis_names}\")\n \n-  # TODO(yashkatariya): Maybe we don't have to be this strict?\n-  if mesh._any_axis_auto_or_manual and in_specs is None:\n+  if (in_specs is None and\n+      not all(mesh._name_to_type[a] == AxisType.Explicit for a in axis_names)):\n     raise TypeError(\n         \"shard_map in_specs argument must be a pytree of\"\n-        \" `jax.sharding.PartitionSpec` instances, but it was None when mesh\"\n-        f\" has `Auto` axes {mesh}\")\n+        \" `jax.sharding.PartitionSpec` instances, but it was `None` when\"\n+        f\" {axis_names=} are of type\"\n+        f\" {', '.join(str(mesh._name_to_type[a]) for a in axis_names)}\")\n \n   if in_specs is not None:\n     _check_specs(SpecErrorType.input, in_specs, axis_names)\n@@ -242,9 +252,8 @@ def wrapped(*args):\n       e, *_ = prefix_errors(in_specs, args)\n       raise e('shard_map in_specs') from None\n \n-    # TODO(yashkatariya): Relax this and convert only `None`s in `in_specs_flat`\n-    # and accept the other specs as is.\n-    if mesh._are_all_axes_explicit and in_specs is None:\n+    if (in_specs is None and\n+        all(mesh._name_to_type[a] == AxisType.Explicit for a in axis_names)):\n       arg_s = [typeof(a).sharding for a in args_flat]\n       assert all(i is None for i in in_specs_flat), in_specs_flat\n       in_specs_flat = [_manual_spec(axis_names, s.spec) for s in arg_s]\n@@ -597,7 +606,8 @@ def _as_manual_mesh(mesh, manual_axes: frozenset):\n     if cur_mesh._name_to_type[a] == AxisType.Auto:\n       auto_axes.add(a)\n     else:\n-      assert cur_mesh._name_to_type[a] == AxisType.Explicit, cur_mesh._name_to_type[a]\n+      assert cur_mesh._name_to_type[a] == AxisType.Explicit, (\n+          a, cur_mesh._name_to_type[a])\n       explicit_axes.add(a)\n \n   new_axis_types = []\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 4d3b265bd869..1abef7b06323 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -36,7 +36,7 @@\n from jax._src import config\n from jax._src import core\n from jax._src import prng\n-from jax._src.shard_map import shard_map, smap, Infer\n+from jax._src.shard_map import shard_map, smap\n from jax._src import test_util as jtu\n from jax._src.lib.mlir.dialects import sdy\n from jax._src.util import safe_zip, safe_map, partition_list, merge_lists\n@@ -971,7 +971,7 @@ def test_in_specs_none_error(self):\n \n     def f(x): return x\n \n-    with self.assertRaisesRegex(TypeError, \"but it was None\"):\n+    with self.assertRaisesRegex(TypeError, \"but it was `None`\"):\n       shard_map(f, mesh=mesh, in_specs=None, out_specs=P())(3.)\n \n     # TODO(mattjj): enable this test once we fix the tree_map(f, None, 3.0) bug\n@@ -3182,7 +3182,7 @@ def h(x):\n \n     @jax.jit\n     def f(x):\n-      return smap(h, in_axes=Infer, out_axes=0, axis_name='x')(x)\n+      return smap(h, out_axes=0, axis_name='x')(x)\n \n     out = f(arr)\n     self.assertArraysEqual(out, np_inp * np_inp)\n@@ -3215,6 +3215,64 @@ def g(x, y):\n     out = g(np.arange(4), np.arange(8))\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('data')))\n \n+  @jtu.with_explicit_mesh((2,), ('x',), axis_types=(AxisType.Auto,))\n+  def test_smap_auto_error(self, mesh):\n+    with self.assertRaisesRegex(TypeError, \"in_axes was not specified\"):\n+      smap(lambda x: x * 2, out_axes=0, axis_name='x')\n+\n+  @jtu.with_explicit_mesh((2, 2), ('x', 'y'),\n+                          axis_types=(AxisType.Explicit, AxisType.Auto))\n+  def test_smap_auto_explicit(self, mesh):\n+    def f(x):\n+      self.assertEqual(x.aval.vma, {'x'})\n+      return x * 2\n+\n+    arr = jax.device_put(np.arange(4), P('x'))\n+    out = jax.jit(smap(f, out_axes=0, axis_name='x'))(arr)\n+    self.assertArraysEqual(out, np.arange(4) * 2)\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P('x')))\n+\n+    def g(x):\n+      self.assertEqual(x.aval.vma, {'y'})\n+      return x * 2\n+\n+    arr = jax.device_put(np.arange(4), P('y'))\n+    out = jax.jit(smap(g, in_axes=0, out_axes=0, axis_name='y'))(arr)\n+    self.assertArraysEqual(out, np.arange(4) * 2)\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P('y')))\n+\n+  @jtu.with_explicit_mesh((2, 2), ('x', 'y'),\n+                          axis_types=(AxisType.Explicit, AxisType.Auto))\n+  def test_smap_auto_explicit_nest(self, mesh):\n+    def g(b):\n+      self.assertEqual(b.aval.vma, {'x', 'y'})\n+      return jnp.sin(b)\n+\n+    def f(a):\n+      self.assertEqual(a.aval.vma, {'y'})\n+      b = a * 2\n+      return smap(g, in_axes=1, out_axes=1, axis_name='x')(b)\n+\n+    arr = jax.device_put(np.arange(16).reshape(8, 2), P('y'))\n+    jax.jit(smap(f, in_axes=0, out_axes=0, axis_name='y'))(arr)  # doesn't crash\n+\n+  @jtu.with_explicit_mesh((2, 2), ('x', 'y'),\n+                          axis_types=(AxisType.Explicit, AxisType.Auto))\n+  def test_smap_auto_explicit_nest_inner_none(self, mesh):\n+    def g(b):\n+      self.assertEqual(b.aval.vma, {'y'})\n+      return jnp.sin(b)\n+\n+    def f(a):\n+      self.assertEqual(a.aval.vma, {'y'})\n+      b = a * 2\n+      # Going manual over explicit axis `x` but in_axes is Infer and since\n+      # input has no sharding, it will default to None.\n+      return smap(g, out_axes=1, axis_name='x')(b)\n+\n+    arr = jax.device_put(np.arange(16).reshape(8, 2), P('y'))\n+    jax.jit(smap(f, in_axes=0, out_axes=0, axis_name='y'))(arr)  # doesn't crash\n+\n \n class FunSpec(NamedTuple):\n   name: str\n\n```",
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move the existing mask handling code to the relayout fn, invoke it from the existing tpu relayout rule.\n\nPiperOrigin-RevId: 757902288",
        "output": "```diff\nCommit: 167d6bc765d05f2f49c9aedf1e1c94500d6eefde\nDate: 2025-05-12T21:01:18Z\nURL: https://github.com/jax-ml/jax/commit/167d6bc765d05f2f49c9aedf1e1c94500d6eefde\nFiles changed: 5\nAdditions: +219, Deletions: -128\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu.td b/jaxlib/mosaic/dialect/tpu/tpu.td\nindex fb72b6948d9d..226fc6285192 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu.td\n+++ b/jaxlib/mosaic/dialect/tpu/tpu.td\n@@ -441,6 +441,7 @@ def TPU_RelayoutOp : TPU_Op<\"relayout\", [SameOperandsAndResultType]> {\n   let arguments = (ins AnyType:$input);\n   let results = (outs AnyType:$output);\n   let assemblyFormat = [{ $input attr-dict `:` type($input) `->` type($output) }];\n+  let hasVerifier = 1;\n }\n \n def TPU_PackMaskOp : TPU_Op<\"pack_vmsk\", [Pure, SameTypeOperands]> {\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\nindex 934088e91506..b5e68bf08370 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n+++ b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n@@ -343,6 +343,54 @@ LogicalResult MemRefSqueezeOp::canonicalize(MemRefSqueezeOp op,\n   return success();\n }\n \n+LogicalResult RelayoutOp::verify() {\n+  auto in_layout_array_attr =\n+      getOperation()->getAttrOfType<ArrayAttr>(\"in_layout\");\n+  if (!in_layout_array_attr || in_layout_array_attr.empty()) {\n+    return emitOpError(\"missing or empty 'in_layout' attribute\");\n+  }\n+  if (in_layout_array_attr.size() != 1) {\n+    return emitOpError(\n+        \"'in_layout' attribute must be an array containing a single \"\n+        \"VectorLayoutAttr\");\n+  }\n+  auto src_vla = dyn_cast<tpu::VectorLayoutAttr>(in_layout_array_attr[0]);\n+  if (!src_vla) {\n+    return emitOpError(\"'in_layout' attribute is not a VectorLayoutAttr\");\n+  }\n+\n+  auto out_layout_array_attr =\n+      getOperation()->getAttrOfType<ArrayAttr>(\"out_layout\");\n+  if (!out_layout_array_attr || out_layout_array_attr.empty()) {\n+    return emitOpError(\"missing or empty 'out_layout' attribute\");\n+  }\n+  if (out_layout_array_attr.size() != 1) {\n+    return emitOpError(\n+        \"'out_layout' attribute must be an array containing a single \"\n+        \"VectorLayoutAttr\");\n+  }\n+  auto dst_vla = dyn_cast<tpu::VectorLayoutAttr>(out_layout_array_attr[0]);\n+  if (!dst_vla) {\n+    return emitOpError(\"'out_layout' attribute is not a VectorLayoutAttr\");\n+  }\n+\n+  VectorType input_type = cast<VectorType>(getInput().getType());\n+  VectorType output_type = cast<VectorType>(getOutput().getType());\n+\n+  if (input_type.getShape() != output_type.getShape()) {\n+    return emitOpError(\"input and output shapes must match\");\n+  }\n+  if (input_type.getElementType() != output_type.getElementType()) {\n+    // Allow i1 to i1 even if bitwidth in layout changes.\n+    if (!(input_type.getElementType().isInteger(1) &&\n+          output_type.getElementType().isInteger(1))) {\n+      return emitOpError(\n+          \"input and output element types must match for non-mask relayouts\");\n+    }\n+  }\n+  return success();\n+}\n+\n LogicalResult MemRefReshapeOp::verify() {\n   auto src_ty = getMemRefType(getInput());\n   auto tgt_ty = getType();\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex f8e18070e5e7..b8ba61e7c914 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -2101,74 +2101,6 @@ LogicalResult tpu_assume_layout_rule(RewriteContext &ctx, Operation &op,\n   return success();\n }\n \n-LogicalResult tpu_relayout_rule(RewriteContext &ctx, Operation &op,\n-                                const ArrayRef<Layout> layouts_in,\n-                                const ArrayRef<Layout> layouts_out) {\n-  TPU_ASSERT_EQ_OP(op.getNumOperands(), 1);\n-  TPU_ASSERT_EQ_OP(op.getNumResults(), 1);\n-  TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n-  TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  TPU_ASSERT_OP(layouts_in[0].has_value());\n-  TPU_ASSERT_OP(layouts_out[0].has_value());\n-  const auto& in_layout = *layouts_in[0];\n-  const auto& out_layout = *layouts_out[0];\n-  auto realyout_op = cast<tpu::RelayoutOp>(op);\n-  auto in_bitwidth = in_layout.bitwidth();\n-  auto out_bitwidth = out_layout.bitwidth();\n-  auto vty = cast<VectorType>(realyout_op.getType());\n-  ImplicitLocOpBuilder builder(op.getLoc(), &op);\n-  if (in_layout == out_layout) {\n-    realyout_op.replaceAllUsesWith(realyout_op.getInput());\n-    realyout_op.erase();\n-    return success();\n-  }\n-  FAILUREOR_ASSIGN_OR_RETURN(\n-      xla::Array<Value> vals,\n-      disassemble(builder, in_layout,\n-                  cast<TypedValue<VectorType>>(realyout_op.getInput()),\n-                  ctx.target_shape,\n-                  /*use_implicit_shape=*/true));\n-  // Packing vector masks from 32-bit to 16-bit.\n-  if (vty.getElementType() == builder.getI1Type() && in_bitwidth == 32 &&\n-      out_bitwidth == 16 &&\n-      in_layout.tiling()[0] == in_layout.packing() * ctx.target_shape[0] &&\n-      in_layout.tiling()[1] == ctx.target_shape[1] &&\n-      in_layout.tiling() == out_layout.tiling() &&\n-      in_layout.offsets() == out_layout.offsets() &&\n-      in_layout.implicit_dim() == out_layout.implicit_dim()) {\n-    std::vector<int64_t> vmsks_shape(vals.dimensions().begin(),\n-                                     vals.dimensions().end());\n-    *(vmsks_shape.end() - 1) = llvm::divideCeil(vmsks_shape.back(), 2);\n-    xla::Array<Value> out_vmsks(vmsks_shape, nullptr);\n-    SmallVector<int64_t> val_idx;\n-    Value default_val =\n-        getFullLikeVector(builder, cast<TypedValue<VectorType>>(*vals.begin()),\n-                          IntegerAttr::get(builder.getI1Type(), 0));\n-    out_vmsks.Each([&](absl::Span<const int64_t> idx, Value *v) {\n-      val_idx.assign(idx.begin(), idx.end());\n-      // TODO(jevinjiang): can be simplified when offset is replicated.\n-      *(val_idx.end() - 1) *= 2;\n-      Value low_part = *(val_idx.end() - 1) < *(vals.dimensions().end() - 1)\n-                           ? vals(val_idx)\n-                           : default_val;\n-      *(val_idx.end() - 1) += 1;\n-      Value high_part = *(val_idx.end() - 1) < *(vals.dimensions().end() - 1)\n-                            ? vals(val_idx)\n-                            : default_val;\n-      const VectorType mask_ty = getNativeVregOrVmaskType(\n-          builder.getI1Type(), in_bitwidth / 2, ctx.target_shape);\n-      *v = builder.create<PackMaskOp>(mask_ty, low_part, high_part);\n-    });\n-    const RollVectorsOp rolled_op =\n-        assemble(builder, vty, out_layout, out_vmsks, ctx.target_shape,\n-                 /*use_implicit_shape=*/true);\n-    op.replaceAllUsesWith(rolled_op);\n-    op.erase();\n-    return success();\n-  }\n-  return op.emitOpError(\"Not implemented: unsupported layout change\");\n-}\n-\n Value createSubelementMask(OpBuilder &builder, const Location loc,\n                            const int bitwidth, const int64_t from,\n                            const int64_t to,\n@@ -4827,60 +4759,6 @@ LogicalResult tpu_prng_random_bits_rule(RewriteContext &ctx, Operation &op,\n   return success();\n }\n \n-const llvm::StringMap<rule_type> &rules() {\n-  static const llvm::StringMap<rule_type> *rules = [] {\n-    static auto rules = new llvm::StringMap<rule_type>{\n-        {arith::ConstantOp::getOperationName(), arith_constant_rule},\n-        {arith::ExtFOp::getOperationName(), arith_extf_rule},\n-        {arith::ExtSIOp::getOperationName(), arith_extsi_rule},\n-        {arith::ExtUIOp::getOperationName(), arith_extui_rule},\n-        {arith::TruncFOp::getOperationName(), arith_truncf_rule},\n-        {arith::TruncIOp::getOperationName(), arith_trunci_rule},\n-        {func::ReturnOp::getOperationName(), func_return_rule},\n-        {scf::ForOp::getOperationName(), scf_for_rule},\n-        {scf::WhileOp::getOperationName(), scf_while_rule},\n-        {scf::ConditionOp::getOperationName(), scf_condition_rule},\n-        {scf::IfOp::getOperationName(), scf_if_rule},\n-        {scf::YieldOp::getOperationName(), yield_rule},\n-        {tpu::YieldOp::getOperationName(), yield_rule},\n-        {tpu::RotateOp::getOperationName(), tpu_rotate_rule},\n-        {tpu::DynamicRotateOp::getOperationName(), tpu_dynamic_rotate_rule},\n-        {tpu::ConcatenateOp::getOperationName(), tpu_concatenate_rule},\n-        {tpu::IotaOp::getOperationName(), tpu_iota_rule},\n-        {tpu::GatherOp::getOperationName(), tpu_gather_rule},\n-        {tpu::DynamicGatherOp::getOperationName(), tpu_dynamic_gather_rule},\n-        {tpu::LoadOp::getOperationName(), tpu_load_rule},\n-        {tpu::StoreOp::getOperationName(), tpu_store_rule},\n-        {tpu::StridedLoadOp::getOperationName(), tpu_strided_load_rule},\n-        {tpu::StridedStoreOp::getOperationName(), tpu_strided_store_rule},\n-        {tpu::VectorStoreOp::getOperationName(), tpu_vector_store_rule},\n-        {tpu::MatmulOp::getOperationName(), tpu_matmul_rule},\n-        {tpu::RegionOp::getOperationName(), tpu_region_rule},\n-        {tpu::BitcastOp::getOperationName(), tpu_bitcast_rule},\n-        {tpu::TraceOp::getOperationName(), tpu_trace_rule},\n-        {tpu::AssumeLayoutOp::getOperationName(), tpu_assume_layout_rule},\n-        {tpu::PRNGRandomBitsOp::getOperationName(), tpu_prng_random_bits_rule},\n-        {tpu::RelayoutOp::getOperationName(), tpu_relayout_rule},\n-        {tpu::FPToSIOp::getOperationName(), tpu_fptosi_rule},\n-        {vector::BroadcastOp::getOperationName(), vector_broadcast_rule},\n-        {vector::ExtractOp::getOperationName(), vector_extract_rule},\n-        {vector::LoadOp::getOperationName(), vector_load_rule},\n-        {vector::MultiDimReductionOp::getOperationName(),\n-         vector_multi_reduction_rule},\n-        {vector::ExtractStridedSliceOp::getOperationName(),\n-         vector_extract_strided_slice_rule},\n-        {vector::ShapeCastOp::getOperationName(), vector_shape_cast_rule},\n-        {vector::StoreOp::getOperationName(), vector_store_rule},\n-        {tpu::TransposeOp::getOperationName(), vector_transpose_rule}};\n-\n-    for (const auto &[name, rule] : mlir::tpu::extensions::rules()) {\n-      rules->insert({name, rule});\n-    }\n-    return rules;\n-  }();\n-  return *rules;\n-}\n-\n // Determines whether we should handle bank conflict for the given stride and\n // max_sublane_offset.\n //\n@@ -6773,12 +6651,20 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n                                            VectorLayout src,\n                                            VectorLayout dst) {\n   const auto target_shape = ctx.target_shape;\n+  VectorType vty = v.getType();\n   const int8_t bitwidth = src.bitwidth();\n-  if (bitwidth != dst.bitwidth()) {\n+  const bool is_mask = vty.getElementTypeBitWidth() == 1;\n+  const bool is_mask_pack =\n+      is_mask && bitwidth == 32 && dst.bitwidth() == 16 &&\n+      src.tiling()[0] == src.packing() * target_shape[0] &&\n+      src.tiling()[1] == target_shape[1] && src.tiling() == dst.tiling() &&\n+      src.offsets() == dst.offsets() &&\n+      src.implicit_dim() == dst.implicit_dim();\n+\n+  if (bitwidth != dst.bitwidth() && !is_mask_pack) {\n     return emitError(v.getLoc(), \"Can't change bitwidth during a relayout\");\n   }\n-  VectorType vty = v.getType();\n-  const bool is_mask = vty.getElementTypeBitWidth() == 1;\n+\n   {\n     // Replication imposes a replication constraint on the *logical* value of\n     // the vector: When moving along a replicated axis, all elements must be\n@@ -6812,6 +6698,38 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n   FAILUREOR_ASSIGN_OR_RETURN(\n       xla::Array<Value> src_tiles,\n       disassemble(builder, src, v, target_shape, /*use_implicit_shape=*/true));\n+  if (is_mask_pack) {\n+    std::vector<int64_t> vmsks_shape(src_tiles.dimensions().begin(),\n+                                     src_tiles.dimensions().end());\n+    *(vmsks_shape.end() - 1) = llvm::divideCeil(vmsks_shape.back(), 2);\n+    xla::Array<Value> out_vmsks(vmsks_shape, nullptr);\n+    SmallVector<int64_t> val_idx;\n+    Value default_val = getFullVector(\n+        builder, v.getLoc(),\n+        cast<TypedValue<VectorType>>(*src_tiles.begin()).getType(),\n+        IntegerAttr::get(builder.getI1Type(), 0));\n+    out_vmsks.Each([&](absl::Span<const int64_t> idx, Value *v_slot_in_array) {\n+      val_idx.assign(idx.begin(), idx.end());\n+      *(val_idx.end() - 1) *= 2;\n+      Value low_part =\n+          *(val_idx.end() - 1) < *(src_tiles.dimensions().end() - 1)\n+              ? src_tiles(val_idx)\n+              : default_val;\n+      *(val_idx.end() - 1) += 1;\n+      Value high_part =\n+          *(val_idx.end() - 1) < *(src_tiles.dimensions().end() - 1)\n+              ? src_tiles(val_idx)\n+              : default_val;\n+      const VectorType mask_ty = getNativeVregOrVmaskType(\n+          builder.getI1Type(), bitwidth / 2, target_shape);\n+      *v_slot_in_array =\n+          builder.create<PackMaskOp>(v.getLoc(), mask_ty, low_part, high_part);\n+    });\n+    return assemble(builder, vty, dst, out_vmsks, target_shape,\n+                    /*use_implicit_shape=*/true)\n+        .getResult();\n+  }\n+\n   if (is_mask) {\n     auto new_tile_ty = getNativeVregOrVmaskType(\n         builder.getIntegerType(bitwidth), bitwidth, target_shape);\n@@ -6823,6 +6741,7 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n   }\n   auto assemble_with_mask_check = [&](xla::Array<Value> &tiles,\n                                       bool use_implicit_shape = false) {\n+\n     if (is_mask) {\n       auto zeros_tile = builder.create<arith::ConstantOp>(\n           tiles.begin()->getLoc(),\n@@ -6941,9 +6860,110 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n       changeOffsets(ctx, builder, v.getLoc(), vty, src, std::move(src_tiles),\n                     dst.offsets()));\n \n-  CHECK_EQ(src, dst);  // At this point we've should be done.\n-  return assemble_with_mask_check(src_tiles,\n-                                  /*use_implicit_shape=*/true);\n+  CHECK_EQ(src, dst);\n+  return assemble_with_mask_check(src_tiles, /*use_implicit_shape=*/true);\n+}\n+\n+LogicalResult tpu_relayout_rule(RewriteContext &ctx, Operation &op,\n+                                const ArrayRef<Layout> layouts_in,\n+                                const ArrayRef<Layout> layouts_out) {\n+  auto tpu_relayout_op = cast<tpu::RelayoutOp>(op);\n+  auto input_val = dyn_cast<TypedValue<VectorType>>(tpu_relayout_op.getInput());\n+\n+  auto in_layout_array_attr =\n+      tpu_relayout_op->getAttrOfType<ArrayAttr>(\"in_layout\");\n+  if (!in_layout_array_attr || in_layout_array_attr.empty()) {\n+    return tpu_relayout_op.emitOpError(\n+        \"missing or empty 'in_layout' attribute\");\n+  }\n+  auto src_vla = dyn_cast<tpu::VectorLayoutAttr>(in_layout_array_attr[0]);\n+  if (!src_vla) {\n+    return tpu_relayout_op.emitOpError(\n+        \"'in_layout' attribute is not a VectorLayoutAttr\");\n+  }\n+  VectorLayout src_layout = src_vla.getLayout().value();\n+\n+  auto out_layout_array_attr =\n+      tpu_relayout_op->getAttrOfType<ArrayAttr>(\"out_layout\");\n+  if (!out_layout_array_attr || out_layout_array_attr.empty()) {\n+    return tpu_relayout_op.emitOpError(\n+        \"missing or empty 'out_layout' attribute\");\n+  }\n+  auto dst_vla = dyn_cast<tpu::VectorLayoutAttr>(out_layout_array_attr[0]);\n+  if (!dst_vla) {\n+    return tpu_relayout_op.emitOpError(\n+        \"'out_layout' attribute is not a VectorLayoutAttr\");\n+  }\n+  VectorLayout dst_layout = dst_vla.getLayout().value();\n+\n+  if (src_layout == dst_layout) {\n+    tpu_relayout_op.replaceAllUsesWith(tpu_relayout_op.getInput());\n+    tpu_relayout_op.erase();\n+    return success();\n+  }\n+\n+  OpBuilder builder(&op);\n+  FAILUREOR_ASSIGN_OR_RETURN(\n+      TypedValue<VectorType> new_v,\n+      relayout(ctx, builder, input_val, src_layout, dst_layout));\n+\n+  tpu_relayout_op.replaceAllUsesWith(new_v);\n+  tpu_relayout_op.erase();\n+  return success();\n+}\n+\n+const llvm::StringMap<rule_type> &rules() {\n+  static const llvm::StringMap<rule_type> *rules = [] {\n+    static auto rules = new llvm::StringMap<rule_type>{\n+        {arith::ConstantOp::getOperationName(), arith_constant_rule},\n+        {arith::ExtFOp::getOperationName(), arith_extf_rule},\n+        {arith::ExtSIOp::getOperationName(), arith_extsi_rule},\n+        {arith::ExtUIOp::getOperationName(), arith_extui_rule},\n+        {arith::TruncFOp::getOperationName(), arith_truncf_rule},\n+        {arith::TruncIOp::getOperationName(), arith_trunci_rule},\n+        {func::ReturnOp::getOperationName(), func_return_rule},\n+        {scf::ForOp::getOperationName(), scf_for_rule},\n+        {scf::WhileOp::getOperationName(), scf_while_rule},\n+        {scf::ConditionOp::getOperationName(), scf_condition_rule},\n+        {scf::IfOp::getOperationName(), scf_if_rule},\n+        {scf::YieldOp::getOperationName(), yield_rule},\n+        {tpu::YieldOp::getOperationName(), yield_rule},\n+        {tpu::RotateOp::getOperationName(), tpu_rotate_rule},\n+        {tpu::DynamicRotateOp::getOperationName(), tpu_dynamic_rotate_rule},\n+        {tpu::ConcatenateOp::getOperationName(), tpu_concatenate_rule},\n+        {tpu::IotaOp::getOperationName(), tpu_iota_rule},\n+        {tpu::GatherOp::getOperationName(), tpu_gather_rule},\n+        {tpu::DynamicGatherOp::getOperationName(), tpu_dynamic_gather_rule},\n+        {tpu::LoadOp::getOperationName(), tpu_load_rule},\n+        {tpu::StoreOp::getOperationName(), tpu_store_rule},\n+        {tpu::StridedLoadOp::getOperationName(), tpu_strided_load_rule},\n+        {tpu::StridedStoreOp::getOperationName(), tpu_strided_store_rule},\n+        {tpu::VectorStoreOp::getOperationName(), tpu_vector_store_rule},\n+        {tpu::MatmulOp::getOperationName(), tpu_matmul_rule},\n+        {tpu::RegionOp::getOperationName(), tpu_region_rule},\n+        {tpu::BitcastOp::getOperationName(), tpu_bitcast_rule},\n+        {tpu::TraceOp::getOperationName(), tpu_trace_rule},\n+        {tpu::AssumeLayoutOp::getOperationName(), tpu_assume_layout_rule},\n+        {tpu::PRNGRandomBitsOp::getOperationName(), tpu_prng_random_bits_rule},\n+        {tpu::RelayoutOp::getOperationName(), tpu_relayout_rule},\n+        {tpu::FPToSIOp::getOperationName(), tpu_fptosi_rule},\n+        {vector::BroadcastOp::getOperationName(), vector_broadcast_rule},\n+        {vector::ExtractOp::getOperationName(), vector_extract_rule},\n+        {vector::LoadOp::getOperationName(), vector_load_rule},\n+        {vector::MultiDimReductionOp::getOperationName(),\n+         vector_multi_reduction_rule},\n+        {vector::ExtractStridedSliceOp::getOperationName(),\n+         vector_extract_strided_slice_rule},\n+        {vector::ShapeCastOp::getOperationName(), vector_shape_cast_rule},\n+        {vector::StoreOp::getOperationName(), vector_store_rule},\n+        {tpu::TransposeOp::getOperationName(), vector_transpose_rule}};\n+\n+    for (const auto &[name, rule] : mlir::tpu::extensions::rules()) {\n+      rules->insert({name, rule});\n+    }\n+    return rules;\n+  }();\n+  return *rules;\n }\n \n // TODO(apaszke): Implement a debug mode that inserts additional assertions.\ndiff --git a/jaxlib/mosaic/dialect/tpu/vreg_util.cc b/jaxlib/mosaic/dialect/tpu/vreg_util.cc\nindex 72e0bf7f0caf..237bbe5cc722 100644\n--- a/jaxlib/mosaic/dialect/tpu/vreg_util.cc\n+++ b/jaxlib/mosaic/dialect/tpu/vreg_util.cc\n@@ -79,6 +79,19 @@ TypedValue<VectorType> getFullLikeVector(ImplicitLocOpBuilder &builder,\n   return getFullVector(builder, vec.getType(), value);\n }\n \n+TypedValue<VectorType> getFullVector(OpBuilder &builder, Location loc,\n+                                     VectorType vty, Attribute value) {\n+  return cast<TypedValue<VectorType>>(\n+      builder.create<arith::ConstantOp>(loc, DenseElementsAttr::get(vty, value))\n+          .getResult());\n+}\n+\n+TypedValue<VectorType> getFullLikeVector(OpBuilder &builder, Location loc,\n+                                         TypedValue<VectorType> vec,\n+                                         Attribute value) {\n+  return getFullVector(builder, loc, vec.getType(), value);\n+}\n+\n TypedValue<VectorType> getZerosVector(ImplicitLocOpBuilder &builder,\n                                       VectorType vty) {\n   return getFullVector(builder, vty, builder.getZeroAttr(vty.getElementType()));\ndiff --git a/jaxlib/mosaic/dialect/tpu/vreg_util.h b/jaxlib/mosaic/dialect/tpu/vreg_util.h\nindex 8c2967e776c7..8833390ef87b 100644\n--- a/jaxlib/mosaic/dialect/tpu/vreg_util.h\n+++ b/jaxlib/mosaic/dialect/tpu/vreg_util.h\n@@ -50,6 +50,15 @@ TypedValue<VectorType> getFullLikeVector(ImplicitLocOpBuilder &builder,\n                                          TypedValue<VectorType> vec,\n                                          Attribute value);\n \n+// Same as above, but takes a `loc` as input, in case of an OpBuilder.\n+TypedValue<VectorType> getFullVector(OpBuilder &builder, Location loc,\n+                                     VectorType vty, Attribute value);\n+\n+// Same as above, but takes a `vec` as input.\n+TypedValue<VectorType> getFullLikeVector(OpBuilder &builder, Location loc,\n+                                         TypedValue<VectorType> vec,\n+                                         Attribute value);\n+\n // Creates a vmask with false flags to bottom (dim = 0)\n // or right (dim = 1) where the flag count corresponds to the (dim_size -\n // padding).\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Support squeezing tiled memrefs to 1d shapes.\n\nPreviously, squeezing to a 1D memref failed w/ verification errors, as we would always use the old layout.\n\nIf we are squeezing from a source to a 1D shape, we need to modify the tile dimensions when we emit the result layout, as the removed dimensions should not be included in the new tiling.\n\nPiperOrigin-RevId: 757887087",
        "output": "```diff\nCommit: 4dfcbc2c934456bbda866b8882d57864210f333d\nDate: 2025-05-12T20:22:39Z\nURL: https://github.com/jax-ml/jax/commit/4dfcbc2c934456bbda866b8882d57864210f333d\nFiles changed: 1\nAdditions: +147, Deletions: -65\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\nindex 134db412042d..934088e91506 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n+++ b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n@@ -13,6 +13,7 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include <algorithm>\n #include <cstddef>\n #include <cstdint>\n #include <optional>\n@@ -22,6 +23,7 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/strings/str_format.h\"\n #include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/Support/FormatVariadic.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n@@ -164,53 +166,115 @@ LogicalResult MemRefSliceOp::canonicalize(MemRefSliceOp op,\n   return success();\n }\n \n+// Computes the dimensions that were squeezed from the source shape to match the\n+// target shape. Returns the dimensions in increasing order.\n+FailureOr<SmallVector<int>> computeSqueezedDimsChecked(\n+    Operation *op, ArrayRef<int64_t> source_shape,\n+    ArrayRef<int64_t> target_shape) {\n+  SmallVector<int> squeezed;\n+  int source_index = source_shape.size() - 1;\n+  int target_index = target_shape.size() - 1;\n+\n+  while (source_index >= 0 || target_index >= 0) {\n+    int64_t target_dim = (target_index >= 0) ? target_shape[target_index] : -1;\n+    if (source_index < 0) {\n+      op->emitError() << llvm::formatv(\n+          \"Target shape is not valid. Source: {0}, Target: {1}.\",\n+          shapeToString(source_shape), shapeToString(target_shape));\n+      return failure();\n+    }\n+    int64_t source_dim = source_shape[source_index];\n+    if (source_dim == target_dim) {\n+      source_index--;\n+      target_index--;\n+    } else {\n+      if (source_dim != 1) {\n+        op->emitError() << llvm::formatv(\n+            \"Target shape is not valid. Source: {0}, Target: {1}.\",\n+            shapeToString(source_shape), shapeToString(target_shape));\n+        return failure();\n+      }\n+      squeezed.push_back(source_index);\n+      source_index--;\n+    }\n+  }\n+\n+  if (source_index != -1 || target_index != -1) {\n+    op->emitError() << \"Shape mismatch after traversal. Source shape: \"\n+                    << shapeToString(source_shape)\n+                    << \", target shape: \" << shapeToString(target_shape);\n+    return failure();\n+  }\n+  std::reverse(squeezed.begin(), squeezed.end());\n+  return squeezed;\n+}\n+\n LogicalResult MemRefSqueezeOp::verify() {\n   auto source_type = getMemRefType(getInput());\n   auto target_type = getType();\n-  // Source and target attributes may be different before propagation is done by\n-  // the canonicalizer, so we allow this when attributes are \"unset\" in the\n-  // target type.\n+\n   if (target_type.getMemorySpace() != nullptr &&\n       target_type.getMemorySpace() != source_type.getMemorySpace()) {\n-    emitOpError(\"Memory spaces do not match.\");\n-    return failure();\n+    return emitOpError(\"Memory spaces do not match.\");\n   }\n+\n   if (target_type.getElementType() != source_type.getElementType()) {\n-    this->emitOpError(\"Element types don't match.\");\n-    return failure();\n-  }\n-  if (!HasMemorySpace(source_type, tpu::MemorySpace::kSemaphoreMem) &&\n-      source_type.getRank() > 1 && target_type.getRank() == 1) {\n-    return emitError(\"Not implemented: squeeze memref to 1d.\");\n+    return emitOpError(\"Element types don't match.\");\n   }\n+\n   auto source_shape = source_type.getShape();\n   auto target_shape = target_type.getShape();\n-  int source_index = source_shape.size() - 1;\n-  int target_index = target_shape.size() - 1;\n-  auto error_msg = llvm::formatv(\n-      \"Target shape is not valid. \"\n-      \"Source type: {0}. Target type: {1}.\",\n-      source_type, target_type);\n-  while (source_index >= 0 || target_index >= 0) {\n-    int target_dim = target_index < 0 ? -1 : target_shape[target_index];\n-    if (source_index < 0) {\n-       // We have run out of source shape but target shape still remains.\n-       emitOpError(error_msg);\n-       return failure();\n+  auto squeezed_or =\n+      computeSqueezedDimsChecked(*this, source_shape, target_shape);\n+  if (failed(squeezed_or)) {\n+    return failure();\n+  }\n+\n+  auto erase_layout_op = getInput().getDefiningOp<tpu::EraseLayoutOp>();\n+  if (!erase_layout_op) {\n+    return success();\n+  }\n+\n+  auto layout_ref = erase_layout_op.getOperand();\n+  MemRefType layout_ty = getMemRefType(layout_ref);\n+  auto layout_attr = dyn_cast<tpu::TiledLayoutAttr>(layout_ty.getLayout());\n+  if (!layout_attr) {\n+    return emitOpError(\n+        \"Input from EraseLayoutOp is expected to have a TiledLayoutAttr.\");\n+  }\n+  auto &squeezed = squeezed_or.value();\n+  if (squeezed.empty() && source_shape != target_shape) {\n+    return failure();\n+  }\n+\n+  auto tiles = layout_attr.getTiles();\n+  if (tiles.size() == 1) {\n+    auto tile = layout_attr.getTiles().front();\n+    auto tile_dims = tile.dimensions();\n+    int first_tiled = source_shape.size() - tile_dims.size();\n+    for (int dim : squeezed) {\n+      if (dim >= first_tiled) {\n+        int tile_idx = dim - first_tiled;\n+        if (tile_idx < 0 || tile_idx >= static_cast<int>(tile_dims.size())) {\n+          return emitOpError() << \"Internal error: tile index out of bounds.\";\n+        }\n+        if (tile_dims[tile_idx] != 1) {\n+          return emitOpError()\n+                 << \"All tiled squeezed dimensions must be of size 1.\";\n+        }\n+      }\n     }\n-    int source_dim = source_shape[source_index];\n-    if (source_dim == target_dim) {\n-       source_index--;\n-       target_index--;\n-    } else {\n-       // Only the source dim can be 1 here.\n-       if (source_dim != 1) {\n-         this->emitOpError(error_msg);\n-         return failure();\n-       }\n-       source_index--;\n+  } else {\n+    auto first_tile = tiles.front();\n+    for (int dim : squeezed) {\n+      int first_tiled = source_shape.size() - first_tile.dimensions().size();\n+      if (dim >= first_tiled) {\n+        return emitOpError() << \"When multiple tiles are present, no tiled \"\n+                                \"dimensions can be squeezed.\";\n+      }\n     }\n   }\n+\n   return success();\n }\n \n@@ -222,42 +286,60 @@ LogicalResult MemRefSqueezeOp::canonicalize(MemRefSqueezeOp op,\n   if (!erase_layout) {\n     return failure();\n   }\n-  // Push layout erasure through squeezing. It is important we see the layout\n-  // for lowering and don't make it hard for other ops to query it.\n+\n   auto layout_ref = erase_layout.getOperand();\n-  MemRefType layout_ty = layout_ref.getType();\n+  MemRefType layout_ty = getMemRefType(layout_ref);\n+  auto layout_attr = dyn_cast<tpu::TiledLayoutAttr>(layout_ty.getLayout());\n+  if (!layout_attr) {\n+    return failure();\n+  }\n+\n   auto source_shape = source_type.getShape();\n   auto target_shape = target_type.getShape();\n-  int source_index = source_shape.size() - 1;\n-  int target_index = target_shape.size() - 1;\n-  auto old_layout = dyn_cast<tpu::TiledLayoutAttr>(layout_ty.getLayout());\n-  auto target_strides = old_layout.getTileStrides();\n-  SmallVector<int64_t> tile_strides(target_strides.begin(),\n-                                    target_strides.end());\n-  // We want to remove all strides that correspond to squeezed dimensions and\n-  // update the corresponding output layout.\n-  while (source_index >= 0 || target_index >= 0) {\n-    int target_dim = target_index < 0 ? -1 : target_shape[target_index];\n-    int source_dim = source_shape[source_index];\n-    if (source_dim == target_dim) {\n-       source_index--;\n-       target_index--;\n-    } else {\n-       // Source index must be 1 here (otherwise verification will have failed).\n-       // We are safe to mutate the strides vector here because we are looping\n-       // backwards.\n-       tile_strides.erase(tile_strides.begin() + source_index);\n-       source_index--;\n+  auto squeezed_or = computeSqueezedDimsChecked(op, source_shape, target_shape);\n+  if (failed(squeezed_or)) {\n+    return failure();\n+  }\n+  auto &squeezed = squeezed_or.value();\n+  if (squeezed.empty() && source_shape != target_shape) {\n+    return failure();\n+  }\n+\n+  SmallVector<int64_t> tile_strides =\n+      llvm::to_vector(layout_attr.getTileStrides());\n+  for (int i = squeezed.size() - 1; i >= 0; --i) {\n+    tile_strides.erase(tile_strides.begin() + squeezed[i]);\n+  }\n+\n+  tpu::TiledLayoutAttr new_layout;\n+  bool target_is_1d = target_shape.size() == 1;\n+  auto tiles = layout_attr.getTiles();\n+  if (target_is_1d && tiles.size() == 1) {\n+    auto tile_dims = llvm::to_vector(tiles.front().dimensions());\n+    int first_tiled = source_shape.size() - tile_dims.size();\n+    for (int i = squeezed.size() - 1; i >= 0; --i) {\n+      int dim = squeezed[i];\n+      if (dim >= first_tiled) {\n+        int tile_idx = dim - first_tiled;\n+        if (tile_idx < 0 || tile_idx >= static_cast<int>(tile_dims.size())) {\n+          return op.emitError() << \"Internal error: tile index out of bounds.\";\n+        }\n+        tile_dims.erase(tile_dims.begin() + tile_idx);\n+      }\n     }\n+    new_layout = tpu::TiledLayoutAttr::get(\n+        op.getContext(), {xla::Tile(tile_dims)}, tile_strides);\n+  } else {\n+    new_layout = tpu::TiledLayoutAttr::get(\n+        op.getContext(), layout_attr.getTiles(), tile_strides);\n   }\n-  auto new_layout = tpu::TiledLayoutAttr::get(\n-      source_type.getContext(), old_layout.getTiles(), tile_strides);\n-  auto new_result_type = MemRefType::get(op.getResult().getType().getShape(),\n-                                         layout_ty.getElementType(), new_layout,\n-                                         layout_ty.getMemorySpace());\n-  auto squeeze = rewriter.create<MemRefSqueezeOp>(op.getLoc(), new_result_type,\n-                                                  layout_ref);\n-  rewriter.replaceOpWithNewOp<EraseLayoutOp>(op, op.getType(), squeeze);\n+\n+  auto new_ty = MemRefType::get(target_shape, layout_ty.getElementType(),\n+                                new_layout, layout_ty.getMemorySpace());\n+\n+  auto new_squeeze =\n+      rewriter.create<MemRefSqueezeOp>(op.getLoc(), new_ty, layout_ref);\n+  rewriter.replaceOpWithNewOp<tpu::EraseLayoutOp>(op, target_type, new_squeeze);\n   return success();\n }\n \n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "remove :custom_call and :runtime from mosaic_gpu since they are in :mosaic_gpu_support now.\n\nPiperOrigin-RevId: 757881025",
        "output": "```diff\nCommit: f6b9f7d7e32272ff10d447fb7c986edbe3fd3ef8\nDate: 2025-05-12T20:06:45Z\nURL: https://github.com/jax-ml/jax/commit/f6b9f7d7e32272ff10d447fb7c986edbe3fd3ef8\nFiles changed: 1\nAdditions: +1, Deletions: -4\ndiff --git a/jaxlib/mosaic/gpu/BUILD b/jaxlib/mosaic/gpu/BUILD\nindex b694258fed1e..115d0c47cc52 100644\n--- a/jaxlib/mosaic/gpu/BUILD\n+++ b/jaxlib/mosaic/gpu/BUILD\n@@ -23,10 +23,7 @@ package(\n py_library(\n     name = \"mosaic_gpu\",\n     data = [\":libmosaic_gpu_runtime.so\"],\n-    deps = [\n-        \":_mosaic_gpu_ext\",\n-        \":mosaic_gpu_support\",\n-    ],\n+    deps = [\":_mosaic_gpu_ext\"],\n )\n \n cc_library(\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28695 from jakevdp:array-api-tests\n\nPiperOrigin-RevId: 757862940",
        "output": "```diff\nCommit: 76e5bc6f5d5c89c9d56d3211dba1953ae8d20cde\nDate: 2025-05-12T19:15:19Z\nURL: https://github.com/jax-ml/jax/commit/76e5bc6f5d5c89c9d56d3211dba1953ae8d20cde\nFiles changed: 2\nAdditions: +4, Deletions: -1\ndiff --git a/.github/workflows/jax-array-api.yml b/.github/workflows/jax-array-api.yml\nindex 825d3ada9a0b..6419cb730b71 100644\n--- a/.github/workflows/jax-array-api.yml\n+++ b/.github/workflows/jax-array-api.yml\n@@ -32,7 +32,7 @@ jobs:\n       with:\n         repository: data-apis/array-api-tests\n         # TODO(jakevdp) update this to a stable release/tag when available.\n-        ref: 'c48410f96fc58e02eea844e6b7f6cc01680f77ce'  # Latest commit as of 2025-04-02\n+        ref: 'c847143beb8d769bde5dbcc063fe19ed7acc2f9b'  # Latest commit as of 2025-05-12\n         submodules: 'true'\n         path: 'array-api-tests'\n     - name: Install dependencies\ndiff --git a/tests/array_api_skips.txt b/tests/array_api_skips.txt\nindex 7534cf6f8acd..7781b93e7820 100644\n--- a/tests/array_api_skips.txt\n+++ b/tests/array_api_skips.txt\n@@ -2,6 +2,7 @@\n \n # finfo return type misalignment (https://github.com/data-apis/array-api/issues/405)\n array_api_tests/test_data_type_functions.py::test_finfo[float32]\n+array_api_tests/test_data_type_functions.py::test_finfo[complex64]\n \n # Test suite attempts in-place mutation:\n array_api_tests/test_array_object.py::test_setitem\n@@ -28,6 +29,8 @@ array_api_tests/test_special_cases.py::test_iop[__ifloordiv__(x1_i is -0 and x2_\n array_api_tests/test_special_cases.py::test_iop[__ifloordiv__(x1_i is -0 and x2_i < 0) -> +0]\n array_api_tests/test_special_cases.py::test_iop[__ifloordiv__(isfinite(x1_i) and x1_i < 0 and x2_i is -infinity) -> +0]\n \n+# Array API expects default value for axis argument.\n+array_api_tests/test_indexing_functions.py::test_take_along_axis\n \n # Returns int32 when int64 is expected\n array_api_tests/test_searching_functions.py::test_searchsorted\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28662 from dsuedholt:faster-stft\n\nPiperOrigin-RevId: 757839805",
        "output": "```diff\nCommit: b845ecdc2c79c439ac10099946f94ab217828d60\nDate: 2025-05-12T18:17:49Z\nURL: https://github.com/jax-ml/jax/commit/b845ecdc2c79c439ac10099946f94ab217828d60\nFiles changed: 1\nAdditions: +3, Deletions: -7\ndiff --git a/jax/_src/scipy/signal.py b/jax/_src/scipy/signal.py\nindex 565909e8a6d1..f8c2563027f5 100644\n--- a/jax/_src/scipy/signal.py\n+++ b/jax/_src/scipy/signal.py\n@@ -566,13 +566,9 @@ def _fft_helper(x: Array, win: Array, detrend_func: Callable[[Array], Array],\n     result = x[..., np.newaxis]\n   else:\n     step = nperseg - noverlap\n-    batch_shape = list(batch_shape)\n-    x = x.reshape((math.prod(batch_shape), signal_length, 1))\n-    result = jax.lax.conv_general_dilated_patches(\n-        x, (nperseg,), (step,),\n-        'VALID',\n-        dimension_numbers=('NTC', 'OIT', 'NTC'))\n-    result = result.reshape(*batch_shape, *result.shape[-2:])\n+    starts = jnp.arange(signal_length - nperseg + 1, step=step)\n+    slice_func = partial(jax.lax.dynamic_slice_in_dim, operand=x, slice_size=nperseg, axis=-1)\n+    result = jax.vmap(slice_func, out_axes=-2)(start_index=starts)\n \n   # Detrend each data segment individually\n   result = detrend_func(result)\n\n```",
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[array API] update test suite to most recent version",
        "output": "```diff\nCommit: 3b9865af0c418634811284953f76e70c347319e9\nDate: 2025-05-12T17:35:04Z\nURL: https://github.com/jax-ml/jax/commit/3b9865af0c418634811284953f76e70c347319e9\nFiles changed: 2\nAdditions: +4, Deletions: -1\ndiff --git a/.github/workflows/jax-array-api.yml b/.github/workflows/jax-array-api.yml\nindex 825d3ada9a0b..6419cb730b71 100644\n--- a/.github/workflows/jax-array-api.yml\n+++ b/.github/workflows/jax-array-api.yml\n@@ -32,7 +32,7 @@ jobs:\n       with:\n         repository: data-apis/array-api-tests\n         # TODO(jakevdp) update this to a stable release/tag when available.\n-        ref: 'c48410f96fc58e02eea844e6b7f6cc01680f77ce'  # Latest commit as of 2025-04-02\n+        ref: 'c847143beb8d769bde5dbcc063fe19ed7acc2f9b'  # Latest commit as of 2025-05-12\n         submodules: 'true'\n         path: 'array-api-tests'\n     - name: Install dependencies\ndiff --git a/tests/array_api_skips.txt b/tests/array_api_skips.txt\nindex 7534cf6f8acd..7781b93e7820 100644\n--- a/tests/array_api_skips.txt\n+++ b/tests/array_api_skips.txt\n@@ -2,6 +2,7 @@\n \n # finfo return type misalignment (https://github.com/data-apis/array-api/issues/405)\n array_api_tests/test_data_type_functions.py::test_finfo[float32]\n+array_api_tests/test_data_type_functions.py::test_finfo[complex64]\n \n # Test suite attempts in-place mutation:\n array_api_tests/test_array_object.py::test_setitem\n@@ -28,6 +29,8 @@ array_api_tests/test_special_cases.py::test_iop[__ifloordiv__(x1_i is -0 and x2_\n array_api_tests/test_special_cases.py::test_iop[__ifloordiv__(x1_i is -0 and x2_i < 0) -> +0]\n array_api_tests/test_special_cases.py::test_iop[__ifloordiv__(isfinite(x1_i) and x1_i < 0 and x2_i is -infinity) -> +0]\n \n+# Array API expects default value for axis argument.\n+array_api_tests/test_indexing_functions.py::test_take_along_axis\n \n # Returns int32 when int64 is expected\n array_api_tests/test_searching_functions.py::test_searchsorted\n\n```",
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "[platform_dependent] Ensure that platform_dependent only lowers for intended platforms\n\nFixes: #28594\n\nCurrently `lax.platform_dependent` allows specifying code that behaves\ndifferently when lowered on different platforms. However, this function\noperates in a confusing way, in that it will create a branch on the\nplatform, but will lower all branches for the **current** lowering platforms.\n\nFor example, in the following code:\n```\n   lax.platform_dependent(x,\n                          cpu=for_cpu, tpu=for_tpu)\n```\n\nIf we lower for CPU, we lower both `for_cpu` and `for_tpu`\nfor CPU (!), but only the branch corresponding to `for_cpu`\nwill actually run.\n\nThis is a problem if, e.g., `for_tpu` does not have a lowering\nfor CPU. We will get an error during lowering. Instead there should\nbe no error during lowering, because that branch is not actually needed.\n\nWe add a new test `test_platform_dependent_with_primitive_with_lowering_error`\nto demonstrate this.\n\nThe solution implememented here is the Solution A from #28594: we\nadd a `branches_platform` param to the `cond` primitive, which is\npropagated by all transformations. This param is used only for the\nconditionals arising from `lax.platform_dependendet`.\nDuring lowering we drop the branches corresponding to the platforms\nthat are not interesting.",
        "output": "```diff\nCommit: f2121a72fc97c555eda6f519dba28dfe883e62cb\nDate: 2025-05-12T15:58:03Z\nURL: https://github.com/jax-ml/jax/commit/f2121a72fc97c555eda6f519dba28dfe883e62cb\nFiles changed: 8\nAdditions: +201, Deletions: -90\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex 5a6456762db7..144cbaf5cd21 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -759,7 +759,8 @@ def jaxpr_to_checkify_jaxpr(\n   out_tree, error_effects = metadata()\n   return checked_jaxpr, out_tree, error_effects\n \n-def cond_error_check(error: Error, enabled_errors, index, *ops, branches):\n+def cond_error_check(error: Error, enabled_errors, index, *ops,\n+                     branches, **params):\n   # Get the error-effects out of all branches so the cond can be called with\n   # a merged error with all these effects.\n   err_vals, err_tree = jtu.tree_flatten(error)\n@@ -780,7 +781,7 @@ def get_error_effects_from_jaxpr(jxpr):\n \n   err_and_outs = lax.cond_p.bind(\n       index, *err_vals, *ops,\n-      branches=tuple(new_branches))\n+      branches=tuple(new_branches), **params)\n \n   # we need to merge metadata across out_trees (a tuple)\n   err0, out = tree_unflatten(out_trees[0], err_and_outs)\ndiff --git a/jax/_src/interpreters/mlir.py b/jax/_src/interpreters/mlir.py\nindex e9deb8d3fff9..f6ef5787ccbf 100644\n--- a/jax/_src/interpreters/mlir.py\n+++ b/jax/_src/interpreters/mlir.py\n@@ -2080,6 +2080,11 @@ def _platforms_for_eqn_ctx(eqn_ctx: core.JaxprEqnContext | None\n     return ('tpu',)\n   return ()\n \n+def _platforms_for_eqn(ctx: LoweringRuleContext) -> tuple[str, ...]:\n+  \"\"\"The lowering platforms for the current eqn\"\"\"\n+  return tuple((_platforms_for_eqn_ctx(ctx.jaxpr_eqn_ctx) or\n+               ctx.platforms or ctx.module_context.platforms))\n+\n \n def lower_per_platform(ctx: LoweringRuleContext,\n                        description: str,\n@@ -2122,8 +2127,7 @@ def lower_per_platform(ctx: LoweringRuleContext,\n    rule_args: the args of the lowering rules.\n    rule_kwargs: the kwargs of the lowering rules.\n   \"\"\"\n-  platforms: Sequence[str] = (_platforms_for_eqn_ctx(ctx.jaxpr_eqn_ctx) or\n-                              ctx.platforms or ctx.module_context.platforms)\n+  platforms: Sequence[str] = _platforms_for_eqn(ctx)\n   # Special case the common case (single-platform lowering)\n   if len(platforms) == 1:\n     rule = platform_rules.get(platforms[0], default_rule)\ndiff --git a/jax/_src/lax/control_flow/__init__.py b/jax/_src/lax/control_flow/__init__.py\nindex f89e4d53a476..44ee94e14ca2 100644\n--- a/jax/_src/lax/control_flow/__init__.py\n+++ b/jax/_src/lax/control_flow/__init__.py\n@@ -34,6 +34,7 @@\n     while_p as while_p,\n )\n from jax._src.lax.control_flow.conditionals import (\n+    BranchesPlatforms as BranchesPlatforms,\n     cond as cond,\n     cond_p as cond_p,\n     switch as switch,\ndiff --git a/jax/_src/lax/control_flow/conditionals.py b/jax/_src/lax/control_flow/conditionals.py\nindex 99fa72421ea1..d875989921d0 100644\n--- a/jax/_src/lax/control_flow/conditionals.py\n+++ b/jax/_src/lax/control_flow/conditionals.py\n@@ -46,6 +46,7 @@\n from jax._src.interpreters import xla\n from jax._src.lax import lax\n from jax._src.traceback_util import api_boundary\n+from jax._src.typing import ArrayLike\n from jax._src.util import safe_map, split_list, partition_list, unzip2\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n@@ -127,9 +128,17 @@ def switch(index, branches, *operands):\n   lo = np.array(0, np.int32)\n   hi = np.array(len(branches) - 1, np.int32)\n   index = lax.clamp(lo, index, hi)\n+  return _switch_internal(index, branches, operands,\n+                          branches_platforms=None)\n \n+\n+def _switch_internal(\n+    index: ArrayLike,\n+    branches: Sequence[Callable],\n+    operands: Sequence[ArrayLike], *,\n+    branches_platforms: BranchesPlatforms | None):\n   if (config.disable_jit.value and core.is_concrete(index)):\n-    return branches[int(index)](*operands)\n+    return branches[int(index)](*operands)  # type: ignore\n \n   dbgs = [api_util.debug_info(\"switch\", branch, operands, {})\n           for branch in branches]\n@@ -159,7 +168,10 @@ def switch(index, branches, *operands):\n     raise NotImplementedError(\n         f'Effects not supported in `switch`: {disallowed_effects}')\n   jaxprs = [replace_jaxpr_effects(jaxpr, joined_effects) for jaxpr in jaxprs]\n-  out = cond_p.bind(index, *consts, *ops, branches=tuple(jaxprs))\n+  params = dict(branches=tuple(jaxprs))\n+  if branches_platforms is not None:\n+    params[\"branches_platforms\"] = branches_platforms\n+  out = cond_p.bind(index, *consts, *ops, **params)\n   out_ = iter(out)\n \n   all_inputs = [*consts, *ops]\n@@ -464,7 +476,7 @@ def _bcast_select_n(pred, *cases):\n     pred = lax.broadcast_in_dim(pred, np.shape(cases[0]), idx)\n   return lax.select_n(pred, *cases)\n \n-def _cond_batching_rule(axis_data, args, dims, branches):\n+def _cond_batching_rule(axis_data, args, dims, *, branches, **params):\n   index, *ops = args\n   index_dim, *op_dims = dims\n   # TODO(sharadmv): clean this up by adding a specific blocklist\n@@ -480,6 +492,9 @@ def _cond_batching_rule(axis_data, args, dims, branches):\n \n \n   if index_dim is not batching.not_mapped:\n+    assert \"branches_platforms\" not in params, (\n+        \"The index of a cond with branches_platforms should be a \"\n+        \"platform_index and should never be mapped\")\n     # Convert to a lax.select. While we could get away with not broadcasting\n     # some operands yet, because all outputs must be broadcast together anyway\n     # for the select we broadcast the input operands for simplicity and leave\n@@ -518,10 +533,11 @@ def _cond_batching_rule(axis_data, args, dims, branches):\n         for jaxpr in branches)\n \n     out_dims = [0 if b else batching.not_mapped for b in out_bat]\n-    out = cond_p.bind(index, *ops, branches=branches_batched)\n+    out = cond_p.bind(index, *ops, branches=branches_batched,\n+                      **params)\n     return out, out_dims\n \n-def _cond_jvp(primals, tangents, branches):\n+def _cond_jvp(primals, tangents, *, branches, **params):\n   nonzeros = [type(t) is not ad_util.Zero for t in tangents]\n \n   index_nz, *ops_nz = nonzeros\n@@ -538,14 +554,15 @@ def _cond_jvp(primals, tangents, branches):\n   _, *ops_dot = tangents\n   ops_dot = _prune_zeros(ops_dot)\n \n-  out = cond_p.bind(index, *ops, *ops_dot, branches=branches_jvp)\n+  out = cond_p.bind(index, *ops, *ops_dot, branches=branches_jvp,\n+                    **params)\n   out_primals, out_tangents = split_list(out, [len(out_nz)])\n   out_tangents_iter = iter(out_tangents)\n   out_tangents = [next(out_tangents_iter) if nz else ad_util.Zero.from_primal_value(p)\n                   for p, nz in zip(out_primals, out_nz)]\n   return out_primals, out_tangents\n \n-def _cond_partial_eval(trace, *tracers, branches):\n+def _cond_partial_eval(trace, *tracers, branches, **params):\n   in_unknowns = [t.pval[0] is not None for t in tracers]\n   index_uk, *ops_uk = in_unknowns\n   if any(isinstance(eff, RefEffect) for branch in branches for eff in\n@@ -556,7 +573,7 @@ def _cond_partial_eval(trace, *tracers, branches):\n   if index_uk:\n     # When the branch index is unknown, we stage out the whole cond.\n     # TODO(mattjj): remove this path when old remat is removed\n-    params = dict(branches=branches)\n+    params = dict(branches=branches, **params)\n     return trace.default_process_primitive(cond_p, tracers, params)\n \n   branches_out_uks = []\n@@ -586,7 +603,8 @@ def _cond_partial_eval(trace, *tracers, branches):\n              for j in branches_known[1:])\n \n   in_consts = [t.pval.get_known() for t in tracers if t.pval.is_known()]\n-  out_consts_res = cond_p.bind(*in_consts, branches=branches_known)\n+  out_consts_res = cond_p.bind(*in_consts, branches=branches_known,\n+                               **params)\n   out_consts, res = split_list(out_consts_res, [len(out_consts_res) - num_res])\n \n   index_tracer = trace.instantiate_const(tracers[0])\n@@ -595,7 +613,7 @@ def _cond_partial_eval(trace, *tracers, branches):\n   res_tracers = map(trace.new_instantiated_const, res)\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(aval), None)\n                  for aval in branches_unknown[0].out_avals]\n-  params = dict(branches=branches_unknown)\n+  params = dict(branches=branches_unknown, **params)\n   name_stack = source_info_util.current_name_stack()[len(trace.name_stack):]\n   source = source_info_util.current().replace(name_stack=name_stack)\n   eqn = pe.new_eqn_recipe(\n@@ -608,6 +626,7 @@ def _cond_partial_eval(trace, *tracers, branches):\n def _cond_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n   index_uk, *ops_uk = unks_in\n   branches = eqn.params['branches']\n+  eqn_rest_params = dict(k_v for k_v in eqn.params.items() if k_v[0] != 'branches')\n \n   # Instantiate all inputs (b/c jaxpr_staged will take all inputs).\n   new_inst = [x for x, inst in zip(eqn.invars, inst_in)\n@@ -664,7 +683,7 @@ def _cond_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n   # Build the known eqn.\n   ins_known, _ = partition_list(unks_in, eqn.invars)  # includes index invar\n   out_binders_known, _ = partition_list(unks_out, eqn.outvars)\n-  params_known = dict(branches=branches_known)\n+  params_known = dict(branches=branches_known, **eqn_rest_params)\n   effects_known = _join_cond_effects(branches_known)\n   eqn_known = pe.new_jaxpr_eqn(\n       ins_known, [*out_binders_known, *res_binders], cond_p, params_known,\n@@ -672,7 +691,7 @@ def _cond_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n \n   # Build the staged eqn.\n   _, out_binders_staged = partition_list(inst_out, eqn.outvars)\n-  params_staged = dict(branches=branches_staged)\n+  params_staged = dict(branches=branches_staged, **eqn_rest_params)\n   effects_staged = _join_cond_effects(branches_staged)\n   eqn_staged = pe.new_jaxpr_eqn(\n       [eqn.invars[0], *res_binders, *eqn.invars[1:]], out_binders_staged,\n@@ -818,7 +837,7 @@ def transposed(*args):\n                                          debug_info=jaxpr.jaxpr.debug_info),\n                             res_avals + jaxpr.out_avals)\n \n-def _cond_transpose(cts, *args, branches):\n+def _cond_transpose(cts, *args, branches, **params):\n   index, *ops = args\n   assert type(index) is not ad.UndefinedPrimal\n   linear = [type(x) is ad.UndefinedPrimal for x in ops]\n@@ -838,7 +857,8 @@ def _cond_transpose(cts, *args, branches):\n   res = ops[:num_res]\n   cts = map(ad.instantiate_zeros, cts)\n \n-  out = cond_p.bind(index, *res, *cts, branches=branches_trans)\n+  out = cond_p.bind(index, *res, *cts, branches=branches_trans,\n+                    **params)\n   assert all(map(core.typecheck, lin_in_avals, out))\n \n   out_iter = iter(out)\n@@ -846,7 +866,8 @@ def _cond_transpose(cts, *args, branches):\n   assert next(out_iter, None) is None\n   return [None] + out\n \n-def _cond_typecheck(bind_time, *in_atoms, branches):\n+def _cond_typecheck(bind_time, *in_atoms, branches, **params):\n+  del params\n   if not bind_time:\n     _, *in_atoms = in_atoms\n   avals = [x.aval for x in in_atoms]\n@@ -900,6 +921,16 @@ def _cond_typecheck(bind_time, *in_atoms, branches):\n       f'called with operands of type {_avals_short(op_avals)}')\n   return jaxpr0.out_avals, joined_effects\n \n+\n+BranchesPlatforms = tuple[tuple[str, ...] | None, ...]\n+# cond_p takes an optional branches_platforms param of type `BranchesPlatforms`\n+# when it is a `platform_dependent` conditional.\n+# In that case, `branches_platforms` is a tuple as long\n+# as `branches` and for each branch it specifies the lowering platforms it\n+# corresponds to. The last element, corresponding to the last branch,\n+# can be `None` to represent a default match-all-lowering-platforms.\n+# The index argument of a `platform_dependent` cond is always a\n+# `platform_index` primitive.\n cond_p = core.Primitive('cond')\n cond_p.multiple_results = True\n cond_p.skip_canonicalization = True\n@@ -915,7 +946,39 @@ def _cond_typecheck(bind_time, *in_atoms, branches):\n pe.dce_rules[cond_p] = _cond_dce_rule\n batching.ragged_prop_rules[cond_p] = batching.ragged_mask_assert_no_op_rule\n \n-def _cond_lowering(ctx, index, *args, branches):\n+def _cond_lowering(ctx, index, *args, branches,\n+                   **params):\n+  if (branches_platforms := params.get(\"branches_platforms\", None)) is not None:\n+    branches_kept: list[core.ClosedJaxpr] = []\n+    index_to_kept_index: dict[int, int] = {}\n+    for p in mlir._platforms_for_eqn(ctx):\n+      # Each `p` must appear in exactly one branches_platforms, or in the\n+      # last default branch. Otherwise, platform_index lowering would have\n+      # failed already.\n+      for b_idx, b_platforms in enumerate(branches_platforms):\n+        if b_platforms is None or p in b_platforms:\n+          if b_idx not in index_to_kept_index:\n+            index_to_kept_index[b_idx] = len(branches_kept)\n+            branches_kept.append(branches[b_idx])\n+          break\n+      else:\n+        assert False, p\n+\n+    # Compute the new index into branches_keep\n+    i32_type = ir.RankedTensorType.get([], mlir.dtype_to_ir_type(dtypes.dtype(np.int32)))\n+    kept_index_case_op = hlo.CaseOp([i32_type],\n+                                    index=index,\n+                                    num_branches=len(branches))\n+    for i in range(len(branches)):\n+      branch = kept_index_case_op.regions[i].blocks.append()\n+      with ir.InsertionPoint(branch):\n+        kept_i = np.int32(index_to_kept_index.get(i, 0))\n+        hlo.return_([mlir.ir_constant(kept_i)])\n+\n+    index = kept_index_case_op\n+    branches = branches_kept\n+    assert branches, \"platform_index lowering should have failed first\"\n+\n   joined_effects = core.join_effects(*(branch.effects for branch in branches))\n   ordered_effects = list(effects.ordered_effects.filter_in(joined_effects))\n   num_tokens = len(ordered_effects)\n@@ -952,7 +1015,8 @@ def _cond_lowering(ctx, index, *args, branches):\n mlir.register_lowering(cond_p, _cond_lowering)\n \n @register_partial_discharge_rule(cond_p)\n-def _cond_state_discharge_rule(should_discharge, in_avals, out_avals, index, *args, branches):\n+def _cond_state_discharge_rule(should_discharge, in_avals, out_avals, index, *args,\n+                               branches, **params):\n   assert not should_discharge[0], \"Can't discharge the index.\"\n   discharged_branches = tuple(\n       discharge_state(branch.jaxpr, (), should_discharge=should_discharge[1:])[0]\n@@ -981,7 +1045,8 @@ def _cond_state_discharge_rule(should_discharge, in_avals, out_avals, index, *ar\n                                   if fwd is None]), ())\n       for branch in discharged_branches\n   )\n-  out_vals_no_fwd = cond_p.bind(index, *args, branches=new_branches)\n+  out_vals_no_fwd = cond_p.bind(index, *args, branches=new_branches,\n+                                **params)\n   out_vals, out_ref_vals_no_fwd = util.split_list(out_vals_no_fwd, [len(out_avals)])\n   # Insert forwarded values into reference outputs\n   ref_val_no_fwd_iter = iter(out_ref_vals_no_fwd)\n@@ -1046,50 +1111,41 @@ def other_platforms_code(*args): ...\n     The value ``per_platform[execution_platform](*args)``.\n   \"\"\"\n   # Join identical branches\n-  platform_branches: list[tuple[list[str], Callable]] = []\n+  branches_platforms_list: list[tuple[list[str], Callable]] = []\n   for pname, pbranch in per_platform.items():\n+    if not callable(pbranch):\n+      raise TypeError(f\"lax.platform_dependent: the '{pname}' branch must \"\n+                      \"be a callable.\")\n     if pname == \"gpu\":\n       raise ValueError(\"Use 'cuda' or 'rocm' for lax.platform_dependent.\")\n-    for ps, b in platform_branches:\n+    for ps, b in branches_platforms_list:\n       if b == pbranch:\n         ps.append(pname)\n         break\n     else:\n-      platform_branches.append(([pname], pbranch))\n-\n-  platforms_lists, branches = util.unzip2(platform_branches)\n-  platform_index = platform_index_p.bind(\n-    platforms=tuple(tuple(ps) for ps in platforms_lists),\n-    has_default=(default is not None))\n+      branches_platforms_list.append(([pname], pbranch))\n \n+  platforms_lists, branches = util.unzip2(branches_platforms_list)\n+  branches_platforms: BranchesPlatforms = tuple(tuple(ps) for ps in platforms_lists)\n   if default is not None:\n+    if not callable(default):\n+      raise TypeError(\"lax.platform_dependent: the 'default' branch must \"\n+                      \"be a callable.\")\n     branches = branches + (default,)\n-  # Use a switch, to get the proper transformation rules for free. Since\n-  # platform index has no dependence on the input data, it won't be vectorized\n-  # under vmap.\n-  # If the switch and the platform_index_p above are in the same compilation\n-  # unit then constant-folding will remove the unnecessary branches. However,\n-  # if we run in eager mode the switch below cannot be constant-folded and\n-  # the compilation may fail if some of the branches contain custom calls not\n-  # recognized on the compilation platform. Detect eager mode and keep only the\n-  # needed branch.\n-  try:\n-    # Note/TODO(mvoz): This actually rarely seems to concretize - we could look into\n-    # core.ensure_compile_time_eval to get better single-branch selection.\n-    platform_index_concrete = core.concrete_or_error(operator.index, platform_index)\n-  except core.ConcretizationTypeError:\n-    return switch(platform_index, branches, *args)\n-  else:\n-    assert 0 <= platform_index_concrete < len(branches)\n-    return branches[platform_index_concrete](*args)\n+    branches_platforms = branches_platforms + (None,)  # type: ignore\n+  platform_index = platform_index_p.bind(platforms=branches_platforms)\n+\n+  if core.is_concrete(platform_index):\n+    return branches[int(platform_index)](*args)\n+  return _switch_internal(platform_index, branches, args,\n+                          branches_platforms=branches_platforms)\n+\n \n # A primitive to compute the index of a platform into a list of platforms.\n # Args:\n-#   platforms: Sequence[Sequence[str]]: a sequence of sequences of platform\n-#     names. If the current lowering platform is in one of the inner sequences\n-#     returns the index of that inner sequence in the outer sequence.\n-#   has_default: if True, and if the lowering platform is not found in\n-#     `platforms` then return `len(platforms)`. Otherwise, raise an error.\n+#   platforms: BranchesPlatforms. If the current lowering\n+#     platform is in one of the inner tuples returns the index of that inner\n+#     tuple in the outer tuple.\n platform_index_p = core.Primitive(\"platform_index\")\n platform_index_p.multiple_results = False\n platform_index_p.def_impl(functools.partial(dispatch.apply_primitive,\n@@ -1101,25 +1157,25 @@ def _platform_index_aval(*_, **__):\n \n def _platform_index_lowering(ctx: mlir.LoweringRuleContext,\n                              *,\n-                             platforms: Sequence[Sequence[str]],\n-                             has_default: bool):\n-  def lower_constant(\n-      ctx: mlir.LoweringRuleContext, *, i: int\n-  ) -> Sequence[ir.Value]:\n+                             platforms: BranchesPlatforms):\n+  def lower_constant(ctx: mlir.LoweringRuleContext, *,\n+                     i: int) -> Sequence[ir.Value]:\n     v = mlir.ir_constant(np.int32(i))\n-    assert isinstance(v, ir.Value), v\n     return [v]\n+\n   platform_rules: dict[str, mlir.LoweringRule] = {}\n+  default_rule = None\n   for i, ps in enumerate(platforms):\n     rule = partial(lower_constant, i=i)\n-    for p in ps:\n-      platform_rules[p] = rule\n+    if ps is None:\n+      default_rule = rule\n+    else:\n+      for p in ps:\n+        platform_rules[p] = rule\n \n-  default_rule = (\n-    partial(lower_constant, i=len(platforms)) if has_default else None)\n   return mlir.lower_per_platform(\n     ctx,\n-    f\"platform_index(platforms={platforms}, has_default={has_default})\",\n+    f\"platform_index(platforms={platforms})\",\n     platform_rules, default_rule, effects.no_effects)\n \n mlir.register_lowering(platform_index_p, _platform_index_lowering)\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 1ea5a048a17e..bba49c75f9df 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -47,7 +47,7 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.lax import control_flow\n from jax._src.lax import lax as lax_internal\n-from jax._src.lax.control_flow import for_loop\n+from jax._src.lax.control_flow import for_loop, BranchesPlatforms\n from jax._src.lib import version as jaxlib_version\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import arith\n@@ -3100,7 +3100,7 @@ def _while_lowering_rule(\n \n \n @register_lowering_rule(lax.cond_p)\n-def _cond_lowering_rule(ctx: LoweringRuleContext, *args, branches):\n+def _cond_lowering_rule(ctx: LoweringRuleContext, *args, branches, **params):\n   index, *args = args\n   constant_index = _fold_and_get_constant_value(index)\n \n@@ -3870,17 +3870,13 @@ def _pad(val):\n def _platform_index_lowering(\n     ctx: mlir.LoweringRuleContext,\n     *,\n-    platforms: Sequence[Sequence[str]],\n-    has_default: bool,\n+    platforms: BranchesPlatforms,\n ):\n   for i, ps in enumerate(platforms):\n     # note - slightly odd structure here, as platforms is a seq[seq[str]]\n-    if \"mosaic\" in ps:\n+    if \"mosaic\" in ps or ps is None:\n       return ir_constant(i)\n \n-  if has_default:\n-    return ir_constant(len(platforms))\n-\n   raise NotImplementedError(\n       \"No mosaic or default platform indexing rule found.\"\n   )\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex b501693bf627..9ead4f16c1a6 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -2598,7 +2598,10 @@ def _while_lowering_rule(\n @register_lowering_rule(lax.cond_p,\n   mgpu.LoweringSemantics.Lane, gpu_core.PrimitiveSemantics.Warp)\n @register_lowering_rule(lax.cond_p, mgpu.LoweringSemantics.Warpgroup)\n-def _cond_lowering_rule(ctx: LoweringRuleContext, index, *args, branches):\n+def _cond_lowering_rule(ctx: LoweringRuleContext, index, *args, branches,\n+                        **params):\n+  if params:\n+    raise NotImplementedError(\"platform_dependent cond\")\n   index_aval, *_arg_avals = ctx.avals_in\n \n   def _yielded_values(outs, avals):\ndiff --git a/jax/experimental/jax2tf/jax2tf.py b/jax/experimental/jax2tf/jax2tf.py\nindex 4c2f35a95c57..786e021e2ff0 100644\n--- a/jax/experimental/jax2tf/jax2tf.py\n+++ b/jax/experimental/jax2tf/jax2tf.py\n@@ -3062,8 +3062,11 @@ def update_computation(arg1: TfVal, arg2: TfVal) -> TfVal:\n \n \n def _cond(\n-    index: TfVal, *operands: TfVal, branches: Sequence[core.ClosedJaxpr]\n+    index: TfVal, *operands: TfVal, branches: Sequence[core.ClosedJaxpr],\n+    **params\n ) -> Sequence[TfVal]:\n+  if params:\n+    raise NotImplementedError(\"jax2tf conversion for platform_dependent\")\n   # tf.cond needs lambdas with no arguments.\n   branches_tf = [\n       partial(_interpret_jaxpr, jaxpr, *operands,\ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex 422ef769e392..d32d761ee1fa 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -37,8 +37,10 @@\n from jax.ad_checkpoint import checkpoint as new_checkpoint, checkpoint_policies\n import jax.numpy as jnp  # scan tests use numpy\n import jax.scipy as jsp\n+from jax._src import dispatch\n from jax._src.lax import control_flow as lax_control_flow\n from jax._src.lax.control_flow import for_loop\n+from jax._src.interpreters import batching\n from jax._src.interpreters import mlir\n \n jax.config.parse_flags_with_absl()\n@@ -137,6 +139,36 @@ def scan_reference(f, init, xs):\n     lambda ctx, x: mlir.hlo.CustomCallOp(\n         [x.type], [x],\n         call_target_name=mlir.ir.StringAttr.get(\"__testing_non_existent_custom_call\")).results)\n+batching.primitive_batchers[prim_non_existent_custom_call] = (\n+    lambda batched_args, batch_dims: (prim_non_existent_custom_call.bind(batched_args[0]),\n+                                      batch_dims[0]))\n+\n+# A JAX primitive that triggers error when lowering on unintended platforms\n+prim_with_lowering_error = core.Primitive(\"__testing_prim_with_lowering_error\")\n+prim_with_lowering_error.def_abstract_eval(lambda x_aval, **_: x_aval)\n+def prim_with_lowering_error_lowering(platform: str,\n+                                      ctx: mlir.LoweringRuleContext, x, *,\n+                                      only_on: str):\n+  if platform != only_on:\n+    raise ValueError(f\"prim_with_lowering_error with only_on={only_on} lowered for {platform}\")\n+  return mlir.hlo.SineOp(x).results\n+def prim_with_lowering_error_batch_rule(batched_args, batch_dims, **params):\n+  xs, = batched_args\n+  xs_bdim, = batch_dims\n+  return prim_with_lowering_error.bind(xs, **params), xs_bdim\n+\n+batching.primitive_batchers[prim_with_lowering_error] = prim_with_lowering_error_batch_rule\n+\n+mlir.register_lowering(\n+    prim_with_lowering_error,\n+    partial(prim_with_lowering_error_lowering, \"cpu\"),\n+    platform=\"cpu\")\n+mlir.register_lowering(\n+    prim_with_lowering_error,\n+    partial(prim_with_lowering_error_lowering, \"tpu\"),\n+    platform=\"tpu\")\n+prim_with_lowering_error.def_impl(partial(dispatch.apply_primitive,\n+                                          prim_with_lowering_error))\n \n \n class LaxControlFlowTest(jtu.JaxTestCase):\n@@ -1378,7 +1410,7 @@ def f(x):\n   @parameterized.named_parameters(\n       {\"testcase_name\": f\"_{name}\", \"cond\": cond}\n       for cond, name in COND_IMPLS)\n-  def testCondGrad2(self, cond):\n+  def testCondGrad2(self, cond=cond_with_new_checkpoint):\n     def f_ref(x):\n       z = jnp.array([1., 2.], x.dtype) * x if x[0] < 2 else jnp.sin(x)\n       return z.sum()\n@@ -2905,18 +2937,13 @@ def f(x):\n     x = np.arange(3, dtype=np.float32)\n     lowered = jax.jit(f).lower(x)\n     stablehlo = lowered.as_text()\n-    self.assertIn(\"stablehlo.case\", stablehlo)\n-    self.assertIn(\"stablehlo.sine\", stablehlo)\n-    self.assertIn(\"stablehlo.cosine\", stablehlo)\n-\n-    # The HLO has been canonicalized and contains only the branch we need\n-    hlo = lowered.as_text(\"hlo\")\n+    # The StableHLO contains only the branch we need\n     if jtu.device_under_test() == \"cpu\":\n-      self.assertIn(\" sine\", hlo)\n-      self.assertNotIn(\" cosine\", hlo)\n+      self.assertIn(\"stablehlo.sine\", stablehlo)\n+      self.assertNotIn(\"stablehlo.cosine\", stablehlo)\n     else:\n-      self.assertNotIn(\" sine\", hlo)\n-      self.assertIn(\" cosine\", hlo)\n+      self.assertNotIn(\"stablehlo.sine\", stablehlo)\n+      self.assertIn(\"stablehlo.cosine\", stablehlo)\n \n   def test_platform_dependent_with_non_existent_custom_call(self):\n     if not jtu.test_device_matches([\"cpu\"]):\n@@ -2939,8 +2966,7 @@ def f(x):\n \n     x = np.arange(3, dtype=np.float32)\n     hlo = str(jax.jit(f).lower(x).compiler_ir())\n-    occurrences = re.findall(prim_non_existent_custom_call.name, hlo)\n-    self.assertLen(occurrences, 3)\n+    self.assertNotIn(prim_non_existent_custom_call.name, hlo)\n \n     res_eager = f(x)\n     self.assertAllClose(res_eager, 3. * np.sin(x))\n@@ -2956,6 +2982,26 @@ def f(x):\n     res_grad = jax.grad(f)(1.)\n     self.assertAllClose(res_grad, 3. * np.cos(1.))\n \n+  def test_platform_dependent_with_primitive_with_lowering_error(self):\n+    if not jtu.test_device_matches([\"cpu\", \"tpu\"]):\n+      self.skipTest(\"Only for CPU and TPU\")\n+\n+    def f(x):\n+      return lax.platform_dependent(\n+          x,\n+          # Check that we only lower on the intended platform\n+          cpu=lambda x: prim_with_lowering_error.bind(x, only_on=\"cpu\"),\n+          tpu=lambda x: prim_with_lowering_error.bind(x, only_on=\"tpu\"))\n+\n+    self.assertAllClose(np.sin(1.), f(1.))  # Eager\n+    self.assertAllClose(np.sin(1.), jax.jit(f)(1.))\n+    self.assertAllClose(np.sin(1.), lax.cond(True, f, lambda x: x, 1.))\n+    self.assertAllClose(1., lax.cond(False, f, lambda x: x, 1.))\n+    self.assertAllClose((0., np.sin(np.arange(8.))),\n+                        lax.scan(lambda carry, x: (carry, f(x)),\n+                                 0., np.arange(8.)))\n+    self.assertAllClose(np.sin(np.arange(8.)), jax.vmap(f)(np.arange(8.)))\n+\n   def test_platform_dependent_multiple_identical_branches(self):\n     x = np.arange(3, dtype=np.float32)\n     def f(x):\n@@ -2965,13 +3011,14 @@ def f(x):\n         tpu=jnp.sin,\n         default=lambda x: x)\n     res = f(x)\n+    on_cpu_tpu = jtu.device_under_test() in [\"cpu\", \"tpu\"]\n     self.assertAllClose(\n       res,\n-      np.sin(x) if jtu.device_under_test() in [\"cpu\", \"tpu\"] else x)\n-    # We only lower the common branches once\n+      np.sin(x) if on_cpu_tpu else x)\n+\n     stablehlo = jax.jit(f).lower(x).as_text()\n     sines = re.findall(r\"stablehlo.sine\", stablehlo)\n-    self.assertEqual(1, len(sines))\n+    self.assertEqual(1 if on_cpu_tpu else 0, len(sines))\n \n   def test_platform_dependent_no_default(self):\n     ctx = contextlib.ExitStack()\n\n```",
        "from_id": [
            "gnecula"
        ]
    },
    {
        "text_input": "Speed up `scipy.signal.stft` by using `lax.dynamic_slice_in_dim` for windowing",
        "output": "```diff\nCommit: e65b317a4cef945eec0ed2378442df743fa1ee31\nDate: 2025-05-12T15:40:46Z\nURL: https://github.com/jax-ml/jax/commit/e65b317a4cef945eec0ed2378442df743fa1ee31\nFiles changed: 1\nAdditions: +3, Deletions: -7\ndiff --git a/jax/_src/scipy/signal.py b/jax/_src/scipy/signal.py\nindex 565909e8a6d1..f8c2563027f5 100644\n--- a/jax/_src/scipy/signal.py\n+++ b/jax/_src/scipy/signal.py\n@@ -566,13 +566,9 @@ def _fft_helper(x: Array, win: Array, detrend_func: Callable[[Array], Array],\n     result = x[..., np.newaxis]\n   else:\n     step = nperseg - noverlap\n-    batch_shape = list(batch_shape)\n-    x = x.reshape((math.prod(batch_shape), signal_length, 1))\n-    result = jax.lax.conv_general_dilated_patches(\n-        x, (nperseg,), (step,),\n-        'VALID',\n-        dimension_numbers=('NTC', 'OIT', 'NTC'))\n-    result = result.reshape(*batch_shape, *result.shape[-2:])\n+    starts = jnp.arange(signal_length - nperseg + 1, step=step)\n+    slice_func = partial(jax.lax.dynamic_slice_in_dim, operand=x, slice_size=nperseg, axis=-1)\n+    result = jax.vmap(slice_func, out_axes=-2)(start_index=starts)\n \n   # Detrend each data segment individually\n   result = detrend_func(result)\n\n```",
        "from_id": [
            "dsuedholt"
        ]
    }
]