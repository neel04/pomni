[
    {
        "text_input": "[Mosaic] Move i1 broadcast lowering logic to Mosaic.\n\nAnd relax the test skip conditions. Somehow we skipped everything before. Also, this should fix https://github.com/jax-ml/jax/issues/29092.\n\nPiperOrigin-RevId: 765380392",
        "output": [
            "```diff\nCommit: 6c18aa8a468e35b8c11b101dceaa43d05b497177\nDate: 2025-05-30T23:33:32Z\nURL: https://github.com/jax-ml/jax/commit/6c18aa8a468e35b8c11b101dceaa43d05b497177\nFiles changed: 3\nAdditions: +69, Deletions: -25\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex e2dfe526ea14..873bf587093a 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -1928,20 +1928,6 @@ def _broadcast_in_dim_lowering_rule(\n   if aval_in.shape == shape:\n     return val\n \n-  if jnp.issubdtype(aval_in.dtype, jnp.bool_):\n-    # Direct broadcasts for bools are not supported in Mosaic due to booleans\n-    # living in mask registers and broadcast operating on vregs. Broadcast as an\n-    # integer instead and cast back to a bool.\n-    # TODO(b/351019164): Implement this logic in Mosaic BroadcastOp instead.\n-    def _proxy_fun(val, *, shape, broadcast_dimensions):\n-      int_val = jnp.where(val, 1, 0)\n-      bcast_val = jax.lax.broadcast_in_dim(int_val, shape, broadcast_dimensions)\n-      return bcast_val == 1\n-    proxy_lowering = lower_fun(\n-        _proxy_fun, multiple_results=False)\n-    return proxy_lowering(\n-        ctx, val, shape=shape, broadcast_dimensions=broadcast_dimensions)\n-\n   if broadcast_dimensions:\n     out_shape_list = [1] * len(shape)\n     for i, s in zip(broadcast_dimensions, aval_in.shape):\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\nindex c963cff0be50..733863546935 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n@@ -566,6 +566,44 @@ LogicalResult canonicalize_extract(const CanonicalizeContext &ctx,\n   return success();\n }\n \n+LogicalResult canonicalize_broadcast(const CanonicalizeContext &ctx,\n+                                     Operation &raw_op) {\n+  auto op = dyn_cast<vector::BroadcastOp>(raw_op);\n+  auto src_ty = op.getSource().getType();\n+  auto src_vty = dyn_cast<VectorType>(src_ty);\n+  if ((src_vty && src_vty.getElementType().isSignlessInteger(1)) ||\n+      op.getSource().getType().isSignlessInteger(1)) {\n+    // Canonicalize i1 broadcast.\n+    // i1 represents vmsk in Mosaic and TPU doesn't support vmsk replication\n+    // directly.\n+    // Instead, convert i1 to i32 vector, broadcast i32, and then convert it\n+    // back to i1.\n+    ImplicitLocOpBuilder builder(op->getLoc(), op.getOperation());\n+    Value i32_src;\n+    if (src_vty) {\n+      i32_src = builder.create<arith::ExtUIOp>(\n+          VectorType::get(src_vty.getShape(), builder.getI32Type()),\n+          op.getSource());\n+    } else {\n+      i32_src =\n+          builder.create<arith::ExtUIOp>(builder.getI32Type(), op.getSource());\n+    }\n+    auto i32_res_vty =\n+        VectorType::get(op.getType().getShape(), builder.getI32Type());\n+    auto bcast = builder.create<vector::BroadcastOp>(i32_res_vty, i32_src);\n+    auto ones = builder.create<arith::ConstantOp>(\n+        i32_res_vty,\n+        SplatElementsAttr::get(i32_res_vty,\n+                               builder.getOneAttr(builder.getI32Type())));\n+    auto cmp =\n+        builder.create<arith::CmpIOp>(arith::CmpIPredicate::eq, bcast, ones);\n+    op.replaceAllUsesWith(cmp.getResult());\n+    op.erase();\n+    return success();\n+  }\n+  return success();\n+}\n+\n LogicalResult canonicalize_select(const CanonicalizeContext &ctx,\n                                   Operation &raw_op) {\n   auto op = dyn_cast<arith::SelectOp>(raw_op);\n@@ -920,6 +958,7 @@ const llvm::StringMap<canonicalize_rule_type> &rules() {\n        canonicalize_multi_dim_reduction},\n       {vector::TransposeOp::getOperationName(), canonicalize_vector_transpose},\n       {vector::ShapeCastOp::getOperationName(), canonicalize_reshape},\n+      {vector::BroadcastOp::getOperationName(), canonicalize_broadcast},\n       {arith::SelectOp::getOperationName(), canonicalize_select},\n       {arith::FPToSIOp::getOperationName(), canonicalize_fptosi},\n       {arith::SIToFPOp::getOperationName(), canonicalize_sitofp},\ndiff --git a/tests/pallas/ops_test.py b/tests/pallas/ops_test.py\nindex c819d050c8a5..e951a9fda827 100644\n--- a/tests/pallas/ops_test.py\n+++ b/tests/pallas/ops_test.py\n@@ -1822,30 +1822,49 @@ def copyitem(x_ref, in_idx_ref, out_idx_ref, o_ref):\n         np.testing.assert_allclose(out[oi], x[ii])\n         np.testing.assert_allclose(out[oi + 1 :], jnp.zeros_like(out[oi + 1 :]))\n \n-  @parameterized.parameters(\n-      ((), (2,), ()),\n-      ((1,), (2,), (0,)),\n-      ((1, 1), (2, 2), (0, 1)),\n-      ((), (2, 2), ()),\n+  @parameterized.product(\n+      shape_spec=[\n+          ((), (2,), ()),\n+          ((1,), (2,), (0,)),\n+          ((1, 128), (8, 128), (0, 1)),  # row broadcasting\n+          ((), (2, 2), ()),\n+      ],\n+      dtype=[jnp.int32, jnp.int16, jnp.int8, jnp.bool_],\n   )\n-  def test_broadcast_in_dim(self, in_shape, out_shape, dims):\n+  def test_broadcast_in_dim(self, shape_spec, dtype):\n     self.skip_if_mosaic_gpu()\n \n-    # The Pallas TPU lowering currently supports only blocks of rank >= 1\n+    in_shape, out_shape, dims = shape_spec\n     if jtu.test_device_matches([\"tpu\"]):\n-      self.skipTest(\"Not supported on TPU\")\n+      if not in_shape:\n+        self.skipTest(\n+            \"The Pallas TPU lowering currently supports only blocks of rank\"\n+            \" >= 1\"\n+        )\n+      if dtype is jnp.bool_ and not jtu.if_cloud_tpu_at_least(2025, 6, 5):\n+        self.skipTest(\"Requires libtpu built after 2025-06-05\")\n+      if (\n+          len(in_shape) == 1\n+          and len(out_shape) == 1\n+          and dtype not in {jnp.int32, jnp.bool_}\n+      ):\n+        self.skipTest(\"Unsupported tiling\")\n \n     @functools.partial(\n         self.pallas_call,\n-        out_shape=jax.ShapeDtypeStruct(out_shape, jnp.float32),\n+        out_shape=jax.ShapeDtypeStruct(out_shape, dtype),\n     )\n     def f(x_ref, o_ref):\n       x = x_ref[...]\n       o_ref[...] = jax.lax.broadcast_in_dim(x, out_shape, dims)\n \n-    x = jnp.arange(int(np.prod(in_shape)), dtype=jnp.float32).reshape(in_shape)\n+    x = (\n+        jnp.arange(math.prod(in_shape), dtype=jnp.int32)\n+        .reshape(in_shape)\n+        .astype(dtype)\n+    )\n     expected = jax.lax.broadcast_in_dim(x, out_shape, dims)\n-    np.testing.assert_allclose(f(x), expected)\n+    np.testing.assert_array_equal(f(x), expected)\n \n   @parameterized.product(\n       lhs_and_rhs_shape=[\n\n```"
        ],
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Allow setting non-string TPU runtime flags. For example:\njax.config.update(\"jax_pjrt_client_create_options\", {\"max_inflight_computations\": 64})\n\nPiperOrigin-RevId: 765357524",
        "output": [
            "```diff\nCommit: 26228f5a0c5dc45147805bd5535443f2f2213dbe\nDate: 2025-05-30T22:24:31Z\nURL: https://github.com/jax-ml/jax/commit/26228f5a0c5dc45147805bd5535443f2f2213dbe\nFiles changed: 2\nAdditions: +23, Deletions: -14\ndiff --git a/jax/_src/config.py b/jax/_src/config.py\nindex e79993958349..9d19ceb8b261 100644\n--- a/jax/_src/config.py\n+++ b/jax/_src/config.py\n@@ -940,11 +940,17 @@ def enum_flag(name, default, *args, **kwargs) -> Flag[str]:\n         'otherwise.'\n         ))\n \n-jax_pjrt_client_create_options = optional_string_state(\n+def _validate_jax_pjrt_client_create_options(new_val):\n+  if new_val is not None and not isinstance(new_val, (str, dict)):\n+      raise ValueError('new string config value must be None or of type dict'\n+                       f' | str, got {new_val} of type {type(new_val)}.')\n+\n+jax_pjrt_client_create_options = string_or_object_state(\n     name='jax_pjrt_client_create_options',\n     default=None,\n     help=('A set of key-value pairs in the format of \"k1:v1;k2:v2\" strings '\n-          'provided to a device platform pjrt client as extra arguments.'))\n+          'provided to a device platform pjrt client as extra arguments.'),\n+    validator=_validate_jax_pjrt_client_create_options)\n \n enable_checks = bool_state(\n     name='jax_enable_checks',\ndiff --git a/jax/_src/xla_bridge.py b/jax/_src/xla_bridge.py\nindex ce0c36fdcca4..22e71d7d9ce6 100644\n--- a/jax/_src/xla_bridge.py\n+++ b/jax/_src/xla_bridge.py\n@@ -449,18 +449,21 @@ def _options_from_jax_configs(plugin_name):\n   options = {}\n \n   pjrt_client_options = config.jax_pjrt_client_create_options.value\n-  pjrt_client_option_list = []\n-  if pjrt_client_options:\n-    pjrt_client_option_list = pjrt_client_options.split(\";\")\n-\n-  for option in pjrt_client_option_list:\n-    option_list = option.split(\":\")\n-    if (len(option_list) != 2):\n-      raise RuntimeError(\n-          \"Multiple ':' separators for option in \"\n-          f\"jax_pjrt_client_create_options: '{option}'. \"\n-          \"Should be in format 'key:value'\")\n-    options[option_list[0]] = option_list[1]\n+  if isinstance(pjrt_client_options, str):\n+    pjrt_client_option_list = []\n+    if pjrt_client_options:\n+      pjrt_client_option_list = pjrt_client_options.split(\";\")\n+\n+    for option in pjrt_client_option_list:\n+      option_list = option.split(\":\")\n+      if (len(option_list) != 2):\n+        raise RuntimeError(\n+            \"Multiple ':' separators for option in \"\n+            f\"jax_pjrt_client_create_options: '{option}'. \"\n+            \"Should be in format 'key:value'\")\n+      options[option_list[0]] = option_list[1]\n+  elif isinstance(pjrt_client_options, dict):\n+    options.update(pjrt_client_options)\n \n   if plugin_name in (\"cuda\", \"rocm\"):\n     visible_devices = (CUDA_VISIBLE_DEVICES.value if plugin_name == \"cuda\"\n\n```"
        ],
        "from_id": [
            "pschuh",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Refactor jax/_src/api.py and associated files in preparation for moving them to their own BUILD rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nThis change stops short of actually making the main jax package build rule depend on the new api build rule, because some downstream targets need to be migrated and pytype errors need to be fixed before we can land the final change.\n\nPiperOrigin-RevId: 765341918",
        "output": [
            "```diff\nCommit: 22f04d92fc1c6c9fa85dd486862f88fce36964f9\nDate: 2025-05-30T21:38:04Z\nURL: https://github.com/jax-ml/jax/commit/22f04d92fc1c6c9fa85dd486862f88fce36964f9\nFiles changed: 11\nAdditions: +135, Deletions: -79\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 91a6e5926e42..67fc208f7841 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -295,8 +295,8 @@ py_library_providing_imports_info(\n     srcs = [\n         \"_src/__init__.py\",\n         \"_src/ad_checkpoint.py\",\n-        \"_src/api.py\",\n-        \"_src/array.py\",\n+        \"_src/api.py\",  # TODO(vanderplas): remove this and depend on :api instead\n+        \"_src/array.py\",  # TODO(vanderplas): remove this and depend on :api instead\n         \"_src/blocked_sampler.py\",\n         \"_src/buffer_callback.py\",\n         \"_src/callback.py\",\n@@ -305,14 +305,14 @@ py_library_providing_imports_info(\n         \"_src/custom_partitioning.py\",\n         \"_src/custom_partitioning_sharding_rule.py\",\n         \"_src/debugging.py\",\n-        \"_src/dispatch.py\",\n+        \"_src/dispatch.py\",  # TODO(vanderplas): remove this and depend on :api instead\n         \"_src/dlpack.py\",\n         \"_src/earray.py\",\n         \"_src/error_check.py\",\n         \"_src/ffi.py\",\n         \"_src/flatten_util.py\",\n-        \"_src/interpreters/__init__.py\",\n-        \"_src/interpreters/pxla.py\",\n+        \"_src/interpreters/__init__.py\",  # TODO(vanderplas): remove this and depend on :api instead\n+        \"_src/interpreters/pxla.py\",  # TODO(vanderplas): remove this and depend on :api instead\n         \"_src/pjit.py\",\n         \"_src/prng.py\",\n         \"_src/public_test_util.py\",\n@@ -375,6 +375,7 @@ py_library_providing_imports_info(\n         \":abstract_arrays\",\n         \":ad\",\n         \":ad_util\",\n+        # \":api\",   # TODO(vanderplas): add this dependency once downstream targets are fixed\n         \":api_util\",\n         \":attrs\",\n         \":basearray\",\n@@ -450,6 +451,53 @@ pytype_strict_library(\n     ],\n )\n \n+pytype_strict_library(\n+    name = \"api\",\n+    srcs = [\n+        \"_src/api.py\",\n+        \"_src/array.py\",\n+        \"_src/dispatch.py\",\n+        \"_src/interpreters/pxla.py\",\n+        \"_src/pjit.py\",\n+    ],\n+    visibility = [\":internal\"] + jax_visibility(\"api\"),\n+    deps = [\n+        \":abstract_arrays\",\n+        \":ad\",\n+        \":api_util\",\n+        \":attrs\",\n+        \":basearray\",\n+        \":batching\",\n+        \":compiler\",\n+        \":config\",\n+        \":core\",\n+        \":deprecations\",\n+        \":dtypes\",\n+        \":effects\",\n+        \":layout\",\n+        \":mesh\",\n+        \":mlir\",\n+        \":monitoring\",\n+        \":op_shardings\",\n+        \":partial_eval\",\n+        \":partition_spec\",\n+        \":profiler\",\n+        \":sharding\",\n+        \":sharding_impls\",\n+        \":sharding_specs\",\n+        \":source_info_util\",\n+        \":stages\",\n+        \":state_types\",\n+        \":traceback_util\",\n+        \":tree_util\",\n+        \":typing\",\n+        \":util\",\n+        \":xla\",\n+        \":xla_bridge\",\n+        \"//jax/_src/lib\",\n+    ] + py_deps(\"numpy\"),\n+)\n+\n pytype_strict_library(\n     name = \"api_util\",\n     srcs = [\"_src/api_util.py\"],\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex c21e8248d52d..229dee979d06 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -66,7 +66,6 @@\n   rebase_donate_argnums, _ensure_index, _ensure_index_tuple,\n   apply_flat_fun_nokwargs, check_callable, debug_info,\n   flat_out_axes)\n-from jax._src.lax import lax as lax_internal\n from jax._src.lib import jax_jit\n from jax._src.lib import xla_client as xc\n from jax._src.lib import pmap_lib\n@@ -477,6 +476,8 @@ def value_and_grad(fun: Callable, argnums: int | Sequence[int] = 0,\n     shapes and types as the corresponding arguments. If ``has_aux`` is True\n     then a tuple of ((value, auxiliary_data), gradient) is returned.\n   \"\"\"\n+  from jax._src.lax import lax as lax_internal  # pytype: disable=import-error\n+\n   if reduce_axes:\n     raise NotImplementedError(\"reduce_axes argument to grad is deprecated\")\n   del reduce_axes\n@@ -889,7 +890,7 @@ def hessian(fun: Callable, argnums: int | Sequence[int] = 0,\n                 argnums, has_aux=has_aux, holomorphic=holomorphic)\n \n def _std_basis(pytree):\n-  import jax.numpy as jnp\n+  import jax.numpy as jnp  # pytype: disable=import-error\n   leaves, _ = tree_flatten(pytree)\n   ndim = sum(map(np.size, leaves))\n   dtype = dtypes.result_type(*leaves)\n@@ -905,6 +906,7 @@ def _jacrev_unravel(output_pytree, input_pytree_leaf, arr):\n     output_pytree, 0, input_pytree_leaf, arr)\n \n def _possible_downcast(x, example):\n+  from jax._src.lax import lax as lax_internal  # pytype: disable=import-error\n   if (dtypes.issubdtype(x.dtype, np.complexfloating) and\n       not dtypes.issubdtype(_dtype(example), np.complexfloating)):\n     x = x.real\n@@ -1483,7 +1485,7 @@ def pmap(\n         \" from pmap.\")\n \n   if config.pmap_shmap_merge.value:\n-    from jax._src.shard_map import pmap\n+    from jax._src.shard_map import pmap  # pytype: disable=import-error\n     return pmap(fun, axis_name, in_axes=in_axes, out_axes=out_axes,\n                 static_broadcasted_argnums=static_broadcasted_argnums,\n                 devices=devices, backend=backend,\ndiff --git a/jax/_src/array.py b/jax/_src/array.py\nindex 9a71b12ed1a8..2514502c27d0 100644\n--- a/jax/_src/array.py\n+++ b/jax/_src/array.py\n@@ -343,8 +343,8 @@ def __format__(self, format_spec):\n       return format(self._value, format_spec)\n \n   def __getitem__(self, idx):\n-    from jax._src.lax import lax\n-    from jax._src.numpy import indexing\n+    from jax._src.lax import lax  # pytype: disable=import-error\n+    from jax._src.numpy import indexing  # pytype: disable=import-error\n     self._check_if_deleted()\n \n     if isinstance(self.sharding, PmapSharding):\n@@ -444,7 +444,7 @@ def __dlpack__(self, *, stream: int | Any | None = None,\n                  max_version: tuple[int, int] | None = None,\n                  dl_device: tuple[DLDeviceType, int] | None = None,\n                  copy: bool | None = None):\n-    from jax._src.dlpack import to_dlpack  # pylint: disable=g-import-not-at-top\n+    from jax._src.dlpack import to_dlpack  # pytype: disable=import-error  # pylint: disable=g-import-not-at-top\n \n     device_set = self.sharding.device_set\n     if len(device_set) > 1:\n@@ -464,7 +464,7 @@ def __dlpack_device__(self) -> tuple[enum.Enum, int]:\n     if len(self._arrays) != 1:\n       raise BufferError(\"__dlpack__ only supported for unsharded arrays.\")\n \n-    from jax._src.dlpack import DLDeviceType  # pylint: disable=g-import-not-at-top\n+    from jax._src.dlpack import DLDeviceType  # pytype: disable=import-error  # pylint: disable=g-import-not-at-top\n \n     if self.platform() == \"cpu\":\n       return DLDeviceType.kDLCPU, 0\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex 028c2cfa125e..b5e588cbc10e 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -26,7 +26,6 @@\n import time\n from typing import Any\n \n-import jax\n from jax._src import api\n from jax._src import array\n from jax._src import basearray\n@@ -34,8 +33,11 @@\n from jax._src import core\n from jax._src import dtypes\n from jax._src import lib\n+from jax._src import pjit\n from jax._src import traceback_util\n from jax._src import util\n+\n+from jax._src import xla_bridge\n from jax._src.abstract_arrays import array_types\n from jax._src.interpreters import ad\n from jax._src.interpreters import batching\n@@ -133,7 +135,7 @@ def get_token_input(\n       # TODO(yueshengys): This might still be buggy in a multi-process SPMD\n       # scenario. Revise the logic later. A distributed shutdown barrier inside\n       # the XLA program may be needed.\n-      return jax.device_put(\n+      return api.device_put(\n           tok, NamedSharding(Mesh(devices, 'x'), PartitionSpec('x')))\n \n     # We only use replicated sharding for the first time when the token for the\n@@ -244,8 +246,7 @@ def jaxpr_has_prim_requiring_devices(jaxpr: core.Jaxpr) -> bool:\n @util.weakref_lru_cache\n def get_intermediate_shardings(\n     jaxpr: core.Jaxpr) -> Sequence[tuple[Sharding, SourceInfo]]:\n-  from jax._src import pjit\n-  from jax._src import shard_map\n+  from jax._src import shard_map  # pytype: disable=import-error\n \n   out = []\n   for eqn in jaxpr.eqns:\n@@ -409,7 +410,7 @@ def result_handler(self, shard_arg_result):\n \n \n def _device_put_sharding_impl(x, aval, device, copy):\n-  from jax.experimental import multihost_utils\n+  from jax.experimental import multihost_utils  # pytype: disable=import-error\n \n   if isinstance(device, Sharding):\n     s = device\n@@ -440,7 +441,7 @@ def _device_put_sharding_impl(x, aval, device, copy):\n         # sharding do not transfer data) or (2) the sharding contains a\n         # different subset of devices on each host. For (1), the input should be\n         # the same on all hosts, but for (2) it need not be.\n-        if jax.process_count() == len(s._internal_device_list.process_indices):  # pytype: disable=attribute-error\n+        if xla_bridge.process_count() == len(s._internal_device_list.process_indices):  # pytype: disable=attribute-error\n           multihost_utils.assert_equal(\n               x, fail_message=(\n                   f\"{type(x)} passed to device_put is not the same on each\"\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex 74f2e028c555..0530e313f310 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -29,9 +29,8 @@\n \n import numpy as np\n \n-import jax\n-\n from jax._src import api\n+from jax._src import array\n from jax._src import compiler\n from jax._src import config\n from jax._src import core\n@@ -41,11 +40,13 @@\n from jax._src import linear_util as lu\n from jax._src import op_shardings\n from jax._src import sharding_specs\n+from jax._src import pjit\n from jax._src import profiler\n from jax._src import sharding_impls\n from jax._src import source_info_util\n from jax._src import stages\n from jax._src import tree_util\n+from jax._src import typing\n from jax._src import util\n from jax._src import xla_bridge as xb\n from jax._src.abstract_arrays import array_types\n@@ -154,7 +155,7 @@ def shard_args(shardings: Sequence[JSharding], layouts, copy_semantics,\n   # from each call in the same order as `args`. Since `batches` is grouped by\n   # types, we cannot simply flatten the results and we have to use the original\n   # indices to put each array back to its original position.\n-  results: list[jax.Array | None] = [None] * len(args)\n+  results: list[typing.Array | None] = [None] * len(args)\n   for t, (indices, a, s, l, cs) in batches.items():\n     outs = shard_arg_handlers[t](a, s, l, cs)\n     for i, out in safe_zip(indices, outs):\n@@ -230,11 +231,9 @@ def _shard_mutable_array(xs, shardings, layouts, copy_semantics):\n \n def batched_device_put(aval: core.ShapedArray,\n                        sharding: JSharding, xs: Sequence[Any],\n-                       devices: Sequence[jax.Device], committed: bool = True):\n+                       devices: Sequence[xc.Device], committed: bool = True):\n   util.test_event(\"batched_device_put_start\")\n   try:\n-    from jax._src import array\n-\n     bufs = [x for x, d in safe_zip(xs, devices)\n             if (isinstance(x, array.ArrayImpl) and\n                 dispatch.is_single_device_sharding(x.sharding) and\n@@ -385,7 +384,6 @@ def _emap_impl(fun: lu.WrappedFun, *args,\n                donated_invars: Sequence[bool],\n                is_explicit_global_axis_size: bool,\n                ):\n-  from jax._src import array\n   # TODO(sharadmv,mattjj): implement these cases\n   if any(d for d in donated_invars):\n     raise NotImplementedError(\"Buffer donation not supported in eager pmap.\")\n@@ -410,12 +408,12 @@ def _emap_impl(fun: lu.WrappedFun, *args,\n   donate_argnums = (1,) if platform in {\"cuda\", \"rocm\", \"tpu\"} else ()\n   new_outvals = []\n   for out_axis_src, out_axis, outval in zip(out_axes_src, out_axes, outvals):\n-    with jax.disable_jit(False):\n+    with api.disable_jit(False):\n       donate_argnums_ = donate_argnums\n       if isinstance(outval, array.ArrayImpl):\n         # We don't want to donate if it's already sharded.\n         donate_argnums_ = ()\n-      out = jax.pmap(\n+      out = api.pmap(\n           lambda _, x: x,\n           in_axes=(0, out_axis_src.get(axis_name)),\n           out_axes=out_axis,\n@@ -448,7 +446,7 @@ def _multi_pmap(f: Callable, info: EmapInfo, names: list[core.AxisName],\n   for i, name in reversed(list(enumerate(names))):\n     in_axes = tuple(arg_axis[i] for arg_axis in all_axes)\n     if any(in_axis is not None for in_axis in in_axes):\n-      f = jax.pmap(\n+      f = api.pmap(\n           f,\n           in_axes=in_axes,\n           axis_name=name,\n@@ -476,11 +474,12 @@ def to_map_tracer(self, val):\n       return MapTracer(self, val, {})\n \n   def process_primitive(self, primitive, tracers, params):\n-    if primitive is jax._src.lax.parallel.axis_index_p:\n-      return self.process_axis_index(**params)\n-    if primitive is jax._src.lax.parallel.psum_p:\n+    from jax._src.lax import parallel  # pytype: disable=import-error\n+    if primitive is parallel.axis_index_p:\n+      return self.process_axis_index(**params)  # pytype: disable=missing-parameter\n+    if primitive is parallel.psum_p:\n       f = HashableFunction(\n-          lambda *xs: jax._src.lax.parallel.psum(\n+          lambda *xs: parallel.psum(\n             xs, axis_name=params['axes'], axis_index_groups=params['axis_index_groups']),\n           (primitive, tuple(params.items())))\n     else:\n@@ -492,7 +491,7 @@ def process_primitive(self, primitive, tracers, params):\n     names = core.get_axis_env().axis_names()\n     all_axes = tuple(_map_schedule(map(s.get, names)) for s in shard_axes)  # pytype: disable=wrong-arg-types  # always-use-return-annotations\n     f_mapped, out_shard_axes = _multi_pmap(f, self.emap_info, names, all_axes)\n-    with core.eval_context(), jax.disable_jit(False):\n+    with core.eval_context(), api.disable_jit(False):\n       outvals = f_mapped(*vals)\n     if primitive.multiple_results:\n       return [MapTracer(self, val, out_shard_axes) for val in outvals]\n@@ -546,11 +545,12 @@ def process_custom_vjp_call(self, primitive, fun, fwd, bwd, tracers,\n       return fun.call_wrapped(*tracers)\n \n   def process_axis_index(self, axis_name):\n+    from jax._src.lax import lax, parallel  # pytype: disable=import-error\n     bind = HashableFunction(\n-        lambda _: jax.lax.axis_index(axis_name),\n-        (jax.lax.axis_index, axis_name))\n+        lambda _: parallel.axis_index(axis_name),\n+        (parallel.axis_index, axis_name))\n     fake_primitive = FakePrimitive(multiple_results=False, bind=bind)\n-    range = jax.lax.iota(np.int32, core.get_axis_env().axis_size(axis_name))\n+    range = lax.iota(np.int32, core.get_axis_env().axis_size(axis_name))\n     dummy_tracer = MapTracer(self, range, {axis_name: 0})\n     return self.process_primitive(fake_primitive, (dummy_tracer,), {})\n \n@@ -695,14 +695,15 @@ def find_replicas(\n \n @lu.transformation2\n def _change_argument_ranks(f, in_axes, out_axes_thunk, *args):\n+  from jax._src.lax import lax  # pytype: disable=import-error\n   args = tuple(\n-      arg if in_axis is None else jax.lax.squeeze(arg, dimensions=(in_axis,))\n+      arg if in_axis is None else lax.squeeze(arg, dimensions=(in_axis,))\n       for in_axis, arg in zip(in_axes, args)\n   )\n   results = f(*args)\n   out_axes = out_axes_thunk()\n   return tuple(\n-      x if axis is None else jax.lax.expand_dims(x, dimensions=(axis,))\n+      x if axis is None else lax.expand_dims(x, dimensions=(axis,))\n       for x, axis in zip(results, out_axes)\n   )\n \n@@ -1276,7 +1277,7 @@ def _handle_token_bufs(self, token_bufs, sharded_token):\n           assert isinstance(token.sharding, sharding_impls.SingleDeviceSharding)\n           token_devices.append(token.sharding._device_assignment[0])\n         s = NamedSharding(Mesh(token_devices, 'x'), P('x'))\n-        global_token_array = jax.make_array_from_single_device_arrays(\n+        global_token_array = array.make_array_from_single_device_arrays(\n             (0,), s, token_buf\n         )\n         dispatch.runtime_tokens.set_token_result(\n@@ -1754,7 +1755,7 @@ class MutationData(NamedTuple):\n def _discharge_refs(\n     jaxpr: core.ClosedJaxpr\n ) -> tuple[core.ClosedJaxpr, Sequence[int | None], MutationData]:\n-  from jax._src.state.discharge import discharge_state\n+  from jax._src.state.discharge import discharge_state  # pytype: disable=import-error\n   jaxpr, in_mut = _move_mutable_consts(jaxpr)\n   new_jaxpr = core.ClosedJaxpr(*discharge_state(jaxpr.jaxpr, jaxpr.consts))\n   count = it.count(len(jaxpr.out_avals))  # new outputs are appended to the end\n@@ -1782,7 +1783,7 @@ def _move_mutable_consts(\n \n @weakref_lru_cache\n def _discharge_internal_refs(jaxpr: core.ClosedJaxpr) -> core.ClosedJaxpr:\n-  from jax._src.state.discharge import discharge_state\n+  from jax._src.state.discharge import discharge_state  # pytype: disable=import-error\n   jaxpr_, consts = discharge_state(jaxpr.jaxpr, jaxpr.consts)\n   jaxpr_._debug_info = jaxpr.jaxpr._debug_info\n   return core.ClosedJaxpr(jaxpr_, consts)\n@@ -2016,8 +2017,6 @@ def _default_rule(prim, num_outvars, *_, **__):\n @weakref_lru_cache\n def get_out_layouts_via_propagation(closed_jaxpr: core.ClosedJaxpr\n                                     ) -> tuple[None | DeviceLocalLayout]:\n-  from jax._src import pjit\n-\n   env = {}  # type: ignore\n   jaxpr = closed_jaxpr.jaxpr\n \n@@ -3229,7 +3228,6 @@ def check_array_xla_sharding_layout_match(\n     in_xla_layouts: Sequence[DeviceLocalLayout],\n     jaxpr_debug_info: core.DebugInfo,\n     kept_var_idx: set[int]) -> None:\n-  from jax._src.array import ArrayImpl\n   # jaxpr_debug_info.arg_names are before DCE, so need to DCE them.\n   arg_names = (\n       [a for i, a in enumerate(jaxpr_debug_info.arg_names)\n@@ -3239,7 +3237,7 @@ def check_array_xla_sharding_layout_match(\n   num_errors = 5\n   for arg, xs, xl, name in safe_zip(\n       args_after_dce, in_xla_shardings, in_xla_layouts, arg_names):\n-    if not isinstance(arg, ArrayImpl):\n+    if not isinstance(arg, array.ArrayImpl):\n       continue\n     if isinstance(xs, (UnspecifiedValue, AUTO)):\n       continue\ndiff --git a/jax/_src/pallas/fuser/BUILD b/jax/_src/pallas/fuser/BUILD\nindex d5b5d128241d..951c08d8f4fa 100644\n--- a/jax/_src/pallas/fuser/BUILD\n+++ b/jax/_src/pallas/fuser/BUILD\n@@ -48,6 +48,7 @@ pytype_strict_library(\n         \":fuser_utils\",\n         \"//jax\",\n         \"//jax:ad_util\",\n+        \"//jax:api\",\n         \"//jax:api_util\",\n         \"//jax:core\",\n         \"//jax:custom_derivatives\",\ndiff --git a/jax/_src/pallas/mosaic_gpu/BUILD b/jax/_src/pallas/mosaic_gpu/BUILD\nindex 8a5d087125b7..78a1bd4f0011 100644\n--- a/jax/_src/pallas/mosaic_gpu/BUILD\n+++ b/jax/_src/pallas/mosaic_gpu/BUILD\n@@ -60,6 +60,7 @@ pytype_strict_library(\n     deps = [\n         \":core\",\n         \"//jax\",\n+        \"//jax:api\",\n         \"//jax:core\",\n         \"//jax:mesh\",\n         \"//jax:mlir\",\ndiff --git a/jax/_src/pallas/triton/BUILD b/jax/_src/pallas/triton/BUILD\nindex acbc11a60039..b13967d5b61c 100644\n--- a/jax/_src/pallas/triton/BUILD\n+++ b/jax/_src/pallas/triton/BUILD\n@@ -60,6 +60,7 @@ pytype_strict_library(\n     deps = [\n         \"//jax\",\n         \"//jax:ad_util\",\n+        \"//jax:api\",\n         \"//jax:api_util\",\n         \"//jax:config\",\n         \"//jax:core\",\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 4113f764e888..f2446e9a4939 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -70,7 +70,7 @@\n     prepare_axis_resources, parse_flatten_op_sharding, canonicalize_sharding,\n     flatten_spec, _internal_use_concrete_mesh)\n from jax._src.layout import Format, DeviceLocalLayout, AutoLayout\n-from jax._src.state import discharge as state_discharge, RefEffect, AbstractRef\n+from jax._src.state.types import RefEffect\n from jax._src.traceback_util import api_boundary\n from jax._src.tree_util import (\n     tree_flatten, tree_unflatten, treedef_is_leaf, tree_structure, tree_leaves,\n@@ -1413,7 +1413,7 @@ def _create_pjit_jaxpr(\n \n   if config.debug_key_reuse.value:\n     # Import here to avoid circular imports\n-    from jax.experimental.key_reuse._core import check_key_reuse_jaxpr\n+    from jax.experimental.key_reuse._core import check_key_reuse_jaxpr  # pytype: disable=import-error\n     check_key_reuse_jaxpr(jaxpr)\n \n   if any(isinstance(c, core.Tracer) or core.typeof(c).mutable for c in consts):\n@@ -2682,38 +2682,6 @@ def _pjit_pp_rule(eqn: core.JaxprEqn,\n core.pp_eqn_rules[pjit_p] = _pjit_pp_rule\n \n \n-def _pjit_state_discharge_rule(\n-    in_avals, out_avals, *args, jaxpr, in_shardings, out_shardings,\n-    in_layouts, out_layouts, **params):\n-  if not all(isinstance(s, UnspecifiedValue) for s in (*in_shardings, *out_shardings)):\n-    raise NotImplementedError\n-\n-  if not (all(l is None for l in in_layouts) and\n-          all(l is None for l in out_layouts)):\n-    raise NotImplementedError\n-\n-  jaxpr, consts = jaxpr.jaxpr, jaxpr.consts\n-  num_outs = len(jaxpr.outvars)\n-  discharged_jaxpr, discharged_consts = state_discharge.discharge_state(jaxpr, consts)\n-  discharged_closed_jaxpr = core.ClosedJaxpr(discharged_jaxpr, discharged_consts)\n-  new_in_shardings = (UnspecifiedValue(),) * len(discharged_jaxpr.invars)\n-  new_out_shardings = (UnspecifiedValue(),) * len(discharged_jaxpr.outvars)\n-  new_in_layouts = (None,) * len(discharged_jaxpr.invars)\n-  new_out_layouts = (None,) * len(discharged_jaxpr.outvars)\n-  out_and_ref_vals = pjit_p.bind(\n-      *args, jaxpr=discharged_closed_jaxpr, in_shardings=new_in_shardings,\n-      out_shardings=new_out_shardings, in_layouts=new_in_layouts,\n-      out_layouts=new_out_layouts, **params)\n-  out_vals, ref_vals = split_list(out_and_ref_vals, [num_outs])\n-  ref_vals_iter = iter(ref_vals)\n-  new_invals = tuple(next(ref_vals_iter) if isinstance(aval, AbstractRef)\n-                     else None for aval in in_avals)\n-  sentinel = object()\n-  assert next(ref_vals_iter, sentinel) is sentinel\n-  return new_invals, out_vals\n-state_discharge.register_discharge_rule(pjit_p)(_pjit_state_discharge_rule)\n-\n-\n # -------------------- with_sharding_constraint --------------------\n \n def check_shardings_are_auto(shardings_flat):\ndiff --git a/jax/_src/state/discharge.py b/jax/_src/state/discharge.py\nindex 100447f12d18..9dce3297b947 100644\n--- a/jax/_src/state/discharge.py\n+++ b/jax/_src/state/discharge.py\n@@ -25,6 +25,8 @@\n from jax._src import api_util\n from jax._src import core\n from jax._src import linear_util as lu\n+from jax._src import pjit\n+from jax._src import sharding_impls\n from jax._src import source_info_util\n from jax._src import tree_util\n from jax._src.interpreters import ad\n@@ -1145,3 +1147,35 @@ def wrapped(args):\n     _, out_flat = split_list(out_const_flat, [len(consts)])\n     return in_tree.unflatten(out_flat)\n   return wrapped\n+\n+\n+@register_discharge_rule(pjit.pjit_p)\n+def _pjit_state_discharge_rule(\n+    in_avals, out_avals, *args, jaxpr, in_shardings, out_shardings,\n+    in_layouts, out_layouts, **params):\n+  if not all(isinstance(s, sharding_impls.UnspecifiedValue) for s in (*in_shardings, *out_shardings)):\n+    raise NotImplementedError\n+\n+  if not (all(l is None for l in in_layouts) and\n+          all(l is None for l in out_layouts)):\n+    raise NotImplementedError\n+\n+  jaxpr, consts = jaxpr.jaxpr, jaxpr.consts\n+  num_outs = len(jaxpr.outvars)\n+  discharged_jaxpr, discharged_consts = discharge_state(jaxpr, consts)\n+  discharged_closed_jaxpr = core.ClosedJaxpr(discharged_jaxpr, discharged_consts)\n+  new_in_shardings = (sharding_impls.UNSPECIFIED,) * len(discharged_jaxpr.invars)\n+  new_out_shardings = (sharding_impls.UNSPECIFIED,) * len(discharged_jaxpr.outvars)\n+  new_in_layouts = (None,) * len(discharged_jaxpr.invars)\n+  new_out_layouts = (None,) * len(discharged_jaxpr.outvars)\n+  out_and_ref_vals = pjit.pjit_p.bind(\n+      *args, jaxpr=discharged_closed_jaxpr, in_shardings=new_in_shardings,\n+      out_shardings=new_out_shardings, in_layouts=new_in_layouts,\n+      out_layouts=new_out_layouts, **params)\n+  out_vals, ref_vals = split_list(out_and_ref_vals, [num_outs])\n+  ref_vals_iter = iter(ref_vals)\n+  new_invals = tuple(next(ref_vals_iter) if isinstance(aval, AbstractRef)\n+                     else None for aval in in_avals)\n+  sentinel = object()\n+  assert next(ref_vals_iter, sentinel) is sentinel\n+  return new_invals, out_vals\ndiff --git a/jax/extend/BUILD b/jax/extend/BUILD\nindex c2a5c48bd2b0..f466f1748654 100644\n--- a/jax/extend/BUILD\n+++ b/jax/extend/BUILD\n@@ -45,6 +45,7 @@ py_library_providing_imports_info(\n         \"//jax:abstract_arrays\",\n         \"//jax:ad\",\n         \"//jax:ad_util\",\n+        \"//jax:api\",\n         \"//jax:core\",\n         \"//jax:custom_derivatives\",\n     ],\n@@ -61,6 +62,7 @@ pytype_strict_library(\n     srcs = [\"backend.py\"],\n     deps = [\n         \"//jax\",\n+        \"//jax:api\",\n         \"//jax:xla_bridge\",\n     ],\n )\n\n```"
        ],
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29130 from mattjj:mutable-array-vmap2\n\nPiperOrigin-RevId: 765330882",
        "output": [
            "```diff\nCommit: ebf0588ebfc086fccfa4e4fcbea8ec899b619524\nDate: 2025-05-30T21:08:57Z\nURL: https://github.com/jax-ml/jax/commit/ebf0588ebfc086fccfa4e4fcbea8ec899b619524\nFiles changed: 2\nAdditions: +38, Deletions: -0\ndiff --git a/jax/_src/state/primitives.py b/jax/_src/state/primitives.py\nindex dbcc67df18cb..3a54644dbd37 100644\n--- a/jax/_src/state/primitives.py\n+++ b/jax/_src/state/primitives.py\n@@ -618,6 +618,13 @@ def _swap_vmap(batched_args, batched_dims, *, tree):\n   val_is_batched = val_dim is not batching.not_mapped\n   idx_is_batched = any(i_dim is not batching.not_mapped\n                        for i_dim in flat_idx_dims)\n+\n+  if not ref_is_batched:\n+    raise Exception(\"performing a set/swap operation with vmapped value on \"\n+                    \"an unbatched mutable array reference \"\n+                    f\"of type {core.typeof(ref)}. Move the mutable array to be \"\n+                    \"an argument to the vmapped function?\")\n+\n   if len(indexers) > 1:\n     raise NotImplementedError(\"Batching with multiple indexers not supported.\")\n   # TODO(sharadmv): handle vmap of multiple indexers\ndiff --git a/tests/mutable_array_test.py b/tests/mutable_array_test.py\nindex 0da335e2fac5..95ebdd818fa6 100644\n--- a/tests/mutable_array_test.py\n+++ b/tests/mutable_array_test.py\n@@ -253,6 +253,27 @@ def f(x):\n     ys = f(xs)\n     self.assertAllClose(ys, xs ** 2, check_dtypes=False)\n \n+  def test_vmap_extensive_inputs(self):\n+    def f(x_ref, val):\n+      x_ref[...] += val\n+      x_ref[...] += val\n+\n+    xs_ref = core.mutable_array(jnp.array([0, 0, 0]))\n+    vals = jnp.arange(3)\n+    jax.vmap(f)(xs_ref, vals)\n+    self.assertAllClose(xs_ref[...], 2 * vals, check_dtypes=False)\n+\n+  def test_vmap_closed_over_read_only(self):\n+    y_ref = core.mutable_array(1)\n+\n+    def f(x_ref):\n+      x_ref[...] += y_ref[...]\n+      x_ref[...] += y_ref[...]\n+\n+    xs_ref = core.mutable_array(jnp.array([0, 0, 0]))\n+    jax.vmap(f)(xs_ref)\n+    self.assertAllClose(xs_ref[...], jnp.array([2, 2, 2]), check_dtypes=False)\n+\n   def test_implicit_bitcast_regression(self):\n     # https://github.com/jax-ml/jax/issues/27683\n     v = core.mutable_array(jnp.array([0, 0, 0]))\n@@ -417,6 +438,16 @@ def false_fun():\n     out_false = f(False)\n     self.assertAllClose(x_ref[...], 2.)\n \n+  def test_vmap_closed_over_ref_write(self):\n+    x_ref = core.mutable_array(jnp.zeros((), 'int32'))\n+\n+    def f(val):\n+      x_ref[...] += val\n+\n+    vals = jnp.arange(3, dtype='int32')\n+    with self.assertRaisesRegex(Exception, \"unbatched mutable array\"):\n+      jax.vmap(f)(vals)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[mutable-arrays] add basic tests for vmap + mutable array",
        "output": [
            "```diff\nCommit: 581cb628a5f6516cfb63df201e18361dd0af6e96\nDate: 2025-05-30T20:23:34Z\nURL: https://github.com/jax-ml/jax/commit/581cb628a5f6516cfb63df201e18361dd0af6e96\nFiles changed: 2\nAdditions: +38, Deletions: -0\ndiff --git a/jax/_src/state/primitives.py b/jax/_src/state/primitives.py\nindex dbcc67df18cb..3a54644dbd37 100644\n--- a/jax/_src/state/primitives.py\n+++ b/jax/_src/state/primitives.py\n@@ -618,6 +618,13 @@ def _swap_vmap(batched_args, batched_dims, *, tree):\n   val_is_batched = val_dim is not batching.not_mapped\n   idx_is_batched = any(i_dim is not batching.not_mapped\n                        for i_dim in flat_idx_dims)\n+\n+  if not ref_is_batched:\n+    raise Exception(\"performing a set/swap operation with vmapped value on \"\n+                    \"an unbatched mutable array reference \"\n+                    f\"of type {core.typeof(ref)}. Move the mutable array to be \"\n+                    \"an argument to the vmapped function?\")\n+\n   if len(indexers) > 1:\n     raise NotImplementedError(\"Batching with multiple indexers not supported.\")\n   # TODO(sharadmv): handle vmap of multiple indexers\ndiff --git a/tests/mutable_array_test.py b/tests/mutable_array_test.py\nindex 0da335e2fac5..95ebdd818fa6 100644\n--- a/tests/mutable_array_test.py\n+++ b/tests/mutable_array_test.py\n@@ -253,6 +253,27 @@ def f(x):\n     ys = f(xs)\n     self.assertAllClose(ys, xs ** 2, check_dtypes=False)\n \n+  def test_vmap_extensive_inputs(self):\n+    def f(x_ref, val):\n+      x_ref[...] += val\n+      x_ref[...] += val\n+\n+    xs_ref = core.mutable_array(jnp.array([0, 0, 0]))\n+    vals = jnp.arange(3)\n+    jax.vmap(f)(xs_ref, vals)\n+    self.assertAllClose(xs_ref[...], 2 * vals, check_dtypes=False)\n+\n+  def test_vmap_closed_over_read_only(self):\n+    y_ref = core.mutable_array(1)\n+\n+    def f(x_ref):\n+      x_ref[...] += y_ref[...]\n+      x_ref[...] += y_ref[...]\n+\n+    xs_ref = core.mutable_array(jnp.array([0, 0, 0]))\n+    jax.vmap(f)(xs_ref)\n+    self.assertAllClose(xs_ref[...], jnp.array([2, 2, 2]), check_dtypes=False)\n+\n   def test_implicit_bitcast_regression(self):\n     # https://github.com/jax-ml/jax/issues/27683\n     v = core.mutable_array(jnp.array([0, 0, 0]))\n@@ -417,6 +438,16 @@ def false_fun():\n     out_false = f(False)\n     self.assertAllClose(x_ref[...], 2.)\n \n+  def test_vmap_closed_over_ref_write(self):\n+    x_ref = core.mutable_array(jnp.zeros((), 'int32'))\n+\n+    def f(val):\n+      x_ref[...] += val\n+\n+    vals = jnp.arange(3, dtype='int32')\n+    with self.assertRaisesRegex(Exception, \"unbatched mutable array\"):\n+      jax.vmap(f)(vals)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "[pallas] Expose TPUInterpretParams in jax.experimental.pallas.tpu\n\nPiperOrigin-RevId: 765266754",
        "output": [
            "```diff\nCommit: c2e7d61323b17481d213190bb779a4b74e7d5356\nDate: 2025-05-30T18:23:46Z\nURL: https://github.com/jax-ml/jax/commit/c2e7d61323b17481d213190bb779a4b74e7d5356\nFiles changed: 3\nAdditions: +23, Deletions: -22\ndiff --git a/jax/experimental/pallas/tpu.py b/jax/experimental/pallas/tpu.py\nindex e27fdaaadd8f..c4d21023a6e6 100644\n--- a/jax/experimental/pallas/tpu.py\n+++ b/jax/experimental/pallas/tpu.py\n@@ -28,6 +28,7 @@\n from jax._src.pallas.mosaic.helpers import sync_copy as sync_copy\n from jax._src.pallas.mosaic.helpers import core_barrier as core_barrier\n from jax._src.pallas.mosaic.helpers import run_on_first_core as run_on_first_core\n+from jax._src.pallas.mosaic.interpret import TPUInterpretParams as TPUInterpretParams\n from jax._src.pallas.mosaic.lowering import LoweringException as LoweringException\n from jax._src.pallas.mosaic.pipeline import BufferedRef as BufferedRef\n from jax._src.pallas.mosaic.pipeline import BufferedRefBase as BufferedRefBase\ndiff --git a/tests/pallas/tpu_pallas_interpret_distributed_test.py b/tests/pallas/tpu_pallas_interpret_distributed_test.py\nindex 4e4776736cf1..c5f1b29fd6bc 100644\n--- a/tests/pallas/tpu_pallas_interpret_distributed_test.py\n+++ b/tests/pallas/tpu_pallas_interpret_distributed_test.py\n@@ -107,7 +107,7 @@ def right_permute_kernel(input_ref, output_ref, send_sem, recv_sem):\n         out_shape=out_shape,\n         grid_spec=grid_spec,\n         compiler_params=pltpu.TPUCompilerParams(collective_id=13),\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n     )\n     # Wrap the kernel within a shard_map to call.\n@@ -228,7 +228,7 @@ def _():\n       all_gather_kernel,\n       out_shape=out_shape,\n       grid_spec=grid_spec,\n-      interpret=mosaic_interpret.TPUInterpretParams(\n+      interpret=pltpu.TPUInterpretParams(\n           dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n       compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n     )\n@@ -388,7 +388,7 @@ def _():\n       all_reduce_kernel,\n       out_shape=out_shape,\n       grid_spec=grid_spec,\n-      interpret=mosaic_interpret.TPUInterpretParams(\n+      interpret=pltpu.TPUInterpretParams(\n           dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n       compiler_params=pltpu.TPUCompilerParams(collective_id=0),\n     )\n@@ -672,7 +672,7 @@ def pallas_reduce_scatter(input_arr):\n         reduce_scatter_kernel,\n         out_shape=out_shape,\n         grid_spec=grid_spec,\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             dma_execution_mode=dma_execution_mode, detect_races=True),\n         compiler_params=pltpu.TPUCompilerParams(collective_id=7),\n       )(input_arr)[0]\n@@ -976,7 +976,7 @@ def pallas_reduce_scatter(input_arr):\n         reduce_scatter_kernel,\n         out_shape=out_shape,\n         grid_spec=grid_spec,\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             dma_execution_mode=dma_execution_mode, detect_races=detect_races),\n         compiler_params=pltpu.TPUCompilerParams(collective_id=19),\n       )(input_arr)[0]\n@@ -1064,7 +1064,7 @@ def run(src_dst_ids):\n               ],\n               out_specs=pl.BlockSpec(memory_space=pltpu.ANY),\n               scratch_shapes=[pltpu.SemaphoreType.DMA, pltpu.SemaphoreType.DMA],\n-              interpret=mosaic_interpret.TPUInterpretParams(\n+              interpret=pltpu.TPUInterpretParams(\n                   dma_execution_mode='eager',\n                   detect_races=True,\n               ),\ndiff --git a/tests/pallas/tpu_pallas_interpret_test.py b/tests/pallas/tpu_pallas_interpret_test.py\nindex 47d4ba3e1acf..871f66d71c53 100644\n--- a/tests/pallas/tpu_pallas_interpret_test.py\n+++ b/tests/pallas/tpu_pallas_interpret_test.py\n@@ -124,7 +124,7 @@ def matmul(x: jax.Array, y: jax.Array):\n               (x.shape[0] // 2, y.shape[1] // 2),\n               lambda i, j: (i, j),\n           ),\n-          interpret=mosaic_interpret.TPUInterpretParams(),\n+          interpret=pltpu.TPUInterpretParams(),\n       )(x, y)\n \n     k1, k2 = jax.random.split(jax.random.key(0))\n@@ -155,7 +155,7 @@ def block_dynamic_slice(x, starts, sizes):\n           dynamic_slice_kernel,\n           grid_spec=grid_spec,\n           out_shape=jax.ShapeDtypeStruct(shape=sizes, dtype=x.dtype),\n-          interpret=mosaic_interpret.TPUInterpretParams(),\n+          interpret=pltpu.TPUInterpretParams(),\n       )\n       block_idx = jnp.array([starts[0] // sizes[0], starts[1] // sizes[1]])\n       return kernel(block_idx, x)\n@@ -189,7 +189,7 @@ def f(s, x):\n           ],\n           out_specs=pl.BlockSpec(x.shape, lambda i: (0, 0)),\n           input_output_aliases={1: 0},\n-          interpret=mosaic_interpret.TPUInterpretParams(),\n+          interpret=pltpu.TPUInterpretParams(),\n       )(s, x)\n \n     s = jnp.array([1], dtype=jnp.int32)\n@@ -224,7 +224,7 @@ def _():\n         ),\n         scratch_shapes=(pltpu.SMEM((1,), jnp.int32),),\n         input_output_aliases={0: 0},\n-        interpret=mosaic_interpret.TPUInterpretParams(),\n+        interpret=pltpu.TPUInterpretParams(),\n     )(x)\n \n     expected = np.zeros((4, 4))\n@@ -264,7 +264,7 @@ def kernel_with_race(x_ref, o_ref, t_ref, sem):\n             pltpu.VMEM(x.shape, x.dtype),\n             pltpu.SemaphoreType.DMA,\n         ],\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             detect_races=True, dma_execution_mode=dma_execution_mode\n         ),\n     )(x).block_until_ready()\n@@ -279,7 +279,7 @@ def kernel_with_race(x_ref, o_ref, t_ref, sem):\n             pltpu.VMEM(x.shape, x.dtype),\n             pltpu.SemaphoreType.DMA,\n         ],\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             detect_races=True, dma_execution_mode=dma_execution_mode\n         ),\n     )(x).block_until_ready()\n@@ -293,7 +293,7 @@ def matmul(x: jax.Array, y: jax.Array):\n       return pl.pallas_call(\n           matmul_kernel,\n           out_shape=jax.ShapeDtypeStruct((x.shape[0], y.shape[1]), x.dtype),\n-          interpret=mosaic_interpret.TPUInterpretParams(\n+          interpret=pltpu.TPUInterpretParams(\n               skip_floating_point_ops=True\n           ),\n       )(x, y)\n@@ -325,7 +325,7 @@ def kernel(o1_ref, o2_ref, o3_ref, t1_ref, t2_ref):\n             pltpu.VMEM((8, 128), jnp.bfloat16),\n             pltpu.VMEM((8, 128), jnp.int16),\n         ],\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             uninitialized_memory=uninitialized_memory\n         ),\n     )()\n@@ -355,7 +355,7 @@ def kernel_call(x, s):\n               pl.BlockSpec(memory_space=pltpu.SMEM),\n           ],\n           out_specs=pl.BlockSpec((8, 256), lambda i, j: (i, 0)),\n-          interpret=mosaic_interpret.TPUInterpretParams(),\n+          interpret=pltpu.TPUInterpretParams(),\n       )(x, s)\n \n     with CountStoreCallbacksContext() as store_callbacks_counter:\n@@ -378,7 +378,7 @@ def kernel_call_dimensions_parallel_arbitrary(s, grid_point_recorder):\n           grid=(4, 4),\n           in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n           out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n-          interpret=mosaic_interpret.TPUInterpretParams(\n+          interpret=pltpu.TPUInterpretParams(\n               random_seed=12345, grid_point_recorder=grid_point_recorder\n           ),\n           compiler_params=pltpu.TPUCompilerParams(\n@@ -436,7 +436,7 @@ def kernel(s_ref, o_ref):\n           grid=(4, 4),\n           in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n           out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n-          interpret=mosaic_interpret.TPUInterpretParams(random_seed=12345),\n+          interpret=pltpu.TPUInterpretParams(random_seed=12345),\n           compiler_params=pltpu.TPUCompilerParams(\n               dimension_semantics=('arbitrary', 'parallel')\n           ),\n@@ -462,7 +462,7 @@ def kernel_call_dynamic_parallel_dimension():\n           grid=(dim_size,),\n           in_specs=[],\n           out_specs=pl.BlockSpec((1,), lambda _: (0,)),\n-          interpret=mosaic_interpret.TPUInterpretParams(),\n+          interpret=pltpu.TPUInterpretParams(),\n           compiler_params=pltpu.TPUCompilerParams(\n               dimension_semantics=('parallel',)\n           ),\n@@ -479,7 +479,7 @@ def f(x):\n       y = jnp.zeros_like(x)\n       def inner(refs):\n         x_ref, y_ref = refs\n-        @pl.core_map(mesh, interpret=mosaic_interpret.TPUInterpretParams())\n+        @pl.core_map(mesh, interpret=pltpu.TPUInterpretParams())\n         def _():\n           num_cores = jax.lax.psum(1, \"x\")\n           slc_size = 16 // num_cores\n@@ -520,7 +520,7 @@ def kernel(x_ref, o_ref, vmem_ref):\n         scratch_shapes=[\n             pltpu.VMEM(x.shape, x.dtype),\n         ],\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             num_cores_per_device=2,\n             detect_races=True,\n         ),\n@@ -554,7 +554,7 @@ def kernel(x_ref, o_ref, vmem_ref):\n         scratch_shapes=[\n             pltpu.VMEM((8, 128), x.dtype),\n         ],\n-        interpret=mosaic_interpret.TPUInterpretParams(\n+        interpret=pltpu.TPUInterpretParams(\n             num_cores_per_device=2,\n             detect_races=True,\n         ),\n@@ -578,7 +578,7 @@ def kernel_call(s, num_cores_per_device, grid_point_recorder):\n           grid=(4, 4),\n           in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n           out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n-          interpret=mosaic_interpret.TPUInterpretParams(\n+          interpret=pltpu.TPUInterpretParams(\n               random_seed=12345,\n               num_cores_per_device=num_cores_per_device,\n               grid_point_recorder=grid_point_recorder,\n\n```"
        ],
        "from_id": [
            "jburnim",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Support interleaved packing on TPUv4-.\n\nThis enables row broadcast for int8 and int4 on TPUv4.\n\nPiperOrigin-RevId: 765252479",
        "output": [
            "```diff\nCommit: d15253e7f5e71b18dad93c2a0e3c10234be37550\nDate: 2025-05-30T17:48:24Z\nURL: https://github.com/jax-ml/jax/commit/d15253e7f5e71b18dad93c2a0e3c10234be37550\nFiles changed: 4\nAdditions: +18, Deletions: -19\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 53d8712d5274..1669d1bf1586 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -3655,8 +3655,7 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n               if (packing != 1) {\n                 if (auto new_dst_vreg = broadcastSubelements(\n                         builder, cast<TypedValue<VectorType>>(dst_vreg),\n-                        subelement_offset, ctx.target_shape,\n-                        ctx.hardware_generation);\n+                        subelement_offset, ctx.target_shape);\n                     succeeded(new_dst_vreg)) {\n                   dst_vreg = *new_dst_vreg;\n                 } else {\ndiff --git a/jaxlib/mosaic/dialect/tpu/vreg_util.cc b/jaxlib/mosaic/dialect/tpu/vreg_util.cc\nindex 237bbe5cc722..90efacf0c676 100644\n--- a/jaxlib/mosaic/dialect/tpu/vreg_util.cc\n+++ b/jaxlib/mosaic/dialect/tpu/vreg_util.cc\n@@ -224,8 +224,7 @@ LogicalResult maskNativeTilingVregs(ImplicitLocOpBuilder &builder,\n \n FailureOr<TypedValue<VectorType>> broadcastSubelements(\n     ImplicitLocOpBuilder &builder, TypedValue<VectorType> vec,\n-    int subelement_idx, std::array<int64_t, 2> target_shape,\n-    int hardware_generation) {\n+    int subelement_idx, std::array<int64_t, 2> target_shape) {\n   int bitwidth = vec.getType().getElementTypeBitWidth();\n   int packing = 32 / bitwidth;\n   if (subelement_idx < 0 || subelement_idx >= packing) {\n@@ -247,17 +246,9 @@ FailureOr<TypedValue<VectorType>> broadcastSubelements(\n       src_vreg_int,\n       getFullVector(builder, vreg_native_int_ty,\n                     builder.getI32IntegerAttr(subelement_idx * bitwidth)));\n-  Value vreg_result_int;\n-  if (hardware_generation >= 5) {\n-    SmallVector<Value> packed_vregs(packing, vreg_subelement_low);\n-    vreg_result_int = builder.create<tpu::PackSubelementsOp>(\n-        vreg_packed_int_ty, packed_vregs, tpu::PackFormat::kInterleaved);\n-  } else {\n-    // This can be virtualized as a tree of shifts and ORs.\n-    return builder.emitError()\n-           << \"broadcastSubelements not implemented for hardware generation \"\n-           << hardware_generation;\n-  }\n+  SmallVector<Value> packed_vregs(packing, vreg_subelement_low);\n+  Value vreg_result_int = builder.create<tpu::PackSubelementsOp>(\n+      vreg_packed_int_ty, packed_vregs, tpu::PackFormat::kInterleaved);\n   return cast<TypedValue<VectorType>>(\n       builder.create<tpu::BitcastVregOp>(vec.getType(), vreg_result_int)\n           .getResult());\ndiff --git a/jaxlib/mosaic/dialect/tpu/vreg_util.h b/jaxlib/mosaic/dialect/tpu/vreg_util.h\nindex 8833390ef87b..90e802fcb8fc 100644\n--- a/jaxlib/mosaic/dialect/tpu/vreg_util.h\n+++ b/jaxlib/mosaic/dialect/tpu/vreg_util.h\n@@ -90,8 +90,7 @@ LogicalResult maskNativeTilingVregs(ImplicitLocOpBuilder &builder,\n // subelement_idx must be between 0 and packing.\n FailureOr<TypedValue<VectorType>> broadcastSubelements(\n     ImplicitLocOpBuilder &builder, TypedValue<VectorType> vec,\n-    int subelement_idx, std::array<int64_t, 2> target_shape,\n-    int hardware_generation);\n+    int subelement_idx, std::array<int64_t, 2> target_shape);\n \n }  // namespace mlir::tpu\n \ndiff --git a/tests/pallas/tpu_ops_test.py b/tests/pallas/tpu_ops_test.py\nindex de87126ebd3f..3f6dc593e333 100644\n--- a/tests/pallas/tpu_ops_test.py\n+++ b/tests/pallas/tpu_ops_test.py\n@@ -197,8 +197,18 @@ def kernel(x_ref, y_ref, out_ref):\n   def test_row_broadcast(self, dtype):\n     if not jtu.if_cloud_tpu_at_least(2025, 1, 10):\n       self.skipTest(\"Requires libtpu built after 2025-01-10\")\n-    if not self.INTERPRET and jtu.get_tpu_version() < 5:\n-      self.skipTest(\"Requires TPUv5+\")\n+    bitwidth = pallas_utils.dtype_bitwidth(dtype)\n+    if not self.INTERPRET and jtu.get_tpu_version() < 4 and bitwidth < 8:\n+      self.skipTest(\"Requires TPUv4+ for sub-byte types\")\n+    if (\n+        not self.INTERPRET\n+        and jtu.get_tpu_version() == 4\n+        and bitwidth < 16\n+        and not jtu.if_cloud_tpu_at_least(2025, 6, 2)\n+    ):\n+      self.skipTest(\n+          \"Requires libtpu built after 2025-06-02 for bitwidth < 16 on TPUv4\"\n+      )\n     def kernel(x_ref, y_ref):\n       y_ref[...] = jnp.broadcast_to(x_ref[pl.ds(3, 1)], y_ref.shape).astype(y_ref.dtype)\n     m, n = 4, 1152\n\n```"
        ],
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "replace mentions of `Compiled.input_layouts` with `Compiled.input_formats`\n\nThis is part of a broader renaming of \"layout\" to \"format\".\n\nPiperOrigin-RevId: 765205967",
        "output": [
            "```diff\nCommit: 213985aa8d20d0b01113e1f5a337a3649ece0a7c\nDate: 2025-05-30T15:42:37Z\nURL: https://github.com/jax-ml/jax/commit/213985aa8d20d0b01113e1f5a337a3649ece0a7c\nFiles changed: 3\nAdditions: +64, Deletions: -64\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex 2072aaf44b5a..74f2e028c555 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -2976,7 +2976,7 @@ def from_hlo(name: str,\n             xla_executable.local_devices(), len(in_shardings), len(out_shardings))\n \n     # xla_in_layouts are all either None or DeviceLocalLayout. Even default\n-    # layout are concrete layouts and they are used in `compiled.input_layouts`\n+    # layout are concrete layouts and they are used in `compiled.input_formats`\n     # to return concrete layouts to users.\n     # `dispatch_in_layouts` replaces default layouts with `None` to simplify\n     # dispatch logic downstream.\ndiff --git a/jax/experimental/array_serialization/serialization_test.py b/jax/experimental/array_serialization/serialization_test.py\nindex 0611388a1d80..0d7e0a48b6c1 100644\n--- a/jax/experimental/array_serialization/serialization_test.py\n+++ b/jax/experimental/array_serialization/serialization_test.py\n@@ -593,10 +593,10 @@ def test_load_with_layout(self):\n     s = NamedSharding(mesh, P('x', 'y'))\n     arr = jax.device_put(np_inp, s)\n \n-    out_layout = jax.jit(lambda x: x.T, out_shardings=Format(DLL.AUTO)).lower(\n-        arr).compile().output_layouts\n+    out_format = jax.jit(lambda x: x.T, out_shardings=Format(DLL.AUTO)).lower(\n+        arr).compile().output_formats\n     self.assertEqual(arr.format.device_local_layout.major_to_minor,\n-                     out_layout.device_local_layout.major_to_minor[::-1])\n+                     out_format.device_local_layout.major_to_minor[::-1])\n \n     ckpt_dir = pathlib.Path(self.create_tempdir('ckpt').full_path)\n     ckpt_path = pathlib.Path(self.create_tempdir(f'{ckpt_dir}/first').full_path)\n@@ -609,9 +609,9 @@ def test_load_with_layout(self):\n             self._on_commit_callback, ckpt_dir, ckpt_dir))\n     manager.wait_until_finished()\n \n-    out, = serialization.run_deserialization([out_layout], tspecs)\n+    out, = serialization.run_deserialization([out_format], tspecs)\n \n-    self.assertEqual(out.format, out_layout)\n+    self.assertEqual(out.format, out_format)\n     self.assertIsInstance(out, array.ArrayImpl)\n     self.assertArraysEqual(out, np_inp)\n     for s in out.addressable_shards:\ndiff --git a/tests/layout_test.py b/tests/layout_test.py\nindex d7b23c75b313..ce0ca17b05de 100644\n--- a/tests/layout_test.py\n+++ b/tests/layout_test.py\n@@ -55,18 +55,18 @@ def init(x, y):\n                             out_shardings=Format(DLL.AUTO)).lower(sds1, sds2)\n     compiled_apply = lowered_apply.compile()\n \n-    arg_layouts, kw_layouts = compiled_apply.input_layouts\n+    arg_formats, kw_layouts = compiled_apply.input_formats\n     self.assertEmpty(kw_layouts)\n \n-    for i, o in zip(arg_layouts, compiled_apply.output_layouts):\n+    for i, o in zip(arg_formats, compiled_apply.output_formats):\n       self.assertEqual(i.device_local_layout.major_to_minor,\n                        o.device_local_layout.major_to_minor[::-1])\n \n     init_compiled = jax.jit(\n-        init, out_shardings=arg_layouts).lower(sds1, sds2).compile()\n+        init, out_shardings=arg_formats).lower(sds1, sds2).compile()\n \n-    for i, o in zip(init_compiled.input_layouts[0],\n-                    init_compiled.output_layouts):\n+    for i, o in zip(init_compiled.input_formats[0],\n+                    init_compiled.output_formats):\n       self.assertEqual(i, o)\n \n     arr1 = jax.device_put(np_inp1, s1)\n@@ -77,16 +77,16 @@ def init(x, y):\n       init_compiled(arr1, arr2)\n     self.assertEqual(init_count(), 1)\n \n-    self.assertEqual(init_out[0].format, init_compiled.output_layouts[0])\n-    self.assertEqual(init_out[1].format, init_compiled.output_layouts[1])\n+    self.assertEqual(init_out[0].format, init_compiled.output_formats[0])\n+    self.assertEqual(init_out[1].format, init_compiled.output_formats[1])\n \n     with jtu.count_aot_jit_cpp_cache_miss() as apply_count:\n       apply_out = compiled_apply(*init_out)\n       compiled_apply(*init_out)\n     self.assertEqual(apply_count(), 1)\n \n-    self.assertEqual(apply_out[0].format, compiled_apply.output_layouts[0])\n-    self.assertEqual(apply_out[1].format, compiled_apply.output_layouts[1])\n+    self.assertEqual(apply_out[0].format, compiled_apply.output_formats[0])\n+    self.assertEqual(apply_out[1].format, compiled_apply.output_formats[1])\n \n     self.assertTupleEqual(apply_out[0].format.device_local_layout.major_to_minor,\n                           init_out[0].format.device_local_layout.major_to_minor[::-1])\n@@ -114,10 +114,10 @@ def f(x):\n     out = compiled(arr)\n \n     self.assertTupleEqual(\n-        compiled.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n+        compiled.input_formats[0][0].device_local_layout.major_to_minor[::-1],\n         (2, 1, 0))\n     self.assertTupleEqual(\n-        compiled.output_layouts.device_local_layout.major_to_minor[::-1],\n+        compiled.output_formats.device_local_layout.major_to_minor[::-1],\n         (2, 1, 0))\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n@@ -125,10 +125,10 @@ def f(x):\n     compiled_auto = jax.jit(f, in_shardings=Format(DLL.AUTO),\n                             out_shardings=Format(DLL.AUTO)).lower(sds).compile()\n     self.assertTupleEqual(\n-        compiled_auto.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n+        compiled_auto.input_formats[0][0].device_local_layout.major_to_minor[::-1],\n         (2, 1, 0))\n     self.assertTupleEqual(\n-        compiled_auto.output_layouts.device_local_layout.major_to_minor[::-1],\n+        compiled_auto.output_formats.device_local_layout.major_to_minor[::-1],\n         (0, 1, 2))\n \n     with self.assertRaisesRegex(\n@@ -149,15 +149,15 @@ def f(x):\n     compiled = jax.jit(f, in_shardings=Format(),\n                        out_shardings=Format(DLL.AUTO)).lower(arr).compile()\n     self.assertTupleEqual(\n-        compiled.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n+        compiled.input_formats[0][0].device_local_layout.major_to_minor[::-1],\n         (1, 0))\n     self.assertTupleEqual(\n-        compiled.output_layouts.device_local_layout.major_to_minor[::-1],\n+        compiled.output_formats.device_local_layout.major_to_minor[::-1],\n         (0, 1))\n \n     out = compiled(arr)\n     self.assertArraysEqual(out, np_inp.T)\n-    self.assertEqual(out.format, compiled.output_layouts)\n+    self.assertEqual(out.format, compiled.output_formats)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('y', 'x')))\n \n   def test_sharding_and_layouts(self):\n@@ -170,11 +170,11 @@ def test_sharding_and_layouts(self):\n                        out_shardings=Format(DLL.AUTO, s)).lower(np_inp).compile()\n     out = compiled(np_inp)\n     self.assertTupleEqual(\n-        compiled.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n+        compiled.input_formats[0][0].device_local_layout.major_to_minor[::-1],\n         (1, 0))\n     if not jtu.test_device_matches(['cpu']):\n       self.assertTupleEqual(\n-          compiled.output_layouts.device_local_layout.major_to_minor[::-1],\n+          compiled.output_formats.device_local_layout.major_to_minor[::-1],\n           (0, 1))\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, s)\n@@ -187,19 +187,19 @@ def f(x, y, z, a, b, c):\n     inps = [np.arange(math.prod(shape)).reshape(shape)] * 6\n     compiled = jax.jit(f, in_shardings=Format(DLL.AUTO),\n                        out_shardings=Format(DLL.AUTO)).lower(*inps).compile()\n-    arg_layouts, _ = compiled.input_layouts\n+    arg_formats, _ = compiled.input_formats\n     out1, out2 = compiled(*inps)\n \n-    compiled2 = jax.jit(f, in_shardings=arg_layouts).lower(*inps).compile()\n+    compiled2 = jax.jit(f, in_shardings=arg_formats).lower(*inps).compile()\n     out3, out4 = compiled2(*inps)\n \n-    for l1, l2 in safe_zip(arg_layouts, compiled2.input_layouts[0]):\n+    for l1, l2 in safe_zip(arg_formats, compiled2.input_formats[0]):\n       self.assertEqual(l1, l2)\n \n     self.assertArraysEqual(out1, out3)\n     self.assertArraysEqual(out2, out4)\n \n-    arrs = [jax.device_put(i, l) for i, l in zip(inps, arg_layouts)]\n+    arrs = [jax.device_put(i, l) for i, l in zip(inps, arg_formats)]\n     out5, out6 = jax.jit(f)(*arrs)\n     self.assertArraysEqual(out1, out5)\n     self.assertArraysEqual(out2, out6)\n@@ -219,8 +219,8 @@ def f(x, y):\n     jf = jax.jit(f, in_shardings=Format(DLL.AUTO, s),\n                  out_shardings=Format(DLL.AUTO, s))\n     compiled = jf.lower(np_inp, np_inp).compile()\n-    arg_layouts, _ = compiled.input_layouts\n-    arrs = [jax.device_put(i, l) for i, l in zip(arrs, arg_layouts)]\n+    arg_formats, _ = compiled.input_formats\n+    arrs = [jax.device_put(i, l) for i, l in zip(arrs, arg_formats)]\n     compiled(*arrs)\n \n   def test_aot_layout_mismatch(self):\n@@ -274,7 +274,7 @@ def test_device_put_concrete_layout(self):\n \n     compiled = jax.jit(\n         lambda x: x * 2, out_shardings=Format(DLL.AUTO)).lower(arr).compile()\n-    col = compiled.output_layouts\n+    col = compiled.output_formats\n \n     out = jax.device_put(np_inp, col)\n     self.assertEqual(out.format, col)\n@@ -306,7 +306,7 @@ def invalid_layout_spec(self):\n     compiled = jax.jit(lambda x: x).lower(x).compile()\n     with self.assertRaisesRegex(\n         ValueError, 'Sharding has to be concrete when layout.*'):\n-      Format(compiled.output_layouts[0], None)\n+      Format(compiled.output_formats[0], None)\n \n   def test_layout_on_sds(self):\n     mesh = jtu.create_mesh((2, 1), ('x', 'y'))\n@@ -314,12 +314,12 @@ def test_layout_on_sds(self):\n     np_inp = np.arange(16).reshape(8, 2)\n     arr = jax.device_put(np_inp, s)\n \n-    out_layout = jax.jit(jnp.sin, out_shardings=Format(DLL.AUTO)).lower(\n-        arr).compile().output_layouts\n+    out_format = jax.jit(jnp.sin, out_shardings=Format(DLL.AUTO)).lower(\n+        arr).compile().output_formats\n \n-    sds = jax.ShapeDtypeStruct(arr.shape, arr.dtype, sharding=out_layout)\n-    arg_layout, _ = jax.jit(lambda x: x * 2).lower(sds).compile().input_layouts\n-    self.assertEqual(arg_layout[0], out_layout)\n+    sds = jax.ShapeDtypeStruct(arr.shape, arr.dtype, sharding=out_format)\n+    arg_format, _ = jax.jit(lambda x: x * 2).lower(sds).compile().input_formats\n+    self.assertEqual(arg_format[0], out_format)\n \n     with self.assertRaisesRegex(\n         TypeError,\n@@ -333,12 +333,12 @@ def test_make_array_from_callback(self):\n     np_inp = np.arange(16).reshape(8, 2)\n     sds = jax.ShapeDtypeStruct(np_inp.shape, np_inp.dtype, sharding=s)\n \n-    layout = jax.jit(lambda x: x * 2).lower(sds).compile().output_layouts\n+    format = jax.jit(lambda x: x * 2).lower(sds).compile().output_formats\n \n-    out = jax.make_array_from_callback(np_inp.shape, layout,\n+    out = jax.make_array_from_callback(np_inp.shape, format,\n                                        lambda idx: np_inp[idx])\n     self.assertArraysEqual(out, np_inp)\n-    self.assertEqual(out.format, layout)\n+    self.assertEqual(out.format, format)\n \n     with self.assertRaisesRegex(\n         TypeError,\n@@ -417,18 +417,18 @@ def test_device_put_user_concrete_layout_multi_device(self):\n     jnp_inp = jnp.arange(math.prod(shape)).reshape(shape)\n     arr = jax.device_put(np_inp, s)\n \n-    custom_layout = Format(DLL(major_to_minor=(0, 1)), s)\n-    out1 = jax.device_put(arr, custom_layout)\n+    custom_format = Format(DLL(major_to_minor=(0, 1)), s)\n+    out1 = jax.device_put(arr, custom_format)\n \n     with jax.sharding.use_mesh(mesh):\n-      out2 = jax.device_put(arr, custom_layout)\n-      out3 = jax.device_put(jnp_inp, custom_layout)\n-      out4 = jax.device_put(np_inp, custom_layout)\n+      out2 = jax.device_put(arr, custom_format)\n+      out3 = jax.device_put(jnp_inp, custom_format)\n+      out4 = jax.device_put(np_inp, custom_format)\n \n     for o in [out1, out2, out3, out4]:\n       self.assertArraysEqual(o, np_inp)\n       self.assertEqual(o.format.device_local_layout.major_to_minor,\n-                       custom_layout.device_local_layout.major_to_minor)\n+                       custom_format.device_local_layout.major_to_minor)\n \n   def test_concrete_layout_jit(self):\n     mesh = jtu.create_mesh((2, 2), ('x', 'y'))\n@@ -619,16 +619,16 @@ def test_sparsecore_compute(self):\n \n     dll = DLL(major_to_minor=(0, 1), _tiling=((8,),))\n     s = SingleDeviceSharding(jax.devices()[0])\n-    sparse_layout = Format(dll, s)\n-    sparecore_arr = jax.device_put(inp, sparse_layout)\n-    dense_layout = Format(DLL(major_to_minor=(0, 1)), s)\n+    sparse_format = Format(dll, s)\n+    sparecore_arr = jax.device_put(inp, sparse_format)\n+    dense_format = Format(DLL(major_to_minor=(0, 1)), s)\n \n     @compute_on('tpu_sparsecore')\n     @jax.jit\n     def sparsecore_compute(x):\n       return x * x\n \n-    @partial(jax.jit, out_shardings=(dense_layout, sparse_layout))\n+    @partial(jax.jit, out_shardings=(dense_format, sparse_format))\n     def f(x, y):\n       return x * 2, sparsecore_compute(y)\n \n@@ -645,8 +645,8 @@ def test_sparsecore_compute_twice(self):\n \n     dll = DLL(major_to_minor=(0, 1), _tiling=((8,),))\n     s = SingleDeviceSharding(jax.devices()[0])\n-    sparse_layout = Format(dll, s)\n-    sparecore_arr = jax.device_put(inp, sparse_layout)\n+    sparse_format = Format(dll, s)\n+    sparecore_arr = jax.device_put(inp, sparse_format)\n \n     @compute_on('tpu_sparsecore')\n     @jax.jit\n@@ -658,7 +658,7 @@ def sparsecore_multiply(x, y):\n     def sparsecore_add(x, y):\n       return x + y\n \n-    @partial(jax.jit, donate_argnums=0, out_shardings=sparse_layout)\n+    @partial(jax.jit, donate_argnums=0, out_shardings=sparse_format)\n     def f(x):\n       return sparsecore_multiply(sparsecore_add(x, x) + 1, x)\n \n@@ -675,12 +675,12 @@ def test_sparsecore_and_host_compute(self):\n     s = SingleDeviceSharding(jax.devices()[0])\n \n     sparse_dll = DLL(major_to_minor=(0, 1), _tiling=((8,),))\n-    sparse_layout = Format(sparse_dll, s)\n-    sparecore_arr = jax.device_put(inp, sparse_layout)\n+    sparse_format = Format(sparse_dll, s)\n+    sparecore_arr = jax.device_put(inp, sparse_format)\n \n     host_dll = DLL(major_to_minor=(0, 1), _tiling=((1,),))\n-    host_layout = Format(host_dll, s)\n-    host_arr = jax.device_put(inp, host_layout)\n+    host_format = Format(host_dll, s)\n+    host_arr = jax.device_put(inp, host_format)\n \n     @compute_on('tpu_sparsecore')\n     @jax.jit\n@@ -694,8 +694,8 @@ def host_compute(x):\n \n     @partial(\n         jax.jit,\n-        in_shardings=(sparse_layout, host_layout),\n-        out_shardings=(sparse_layout, host_layout),\n+        in_shardings=(sparse_format, host_format),\n+        out_shardings=(sparse_format, host_format),\n     )\n     def f(x, y):\n       return sparsecore_compute(x), host_compute(y)\n@@ -710,8 +710,8 @@ def test_cpp_layout_cache_miss(self):\n     arr = jax.device_put(np_inp, s)\n \n     arr_m2m = arr.format.device_local_layout.major_to_minor\n-    custom_layout = Format(DLL(major_to_minor=arr_m2m[::-1]), s)\n-    arr2 = jax.device_put(np_inp, custom_layout)\n+    custom_format = Format(DLL(major_to_minor=arr_m2m[::-1]), s)\n+    arr2 = jax.device_put(np_inp, custom_format)\n \n     @jax.jit\n     def f(x):\n@@ -731,9 +731,9 @@ def test_layout_donation_with_default_layout(self):\n     shape = (16, 16)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     arr = jax.device_put(np_inp, s)\n-    out_layout = Format(arr.format.device_local_layout, s)\n+    out_format = Format(arr.format.device_local_layout, s)\n \n-    @partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\n+    @partial(jax.jit, out_shardings=out_format, donate_argnums=0)\n     def f(x):\n       return x * 2\n \n@@ -743,7 +743,7 @@ def f(x):\n \n     out = f(arr)\n     self.assertArraysEqual(out, np_inp * 2)\n-    self.assertEqual(out.format, out_layout)\n+    self.assertEqual(out.format, out_format)\n \n   def test_with_layout_constraint(self):\n     if not jtu.test_device_matches(['tpu']):\n\n```"
        ],
        "from_id": [
            "froystig",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29113 from hawkinsp:install\n\nPiperOrigin-RevId: 765199706",
        "output": [
            "```diff\nCommit: 5a066bccc34ca63daa6b73eb1407f296dcecfea0\nDate: 2025-05-30T15:25:28Z\nURL: https://github.com/jax-ml/jax/commit/5a066bccc34ca63daa6b73eb1407f296dcecfea0\nFiles changed: 2\nAdditions: +16, Deletions: -17\ndiff --git a/README.md b/README.md\nindex 14a0a06ae700..e6af1b344f24 100644\n--- a/README.md\n+++ b/README.md\n@@ -225,14 +225,14 @@ Notebook](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html).\n \n ### Supported platforms\n \n-|            | Linux x86_64 | Linux aarch64 | Mac x86_64   | Mac aarch64  | Windows x86_64 | Windows WSL2 x86_64 |\n-|------------|--------------|---------------|--------------|--------------|----------------|---------------------|\n-| CPU        | yes          | yes           | yes          | yes          | yes            | yes                 |\n-| NVIDIA GPU | yes          | yes           | no           | n/a          | no             | experimental        |\n-| Google TPU | yes          | n/a           | n/a          | n/a          | n/a            | n/a                 |\n-| AMD GPU    | yes          | no            | experimental | n/a          | no             | no                  |\n-| Apple GPU  | n/a          | no            | n/a          | experimental | n/a            | n/a                 |\n-| Intel GPU  | experimental | n/a           | n/a          | n/a          | no             | no                  |\n+|            | Linux x86_64 | Linux aarch64 | Mac aarch64  | Windows x86_64 | Windows WSL2 x86_64 |\n+|------------|--------------|---------------|--------------|----------------|---------------------|\n+| CPU        | yes          | yes           | yes          | yes            | yes                 |\n+| NVIDIA GPU | yes          | yes           | n/a          | no             | experimental        |\n+| Google TPU | yes          | n/a           | n/a          | n/a            | n/a                 |\n+| AMD GPU    | yes          | no            | n/a          | no             | no                  |\n+| Apple GPU  | n/a          | no            | experimental | n/a            | n/a                 |\n+| Intel GPU  | experimental | n/a           | n/a          | no             | no                  |\n \n \n ### Instructions\ndiff --git a/docs/installation.md b/docs/installation.md\nindex 1314a2efa0a8..4019f6461473 100644\n--- a/docs/installation.md\n+++ b/docs/installation.md\n@@ -28,14 +28,14 @@ different builds for different operating systems and accelerators.\n \n The table below shows all supported platforms and installation options. Check if your setup is supported; and if it says _\"yes\"_ or _\"experimental\"_, then click on the corresponding link to learn how to install JAX in greater detail.\n \n-|                  | Linux, x86_64                         | Linux, aarch64                  | Mac, x86_64                           | Mac, aarch64                          | Windows, x86_64          | Windows WSL2, x86_64                     |\n-|------------------|---------------------------------------|---------------------------------|---------------------------------------|---------------------------------------|--------------------------|------------------------------------------|\n-| CPU              | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>`        | {ref}`jax≤0.4.38 only <install-cpu>`  | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>` | {ref}`yes <install-cpu>`                 |\n-| NVIDIA GPU       | {ref}`yes <install-nvidia-gpu>`       | {ref}`yes <install-nvidia-gpu>` | no                                    | n/a                                   | no                       | {ref}`experimental <install-nvidia-gpu>` |\n-| Google Cloud TPU | {ref}`yes <install-google-tpu>`       | n/a                             | n/a                                   | n/a                                   | n/a                      | n/a                                      |\n-| AMD GPU          | {ref}`yes <install-amd-gpu>`          | no                              | {ref}`experimental <install-mac-gpu>` | n/a                                   | no                       | no                                       |\n-| Apple GPU        | n/a                                   | no                              | n/a                                   | {ref}`experimental <install-mac-gpu>` | n/a                      | n/a                                      |\n-| Intel GPU        | {ref}`experimental <install-intel-gpu>`| n/a                            | n/a                                   | n/a                                     | no                       | no                                       |\n+|                  | Linux, x86_64                         | Linux, aarch64                  | Mac, aarch64                          | Windows, x86_64          | Windows WSL2, x86_64                     |\n+|------------------|---------------------------------------|---------------------------------|---------------------------------------|--------------------------|------------------------------------------|\n+| CPU              | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>`        | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>` | {ref}`yes <install-cpu>`                 |\n+| NVIDIA GPU       | {ref}`yes <install-nvidia-gpu>`       | {ref}`yes <install-nvidia-gpu>` | n/a                                   | no                       | {ref}`experimental <install-nvidia-gpu>` |\n+| Google Cloud TPU | {ref}`yes <install-google-tpu>`       | n/a                             | n/a                                   | n/a                      | n/a                                      |\n+| AMD GPU          | {ref}`yes <install-amd-gpu>`          | no                              | n/a                                   | no                       | no                                       |\n+| Apple GPU        | n/a                                   | no                              | {ref}`experimental <install-mac-gpu>` | n/a                      | n/a                                      |\n+| Intel GPU        | {ref}`experimental <install-intel-gpu>`| n/a                            | n/a                                     | no                       | no                                       |\n \n \n (install-cpu)=\n@@ -48,7 +48,6 @@ operating systems and architectures:\n \n - Linux, x86_64\n - Linux, aarch64\n-- macOS, Intel\n - macOS, Apple ARM-based\n - Windows, x86_64 (*experimental*)\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Don't sort replicated and unreduced axes wrt mesh axis names as they are not set and their order actually matters for all-reduce.\n\nPiperOrigin-RevId: 765199626",
        "output": [
            "```diff\nCommit: 73c016a534af51614741d70d36c2c75ca59f2dcc\nDate: 2025-05-30T15:23:16Z\nURL: https://github.com/jax-ml/jax/commit/73c016a534af51614741d70d36c2c75ca59f2dcc\nFiles changed: 1\nAdditions: +3, Deletions: -12\ndiff --git a/jax/_src/named_sharding.py b/jax/_src/named_sharding.py\nindex faf0b2a9f2b2..0b4efdb41d25 100644\n--- a/jax/_src/named_sharding.py\n+++ b/jax/_src/named_sharding.py\n@@ -288,13 +288,6 @@ def _custom_repr(self):\n     priority_repr = '' if self.priority is None else f'p{self.priority}'\n     return f'{{{axes_repr}{open_repr}}}{priority_repr}'\n \n-def _get_axes(axes, mesh_shape):\n-  if not axes:\n-    return ()\n-  assert mesh_shape is not None\n-  # Sort wrt mesh axis names so order is deterministic and doesn't hang in\n-  # McJAX.\n-  return tuple(n for n, _ in mesh_shape if n in axes)\n \n @dataclasses.dataclass(kw_only=True)\n class SdyArray:\n@@ -314,13 +307,11 @@ def build(self) -> sdy.TensorShardingAttr:\n           [sdy.MeshAxisAttr.get(name, size) for name, size in self.mesh_shape],\n           ldi)\n \n-    replicated_axes = _get_axes(self.replicated_axes, self.mesh_shape)\n-    unreduced_axes = _get_axes(self.unreduced_axes, self.mesh_shape)\n     return sdy.TensorShardingAttr.get(\n         mesh_attr,\n         [dim_sharding.build() for dim_sharding in self.dim_shardings],\n-        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes],\n-        unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in unreduced_axes])\n+        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in self.replicated_axes],\n+        unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in self.unreduced_axes])\n \n   def __repr__(self):\n     dim_sharding_repr = ', '.join(\n@@ -342,7 +333,7 @@ def modify_sdy_sharding_wrt_axis_types(sdy_sharding: SdyArray, mesh):\n       dim_shardings.append(SdyDim(axes=[], is_open=True)\n                            if not d.axes and not d.is_open else d)\n       used_axes.extend(d.axes)\n-    remaining_axes = set(mesh.axis_names) - set(used_axes)\n+    remaining_axes = tuple(n for n in mesh.axis_names if n not in used_axes)\n     replicated_axes = tuple(r for r in remaining_axes\n                             if mesh._name_to_type[r] == mesh_lib.AxisType.Explicit)\n     return SdyArray(mesh_shape=sdy_sharding.mesh_shape,\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29097 from MichaelHudgins:more-actions-fixes\n\nPiperOrigin-RevId: 765167817",
        "output": [
            "```diff\nCommit: 2d4baf4772b3ad2a3832104458aa69bbf1456953\nDate: 2025-05-30T13:38:54Z\nURL: https://github.com/jax-ml/jax/commit/2d4baf4772b3ad2a3832104458aa69bbf1456953\nFiles changed: 6\nAdditions: +4, Deletions: -47\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex 09c9d173e0d0..cc3ae89d97f9 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -16,22 +16,18 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version to test?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n \n jobs:\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 458589199c53..d30e1b56dab8 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -17,33 +17,28 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version to test?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       jaxlib-version:\n         description: \"Which jaxlib version to test? (head/pypi_latest)\"\n         type: string\n-        required: true\n         default: \"head\"\n       gcs_download_uri:\n         description: \"GCS location URI from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 72d554aa5d1b..95ab90412494 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -12,7 +12,6 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: choice\n-        required: true\n         default: \"linux-x86-n2-16\"\n         options:\n         - \"linux-x86-n2-16\"\n@@ -21,7 +20,6 @@ on:\n       artifact:\n         description: \"Which JAX artifact to build?\"\n         type: choice\n-        required: true\n         default: \"jaxlib\"\n         options:\n         - \"jax\"\n@@ -31,7 +29,6 @@ on:\n       python:\n         description: \"Which python version should the artifact be built for?\"\n         type: choice\n-        required: false\n         default: \"3.12\"\n         options:\n         - \"3.10\"\n@@ -41,7 +38,6 @@ on:\n       clone_main_xla:\n         description: \"Should latest XLA be used?\"\n         type: choice\n-        required: false\n         default: \"0\"\n         options:\n         - \"1\"\n@@ -49,7 +45,6 @@ on:\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: choice\n-        required: false\n         default: 'no'\n         options:\n         - 'yes'\n@@ -59,31 +54,25 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       artifact:\n         description: \"Which JAX artifact to build?\"\n         type: string\n-        required: true\n         default: \"jaxlib\"\n       python:\n         description: \"Which python version should the artifact be built for?\"\n         type: string\n-        required: false\n         default: \"3.12\"\n       clone_main_xla:\n         description: \"Should latest XLA be used?\"\n         type: string\n-        required: false\n         default: \"0\"\n       upload_artifacts_to_gcs:\n         description: \"Should the artifacts be uploaded to a GCS bucket?\"\n-        required: true\n         default: true\n         type: boolean\n       gcs_upload_uri:\n         description: \"GCS location prefix to where the artifacts should be uploaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n     outputs:\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex a92f2d96dc89..95086257c62b 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -17,34 +17,28 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version should the artifact be built for?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       download-jax-only-from-gcs:\n         description: \"Whether to download only the jax wheel from GCS (e.g for testing a jax only release)\"\n-        required: false\n         default: '0'\n         type: string\n       gcs_download_uri:\n         description: \"GCS location prefix from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n-\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 6fa4e14f8b85..d576370bb772 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -17,44 +17,36 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version to test?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       cuda-version:\n         description: \"Which CUDA version to test?\"\n         type: string\n-        required: true\n         default: \"12.8\"\n       use-nvidia-pip-wheels:\n         description: \"Whether to download CUDA packages from PyPI?\"\n         type: boolean\n-        required: false\n         default: false\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       download-jax-only-from-gcs:\n         description: \"Whether to download only the jax wheel from GCS (e.g for testing a jax only release)\"\n-        required: false\n         default: '0'\n         type: string\n       gcs_download_uri:\n         description: \"GCS location prefix from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n-\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 5f56b165c295..313bbede52f5 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -22,32 +22,26 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-ct5lp-224-8tpu\"\n       cores:\n         description: \"How many TPU cores should the test use?\"\n         type: string\n-        required: true\n         default: \"8\"\n       tpu-type:\n         description: \"Which TPU type is used for testing?\"\n         type: string\n-        required: true\n         default: \"v5e-8\"\n       python:\n         description: \"Which Python version should be used for testing?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       run-full-tpu-test-suite:\n         description: \"Should the full TPU test suite be run?\"\n         type: string\n-        required: false\n         default: \"0\"\n       libtpu-version-type:\n         description: \"Which libtpu version should be used for testing?\"\n         type: string\n-        required: false\n         # Choices are:\n         # - \"nightly\": Use the nightly libtpu wheel.\n         # - \"pypi_latest\": Use the latest libtpu wheel from PyPI.\n@@ -55,20 +49,17 @@ on:\n         default: \"nightly\"\n       download-jax-only-from-gcs:\n         description: \"Whether to download only the jax wheel from GCS (e.g for testing a jax only release)\"\n-        required: false\n         default: '0'\n         type: string\n       gcs_download_uri:\n         description: \"GCS location prefix from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n-\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Remove Mac x86 from the installation instructions.\n\nWe have not been shipping Mac x86 for some time.",
        "output": [
            "```diff\nCommit: 6564a4bb5f9eac51e449f4403997b53c873ffa75\nDate: 2025-05-30T13:10:51Z\nURL: https://github.com/jax-ml/jax/commit/6564a4bb5f9eac51e449f4403997b53c873ffa75\nFiles changed: 2\nAdditions: +16, Deletions: -17\ndiff --git a/README.md b/README.md\nindex 14a0a06ae700..e6af1b344f24 100644\n--- a/README.md\n+++ b/README.md\n@@ -225,14 +225,14 @@ Notebook](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html).\n \n ### Supported platforms\n \n-|            | Linux x86_64 | Linux aarch64 | Mac x86_64   | Mac aarch64  | Windows x86_64 | Windows WSL2 x86_64 |\n-|------------|--------------|---------------|--------------|--------------|----------------|---------------------|\n-| CPU        | yes          | yes           | yes          | yes          | yes            | yes                 |\n-| NVIDIA GPU | yes          | yes           | no           | n/a          | no             | experimental        |\n-| Google TPU | yes          | n/a           | n/a          | n/a          | n/a            | n/a                 |\n-| AMD GPU    | yes          | no            | experimental | n/a          | no             | no                  |\n-| Apple GPU  | n/a          | no            | n/a          | experimental | n/a            | n/a                 |\n-| Intel GPU  | experimental | n/a           | n/a          | n/a          | no             | no                  |\n+|            | Linux x86_64 | Linux aarch64 | Mac aarch64  | Windows x86_64 | Windows WSL2 x86_64 |\n+|------------|--------------|---------------|--------------|----------------|---------------------|\n+| CPU        | yes          | yes           | yes          | yes            | yes                 |\n+| NVIDIA GPU | yes          | yes           | n/a          | no             | experimental        |\n+| Google TPU | yes          | n/a           | n/a          | n/a            | n/a                 |\n+| AMD GPU    | yes          | no            | n/a          | no             | no                  |\n+| Apple GPU  | n/a          | no            | experimental | n/a            | n/a                 |\n+| Intel GPU  | experimental | n/a           | n/a          | no             | no                  |\n \n \n ### Instructions\ndiff --git a/docs/installation.md b/docs/installation.md\nindex 1314a2efa0a8..4019f6461473 100644\n--- a/docs/installation.md\n+++ b/docs/installation.md\n@@ -28,14 +28,14 @@ different builds for different operating systems and accelerators.\n \n The table below shows all supported platforms and installation options. Check if your setup is supported; and if it says _\"yes\"_ or _\"experimental\"_, then click on the corresponding link to learn how to install JAX in greater detail.\n \n-|                  | Linux, x86_64                         | Linux, aarch64                  | Mac, x86_64                           | Mac, aarch64                          | Windows, x86_64          | Windows WSL2, x86_64                     |\n-|------------------|---------------------------------------|---------------------------------|---------------------------------------|---------------------------------------|--------------------------|------------------------------------------|\n-| CPU              | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>`        | {ref}`jax≤0.4.38 only <install-cpu>`  | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>` | {ref}`yes <install-cpu>`                 |\n-| NVIDIA GPU       | {ref}`yes <install-nvidia-gpu>`       | {ref}`yes <install-nvidia-gpu>` | no                                    | n/a                                   | no                       | {ref}`experimental <install-nvidia-gpu>` |\n-| Google Cloud TPU | {ref}`yes <install-google-tpu>`       | n/a                             | n/a                                   | n/a                                   | n/a                      | n/a                                      |\n-| AMD GPU          | {ref}`yes <install-amd-gpu>`          | no                              | {ref}`experimental <install-mac-gpu>` | n/a                                   | no                       | no                                       |\n-| Apple GPU        | n/a                                   | no                              | n/a                                   | {ref}`experimental <install-mac-gpu>` | n/a                      | n/a                                      |\n-| Intel GPU        | {ref}`experimental <install-intel-gpu>`| n/a                            | n/a                                   | n/a                                     | no                       | no                                       |\n+|                  | Linux, x86_64                         | Linux, aarch64                  | Mac, aarch64                          | Windows, x86_64          | Windows WSL2, x86_64                     |\n+|------------------|---------------------------------------|---------------------------------|---------------------------------------|--------------------------|------------------------------------------|\n+| CPU              | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>`        | {ref}`yes <install-cpu>`              | {ref}`yes <install-cpu>` | {ref}`yes <install-cpu>`                 |\n+| NVIDIA GPU       | {ref}`yes <install-nvidia-gpu>`       | {ref}`yes <install-nvidia-gpu>` | n/a                                   | no                       | {ref}`experimental <install-nvidia-gpu>` |\n+| Google Cloud TPU | {ref}`yes <install-google-tpu>`       | n/a                             | n/a                                   | n/a                      | n/a                                      |\n+| AMD GPU          | {ref}`yes <install-amd-gpu>`          | no                              | n/a                                   | no                       | no                                       |\n+| Apple GPU        | n/a                                   | no                              | {ref}`experimental <install-mac-gpu>` | n/a                      | n/a                                      |\n+| Intel GPU        | {ref}`experimental <install-intel-gpu>`| n/a                            | n/a                                     | no                       | no                                       |\n \n \n (install-cpu)=\n@@ -48,7 +48,6 @@ operating systems and architectures:\n \n - Linux, x86_64\n - Linux, aarch64\n-- macOS, Intel\n - macOS, Apple ARM-based\n - Windows, x86_64 (*experimental*)\n \n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Unconditionally emit line info for Mosaic GPU kernels\n\nI also changed the lowering to override --jax_include_full_tracebacks_in_locations\nso that we get a single location per emitted op, since the\nensure-debug-info-scope-on-llvm-func pass in MLIR does not correctly handle\nnested CallSiteLocs.\n\nPiperOrigin-RevId: 765112273",
        "output": [
            "```diff\nCommit: a9407763d9abf80f8636631065f9256cf0238e6d\nDate: 2025-05-30T10:13:38Z\nURL: https://github.com/jax-ml/jax/commit/a9407763d9abf80f8636631065f9256cf0238e6d\nFiles changed: 3\nAdditions: +13, Deletions: -12\ndiff --git a/jax/_src/pallas/mosaic_gpu/BUILD b/jax/_src/pallas/mosaic_gpu/BUILD\nindex 74b44fb8f991..8a5d087125b7 100644\n--- a/jax/_src/pallas/mosaic_gpu/BUILD\n+++ b/jax/_src/pallas/mosaic_gpu/BUILD\n@@ -45,6 +45,7 @@ pytype_strict_library(\n         \":core\",\n         \":lowering\",\n         \"//jax\",\n+        \"//jax:config\",\n         \"//jax:core\",\n         \"//jax:mlir\",\n         \"//jax:mosaic_gpu\",\ndiff --git a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\nindex ccbe4d36edc9..1d55a6e862a0 100644\n--- a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n+++ b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n@@ -24,6 +24,7 @@\n \n import jax\n from jax import lax\n+from jax._src import config\n from jax._src import core as jax_core\n from jax._src import sharding_impls\n from jax._src.interpreters import mlir\n@@ -73,9 +74,12 @@ def pallas_call_lowering(\n     if isinstance(axis_context, sharding_impls.SPMDAxisContext):\n       jax_mesh = axis_context.mesh\n \n-  lowering_result = lowering.lower_pipelined_jaxpr_to_module(\n-      grid_mapping, mesh, jax_mesh, jaxpr, params, cost_estimate\n-  )\n+  # TODO(slebedev): Remove this once the ensure-debug-info-scope-on-llvm-func\n+  # pass correctly handles full tracebacks.\n+  with config.include_full_tracebacks_in_locations(False):\n+    lowering_result = lowering.lower_pipelined_jaxpr_to_module(\n+        grid_mapping, mesh, jax_mesh, jaxpr, params, cost_estimate\n+    )\n   if debug:\n     print(f\"\\nThe Mosaic GPU module for pallas_call {debug_info.func_src_info}:\")\n     print(lowering_result.module.operation)\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 5253d4590658..214521ce3764 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -343,7 +343,6 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n     mlir::LLVM::registerDIScopeForLLVMFuncOpPass();\n     return true;\n   });\n-  bool emit_line_info = getenv(\"MOSAIC_GPU_LINE_INFO\") != nullptr;\n   const char *cuda_root = GetCUDARoot();\n   if (!cuda_root) {\n     return mlir::failure();\n@@ -360,8 +359,8 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n         convert-scf-to-cf,\n         convert-nvvm-to-llvm,\n         expand-strided-metadata,\n-        nvvm-attach-target{O=3 chip=)\", sm, \" fast=false features=+\",\n-      ptx_isa,\n+        nvvm-attach-target{O=3 chip=)\",\n+      sm, \" fast=false features=+\", ptx_isa,\n       R\"( ftz=false  module= triple=nvptx64-nvidia-cuda},\n         lower-affine,\n         convert-arith-to-llvm{index-bitwidth=0},\n@@ -369,7 +368,6 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n         canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true},\n         cse,\n         )\",\n-      emit_line_info ? \"\" : \"gpu.module(strip-debuginfo),\",\n       R\"(\n         gpu.module(convert-gpu-to-nvvm{has-redux=false index-bitwidth=64 use-bare-ptr-memref-call-conv=false}),\n         gpu.module(canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}),\n@@ -377,13 +375,11 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n         gpu.module(mosaic-byval-insertion),\n         gpu.module(reconcile-unrealized-casts),\n         mosaic-convert-gpu-to-llvm,\n-        )\",\n-      emit_line_info ? \"ensure-debug-info-scope-on-llvm-func{emission-kind=DebugDirectivesOnly},\" : \"\",\n-      \"gpu-module-to-binary{format=\",\n+        ensure-debug-info-scope-on-llvm-func{emission-kind=DebugDirectivesOnly},\n+        gpu-module-to-binary{format=)\",\n       mlir::gpu::stringifyCompilationTarget(target).str(),\n       (!nvshmem_path.empty() ? \" l=\" + nvshmem_path : \"\"),\n-      (emit_line_info ? \"  opts=-lineinfo\" : \"\"),\n-      \" toolkit=\", cuda_root,\n+      \"  opts=-lineinfo toolkit=\", cuda_root,\n       R\"(},\n         convert-math-to-llvm{approximate-log1p=true},\n         canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true},\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Move the semaphore implementation to Mosaic\n\nPallas lowering should not be doing any heavy lifting here. The implementation\nis quite low level and should ideally live closer to where other synchronization\nprimitives are implemented.\n\nPiperOrigin-RevId: 765092823",
        "output": [
            "```diff\nCommit: 75b2c7e553e7ad9a141e0d94ff45af31eacfebd3\nDate: 2025-05-30T09:04:19Z\nURL: https://github.com/jax-ml/jax/commit/75b2c7e553e7ad9a141e0d94ff45af31eacfebd3\nFiles changed: 4\nAdditions: +125, Deletions: -36\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 003fa0419f63..ce74a5ba7c05 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -3108,13 +3108,8 @@ def _semaphore_signal_lowering_rule(\n   # receive a signal).\n   if ctx.module_ctx.auto_barriers:\n     mgpu.utils.warpgroup_barrier()\n-  pred = ctx.module_ctx.single_wg_lane_predicate\n-  llvm_dialect.inline_asm(\n-    i32,\n-    [sem_ptr, val, pred],\n-    \"@$3 atom.add.release.sys.global.u32 $0, [$1], $2;\",\n-    \"=r,l,r,b\",\n-    has_side_effects=True,\n+  mgpu_utils.SemaphoreRef(sem_ptr).signal(\n+      val, predicate=ctx.module_ctx.single_wg_lane_predicate\n   )\n   return ()\n \n@@ -3127,35 +3122,9 @@ def _semaphore_wait_lowering_rule(ctx: LoweringRuleContext, *args, args_tree):\n     raise NotImplementedError(\n         f\"Unhandled transforms for semaphore_wait: {transforms}\"\n     )\n-\n-  sem_ptr = mgpu.utils.memref_ptr(sem)\n-  i32_ty = ir.IntegerType.get_signless(32)\n-  ne_pred = arith_dialect.CmpIPredicate.ne\n-  val = _ir_constant(value, i32_ty)\n-\n-  with mgpu.single_thread(scope=mgpu.ThreadSubset.WARPGROUP):\n-    # Create the while loop for busy waiting\n-    while_op = scf_dialect.WhileOp([i32_ty], [val])\n-    before_block = while_op.before.blocks.append(i32_ty)\n-    with ir.InsertionPoint.at_block_begin(before_block):\n-      [expected_in_memory] = before_block.arguments\n-      new_val = arith_dialect.subi(expected_in_memory, val)\n-      in_memory = llvm_dialect.inline_asm(\n-        i32_ty,\n-        [sem_ptr, expected_in_memory, new_val],\n-        \"atom.acquire.sys.global.cas.b32 $0, [$1], $2, $3;\",\n-        \"=r,l,r,r\",\n-        has_side_effects=True,\n-      )\n-      comparison = arith_dialect.cmpi(ne_pred, in_memory, expected_in_memory)\n-      new_expected_in_memory = arith_dialect.maxui(in_memory, val)\n-      scf_dialect.condition(comparison, [new_expected_in_memory])\n-    after_block = while_op.after.blocks.append(i32_ty)\n-    with ir.InsertionPoint.at_block_begin(after_block):\n-      scf_dialect.yield_(after_block.arguments)\n-  # NOTE: This barrier is necessary for a correct lowering of this op and can't\n-  # be removed even if auto_barriers is False.\n-  mgpu_utils.warpgroup_barrier()\n+  i32 = ir.IntegerType.get_signless(32)\n+  val = _ir_constant(value, i32)\n+  mgpu_utils.SemaphoreRef(mgpu.utils.memref_ptr(sem)).wait(val)\n   return ()\n \n \ndiff --git a/jax/experimental/mosaic/gpu/__init__.py b/jax/experimental/mosaic/gpu/__init__.py\nindex 82155f86d9ea..cd207c2b2519 100644\n--- a/jax/experimental/mosaic/gpu/__init__.py\n+++ b/jax/experimental/mosaic/gpu/__init__.py\n@@ -74,6 +74,7 @@\n     DynamicSlice as DynamicSlice,\n     Partition as Partition,\n     Partition1D as Partition1D,\n+    SemaphoreRef as SemaphoreRef,\n     ThreadSubset as ThreadSubset,\n     bitwidth as bitwidth,\n     bytewidth as bytewidth,\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex 51b6ed4612ca..a76e077ff463 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -744,6 +744,9 @@ def warpgroup_barrier():\n       has_side_effects=True,\n   )\n \n+def warp_barrier():\n+  nvvm.bar_warp_sync(c(0xffffffff, ir.IntegerType.get_signless(32)))\n+\n \n @dataclasses.dataclass(frozen=True)\n class BarrierRef:\n@@ -1046,6 +1049,67 @@ def wait_parity(self, *args, **kwargs):\n     self.barrier.wait_parity(*args, **kwargs)\n \n \n+@dataclasses.dataclass(frozen=True)\n+class SemaphoreRef:\n+  ptr: ir.Value\n+\n+  def signal(self, value: ir.Value | int, predicate: ir.Value | None = None):\n+    i32 = ir.IntegerType.get_signless(32)\n+    if not isinstance(value, ir.Value):\n+      value = c(value, i32)\n+    elif value.type != i32:\n+      raise ValueError(f\"Expected a i32 value, got {value.type}\")\n+    if predicate is None:\n+      predicate = single_thread_predicate(ThreadSubset.WARPGROUP)\n+    llvm.inline_asm(\n+      i32,\n+      [self.ptr, value, predicate],\n+      \"@$3 atom.add.release.sys.global.u32 $0, [$1], $2;\",\n+      \"=r,l,r,b\",\n+      has_side_effects=True,\n+    )\n+\n+  def wait(\n+      self,\n+      value: ir.Value | int = 1,\n+      scope: ThreadSubset = ThreadSubset.WARPGROUP,\n+  ):\n+    i32 = ir.IntegerType.get_signless(32)\n+    if not isinstance(value, ir.Value):\n+      value = c(value, i32)\n+    elif value.type != i32:\n+      raise ValueError(f\"Expected a i32 value, got {value.type}\")\n+\n+    ne_pred = arith.CmpIPredicate.ne\n+\n+    with single_thread(scope=scope):\n+      # Create the while loop for busy waiting\n+      while_op = scf.WhileOp([i32], [value])\n+      before_block = while_op.before.blocks.append(i32)\n+      with ir.InsertionPoint.at_block_begin(before_block):\n+        [expected_in_memory] = before_block.arguments\n+        new_val = arith.subi(expected_in_memory, value)\n+        in_memory = llvm.inline_asm(\n+          i32,\n+          [self.ptr, expected_in_memory, new_val],\n+          \"atom.acquire.sys.global.cas.b32 $0, [$1], $2, $3;\",\n+          \"=r,l,r,r\",\n+          has_side_effects=True,\n+        )\n+        comparison = arith.cmpi(ne_pred, in_memory, expected_in_memory)\n+        new_expected_in_memory = arith.maxui(in_memory, value)\n+        scf.condition(comparison, [new_expected_in_memory])\n+      after_block = while_op.after.blocks.append(i32)\n+      with ir.InsertionPoint.at_block_begin(after_block):\n+        scf.yield_(after_block.arguments)\n+    if scope == ThreadSubset.WARPGROUP:\n+      warpgroup_barrier()\n+    elif scope == ThreadSubset.WARP:\n+      warp_barrier()\n+    else:\n+      raise ValueError(f\"Unsupported scope: {scope}\")\n+\n+\n class Partition:\n   source_bounds: tuple[int, ...]\n   target_bounds: tuple[int, ...]\ndiff --git a/tests/mosaic/gpu_test_distributed.py b/tests/mosaic/gpu_test_distributed.py\nindex cf3913771983..c289b27c0be1 100644\n--- a/tests/mosaic/gpu_test_distributed.py\n+++ b/tests/mosaic/gpu_test_distributed.py\n@@ -23,6 +23,7 @@\n from jax._src.interpreters import mlir\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import arith\n+from jax._src.lib.mlir.dialects import memref\n from jax.experimental.mosaic.gpu import dialect as mgpu_dialect  # pylint: disable=g-importing-member\n from jax.experimental import shard\n from jax.experimental import multihost_utils\n@@ -70,6 +71,28 @@ def setUp(self):\n \n class ProfilerTest(TestCase):\n \n+  def test_get_device_id(self):\n+    index = ir.IndexType.get()\n+    def kernel(ctx, dst, _):\n+      device_id = ctx.device_id()\n+      memref.store(device_id, dst, [arith.constant(index, 0)])\n+    mesh = jax.make_mesh(\n+        (jax.device_count(),), (\"x\",), axis_types=(jax.sharding.AxisType.Explicit,)\n+    )\n+    with jax.sharding.use_mesh(mesh):\n+      out_shape = jax.ShapeDtypeStruct((1,), jnp.int32)\n+      y = jax.jit(\n+          jax.shard_map(\n+              mgpu.as_gpu_kernel(\n+                  kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ()\n+              ),\n+              out_specs=P(\"x\"),\n+              check_vma=False,\n+          )\n+      )()\n+      y_np = multihost_utils.process_allgather(y, tiled=True)\n+      np.testing.assert_array_equal(y_np, np.arange(jax.device_count()))\n+\n   def test_remote_async_copy(self):\n     i32 = ir.IntegerType.get_signless(32)\n     def kernel(ctx, src, dst, scratch):\n@@ -99,6 +122,38 @@ def kernel(ctx, src, dst, scratch):\n           y_np, np.concatenate(np.split(x_np, 2)[::-1], axis=0)\n       )\n \n+  def test_remote_semaphore(self):\n+    i32 = ir.IntegerType.get_signless(32)\n+    def kernel(ctx, sem, _):\n+      my_device = ctx.device_id()\n+      other_device = arith.subi(arith.constant(i32, 1), my_device)\n+      my_sem = mgpu.SemaphoreRef(mgpu.utils.memref_ptr(sem))\n+      other_dst = ctx.to_remote(sem, other_device)\n+      other_sem = mgpu.SemaphoreRef(mgpu.utils.memref_ptr(other_dst))\n+      # We signal and wait a different amount on each device to make sure we're\n+      # really communicating here.\n+      other_sem.signal(arith.addi(arith.constant(i32, 1), other_device))\n+      @mgpu.fori(arith.addi(arith.constant(i32, 1), my_device), None)\n+      def wait_loop(i, _):\n+        my_sem.wait(1)\n+\n+    mesh = jax.make_mesh(\n+        (2,), (\"x\",), axis_types=(jax.sharding.AxisType.Explicit,)\n+    )\n+    with jax.sharding.use_mesh(mesh):\n+      sem = shard.reshard(jnp.zeros((1,), dtype=jnp.int32), P())\n+      out_sem = jax.jit(\n+          jax.shard_map(\n+              mgpu.as_gpu_kernel(\n+                  kernel, (1, 1, 1), (128, 1, 1), (), (), (), inout_shape=sem\n+              ),\n+              out_specs=P(\"x\"),\n+              check_vma=False,\n+          )\n+      )(sem)\n+      out_sems = multihost_utils.process_allgather(out_sem, tiled=True)\n+      np.testing.assert_array_equal(out_sems, np.zeros_like(out_sems))\n+\n \n if __name__ == \"__main__\":\n   # This test doesn't work with the platform allocator, so we override it\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "rename `Array.layout` to `Array.format`\n\nThis change renames the attribute and updates the codebase to refer to the new name. It should have minimal external effect, since it keeps a `layout` alias for the attribute.\n\nCo-authored-by: Yash Katariya <yashkatariya@google.com>\nPiperOrigin-RevId: 764967359",
        "output": [
            "```diff\nCommit: 7ff6f0d01a7e324787b238f6998028a4a2686625\nDate: 2025-05-30T01:35:42Z\nURL: https://github.com/jax-ml/jax/commit/7ff6f0d01a7e324787b238f6998028a4a2686625\nFiles changed: 8\nAdditions: +46, Deletions: -43\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 2630fc7ae1be..c21e8248d52d 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -2909,12 +2909,12 @@ def update(self, **kwargs):\n       if self._dll is not None and isinstance(s, Sharding):\n         raise ValueError(\n             f\"You are updating ShapeDtypeStruct with a {type(s)} when the\"\n-            f\" original ShapeDtypeStruct had a concrete layout {self.layout}.\"\n+            f\" original ShapeDtypeStruct had a concrete layout {self.format}.\"\n             \" This might lead to bugs. If you want to do this, create a new\"\n             \" ShapeDtypeStruct via the constructor.\")\n       sharding = s\n     else:\n-      sharding = self.layout\n+      sharding = self.format\n     return ShapeDtypeStruct(\n         shape=kwargs.pop('shape', self.shape),\n         dtype=kwargs.pop('dtype', self.dtype),\ndiff --git a/jax/_src/array.py b/jax/_src/array.py\nindex 29c7a17b07f1..9a71b12ed1a8 100644\n--- a/jax/_src/array.py\n+++ b/jax/_src/array.py\n@@ -547,7 +547,7 @@ def addressable_shards(self) -> Sequence[Shard]:\n     return out\n \n   @property\n-  def layout(self):\n+  def format(self):\n     # TODO(yashkatariya): Remove the deleted check from here.\n     if self.is_deleted():\n       return Format(None, self.sharding)\n@@ -561,6 +561,9 @@ def layout(self):\n       else:\n         raise\n \n+  # TODO(frostig, yashkatariya): remove\n+  layout = format\n+\n   @property\n   def global_shards(self) -> Sequence[Shard]:\n     \"\"\"Returns list of all `Shard`s of the Array across all devices.\n@@ -812,7 +815,7 @@ def get_data(index: Index | None) -> ArrayImpl | np.ndarray:\n         and sharding.is_fully_replicated\n         and first_value.is_fully_replicated\n         and first_value.sharding._device_assignment == tuple(devices)\n-        and first_value.layout.device_local_layout == dll):\n+        and first_value.format.device_local_layout == dll):\n       return first_value\n \n   if dtypes.issubdtype(aval.dtype, dtypes.extended):\n@@ -1197,7 +1200,7 @@ def _array_shard_arg(xs, shardings, layouts, copy_semantics):\n     x._check_if_deleted()\n     indices, same_indices = _sharding_indices_and_eq(x.sharding, x.shape, sharding)\n     same_layout = (True if layout is None else\n-                   x.layout.device_local_layout == layout)\n+                   x.format.device_local_layout == layout)\n \n     if not x.is_fully_addressable:\n       if same_indices and same_layout:\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex a7c8d4ea7380..028c2cfa125e 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -497,7 +497,7 @@ def _device_put_impl(\n   if isinstance(device, Format):\n     l = device\n     dll = l.device_local_layout\n-    x_dll = x.layout.device_local_layout if hasattr(x, 'layout') else None\n+    x_dll = x.format.device_local_layout if hasattr(x, 'format') else None\n     if dll is None and l.sharding is None:\n       return _device_put_sharding_impl(x, aval, l.sharding, copy)\n     if (not isinstance(l.sharding, Sharding) or\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex af1f8217951c..2072aaf44b5a 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -3255,11 +3255,11 @@ def check_array_xla_sharding_layout_match(\n           'sharding'))\n \n     if (not db_xs and arg._committed and\n-        arg.layout.device_local_layout is not None and xl is not None and\n-        arg.layout.device_local_layout != xl):\n+        arg.format.device_local_layout is not None and xl is not None and\n+        arg.format.device_local_layout != xl):\n       errors.append(\n           (\"Got input layout(s) that compiled object was called with: \"\n-          f\"{arg.layout.device_local_layout} and layout(s) the computation was \"\n+          f\"{arg.format.device_local_layout} and layout(s) the computation was \"\n           f\"compiled with: {xl} for arg {name} with \"\n           f\"shape: {arg.aval.str_short()}\",\n           'layout'))\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex f012459296a1..4113f764e888 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -1651,8 +1651,8 @@ def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n     # below. We cannot replace default layout with None to raise nicer errors.\n     # `dispatch_arg_layout` replaces default layouts with `None` to simplify\n     # dispatch and lowering logic downstream.\n-    if hasattr(arg, 'layout'):\n-      arg_layout = arg.layout.device_local_layout\n+    if hasattr(arg, 'format'):\n+      arg_layout = arg.format.device_local_layout\n       dispatch_arg_layout = (None if pxla.is_default_layout(arg_layout, rs, aval)\n                              else arg_layout)\n     else:\n@@ -1670,7 +1670,7 @@ def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n         resolved_in_layouts.append(None)\n     else:\n       # arg_layout can be None because some backends don't implement the\n-      # required layout methods. Hence `arr.layout` can return\n+      # required layout methods. Hence `arr.format` can return\n       # `Format(None, sharding)`\n       if (committed\n           and not is_pmap_sharding\n@@ -2845,7 +2845,7 @@ def _sharding_constraint_impl(x, sharding, layout, context_mesh,\n     # Run a jit here to raise good errors when device assignment don't match.\n     return api.jit(_identity_fn, out_shardings=sharding)(x)\n   else:\n-    if (hasattr(x, 'layout') and x.layout.device_local_layout == layout and\n+    if (hasattr(x, 'format') and x.format.device_local_layout == layout and\n         x.sharding.is_equivalent_to(sharding, x.ndim)):\n       return x\n     return api.jit(_identity_fn, out_shardings=Format(layout, sharding))(x)\n@@ -3193,7 +3193,7 @@ def _layout_constraint_impl(x, *, layout):\n     raise ValueError(\n         'with_layout_constraint in eager mode can only be applied to'\n         f' jax.Arrays. Got {type(x)}')\n-  if x.layout.device_local_layout == layout:  # type: ignore\n+  if x.format.device_local_layout == layout:  # type: ignore\n     return x\n   return api.jit(_identity_fn, out_shardings=Format(layout, x.sharding))(x)\n layout_constraint_p.def_impl(_layout_constraint_impl)\ndiff --git a/jax/experimental/array_serialization/serialization_test.py b/jax/experimental/array_serialization/serialization_test.py\nindex 3bee72967101..0611388a1d80 100644\n--- a/jax/experimental/array_serialization/serialization_test.py\n+++ b/jax/experimental/array_serialization/serialization_test.py\n@@ -595,7 +595,7 @@ def test_load_with_layout(self):\n \n     out_layout = jax.jit(lambda x: x.T, out_shardings=Format(DLL.AUTO)).lower(\n         arr).compile().output_layouts\n-    self.assertEqual(arr.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(arr.format.device_local_layout.major_to_minor,\n                      out_layout.device_local_layout.major_to_minor[::-1])\n \n     ckpt_dir = pathlib.Path(self.create_tempdir('ckpt').full_path)\n@@ -611,7 +611,7 @@ def test_load_with_layout(self):\n \n     out, = serialization.run_deserialization([out_layout], tspecs)\n \n-    self.assertEqual(out.layout, out_layout)\n+    self.assertEqual(out.format, out_layout)\n     self.assertIsInstance(out, array.ArrayImpl)\n     self.assertArraysEqual(out, np_inp)\n     for s in out.addressable_shards:\ndiff --git a/tests/layout_test.py b/tests/layout_test.py\nindex cfec2253dfc8..d7b23c75b313 100644\n--- a/tests/layout_test.py\n+++ b/tests/layout_test.py\n@@ -77,21 +77,21 @@ def init(x, y):\n       init_compiled(arr1, arr2)\n     self.assertEqual(init_count(), 1)\n \n-    self.assertEqual(init_out[0].layout, init_compiled.output_layouts[0])\n-    self.assertEqual(init_out[1].layout, init_compiled.output_layouts[1])\n+    self.assertEqual(init_out[0].format, init_compiled.output_layouts[0])\n+    self.assertEqual(init_out[1].format, init_compiled.output_layouts[1])\n \n     with jtu.count_aot_jit_cpp_cache_miss() as apply_count:\n       apply_out = compiled_apply(*init_out)\n       compiled_apply(*init_out)\n     self.assertEqual(apply_count(), 1)\n \n-    self.assertEqual(apply_out[0].layout, compiled_apply.output_layouts[0])\n-    self.assertEqual(apply_out[1].layout, compiled_apply.output_layouts[1])\n+    self.assertEqual(apply_out[0].format, compiled_apply.output_layouts[0])\n+    self.assertEqual(apply_out[1].format, compiled_apply.output_layouts[1])\n \n-    self.assertTupleEqual(apply_out[0].layout.device_local_layout.major_to_minor,\n-                          init_out[0].layout.device_local_layout.major_to_minor[::-1])\n-    self.assertTupleEqual(apply_out[1].layout.device_local_layout.major_to_minor,\n-                          init_out[1].layout.device_local_layout.major_to_minor[::-1])\n+    self.assertTupleEqual(apply_out[0].format.device_local_layout.major_to_minor,\n+                          init_out[0].format.device_local_layout.major_to_minor[::-1])\n+    self.assertTupleEqual(apply_out[1].format.device_local_layout.major_to_minor,\n+                          init_out[1].format.device_local_layout.major_to_minor[::-1])\n \n     self.assertArraysEqual(init_out[0], np_inp1 * 2)\n     self.assertArraysEqual(init_out[1], np_inp2 * 2)\n@@ -157,7 +157,7 @@ def f(x):\n \n     out = compiled(arr)\n     self.assertArraysEqual(out, np_inp.T)\n-    self.assertEqual(out.layout, compiled.output_layouts)\n+    self.assertEqual(out.format, compiled.output_layouts)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('y', 'x')))\n \n   def test_sharding_and_layouts(self):\n@@ -277,11 +277,11 @@ def test_device_put_concrete_layout(self):\n     col = compiled.output_layouts\n \n     out = jax.device_put(np_inp, col)\n-    self.assertEqual(out.layout, col)\n+    self.assertEqual(out.format, col)\n     self.assertArraysEqual(out, np_inp)\n     for s in out.addressable_shards:\n-      self.assertEqual(out.layout.device_local_layout,\n-                       s.data.layout.device_local_layout)\n+      self.assertEqual(out.format.device_local_layout,\n+                       s.data.format.device_local_layout)\n \n   def test_device_put_non_concrete_layout_error(self):\n     np_inp = np.arange(16).reshape(8, 2)\n@@ -338,7 +338,7 @@ def test_make_array_from_callback(self):\n     out = jax.make_array_from_callback(np_inp.shape, layout,\n                                        lambda idx: np_inp[idx])\n     self.assertArraysEqual(out, np_inp)\n-    self.assertEqual(out.layout, layout)\n+    self.assertEqual(out.format, layout)\n \n     with self.assertRaisesRegex(\n         TypeError,\n@@ -370,9 +370,9 @@ def f(x):\n       return jax.lax.with_sharding_constraint(y, Format(custom_dll, s))\n \n     out = f(arr)\n-    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(out.format.device_local_layout.major_to_minor,\n                      custom_dll.major_to_minor)\n-    self.assertEqual(out.layout, arr.layout)\n+    self.assertEqual(out.format, arr.format)\n     self.assertArraysEqual(out, np_inp.T)\n \n   def test_wsc_bfloat16_concrete_layout(self):\n@@ -393,9 +393,9 @@ def f(x):\n       return jax.lax.with_sharding_constraint(y, Format(custom_dll, s))\n \n     out = f(arr)\n-    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(out.format.device_local_layout.major_to_minor,\n                      custom_dll.major_to_minor)\n-    self.assertEqual(out.layout, arr.layout)\n+    self.assertEqual(out.format, arr.format)\n     self.assertArraysEqual(out, inp.T)\n \n   def test_device_put_user_concrete_layout(self):\n@@ -405,7 +405,7 @@ def test_device_put_user_concrete_layout(self):\n     s = SingleDeviceSharding(jax.devices()[0])\n \n     out = jax.device_put(np_inp, Format(dll, s))\n-    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(out.format.device_local_layout.major_to_minor,\n                      dll.major_to_minor)\n     self.assertArraysEqual(out, np_inp)\n \n@@ -427,7 +427,7 @@ def test_device_put_user_concrete_layout_multi_device(self):\n \n     for o in [out1, out2, out3, out4]:\n       self.assertArraysEqual(o, np_inp)\n-      self.assertEqual(o.layout.device_local_layout.major_to_minor,\n+      self.assertEqual(o.format.device_local_layout.major_to_minor,\n                        custom_layout.device_local_layout.major_to_minor)\n \n   def test_concrete_layout_jit(self):\n@@ -445,7 +445,7 @@ def f(x):\n \n     out = f(arr)\n     self.assertArraysEqual(out, np_inp.T)\n-    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(out.format.device_local_layout.major_to_minor,\n                      custom_dll.major_to_minor)\n \n   def test_compatible_aval_error(self):\n@@ -489,7 +489,7 @@ def f(x):\n \n     out = f(arr)\n     self.assertArraysEqual(out, np_inp.T)\n-    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(out.format.device_local_layout.major_to_minor,\n                      custom_dll.major_to_minor[::-1])\n \n     custom_dll2 = DLL(major_to_minor=(1, 0))\n@@ -709,7 +709,7 @@ def test_cpp_layout_cache_miss(self):\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     arr = jax.device_put(np_inp, s)\n \n-    arr_m2m = arr.layout.device_local_layout.major_to_minor\n+    arr_m2m = arr.format.device_local_layout.major_to_minor\n     custom_layout = Format(DLL(major_to_minor=arr_m2m[::-1]), s)\n     arr2 = jax.device_put(np_inp, custom_layout)\n \n@@ -731,7 +731,7 @@ def test_layout_donation_with_default_layout(self):\n     shape = (16, 16)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     arr = jax.device_put(np_inp, s)\n-    out_layout = Format(arr.layout.device_local_layout, s)\n+    out_layout = Format(arr.format.device_local_layout, s)\n \n     @partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\n     def f(x):\n@@ -743,7 +743,7 @@ def f(x):\n \n     out = f(arr)\n     self.assertArraysEqual(out, np_inp * 2)\n-    self.assertEqual(out.layout, out_layout)\n+    self.assertEqual(out.format, out_layout)\n \n   def test_with_layout_constraint(self):\n     if not jtu.test_device_matches(['tpu']):\n@@ -755,7 +755,7 @@ def test_with_layout_constraint(self):\n     arr = jax.device_put(np_inp, s)\n \n     # Create a custom layout instead of using `arr.layout` to test the API.\n-    custom_dll = DLL(major_to_minor=arr.layout.dll.major_to_minor[::-1])\n+    custom_dll = DLL(major_to_minor=arr.format.dll.major_to_minor[::-1])\n \n     def f(x):\n       y = x.T\n@@ -768,7 +768,7 @@ def f(x):\n \n     f = jax.jit(f)\n     out = f(arr)\n-    self.assertEqual(out.layout.device_local_layout.major_to_minor,\n+    self.assertEqual(out.format.device_local_layout.major_to_minor,\n                      custom_dll.major_to_minor)\n     self.assertArraysEqual(out, np_inp.T * 2)\n \ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 751fd63823e3..dd5c5d46e62f 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -5001,7 +5001,7 @@ def test_sds_update(self):\n     new_layout = Format(DLL((1, 0)), NamedSharding(mesh, P('x')))\n     s4_u = s4.update(sharding=new_layout)\n     self.assertEqual(s4_u.sharding, new_layout.sharding)\n-    self.assertEqual(s4_u.layout, new_layout)\n+    self.assertEqual(s4_u.format, new_layout)\n \n     with self.assertRaisesRegex(ValueError, \"updating ShapeDtypeStruct\"):\n       s4.update(sharding=NamedSharding(mesh, P('x')))\n\n```"
        ],
        "from_id": [
            "froystig",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Make 1D tiling agnostic to large 2nd minor flags.\n\nPiperOrigin-RevId: 764937293",
        "output": [
            "```diff\nCommit: 663e50f72cc6cf420e78f865837f82112d2425b6\nDate: 2025-05-29T23:47:36Z\nURL: https://github.com/jax-ml/jax/commit/663e50f72cc6cf420e78f865837f82112d2425b6\nFiles changed: 1\nAdditions: +12, Deletions: -6\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_memref_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_memref_layout.cc\nindex bfb9be87dfd0..b772c5c8a114 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_memref_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_memref_layout.cc\n@@ -58,10 +58,12 @@ namespace mlir::tpu {\n //     enabled by XLA for memrefs.\n //   bitwidth: The bitwidth of the element type of the operand.\n //   is_kernel_argument: Whether the operand is a kernel argument.\n+//   is_1d: Whether the operand is 1D.\n int getTilingFactor(const int src_sublane, const int hardware_generation,\n                     const int64_t target_sublane_count,\n                     const TpuTilingFlags &tpu_tiling_flags,\n-                    const int8_t bitwidth, const bool is_kernel_argument) {\n+                    const int8_t bitwidth, const bool is_kernel_argument,\n+                    const bool is_1d) {\n   CHECK(llvm::isPowerOf2_32(bitwidth));\n   CHECK_LE(2, bitwidth);\n   CHECK_LE(bitwidth, 32);\n@@ -76,6 +78,10 @@ int getTilingFactor(const int src_sublane, const int hardware_generation,\n   const int max_normal_tiling = tiling_sublane;\n \n   int large_tiling = [&] {\n+    if (is_1d) {\n+      // 1D tiling is always compact.\n+      return tiling_sublane;\n+    }\n     if (bitwidth == 2) {\n       return target_sublane_count * 16;\n     }\n@@ -151,9 +157,9 @@ FailureOr<TiledLayoutAttr> inferLayout(MemRefType memref_ty,\n       auto src_sublane =\n           llvm::divideCeil(memref_ty.getShape().back(), lane_count);\n       const int64_t leading_tile =\n-          getTilingFactor(src_sublane, hardware_generation,\n-                          sublane_count, tpu_tiling_flags, bitwidth,\n-                          is_kernel_argument) *\n+          getTilingFactor(src_sublane, hardware_generation, sublane_count,\n+                          tpu_tiling_flags, bitwidth, is_kernel_argument,\n+                          /*is_1d=*/true) *\n           lane_count;\n       SmallVector<xla::Tile> tiles{xla::Tile({leading_tile})};\n       if (bitwidth != 32) {\n@@ -173,8 +179,8 @@ FailureOr<TiledLayoutAttr> inferLayout(MemRefType memref_ty,\n     const int64_t src_sublane = shape[shape.size() - 2];\n     if (leading_tile_rows == 0) {\n       leading_tile_rows = getTilingFactor(\n-          src_sublane, hardware_generation, sublane_count,\n-          tpu_tiling_flags, bitwidth, is_kernel_argument);\n+          src_sublane, hardware_generation, sublane_count, tpu_tiling_flags,\n+          bitwidth, is_kernel_argument, /*is_1d=*/false);\n     }\n     SmallVector<xla::Tile> tiles{xla::Tile({leading_tile_rows, lane_count})};\n     if (bitwidth != 32) {\n\n```"
        ],
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29101 from mattjj:hijax\n\nPiperOrigin-RevId: 764930280",
        "output": [
            "```diff\nCommit: da106b971af143906c8cbec8f4f2dfb7b57a11ce\nDate: 2025-05-29T23:27:42Z\nURL: https://github.com/jax-ml/jax/commit/da106b971af143906c8cbec8f4f2dfb7b57a11ce\nFiles changed: 8\nAdditions: +665, Deletions: -200\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex b20b85a43b6e..24150aba6584 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,7 +88,8 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info', '_is_high', '_final_typechange_env']\n+               '_effects', '_debug_info', '_is_high',\n+               '_initial_typechange_env', '_final_typechange_env']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -97,6 +98,7 @@ class Jaxpr:\n   _effects: Effects\n   _debug_info: DebugInfo\n   _is_high: bool\n+  _initial_typechange_env: dict[Var, Any]\n   _final_typechange_env: dict[Var, Any]\n \n   @property\n@@ -127,6 +129,10 @@ def debug_info(self) -> DebugInfo:\n   def is_high(self) -> bool:\n     return self._is_high\n \n+  @property\n+  def initial_typechange_env(self) -> dict[Var, Any]:\n+    return self._initial_typechange_env\n+\n   @property\n   def final_typechange_env(self) -> dict[Var, Any]:\n     return self._final_typechange_env\n@@ -139,6 +145,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n                is_high: bool = False,\n+               initial_typechange_env: dict | None = None,\n                final_typechange_env: dict | None = None,\n                ):\n     \"\"\"\n@@ -165,6 +172,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n     self._is_high = is_high\n+    self._initial_typechange_env = initial_typechange_env or {}\n     self._final_typechange_env = final_typechange_env or {}\n \n   def __str__(self):\n@@ -193,6 +201,8 @@ def replace(self, **kwargs):\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n         is_high=kwargs.pop(\"is_high\", self.is_high),\n+        initial_typechange_env=kwargs.pop(\"initial_typechange_env\",\n+                                          self.initial_typechange_env),\n         final_typechange_env=kwargs.pop(\"final_typechange_env\",\n                                         self.final_typechange_env),\n     )\n@@ -222,6 +232,22 @@ def subjaxprs(jaxpr: Jaxpr) -> Iterator[Jaxpr]:\n     yield from jaxprs_in_params(eqn.params)\n \n \n+@dataclass(frozen=True)\n+class TypeChange:\n+  aval: AbstractValue\n+  initial_type_state: Any\n+  final_type_state: Any\n+\n+  def to_tangent_aval(self):\n+    return TypeChange(self.aval.to_tangent_aval(),\n+                      self.initial_type_state.to_tangent_aval(),\n+                      self.final_type_state.to_tangent_aval())\n+\n+  def normalize(self):\n+    return TypeChange(self.aval.normalize(),\n+                      self.initial_type_state.normalize(),\n+                      self.final_type_state.normalize())\n+\n class ClosedJaxpr:\n   __slots__ = ['__weakref__', '_jaxpr', '_consts']\n \n@@ -241,6 +267,13 @@ def __init__(self, jaxpr: Jaxpr, consts: Sequence):\n   def in_avals(self):\n     return [v.aval for v in self.jaxpr.invars]\n \n+  @property\n+  def in_avals_aug(self):\n+    ienv = self.jaxpr.initial_typechange_env\n+    fenv = self.jaxpr.final_typechange_env\n+    return [TypeChange(v.aval, ienv[v], fenv[v]) if v.aval.mutable else v.aval\n+            for v in self.jaxpr.invars]\n+\n   @property\n   def out_avals(self):\n     return [v.aval for v in self.jaxpr.outvars]\n@@ -542,10 +575,6 @@ def _true_bind(self, *args, **params):\n     # is called frequently and it's slightly faster to avoid using a context\n     # manager object.\n     prev_trace = trace_ctx.trace\n-\n-    if self.is_high(**params) and prev_trace.requires_low:\n-      return self.to_lojax(*args, **params)  # type: ignore\n-\n     trace_ctx.set_trace(eval_trace)\n     try:\n       return self.bind_with_trace(prev_trace, args, params)\n@@ -553,6 +582,11 @@ def _true_bind(self, *args, **params):\n       trace_ctx.set_trace(prev_trace)\n \n   def bind_with_trace(self, trace, args, params):\n+    # TODO(mattjj,dougalm): remove this block?\n+    if self.is_high(**params) and trace.requires_low:\n+      with set_current_trace(trace):\n+        return self.to_lojax(*args, **params)  # type: ignore\n+\n     return trace.process_primitive(self, args, params)\n \n   def def_impl(self, impl):\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 7cbdfff01462..0cd99a197f66 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -641,6 +641,9 @@ def to_concrete_value(self):\n   def get_referent(self):\n     return core.get_referent(self.primal)\n \n+  def type_state(self):\n+    return self.primal.type_state()\n+\n def _primal_tangent_shapes_match(primal, tangent):\n   if type(tangent) is not Zero:\n     primal_aval = get_aval(primal).strip_weak_type()\n@@ -1166,8 +1169,9 @@ def _jvp_jaxpr(jaxpr: core.ClosedJaxpr,\n                    debug_info=jaxpr.jaxpr.debug_info)\n   f_jvp, out_nonzeros = f_jvp_traceable(\n       jvp(f, instantiate=instantiate, transform_stack=False), nonzeros)\n-  tangent_avals = [aval.to_tangent_aval() for aval, nz in zip(jaxpr.in_avals, nonzeros) if nz]\n-  avals_in = list(it.chain(jaxpr.in_avals, tangent_avals))\n+  tangent_avals = [aval.to_tangent_aval()\n+                   for aval, nz in zip(jaxpr.in_avals_aug, nonzeros) if nz]\n+  avals_in = list(it.chain(jaxpr.in_avals_aug, tangent_avals))\n   jaxpr_out, avals_out, literals_out, () = pe.trace_to_jaxpr_dynamic(\n       f_jvp, avals_in)\n   return core.ClosedJaxpr(jaxpr_out, literals_out), out_nonzeros()\n@@ -1189,14 +1193,12 @@ def rearrange_binders(jaxpr: core.ClosedJaxpr, primals_in, tangents_in, primals_\n   new_invars = _perm(primals_in, tangents_in, jaxpr.jaxpr.invars)\n   new_outvars = _perm(primals_out, tangents_out, jaxpr.jaxpr.outvars)\n   new_debug_info = jaxpr.jaxpr.debug_info\n-  new_arg_names = tuple(_perm(primals_in, tangents_in,\n-                              jaxpr.jaxpr.debug_info.safe_arg_names(len(jaxpr.jaxpr.invars))))\n-  new_result_paths = tuple(_perm(primals_out, tangents_out,\n-                                  jaxpr.jaxpr.debug_info.safe_result_paths(len(jaxpr.jaxpr.outvars))))\n+  arg_names = jaxpr.jaxpr.debug_info.safe_arg_names(len(jaxpr.in_avals))\n+  result_paths = jaxpr.jaxpr.debug_info.safe_result_paths(len(jaxpr.out_avals))\n+  new_arg_names = tuple(_perm(primals_in, tangents_in, arg_names))\n+  new_result_paths = tuple(_perm(primals_out, tangents_out, result_paths))\n   new_debug_info = new_debug_info._replace(\n-      arg_names=new_arg_names,\n-      result_paths=new_result_paths,\n-  )\n+      arg_names=new_arg_names, result_paths=new_result_paths)\n   constvars = jaxpr.jaxpr.constvars\n   new_effects = pe._renumber_effects(\n       (*constvars, *new_invars), (*constvars, *jaxpr.jaxpr.invars),\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 3c499429a663..444b60f15fa5 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -896,10 +896,8 @@ def convert_constvars_jaxpr(jaxpr: Jaxpr) -> Jaxpr:\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   dbg = jaxpr.debug_info._replace(\n       arg_names=(\"\",) * len(jaxpr.constvars) + jaxpr.debug_info.arg_names)\n-  lifted_jaxpr = Jaxpr(constvars=(),\n-                       invars=jaxpr.constvars + jaxpr.invars,\n-                       outvars=jaxpr.outvars, eqns=jaxpr.eqns,\n-                       effects=jaxpr.effects, debug_info=dbg)\n+  lifted_jaxpr = jaxpr.replace(\n+      constvars=(), invars=jaxpr.constvars + jaxpr.invars, debug_info=dbg)\n   config.enable_checks.value and core.check_jaxpr(lifted_jaxpr)\n   return lifted_jaxpr\n \n@@ -1014,10 +1012,9 @@ def fun(*known_vals_in):\n     known_vals_out = [pval.get_known() for pval in out_pvals if pval.is_known()]\n     return [*known_vals_out, *residuals]\n \n-  known_avals = [a for a, uk in zip(jaxpr.in_avals, in_unknowns) if not uk]\n+  known_avals = [a for a, uk in zip(jaxpr.in_avals_aug, in_unknowns) if not uk]\n   jaxpr_known, _, consts_known, () = trace_to_jaxpr_dynamic(\n-      lu.wrap_init(fun, debug_info=f.debug_info),\n-      known_avals)\n+      lu.wrap_init(fun, debug_info=f.debug_info), known_avals)\n   (out_unknowns, jaxpr_unknown, res_avals), = cell  # pytype: disable=bad-unpacking\n \n   # check jaxpr_known and jaxpr_unknown in isolation\n@@ -1579,6 +1576,20 @@ def dce_jaxpr_closed_call_rule(used_outputs: list[bool], eqn: JaxprEqn\n def close_jaxpr(jaxpr: Jaxpr) -> ClosedJaxpr:\n   return ClosedJaxpr(jaxpr, ())\n \n+def move_invars_right(jaxpr: ClosedJaxpr, to_move: Sequence[bool]):\n+  return _move_invars_right(jaxpr, tuple(to_move))\n+\n+@weakref_lru_cache\n+def _move_invars_right(jaxpr: ClosedJaxpr, to_move: tuple[bool, ...]):\n+  invars, rest = split_list(jaxpr.jaxpr.invars, [len(to_move)])\n+  left_invars, right_invars = partition_list(to_move, invars)\n+  new_invars = [*left_invars, *right_invars, *rest]\n+  new_effs = _renumber_effects(\n+      (*jaxpr.jaxpr.constvars, *new_invars),\n+      (*jaxpr.jaxpr.constvars, *jaxpr.jaxpr.invars),\n+      jaxpr.jaxpr.effects)\n+  return jaxpr.replace(jaxpr=jaxpr.jaxpr.replace(invars=new_invars, effects=new_effs))\n+\n def move_binders_to_front(closed_jaxpr: ClosedJaxpr, to_move: Sequence[bool]\n                           ) -> ClosedJaxpr:\n   \"\"\"Reorder `invars` by moving those indicated in `to_move` to the front.\"\"\"\n@@ -1640,6 +1651,10 @@ def full_lower(self):\n     if val is None: return self\n     return core.full_lower(val)\n \n+  def type_state(self):\n+    var = self._trace.frame.tracer_to_var.get(id(self))\n+    return self._trace.frame.current_typechange_env[var]\n+\n   def _contents(self):\n     return ()\n \n@@ -1735,7 +1750,8 @@ class JaxprStackFrame:\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n   is_high: bool\n-  final_typechange_env: dict\n+  initial_typechange_env: dict\n+  current_typechange_env: dict\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1751,7 +1767,8 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_vars = []\n     self.debug_info = debug_info\n     self.is_high = False\n-    self.final_typechange_env = {}\n+    self.initial_typechange_env = {}\n+    self.current_typechange_env = {}\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1777,8 +1794,11 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n+    final_typechange_env = {v: s for v, s in self.current_typechange_env.items()\n+                            if v in self.initial_typechange_env}\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info, self.is_high, self.final_typechange_env)\n+                  debug_info, self.is_high, self.initial_typechange_env,\n+                  final_typechange_env)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1895,8 +1915,6 @@ def new_arg(self, aval, source_info: SourceInfo):\n     self.frame.tracers.append(tracer)\n     self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n     self.frame.invars.append(var)\n-    if aval.mutable:\n-      self.frame.final_typechange_env[var] = aval\n     return tracer\n \n   def new_const(self, c, source_info: SourceInfo):\n@@ -1921,6 +1939,8 @@ def _new_const(self, aval, c, source_info: SourceInfo) -> DynamicJaxprTracer:\n       self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n       self.frame.constid_to_tracer[id(c)] = tracer\n       self.frame.constvar_to_val[var] = c\n+      if aval.mutable:\n+        self.frame.initial_typechange_env[var] = c.type_state()\n     return tracer\n \n   def get_const(self, tracer) -> Any:\n@@ -2235,18 +2255,24 @@ def trace_to_jaxpr_dynamic(\n            list[tuple[PyTreeDef, PyTreeDef, tuple[Any, str, AttrKind]]]]:\n   keep_inputs = [True] * len(in_avals) if keep_inputs is None else keep_inputs\n   trace = DynamicJaxprTrace(fun.debug_info, lower=lower)\n+  in_avals_ = [a.aval if isinstance(a, core.TypeChange) else a for a in in_avals]\n   with core.ensure_no_leaks(trace), source_info_util.reset_name_stack():\n     source_info = source_info_util.current()\n     in_tracers = _input_type_to_tracers(\n-        partial(trace.new_arg, source_info=source_info), in_avals)\n+        partial(trace.new_arg, source_info=source_info), in_avals_)\n     in_tracers = [t for t, keep in zip(in_tracers, keep_inputs) if keep]\n+    trace.frame.initial_typechange_env = initial_typechange_env = {\n+        v: a.initial_type_state for v, a in zip(trace.frame.invars, in_avals)\n+        if isinstance(a, core.TypeChange)}\n+    trace.frame.current_typechange_env = dict(initial_typechange_env)\n+\n     try:\n       with core.set_current_trace(trace):\n         ans = fun.call_wrapped(*in_tracers)\n       _check_returned_jaxtypes(fun.debug_info, ans)\n       out_tracers = map(partial(trace.to_jaxpr_tracer, source_info=source_info), ans)\n       _check_no_returned_refs(fun.debug_info, out_tracers)\n-      jaxpr, consts, attrs_tracked = trace.to_jaxpr(out_tracers, fun.debug_info)\n+      jaxpr, consts, attrs_tracked = trace.frame.to_jaxpr(trace, out_tracers, fun.debug_info)\n       del fun, in_tracers, out_tracers, ans\n     finally:\n       trace.frame.reset_states(trace)\n@@ -2718,21 +2744,38 @@ def _linearize_of_pmap_hack(f: lu.WrappedFun, jaxpr, consts) -> tuple[Jaxpr, lis\n \n @weakref_lru_cache\n def lower_jaxpr(hi_jaxpr):\n-  in_avals = [lo_ty for t in hi_jaxpr.in_avals for lo_ty in t.lo_ty()]\n+  initial_env = hi_jaxpr.jaxpr.initial_typechange_env\n+  lo_avals = [lo_ty for v in hi_jaxpr.jaxpr.invars\n+              for lo_ty in (v.aval.lo_ty_(initial_env[v]) if v.aval.mutable\n+                            else v.aval.lo_ty())]\n   f = lu.wrap_init(partial(lower_traceable, hi_jaxpr),\n                    debug_info=hi_jaxpr.jaxpr.debug_info)\n-  lo_jaxpr, _, consts, () = trace_to_jaxpr_dynamic(f, in_avals, lower=True)\n-  return core.ClosedJaxpr(lo_jaxpr, consts)\n+  lo_jaxpr, _, lo_consts, () = trace_to_jaxpr_dynamic(f, lo_avals, lower=True)\n+  return core.ClosedJaxpr(lo_jaxpr, lo_consts)\n \n def lower_traceable(jaxpr, *lo_args):\n+  env = jaxpr.jaxpr.initial_typechange_env\n   lo_args_ = iter(lo_args)\n-  hi_args = [t.raise_val(*it.islice(lo_args_, len(t.lo_ty())))\n-             for t in jaxpr.in_avals]\n+  hi_args = [v.aval.raise_val(*it.islice(lo_args_, len(v.aval.lo_ty())))\n+             if not v.aval.mutable else\n+             v.aval.new_from_loval(env[v], *it.islice(lo_args_, len(v.aval.lo_ty_(env[v]))))\n+             for v in jaxpr.jaxpr.invars]\n   assert (problem := next(lo_args_, None)) is None\n   hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n   mut_outs = [lo_val for v, ty in jaxpr.jaxpr.final_typechange_env.items()\n-              for lo_val in ty.get(hi_args[in_idx[v]])]\n-  lo_outs = [lo_val for t, hi_val in zip(jaxpr.out_avals, hi_outs)\n-             for lo_val in t.lower_val(hi_val)]\n+              for lo_val in v.aval.read_loval(ty, hi_args[in_idx[v]])]\n+  lo_outs = [lo_val for v, hi_val in zip(jaxpr.jaxpr.outvars, hi_outs)\n+             for lo_val in v.aval.lower_val(hi_val)]\n   return mut_outs + lo_outs\n+\n+def convert_const_himutables(jaxpr):\n+  move = [core.typeof(c).mutable for c in jaxpr.consts]\n+  constvals, in_mutables = partition_list(move, jaxpr.consts)\n+  constvars, boxvars = partition_list(move, jaxpr.jaxpr.constvars)\n+  invars = *boxvars, *jaxpr.jaxpr.invars\n+  effects = make_jaxpr_effects(constvars, invars, jaxpr.jaxpr.outvars,\n+                               jaxpr.jaxpr.eqns)\n+  new_jaxpr = jaxpr.jaxpr.replace(constvars=constvars, invars=invars,\n+                                  effects=effects)\n+  return jaxpr.replace(jaxpr=new_jaxpr, consts=constvals), in_mutables\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex 8ebf4133a5fd..af1f8217951c 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -1775,6 +1775,7 @@ def _move_mutable_consts(\n   constvars, mutvars = partition_list(hoist, jaxpr.constvars)\n   invars = (*jaxpr.invars, *mutvars)\n   effects = pe.make_jaxpr_effects(constvars, invars, jaxpr.outvars, jaxpr.eqns)\n+  # TODO(mattjj): debug_info must be updated...\n   jaxpr = core.Jaxpr(constvars, invars, jaxpr.outvars, jaxpr.eqns,\n                      effects, closed_jaxpr.jaxpr.debug_info)\n   return core.ClosedJaxpr(jaxpr, consts), in_mut\n@@ -2181,8 +2182,7 @@ def lower_sharding_computation(\n   The caller of this code can pass in a singleton UNSPECIFIED because the\n   number of out_avals might not be known at that time and\n   lower_sharding_computation calculates the number of out_avals so it can apply\n-  the singleton UNSPECIFIED to all out_avals.\n-  \"\"\"\n+  the singleton UNSPECIFIED to all out_avals.\"\"\"\n   auto_spmd_lowering = check_if_any_auto(\n       it.chain.from_iterable([in_shardings, out_shardings]))\n \ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex b9ce8ae09380..4df4c517090b 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -17,7 +17,7 @@\n from collections.abc import Callable, Sequence\n from functools import partial\n import inspect\n-import itertools\n+import itertools as it\n import operator\n from typing import Any, TypeVar\n import weakref\n@@ -438,11 +438,11 @@ def _merge_attrs_out(attrs_tracked, out_state, out_append):\n   out_attrs = []\n   for _, out_tree, (_, _, k) in attrs_tracked:\n     if k in (pe.ReadWrite, pe.BoxAttr):\n-      out_attrs.extend(itertools.islice(out_state_, out_tree.num_leaves))\n+      out_attrs.extend(it.islice(out_state_, out_tree.num_leaves))\n     elif k is pe.Append:\n       out_attrs.append(next(out_append_))\n     elif k is pe.ListAttr:\n-      out_attrs.extend(itertools.islice(out_append_, out_tree.num_leaves))\n+      out_attrs.extend(it.islice(out_append_, out_tree.num_leaves))\n     else:\n       assert False\n   assert next(out_state_, None) is next(out_append_, None) is None\n@@ -931,7 +931,7 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n   ys_avals = [core.unmapped_aval(length, 0, y_aval)\n               for y_aval in y_avals]\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(a), None)\n-                 for a in itertools.chain(carry_avals, ys_avals)]\n+                 for a in it.chain(carry_avals, ys_avals)]\n   del carry_avals, y_avals\n   # Create equation.\n   linear_unknown = tuple([False] * len(intensive_res) +\n@@ -1500,6 +1500,17 @@ def arrange_jaxpr_args_for_wrapped(args):\n   assert len(refs_out_matching_in_avals) == len(in_avals)\n   return refs_out_matching_in_avals, [*carry_out, *ys]\n \n+def _scan_staging(trace, *args, **params):\n+  outs = trace.default_process_primitive(scan_p, args, params)\n+  jaxpr = params['jaxpr']\n+  trace.frame.is_high = jaxpr.jaxpr.is_high\n+  invars = [trace.frame.tracer_to_var[id(t)] for t in args]\n+  var_map = dict(zip(jaxpr.jaxpr.invars, invars))\n+  final_env = {var_map[v]: ty for v, ty in\n+               jaxpr.jaxpr.final_typechange_env.items()}\n+  trace.frame.current_typechange_env.update(final_env)\n+  return outs\n+\n scan_p = core.Primitive(\"scan\")\n scan_p.multiple_results = True\n scan_p.skip_canonicalization = True\n@@ -1518,6 +1529,65 @@ def arrange_jaxpr_args_for_wrapped(args):\n pe.padding_rules[scan_p] = _scan_padding_rule\n pe.dce_rules[scan_p] = _scan_dce_rule\n state_discharge.register_partial_discharge_rule(scan_p)(_scan_state_partial_discharge_rule)\n+pe.custom_staging_rules[scan_p] = _scan_staging\n+\n+def _is_high(jaxpr, **_) -> bool:\n+  return jaxpr.jaxpr.is_high\n+scan_p.is_high = _is_high  # type: ignore\n+\n+def _to_lojax(*hi_args, jaxpr, num_carry, num_consts, linear, **params):\n+  ienv, fenv = jaxpr.jaxpr.initial_typechange_env, jaxpr.jaxpr.final_typechange_env\n+\n+  # move box binders and hi_args from consts slots to carry slots\n+  to_move = [t.mutable for t in jaxpr.in_avals[:num_consts]]\n+  jaxpr = pe.move_invars_right(jaxpr, to_move)\n+  hi_args = _move_right(hi_args, to_move)\n+  num_consts -= sum(to_move)\n+  num_carry += sum(to_move)\n+\n+  # expand num_consts, num_carry, linear according to lo types\n+  const_invars, carry_invars, _ = split_list(jaxpr.jaxpr.invars, [num_consts, num_carry])\n+  num_consts = sum(len(v.aval.lo_ty() if not v.aval.mutable\n+                       else v.aval.lo_ty_(ienv[v])) for v in const_invars)\n+  num_carry = sum(len(v.aval.lo_ty() if not v.aval.mutable\n+                      else v.aval.lo_ty_(ienv[v])) for v in carry_invars)\n+  linear = [l for v, l_ in zip(jaxpr.jaxpr.invars, linear)\n+            for l in (l_,) * len(v.aval.lo_ty() if not v.aval.mutable\n+                                 else v.aval.lo_ty_(ienv[v]))]\n+  lo_muts_out = sum(len(m.leaf_avals) for m in fenv.values())  # TODO hardcoded\n+\n+  # collect lo inputs values\n+  lo_args = [lo_val for v, x in zip(jaxpr.jaxpr.invars, hi_args)\n+             for lo_val in (v.aval.read_loval(ienv[v], x) if v.aval.mutable\n+                            else v.aval.lower_val(x))]\n+\n+  # lower the jaxpr and bind it using lo input values\n+  lo_jaxpr = pe.lower_jaxpr(jaxpr)\n+  all_outs = scan_p.bind(*lo_args, jaxpr=lo_jaxpr, num_consts=num_consts,\n+                         num_carry=num_carry, linear=tuple(linear), **params)\n+  out_mut, lo_outs = split_list(all_outs, [lo_muts_out])\n+\n+  # collect and apply mutations\n+  out_mut_ = iter(out_mut)\n+  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n+  for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n+    lo_vals = it.islice(out_mut_, len(var.aval.lo_ty_(ty)))\n+    var.aval.update_from_loval(ty, hi_args[in_idx[var]], *lo_vals)\n+  assert next(out_mut_, None) is None\n+\n+  # collect output values into hi types\n+  lo_outs_ = iter(lo_outs)\n+  hi_outs = [t.raise_val(*it.islice(lo_outs_, len(t.lo_ty())))\n+             for t in jaxpr.out_avals]\n+  assert next(lo_outs_, None) is None\n+\n+  return hi_outs\n+scan_p.to_lojax = _to_lojax\n+\n+def _move_right(lst, to_move):\n+  lst, rest = split_list(lst, [len(to_move)])\n+  left, right = partition_list(to_move, lst)\n+  return [*left, *right, *rest]\n \n def _propagate_mem_kind_scan(*xm, reverse, length, num_consts, num_carry, jaxpr,\n                              linear, unroll, _split_transpose):\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 94a754a1a597..f012459296a1 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -590,6 +590,8 @@ def _infer_params_impl(\n     in_type = in_avals = tuple(core.shaped_abstractify(x) for x in explicit_args)  # type: ignore\n   else:\n     in_type = in_avals  # type: ignore\n+    in_type = tuple(core.TypeChange(a, x.type_state(), None) if a.mutable  # type: ignore\n+                    else a for a, x in zip(in_type, explicit_args))\n   assert in_avals is not None\n \n   in_shardings_flat, in_layouts_flat = _process_in_axis_resources(\n@@ -705,7 +707,7 @@ def _infer_params_internal(\n   if entry.pjit_params is None:\n     p, args_flat = _infer_params_impl(\n         fun, ji, ctx_mesh, dbg, args, kwargs, in_avals=avals)\n-    if p.attrs_tracked or p.box_data:  # if attrs/boxes, don't populate cache\n+    if p.attrs_tracked or p.box_data or p.params['jaxpr'].jaxpr.is_high:\n       return p, p.consts + args_flat\n     entry.pjit_params = p\n   return entry.pjit_params, entry.pjit_params.consts + dynargs\n@@ -1407,16 +1409,14 @@ def _create_pjit_jaxpr(\n           lu.annotate(fun, cast(core.InputType, in_type)))\n       attrs_tracked = []\n     else:\n-      jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n-          fun, in_type)\n-      # assert attr_data is sentinel or attr_data matches attrs_tracked\n+      jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(fun, in_type)\n \n   if config.debug_key_reuse.value:\n     # Import here to avoid circular imports\n     from jax.experimental.key_reuse._core import check_key_reuse_jaxpr\n     check_key_reuse_jaxpr(jaxpr)\n \n-  if any(isinstance(c, core.Tracer) for c in consts):\n+  if any(isinstance(c, core.Tracer) or core.typeof(c).mutable for c in consts):\n     closed_jaxpr = pe.close_jaxpr(pe.convert_constvars_jaxpr(jaxpr))\n     final_consts = consts\n   else:\n@@ -1561,21 +1561,41 @@ def _is_high(jaxpr, **_) -> bool:\n   return jaxpr.jaxpr.is_high\n pjit_p.is_high = _is_high  # type: ignore\n \n-def _to_lojax( *hi_args, jaxpr, **params):\n-  params, num_mutants = _lojax_expand_params(jaxpr, **params)\n+def _to_lojax(*hi_args, jaxpr, **params):\n+  ienv, fenv = jaxpr.jaxpr.initial_typechange_env, jaxpr.jaxpr.final_typechange_env\n \n-  lo_args = [lo_val for t, hi_val in zip(jaxpr.in_avals, hi_args)\n-             for lo_val in t.lower_val(hi_val)]\n+  # convert closed-over boxes to explicit args\n+  jaxpr, closed_over_himutables = pe.convert_const_himutables(jaxpr)\n+  hi_args = [*closed_over_himutables, *hi_args]\n+  params = _converted_mutables_add_params(len(closed_over_himutables), **params)\n+\n+  # expand pjit params that must match number of lo inputs/outputs\n+  lo_nums_in = [len(v.aval.lo_ty() if not v.aval.mutable\n+                    else v.aval.lo_ty_(ienv[v]))\n+                for v in jaxpr.jaxpr.invars]\n+  lo_nums_out = [len(t.lo_ty()) for t in jaxpr.out_avals]\n+  lo_muts_out = sum(len(m.leaf_avals) for m in fenv.values())  # TODO hardcoded\n+  params = _lojax_expand_params(lo_nums_in, lo_nums_out, lo_muts_out, **params)\n+\n+  # collect lo input values\n+  lo_args = [lo_val for v, x in zip(jaxpr.jaxpr.invars, hi_args)\n+             for lo_val in (v.aval.read_loval(ienv[v], x) if v.aval.mutable\n+                            else v.aval.lower_val(x))]\n+\n+  # lower the jaxpr and bind it using lo input values\n   lo_jaxpr = pe.lower_jaxpr(jaxpr)\n   all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, **params)\n-  out_mut, lo_outs = split_list(all_outs, [num_mutants])\n+  out_mut, lo_outs = split_list(all_outs, [lo_muts_out])\n \n+  # collect and apply mutations\n   out_mut_ = iter(out_mut)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n   for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n-    ty.set(hi_args[in_idx[var]], *it.islice(out_mut_, len(ty.lo_ty())))\n+    lo_vals = it.islice(out_mut_, len(var.aval.lo_ty_(ty)))\n+    var.aval.update_from_loval(ty, hi_args[in_idx[var]], *lo_vals)\n   assert next(out_mut_, None) is None\n \n+  # collect output values into hi types\n   lo_outs_ = iter(lo_outs)\n   hi_outs = [t.raise_val(*it.islice(lo_outs_, len(t.lo_ty())))\n              for t in jaxpr.out_avals]\n@@ -1584,29 +1604,35 @@ def _to_lojax( *hi_args, jaxpr, **params):\n   return hi_outs\n pjit_p.to_lojax = _to_lojax\n \n+def _converted_mutables_add_params(\n+    n, *, donated_invars, in_shardings, in_layouts, **params):\n+  donated_invars = (False,) * n + donated_invars\n+  in_shardings = (UNSPECIFIED,) * n + in_shardings\n+  in_layouts = (None,) * n + in_layouts\n+  return dict(params, donated_invars=donated_invars, in_shardings=in_shardings,\n+              in_layouts=in_layouts)\n+\n def _lojax_expand_params(\n-    hi_jaxpr, *, donated_invars, in_shardings, in_layouts, out_shardings,\n-    out_layouts, **params):\n+    nums_in, nums_out, muts_out, *, donated_invars, in_shardings, in_layouts,\n+    out_shardings, out_layouts, **params):\n   # some pjit params match the length of hi_jaxpr.invars/outvars, so when\n   # lowering we must expand them to match their number of lojax types\n-  def expand(hi_tys, xs):\n-    return tuple(y for hi, x in zip(hi_tys, xs) for y in (x,) * len(hi.lo_ty()))\n-  donated_invars = expand(hi_jaxpr.in_avals , donated_invars)\n-  in_shardings   = expand(hi_jaxpr.in_avals , in_shardings  )\n-  in_layouts     = expand(hi_jaxpr.in_avals , in_layouts    )\n-  out_shardings  = expand(hi_jaxpr.out_avals, out_shardings )\n-  out_layouts    = expand(hi_jaxpr.out_avals, out_layouts   )\n+  def expand(ns, xs):\n+    return tuple(y for n, x in zip(ns, xs) for y in (x,) * n)\n+  donated_invars = expand(nums_in , donated_invars)\n+  in_shardings   = expand(nums_in , in_shardings  )\n+  in_layouts     = expand(nums_in , in_layouts    )\n+  out_shardings  = expand(nums_out, out_shardings )\n+  out_layouts    = expand(nums_out, out_layouts   )\n \n   # also, the lo_jaxpr has pure outputs corresponding to mutable hi_jaxpr types\n-  num_mutants = sum(len(hi_ty.lo_ty()) for hi_ty in\n-                    hi_jaxpr.jaxpr.final_typechange_env.values())\n-  out_shardings = (UNSPECIFIED,) * num_mutants + out_shardings\n-  out_layouts = (None,) * num_mutants + out_layouts\n+  out_shardings = (UNSPECIFIED,) * muts_out + out_shardings\n+  out_layouts = (None,) * muts_out + out_layouts\n \n   new_params = dict(params, donated_invars=donated_invars,\n                     in_shardings=in_shardings, in_layouts=in_layouts,\n                     out_shardings=out_shardings, out_layouts=out_layouts)\n-  return new_params, num_mutants\n+  return new_params\n \n \n def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n@@ -1948,6 +1974,7 @@ def pjit_staging_rule(trace, *args, **params):\n \n   jaxpr = params['jaxpr']\n   source_info = source_info_util.current()\n+  consts = []\n   if config.dynamic_shapes.value:\n     jaxpr, in_fwd, out_shardings, out_layouts = _pjit_forwarding(\n         jaxpr, params['out_shardings'], params['out_layouts'])\n@@ -1981,6 +2008,14 @@ def pjit_staging_rule(trace, *args, **params):\n         pjit_p, (*args, *consts), new_params)\n   else:\n     out_tracers = trace.default_process_primitive(pjit_p, args, params)\n+\n+  trace.frame.is_high = jaxpr.jaxpr.is_high\n+  invars = [trace.frame.tracer_to_var[id(t)] for t in it.chain(args, consts)]\n+  var_map = dict(zip(jaxpr.jaxpr.invars, invars))\n+  final_env = {var_map[v]: ty for v, ty in\n+               jaxpr.jaxpr.final_typechange_env.items()}\n+  trace.frame.current_typechange_env.update(final_env)\n+\n   return out_tracers\n pe.custom_staging_rules[pjit_p] = pjit_staging_rule\n \ndiff --git a/tests/attrs_test.py b/tests/attrs_test.py\nindex 60a3753a7ba5..90083626fb8e 100644\n--- a/tests/attrs_test.py\n+++ b/tests/attrs_test.py\n@@ -1056,7 +1056,7 @@ def f(x):\n     self.assertAllClose(box.get(), 2.0)\n \n   @parameterized.parameters([False, True])\n-  def test_grad_closrue_stop_gradient(self, jit):\n+  def test_grad_closure_stop_gradient(self, jit):\n     box = Box(0.0)\n \n     def f(x):\n@@ -1124,7 +1124,6 @@ def f(lst, x):\n       lst.append(2.0)\n       lst.append({'c': x + 3.0})\n \n-\n     tracing_ok = True\n     lst1 = List()\n     f(lst1, 0)\ndiff --git a/tests/hijax_test.py b/tests/hijax_test.py\nindex 21034d164d28..8b7d045c6c5a 100644\n--- a/tests/hijax_test.py\n+++ b/tests/hijax_test.py\n@@ -17,18 +17,19 @@\n from dataclasses import dataclass\n from functools import partial\n import itertools as it\n+from typing import Any\n import unittest\n \n-from absl.testing import absltest\n+from absl.testing import absltest, parameterized\n \n import jax\n import jax.numpy as jnp\n \n from jax._src import config\n from jax._src import core\n-from jax._src import dtypes\n from jax._src.interpreters import ad\n from jax._src.interpreters import partial_eval as pe\n+from jax._src import ad_util\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip, safe_map\n \n@@ -37,6 +38,8 @@\n map, unsafe_map = safe_map, map\n zip, unsafe_zip = safe_zip, zip\n \n+PyTreeDef = Any\n+\n \n # TODO(mattjj,dougalm): move HiPrimitive, Box, etc out of tests and into library\n class HiPrimitive(core.Primitive):\n@@ -65,124 +68,6 @@ def jvp(self, primals, tangents, **params):\n   def transpose(self, *args, **params):\n     assert False  # TODO\n \n-\n-class BoxTy(core.AbstractValue):\n-  mutable = True\n-\n-  def __init__(self, leaf_avals, treedef):\n-    self._leaf_avals = leaf_avals  # hijax avals\n-    self._treedef = treedef\n-\n-  # aval interface: hashability and str_short\n-  def __hash__(self):\n-    return hash((self._leaf_avals, self._treedef))\n-\n-  def __eq__(self, other):\n-    return (isinstance(other, BoxTy) and self._leaf_avals == other._leaf_avals\n-            and self._treedef == other._treedef)\n-\n-  def str_short(self, short_dtypes=False):\n-    return 'BoxTy'\n-\n-  # hijax interface: lower val, raise val, and low type\n-  def lo_ty(self):\n-    return [lo_aval for hi_aval in self._leaf_avals for lo_aval in hi_aval.lo_ty()]\n-\n-  def lower_val(self, box):\n-    leaf_vals, treedef = jax.tree.flatten(box._val)\n-    assert treedef == self._treedef\n-    return [lo_val for hi_aval, hi_val in zip(self._leaf_avals, leaf_vals)\n-            for lo_val in hi_aval.lower_val(hi_val)]\n-\n-  def raise_val(self, *lo_vals):\n-    lo_vals_ = iter(lo_vals)\n-    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n-               for hi_ty in self._leaf_avals]\n-    assert next(lo_vals_, None) is None\n-    return Box(jax.tree.unflatten(self._treedef, hi_vals))  # will be mutated\n-\n-  # mutable interface: get/set\n-  def get(self, box):\n-    leaf_vals, treedef = jax.tree.flatten(box._val)\n-    assert treedef == self._treedef\n-    return [lo_val for hi_ty, hi_val in zip(self._leaf_avals, leaf_vals)\n-            for lo_val in hi_ty.lower_val(hi_val)]\n-\n-  def set(self, box, *lo_vals):\n-    lo_vals_ = iter(lo_vals)\n-    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n-               for hi_ty in self._leaf_avals]\n-    assert next(lo_vals_, None) is None\n-    box._val = jax.tree.unflatten(self._treedef, hi_vals)\n-\n-  # TODO placeholder thing\n-  def to_tangent_aval(self):\n-    return core.ShapedArray((), dtypes.float0)  # TODO revise placeholder\n-\n-class Box:  # noqa: F811\n-  def __init__(self, val):\n-    self._val = val\n-\n-  @property\n-  def ty(self):\n-    leaves, treedef = jax.tree.flatten(self._val)\n-    leaf_avals = tuple(map(core.typeof, leaves))\n-    return BoxTy(leaf_avals, treedef)\n-core.pytype_aval_mappings[Box] = lambda b: b.ty\n-\n-\n-class BoxSet(HiPrimitive):\n-  multiple_results = True\n-\n-  def is_high(self, *, treedef) -> bool: return True\n-\n-  def staging(self, trace, box, *leaves, treedef):\n-    super().staging(trace, box, *leaves, treedef=treedef)\n-    avals = tuple(t.aval for t in leaves)\n-    trace.frame.final_typechange_env[trace.getvar(box)] = BoxTy(avals, treedef)\n-\n-  def abstract_eval(self, box_ty, *leaf_avals, treedef):\n-    return [], set()  # TODO better typechecking...\n-\n-  def to_lojax(_, box, *leaves, treedef):\n-    box._val = jax.tree.unflatten(treedef, leaves)\n-    return []\n-\n-  def jvp(_, primals, tangents, *, treedef):\n-    assert False  # TODO\n-\n-  def transpose(_, *args, treedef):\n-    assert False  # TODO\n-box_set_p = BoxSet('box_set')\n-\n-def box_set(box, val):\n-  leaves, treedef = jax.tree.flatten(val)\n-  box_set_p.bind(box, *leaves, treedef=treedef)\n-\n-\n-class BoxGet(HiPrimitive):\n-  multiple_results = True\n-\n-  def is_high(self) -> bool: return True\n-\n-  def abstract_eval(self, box_ty):\n-    return box_ty._leaf_avals, set()\n-\n-  def to_lojax(_, box):\n-    return jax.tree.leaves(box._val)\n-\n-  def jvp(_, primals, tangents):\n-    assert False  # TODO\n-\n-  def transpose(_, *args):\n-    assert False  # TODO\n-box_get_p = BoxGet('box_get')\n-\n-def box_get(box):\n-  leaf_vals = box_get_p.bind(box)\n-  return jax.tree.unflatten(core.typeof(box)._treedef, leaf_vals)\n-\n-\n class HijaxTest(jtu.JaxTestCase):\n \n   def test_custom_types_and_primitive(self):\n@@ -194,8 +79,6 @@ class MyArray:\n \n     @dataclass(frozen=True)\n     class MyTy(core.AbstractValue):\n-      mutable = False\n-\n       def to_tangent_aval(self):\n         return MyTy()\n       def str_short(self, short_dtypes=False):\n@@ -324,6 +207,392 @@ def f(x):\n     self.assertIsInstance(a_grad, MyArray)\n     self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n \n+\n+def new_box():\n+  (), treedef = jax.tree.flatten(None)\n+  return new_box_p.bind(treedef=treedef)\n+\n+def box_get(box):\n+  tys = box.type_state()\n+  leaf_vals = box_get_p.bind(box, avals=tys.leaf_avals)\n+  return jax.tree.unflatten(tys.treedef, leaf_vals)\n+\n+def box_set(box, val):\n+  leaves, treedef = jax.tree.flatten(val)\n+  box_set_p.bind(box, *leaves, treedef=treedef)\n+\n+@dataclass(frozen=True)\n+class BoxTypeState:\n+  leaf_avals: tuple[core.AbstractValue, ...]\n+  treedef: PyTreeDef\n+\n+  def to_tangent_aval(self):\n+    return BoxTypeState(tuple(a.to_tangent_aval() for a in self.leaf_avals),\n+                        self.treedef)\n+\n+  def normalize(self):\n+    return BoxTypeState(tuple(a.normalize() for a in self.leaf_avals),\n+                        self.treedef)\n+\n+class BoxTy(core.AbstractValue):\n+  mutable = True\n+\n+  # forwarded to value\n+  get = core.aval_method(box_get)\n+  set = core.aval_method(box_set)\n+\n+  # aval interface: hashability and str_short\n+  def __hash__(self): return hash(BoxTy)\n+  def __eq__(self, other): return isinstance(other, BoxTy)\n+\n+  def str_short(self, short_dtypes=False):\n+    return 'BoxTy'\n+\n+  # mutable interface\n+  def lo_ty_(self, box_state):\n+    return [lo_ty for t in box_state.leaf_avals for lo_ty in t.lo_ty()]\n+\n+  def new_from_loval(self, box_state: BoxTypeState, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in box_state.leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    return Box(jax.tree.unflatten(box_state.treedef, hi_vals))  # will be mutated\n+\n+  def read_loval(self, box_state: BoxTypeState, box):\n+    leaf_vals, treedef = jax.tree.flatten(box_get(box))\n+    assert treedef == box_state.treedef\n+    return [lo_val for hi_ty, hi_val in zip(box_state.leaf_avals, leaf_vals)\n+            for lo_val in hi_ty.lower_val(hi_val)]\n+\n+  def update_from_loval(self, box_state: BoxTypeState, box, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in box_state.leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    box_set(box, jax.tree.unflatten(box_state.treedef, hi_vals))\n+\n+  def to_tangent_aval(self):\n+    return BoxTy()\n+\n+class Box:  # noqa: F811\n+  def __init__(self, val):\n+    self._val = val\n+\n+  def get(self):\n+    return box_get(self)\n+\n+  def set(self, val):\n+    box_set(self, val)\n+\n+  @property\n+  def ty(self):\n+    return BoxTy()\n+\n+  def type_state(self):\n+    leaves, treedef = jax.tree.flatten(self._val)\n+    leaf_avals = tuple(map(core.typeof, leaves))\n+    return BoxTypeState(leaf_avals, treedef)\n+core.pytype_aval_mappings[Box] = lambda b: b.ty\n+\n+\n+class NewBox(HiPrimitive):\n+  def is_high(self, *, treedef) -> bool: return True\n+\n+  def staging(self, trace, *, treedef):\n+    tracer = super().staging(trace, treedef=treedef)\n+    var = trace.frame.tracer_to_var[id(tracer)]\n+    leaves, treedef = jax.tree.flatten(None)\n+    trace.frame.current_typechange_env[var] = BoxTypeState(leaves, treedef)\n+    return tracer\n+\n+  def abstract_eval(self, *, treedef):\n+    return BoxTy(), set()\n+\n+  def to_lojax(_, *, treedef):\n+    return Box(None)\n+\n+  def jvp(_, primals, tangents, *, treedef):\n+    assert False  # TODO\n+\n+  def transpose(_, *args, treedef):\n+    assert False  # TODO\n+new_box_p = NewBox('new_box')\n+\n+\n+class BoxSet(HiPrimitive):\n+  multiple_results = True\n+\n+  def is_high(self, *, treedef) -> bool: return True\n+\n+  def staging(self, trace, box_tracer, *leaves, treedef):\n+    super().staging(trace, box_tracer, *leaves, treedef=treedef)\n+    var = trace.getvar(box_tracer)\n+    avals = tuple(t.aval for t in leaves)\n+    trace.frame.current_typechange_env[var] = BoxTypeState(avals, treedef)\n+    return []\n+\n+  def abstract_eval(self, box_ty, *leaf_avals, treedef):\n+    return [], set()  # TODO better typechecking...\n+\n+  def to_lojax(_, box, *leaves, treedef):\n+    box._val = jax.tree.unflatten(treedef, leaves)\n+    return []\n+\n+  def jvp(_, primals, tangents, *, treedef):\n+    box, *vals = primals\n+    box_dot, *val_dots = tangents\n+    if type(box_dot) is ad_util.Zero:\n+      raise Exception(\"you're an idiot\")\n+    box_set_p.bind(box, *vals, treedef=treedef)\n+    box_set_p.bind(box_dot, *val_dots, treedef=treedef)\n+    return [], []\n+\n+  def transpose(_, *args, treedef):\n+    assert False  # TODO\n+box_set_p = BoxSet('box_set')\n+\n+\n+class BoxGet(HiPrimitive):\n+  multiple_results = True\n+\n+  def abstract_eval(self, box_ty, *, avals):\n+    return avals, set()\n+\n+  def to_lojax(_, box, *, avals):\n+    return jax.tree.leaves(box._val)\n+\n+  def jvp(_, primals, tangents, *, avals):\n+    (box,), (box_dot,) = primals, tangents\n+    return (box_get_p.bind(box, avals=avals),\n+            box_get_p.bind(box_dot, avals=[a.to_tangent_aval() for a in avals]))\n+\n+  def transpose(_, *args):\n+    assert False  # TODO\n+box_get_p = BoxGet('box_get')\n+\n+\n+\n+class BoxTest(jtu.JaxTestCase):\n+\n+  def test_jit_arg(self):\n+    @jax.jit\n+    def f(box, x):\n+      assert tracing_ok\n+      box.set(box.get() + x)\n+\n+    tracing_ok = True\n+    box1 = Box(1.0)\n+    f(box1, 1.)\n+    self.assertAllClose(box1.get(), 2.0)\n+\n+    tracing_ok = False\n+    box2 = Box(2.0)\n+    f(box2, 2.)\n+    self.assertAllClose(box2.get(), 4.0)\n+\n+  def test_jit_arg2(self):\n+    # set without get\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    box = Box(0.0)\n+    f(box, 1.)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+\n+  def test_jit_arg_in_pytree(self):\n+    @jax.jit\n+    def f(dct, x):\n+      assert tracing_ok\n+      box = dct['box']\n+      box.set(box.get() + x)\n+\n+    tracing_ok = True\n+    box1 = Box(1.0)\n+    f({'box': box1, 'a': 1.0}, 1.)\n+    self.assertAllClose(box1.get(), 2.0)\n+\n+    tracing_ok = False\n+    box2 = Box(2.0)\n+    f({'box': box2, 'a': 2.0}, 2.)\n+    self.assertAllClose(box2.get(), 4.0)\n+\n+    tracing_ok = True\n+    box3 = Box(3)  # int, dtype changed\n+    f({'box': box3, 'a': 2.0}, 2.)\n+    self.assertAllClose(box3.get(), 5.0)\n+\n+  def test_jit_closure(self):\n+    box = Box(1.0)\n+\n+    @jax.jit\n+    def f(x):\n+      assert tracing_ok\n+      box.set(box.get() + x)\n+\n+    tracing_ok = True\n+    f(2.0)\n+    self.assertAllClose(box.get(), 3.0)\n+    tracing_ok = False\n+    f(5.0)\n+    self.assertAllClose(box.get(), 8.0)\n+\n+  def test_jit_closure_nested(self):\n+    box = Box(5.0)\n+\n+    @jax.jit\n+    def f(x):\n+      box.set(box.get() + x)\n+\n+    @jax.jit\n+    def g(x):\n+      f(x)\n+\n+    g(3.0)\n+    self.assertAllClose(box.get(), 8.0)\n+\n+  def test_jit_closure_nested2(self):\n+    @jax.jit\n+    def h(x):\n+      box = new_box()\n+      box.set(x)\n+\n+      @jax.jit\n+      def k(x):\n+        box.set(box.get() + x)\n+\n+      k(1.0)\n+      k(1.0)\n+      return box.get()\n+\n+    ans = h(2.0)\n+    self.assertAllClose(ans, 4.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_jvp_closure_stop_gradient(self, jit):\n+    box = Box(1.0)\n+\n+    def f(x):\n+      y = 2 * x\n+      box.set(box.get() + jax.lax.stop_gradient(y))\n+      return y\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    y, y_dot = jax.jvp(f, (1.0,), (1.0,))\n+    self.assertAllClose(y, 2.0)\n+    self.assertAllClose(y_dot, 2.0)\n+    self.assertAllClose(box.get(), 3.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_jvp_arg(self, jit):\n+    def f(box, x):\n+      box.set(box.get() + x)\n+      return x\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    box = Box(5.0)\n+    box_dot = Box(1.0)\n+    y, y_dot = jax.jvp(f, (box, 2.), (box_dot, 1.))\n+    self.assertAllClose(y, 2.0)\n+    self.assertAllClose(y_dot, 1.0)\n+    self.assertAllClose(box.get(), 7.0)\n+    self.assertAllClose(box_dot.get(), 2.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_custom_vjp_plumbing(self, jit):\n+    box = Box(0.0)\n+\n+    @jax.custom_vjp\n+    def foo(x):\n+      return x\n+    def foo_fwd(x):\n+      return foo(x), None\n+    def foo_bwd(_, g):\n+      box.set(g)\n+      return g,\n+    foo.defvjp(foo_fwd, foo_bwd)\n+\n+    def f(x):\n+      x = 2 * x\n+      x = foo(x)\n+      x = 2 * x\n+      return x\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    jax.grad(f)(1.0)\n+    self.assertAllClose(box.get(), 2.0)\n+\n+  # TODO(mattjj,dougalm): make this work...\n+  # @parameterized.parameters([False, True])\n+  # def test_custom_vjp_plumbing_abstracted(self, jit):\n+  #   box = Box(0.0)\n+\n+  #   @jax.custom_vjp\n+  #   def foo(box, x):\n+  #     return x\n+  #   def foo_fwd(box, x):\n+  #     return x, box\n+  #   def foo_bwd(box, g):\n+  #     box.set(g)\n+  #     return None, g\n+  #   foo.defvjp(foo_fwd, foo_bwd)\n+\n+  #   def f(box, x):\n+  #     x = 2 * x\n+  #     x = foo(box, x)\n+  #     x = 2 * x\n+  #     return x\n+\n+  #   if jit:\n+  #     f = jax.jit(f)\n+\n+  #   jax.grad(partial(f, box))(1.0)\n+  #   self.assertAllClose(box.get(), 2.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_grad_closure_stop_gradient(self, jit):\n+    box = Box(0.0)\n+\n+    def f(x):\n+      y = x * 2\n+      box.set(box.get() + jax.lax.stop_gradient(y))\n+      return y\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    g = jax.grad(f)(1.0)\n+    self.assertAllClose(g, 2.0)\n+    self.assertAllClose(box.get(), 2.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_scan_basic(self, jit):\n+    box = Box(1.0)\n+\n+    def double_it_10():\n+      def body(_, __):\n+        box.set(box.get() * 2)\n+        return None, None\n+      _, _ = jax.lax.scan(body, None, None, length=10)\n+\n+    if jit:\n+      double_it_10 = jax.jit(double_it_10)\n+\n+    double_it_10()\n+    self.assertAllClose(box.get(), 1024., check_dtypes=False)\n+\n+  # TODO error-checking tests from attrs_test.py\n+\n+  ###\n+\n   def test_box_autodiff(self):\n     if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n \n@@ -336,7 +605,7 @@ def abstract_eval(_, box_aval, x_aval):\n         return x_aval, set()\n \n       def to_lojax(_, box, x):\n-        assert False  # TODO\n+        return x\n \n       def jvp(_, primals, tangents):\n         box, x = primals\n@@ -351,14 +620,6 @@ def transpose(self, *args):\n     def stash_tangents(box, x):\n       return stash_tangents_p.bind(box, x)\n \n-    @jax.jit\n-    def f(box, x):\n-      box_set(box, x)\n-\n-    box = Box(0.0)\n-    f(box, 1.)\n-    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n-\n     @jax.jit\n     def f(box, x):\n       x = stash_tangents(box, x)\n@@ -449,5 +710,26 @@ def f(box):\n     self.assertAllClose(b_.arr, 2, check_dtypes=False)\n \n \n+class ListTy(core.AbstractValue):\n+  mutable = True\n+\n+  # forwarded to value\n+  get = core.aval_method(box_get)\n+  set = core.aval_method(box_set)\n+\n+  # aval interface: hashability and str_short\n+  def __hash__(self): return hash(BoxTy)\n+  def __eq__(self, other): return isinstance(other, BoxTy)\n+\n+  def str_short(self, short_dtypes=False):\n+    return 'ListTy'\n+\n+  # TODO\n+\n+class ListTest(jtu.JaxTestCase):\n+  ...\n+\n+\n+\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "rename layout to format, part 1\n\nWe want to rename layouts to formats, since layout is overloaded, both with its own device-local layout, and with XLA's layout (which corresponds to the device-local layout).\n\nThis change specifically focuses on renaming the `Layout` type to `Format` and using the new type constructor throughout the codebase. It should have minimal external effect, since it sets up `Layout` as a public alias of the newly renamed `Format`.\n\nThis change does not yet change most variable and attribute names, so it leaves around various names like `layout` that now have type `Format`. Next up, we should rename these for clarity, among other things.\n\nCo-authored-by: Yash Katariya <yashkatariya@google.com>\nPiperOrigin-RevId: 764922667",
        "output": [
            "```diff\nCommit: 2c838d4ae3fff34242d5a2993e6892c8214c9177\nDate: 2025-05-29T23:06:06Z\nURL: https://github.com/jax-ml/jax/commit/2c838d4ae3fff34242d5a2993e6892c8214c9177\nFiles changed: 14\nAdditions: +159, Deletions: -131\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 3ff103997dc7..2630fc7ae1be 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -74,7 +74,7 @@\n from jax._src.mesh import get_concrete_mesh\n from jax._src.sharding_impls import (\n     PmapSharding, TransferToMemoryKind, PartitionSpec as P, NamedSharding)\n-from jax._src.layout import Layout, AutoLayout\n+from jax._src.layout import Format, AutoLayout\n from jax._src.traceback_util import api_boundary\n from jax._src import tree_util\n from jax._src.util import unzip2, safe_map, safe_zip, wraps, split_list\n@@ -2501,10 +2501,10 @@ def _check_string_compatible_sharding(s):\n @lru_cache(maxsize=2048)\n def _check_sharding(aval, s):\n   if (s is not None and\n-      not isinstance(s, (xc.Device, Sharding, Layout, TransferToMemoryKind))):\n+      not isinstance(s, (xc.Device, Sharding, Format, TransferToMemoryKind))):\n     raise ValueError(\n         \"`jax.device_put` only accepts `None`, `jax.sharding.Sharding`,\"\n-        \" `jax.Device`, `Layout` or a pytree of these values. Received\"\n+        \" `jax.Device`, `Format` or a pytree of these values. Received\"\n         f\" invalid value: {s}\")\n \n   if isinstance(aval, core.ShapedArray) and dtypes.is_string_dtype(aval.dtype):\n@@ -2530,8 +2530,8 @@ def pspec_to_sharding(val):\n \n def device_put(\n     x,\n-    device: None | xc.Device | Sharding | P | Layout | Any | TransferToMemoryKind = None,\n-    *, src: None | xc.Device | Sharding | P | Layout | Any | TransferToMemoryKind = None,\n+    device: None | xc.Device | Sharding | P | Format | Any | TransferToMemoryKind = None,\n+    *, src: None | xc.Device | Sharding | P | Format | Any | TransferToMemoryKind = None,\n     donate: bool | Any = False, may_alias: bool | None | Any = None):\n   \"\"\"Transfers ``x`` to ``device``.\n \n@@ -2827,18 +2827,18 @@ def __init__(self, shape, dtype, *, sharding=None, weak_type=False):\n     if dtype is None:\n       raise ValueError(\"ShapeDtypeStruct: dtype must be specified.\")\n     self.dtype = dtype if dtypes.issubdtype(dtype, dtypes.extended) else np.dtype(dtype)\n-    if sharding is not None and not isinstance(sharding, (Sharding, Layout, P)):\n+    if sharding is not None and not isinstance(sharding, (Sharding, Format, P)):\n       raise ValueError(\n           \"sharding should be an instance of `jax.sharding.Sharding`, \"\n           \"`jax.sharding.PartitionSpec` or\"\n-          f\" `jax.experimental.layout.Layout`. Got {sharding} of type\"\n+          f\" `jax.experimental.layout.Format`. Got {sharding} of type\"\n           f\" {type(sharding)}.\")\n-    if (isinstance(sharding, Layout) and\n+    if (isinstance(sharding, Format) and\n         isinstance(sharding.device_local_layout, AutoLayout)):\n       raise TypeError(\n           \"`DeviceLocalLayout.AUTO` cannot be used in place of a device-local\"\n           f\" layout in a `ShapeDtypeStruct`. Got {sharding}\")\n-    if isinstance(sharding, Layout):\n+    if isinstance(sharding, Format):\n       self.sharding = sharding.sharding\n     elif isinstance(sharding, P):\n       # TODO(yashkatariya): Should this be abstract mesh?\n@@ -2851,15 +2851,18 @@ def __init__(self, shape, dtype, *, sharding=None, weak_type=False):\n       self.sharding = NamedSharding(cur_mesh, sharding)\n     else:\n       self.sharding = sharding\n-    self._dll = sharding.device_local_layout if isinstance(sharding, Layout) else None\n+    self._dll = (sharding.device_local_layout if isinstance(sharding, Format)\n+                 else None)\n     self.weak_type = weak_type\n \n   size = property(lambda self: math.prod(self.shape))\n   ndim = property(lambda self: len(self.shape))\n \n   @property\n-  def layout(self):\n-    return Layout(self._dll, self.sharding)\n+  def format(self):\n+    return Format(self._dll, self.sharding)\n+\n+  layout = format\n \n   def __len__(self):\n     try:\n@@ -2869,7 +2872,7 @@ def __len__(self):\n \n   def __repr__(self):\n     sh = f\", sharding={self.sharding}\" if self.sharding is not None else \"\"\n-    l = f\", layout={self.layout}\" if self._dll is not None else \"\"\n+    l = f\", format={self._dll}\" if self._dll is not None else \"\"\n     wt = f\", weak_type={self.weak_type}\" if self.weak_type else \"\"\n     return (f\"{type(self).__name__}(shape={self.shape}, \"\n             f\"dtype={self.dtype.name}{sh}{l}{wt})\")\n@@ -2880,11 +2883,13 @@ def __eq__(self, other):\n     if not isinstance(other, ShapeDtypeStruct):\n       return False\n     else:\n-      return ((self.shape, self.dtype, self.sharding, self.layout, self.weak_type) ==\n-              (other.shape, other.dtype, other.sharding, other.layout, other.weak_type))\n+      return ((self.shape, self.dtype, self.sharding, self._dll, self.weak_type) ==\n+              (other.shape, other.dtype, other.sharding, other._dll, other.weak_type))\n \n   def __hash__(self):\n-    return hash((self.shape, self.dtype, self.sharding, self.layout,\n+    # TODO(frostig): avoid the conversion from dict by addressing\n+    # https://github.com/jax-ml/jax/issues/8182\n+    return hash((self.shape, self.dtype, self.sharding, self._dll,\n                  self.weak_type))\n \n   def __setattr__(self, name, value):\ndiff --git a/jax/_src/array.py b/jax/_src/array.py\nindex 422fa5086e62..29c7a17b07f1 100644\n--- a/jax/_src/array.py\n+++ b/jax/_src/array.py\n@@ -36,7 +36,7 @@\n from jax._src.interpreters import mlir\n from jax._src.interpreters import pxla\n from jax._src.interpreters import xla\n-from jax._src.layout import AutoLayout, DeviceLocalLayout, Layout\n+from jax._src.layout import AutoLayout, DeviceLocalLayout, Format\n from jax._src.lib import xla_client as xc\n from jax._src.lib import _jax\n from jax._src.sharding import Sharding\n@@ -550,14 +550,14 @@ def addressable_shards(self) -> Sequence[Shard]:\n   def layout(self):\n     # TODO(yashkatariya): Remove the deleted check from here.\n     if self.is_deleted():\n-      return Layout(None, self.sharding)\n+      return Format(None, self.sharding)\n     try:\n-      return Layout(DeviceLocalLayout.from_pjrt_layout(self._pjrt_layout),\n+      return Format(DeviceLocalLayout.from_pjrt_layout(self._pjrt_layout),\n                     self.sharding)\n     except _jax.XlaRuntimeError as e:\n       msg, *_ = e.args\n       if type(msg) is str and msg.startswith(\"UNIMPLEMENTED\"):\n-        return Layout(None, self.sharding)\n+        return Format(None, self.sharding)\n       else:\n         raise\n \n@@ -711,7 +711,7 @@ def _get_and_check_dtype(arrays: Sequence[basearray.Array | np.ndarray],\n # TODO(yashkatariya): Remove None from callback input type.\n \n def make_array_from_callback(\n-    shape: Shape, sharding: Sharding | Layout,\n+    shape: Shape, sharding: Sharding | Format,\n     data_callback: Callable[[Index | None], ArrayLike],\n     dtype: DTypeLike | None = None) -> ArrayImpl:\n   # pyformat: disable\n@@ -756,12 +756,12 @@ def make_array_from_callback(\n     (4, 2)\n   \"\"\"\n   # pyformat: enable\n-  dll = sharding.device_local_layout if isinstance(sharding, Layout) else None\n+  dll = sharding.device_local_layout if isinstance(sharding, Format) else None\n   if isinstance(dll, AutoLayout):\n     raise TypeError(\n         \"`DeviceLocalLayout.AUTO` cannot be used in place of a device-local\"\n         f\" layout when calling `jax.make_array_from_callback`. Got {sharding}\")\n-  sharding = sharding.sharding if isinstance(sharding, Layout) else sharding\n+  sharding = sharding.sharding if isinstance(sharding, Format) else sharding\n   if not isinstance(sharding, Sharding):\n     raise TypeError(\n         f\"sharding should be an instance of `jax.sharding`. Got {sharding} of\"\n@@ -823,7 +823,7 @@ def get_data(index: Index | None) -> ArrayImpl | np.ndarray:\n     )\n \n   if dll is not None:\n-    devices = [Layout(dll, SingleDeviceSharding(d)) for d in devices]\n+    devices = [Format(dll, SingleDeviceSharding(d)) for d in devices]\n     # pxla.batched_device_put doesn't support Layout... Take the slow route\n     arrays = api.device_put(per_device_values, devices)\n     return ArrayImpl(aval, sharding, arrays, committed=True)\n@@ -1218,7 +1218,7 @@ def _array_shard_arg(xs, shardings, layouts, copy_semantics):\n         batch_cs.append(cs)\n       # Resharding starts here:\n       elif not same_layout:\n-        results.append(api.device_put(x, Layout(layout, sharding)))\n+        results.append(api.device_put(x, Format(layout, sharding)))\n       elif dispatch.is_single_device_sharding(x.sharding):\n         results.append(shard_device_array(x, devices, indices, sharding))\n       else:\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex d1ea7439cb0c..a7c8d4ea7380 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -43,7 +43,7 @@\n from jax._src.interpreters import pxla\n from jax._src.interpreters import xla\n from jax._src.api_util import InternalFloatingPointError\n-from jax._src.layout import DeviceLocalLayout, Layout\n+from jax._src.layout import DeviceLocalLayout, Format\n from jax._src.lib import xla_client as xc\n from jax._src.mesh import AbstractMesh, Mesh\n from jax._src.monitoring import record_scalar, record_event_duration_secs, record_event_time_span\n@@ -479,8 +479,8 @@ def _device_put_sharding_impl(x, aval, device, copy):\n \n \n def _device_put_impl(\n-    x, *, device: Device | Sharding | Layout | None,\n-    src: Device | Sharding | Layout | None, copy: CopySemantics):\n+    x, *, device: Device | Sharding | Format | None,\n+    src: Device | Sharding | Format | None, copy: CopySemantics):\n   if (isinstance(device, TransferToMemoryKind) or\n       isinstance(src, TransferToMemoryKind)):\n     raise ValueError(\n@@ -494,7 +494,7 @@ def _device_put_impl(\n     raise TypeError(\n         f\"Argument '{x}' of type {type(x)} is not a valid JAX type\") from err\n \n-  if isinstance(device, Layout):\n+  if isinstance(device, Format):\n     l = device\n     dll = l.device_local_layout\n     x_dll = x.layout.device_local_layout if hasattr(x, 'layout') else None\n@@ -519,8 +519,8 @@ def _device_put_impl(\n \n def _batched_device_put_impl(\n     *xs,\n-    devices: Sequence[Device | Sharding | Layout | None],\n-    srcs: Sequence[Device | Sharding | Layout | None],\n+    devices: Sequence[Device | Sharding | Format | None],\n+    srcs: Sequence[Device | Sharding | Format | None],\n     copy_semantics: Sequence[CopySemantics]):\n   ys = []\n   dsa_indices, dsa_xs, dsa_shardings, dsa_copy_semantics = [], [], [], []\n@@ -536,7 +536,7 @@ def _batched_device_put_impl(\n   if dsa_xs:\n     # Batch shard_arg calls. Helps improve efficiency for backends that support\n     # efficient batch transfer.\n-    # device_put handles `Layout` via a different path, so just pass `None` as\n+    # device_put handles `Format` via a different path, so just pass `None` as\n     # the layout here.\n     shard_arg_results = pxla.shard_args(dsa_shardings, [None] * len(dsa_xs),\n                                         dsa_copy_semantics, dsa_xs)\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex d0a22cd784b4..8ebf4133a5fd 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -56,7 +56,7 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import mlir\n from jax._src.interpreters import xla\n-from jax._src.layout import DeviceLocalLayout, AutoLayout, Layout\n+from jax._src.layout import DeviceLocalLayout, AutoLayout, Format\n from jax._src.lib import xla_client as xc\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n@@ -208,7 +208,7 @@ def _shard_np_array(xs, shardings, layouts, copy_semantics):\n       x = np.zeros(x.shape, dtype=np.dtype(bool))\n     aval = core.shaped_abstractify(x)\n     if layout is not None:\n-      results.append(api.device_put(x, Layout(layout, sharding)))\n+      results.append(api.device_put(x, Format(layout, sharding)))\n     else:\n       if sharding.is_fully_replicated:\n         shards = [x] * len(devices)\ndiff --git a/jax/_src/layout.py b/jax/_src/layout.py\nindex 3675433c43d8..c50c1787b94e 100644\n--- a/jax/_src/layout.py\n+++ b/jax/_src/layout.py\n@@ -94,7 +94,7 @@ def check_compatible_aval(self, aval_shape: Shape):\n ShardingOptions = Union[Sharding, None, AutoSharding]\n \n \n-class Layout:\n+class Format:\n   __slots__ = ['device_local_layout', 'sharding']\n \n   def __init__(self, device_local_layout: LayoutOptions = None,\n@@ -139,7 +139,9 @@ def __hash__(self):\n     return hash((self.device_local_layout, self.sharding))\n \n   def __eq__(self, other):\n-    if not isinstance(other, Layout):\n+    if not isinstance(other, Format):\n       return False\n     return (self.device_local_layout == other.device_local_layout and\n             self.sharding == other.sharding)\n+\n+Layout = Format  # TODO(frostig, yashkatariya): remove this alias\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex d5286be8e0c9..94a754a1a597 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -69,7 +69,7 @@\n     SingleDeviceSharding, PmapSharding, AUTO, UNSPECIFIED, UnspecifiedValue,\n     prepare_axis_resources, parse_flatten_op_sharding, canonicalize_sharding,\n     flatten_spec, _internal_use_concrete_mesh)\n-from jax._src.layout import Layout, DeviceLocalLayout, AutoLayout\n+from jax._src.layout import Format, DeviceLocalLayout, AutoLayout\n from jax._src.state import discharge as state_discharge, RefEffect, AbstractRef\n from jax._src.traceback_util import api_boundary\n from jax._src.tree_util import (\n@@ -374,13 +374,13 @@ def _split_layout_and_sharding(entries):\n   layouts, shardings = [], []\n \n   for e in entries_flat:\n-    if isinstance(e, Layout):\n+    if isinstance(e, Format):\n       layouts.append(e.device_local_layout)\n       shardings.append(e.sharding)\n     elif isinstance(e, (DeviceLocalLayout, AutoLayout)):\n       raise ValueError(\n           '`jax.jit` does not accept device-local layouts directly. Create '\n-          'a `Layout` instance wrapping this device-local layout and pass '\n+          'a `Format` instance wrapping this device-local layout and pass '\n           f'that to `jit` instead. Got {e}')\n     else:\n       layouts.append(None)\n@@ -1645,7 +1645,7 @@ def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n     else:\n       # arg_layout can be None because some backends don't implement the\n       # required layout methods. Hence `arr.layout` can return\n-      # `Layout(None, sharding)`\n+      # `Format(None, sharding)`\n       if (committed\n           and not is_pmap_sharding\n           and arg_layout is not None\n@@ -2813,7 +2813,7 @@ def _sharding_constraint_impl(x, sharding, layout, context_mesh,\n     if (hasattr(x, 'layout') and x.layout.device_local_layout == layout and\n         x.sharding.is_equivalent_to(sharding, x.ndim)):\n       return x\n-    return api.jit(_identity_fn, out_shardings=Layout(layout, sharding))(x)\n+    return api.jit(_identity_fn, out_shardings=Format(layout, sharding))(x)\n \n \n sharding_constraint_p = core.Primitive(\"sharding_constraint\")\n@@ -3160,7 +3160,7 @@ def _layout_constraint_impl(x, *, layout):\n         f' jax.Arrays. Got {type(x)}')\n   if x.layout.device_local_layout == layout:  # type: ignore\n     return x\n-  return api.jit(_identity_fn, out_shardings=Layout(layout, x.sharding))(x)\n+  return api.jit(_identity_fn, out_shardings=Format(layout, x.sharding))(x)\n layout_constraint_p.def_impl(_layout_constraint_impl)\n \n def _layout_constraint_hlo_lowering(ctx, x_node, *, layout):\ndiff --git a/jax/_src/stages.py b/jax/_src/stages.py\nindex d92d1ccb2aa3..17649aae3081 100644\n--- a/jax/_src/stages.py\n+++ b/jax/_src/stages.py\n@@ -46,7 +46,7 @@\n from jax._src import typing\n from jax._src import util\n from jax._src.sharding_impls import UnspecifiedValue, AUTO\n-from jax._src.layout import Layout\n+from jax._src.layout import Format, DeviceLocalLayout\n from jax._src.interpreters import mlir\n from jax._src.lib.mlir import ir\n from jax._src.lib import _jax\n@@ -105,7 +105,7 @@ def input_layouts(self):\n \n   def output_layouts(self):\n     raise NotImplementedError(\n-        \"compiled executable carries no input layout information\")\n+        \"compiled executable carries no output layout information\")\n \n   def as_text(self) -> str:\n     \"\"\"A human-readable text representation of this executable.\n@@ -438,39 +438,58 @@ def runtime_executable(self) -> Any | None:\n     \"\"\"\n     return self._executable.runtime_executable()\n \n-  @property\n-  def input_shardings(self):  # PyTree[sharding.Sharding]\n+  def _input_shardings_flat(self):\n     shardings_flat = self._executable._in_shardings\n     # Some input shardings got DCE'd\n     if self.in_tree.num_leaves > len(shardings_flat):\n       iter_shardings_flat = iter(shardings_flat)\n       shardings_flat = [next(iter_shardings_flat) if i in self._executable._kept_var_idx\n                         else None for i in range(self.in_tree.num_leaves)]\n+    return shardings_flat\n+\n+  @property\n+  def input_shardings(self):  # -> PyTree[sharding.Sharding]\n+    shardings_flat = self._input_shardings_flat()\n     return tree_util.tree_unflatten(self.in_tree, shardings_flat)  # pytype: disable=attribute-error\n \n   @property\n-  def output_shardings(self):  # PyTree[sharding.Sharding]\n+  def output_shardings(self):  # -> PyTree[sharding.Sharding]\n     shardings_flat = self._executable._out_shardings\n     return tree_util.tree_unflatten(self.out_tree, shardings_flat)  # pytype: disable=attribute-error\n \n-  @property\n-  def input_layouts(self):\n-    dll_flat = self._executable._xla_in_layouts\n-    layouts_flat = [Layout(l, s)\n-                    for l, s in zip(dll_flat, self._executable._in_shardings)]\n+  def _input_layouts_flat(self):\n+    layouts_flat = self._executable._xla_in_layouts\n     # Some input layouts got DCE'd\n     if self.in_tree.num_leaves > len(layouts_flat):\n       iter_layouts_flat = iter(layouts_flat)\n       layouts_flat = [next(iter_layouts_flat) if i in self._executable._kept_var_idx\n-                      else Layout() for i in range(self.in_tree.num_leaves)]\n-    return tree_util.tree_unflatten(self.in_tree, layouts_flat)  # pytype: disable=attribute-error\n+                      else None for i in range(self.in_tree.num_leaves)]\n+    return layouts_flat\n+\n+  @property\n+  def input_formats(self):\n+    layouts_flat = self._input_layouts_flat()\n+    shardings_flat = self._input_shardings_flat()\n+    formats_flat = [Format(l, s) for l, s in zip(layouts_flat, shardings_flat)]\n+    return tree_util.tree_unflatten(self.in_tree, formats_flat)  # pytype: disable=attribute-error\n+\n+  @property\n+  def output_formats(self):\n+    layouts_flat = self._executable._xla_out_layouts\n+    shardings_flat = self._executable._out_shardings\n+    assert all(isinstance(l, DeviceLocalLayout) for l in layouts_flat)\n+    formats_flat = [Format(l, s) for l, s in zip(layouts_flat, shardings_flat)]\n+    return tree_util.tree_unflatten(self.out_tree, formats_flat)  # pytype: disable=attribute-error\n+\n+  # TODO(frostig, yashkatariya): remove\n+  @property\n+  def input_layouts(self):\n+    return self.input_formats\n \n+  # TODO(frostig, yashkatariya): remove\n   @property\n   def output_layouts(self):\n-    dll_flat = self._executable._xla_out_layouts\n-    layouts_flat = [Layout(l, s)\n-                    for l, s in zip(dll_flat, self._executable._out_shardings)]\n-    return tree_util.tree_unflatten(self.out_tree, layouts_flat)  # pytype: disable=attribute-error\n+    return self.output_formats\n \n   @staticmethod\n   def call(*args, **kwargs):\ndiff --git a/jax/experimental/array_serialization/serialization.py b/jax/experimental/array_serialization/serialization.py\nindex 82e9e3dc938b..44b2eb9ccd03 100644\n--- a/jax/experimental/array_serialization/serialization.py\n+++ b/jax/experimental/array_serialization/serialization.py\n@@ -32,7 +32,7 @@\n from jax._src import sharding\n from jax._src import typing\n from jax._src import util\n-from jax._src.layout import Layout\n+from jax._src.layout import Format\n from jax._src.lib import _jax\n from jax.experimental.array_serialization import tensorstore_impl as ts_impl\n # ruff: noqa: F401\n@@ -352,7 +352,7 @@ def serialize_with_paths(\n         transaction=transaction,\n     )\n \n-  def deserialize(self, shardings: Sequence[sharding.Sharding | Layout],\n+  def deserialize(self, shardings: Sequence[sharding.Sharding | Format],\n                   tensorstore_specs: Sequence[dict[str, Any]],\n                   global_shapes: Sequence[array.Shape] | None = None,\n                   dtypes: Sequence[typing.DTypeLike] | None = None,\ndiff --git a/jax/experimental/array_serialization/serialization_test.py b/jax/experimental/array_serialization/serialization_test.py\nindex 9a6b91d04c9a..3bee72967101 100644\n--- a/jax/experimental/array_serialization/serialization_test.py\n+++ b/jax/experimental/array_serialization/serialization_test.py\n@@ -27,7 +27,7 @@\n from jax._src import config\n from jax._src import test_util as jtu\n from jax._src.layout import DeviceLocalLayout as DLL\n-from jax._src.layout import Layout\n+from jax._src.layout import Format\n from jax.experimental.array_serialization import serialization\n from jax.experimental.array_serialization import tensorstore_impl as ts_impl\n import jax.numpy as jnp\n@@ -593,7 +593,7 @@ def test_load_with_layout(self):\n     s = NamedSharding(mesh, P('x', 'y'))\n     arr = jax.device_put(np_inp, s)\n \n-    out_layout = jax.jit(lambda x: x.T, out_shardings=Layout(DLL.AUTO)).lower(\n+    out_layout = jax.jit(lambda x: x.T, out_shardings=Format(DLL.AUTO)).lower(\n         arr).compile().output_layouts\n     self.assertEqual(arr.layout.device_local_layout.major_to_minor,\n                      out_layout.device_local_layout.major_to_minor[::-1])\ndiff --git a/jax/experimental/array_serialization/tensorstore_impl.py b/jax/experimental/array_serialization/tensorstore_impl.py\nindex 873cc82da95e..7578bbb831e0 100644\n--- a/jax/experimental/array_serialization/tensorstore_impl.py\n+++ b/jax/experimental/array_serialization/tensorstore_impl.py\n@@ -25,7 +25,7 @@\n import jax\n from jax import numpy as jnp\n from jax._src import array\n-from jax._src.layout import Layout\n+from jax._src.layout import Format\n from jax._src import typing\n import numpy as np\n import tensorstore as ts\n@@ -424,7 +424,7 @@ def estimate_read_memory_footprint(t: ts.TensorStore,\n \n \n async def async_deserialize(\n-    user_in_sharding: jax.sharding.Sharding | Layout,\n+    user_in_sharding: jax.sharding.Sharding | Format,\n     tensorstore_spec: ts.Spec | dict[str, Any],\n     global_shape: Sequence[int] | None = None,\n     dtype=None,\n@@ -435,13 +435,13 @@ async def async_deserialize(\n ):\n   \"\"\"Main performant deserialization routine for arrays using tensorstore.\"\"\"\n   in_sharding = (user_in_sharding.sharding\n-                 if isinstance(user_in_sharding, Layout) else user_in_sharding)\n+                 if isinstance(user_in_sharding, Format) else user_in_sharding)\n   if not isinstance(in_sharding, jax.sharding.Sharding):\n     raise ValueError(\n         'sharding passed to deserialization should be specified, concrete and'\n         f' an instance of `jax.sharding.Sharding`. Got {in_sharding}')\n   dll = (user_in_sharding.device_local_layout\n-         if isinstance(user_in_sharding, Layout) else None)\n+         if isinstance(user_in_sharding, Format) else None)\n   t = await ts.open(\n       tensorstore_spec,\n       open=True,\n@@ -476,7 +476,7 @@ async def cb(index: array.Index, device: jax.Device):\n     if out.dtype == jnp.int4:\n       out = jnp.asarray(out)  # type: ignore\n     result = jax.device_put(\n-        out, Layout(dll, jax.sharding.SingleDeviceSharding(device)))\n+        out, Format(dll, jax.sharding.SingleDeviceSharding(device)))\n     if byte_limiter is not None:\n       # NB: `out` actually might not be ready for garbage collection by the\n       # time we call release_bytes . Thus peak memory usage still might grow\n@@ -495,7 +495,7 @@ async def cb(index: array.Index, device: jax.Device):\n \n \n # TODO(rdyro): Remove this function.\n-def _run_deserialization(shardings: Sequence[jax.sharding.Sharding | Layout],\n+def _run_deserialization(shardings: Sequence[jax.sharding.Sharding | Format],\n                         tensorstore_specs: Sequence[dict[str, Any]],\n                         global_shapes: Sequence[array.Shape] | None = None,\n                         dtypes: Sequence[typing.DTypeLike] | None = None,\ndiff --git a/jax/experimental/layout.py b/jax/experimental/layout.py\nindex e98cfbc68104..1c243541d99b 100644\n--- a/jax/experimental/layout.py\n+++ b/jax/experimental/layout.py\n@@ -14,8 +14,10 @@\n \n from jax._src.layout import (\n     DeviceLocalLayout as DeviceLocalLayout,\n-    Layout as Layout,\n+    Format as Format,\n )\n from jax._src.pjit import (\n     with_layout_constraint as with_layout_constraint,\n )\n+\n+Layout = Format\ndiff --git a/tests/layout_test.py b/tests/layout_test.py\nindex c15816d7794a..cfec2253dfc8 100644\n--- a/tests/layout_test.py\n+++ b/tests/layout_test.py\n@@ -23,7 +23,7 @@\n from jax._src import config\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip\n-from jax.experimental.layout import (with_layout_constraint, Layout,\n+from jax.experimental.layout import (with_layout_constraint, Format,\n                                      DeviceLocalLayout as DLL)\n from jax.experimental.compute_on import compute_on\n \n@@ -51,8 +51,8 @@ def init(x, y):\n     sds1 = jax.ShapeDtypeStruct(np_inp1.shape, np_inp1.dtype, sharding=s1)\n     sds2 = jax.ShapeDtypeStruct(np_inp2.shape, np_inp2.dtype, sharding=s2)\n \n-    lowered_apply = jax.jit(apply, in_shardings=Layout(DLL.AUTO),\n-                            out_shardings=Layout(DLL.AUTO)).lower(sds1, sds2)\n+    lowered_apply = jax.jit(apply, in_shardings=Format(DLL.AUTO),\n+                            out_shardings=Format(DLL.AUTO)).lower(sds1, sds2)\n     compiled_apply = lowered_apply.compile()\n \n     arg_layouts, kw_layouts = compiled_apply.input_layouts\n@@ -122,8 +122,8 @@ def f(x):\n     self.assertArraysEqual(out, np_inp.T)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P(None, 'y', 'x')))\n \n-    compiled_auto = jax.jit(f, in_shardings=Layout(DLL.AUTO),\n-                            out_shardings=Layout(DLL.AUTO)).lower(sds).compile()\n+    compiled_auto = jax.jit(f, in_shardings=Format(DLL.AUTO),\n+                            out_shardings=Format(DLL.AUTO)).lower(sds).compile()\n     self.assertTupleEqual(\n         compiled_auto.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n         (2, 1, 0))\n@@ -146,8 +146,8 @@ def test_in_layouts_out_layouts(self):\n     def f(x):\n       return x.T\n \n-    compiled = jax.jit(f, in_shardings=Layout(),\n-                       out_shardings=Layout(DLL.AUTO)).lower(arr).compile()\n+    compiled = jax.jit(f, in_shardings=Format(),\n+                       out_shardings=Format(DLL.AUTO)).lower(arr).compile()\n     self.assertTupleEqual(\n         compiled.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n         (1, 0))\n@@ -166,8 +166,8 @@ def test_sharding_and_layouts(self):\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     s = NamedSharding(mesh, P('x', 'y'))\n \n-    compiled = jax.jit(lambda x: x.T, in_shardings=Layout(DLL.AUTO, s),\n-                       out_shardings=Layout(DLL.AUTO, s)).lower(np_inp).compile()\n+    compiled = jax.jit(lambda x: x.T, in_shardings=Format(DLL.AUTO, s),\n+                       out_shardings=Format(DLL.AUTO, s)).lower(np_inp).compile()\n     out = compiled(np_inp)\n     self.assertTupleEqual(\n         compiled.input_layouts[0][0].device_local_layout.major_to_minor[::-1],\n@@ -185,8 +185,8 @@ def f(x, y, z, a, b, c):\n \n     shape = (8, 2)\n     inps = [np.arange(math.prod(shape)).reshape(shape)] * 6\n-    compiled = jax.jit(f, in_shardings=Layout(DLL.AUTO),\n-                       out_shardings=Layout(DLL.AUTO)).lower(*inps).compile()\n+    compiled = jax.jit(f, in_shardings=Format(DLL.AUTO),\n+                       out_shardings=Format(DLL.AUTO)).lower(*inps).compile()\n     arg_layouts, _ = compiled.input_layouts\n     out1, out2 = compiled(*inps)\n \n@@ -216,8 +216,8 @@ def test_no_error_dced_args(self):\n     def f(x, y):\n       return x * 2\n \n-    jf = jax.jit(f, in_shardings=Layout(DLL.AUTO, s),\n-                 out_shardings=Layout(DLL.AUTO, s))\n+    jf = jax.jit(f, in_shardings=Format(DLL.AUTO, s),\n+                 out_shardings=Format(DLL.AUTO, s))\n     compiled = jf.lower(np_inp, np_inp).compile()\n     arg_layouts, _ = compiled.input_layouts\n     arrs = [jax.device_put(i, l) for i, l in zip(arrs, arg_layouts)]\n@@ -244,10 +244,10 @@ def f(x):\n     with self.assertRaisesRegex(\n         ValueError,\n         'Layout passed to jit does not match the layout on the respective arg'):\n-      jax.jit(f, in_shardings=Layout(DLL.AUTO)).lower(arr)\n+      jax.jit(f, in_shardings=Format(DLL.AUTO)).lower(arr)\n \n-    compiled = jax.jit(f, in_shardings=Layout(DLL.AUTO),\n-                       out_shardings=Layout(DLL.AUTO)).lower(sds).compile()\n+    compiled = jax.jit(f, in_shardings=Format(DLL.AUTO),\n+                       out_shardings=Format(DLL.AUTO)).lower(sds).compile()\n \n     with self.assertRaisesRegex(\n         ValueError,\n@@ -273,7 +273,7 @@ def test_device_put_concrete_layout(self):\n     arr = jax.device_put(np_inp, s)\n \n     compiled = jax.jit(\n-        lambda x: x * 2, out_shardings=Layout(DLL.AUTO)).lower(arr).compile()\n+        lambda x: x * 2, out_shardings=Format(DLL.AUTO)).lower(arr).compile()\n     col = compiled.output_layouts\n \n     out = jax.device_put(np_inp, col)\n@@ -286,17 +286,17 @@ def test_device_put_concrete_layout(self):\n   def test_device_put_non_concrete_layout_error(self):\n     np_inp = np.arange(16).reshape(8, 2)\n \n-    l1 = Layout(DLL.AUTO, SingleDeviceSharding(jax.devices()[0]))\n+    l1 = Format(DLL.AUTO, SingleDeviceSharding(jax.devices()[0]))\n     with self.assertRaisesRegex(\n         ValueError, 'sharding and device_local_layout.*should be concrete'):\n       jax.device_put(np_inp, l1)\n \n-    l2 = Layout(DLL.AUTO)\n+    l2 = Format(DLL.AUTO)\n     with self.assertRaisesRegex(\n         ValueError, 'sharding and device_local_layout.*should be concrete'):\n       jax.device_put(np_inp, l2)\n \n-    l3 = Layout(None, SingleDeviceSharding(jax.devices()[0]))\n+    l3 = Format(None, SingleDeviceSharding(jax.devices()[0]))\n     out = jax.device_put(np_inp, l3)\n     self.assertArraysEqual(out, np_inp)\n     self.assertTrue(out._committed)\n@@ -306,7 +306,7 @@ def invalid_layout_spec(self):\n     compiled = jax.jit(lambda x: x).lower(x).compile()\n     with self.assertRaisesRegex(\n         ValueError, 'Sharding has to be concrete when layout.*'):\n-      Layout(compiled.output_layouts[0], None)\n+      Format(compiled.output_layouts[0], None)\n \n   def test_layout_on_sds(self):\n     mesh = jtu.create_mesh((2, 1), ('x', 'y'))\n@@ -314,7 +314,7 @@ def test_layout_on_sds(self):\n     np_inp = np.arange(16).reshape(8, 2)\n     arr = jax.device_put(np_inp, s)\n \n-    out_layout = jax.jit(jnp.sin, out_shardings=Layout(DLL.AUTO)).lower(\n+    out_layout = jax.jit(jnp.sin, out_shardings=Format(DLL.AUTO)).lower(\n         arr).compile().output_layouts\n \n     sds = jax.ShapeDtypeStruct(arr.shape, arr.dtype, sharding=out_layout)\n@@ -325,7 +325,7 @@ def test_layout_on_sds(self):\n         TypeError,\n         'DeviceLocalLayout.AUTO` cannot be used in place of a device-local'\n         ' layout in a `ShapeDtypeStruct`'):\n-      jax.ShapeDtypeStruct(arr.shape, arr.dtype, sharding=Layout(DLL.AUTO))\n+      jax.ShapeDtypeStruct(arr.shape, arr.dtype, sharding=Format(DLL.AUTO))\n \n   def test_make_array_from_callback(self):\n     mesh = jtu.create_mesh((2, 1), ('x', 'y'))\n@@ -344,13 +344,13 @@ def test_make_array_from_callback(self):\n         TypeError,\n         '`DeviceLocalLayout.AUTO` cannot be used in place of a device-local'\n         ' layout'):\n-      jax.make_array_from_callback(np_inp.shape, Layout(DLL.AUTO, s),\n+      jax.make_array_from_callback(np_inp.shape, Format(DLL.AUTO, s),\n                                    lambda idx: np_inp[idx])\n \n     with self.assertRaisesRegex(\n         TypeError, 'sharding should be an instance of `jax.sharding`'):\n       jax.make_array_from_callback(\n-          np_inp.shape, Layout(None, None), lambda idx: np_inp[idx])\n+          np_inp.shape, Format(None, None), lambda idx: np_inp[idx])\n \n   def test_wsc_concrete_layout(self):\n     mesh = jtu.create_mesh((2, 2), ('x', 'y'))\n@@ -367,7 +367,7 @@ def f(x):\n       y = x.T\n       # Constrain `y` to the original layout of `arr` because without it,\n       # the layout of `y` would be the transpose of `arr`.\n-      return jax.lax.with_sharding_constraint(y, Layout(custom_dll, s))\n+      return jax.lax.with_sharding_constraint(y, Format(custom_dll, s))\n \n     out = f(arr)\n     self.assertEqual(out.layout.device_local_layout.major_to_minor,\n@@ -390,7 +390,7 @@ def f(x):\n       y = x.T\n       # Constrain `y` to the original layout of `arr` because without it,\n       # the layout of `y` would be the transpose of `arr`.\n-      return jax.lax.with_sharding_constraint(y, Layout(custom_dll, s))\n+      return jax.lax.with_sharding_constraint(y, Format(custom_dll, s))\n \n     out = f(arr)\n     self.assertEqual(out.layout.device_local_layout.major_to_minor,\n@@ -404,7 +404,7 @@ def test_device_put_user_concrete_layout(self):\n     dll = DLL(major_to_minor=(1, 0))\n     s = SingleDeviceSharding(jax.devices()[0])\n \n-    out = jax.device_put(np_inp, Layout(dll, s))\n+    out = jax.device_put(np_inp, Format(dll, s))\n     self.assertEqual(out.layout.device_local_layout.major_to_minor,\n                      dll.major_to_minor)\n     self.assertArraysEqual(out, np_inp)\n@@ -417,7 +417,7 @@ def test_device_put_user_concrete_layout_multi_device(self):\n     jnp_inp = jnp.arange(math.prod(shape)).reshape(shape)\n     arr = jax.device_put(np_inp, s)\n \n-    custom_layout = Layout(DLL(major_to_minor=(0, 1)), s)\n+    custom_layout = Format(DLL(major_to_minor=(0, 1)), s)\n     out1 = jax.device_put(arr, custom_layout)\n \n     with jax.sharding.use_mesh(mesh):\n@@ -441,7 +441,7 @@ def f(x):\n       return x.T\n \n     custom_dll = DLL(major_to_minor=(0, 1))\n-    f = jax.jit(f, out_shardings=Layout(custom_dll, s))\n+    f = jax.jit(f, out_shardings=Format(custom_dll, s))\n \n     out = f(arr)\n     self.assertArraysEqual(out, np_inp.T)\n@@ -450,7 +450,7 @@ def f(x):\n \n   def test_compatible_aval_error(self):\n     custom_dll = DLL(major_to_minor=(0, 1, 2))\n-    l = Layout(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n+    l = Format(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n     inp = np.arange(8)\n \n     @partial(jax.jit, in_shardings=l)\n@@ -464,7 +464,7 @@ def f(x):\n \n   def test_incompatible_aval_error_device_put(self):\n     custom_dll = DLL(major_to_minor=(0, 1, 2))\n-    l = Layout(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n+    l = Format(custom_dll, SingleDeviceSharding(jax.devices()[0]))\n     inp = np.arange(8)\n \n     with self.assertRaisesRegex(\n@@ -482,8 +482,8 @@ def test_concrete_layout_in_shardings(self):\n     custom_dll = DLL(major_to_minor=(0, 1))\n \n     @partial(jax.jit,\n-             in_shardings=Layout(custom_dll, s),\n-             out_shardings=Layout(DLL.AUTO))\n+             in_shardings=Format(custom_dll, s),\n+             out_shardings=Format(DLL.AUTO))\n     def f(x):\n       return x.T\n \n@@ -494,7 +494,7 @@ def f(x):\n \n     custom_dll2 = DLL(major_to_minor=(1, 0))\n \n-    @partial(jax.jit, in_shardings=Layout(custom_dll2, s))\n+    @partial(jax.jit, in_shardings=Format(custom_dll2, s))\n     def g(x):\n       return x.T\n \n@@ -508,7 +508,7 @@ def test_in_layouts_jit_jnp_input(self):\n     sharding = jax.sharding.SingleDeviceSharding(jax.devices()[0])\n \n     f = jax.jit(lambda x: x + 1,\n-                in_shardings=Layout(major_last_layout, sharding))\n+                in_shardings=Format(major_last_layout, sharding))\n \n     arr = jnp.arange(8 * 128).reshape(8, 128)\n     out = f(arr)\n@@ -533,9 +533,9 @@ def test_layout_donation(self):\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n \n     custom_dll = DLL(major_to_minor=(0, 1))\n-    arr = jax.device_put(np_inp, Layout(custom_dll, s))\n+    arr = jax.device_put(np_inp, Format(custom_dll, s))\n \n-    @partial(jax.jit, in_shardings=Layout(custom_dll, s), donate_argnums=0)\n+    @partial(jax.jit, in_shardings=Format(custom_dll, s), donate_argnums=0)\n     def f(x):\n       return x\n \n@@ -550,7 +550,7 @@ def test_layout_donation_auto(self):\n \n     arr = jax.device_put(np_inp, s)\n \n-    @partial(jax.jit, out_shardings=Layout(DLL.AUTO), donate_argnums=0)\n+    @partial(jax.jit, out_shardings=Format(DLL.AUTO), donate_argnums=0)\n     def f(x):\n       return x * x\n \n@@ -564,7 +564,7 @@ def test_layout_donation_matching_in_and_out(self):\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n \n     custom_dll = DLL(major_to_minor=(0, 1))\n-    l = Layout(custom_dll, s)\n+    l = Format(custom_dll, s)\n     arr = jax.device_put(np_inp, l)\n \n     @partial(jax.jit, in_shardings=l, out_shardings=l, donate_argnums=0)\n@@ -582,7 +582,7 @@ def test_layout_donation_mismatching_in_and_out_fails(self):\n     np_inp = np.arange(math.prod(shape), dtype=jnp.bfloat16).reshape(shape)\n \n     custom_dll1 = DLL(major_to_minor=(1, 0), _tiling=((8,128), (2,1)))\n-    l1 = Layout(custom_dll1, s)\n+    l1 = Format(custom_dll1, s)\n     arr = jax.device_put(np_inp, s)\n \n     @partial(jax.jit, out_shardings=l1, donate_argnums=0)\n@@ -594,7 +594,7 @@ def f(x):\n     self.assertFalse(arr.is_deleted())\n \n   def test_donation_error_on_auto(self):\n-    @partial(jax.jit, donate_argnums=0, in_shardings=Layout(DLL.AUTO))\n+    @partial(jax.jit, donate_argnums=0, in_shardings=Format(DLL.AUTO))\n     def f(x):\n       return x * 2\n \n@@ -602,7 +602,7 @@ def f(x):\n         ValueError, \".*Did you mean to set the.*output layout.*AUTO.*\"):\n       f(jnp.arange(8))\n \n-    @partial(jax.jit, donate_argnums=0, out_shardings=Layout(DLL.AUTO))\n+    @partial(jax.jit, donate_argnums=0, out_shardings=Format(DLL.AUTO))\n     def g(x):\n       return x * 2\n \n@@ -619,9 +619,9 @@ def test_sparsecore_compute(self):\n \n     dll = DLL(major_to_minor=(0, 1), _tiling=((8,),))\n     s = SingleDeviceSharding(jax.devices()[0])\n-    sparse_layout = Layout(dll, s)\n+    sparse_layout = Format(dll, s)\n     sparecore_arr = jax.device_put(inp, sparse_layout)\n-    dense_layout = Layout(DLL(major_to_minor=(0, 1)), s)\n+    dense_layout = Format(DLL(major_to_minor=(0, 1)), s)\n \n     @compute_on('tpu_sparsecore')\n     @jax.jit\n@@ -645,7 +645,7 @@ def test_sparsecore_compute_twice(self):\n \n     dll = DLL(major_to_minor=(0, 1), _tiling=((8,),))\n     s = SingleDeviceSharding(jax.devices()[0])\n-    sparse_layout = Layout(dll, s)\n+    sparse_layout = Format(dll, s)\n     sparecore_arr = jax.device_put(inp, sparse_layout)\n \n     @compute_on('tpu_sparsecore')\n@@ -675,11 +675,11 @@ def test_sparsecore_and_host_compute(self):\n     s = SingleDeviceSharding(jax.devices()[0])\n \n     sparse_dll = DLL(major_to_minor=(0, 1), _tiling=((8,),))\n-    sparse_layout = Layout(sparse_dll, s)\n+    sparse_layout = Format(sparse_dll, s)\n     sparecore_arr = jax.device_put(inp, sparse_layout)\n \n     host_dll = DLL(major_to_minor=(0, 1), _tiling=((1,),))\n-    host_layout = Layout(host_dll, s)\n+    host_layout = Format(host_dll, s)\n     host_arr = jax.device_put(inp, host_layout)\n \n     @compute_on('tpu_sparsecore')\n@@ -710,7 +710,7 @@ def test_cpp_layout_cache_miss(self):\n     arr = jax.device_put(np_inp, s)\n \n     arr_m2m = arr.layout.device_local_layout.major_to_minor\n-    custom_layout = Layout(DLL(major_to_minor=arr_m2m[::-1]), s)\n+    custom_layout = Format(DLL(major_to_minor=arr_m2m[::-1]), s)\n     arr2 = jax.device_put(np_inp, custom_layout)\n \n     @jax.jit\n@@ -731,7 +731,7 @@ def test_layout_donation_with_default_layout(self):\n     shape = (16, 16)\n     np_inp = np.arange(math.prod(shape)).reshape(shape)\n     arr = jax.device_put(np_inp, s)\n-    out_layout = Layout(arr.layout.device_local_layout, s)\n+    out_layout = Format(arr.layout.device_local_layout, s)\n \n     @partial(jax.jit, out_shardings=out_layout, donate_argnums=0)\n     def f(x):\ndiff --git a/tests/memories_test.py b/tests/memories_test.py\nindex e0a42d4a0146..fd40330f8db8 100644\n--- a/tests/memories_test.py\n+++ b/tests/memories_test.py\n@@ -25,7 +25,7 @@\n from jax import lax\n from jax._src import test_util as jtu\n from jax._src import xla_bridge as xb\n-from jax._src.layout import DeviceLocalLayout as DLL, Layout\n+from jax._src.layout import DeviceLocalLayout as DLL, Format\n from jax._src import config\n from jax.ad_checkpoint import checkpoint_name, checkpoint as new_checkpoint\n import jax.numpy as jnp\n@@ -1574,8 +1574,8 @@ def test_fn(x_in, y_in):\n     y = jnp.reshape(y, (16, 64))\n     custom_dll = DLL(major_to_minor=(0, 1), _tiling=((8, 128),))\n     custom_dll_linear = DLL(major_to_minor=(0, 1), _tiling=((1,),))\n-    x = jax.device_put(x, Layout(custom_dll, sharding))\n-    y = jax.device_put(y, Layout(custom_dll_linear, p_sharding))\n+    x = jax.device_put(x, Format(custom_dll, sharding))\n+    y = jax.device_put(y, Format(custom_dll_linear, p_sharding))\n \n     x1 = jnp.arange(0, 1024, dtype=jnp.float32)\n     x1 = jnp.reshape(x1, (16, 64))\n@@ -1585,8 +1585,8 @@ def test_fn(x_in, y_in):\n     jit_fn = jax.jit(\n         test_fn,\n         out_shardings=(\n-            Layout(custom_dll, sharding),\n-            Layout(custom_dll_linear, p_sharding),\n+            Format(custom_dll, sharding),\n+            Format(custom_dll_linear, p_sharding),\n         ),\n     )\n     x_out, y_out = jit_fn(x, y)\n@@ -1613,8 +1613,8 @@ def test_fn(x_in, y_in):\n     y = jnp.reshape(y, (32, 64))\n     custom_dll = DLL(major_to_minor=(0, 1), _tiling=((8, 128),))\n     custom_dll_linear = DLL(major_to_minor=(0, 1), _tiling=((1,),))\n-    x = jax.device_put(x, Layout(custom_dll, sharding))\n-    y = jax.device_put(y, Layout(custom_dll_linear, p_sharding))\n+    x = jax.device_put(x, Format(custom_dll, sharding))\n+    y = jax.device_put(y, Format(custom_dll_linear, p_sharding))\n \n     x1 = jnp.arange(0, 2048, dtype=jnp.float32)\n     x1 = jnp.reshape(x1, (32, 64))\n@@ -1624,8 +1624,8 @@ def test_fn(x_in, y_in):\n     jit_fn = jax.jit(\n         test_fn,\n         out_shardings=(\n-            Layout(custom_dll, sharding),\n-            Layout(custom_dll_linear, p_sharding),\n+            Format(custom_dll, sharding),\n+            Format(custom_dll_linear, p_sharding),\n         ),\n     )\n     x_out, y_out = jit_fn(x, y)\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex d37b21bd2460..751fd63823e3 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -55,7 +55,7 @@\n     SingleDeviceSharding, parse_flatten_op_sharding)\n from jax._src.pjit import (pjit, mesh_cast, auto_axes, explicit_axes,\n                            use_auto_axes, use_explicit_axes, reshard)\n-from jax._src.layout import Layout, DeviceLocalLayout as DLL\n+from jax._src.layout import Format, DeviceLocalLayout as DLL\n from jax._src.named_sharding import DuplicateSpecError\n from jax._src import mesh as mesh_lib\n from jax._src.mesh import AxisType\n@@ -4997,8 +4997,8 @@ def test_sds_update(self):\n \n     sh = NamedSharding(mesh, P())\n     s4 = jax.ShapeDtypeStruct((2, 2), jnp.int32,\n-                              sharding=Layout(DLL((0, 1)), sh))\n-    new_layout = Layout(DLL((1, 0)), NamedSharding(mesh, P('x')))\n+                              sharding=Format(DLL((0, 1)), sh))\n+    new_layout = Format(DLL((1, 0)), NamedSharding(mesh, P('x')))\n     s4_u = s4.update(sharding=new_layout)\n     self.assertEqual(s4_u.sharding, new_layout.sharding)\n     self.assertEqual(s4_u.layout, new_layout)\n\n```"
        ],
        "from_id": [
            "froystig",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[hijax] all pre-existing Box tests passing, still using typechange env\n\nCo-authored-by: Dougal Maclaurin <dougalm@google.com>",
        "output": [
            "```diff\nCommit: 7eec8e1b6a6ed30fd8ce6a5d42134e0d2e8492aa\nDate: 2025-05-29T22:40:07Z\nURL: https://github.com/jax-ml/jax/commit/7eec8e1b6a6ed30fd8ce6a5d42134e0d2e8492aa\nFiles changed: 8\nAdditions: +665, Deletions: -200\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex b20b85a43b6e..24150aba6584 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,7 +88,8 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info', '_is_high', '_final_typechange_env']\n+               '_effects', '_debug_info', '_is_high',\n+               '_initial_typechange_env', '_final_typechange_env']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -97,6 +98,7 @@ class Jaxpr:\n   _effects: Effects\n   _debug_info: DebugInfo\n   _is_high: bool\n+  _initial_typechange_env: dict[Var, Any]\n   _final_typechange_env: dict[Var, Any]\n \n   @property\n@@ -127,6 +129,10 @@ def debug_info(self) -> DebugInfo:\n   def is_high(self) -> bool:\n     return self._is_high\n \n+  @property\n+  def initial_typechange_env(self) -> dict[Var, Any]:\n+    return self._initial_typechange_env\n+\n   @property\n   def final_typechange_env(self) -> dict[Var, Any]:\n     return self._final_typechange_env\n@@ -139,6 +145,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n                is_high: bool = False,\n+               initial_typechange_env: dict | None = None,\n                final_typechange_env: dict | None = None,\n                ):\n     \"\"\"\n@@ -165,6 +172,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n     self._is_high = is_high\n+    self._initial_typechange_env = initial_typechange_env or {}\n     self._final_typechange_env = final_typechange_env or {}\n \n   def __str__(self):\n@@ -193,6 +201,8 @@ def replace(self, **kwargs):\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n         is_high=kwargs.pop(\"is_high\", self.is_high),\n+        initial_typechange_env=kwargs.pop(\"initial_typechange_env\",\n+                                          self.initial_typechange_env),\n         final_typechange_env=kwargs.pop(\"final_typechange_env\",\n                                         self.final_typechange_env),\n     )\n@@ -222,6 +232,22 @@ def subjaxprs(jaxpr: Jaxpr) -> Iterator[Jaxpr]:\n     yield from jaxprs_in_params(eqn.params)\n \n \n+@dataclass(frozen=True)\n+class TypeChange:\n+  aval: AbstractValue\n+  initial_type_state: Any\n+  final_type_state: Any\n+\n+  def to_tangent_aval(self):\n+    return TypeChange(self.aval.to_tangent_aval(),\n+                      self.initial_type_state.to_tangent_aval(),\n+                      self.final_type_state.to_tangent_aval())\n+\n+  def normalize(self):\n+    return TypeChange(self.aval.normalize(),\n+                      self.initial_type_state.normalize(),\n+                      self.final_type_state.normalize())\n+\n class ClosedJaxpr:\n   __slots__ = ['__weakref__', '_jaxpr', '_consts']\n \n@@ -241,6 +267,13 @@ def __init__(self, jaxpr: Jaxpr, consts: Sequence):\n   def in_avals(self):\n     return [v.aval for v in self.jaxpr.invars]\n \n+  @property\n+  def in_avals_aug(self):\n+    ienv = self.jaxpr.initial_typechange_env\n+    fenv = self.jaxpr.final_typechange_env\n+    return [TypeChange(v.aval, ienv[v], fenv[v]) if v.aval.mutable else v.aval\n+            for v in self.jaxpr.invars]\n+\n   @property\n   def out_avals(self):\n     return [v.aval for v in self.jaxpr.outvars]\n@@ -542,10 +575,6 @@ def _true_bind(self, *args, **params):\n     # is called frequently and it's slightly faster to avoid using a context\n     # manager object.\n     prev_trace = trace_ctx.trace\n-\n-    if self.is_high(**params) and prev_trace.requires_low:\n-      return self.to_lojax(*args, **params)  # type: ignore\n-\n     trace_ctx.set_trace(eval_trace)\n     try:\n       return self.bind_with_trace(prev_trace, args, params)\n@@ -553,6 +582,11 @@ def _true_bind(self, *args, **params):\n       trace_ctx.set_trace(prev_trace)\n \n   def bind_with_trace(self, trace, args, params):\n+    # TODO(mattjj,dougalm): remove this block?\n+    if self.is_high(**params) and trace.requires_low:\n+      with set_current_trace(trace):\n+        return self.to_lojax(*args, **params)  # type: ignore\n+\n     return trace.process_primitive(self, args, params)\n \n   def def_impl(self, impl):\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 7cbdfff01462..0cd99a197f66 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -641,6 +641,9 @@ def to_concrete_value(self):\n   def get_referent(self):\n     return core.get_referent(self.primal)\n \n+  def type_state(self):\n+    return self.primal.type_state()\n+\n def _primal_tangent_shapes_match(primal, tangent):\n   if type(tangent) is not Zero:\n     primal_aval = get_aval(primal).strip_weak_type()\n@@ -1166,8 +1169,9 @@ def _jvp_jaxpr(jaxpr: core.ClosedJaxpr,\n                    debug_info=jaxpr.jaxpr.debug_info)\n   f_jvp, out_nonzeros = f_jvp_traceable(\n       jvp(f, instantiate=instantiate, transform_stack=False), nonzeros)\n-  tangent_avals = [aval.to_tangent_aval() for aval, nz in zip(jaxpr.in_avals, nonzeros) if nz]\n-  avals_in = list(it.chain(jaxpr.in_avals, tangent_avals))\n+  tangent_avals = [aval.to_tangent_aval()\n+                   for aval, nz in zip(jaxpr.in_avals_aug, nonzeros) if nz]\n+  avals_in = list(it.chain(jaxpr.in_avals_aug, tangent_avals))\n   jaxpr_out, avals_out, literals_out, () = pe.trace_to_jaxpr_dynamic(\n       f_jvp, avals_in)\n   return core.ClosedJaxpr(jaxpr_out, literals_out), out_nonzeros()\n@@ -1189,14 +1193,12 @@ def rearrange_binders(jaxpr: core.ClosedJaxpr, primals_in, tangents_in, primals_\n   new_invars = _perm(primals_in, tangents_in, jaxpr.jaxpr.invars)\n   new_outvars = _perm(primals_out, tangents_out, jaxpr.jaxpr.outvars)\n   new_debug_info = jaxpr.jaxpr.debug_info\n-  new_arg_names = tuple(_perm(primals_in, tangents_in,\n-                              jaxpr.jaxpr.debug_info.safe_arg_names(len(jaxpr.jaxpr.invars))))\n-  new_result_paths = tuple(_perm(primals_out, tangents_out,\n-                                  jaxpr.jaxpr.debug_info.safe_result_paths(len(jaxpr.jaxpr.outvars))))\n+  arg_names = jaxpr.jaxpr.debug_info.safe_arg_names(len(jaxpr.in_avals))\n+  result_paths = jaxpr.jaxpr.debug_info.safe_result_paths(len(jaxpr.out_avals))\n+  new_arg_names = tuple(_perm(primals_in, tangents_in, arg_names))\n+  new_result_paths = tuple(_perm(primals_out, tangents_out, result_paths))\n   new_debug_info = new_debug_info._replace(\n-      arg_names=new_arg_names,\n-      result_paths=new_result_paths,\n-  )\n+      arg_names=new_arg_names, result_paths=new_result_paths)\n   constvars = jaxpr.jaxpr.constvars\n   new_effects = pe._renumber_effects(\n       (*constvars, *new_invars), (*constvars, *jaxpr.jaxpr.invars),\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 3c499429a663..444b60f15fa5 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -896,10 +896,8 @@ def convert_constvars_jaxpr(jaxpr: Jaxpr) -> Jaxpr:\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   dbg = jaxpr.debug_info._replace(\n       arg_names=(\"\",) * len(jaxpr.constvars) + jaxpr.debug_info.arg_names)\n-  lifted_jaxpr = Jaxpr(constvars=(),\n-                       invars=jaxpr.constvars + jaxpr.invars,\n-                       outvars=jaxpr.outvars, eqns=jaxpr.eqns,\n-                       effects=jaxpr.effects, debug_info=dbg)\n+  lifted_jaxpr = jaxpr.replace(\n+      constvars=(), invars=jaxpr.constvars + jaxpr.invars, debug_info=dbg)\n   config.enable_checks.value and core.check_jaxpr(lifted_jaxpr)\n   return lifted_jaxpr\n \n@@ -1014,10 +1012,9 @@ def fun(*known_vals_in):\n     known_vals_out = [pval.get_known() for pval in out_pvals if pval.is_known()]\n     return [*known_vals_out, *residuals]\n \n-  known_avals = [a for a, uk in zip(jaxpr.in_avals, in_unknowns) if not uk]\n+  known_avals = [a for a, uk in zip(jaxpr.in_avals_aug, in_unknowns) if not uk]\n   jaxpr_known, _, consts_known, () = trace_to_jaxpr_dynamic(\n-      lu.wrap_init(fun, debug_info=f.debug_info),\n-      known_avals)\n+      lu.wrap_init(fun, debug_info=f.debug_info), known_avals)\n   (out_unknowns, jaxpr_unknown, res_avals), = cell  # pytype: disable=bad-unpacking\n \n   # check jaxpr_known and jaxpr_unknown in isolation\n@@ -1579,6 +1576,20 @@ def dce_jaxpr_closed_call_rule(used_outputs: list[bool], eqn: JaxprEqn\n def close_jaxpr(jaxpr: Jaxpr) -> ClosedJaxpr:\n   return ClosedJaxpr(jaxpr, ())\n \n+def move_invars_right(jaxpr: ClosedJaxpr, to_move: Sequence[bool]):\n+  return _move_invars_right(jaxpr, tuple(to_move))\n+\n+@weakref_lru_cache\n+def _move_invars_right(jaxpr: ClosedJaxpr, to_move: tuple[bool, ...]):\n+  invars, rest = split_list(jaxpr.jaxpr.invars, [len(to_move)])\n+  left_invars, right_invars = partition_list(to_move, invars)\n+  new_invars = [*left_invars, *right_invars, *rest]\n+  new_effs = _renumber_effects(\n+      (*jaxpr.jaxpr.constvars, *new_invars),\n+      (*jaxpr.jaxpr.constvars, *jaxpr.jaxpr.invars),\n+      jaxpr.jaxpr.effects)\n+  return jaxpr.replace(jaxpr=jaxpr.jaxpr.replace(invars=new_invars, effects=new_effs))\n+\n def move_binders_to_front(closed_jaxpr: ClosedJaxpr, to_move: Sequence[bool]\n                           ) -> ClosedJaxpr:\n   \"\"\"Reorder `invars` by moving those indicated in `to_move` to the front.\"\"\"\n@@ -1640,6 +1651,10 @@ def full_lower(self):\n     if val is None: return self\n     return core.full_lower(val)\n \n+  def type_state(self):\n+    var = self._trace.frame.tracer_to_var.get(id(self))\n+    return self._trace.frame.current_typechange_env[var]\n+\n   def _contents(self):\n     return ()\n \n@@ -1735,7 +1750,8 @@ class JaxprStackFrame:\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n   is_high: bool\n-  final_typechange_env: dict\n+  initial_typechange_env: dict\n+  current_typechange_env: dict\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1751,7 +1767,8 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_vars = []\n     self.debug_info = debug_info\n     self.is_high = False\n-    self.final_typechange_env = {}\n+    self.initial_typechange_env = {}\n+    self.current_typechange_env = {}\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1777,8 +1794,11 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n+    final_typechange_env = {v: s for v, s in self.current_typechange_env.items()\n+                            if v in self.initial_typechange_env}\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info, self.is_high, self.final_typechange_env)\n+                  debug_info, self.is_high, self.initial_typechange_env,\n+                  final_typechange_env)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1895,8 +1915,6 @@ def new_arg(self, aval, source_info: SourceInfo):\n     self.frame.tracers.append(tracer)\n     self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n     self.frame.invars.append(var)\n-    if aval.mutable:\n-      self.frame.final_typechange_env[var] = aval\n     return tracer\n \n   def new_const(self, c, source_info: SourceInfo):\n@@ -1921,6 +1939,8 @@ def _new_const(self, aval, c, source_info: SourceInfo) -> DynamicJaxprTracer:\n       self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n       self.frame.constid_to_tracer[id(c)] = tracer\n       self.frame.constvar_to_val[var] = c\n+      if aval.mutable:\n+        self.frame.initial_typechange_env[var] = c.type_state()\n     return tracer\n \n   def get_const(self, tracer) -> Any:\n@@ -2235,18 +2255,24 @@ def trace_to_jaxpr_dynamic(\n            list[tuple[PyTreeDef, PyTreeDef, tuple[Any, str, AttrKind]]]]:\n   keep_inputs = [True] * len(in_avals) if keep_inputs is None else keep_inputs\n   trace = DynamicJaxprTrace(fun.debug_info, lower=lower)\n+  in_avals_ = [a.aval if isinstance(a, core.TypeChange) else a for a in in_avals]\n   with core.ensure_no_leaks(trace), source_info_util.reset_name_stack():\n     source_info = source_info_util.current()\n     in_tracers = _input_type_to_tracers(\n-        partial(trace.new_arg, source_info=source_info), in_avals)\n+        partial(trace.new_arg, source_info=source_info), in_avals_)\n     in_tracers = [t for t, keep in zip(in_tracers, keep_inputs) if keep]\n+    trace.frame.initial_typechange_env = initial_typechange_env = {\n+        v: a.initial_type_state for v, a in zip(trace.frame.invars, in_avals)\n+        if isinstance(a, core.TypeChange)}\n+    trace.frame.current_typechange_env = dict(initial_typechange_env)\n+\n     try:\n       with core.set_current_trace(trace):\n         ans = fun.call_wrapped(*in_tracers)\n       _check_returned_jaxtypes(fun.debug_info, ans)\n       out_tracers = map(partial(trace.to_jaxpr_tracer, source_info=source_info), ans)\n       _check_no_returned_refs(fun.debug_info, out_tracers)\n-      jaxpr, consts, attrs_tracked = trace.to_jaxpr(out_tracers, fun.debug_info)\n+      jaxpr, consts, attrs_tracked = trace.frame.to_jaxpr(trace, out_tracers, fun.debug_info)\n       del fun, in_tracers, out_tracers, ans\n     finally:\n       trace.frame.reset_states(trace)\n@@ -2718,21 +2744,38 @@ def _linearize_of_pmap_hack(f: lu.WrappedFun, jaxpr, consts) -> tuple[Jaxpr, lis\n \n @weakref_lru_cache\n def lower_jaxpr(hi_jaxpr):\n-  in_avals = [lo_ty for t in hi_jaxpr.in_avals for lo_ty in t.lo_ty()]\n+  initial_env = hi_jaxpr.jaxpr.initial_typechange_env\n+  lo_avals = [lo_ty for v in hi_jaxpr.jaxpr.invars\n+              for lo_ty in (v.aval.lo_ty_(initial_env[v]) if v.aval.mutable\n+                            else v.aval.lo_ty())]\n   f = lu.wrap_init(partial(lower_traceable, hi_jaxpr),\n                    debug_info=hi_jaxpr.jaxpr.debug_info)\n-  lo_jaxpr, _, consts, () = trace_to_jaxpr_dynamic(f, in_avals, lower=True)\n-  return core.ClosedJaxpr(lo_jaxpr, consts)\n+  lo_jaxpr, _, lo_consts, () = trace_to_jaxpr_dynamic(f, lo_avals, lower=True)\n+  return core.ClosedJaxpr(lo_jaxpr, lo_consts)\n \n def lower_traceable(jaxpr, *lo_args):\n+  env = jaxpr.jaxpr.initial_typechange_env\n   lo_args_ = iter(lo_args)\n-  hi_args = [t.raise_val(*it.islice(lo_args_, len(t.lo_ty())))\n-             for t in jaxpr.in_avals]\n+  hi_args = [v.aval.raise_val(*it.islice(lo_args_, len(v.aval.lo_ty())))\n+             if not v.aval.mutable else\n+             v.aval.new_from_loval(env[v], *it.islice(lo_args_, len(v.aval.lo_ty_(env[v]))))\n+             for v in jaxpr.jaxpr.invars]\n   assert (problem := next(lo_args_, None)) is None\n   hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n   mut_outs = [lo_val for v, ty in jaxpr.jaxpr.final_typechange_env.items()\n-              for lo_val in ty.get(hi_args[in_idx[v]])]\n-  lo_outs = [lo_val for t, hi_val in zip(jaxpr.out_avals, hi_outs)\n-             for lo_val in t.lower_val(hi_val)]\n+              for lo_val in v.aval.read_loval(ty, hi_args[in_idx[v]])]\n+  lo_outs = [lo_val for v, hi_val in zip(jaxpr.jaxpr.outvars, hi_outs)\n+             for lo_val in v.aval.lower_val(hi_val)]\n   return mut_outs + lo_outs\n+\n+def convert_const_himutables(jaxpr):\n+  move = [core.typeof(c).mutable for c in jaxpr.consts]\n+  constvals, in_mutables = partition_list(move, jaxpr.consts)\n+  constvars, boxvars = partition_list(move, jaxpr.jaxpr.constvars)\n+  invars = *boxvars, *jaxpr.jaxpr.invars\n+  effects = make_jaxpr_effects(constvars, invars, jaxpr.jaxpr.outvars,\n+                               jaxpr.jaxpr.eqns)\n+  new_jaxpr = jaxpr.jaxpr.replace(constvars=constvars, invars=invars,\n+                                  effects=effects)\n+  return jaxpr.replace(jaxpr=new_jaxpr, consts=constvals), in_mutables\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex d0a22cd784b4..276e2b18cd40 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -1775,6 +1775,7 @@ def _move_mutable_consts(\n   constvars, mutvars = partition_list(hoist, jaxpr.constvars)\n   invars = (*jaxpr.invars, *mutvars)\n   effects = pe.make_jaxpr_effects(constvars, invars, jaxpr.outvars, jaxpr.eqns)\n+  # TODO(mattjj): debug_info must be updated...\n   jaxpr = core.Jaxpr(constvars, invars, jaxpr.outvars, jaxpr.eqns,\n                      effects, closed_jaxpr.jaxpr.debug_info)\n   return core.ClosedJaxpr(jaxpr, consts), in_mut\n@@ -2181,8 +2182,7 @@ def lower_sharding_computation(\n   The caller of this code can pass in a singleton UNSPECIFIED because the\n   number of out_avals might not be known at that time and\n   lower_sharding_computation calculates the number of out_avals so it can apply\n-  the singleton UNSPECIFIED to all out_avals.\n-  \"\"\"\n+  the singleton UNSPECIFIED to all out_avals.\"\"\"\n   auto_spmd_lowering = check_if_any_auto(\n       it.chain.from_iterable([in_shardings, out_shardings]))\n \ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex b9ce8ae09380..4df4c517090b 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -17,7 +17,7 @@\n from collections.abc import Callable, Sequence\n from functools import partial\n import inspect\n-import itertools\n+import itertools as it\n import operator\n from typing import Any, TypeVar\n import weakref\n@@ -438,11 +438,11 @@ def _merge_attrs_out(attrs_tracked, out_state, out_append):\n   out_attrs = []\n   for _, out_tree, (_, _, k) in attrs_tracked:\n     if k in (pe.ReadWrite, pe.BoxAttr):\n-      out_attrs.extend(itertools.islice(out_state_, out_tree.num_leaves))\n+      out_attrs.extend(it.islice(out_state_, out_tree.num_leaves))\n     elif k is pe.Append:\n       out_attrs.append(next(out_append_))\n     elif k is pe.ListAttr:\n-      out_attrs.extend(itertools.islice(out_append_, out_tree.num_leaves))\n+      out_attrs.extend(it.islice(out_append_, out_tree.num_leaves))\n     else:\n       assert False\n   assert next(out_state_, None) is next(out_append_, None) is None\n@@ -931,7 +931,7 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n   ys_avals = [core.unmapped_aval(length, 0, y_aval)\n               for y_aval in y_avals]\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(a), None)\n-                 for a in itertools.chain(carry_avals, ys_avals)]\n+                 for a in it.chain(carry_avals, ys_avals)]\n   del carry_avals, y_avals\n   # Create equation.\n   linear_unknown = tuple([False] * len(intensive_res) +\n@@ -1500,6 +1500,17 @@ def arrange_jaxpr_args_for_wrapped(args):\n   assert len(refs_out_matching_in_avals) == len(in_avals)\n   return refs_out_matching_in_avals, [*carry_out, *ys]\n \n+def _scan_staging(trace, *args, **params):\n+  outs = trace.default_process_primitive(scan_p, args, params)\n+  jaxpr = params['jaxpr']\n+  trace.frame.is_high = jaxpr.jaxpr.is_high\n+  invars = [trace.frame.tracer_to_var[id(t)] for t in args]\n+  var_map = dict(zip(jaxpr.jaxpr.invars, invars))\n+  final_env = {var_map[v]: ty for v, ty in\n+               jaxpr.jaxpr.final_typechange_env.items()}\n+  trace.frame.current_typechange_env.update(final_env)\n+  return outs\n+\n scan_p = core.Primitive(\"scan\")\n scan_p.multiple_results = True\n scan_p.skip_canonicalization = True\n@@ -1518,6 +1529,65 @@ def arrange_jaxpr_args_for_wrapped(args):\n pe.padding_rules[scan_p] = _scan_padding_rule\n pe.dce_rules[scan_p] = _scan_dce_rule\n state_discharge.register_partial_discharge_rule(scan_p)(_scan_state_partial_discharge_rule)\n+pe.custom_staging_rules[scan_p] = _scan_staging\n+\n+def _is_high(jaxpr, **_) -> bool:\n+  return jaxpr.jaxpr.is_high\n+scan_p.is_high = _is_high  # type: ignore\n+\n+def _to_lojax(*hi_args, jaxpr, num_carry, num_consts, linear, **params):\n+  ienv, fenv = jaxpr.jaxpr.initial_typechange_env, jaxpr.jaxpr.final_typechange_env\n+\n+  # move box binders and hi_args from consts slots to carry slots\n+  to_move = [t.mutable for t in jaxpr.in_avals[:num_consts]]\n+  jaxpr = pe.move_invars_right(jaxpr, to_move)\n+  hi_args = _move_right(hi_args, to_move)\n+  num_consts -= sum(to_move)\n+  num_carry += sum(to_move)\n+\n+  # expand num_consts, num_carry, linear according to lo types\n+  const_invars, carry_invars, _ = split_list(jaxpr.jaxpr.invars, [num_consts, num_carry])\n+  num_consts = sum(len(v.aval.lo_ty() if not v.aval.mutable\n+                       else v.aval.lo_ty_(ienv[v])) for v in const_invars)\n+  num_carry = sum(len(v.aval.lo_ty() if not v.aval.mutable\n+                      else v.aval.lo_ty_(ienv[v])) for v in carry_invars)\n+  linear = [l for v, l_ in zip(jaxpr.jaxpr.invars, linear)\n+            for l in (l_,) * len(v.aval.lo_ty() if not v.aval.mutable\n+                                 else v.aval.lo_ty_(ienv[v]))]\n+  lo_muts_out = sum(len(m.leaf_avals) for m in fenv.values())  # TODO hardcoded\n+\n+  # collect lo inputs values\n+  lo_args = [lo_val for v, x in zip(jaxpr.jaxpr.invars, hi_args)\n+             for lo_val in (v.aval.read_loval(ienv[v], x) if v.aval.mutable\n+                            else v.aval.lower_val(x))]\n+\n+  # lower the jaxpr and bind it using lo input values\n+  lo_jaxpr = pe.lower_jaxpr(jaxpr)\n+  all_outs = scan_p.bind(*lo_args, jaxpr=lo_jaxpr, num_consts=num_consts,\n+                         num_carry=num_carry, linear=tuple(linear), **params)\n+  out_mut, lo_outs = split_list(all_outs, [lo_muts_out])\n+\n+  # collect and apply mutations\n+  out_mut_ = iter(out_mut)\n+  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n+  for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n+    lo_vals = it.islice(out_mut_, len(var.aval.lo_ty_(ty)))\n+    var.aval.update_from_loval(ty, hi_args[in_idx[var]], *lo_vals)\n+  assert next(out_mut_, None) is None\n+\n+  # collect output values into hi types\n+  lo_outs_ = iter(lo_outs)\n+  hi_outs = [t.raise_val(*it.islice(lo_outs_, len(t.lo_ty())))\n+             for t in jaxpr.out_avals]\n+  assert next(lo_outs_, None) is None\n+\n+  return hi_outs\n+scan_p.to_lojax = _to_lojax\n+\n+def _move_right(lst, to_move):\n+  lst, rest = split_list(lst, [len(to_move)])\n+  left, right = partition_list(to_move, lst)\n+  return [*left, *right, *rest]\n \n def _propagate_mem_kind_scan(*xm, reverse, length, num_consts, num_carry, jaxpr,\n                              linear, unroll, _split_transpose):\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex d5286be8e0c9..c767647195b8 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -590,6 +590,8 @@ def _infer_params_impl(\n     in_type = in_avals = tuple(core.shaped_abstractify(x) for x in explicit_args)  # type: ignore\n   else:\n     in_type = in_avals  # type: ignore\n+    in_type = tuple(core.TypeChange(a, x.type_state(), None) if a.mutable  # type: ignore\n+                    else a for a, x in zip(in_type, explicit_args))\n   assert in_avals is not None\n \n   in_shardings_flat, in_layouts_flat = _process_in_axis_resources(\n@@ -705,7 +707,7 @@ def _infer_params_internal(\n   if entry.pjit_params is None:\n     p, args_flat = _infer_params_impl(\n         fun, ji, ctx_mesh, dbg, args, kwargs, in_avals=avals)\n-    if p.attrs_tracked or p.box_data:  # if attrs/boxes, don't populate cache\n+    if p.attrs_tracked or p.box_data or p.params['jaxpr'].jaxpr.is_high:\n       return p, p.consts + args_flat\n     entry.pjit_params = p\n   return entry.pjit_params, entry.pjit_params.consts + dynargs\n@@ -1407,16 +1409,14 @@ def _create_pjit_jaxpr(\n           lu.annotate(fun, cast(core.InputType, in_type)))\n       attrs_tracked = []\n     else:\n-      jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n-          fun, in_type)\n-      # assert attr_data is sentinel or attr_data matches attrs_tracked\n+      jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(fun, in_type)\n \n   if config.debug_key_reuse.value:\n     # Import here to avoid circular imports\n     from jax.experimental.key_reuse._core import check_key_reuse_jaxpr\n     check_key_reuse_jaxpr(jaxpr)\n \n-  if any(isinstance(c, core.Tracer) for c in consts):\n+  if any(isinstance(c, core.Tracer) or core.typeof(c).mutable for c in consts):\n     closed_jaxpr = pe.close_jaxpr(pe.convert_constvars_jaxpr(jaxpr))\n     final_consts = consts\n   else:\n@@ -1561,21 +1561,41 @@ def _is_high(jaxpr, **_) -> bool:\n   return jaxpr.jaxpr.is_high\n pjit_p.is_high = _is_high  # type: ignore\n \n-def _to_lojax( *hi_args, jaxpr, **params):\n-  params, num_mutants = _lojax_expand_params(jaxpr, **params)\n+def _to_lojax(*hi_args, jaxpr, **params):\n+  ienv, fenv = jaxpr.jaxpr.initial_typechange_env, jaxpr.jaxpr.final_typechange_env\n \n-  lo_args = [lo_val for t, hi_val in zip(jaxpr.in_avals, hi_args)\n-             for lo_val in t.lower_val(hi_val)]\n+  # convert closed-over boxes to explicit args\n+  jaxpr, closed_over_himutables = pe.convert_const_himutables(jaxpr)\n+  hi_args = [*closed_over_himutables, *hi_args]\n+  params = _converted_mutables_add_params(len(closed_over_himutables), **params)\n+\n+  # expand pjit params that must match number of lo inputs/outputs\n+  lo_nums_in = [len(v.aval.lo_ty() if not v.aval.mutable\n+                    else v.aval.lo_ty_(ienv[v]))\n+                for v in jaxpr.jaxpr.invars]\n+  lo_nums_out = [len(t.lo_ty()) for t in jaxpr.out_avals]\n+  lo_muts_out = sum(len(m.leaf_avals) for m in fenv.values())  # TODO hardcoded\n+  params = _lojax_expand_params(lo_nums_in, lo_nums_out, lo_muts_out, **params)\n+\n+  # collect lo input values\n+  lo_args = [lo_val for v, x in zip(jaxpr.jaxpr.invars, hi_args)\n+             for lo_val in (v.aval.read_loval(ienv[v], x) if v.aval.mutable\n+                            else v.aval.lower_val(x))]\n+\n+  # lower the jaxpr and bind it using lo input values\n   lo_jaxpr = pe.lower_jaxpr(jaxpr)\n   all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, **params)\n-  out_mut, lo_outs = split_list(all_outs, [num_mutants])\n+  out_mut, lo_outs = split_list(all_outs, [lo_muts_out])\n \n+  # collect and apply mutations\n   out_mut_ = iter(out_mut)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n   for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n-    ty.set(hi_args[in_idx[var]], *it.islice(out_mut_, len(ty.lo_ty())))\n+    lo_vals = it.islice(out_mut_, len(var.aval.lo_ty_(ty)))\n+    var.aval.update_from_loval(ty, hi_args[in_idx[var]], *lo_vals)\n   assert next(out_mut_, None) is None\n \n+  # collect output values into hi types\n   lo_outs_ = iter(lo_outs)\n   hi_outs = [t.raise_val(*it.islice(lo_outs_, len(t.lo_ty())))\n              for t in jaxpr.out_avals]\n@@ -1584,29 +1604,35 @@ def _to_lojax( *hi_args, jaxpr, **params):\n   return hi_outs\n pjit_p.to_lojax = _to_lojax\n \n+def _converted_mutables_add_params(\n+    n, *, donated_invars, in_shardings, in_layouts, **params):\n+  donated_invars = (False,) * n + donated_invars\n+  in_shardings = (UNSPECIFIED,) * n + in_shardings\n+  in_layouts = (None,) * n + in_layouts\n+  return dict(params, donated_invars=donated_invars, in_shardings=in_shardings,\n+              in_layouts=in_layouts)\n+\n def _lojax_expand_params(\n-    hi_jaxpr, *, donated_invars, in_shardings, in_layouts, out_shardings,\n-    out_layouts, **params):\n+    nums_in, nums_out, muts_out, *, donated_invars, in_shardings, in_layouts,\n+    out_shardings, out_layouts, **params):\n   # some pjit params match the length of hi_jaxpr.invars/outvars, so when\n   # lowering we must expand them to match their number of lojax types\n-  def expand(hi_tys, xs):\n-    return tuple(y for hi, x in zip(hi_tys, xs) for y in (x,) * len(hi.lo_ty()))\n-  donated_invars = expand(hi_jaxpr.in_avals , donated_invars)\n-  in_shardings   = expand(hi_jaxpr.in_avals , in_shardings  )\n-  in_layouts     = expand(hi_jaxpr.in_avals , in_layouts    )\n-  out_shardings  = expand(hi_jaxpr.out_avals, out_shardings )\n-  out_layouts    = expand(hi_jaxpr.out_avals, out_layouts   )\n+  def expand(ns, xs):\n+    return tuple(y for n, x in zip(ns, xs) for y in (x,) * n)\n+  donated_invars = expand(nums_in , donated_invars)\n+  in_shardings   = expand(nums_in , in_shardings  )\n+  in_layouts     = expand(nums_in , in_layouts    )\n+  out_shardings  = expand(nums_out, out_shardings )\n+  out_layouts    = expand(nums_out, out_layouts   )\n \n   # also, the lo_jaxpr has pure outputs corresponding to mutable hi_jaxpr types\n-  num_mutants = sum(len(hi_ty.lo_ty()) for hi_ty in\n-                    hi_jaxpr.jaxpr.final_typechange_env.values())\n-  out_shardings = (UNSPECIFIED,) * num_mutants + out_shardings\n-  out_layouts = (None,) * num_mutants + out_layouts\n+  out_shardings = (UNSPECIFIED,) * muts_out + out_shardings\n+  out_layouts = (None,) * muts_out + out_layouts\n \n   new_params = dict(params, donated_invars=donated_invars,\n                     in_shardings=in_shardings, in_layouts=in_layouts,\n                     out_shardings=out_shardings, out_layouts=out_layouts)\n-  return new_params, num_mutants\n+  return new_params\n \n \n def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n@@ -1948,6 +1974,7 @@ def pjit_staging_rule(trace, *args, **params):\n \n   jaxpr = params['jaxpr']\n   source_info = source_info_util.current()\n+  consts = []\n   if config.dynamic_shapes.value:\n     jaxpr, in_fwd, out_shardings, out_layouts = _pjit_forwarding(\n         jaxpr, params['out_shardings'], params['out_layouts'])\n@@ -1981,6 +2008,14 @@ def pjit_staging_rule(trace, *args, **params):\n         pjit_p, (*args, *consts), new_params)\n   else:\n     out_tracers = trace.default_process_primitive(pjit_p, args, params)\n+\n+  trace.frame.is_high = jaxpr.jaxpr.is_high\n+  invars = [trace.frame.tracer_to_var[id(t)] for t in it.chain(args, consts)]\n+  var_map = dict(zip(jaxpr.jaxpr.invars, invars))\n+  final_env = {var_map[v]: ty for v, ty in\n+               jaxpr.jaxpr.final_typechange_env.items()}\n+  trace.frame.current_typechange_env.update(final_env)\n+\n   return out_tracers\n pe.custom_staging_rules[pjit_p] = pjit_staging_rule\n \ndiff --git a/tests/attrs_test.py b/tests/attrs_test.py\nindex 60a3753a7ba5..90083626fb8e 100644\n--- a/tests/attrs_test.py\n+++ b/tests/attrs_test.py\n@@ -1056,7 +1056,7 @@ def f(x):\n     self.assertAllClose(box.get(), 2.0)\n \n   @parameterized.parameters([False, True])\n-  def test_grad_closrue_stop_gradient(self, jit):\n+  def test_grad_closure_stop_gradient(self, jit):\n     box = Box(0.0)\n \n     def f(x):\n@@ -1124,7 +1124,6 @@ def f(lst, x):\n       lst.append(2.0)\n       lst.append({'c': x + 3.0})\n \n-\n     tracing_ok = True\n     lst1 = List()\n     f(lst1, 0)\ndiff --git a/tests/hijax_test.py b/tests/hijax_test.py\nindex 21034d164d28..8b7d045c6c5a 100644\n--- a/tests/hijax_test.py\n+++ b/tests/hijax_test.py\n@@ -17,18 +17,19 @@\n from dataclasses import dataclass\n from functools import partial\n import itertools as it\n+from typing import Any\n import unittest\n \n-from absl.testing import absltest\n+from absl.testing import absltest, parameterized\n \n import jax\n import jax.numpy as jnp\n \n from jax._src import config\n from jax._src import core\n-from jax._src import dtypes\n from jax._src.interpreters import ad\n from jax._src.interpreters import partial_eval as pe\n+from jax._src import ad_util\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip, safe_map\n \n@@ -37,6 +38,8 @@\n map, unsafe_map = safe_map, map\n zip, unsafe_zip = safe_zip, zip\n \n+PyTreeDef = Any\n+\n \n # TODO(mattjj,dougalm): move HiPrimitive, Box, etc out of tests and into library\n class HiPrimitive(core.Primitive):\n@@ -65,124 +68,6 @@ def jvp(self, primals, tangents, **params):\n   def transpose(self, *args, **params):\n     assert False  # TODO\n \n-\n-class BoxTy(core.AbstractValue):\n-  mutable = True\n-\n-  def __init__(self, leaf_avals, treedef):\n-    self._leaf_avals = leaf_avals  # hijax avals\n-    self._treedef = treedef\n-\n-  # aval interface: hashability and str_short\n-  def __hash__(self):\n-    return hash((self._leaf_avals, self._treedef))\n-\n-  def __eq__(self, other):\n-    return (isinstance(other, BoxTy) and self._leaf_avals == other._leaf_avals\n-            and self._treedef == other._treedef)\n-\n-  def str_short(self, short_dtypes=False):\n-    return 'BoxTy'\n-\n-  # hijax interface: lower val, raise val, and low type\n-  def lo_ty(self):\n-    return [lo_aval for hi_aval in self._leaf_avals for lo_aval in hi_aval.lo_ty()]\n-\n-  def lower_val(self, box):\n-    leaf_vals, treedef = jax.tree.flatten(box._val)\n-    assert treedef == self._treedef\n-    return [lo_val for hi_aval, hi_val in zip(self._leaf_avals, leaf_vals)\n-            for lo_val in hi_aval.lower_val(hi_val)]\n-\n-  def raise_val(self, *lo_vals):\n-    lo_vals_ = iter(lo_vals)\n-    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n-               for hi_ty in self._leaf_avals]\n-    assert next(lo_vals_, None) is None\n-    return Box(jax.tree.unflatten(self._treedef, hi_vals))  # will be mutated\n-\n-  # mutable interface: get/set\n-  def get(self, box):\n-    leaf_vals, treedef = jax.tree.flatten(box._val)\n-    assert treedef == self._treedef\n-    return [lo_val for hi_ty, hi_val in zip(self._leaf_avals, leaf_vals)\n-            for lo_val in hi_ty.lower_val(hi_val)]\n-\n-  def set(self, box, *lo_vals):\n-    lo_vals_ = iter(lo_vals)\n-    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n-               for hi_ty in self._leaf_avals]\n-    assert next(lo_vals_, None) is None\n-    box._val = jax.tree.unflatten(self._treedef, hi_vals)\n-\n-  # TODO placeholder thing\n-  def to_tangent_aval(self):\n-    return core.ShapedArray((), dtypes.float0)  # TODO revise placeholder\n-\n-class Box:  # noqa: F811\n-  def __init__(self, val):\n-    self._val = val\n-\n-  @property\n-  def ty(self):\n-    leaves, treedef = jax.tree.flatten(self._val)\n-    leaf_avals = tuple(map(core.typeof, leaves))\n-    return BoxTy(leaf_avals, treedef)\n-core.pytype_aval_mappings[Box] = lambda b: b.ty\n-\n-\n-class BoxSet(HiPrimitive):\n-  multiple_results = True\n-\n-  def is_high(self, *, treedef) -> bool: return True\n-\n-  def staging(self, trace, box, *leaves, treedef):\n-    super().staging(trace, box, *leaves, treedef=treedef)\n-    avals = tuple(t.aval for t in leaves)\n-    trace.frame.final_typechange_env[trace.getvar(box)] = BoxTy(avals, treedef)\n-\n-  def abstract_eval(self, box_ty, *leaf_avals, treedef):\n-    return [], set()  # TODO better typechecking...\n-\n-  def to_lojax(_, box, *leaves, treedef):\n-    box._val = jax.tree.unflatten(treedef, leaves)\n-    return []\n-\n-  def jvp(_, primals, tangents, *, treedef):\n-    assert False  # TODO\n-\n-  def transpose(_, *args, treedef):\n-    assert False  # TODO\n-box_set_p = BoxSet('box_set')\n-\n-def box_set(box, val):\n-  leaves, treedef = jax.tree.flatten(val)\n-  box_set_p.bind(box, *leaves, treedef=treedef)\n-\n-\n-class BoxGet(HiPrimitive):\n-  multiple_results = True\n-\n-  def is_high(self) -> bool: return True\n-\n-  def abstract_eval(self, box_ty):\n-    return box_ty._leaf_avals, set()\n-\n-  def to_lojax(_, box):\n-    return jax.tree.leaves(box._val)\n-\n-  def jvp(_, primals, tangents):\n-    assert False  # TODO\n-\n-  def transpose(_, *args):\n-    assert False  # TODO\n-box_get_p = BoxGet('box_get')\n-\n-def box_get(box):\n-  leaf_vals = box_get_p.bind(box)\n-  return jax.tree.unflatten(core.typeof(box)._treedef, leaf_vals)\n-\n-\n class HijaxTest(jtu.JaxTestCase):\n \n   def test_custom_types_and_primitive(self):\n@@ -194,8 +79,6 @@ class MyArray:\n \n     @dataclass(frozen=True)\n     class MyTy(core.AbstractValue):\n-      mutable = False\n-\n       def to_tangent_aval(self):\n         return MyTy()\n       def str_short(self, short_dtypes=False):\n@@ -324,6 +207,392 @@ def f(x):\n     self.assertIsInstance(a_grad, MyArray)\n     self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n \n+\n+def new_box():\n+  (), treedef = jax.tree.flatten(None)\n+  return new_box_p.bind(treedef=treedef)\n+\n+def box_get(box):\n+  tys = box.type_state()\n+  leaf_vals = box_get_p.bind(box, avals=tys.leaf_avals)\n+  return jax.tree.unflatten(tys.treedef, leaf_vals)\n+\n+def box_set(box, val):\n+  leaves, treedef = jax.tree.flatten(val)\n+  box_set_p.bind(box, *leaves, treedef=treedef)\n+\n+@dataclass(frozen=True)\n+class BoxTypeState:\n+  leaf_avals: tuple[core.AbstractValue, ...]\n+  treedef: PyTreeDef\n+\n+  def to_tangent_aval(self):\n+    return BoxTypeState(tuple(a.to_tangent_aval() for a in self.leaf_avals),\n+                        self.treedef)\n+\n+  def normalize(self):\n+    return BoxTypeState(tuple(a.normalize() for a in self.leaf_avals),\n+                        self.treedef)\n+\n+class BoxTy(core.AbstractValue):\n+  mutable = True\n+\n+  # forwarded to value\n+  get = core.aval_method(box_get)\n+  set = core.aval_method(box_set)\n+\n+  # aval interface: hashability and str_short\n+  def __hash__(self): return hash(BoxTy)\n+  def __eq__(self, other): return isinstance(other, BoxTy)\n+\n+  def str_short(self, short_dtypes=False):\n+    return 'BoxTy'\n+\n+  # mutable interface\n+  def lo_ty_(self, box_state):\n+    return [lo_ty for t in box_state.leaf_avals for lo_ty in t.lo_ty()]\n+\n+  def new_from_loval(self, box_state: BoxTypeState, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in box_state.leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    return Box(jax.tree.unflatten(box_state.treedef, hi_vals))  # will be mutated\n+\n+  def read_loval(self, box_state: BoxTypeState, box):\n+    leaf_vals, treedef = jax.tree.flatten(box_get(box))\n+    assert treedef == box_state.treedef\n+    return [lo_val for hi_ty, hi_val in zip(box_state.leaf_avals, leaf_vals)\n+            for lo_val in hi_ty.lower_val(hi_val)]\n+\n+  def update_from_loval(self, box_state: BoxTypeState, box, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in box_state.leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    box_set(box, jax.tree.unflatten(box_state.treedef, hi_vals))\n+\n+  def to_tangent_aval(self):\n+    return BoxTy()\n+\n+class Box:  # noqa: F811\n+  def __init__(self, val):\n+    self._val = val\n+\n+  def get(self):\n+    return box_get(self)\n+\n+  def set(self, val):\n+    box_set(self, val)\n+\n+  @property\n+  def ty(self):\n+    return BoxTy()\n+\n+  def type_state(self):\n+    leaves, treedef = jax.tree.flatten(self._val)\n+    leaf_avals = tuple(map(core.typeof, leaves))\n+    return BoxTypeState(leaf_avals, treedef)\n+core.pytype_aval_mappings[Box] = lambda b: b.ty\n+\n+\n+class NewBox(HiPrimitive):\n+  def is_high(self, *, treedef) -> bool: return True\n+\n+  def staging(self, trace, *, treedef):\n+    tracer = super().staging(trace, treedef=treedef)\n+    var = trace.frame.tracer_to_var[id(tracer)]\n+    leaves, treedef = jax.tree.flatten(None)\n+    trace.frame.current_typechange_env[var] = BoxTypeState(leaves, treedef)\n+    return tracer\n+\n+  def abstract_eval(self, *, treedef):\n+    return BoxTy(), set()\n+\n+  def to_lojax(_, *, treedef):\n+    return Box(None)\n+\n+  def jvp(_, primals, tangents, *, treedef):\n+    assert False  # TODO\n+\n+  def transpose(_, *args, treedef):\n+    assert False  # TODO\n+new_box_p = NewBox('new_box')\n+\n+\n+class BoxSet(HiPrimitive):\n+  multiple_results = True\n+\n+  def is_high(self, *, treedef) -> bool: return True\n+\n+  def staging(self, trace, box_tracer, *leaves, treedef):\n+    super().staging(trace, box_tracer, *leaves, treedef=treedef)\n+    var = trace.getvar(box_tracer)\n+    avals = tuple(t.aval for t in leaves)\n+    trace.frame.current_typechange_env[var] = BoxTypeState(avals, treedef)\n+    return []\n+\n+  def abstract_eval(self, box_ty, *leaf_avals, treedef):\n+    return [], set()  # TODO better typechecking...\n+\n+  def to_lojax(_, box, *leaves, treedef):\n+    box._val = jax.tree.unflatten(treedef, leaves)\n+    return []\n+\n+  def jvp(_, primals, tangents, *, treedef):\n+    box, *vals = primals\n+    box_dot, *val_dots = tangents\n+    if type(box_dot) is ad_util.Zero:\n+      raise Exception(\"you're an idiot\")\n+    box_set_p.bind(box, *vals, treedef=treedef)\n+    box_set_p.bind(box_dot, *val_dots, treedef=treedef)\n+    return [], []\n+\n+  def transpose(_, *args, treedef):\n+    assert False  # TODO\n+box_set_p = BoxSet('box_set')\n+\n+\n+class BoxGet(HiPrimitive):\n+  multiple_results = True\n+\n+  def abstract_eval(self, box_ty, *, avals):\n+    return avals, set()\n+\n+  def to_lojax(_, box, *, avals):\n+    return jax.tree.leaves(box._val)\n+\n+  def jvp(_, primals, tangents, *, avals):\n+    (box,), (box_dot,) = primals, tangents\n+    return (box_get_p.bind(box, avals=avals),\n+            box_get_p.bind(box_dot, avals=[a.to_tangent_aval() for a in avals]))\n+\n+  def transpose(_, *args):\n+    assert False  # TODO\n+box_get_p = BoxGet('box_get')\n+\n+\n+\n+class BoxTest(jtu.JaxTestCase):\n+\n+  def test_jit_arg(self):\n+    @jax.jit\n+    def f(box, x):\n+      assert tracing_ok\n+      box.set(box.get() + x)\n+\n+    tracing_ok = True\n+    box1 = Box(1.0)\n+    f(box1, 1.)\n+    self.assertAllClose(box1.get(), 2.0)\n+\n+    tracing_ok = False\n+    box2 = Box(2.0)\n+    f(box2, 2.)\n+    self.assertAllClose(box2.get(), 4.0)\n+\n+  def test_jit_arg2(self):\n+    # set without get\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    box = Box(0.0)\n+    f(box, 1.)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+\n+  def test_jit_arg_in_pytree(self):\n+    @jax.jit\n+    def f(dct, x):\n+      assert tracing_ok\n+      box = dct['box']\n+      box.set(box.get() + x)\n+\n+    tracing_ok = True\n+    box1 = Box(1.0)\n+    f({'box': box1, 'a': 1.0}, 1.)\n+    self.assertAllClose(box1.get(), 2.0)\n+\n+    tracing_ok = False\n+    box2 = Box(2.0)\n+    f({'box': box2, 'a': 2.0}, 2.)\n+    self.assertAllClose(box2.get(), 4.0)\n+\n+    tracing_ok = True\n+    box3 = Box(3)  # int, dtype changed\n+    f({'box': box3, 'a': 2.0}, 2.)\n+    self.assertAllClose(box3.get(), 5.0)\n+\n+  def test_jit_closure(self):\n+    box = Box(1.0)\n+\n+    @jax.jit\n+    def f(x):\n+      assert tracing_ok\n+      box.set(box.get() + x)\n+\n+    tracing_ok = True\n+    f(2.0)\n+    self.assertAllClose(box.get(), 3.0)\n+    tracing_ok = False\n+    f(5.0)\n+    self.assertAllClose(box.get(), 8.0)\n+\n+  def test_jit_closure_nested(self):\n+    box = Box(5.0)\n+\n+    @jax.jit\n+    def f(x):\n+      box.set(box.get() + x)\n+\n+    @jax.jit\n+    def g(x):\n+      f(x)\n+\n+    g(3.0)\n+    self.assertAllClose(box.get(), 8.0)\n+\n+  def test_jit_closure_nested2(self):\n+    @jax.jit\n+    def h(x):\n+      box = new_box()\n+      box.set(x)\n+\n+      @jax.jit\n+      def k(x):\n+        box.set(box.get() + x)\n+\n+      k(1.0)\n+      k(1.0)\n+      return box.get()\n+\n+    ans = h(2.0)\n+    self.assertAllClose(ans, 4.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_jvp_closure_stop_gradient(self, jit):\n+    box = Box(1.0)\n+\n+    def f(x):\n+      y = 2 * x\n+      box.set(box.get() + jax.lax.stop_gradient(y))\n+      return y\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    y, y_dot = jax.jvp(f, (1.0,), (1.0,))\n+    self.assertAllClose(y, 2.0)\n+    self.assertAllClose(y_dot, 2.0)\n+    self.assertAllClose(box.get(), 3.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_jvp_arg(self, jit):\n+    def f(box, x):\n+      box.set(box.get() + x)\n+      return x\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    box = Box(5.0)\n+    box_dot = Box(1.0)\n+    y, y_dot = jax.jvp(f, (box, 2.), (box_dot, 1.))\n+    self.assertAllClose(y, 2.0)\n+    self.assertAllClose(y_dot, 1.0)\n+    self.assertAllClose(box.get(), 7.0)\n+    self.assertAllClose(box_dot.get(), 2.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_custom_vjp_plumbing(self, jit):\n+    box = Box(0.0)\n+\n+    @jax.custom_vjp\n+    def foo(x):\n+      return x\n+    def foo_fwd(x):\n+      return foo(x), None\n+    def foo_bwd(_, g):\n+      box.set(g)\n+      return g,\n+    foo.defvjp(foo_fwd, foo_bwd)\n+\n+    def f(x):\n+      x = 2 * x\n+      x = foo(x)\n+      x = 2 * x\n+      return x\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    jax.grad(f)(1.0)\n+    self.assertAllClose(box.get(), 2.0)\n+\n+  # TODO(mattjj,dougalm): make this work...\n+  # @parameterized.parameters([False, True])\n+  # def test_custom_vjp_plumbing_abstracted(self, jit):\n+  #   box = Box(0.0)\n+\n+  #   @jax.custom_vjp\n+  #   def foo(box, x):\n+  #     return x\n+  #   def foo_fwd(box, x):\n+  #     return x, box\n+  #   def foo_bwd(box, g):\n+  #     box.set(g)\n+  #     return None, g\n+  #   foo.defvjp(foo_fwd, foo_bwd)\n+\n+  #   def f(box, x):\n+  #     x = 2 * x\n+  #     x = foo(box, x)\n+  #     x = 2 * x\n+  #     return x\n+\n+  #   if jit:\n+  #     f = jax.jit(f)\n+\n+  #   jax.grad(partial(f, box))(1.0)\n+  #   self.assertAllClose(box.get(), 2.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_grad_closure_stop_gradient(self, jit):\n+    box = Box(0.0)\n+\n+    def f(x):\n+      y = x * 2\n+      box.set(box.get() + jax.lax.stop_gradient(y))\n+      return y\n+\n+    if jit:\n+      f = jax.jit(f)\n+\n+    g = jax.grad(f)(1.0)\n+    self.assertAllClose(g, 2.0)\n+    self.assertAllClose(box.get(), 2.0)\n+\n+  @parameterized.parameters([False, True])\n+  def test_scan_basic(self, jit):\n+    box = Box(1.0)\n+\n+    def double_it_10():\n+      def body(_, __):\n+        box.set(box.get() * 2)\n+        return None, None\n+      _, _ = jax.lax.scan(body, None, None, length=10)\n+\n+    if jit:\n+      double_it_10 = jax.jit(double_it_10)\n+\n+    double_it_10()\n+    self.assertAllClose(box.get(), 1024., check_dtypes=False)\n+\n+  # TODO error-checking tests from attrs_test.py\n+\n+  ###\n+\n   def test_box_autodiff(self):\n     if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n \n@@ -336,7 +605,7 @@ def abstract_eval(_, box_aval, x_aval):\n         return x_aval, set()\n \n       def to_lojax(_, box, x):\n-        assert False  # TODO\n+        return x\n \n       def jvp(_, primals, tangents):\n         box, x = primals\n@@ -351,14 +620,6 @@ def transpose(self, *args):\n     def stash_tangents(box, x):\n       return stash_tangents_p.bind(box, x)\n \n-    @jax.jit\n-    def f(box, x):\n-      box_set(box, x)\n-\n-    box = Box(0.0)\n-    f(box, 1.)\n-    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n-\n     @jax.jit\n     def f(box, x):\n       x = stash_tangents(box, x)\n@@ -449,5 +710,26 @@ def f(box):\n     self.assertAllClose(b_.arr, 2, check_dtypes=False)\n \n \n+class ListTy(core.AbstractValue):\n+  mutable = True\n+\n+  # forwarded to value\n+  get = core.aval_method(box_get)\n+  set = core.aval_method(box_set)\n+\n+  # aval interface: hashability and str_short\n+  def __hash__(self): return hash(BoxTy)\n+  def __eq__(self, other): return isinstance(other, BoxTy)\n+\n+  def str_short(self, short_dtypes=False):\n+    return 'ListTy'\n+\n+  # TODO\n+\n+class ListTest(jtu.JaxTestCase):\n+  ...\n+\n+\n+\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add support for inout arguments\n\nThey are passed in between inputs and outputs to the kernel body and returned\nafter outputs from the kernel.\n\nPiperOrigin-RevId: 764898480",
        "output": [
            "```diff\nCommit: 5b1372998c1c20f1e938ee4b715fd96d0cd54feb\nDate: 2025-05-29T22:00:43Z\nURL: https://github.com/jax-ml/jax/commit/5b1372998c1c20f1e938ee4b715fd96d0cd54feb\nFiles changed: 4\nAdditions: +59, Deletions: -14\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 84caff69a090..003fa0419f63 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -907,6 +907,7 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n           block=block,\n           in_shapes=(*in_shapes, *semaphores_shape),\n           out_shape=(*out_shapes, *semaphores_shape),\n+          inout_shape=(),\n           smem_scratch_shape=scratch_buffers,\n           lowering_semantics=lowering_semantics,\n           module_name=mlir.sanitize_name(debug_info.func_name),\ndiff --git a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\nindex a14ccbb7daa9..ccbe4d36edc9 100644\n--- a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n+++ b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n@@ -105,6 +105,7 @@ def zero_init_gmem_scratch():\n       *args, *scratch_args,\n       module=module,\n       out_types=lowering_result.new_out_shapes,\n+      inout_types=(),\n       input_output_aliases=input_output_aliases,\n       use_custom_barrier=False, # False until we add get_barrier_semaphore() feature\n   )\ndiff --git a/jax/experimental/mosaic/gpu/core.py b/jax/experimental/mosaic/gpu/core.py\nindex 193fd1bd3589..79a0cd56328b 100644\n--- a/jax/experimental/mosaic/gpu/core.py\n+++ b/jax/experimental/mosaic/gpu/core.py\n@@ -26,6 +26,7 @@\n from typing import Any, Callable, Generic, TypeVar\n import weakref\n \n+import itertools\n import jax\n from jax._src import sharding_impls\n from jax._src.interpreters import mlir\n@@ -124,9 +125,12 @@ def supports_cross_device_collectives():\n \n \n @mosaic_gpu_p.def_abstract_eval\n-def _mosaic_gpu_abstract_eval(*_, module, out_types):\n+def _mosaic_gpu_abstract_eval(*_, module, out_types, inout_types):\n   del module  # Unused.\n-  return [jax._src.core.ShapedArray(t.shape, t.dtype) for t in out_types]\n+  return [\n+      jax._src.core.ShapedArray(t.shape, t.dtype)\n+      for t in itertools.chain(out_types, inout_types)\n+  ]\n \n \n def _has_communication(module, **_):\n@@ -146,6 +150,7 @@ def _mosaic_gpu_lowering_rule(\n     *args,\n     module,\n     out_types,\n+    inout_types,\n     input_output_aliases: tuple[tuple[int, int], ...] = (),\n     use_custom_barrier: bool = False,\n ):\n@@ -170,8 +175,19 @@ def _mosaic_gpu_lowering_rule(\n     else:\n       raise NotImplementedError(f\"Unsupported sharding context: {axis_context}\")\n \n-  assert len(args) == len(ctx.avals_in)\n-  assert len(out_types) == len(ctx.avals_out)\n+  if inout_types:\n+    if input_output_aliases:\n+      raise ValueError(\n+          \"input_output_aliases and inout_types are mutually exclusive\"\n+      )\n+    num_inputs = len(ctx.avals_in)\n+    num_outputs = len(ctx.avals_out)\n+    input_output_aliases = tuple(\n+        (num_inputs - 1 - i, num_outputs - 1 - i)\n+        for i in range(len(inout_types))\n+    )\n+  assert len(ctx.avals_in) == len(args)\n+  assert len(ctx.avals_out) == len(out_types) + len(inout_types)\n   module = _run_serde_pass(\n       module,\n       serialize=True,\n@@ -562,6 +578,7 @@ def _lower_as_gpu_kernel(\n     block: tuple[int, int, int],\n     in_shapes: tuple[Any, ...],\n     out_shape,\n+    inout_shape,\n     smem_scratch_shape: ShapeTree | Union[ShapeTree],\n     lowering_semantics: LoweringSemantics,\n     module_name: str,\n@@ -576,13 +593,14 @@ def _shape_to_ref_ty(shape: jax.ShapeDtypeStruct) -> ir.MemRefType:\n     return ir.MemRefType.get(shape.shape, utils.dtype_to_ir_type(shape.dtype))\n \n   in_ref_tys = [_shape_to_ref_ty(t) for t in in_shapes]\n+  inout_ref_tys = [_shape_to_ref_ty(t) for t in inout_shape]\n \n   unwrap_output_tuple = False\n   if isinstance(out_shape, list):\n     out_shape = tuple(out_shape)\n   elif not isinstance(out_shape, tuple):\n     out_shape = (out_shape,)\n-    unwrap_output_tuple = True\n+    unwrap_output_tuple = not inout_shape\n   out_ref_tys = [_shape_to_ref_ty(t) for t in out_shape]\n   if prof_spec is not None:\n     out_shape = (*out_shape, prof_spec.jax_buffer_type(grid, block))\n@@ -610,19 +628,18 @@ def main(token_ptr, buffers):\n       nonlocal launch_ctx\n       token = builtin.unrealized_conversion_cast([token_ty], [token_ptr])\n       arg_refs = []\n-      for i, ref_ty in enumerate([*in_ref_tys, *out_ref_tys]):\n+      # XLA will pass in inout refs again as outputs, but we ignore them.\n+      for i, ref_ty in enumerate([*in_ref_tys, *inout_ref_tys, *out_ref_tys]):\n         ptr = llvm.LoadOp(ptr_ty, llvm.GEPOp(ptr_ty, buffers, [], [i], ptr_ty, llvm.GEPNoWrapFlags.none))\n         arg_refs.append(utils.ptr_as_memref(ptr, ir.MemRefType(ref_ty)))\n-      in_refs = arg_refs[:len(in_ref_tys)]\n-      out_refs = arg_refs[len(in_ref_tys):]\n-      prof_buffer = out_refs.pop() if prof_spec is not None else None\n+      prof_buffer = arg_refs.pop() if prof_spec is not None else None\n       with _launch(\n           token, grid, cluster, block, smem_scratch_shape,\n           lowering_semantics, module, prof_spec, prof_buffer\n       ) as (_launch_ctx, smem_refs):\n         nonlocal launch_ctx\n         launch_ctx = _launch_ctx\n-        body(launch_ctx, *in_refs, *out_refs, smem_refs)\n+        body(launch_ctx, *arg_refs, smem_refs)\n     main.func_op.attributes[\"llvm.emit_c_interface\"] = ir.UnitAttr.get()\n   sym_tab = ir.SymbolTable(module.operation)\n   sym_tab.insert(main.func_op)\n@@ -680,16 +697,22 @@ def as_gpu_kernel(\n     kernel_name: str | None = None,\n     ir_version: int | None = None,\n     thread_semantics: LoweringSemantics = LoweringSemantics.Lane,\n+    inout_shape = (),\n ):\n   if isinstance(in_shape, list):\n     in_shape = tuple(in_shape)\n   elif not isinstance(in_shape, tuple):\n     in_shape = (in_shape,)\n+  if isinstance(inout_shape, list):\n+    inout_shape = tuple(inout_shape)\n+  elif not isinstance(inout_shape, tuple):\n+    inout_shape = (inout_shape,)\n \n   module, out_shape, unwrap_output_tuple, launch_ctx = (\n       _lower_as_gpu_kernel(\n-          body, grid, cluster, block, in_shape, out_shape, smem_scratch_shape,\n-          thread_semantics, module_name, kernel_name, prof_spec\n+          body, grid, cluster, block, in_shape, out_shape, inout_shape,\n+          smem_scratch_shape, thread_semantics, module_name, kernel_name,\n+          prof_spec\n       )\n   )\n \n@@ -711,7 +734,7 @@ def as_gpu_kernel(\n   if launch_ctx.is_device_collective and not supports_cross_device_collectives():\n     raise RuntimeError(\"Kernel is a cross-device collective but no support is available.\")\n \n-  expected_arg_tys, expected_arg_treedef = jax.tree.flatten(in_shape)\n+  expected_arg_tys, expected_arg_treedef = jax.tree.flatten((*in_shape, *inout_shape))\n   def _check_args(*args):\n     arg_treedef = jax.tree.structure(args)\n     if arg_treedef != expected_arg_treedef:\n@@ -735,7 +758,7 @@ def _check_args(*args):\n         )\n \n   def bind(*args) -> Any:\n-    return mosaic_gpu_p.bind(*args, module=module, out_types=out_shape)\n+    return mosaic_gpu_p.bind(*args, module=module, out_types=out_shape, inout_types=inout_shape)\n \n   if prof_spec is not None:\n     @jax.jit\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 99b7d67cd691..a56aa04f6f60 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -3543,6 +3543,26 @@ def test_pass_is_registered(self):\n       pipeline.run(module.operation)\n \n \n+class ApiTest(TestCase):\n+\n+  def test_inout(self):\n+    def kernel(ctx, src, inout, dst, smem):\n+      val = memref.load(inout, [])\n+      gpu.barrier()\n+      new_val = arith.constant(ir.IntegerType.get_signless(32), 42)\n+      memref.store(new_val, inout, [])\n+      x = mgpu.FragmentedArray.load_strided(src, is_signed=True)\n+      (x + val).store_untiled(dst)\n+    x = jnp.arange(128, dtype=jnp.int32)\n+    y = jnp.asarray(2.0, dtype=jnp.int32)\n+    kernel = mgpu.as_gpu_kernel(\n+        kernel, (1, 1, 1), (128, 1, 1), x, x, (), inout_shape=y,\n+    )\n+    xo, yo = kernel(x, y)\n+    np.testing.assert_array_equal(xo, x + 2.0)\n+    np.testing.assert_array_equal(yo, jnp.asarray(42, dtype=jnp.int32))\n+\n+\n if hp is not None:\n   @hps.composite\n   def tiled_layouts(\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas] Add a base class for custom BufferedRef implementations.\n\nPiperOrigin-RevId: 764889905",
        "output": [
            "```diff\nCommit: 81769209c08fe6be844ab703c35e7eb31bc1c54b\nDate: 2025-05-29T21:40:22Z\nURL: https://github.com/jax-ml/jax/commit/81769209c08fe6be844ab703c35e7eb31bc1c54b\nFiles changed: 2\nAdditions: +149, Deletions: -100\ndiff --git a/jax/_src/pallas/mosaic/pipeline.py b/jax/_src/pallas/mosaic/pipeline.py\nindex 4ef22179260b..f4dab313fb6f 100644\n--- a/jax/_src/pallas/mosaic/pipeline.py\n+++ b/jax/_src/pallas/mosaic/pipeline.py\n@@ -242,9 +242,134 @@ def _get_dim_size(bd):\n   block_shape_nones = tuple(_get_dim_size(x) for x in spec.block_shape)\n   return tuple(x for x in block_shape_nones if x is not None)\n \n+\n+class BufferedRefBase:\n+  \"\"\"Abstract interface for BufferedRefs.\"\"\"\n+\n+  @property\n+  def spec(self) -> pl.BlockSpec:\n+    raise NotImplementedError()\n+\n+  @property\n+  def buffer_type(self) -> BufferType:\n+    raise NotImplementedError()\n+\n+  @property\n+  def is_input(self):\n+    return self.buffer_type in [\n+        BufferType.INPUT,\n+        BufferType.ACCUMULATOR,\n+        BufferType.INPUT_OUTPUT,\n+    ]\n+\n+  @property\n+  def is_output(self):\n+    return self.buffer_type in [\n+        BufferType.OUTPUT,\n+        BufferType.ACCUMULATOR,\n+        BufferType.INPUT_OUTPUT,\n+    ]\n+\n+  @property\n+  def is_accumulator(self):\n+    return self.buffer_type == BufferType.ACCUMULATOR\n+\n+  @property\n+  def is_input_output(self):\n+    return self.buffer_type == BufferType.INPUT_OUTPUT\n+\n+  @property\n+  def is_manual(self):\n+    return self.buffer_type == BufferType.MANUAL\n+\n+  def init_slots(self):\n+    \"\"\"Initialize slot indices.\"\"\"\n+    raise NotImplementedError()\n+\n+  def swap_slots(self):\n+    \"\"\"Switch to the next slot.\"\"\"\n+    raise NotImplementedError()\n+\n+  @property\n+  def block_shape(self) -> Sequence[pl.BlockDim | int | None] | None:\n+    return self.spec.block_shape\n+\n+  @property\n+  def compute_index(self):\n+    return self.spec.index_map\n+\n+  def get_dma_slice(self, src_shape, src_dtype, grid_indices):\n+    # We need to handle blocks that might go OOB in the src array. An in bounds\n+    # block looks like this (for array shape (600, 600) and block shape\n+    # (256, 256)):\n+    #\n+    # +--------------+------------------|\n+    # | Block (0,0)  |                  |\n+    # | (256, 256)   |                  |\n+    # +--------------+                  |\n+    # |    A (600, 600)                 |\n+    # |                                 |\n+    # +---------------------------------+\n+    #\n+    # For in-bounds blocks, we don't need to do anything special.\n+    # An out-of-bounds block looks like this:\n+    #\n+    # +--------------+------------------|\n+    # |                                 |\n+    # |                                 |\n+    # +                                 |\n+    # |    A (600, 600)                 |\n+    # +--------------+                  |\n+    # | Block (2,0)  |                  |\n+    # + --------------------------------|\n+    # | XXXXXXXXXX   |\n+    # +--------------+\n+    # where the X's indicate where the block is out of bounds.\n+    #\n+    # When we have an out of bounds block like this, we need to truncate it to\n+    # a tile boundary (tiles are (8, 128) along the two minormost dimensions).\n+    # In this case, we'll have a block that is indexing the\n+    # 512:768 elements of A along the first dimension. We need to convert 768\n+    # into 600 (600 % 8 == 0), so our indexing will look like this:\n+\n+    # +--------------+------------------|\n+    # |                                 |\n+    # |                                 |\n+    # +                                 |\n+    # |    A (600, 600)                 |\n+    # +--------------+                  |\n+    # | Block (2,0)  |                  |\n+    # + --------------------------------|\n+    # where it is now a (88, 256) sized block.\n+    #\n+    # Suppose A is now (601, 600), instead of picking a (88, 256)-sized block\n+    # for the last iteration on that dimension, we will pick the next highest\n+    # tile multiple, i.e. (96, 256).\n+    if len(src_shape) < 2:\n+      raise NotImplementedError(\"Must use >1D values.\")\n+\n+    tiling = _make_tiling(src_shape, src_dtype)\n+    block_indices = self.compute_index(*grid_indices)\n+    return tuple(\n+        _make_block_slice(bi, bs, ss, t)\n+        for bi, bs, ss, t in zip(\n+            block_indices, self.block_shape, src_shape, tiling, strict=True\n+        )\n+    )\n+\n+  def bind_existing_ref(self, window_ref, indices):\n+    \"\"\"For handling VMEM references, the pipeline aliases the existing ref.\"\"\"\n+    del window_ref, indices\n+    return self\n+\n+  def with_spec(self, spec: pl.BlockSpec) -> 'BufferedRefBase':\n+    \"\"\"Returns a new BufferedRefBase with the given block spec.\"\"\"\n+    raise NotImplementedError()\n+\n+\n @tree_util.register_pytree_node_class\n @dataclasses.dataclass(frozen=True)\n-class BufferedRef:\n+class BufferedRef(BufferedRefBase):\n   \"\"\"A helper class to automate VMEM double buffering in pallas pipelines.\n \n   Attributes:\n@@ -257,7 +382,6 @@ class BufferedRef:\n       reference, this simply points to the existing ref.\n     accum_ref: accumulating buffer used by accumulator BufferedRefs.\n     current_slot: current slot index to the working buffer.\n-    next_slot: slot that will point to the working buffer in the next iteration.\n     sem_recvs: Double buffered semaphores for input DMAs.\n     sem_sends: Double buffered semaphores for output DMAs.\n     block_shape: passthrough property for the BlockSpec's block_shape.\n@@ -272,33 +396,37 @@ class BufferedRef:\n     swap: Tracks whether the BufferedRef slots need to be swapped before next\n       copy.\n   \"\"\"\n-  spec: pl.BlockSpec       # static metadata\n+  _spec: pl.BlockSpec       # static metadata\n   dtype: Any               # static metadata\n-  buffer_type: BufferType  # static metadata\n+  _buffer_type: BufferType  # static metadata\n   window_ref: ArrayRef | None\n   accum_ref: ArrayRef | None\n   current_slot: ArrayRef | None\n-  # TODO(ramiroleal): Unused by class. Remove argument from\n-  # BufferedRef instantiations.\n-  next_slot: ArrayRef | None\n   sem_recvs: SemaphoreTuple | None\n   sem_sends: SemaphoreTuple | None\n   # TODO(ramiroleal): Improve prefetch/postyeet interface to avoid\n   # using this ref.\n   swap: ArrayRef | None\n \n+  @property\n+  def spec(self):\n+    return self._spec\n+\n+  @property\n+  def buffer_type(self):\n+    return self._buffer_type\n+\n   def tree_flatten(self):\n     return (\n         (\n             self.window_ref,\n             self.accum_ref,\n             self.current_slot,\n-            self.next_slot,\n             self.sem_recvs,\n             self.sem_sends,\n             self.swap,\n         ),\n-        (self.spec, self.dtype, self.buffer_type),\n+        (self._spec, self.dtype, self._buffer_type),\n     )\n \n   @classmethod\n@@ -334,13 +462,12 @@ def create(cls, spec: pl.BlockSpec, dtype, buffer_type, needs_swap_ref=True\n       # reference is already in VMEM, we just need allocate the accumulation\n       # buffer and we will refer to the original reference slices directly.\n       return cls(\n-          spec=spec,\n+          _spec=spec,\n           dtype=dtype,\n-          buffer_type=buffer_type,\n+          _buffer_type=buffer_type,\n           window_ref=None,  # to be bound to existing ref by the pipeline routine\n           accum_ref=accum_ref,\n           current_slot=None,\n-          next_slot=None,\n           sem_recvs=None,\n           sem_sends=None,\n           swap=None,\n@@ -348,13 +475,12 @@ def create(cls, spec: pl.BlockSpec, dtype, buffer_type, needs_swap_ref=True\n     else:\n       memory_space = SMEM if spec.memory_space == SMEM else VMEM\n       return cls(\n-          spec=spec,\n+          _spec=spec,\n           dtype=dtype,\n-          buffer_type=buffer_type,\n+          _buffer_type=buffer_type,\n           window_ref=memory_space((2,) + block_shape, dtype),\n           accum_ref=accum_ref,\n           current_slot=SMEM((1,), jnp.int32),\n-          next_slot=None,\n           sem_recvs=(\n               None\n               if buffer_type is BufferType.OUTPUT\n@@ -396,6 +522,10 @@ def compute_index(self):\n   def memory_space(self):\n     return self.spec.memory_space\n \n+  def with_spec(self, spec: pl.BlockSpec) -> 'BufferedRef':\n+    \"\"\"Returns a new BufferedRef with the given block spec.\"\"\"\n+    return dataclasses.replace(self, _spec=spec)\n+\n   @property\n   def current_ref(self):\n     buffer_slice = tuple(\n@@ -409,30 +539,6 @@ def current_ref(self):\n     else:\n       return self.window_ref.at[(self.current_slot_index, *buffer_slice)]\n \n-  @property\n-  def is_input(self):\n-    return self.buffer_type in [\n-        BufferType.INPUT,\n-        BufferType.ACCUMULATOR,\n-        BufferType.INPUT_OUTPUT,\n-    ]\n-\n-  @property\n-  def is_output(self):\n-    return self.buffer_type in [\n-        BufferType.OUTPUT,\n-        BufferType.ACCUMULATOR,\n-        BufferType.INPUT_OUTPUT,\n-    ]\n-\n-  @property\n-  def is_accumulator(self):\n-    return self.buffer_type == BufferType.ACCUMULATOR\n-\n-  @property\n-  def is_input_output(self):\n-    return self.buffer_type == BufferType.INPUT_OUTPUT\n-\n   @property\n   def current_slot_index(self):\n     \"\"\"Index in double buffer corresponding to the current slot.\"\"\"\n@@ -491,65 +597,6 @@ def swap_slots(self):\n     if self.swap is not None:\n       self.swap[0] = False\n \n-  def get_dma_slice(self, src_shape, src_dtype, grid_indices):\n-    # We need to handle blocks that might go OOB in the src array. An in bounds\n-    # block looks like this (for array shape (600, 600) and block shape\n-    # (256, 256)):\n-    #\n-    # +--------------+------------------|\n-    # | Block (0,0)  |                  |\n-    # | (256, 256)   |                  |\n-    # +--------------+                  |\n-    # |    A (600, 600)                 |\n-    # |                                 |\n-    # +---------------------------------+\n-    #\n-    # For in-bounds blocks, we don't need to do anything special.\n-    # An out-of-bounds block looks like this:\n-    #\n-    # +--------------+------------------|\n-    # |                                 |\n-    # |                                 |\n-    # +                                 |\n-    # |    A (600, 600)                 |\n-    # +--------------+                  |\n-    # | Block (2,0)  |                  |\n-    # + --------------------------------|\n-    # | XXXXXXXXXX   |\n-    # +--------------+\n-    # where the X's indicate where the block is out of bounds.\n-    #\n-    # When we have an out of bounds block like this, we need to truncate it to\n-    # a tile boundary (tiles are (8, 128) along the two minormost dimensions).\n-    # In this case, we'll have a block that is indexing the\n-    # 512:768 elements of A along the first dimension. We need to convert 768\n-    # into 600 (600 % 8 == 0), so our indexing will look like this:\n-\n-    # +--------------+------------------|\n-    # |                                 |\n-    # |                                 |\n-    # +                                 |\n-    # |    A (600, 600)                 |\n-    # +--------------+                  |\n-    # | Block (2,0)  |                  |\n-    # + --------------------------------|\n-    # where it is now a (88, 256) sized block.\n-    #\n-    # Suppose A is now (601, 600), instead of picking a (88, 256)-sized block\n-    # for the last iteration on that dimension, we will pick the next highest\n-    # tile multiple, i.e. (96, 256).\n-    if len(src_shape) < 2:\n-      raise NotImplementedError(\"Must use >1D values.\")\n-\n-    tiling = _make_tiling(src_shape, src_dtype)\n-    block_indices = self.compute_index(*grid_indices)\n-    return tuple(\n-        _make_block_slice(bi, bs, ss, t)\n-        for bi, bs, ss, t in zip(\n-            block_indices, self.block_shape, src_shape, tiling, strict=True\n-        )\n-    )\n-\n   def copy_in(self, src_ref, grid_indices):\n     \"\"\"Starts copy of HBM dma slice into the current slot.\"\"\"\n     assert self.is_input\n@@ -674,7 +721,8 @@ def accumulate(self):\n # Helper to tree map over BufferedRefs as leaves.\n map_brefs = functools.partial(\n     jax.tree.map,\n-    is_leaf=lambda x: isinstance(x, BufferedRef))\n+    is_leaf=lambda x: isinstance(x, BufferedRefBase)\n+)\n \n \n def _filter_indices(\n@@ -922,7 +970,7 @@ def _end():\n         buffered_ref.wait_out(dst_ref, self.indices)\n \n   def swap_slots(self, buffered_ref, hbm_ref, schedule=None):\n-    if buffered_ref.swap is not None:\n+    if isinstance(buffered_ref, BufferedRef) and buffered_ref.swap is not None:\n       swap = buffered_ref.swap[0]\n     else:\n       # If we are not using an SMEM `swap` tensor to keep track of\ndiff --git a/jax/experimental/pallas/tpu.py b/jax/experimental/pallas/tpu.py\nindex 401b2fe66c45..e27fdaaadd8f 100644\n--- a/jax/experimental/pallas/tpu.py\n+++ b/jax/experimental/pallas/tpu.py\n@@ -30,6 +30,7 @@\n from jax._src.pallas.mosaic.helpers import run_on_first_core as run_on_first_core\n from jax._src.pallas.mosaic.lowering import LoweringException as LoweringException\n from jax._src.pallas.mosaic.pipeline import BufferedRef as BufferedRef\n+from jax._src.pallas.mosaic.pipeline import BufferedRefBase as BufferedRefBase\n from jax._src.pallas.mosaic.pipeline import emit_pipeline as emit_pipeline\n from jax._src.pallas.mosaic.pipeline import emit_pipeline_with_allocations as emit_pipeline_with_allocations\n from jax._src.pallas.mosaic.pipeline import get_pipeline_schedule as get_pipeline_schedule\n\n```"
        ],
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add non-collective blackwell matmul example\n\nPiperOrigin-RevId: 764889122",
        "output": [
            "```diff\nCommit: a808fe89efcd4929c8c84bb81bd04156b9199e9e\nDate: 2025-05-29T21:38:12Z\nURL: https://github.com/jax-ml/jax/commit/a808fe89efcd4929c8c84bb81bd04156b9199e9e\nFiles changed: 4\nAdditions: +341, Deletions: -0\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex a56733a89f60..84caff69a090 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -1543,6 +1543,8 @@ def _slice_lowering_rule(\n \n \n @register_lowering_rule(lax.select_n_p, mgpu.LoweringSemantics.Lane)\n+@register_lowering_rule(lax.select_n_p, mgpu.LoweringSemantics.Lane,\n+                        gpu_core.PrimitiveSemantics.Warp)\n @register_lowering_rule(lax.select_n_p, mgpu.LoweringSemantics.Warpgroup)\n def _select_n_lowering_rule(ctx: LoweringRuleContext, pred, *cases):\n   if len(cases) != 2:\n@@ -1551,6 +1553,10 @@ def _select_n_lowering_rule(ctx: LoweringRuleContext, pred, *cases):\n         f\" {len(cases)}\"\n     )\n   pred_aval, *cases_avals = ctx.avals_in\n+  if ctx.module_ctx.primitive_semantics == gpu_core.PrimitiveSemantics.Warp:\n+    if not all(aval.shape == () for aval in ctx.avals_in):\n+      raise NotImplementedError(\n+          \"Can only select on scalars in warp-level lowering.\")\n   [out_aval] = ctx.avals_out\n   if ctx.module_ctx.lowering_semantics == mgpu.LoweringSemantics.Lane:\n     pred = _ensure_fa(pred, pred_aval.dtype)\ndiff --git a/jax/experimental/pallas/ops/gpu/blackwell_matmul_mgpu.py b/jax/experimental/pallas/ops/gpu/blackwell_matmul_mgpu.py\nnew file mode 100644\nindex 000000000000..df8365f843a8\n--- /dev/null\n+++ b/jax/experimental/pallas/ops/gpu/blackwell_matmul_mgpu.py\n@@ -0,0 +1,230 @@\n+# Copyright 2025 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Matrix Multiplication kernel for Blackwell GPUs.\"\"\"\n+import dataclasses\n+import functools\n+import itertools\n+import jax\n+from jax import lax\n+from jax._src import test_util as jtu  # noqa: F401\n+from jax.experimental.mosaic.gpu import profiler\n+import jax.experimental.pallas as pl\n+import jax.experimental.pallas.mosaic_gpu as plgpu\n+import jax.numpy as jnp\n+import numpy as np\n+\n+\n+@dataclasses.dataclass(frozen=True)\n+class TuningConfig:\n+  block_m: int\n+  block_n: int\n+  block_k: int\n+  max_concurrent_steps: int\n+  collective: bool\n+\n+\n+def _find_swizzle(dim_size_bits: int):\n+  \"\"\"Finds the largest swizzle that fits the dimension size.\"\"\"\n+  for swizzle_bytes in (128, 64, 32, 16):\n+    if dim_size_bits % (swizzle_bytes * 8) == 0:\n+      return swizzle_bytes\n+  raise ValueError(\n+      f\"Dimension size has {dim_size_bits} bits, which is not a multiple of 128\"\n+  )\n+\n+\n+def matmul_kernel(a, b, config: TuningConfig):\n+  dtype = a.dtype\n+  if a.dtype != b.dtype:\n+    raise ValueError(\n+        f\"Matmul LHS and RHS have incompatible dtypes {a.dtype} vs {b.dtype}\"\n+    )\n+  m, k = a.shape\n+  k2, n = b.shape\n+  if k != k2:\n+    raise ValueError(\n+        f\"Matmul LHS and RHS have incompatible shapes {a.shape} vs {b.shape}\"\n+    )\n+  collective = config.collective\n+  if collective:\n+    raise ValueError(\"Collective matmul is not supported yet.\")\n+  block_m, block_n, block_k = (config.block_m, config.block_n, config.block_k)\n+  swizzle = _find_swizzle(block_k * jnp.dtype(dtype).itemsize * 8)\n+  swizzle_elems = swizzle // jnp.dtype(dtype).itemsize\n+  transforms = (\n+      plgpu.TilingTransform((8, swizzle_elems)),\n+      plgpu.SwizzleTransform(swizzle),\n+  )\n+  block_lhs = (block_m, block_k)\n+  block_rhs = (block_k, block_n)\n+  block_out = (block_m, block_n)\n+  if m % block_m != 0:\n+    raise ValueError(f\"{m=} must be divisible by {block_m=}\")\n+  if n % block_n != 0:\n+    raise ValueError(f\"{n=} must be divisible by {block_n=}\")\n+  if k % block_k != 0:\n+    raise ValueError(f\"{k=} must be divisible by {block_k=}\")\n+  m_iters = m // block_m\n+  n_iters = n // block_n\n+  k_iters = k // block_k\n+  max_concurrent_steps = config.max_concurrent_steps\n+\n+  def kernel(a_gmem, b_gmem, out_gmem,\n+             a_smem, b_smem, acc_tmem, acc_smem,\n+             a_tma_barrier, b_tma_barrier, consumed_barrier):\n+    m_index = lax.axis_index(\"m\")\n+    n_index = lax.axis_index(\"n\")\n+    slice_m = pl.ds(m_index * block_m, block_m)\n+    slice_n = pl.ds(n_index * block_n, block_n)\n+    acc_slice_m = pl.ds(m_index * block_m, block_m)\n+    acc_slice_n = pl.ds(n_index * block_n, block_n)\n+\n+    @pl.core_map(plgpu.WarpMesh(axis_name=\"warp\"))\n+    def _per_warp():\n+      warp_id = lax.axis_index(\"warp\")\n+\n+      @pl.when(warp_id == 0)\n+      def _memory():\n+        def _loop_body(ki, _):\n+          slot = lax.rem(ki, max_concurrent_steps)\n+\n+          @pl.when(ki >= max_concurrent_steps)\n+          def _():\n+            plgpu.barrier_wait(consumed_barrier.at[slot])\n+\n+          slice_k = pl.ds(ki * block_k, block_k)\n+          plgpu.copy_gmem_to_smem(\n+              a_gmem.at[slice_m, slice_k],\n+              a_smem.at[slot],\n+              a_tma_barrier.at[slot],\n+          )\n+          plgpu.copy_gmem_to_smem(\n+              b_gmem.at[slice_k, slice_n],\n+              b_smem.at[slot],\n+              b_tma_barrier.at[slot],\n+          )\n+\n+        lax.fori_loop(0, k_iters, _loop_body, None)\n+\n+      @pl.when(warp_id == 1)\n+      def _compute():\n+        def _loop_body(ki, _):\n+          slot = lax.rem(ki, max_concurrent_steps)\n+          plgpu.barrier_wait(a_tma_barrier.at[slot])\n+          plgpu.barrier_wait(b_tma_barrier.at[slot])\n+          is_last_iter = ki >= k_iters - 1\n+          barrier_slot = lax.select_n(is_last_iter,\n+                                      slot, max_concurrent_steps)\n+          plgpu.tcgen05_mma(\n+              acc_tmem,\n+              a_smem.at[slot],\n+              b_smem.at[slot],\n+              consumed_barrier.at[barrier_slot],\n+              accumulate=(ki > 0),\n+          )\n+        lax.fori_loop(0, k_iters, _loop_body, None)\n+\n+    plgpu.barrier_wait(consumed_barrier.at[max_concurrent_steps])\n+    acc_smem[...] = acc_tmem[...].astype(dtype)\n+    plgpu.commit_smem()\n+    plgpu.copy_smem_to_gmem(\n+        acc_smem, out_gmem.at[acc_slice_m, acc_slice_n]\n+    )\n+    plgpu.wait_smem_to_gmem(0)\n+\n+  f = plgpu.kernel(\n+      kernel,\n+      out_shape=jax.ShapeDtypeStruct((m, n), dtype),\n+      grid=(m_iters, n_iters),\n+      grid_names=(\"m\", \"n\"),\n+      # TODO(justinfu): Add collective support.\n+      cluster_names=(),\n+      cluster=(),\n+      scratch_shapes=(   # type: ignore\n+        plgpu.SMEM(\n+            (max_concurrent_steps, *block_lhs), dtype, transforms=transforms\n+        ),\n+        plgpu.SMEM(\n+            (max_concurrent_steps, *block_rhs), dtype, transforms=transforms\n+        ),\n+        plgpu.TMEM(block_out, jnp.float32, collective=collective),\n+        plgpu.SMEM(block_out, dtype, transforms=transforms),\n+        plgpu.Barrier(\n+            num_arrivals=1, num_barriers=max_concurrent_steps\n+        ),\n+        plgpu.Barrier(\n+            num_arrivals=1, num_barriers=max_concurrent_steps\n+        ),\n+        plgpu.Barrier(\n+            num_arrivals=1,\n+            num_barriers=max_concurrent_steps + 1,\n+            for_tensor_core=True,\n+        ),\n+      )\n+  )\n+  return f(a, b)\n+\n+\n+def main(_) -> None:\n+  problem_it = itertools.product(\n+      (1024, 4096, 8192), (1024, 4096, 8192), (1024, 8192)\n+  )\n+  for M, N, K in problem_it:\n+    print(f\"==== {M=} {N=} {K=} ====\")\n+    matmul_flops = 2 * M * N * K\n+    peak_flops = 2.25e15  # f16 TensorCore peak = 2250 TFLOPS\n+    a = jax.random.uniform(jax.random.key(0), (M, K), jnp.bfloat16)\n+    b = jax.random.uniform(jax.random.key(1), (K, N), jnp.bfloat16)\n+    tuning_it = itertools.product(\n+        (128,), (128, 256), (64, 128), (2, 3, 4), (False,)\n+    )\n+    best_util = -float(\"inf\")\n+    for (block_m, block_n, block_k,\n+         max_concurrent_steps, collective) in tuning_it:\n+      config = TuningConfig(\n+          block_m=block_m,\n+          block_n=block_n,\n+          block_k=block_k,\n+          max_concurrent_steps=max_concurrent_steps,\n+          collective=collective,\n+      )\n+      try:\n+        out, runtime_ms = profiler.measure(\n+            functools.partial(matmul_kernel, config=config)\n+        )(a, b)\n+      except ValueError as e:\n+        if \"exceeds available shared memory\" in e.args[0]:\n+          continue\n+        raise\n+      if M * N * K <= 1024 * 1024 * 1024:\n+        expected = a @ b\n+        np.testing.assert_allclose(out, expected)\n+      runtime_us = runtime_ms * 1e3   # type: ignore\n+      optimal_time = matmul_flops / peak_flops * 1e6  # us\n+      achieved_tc_util = optimal_time / runtime_us * 100\n+      if achieved_tc_util > best_util:\n+        best_util = achieved_tc_util\n+      print(\n+          f\"{block_m=} {block_n=} {block_k=} {max_concurrent_steps=}:  \"\n+          f\"{runtime_us:<7.1f}us\"\n+          f\" = {achieved_tc_util:4.1f}% TC utilization\"\n+      )\n+    print(f\"\\tBest utilization: {best_util:4.1f}%\")\n+\n+\n+if __name__ == \"__main__\":\n+  from absl import app\n+\n+  jax.config.config_with_absl()\n+  app.run(main)\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 8be899123de0..48eddae69a60 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -805,6 +805,23 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"mgpu_matmul_test\",\n+    srcs = [\"mgpu_matmul_test.py\"],\n+    enable_backends = [],\n+    enable_configs = [],  # TODO(justinfu): Enable B200 when available.\n+    env = {\"XLA_FLAGS\": \"--xla_gpu_autotune_level=0\"},\n+    shard_count = 8,\n+    deps = [\n+        \"//jax:pallas\",\n+        \"//jax:pallas_experimental_gpu_ops\",\n+        \"//jax:pallas_mosaic_gpu\",\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n+)\n+\n jax_multiplatform_test(\n     name = \"mgpu_ragged_dot_run\",\n     srcs = [\"//jax/experimental/pallas/ops/gpu:ragged_dot_mgpu.py\"],\ndiff --git a/tests/pallas/mgpu_matmul_test.py b/tests/pallas/mgpu_matmul_test.py\nnew file mode 100644\nindex 000000000000..4013db78f6a2\n--- /dev/null\n+++ b/tests/pallas/mgpu_matmul_test.py\n@@ -0,0 +1,88 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Test different parameterizations of matrix multiplication.\"\"\"\n+\n+import contextlib\n+import os\n+\n+from absl.testing import absltest\n+from absl.testing import parameterized\n+from jax._src import config\n+from jax._src import test_util as jtu\n+from jax._src.pallas import pallas_call\n+import jax.numpy as jnp\n+import numpy as np\n+\n+\n+# pylint: disable=g-import-not-at-top\n+try:\n+  # We only import this to see if Mosaic is available.\n+  import jax.experimental.mosaic.gpu  # noqa: F401\n+except ImportError:\n+  blackwell_matmul_mgpu = None\n+else:\n+  from jax.experimental.pallas.ops.gpu import blackwell_matmul_mgpu\n+\n+\n+config.parse_flags_with_absl()\n+os.environ[\"XLA_FLAGS\"] = (\n+    os.environ.get(\"XLA_FLAGS\", \"\") + \" --xla_gpu_autotune_level=0\")\n+\n+\n+@jtu.with_config(jax_traceback_filtering=\"off\")\n+class MatrixMultiplicationSm100ATest(jtu.JaxTestCase):\n+\n+  def setUp(self):\n+    super().setUp()\n+    if blackwell_matmul_mgpu is None:\n+      self.skipTest(\"Mosaic GPU not available.\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_equal(\"10.0\")):\n+      self.skipTest(\"Only works on GPU with capability sm100a\")\n+    context_stack = contextlib.ExitStack()\n+    context_stack.enter_context(pallas_call._PALLAS_USE_MOSAIC_GPU(True))\n+    self.addCleanup(context_stack.close)\n+\n+  @parameterized.product(\n+      m=(1024, 4096),\n+      k=(1024, 4096),\n+      n=(1024, 4096),\n+      dtype=(jnp.float16,),\n+  )\n+  def test_matmul(\n+      self,\n+      m,\n+      n,\n+      k,\n+      dtype,\n+  ):\n+    k1, k2, = jax.random.split(jax.random.key(42), 2)\n+    a = jax.random.normal(k1, (m, k), dtype)\n+    b = jax.random.normal(k2, (k, n), dtype)\n+\n+    out = blackwell_matmul_mgpu.matmul_kernel(\n+        a,\n+        b,\n+        blackwell_matmul_mgpu.TuningConfig(\n+            block_m=128, block_n=128, block_k=128,\n+            max_concurrent_steps=2,\n+            collective=False,\n+        ),\n+    )\n+    out_ref = a @ b\n+    np.testing.assert_allclose(out, out_ref, atol=2e-3, rtol=1e-3)\n+\n+if __name__ == \"__main__\":\n+  absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] Added `pl.loop` -- a decorator for writing stateless loops\n\nPiperOrigin-RevId: 764871167",
        "output": [
            "```diff\nCommit: 976aa7ac31f49957803547ebff80720e39aba04e\nDate: 2025-05-29T20:55:32Z\nURL: https://github.com/jax-ml/jax/commit/976aa7ac31f49957803547ebff80720e39aba04e\nFiles changed: 6\nAdditions: +28, Deletions: -14\ndiff --git a/jax/_src/pallas/helpers.py b/jax/_src/pallas/helpers.py\nindex 5c77d0a04f09..71004cd405a3 100644\n--- a/jax/_src/pallas/helpers.py\n+++ b/jax/_src/pallas/helpers.py\n@@ -13,6 +13,8 @@\n # limitations under the License.\n \"\"\"Pallas helper functions.\"\"\"\n \n+from collections.abc import Callable\n+\n import jax\n from jax._src import checkify\n from jax._src import config\n@@ -69,6 +71,20 @@ def _wrapped(f):\n   return _wrapped\n \n \n+def loop(\n+    lower: jax.typing.ArrayLike,\n+    upper: jax.typing.ArrayLike,\n+    *,\n+    unroll: int | bool | None = None,\n+) -> Callable[[Callable[[jax.Array], None]], None]:\n+  def decorator(body):\n+    jax.lax.fori_loop(\n+        lower, upper, lambda idx, _: body(idx), init_val=None, unroll=unroll\n+    )\n+\n+  return decorator\n+\n+\n _ENABLE_DEBUG_CHECKS = config.bool_state(\n     \"jax_pallas_enable_debug_checks\",\n     default=False,\ndiff --git a/jax/_src/pallas/mosaic_gpu/pipeline.py b/jax/_src/pallas/mosaic_gpu/pipeline.py\nindex f85b73b6b946..be9f663a42b7 100644\n--- a/jax/_src/pallas/mosaic_gpu/pipeline.py\n+++ b/jax/_src/pallas/mosaic_gpu/pipeline.py\n@@ -764,12 +764,10 @@ def memory_loop_body(step, carry):\n                     memory_loop_body, (indices,))\n       # Await all the arrivals to not leave barriers in a bad state.\n       # We only need to account for the prologue steps.\n-      def _epi_step(step, _):\n+      @pl.loop(0, prologue_steps, unroll=not has_dynamic_grid)\n+      def _epi_step(step):\n         for barrier in consumed_barrier_refs:\n           gpu_primitives.barrier_wait(barrier.at[step])\n-      jax.lax.fori_loop(\n-          0, prologue_steps, _epi_step, None, unroll=not has_dynamic_grid\n-      )\n \n     wg_idx = lax.axis_index(wg_axis)\n     lax.cond(\ndiff --git a/jax/experimental/pallas/__init__.py b/jax/experimental/pallas/__init__.py\nindex caf77a3c4fce..da2bc9119dd0 100644\n--- a/jax/experimental/pallas/__init__.py\n+++ b/jax/experimental/pallas/__init__.py\n@@ -38,6 +38,7 @@\n from jax._src.pallas.cost_estimate import estimate_cost as estimate_cost\n from jax._src.pallas.helpers import empty as empty\n from jax._src.pallas.helpers import empty_like as empty_like\n+from jax._src.pallas.helpers import loop as loop\n from jax._src.pallas.helpers import when as when\n from jax._src.pallas.helpers import debug_check as debug_check\n from jax._src.pallas.helpers import debug_checks_enabled as debug_checks_enabled\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex 447e3affd7c1..650668daf67a 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -245,7 +245,8 @@ def _memory_wg():\n         plgpu.copy_gmem_to_smem(k_ref.at[s], k_smem.at[i], k_barriers.at[i])\n         plgpu.copy_gmem_to_smem(v_ref.at[s], v_smem.at[i], v_barriers.at[i])\n \n-      def kv_loop(kv_step, _):\n+      @pl.loop(0, block_max_kv_steps - max_concurrent_steps)\n+      def _kv_loop(kv_step):\n         tma_step = kv_step + max_concurrent_steps\n         tma_slot = lax.rem(kv_step, jnp.array(max_concurrent_steps, kv_step.dtype))\n         s = (batch, pl.ds(tma_step * block_kv, block_kv), kv_head)\n@@ -253,7 +254,6 @@ def kv_loop(kv_step, _):\n         plgpu.copy_gmem_to_smem(k_ref.at[s], k_smem.at[tma_slot], k_barriers.at[tma_slot])\n         plgpu.barrier_wait(v_consumed_barriers.at[tma_slot])\n         plgpu.copy_gmem_to_smem(v_ref.at[s], v_smem.at[tma_slot], v_barriers.at[tma_slot])\n-      lax.fori_loop(0, block_max_kv_steps - max_concurrent_steps, kv_loop, None)\n \n   def entry(q_ref, k_ref, v_ref, out_ref, lse_ref):\n     compute_wgs = 2\ndiff --git a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\nindex 36d29cf082d8..5e4dda4494ba 100644\n--- a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n@@ -107,12 +107,13 @@ def m_loop(mi, _):\n \n       # For some reason ptxas spills if we unroll the loop over k\n       copy_block = 32\n-      def k_copy_loop(ki, _):\n+      @pl.loop(0, k // copy_block)\n+      def _k_copy_loop(ki):\n         k_slice = pl.ds(ki * copy_block, copy_block)\n         scratch_ref[0, :, k_slice] = lhs_ref[m_tile_slice, k_slice]\n-      jax.lax.fori_loop(0, k // copy_block, k_copy_loop, None)\n \n-      def device_loop(device_offset, _):\n+      @pl.loop(0, num_devices)\n+      def _device_loop(device_offset):\n         # Loop invariant: scratch_ref.at[scratch_slot] is ready to be used\n         # We're double buffering the scratch space. At each step, we read from\n         # scratch_ref.at[scratch_slot] and write to scratch_ref.at[next_scratch_slot]\n@@ -168,7 +169,7 @@ def k_loop(idxs, lhs_smem, rhs_smem):\n           )\n           # Wait for the next scratch to arrive --- see the loop invariant.\n           pl.semaphore_wait(received_sem)\n-      jax.lax.fori_loop(0, num_devices, device_loop, None)\n+\n     grid_size = m_shard // block_m\n     m_steps = grid_size // num_sms + jnp.int32(sm_id < grid_size % num_sms)\n     # TODO(apaszke): Use the ND-loop helper.\ndiff --git a/jax/experimental/pallas/ops/tpu/flash_attention.py b/jax/experimental/pallas/ops/tpu/flash_attention.py\nindex ef8dd61abacb..06746986a15e 100644\n--- a/jax/experimental/pallas/ops/tpu/flash_attention.py\n+++ b/jax/experimental/pallas/ops/tpu/flash_attention.py\n@@ -383,10 +383,8 @@ def start_new_sequence():\n \n   @pl.when(should_run)\n   def run():\n-    @functools.partial(\n-        lax.fori_loop, 0, block_k_major // block_k, init_val=None, unroll=True\n-    )\n-    def body(i, _):\n+    @pl.loop(0, block_k_major // block_k, unroll=True)\n+    def _body(i):\n       m_prev = m_scratch_ref[batch_idx]\n       l_prev = l_scratch_ref[batch_idx]\n       q = q_tile_ref[batch_idx]  # [block_q, head_dim]\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas Fuser] Use lu transformation to physicalize fwd/bwd functions in custom_vjp rule\n\nPiperOrigin-RevId: 764871024",
        "output": [
            "```diff\nCommit: 3abdf560cc912320e0c0b69bae9851d7d1b93d6b\nDate: 2025-05-29T20:53:32Z\nURL: https://github.com/jax-ml/jax/commit/3abdf560cc912320e0c0b69bae9851d7d1b93d6b\nFiles changed: 2\nAdditions: +38, Deletions: -0\ndiff --git a/jax/_src/pallas/fuser/BUILD b/jax/_src/pallas/fuser/BUILD\nindex a4c3402f5309..d5b5d128241d 100644\n--- a/jax/_src/pallas/fuser/BUILD\n+++ b/jax/_src/pallas/fuser/BUILD\n@@ -115,6 +115,7 @@ pytype_strict_library(\n         \"//jax\",\n         \"//jax:api_util\",\n         \"//jax:core\",\n+        \"//jax:custom_derivatives\",\n         \"//jax:dtypes\",\n         \"//jax:partial_eval\",\n         \"//jax:source_info_util\",\ndiff --git a/jax/_src/pallas/fuser/fusible_dtype.py b/jax/_src/pallas/fuser/fusible_dtype.py\nindex 7d9c2ca67855..152b20ff66ea 100644\n--- a/jax/_src/pallas/fuser/fusible_dtype.py\n+++ b/jax/_src/pallas/fuser/fusible_dtype.py\n@@ -17,11 +17,13 @@\n import abc\n import dataclasses\n import functools\n+import itertools as it\n from typing import Any, Sequence, TypeVar\n \n import jax\n from jax._src import api_util\n from jax._src import core\n+from jax._src import custom_derivatives\n from jax._src import dtypes\n from jax._src import linear_util as lu\n from jax._src import source_info_util\n@@ -312,6 +314,41 @@ def _cond_physicalize_rule(ctx: Context, *args, branches, **kwargs):\n _physicalize_rules[conditionals.cond_p] = _cond_physicalize_rule\n \n \n+@lu.transformation2\n+def _physicalize_transform(f, *args):\n+  vals, zeros = args[::2], args[1::2]\n+  assert len(vals) == len(zeros)\n+  wrapper = lambda *inner_vals: f(\n+      *it.chain.from_iterable(zip(inner_vals, zeros))\n+  )\n+  return physicalize(wrapper)(*vals)\n+\n+\n+@lu.transformation2\n+def _physicalize_transform_bwd(f, const_avals, *args):\n+  return [custom_derivatives.Zero(a) for a in const_avals] + list(\n+      physicalize(f)(*args)\n+  )\n+\n+\n+def _custom_vjp_call_physicalize_rule(\n+    ctx: Context, *args, call_jaxpr, num_consts, fwd_jaxpr_thunk, bwd, **kwargs\n+):\n+  _assert_no_fusion_types(ctx.avals_out)\n+  new_jaxpr = physicalize_closed_jaxpr(call_jaxpr)\n+  fun = lu.wrap_init(core.jaxpr_as_fun(new_jaxpr),\n+                     debug_info=call_jaxpr.jaxpr.debug_info)\n+  fwd = custom_derivatives.lift_fwd(num_consts, fwd_jaxpr_thunk)\n+  fwd_physicalized = _physicalize_transform(fwd)\n+  const_avals, _ = util.split_list(new_jaxpr.in_avals, [num_consts])\n+  bwd_physicalized = _physicalize_transform_bwd(bwd, const_avals)\n+  return custom_derivatives.custom_vjp_call_p.bind(\n+      fun, fwd_physicalized, bwd_physicalized, *args, **kwargs\n+  )\n+\n+_physicalize_rules[custom_derivatives.custom_vjp_call_p] = _custom_vjp_call_physicalize_rule\n+\n+\n def _run_state_rule(ctx: Context, *args, jaxpr, which_linear, is_initialized):\n   _assert_no_fusion_types(ctx.avals_in)\n   _assert_no_fusion_types(ctx.avals_out)\n\n```"
        ],
        "from_id": [
            "sharadmv",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29076 from hanzlfs:zhonglin/mosaic/collective_matmul\n\nPiperOrigin-RevId: 764852609",
        "output": [
            "```diff\nCommit: 4e5725bba22976a78a89a86d79bc2979a1bbd5ee\nDate: 2025-05-29T20:09:32Z\nURL: https://github.com/jax-ml/jax/commit/4e5725bba22976a78a89a86d79bc2979a1bbd5ee\nFiles changed: 2\nAdditions: +9, Deletions: -4\ndiff --git a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\nindex 854d75dbf6a3..36d29cf082d8 100644\n--- a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n@@ -42,6 +42,7 @@ def all_gather_lhs_matmul(\n     block_n: int,\n     block_k: int,\n     max_concurrent_steps: int,\n+    dtype: jnp.dtype = jnp.float16,\n ) -> jax.Array:\n   if (num_devices := jax.device_count()) != jax.process_count():\n     raise ValueError(\"The kernel only supports one device per process\")\n@@ -49,6 +50,8 @@ def all_gather_lhs_matmul(\n     raise ValueError(\"The kernel can only work over all devices in a Mesh.\")\n   if max_concurrent_steps < 2:\n     raise ValueError(\"max_concurrent_steps must be >= 2\")\n+  if jnp.dtype(dtype) not in map(jnp.dtype, [jnp.float16, jnp.bfloat16]):\n+    raise NotImplementedError(f\"Only f16 and bf16 are supported, got dtype: {dtype}\")\n \n   num_sms = 132  # There are 132 SMs on a H100 SXM GPU.\n \n@@ -121,7 +124,7 @@ def device_loop(device_offset, _):\n         @functools.partial(\n             pl.run_scoped,\n             acc_ref=plgpu.ACC((block_m, block_n)),\n-            out_smem=plgpu.SMEM((block_m, block_n), jnp.float16, transforms=transforms),\n+            out_smem=plgpu.SMEM((block_m, block_n), dtype, transforms=transforms),\n         )\n         def _(acc_ref, out_smem):\n           pl.semaphore_wait(capacity_sem)\n@@ -173,8 +176,8 @@ def k_loop(idxs, lhs_smem, rhs_smem):\n \n   result, _ = plgpu.kernel(\n       kernel_body,\n-      out_shape=[jax.ShapeDtypeStruct((axis_size * m_shard, n_shard), jnp.float16),\n-                  jax.ShapeDtypeStruct((num_sms, 2, block_m, k), jnp.float16)],\n+      out_shape=[jax.ShapeDtypeStruct((axis_size * m_shard, n_shard), dtype),\n+                  jax.ShapeDtypeStruct((num_sms, 2, block_m, k), dtype)],\n       scratch_shapes=[\n           plgpu.SemaphoreType.REGULAR, plgpu.SemaphoreType.REGULAR,\n       ],\ndiff --git a/tests/pallas/mgpu_collective_matmul_test.py b/tests/pallas/mgpu_collective_matmul_test.py\nindex 386162b1992c..3760c7ccddb7 100644\n--- a/tests/pallas/mgpu_collective_matmul_test.py\n+++ b/tests/pallas/mgpu_collective_matmul_test.py\n@@ -68,6 +68,7 @@ def setUp(self):\n       block_n=(64, 128, 192),\n       block_k=(64, 128),\n       max_concurrent_steps=(2, 4),\n+      dtype=(jnp.float16, jnp.bfloat16),\n   )\n   def test_all_gather_lhs_matmul(\n       self,\n@@ -78,9 +79,9 @@ def test_all_gather_lhs_matmul(\n       block_n,\n       block_k,\n       max_concurrent_steps,\n+      dtype,\n   ):\n     num_devices = jax.device_count()\n-    dtype = jnp.float16\n     lhs_smem_size = block_m * block_k * max_concurrent_steps * 2\n     rhs_smem_size = block_k * block_n * max_concurrent_steps * 2\n     # H100 SMEM limit is 228kB.\n@@ -118,6 +119,7 @@ def run(body):\n             block_n=block_n,\n             block_k=block_k,\n             max_concurrent_steps=max_concurrent_steps,\n+            dtype=dtype,\n         )\n     )\n     ref_out = run(lambda x, y: lax.all_gather(x, \"x\", axis=0, tiled=True) @ y)\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reverts 42977e51816b9eb42c7360abe05f56cad70e894a\n\nPiperOrigin-RevId: 764832745",
        "output": [
            "```diff\nCommit: 448c07d006e5cbc0cdd95ee7e477b9bdc606e1e9\nDate: 2025-05-29T19:16:31Z\nURL: https://github.com/jax-ml/jax/commit/448c07d006e5cbc0cdd95ee7e477b9bdc606e1e9\nFiles changed: 2\nAdditions: +0, Deletions: -21\ndiff --git a/jax/_src/pallas/fuser/BUILD b/jax/_src/pallas/fuser/BUILD\nindex d5b5d128241d..a4c3402f5309 100644\n--- a/jax/_src/pallas/fuser/BUILD\n+++ b/jax/_src/pallas/fuser/BUILD\n@@ -115,7 +115,6 @@ pytype_strict_library(\n         \"//jax\",\n         \"//jax:api_util\",\n         \"//jax:core\",\n-        \"//jax:custom_derivatives\",\n         \"//jax:dtypes\",\n         \"//jax:partial_eval\",\n         \"//jax:source_info_util\",\ndiff --git a/jax/_src/pallas/fuser/fusible_dtype.py b/jax/_src/pallas/fuser/fusible_dtype.py\nindex 09cd8f57dbc1..7d9c2ca67855 100644\n--- a/jax/_src/pallas/fuser/fusible_dtype.py\n+++ b/jax/_src/pallas/fuser/fusible_dtype.py\n@@ -22,7 +22,6 @@\n import jax\n from jax._src import api_util\n from jax._src import core\n-from jax._src import custom_derivatives\n from jax._src import dtypes\n from jax._src import linear_util as lu\n from jax._src import source_info_util\n@@ -313,25 +312,6 @@ def _cond_physicalize_rule(ctx: Context, *args, branches, **kwargs):\n _physicalize_rules[conditionals.cond_p] = _cond_physicalize_rule\n \n \n-def _custom_vjp_call_physicalize_rule(\n-    ctx: Context, *args, call_jaxpr, num_consts, fwd_jaxpr_thunk, bwd, **kwargs\n-):\n-  _assert_no_fusion_types(ctx.avals_out)\n-  new_jaxpr = physicalize_closed_jaxpr(call_jaxpr)\n-  fun = lu.wrap_init(core.jaxpr_as_fun(new_jaxpr),\n-                     debug_info=call_jaxpr.jaxpr.debug_info)\n-  fwd = custom_derivatives.lift_fwd(num_consts, fwd_jaxpr_thunk)\n-  new_fwd = lu.wrap_init(physicalize(fwd.f_transformed), debug_info=fwd.debug_info)\n-  const_avals, _ = util.split_list(new_jaxpr.in_avals, [num_consts])\n-  bwd = custom_derivatives._handle_consts_in_bwd(bwd, const_avals)\n-  new_bwd = lu.wrap_init(physicalize(bwd.f_transformed), debug_info=bwd.debug_info)\n-  return custom_derivatives.custom_vjp_call_p.bind(\n-      fun, new_fwd, new_bwd, *args, **kwargs\n-  )\n-\n-_physicalize_rules[custom_derivatives.custom_vjp_call_p] = _custom_vjp_call_physicalize_rule\n-\n-\n def _run_state_rule(ctx: Context, *args, jaxpr, which_linear, is_initialized):\n   _assert_no_fusion_types(ctx.avals_in)\n   _assert_no_fusion_types(ctx.avals_out)\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Lock down more permissions and update default usage for some workflows",
        "output": [
            "```diff\nCommit: da845deb30955cfb32d265457903ad90fc3e2eb7\nDate: 2025-05-29T18:53:29Z\nURL: https://github.com/jax-ml/jax/commit/da845deb30955cfb32d265457903ad90fc3e2eb7\nFiles changed: 6\nAdditions: +4, Deletions: -47\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex 09c9d173e0d0..cc3ae89d97f9 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -16,22 +16,18 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version to test?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n \n jobs:\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 458589199c53..d30e1b56dab8 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -17,33 +17,28 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version to test?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       jaxlib-version:\n         description: \"Which jaxlib version to test? (head/pypi_latest)\"\n         type: string\n-        required: true\n         default: \"head\"\n       gcs_download_uri:\n         description: \"GCS location URI from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 72d554aa5d1b..95ab90412494 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -12,7 +12,6 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: choice\n-        required: true\n         default: \"linux-x86-n2-16\"\n         options:\n         - \"linux-x86-n2-16\"\n@@ -21,7 +20,6 @@ on:\n       artifact:\n         description: \"Which JAX artifact to build?\"\n         type: choice\n-        required: true\n         default: \"jaxlib\"\n         options:\n         - \"jax\"\n@@ -31,7 +29,6 @@ on:\n       python:\n         description: \"Which python version should the artifact be built for?\"\n         type: choice\n-        required: false\n         default: \"3.12\"\n         options:\n         - \"3.10\"\n@@ -41,7 +38,6 @@ on:\n       clone_main_xla:\n         description: \"Should latest XLA be used?\"\n         type: choice\n-        required: false\n         default: \"0\"\n         options:\n         - \"1\"\n@@ -49,7 +45,6 @@ on:\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: choice\n-        required: false\n         default: 'no'\n         options:\n         - 'yes'\n@@ -59,31 +54,25 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       artifact:\n         description: \"Which JAX artifact to build?\"\n         type: string\n-        required: true\n         default: \"jaxlib\"\n       python:\n         description: \"Which python version should the artifact be built for?\"\n         type: string\n-        required: false\n         default: \"3.12\"\n       clone_main_xla:\n         description: \"Should latest XLA be used?\"\n         type: string\n-        required: false\n         default: \"0\"\n       upload_artifacts_to_gcs:\n         description: \"Should the artifacts be uploaded to a GCS bucket?\"\n-        required: true\n         default: true\n         type: boolean\n       gcs_upload_uri:\n         description: \"GCS location prefix to where the artifacts should be uploaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n     outputs:\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex a92f2d96dc89..95086257c62b 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -17,34 +17,28 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version should the artifact be built for?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       download-jax-only-from-gcs:\n         description: \"Whether to download only the jax wheel from GCS (e.g for testing a jax only release)\"\n-        required: false\n         default: '0'\n         type: string\n       gcs_download_uri:\n         description: \"GCS location prefix from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n-\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 6fa4e14f8b85..d576370bb772 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -17,44 +17,36 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-n2-16\"\n       python:\n         description: \"Which python version to test?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       cuda-version:\n         description: \"Which CUDA version to test?\"\n         type: string\n-        required: true\n         default: \"12.8\"\n       use-nvidia-pip-wheels:\n         description: \"Whether to download CUDA packages from PyPI?\"\n         type: boolean\n-        required: false\n         default: false\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n-        required: true\n         default: \"0\"\n       download-jax-only-from-gcs:\n         description: \"Whether to download only the jax wheel from GCS (e.g for testing a jax only release)\"\n-        required: false\n         default: '0'\n         type: string\n       gcs_download_uri:\n         description: \"GCS location prefix from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n-\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 5f56b165c295..313bbede52f5 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -22,32 +22,26 @@ on:\n       runner:\n         description: \"Which runner should the workflow run on?\"\n         type: string\n-        required: true\n         default: \"linux-x86-ct5lp-224-8tpu\"\n       cores:\n         description: \"How many TPU cores should the test use?\"\n         type: string\n-        required: true\n         default: \"8\"\n       tpu-type:\n         description: \"Which TPU type is used for testing?\"\n         type: string\n-        required: true\n         default: \"v5e-8\"\n       python:\n         description: \"Which Python version should be used for testing?\"\n         type: string\n-        required: true\n         default: \"3.12\"\n       run-full-tpu-test-suite:\n         description: \"Should the full TPU test suite be run?\"\n         type: string\n-        required: false\n         default: \"0\"\n       libtpu-version-type:\n         description: \"Which libtpu version should be used for testing?\"\n         type: string\n-        required: false\n         # Choices are:\n         # - \"nightly\": Use the nightly libtpu wheel.\n         # - \"pypi_latest\": Use the latest libtpu wheel from PyPI.\n@@ -55,20 +49,17 @@ on:\n         default: \"nightly\"\n       download-jax-only-from-gcs:\n         description: \"Whether to download only the jax wheel from GCS (e.g for testing a jax only release)\"\n-        required: false\n         default: '0'\n         type: string\n       gcs_download_uri:\n         description: \"GCS location prefix from where the artifacts should be downloaded\"\n-        required: true\n         default: 'gs://general-ml-ci-transient/jax-github-actions/jax/${{ github.workflow }}/${{ github.run_number }}/${{ github.run_attempt }}'\n         type: string\n       halt-for-connection:\n         description: 'Should this workflow run wait for a remote connection?'\n         type: string\n-        required: false\n         default: 'no'\n-\n+permissions: {}\n jobs:\n   run-tests:\n     defaults:\n\n```"
        ],
        "from_id": [
            "MichaelHudgins"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Check that the device order in the mesh follows logical_ids\n\nAs the comment in the code explains, we expect that the mesh ordering follows\ndevice ids, which should always equal the NVSHMEM PE ids that Mosaic uses for\nits collective implementations. Any divergence would have to be resolved through\nan extra translation layer at runtime.\n\nPiperOrigin-RevId: 764819378",
        "output": [
            "```diff\nCommit: 57fe3f2aa60239e792ff46607372b4826440033b\nDate: 2025-05-29T18:44:47Z\nURL: https://github.com/jax-ml/jax/commit/57fe3f2aa60239e792ff46607372b4826440033b\nFiles changed: 2\nAdditions: +57, Deletions: -0\ndiff --git a/jax/experimental/mosaic/gpu/core.py b/jax/experimental/mosaic/gpu/core.py\nindex 4ed551654a0e..193fd1bd3589 100644\n--- a/jax/experimental/mosaic/gpu/core.py\n+++ b/jax/experimental/mosaic/gpu/core.py\n@@ -27,6 +27,7 @@\n import weakref\n \n import jax\n+from jax._src import sharding_impls\n from jax._src.interpreters import mlir\n from jax._src.lib import mosaic_gpu_dialect as dialect\n from jaxlib.mlir import ir\n@@ -127,6 +128,15 @@ def _mosaic_gpu_abstract_eval(*_, module, out_types):\n   del module  # Unused.\n   return [jax._src.core.ShapedArray(t.shape, t.dtype) for t in out_types]\n \n+\n+def _has_communication(module, **_):\n+  empty_str_attr = ir.StringAttr.get(\"\")\n+  for op in module.body:\n+    if \"nvshmem\" in getattr(op, \"sym_name\", empty_str_attr).value:\n+      return True\n+  return False\n+\n+\n # TODO(apaszke): Implement a proper system for managing kernel lifetimes\n KNOWN_KERNELS = {}\n \n@@ -139,6 +149,27 @@ def _mosaic_gpu_lowering_rule(\n     input_output_aliases: tuple[tuple[int, int], ...] = (),\n     use_custom_barrier: bool = False,\n ):\n+  axis_context = ctx.module_context.axis_context\n+  if _has_communication(module):\n+    # Those checks are trying to ensure that the logical device ids are\n+    # consistent with the NVSHMEM PE ids that Mosaic will be using for\n+    # communication. Any divergence here would require us to implement a logical\n+    # to physical translation, which is currently not implemented.\n+    if isinstance(axis_context, sharding_impls.SPMDAxisContext):\n+      mesh = axis_context.mesh\n+      if not np.array_equal(mesh.device_ids.ravel(), np.arange(mesh.size)):\n+        raise NotImplementedError(\n+            \"Mosaic GPU only supports meshes with device ordering that follows\"\n+            \" row-major device ids.\"\n+        )\n+    elif isinstance(axis_context, sharding_impls.ShardingContext):\n+      if axis_context.num_devices != 1:\n+        raise NotImplementedError(\n+            \"Mosaic GPU only supports single-device meshes in ShardingContext.\"\n+        )\n+    else:\n+      raise NotImplementedError(f\"Unsupported sharding context: {axis_context}\")\n+\n   assert len(args) == len(ctx.avals_in)\n   assert len(out_types) == len(ctx.avals_out)\n   module = _run_serde_pass(\ndiff --git a/tests/pallas/gpu_pallas_distributed_test.py b/tests/pallas/gpu_pallas_distributed_test.py\nindex 3aeee352ff6d..163adc385b23 100644\n--- a/tests/pallas/gpu_pallas_distributed_test.py\n+++ b/tests/pallas/gpu_pallas_distributed_test.py\n@@ -116,6 +116,32 @@ def kernel(y_ref, sem):\n     )()\n     np.testing.assert_allclose(y, jnp.ones_like(y))\n \n+  def test_permuted_mesh(self):\n+    def kernel(y_ref, sem):\n+      other_dev_id = 1 - lax.axis_index('x')\n+      pl.semaphore_signal(sem, 1, device_id=other_dev_id,\n+                          device_id_type=pl.DeviceIdType.LOGICAL)\n+      pl.semaphore_wait(sem)\n+\n+    kernel_call = pl.pallas_call(\n+        kernel,\n+        out_specs=pl.BlockSpec(memory_space=plgpu.GMEM),\n+        out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32),\n+        scratch_shapes=[plgpu.SemaphoreType.REGULAR],\n+    )\n+    mesh = jax.sharding.Mesh(jax.devices()[::-1], ['x'])  # Reverse the devices.\n+    f = jax.jit(\n+        shard_map.shard_map(\n+            kernel_call, mesh, in_specs=(), out_specs=P(None), check_rep=False,\n+        )\n+    )\n+    msg = (\n+        'Mosaic GPU only supports meshes with device ordering that follows'\n+        ' row-major device ids.'\n+    )\n+    with self.assertRaisesRegex(NotImplementedError, msg):\n+      f()\n+\n \n if __name__ == '__main__':\n   # This test doesn't work with the platform allocator, so we override it\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add dtype arg collective_matmul_mgpu.py to support bfloat16",
        "output": [
            "```diff\nCommit: 1e334cfdd27b82f4af98e0a744b5af0e2a3634ec\nDate: 2025-05-29T17:56:13Z\nURL: https://github.com/jax-ml/jax/commit/1e334cfdd27b82f4af98e0a744b5af0e2a3634ec\nFiles changed: 2\nAdditions: +9, Deletions: -4\ndiff --git a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\nindex 854d75dbf6a3..36d29cf082d8 100644\n--- a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n@@ -42,6 +42,7 @@ def all_gather_lhs_matmul(\n     block_n: int,\n     block_k: int,\n     max_concurrent_steps: int,\n+    dtype: jnp.dtype = jnp.float16,\n ) -> jax.Array:\n   if (num_devices := jax.device_count()) != jax.process_count():\n     raise ValueError(\"The kernel only supports one device per process\")\n@@ -49,6 +50,8 @@ def all_gather_lhs_matmul(\n     raise ValueError(\"The kernel can only work over all devices in a Mesh.\")\n   if max_concurrent_steps < 2:\n     raise ValueError(\"max_concurrent_steps must be >= 2\")\n+  if jnp.dtype(dtype) not in map(jnp.dtype, [jnp.float16, jnp.bfloat16]):\n+    raise NotImplementedError(f\"Only f16 and bf16 are supported, got dtype: {dtype}\")\n \n   num_sms = 132  # There are 132 SMs on a H100 SXM GPU.\n \n@@ -121,7 +124,7 @@ def device_loop(device_offset, _):\n         @functools.partial(\n             pl.run_scoped,\n             acc_ref=plgpu.ACC((block_m, block_n)),\n-            out_smem=plgpu.SMEM((block_m, block_n), jnp.float16, transforms=transforms),\n+            out_smem=plgpu.SMEM((block_m, block_n), dtype, transforms=transforms),\n         )\n         def _(acc_ref, out_smem):\n           pl.semaphore_wait(capacity_sem)\n@@ -173,8 +176,8 @@ def k_loop(idxs, lhs_smem, rhs_smem):\n \n   result, _ = plgpu.kernel(\n       kernel_body,\n-      out_shape=[jax.ShapeDtypeStruct((axis_size * m_shard, n_shard), jnp.float16),\n-                  jax.ShapeDtypeStruct((num_sms, 2, block_m, k), jnp.float16)],\n+      out_shape=[jax.ShapeDtypeStruct((axis_size * m_shard, n_shard), dtype),\n+                  jax.ShapeDtypeStruct((num_sms, 2, block_m, k), dtype)],\n       scratch_shapes=[\n           plgpu.SemaphoreType.REGULAR, plgpu.SemaphoreType.REGULAR,\n       ],\ndiff --git a/tests/pallas/mgpu_collective_matmul_test.py b/tests/pallas/mgpu_collective_matmul_test.py\nindex 386162b1992c..3760c7ccddb7 100644\n--- a/tests/pallas/mgpu_collective_matmul_test.py\n+++ b/tests/pallas/mgpu_collective_matmul_test.py\n@@ -68,6 +68,7 @@ def setUp(self):\n       block_n=(64, 128, 192),\n       block_k=(64, 128),\n       max_concurrent_steps=(2, 4),\n+      dtype=(jnp.float16, jnp.bfloat16),\n   )\n   def test_all_gather_lhs_matmul(\n       self,\n@@ -78,9 +79,9 @@ def test_all_gather_lhs_matmul(\n       block_n,\n       block_k,\n       max_concurrent_steps,\n+      dtype,\n   ):\n     num_devices = jax.device_count()\n-    dtype = jnp.float16\n     lhs_smem_size = block_m * block_k * max_concurrent_steps * 2\n     rhs_smem_size = block_k * block_n * max_concurrent_steps * 2\n     # H100 SMEM limit is 228kB.\n@@ -118,6 +119,7 @@ def run(body):\n             block_n=block_n,\n             block_k=block_k,\n             max_concurrent_steps=max_concurrent_steps,\n+            dtype=dtype,\n         )\n     )\n     ref_out = run(lambda x, y: lax.all_gather(x, \"x\", axis=0, tiled=True) @ y)\n\n```"
        ],
        "from_id": [
            "hanzlfs"
        ]
    },
    {
        "text_input": "[pallas:mosaic] Enabled more lowering rules for all kernel types\n\nPiperOrigin-RevId: 764795007",
        "output": [
            "```diff\nCommit: 64ef37a6fe33ba4c264750bbbb0bdb086406818c\nDate: 2025-05-29T17:48:58Z\nURL: https://github.com/jax-ml/jax/commit/64ef37a6fe33ba4c264750bbbb0bdb086406818c\nFiles changed: 1\nAdditions: +7, Deletions: -4\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex c6aaf77199b5..e2dfe526ea14 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -3212,15 +3212,16 @@ def _debug_callback_lowering_rule(ctx: LoweringRuleContext, *args, **kwargs):\n   return []\n \n \n-@register_lowering_rule(primitives.program_id_p)\n+@register_lowering_rule(\n+    primitives.program_id_p, kernel_types=[*tpu_core.KernelType]\n+)\n def _program_id_lowering_rule(ctx: LoweringRuleContext, *, axis: int):\n-\n   if ctx.lowering_context.user_grid_indices is None:\n     raise ValueError(\n         f\"program id: {axis} was passed, but user did not provide a grid.\"\n     )\n   length = len(ctx.lowering_context.user_grid_indices)\n-  if not (0 <= axis < length):\n+  if axis not in range(length):\n     raise ValueError(\n         f\"user passed in program id with axis: {axis}, but grid only has\"\n         f\" length: {length}\"\n@@ -3228,7 +3229,9 @@ def _program_id_lowering_rule(ctx: LoweringRuleContext, *, axis: int):\n   return ctx.lowering_context.user_grid_indices[axis]\n \n \n-@register_lowering_rule(primitives.num_programs_p)\n+@register_lowering_rule(\n+    primitives.num_programs_p, kernel_types=[*tpu_core.KernelType]\n+)\n def _num_programs_lowering_rule(ctx: LoweringRuleContext, *, axis: int):\n   mapped_axes = set(ctx.lowering_context.mapped_dims)\n   seen_user_axes = 0\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28972 from vfdev-5:fix-tsan-314-jax-build-step\n\nPiperOrigin-RevId: 764792000",
        "output": [
            "```diff\nCommit: 63c1b8a74a9e55ff1ceac4c02c6f2470046db732\nDate: 2025-05-29T17:41:39Z\nURL: https://github.com/jax-ml/jax/commit/63c1b8a74a9e55ff1ceac4c02c6f2470046db732\nFiles changed: 1\nAdditions: +7, Deletions: -3\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex 67ff8dd93e3d..c3ee37dd82f4 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -120,6 +120,7 @@ jobs:\n       - name: Build TSAN Numpy wheel\n         if: steps.cache-numpy-tsan-restore.outputs.cache-hit != 'true'\n         run: |\n+          set -eux\n           cd numpy\n \n           # If we restored cpython from cache, we need to get python interpreter from python-tsan.tgz\n@@ -135,7 +136,6 @@ jobs:\n           export PATH=${GITHUB_WORKSPACE}/cpython-tsan/bin/:$PATH\n \n           python3 -m pip install uv~=0.5.30\n-\n           python3 -m uv pip install -r requirements/build_requirements.txt\n \n           CC=clang-18 CXX=clang++-18 python3 -m pip wheel --wheel-dir dist -v . --no-build-isolation -Csetup-args=-Db_sanitize=thread -Csetup-args=-Dbuildtype=debugoptimized\n@@ -272,11 +272,15 @@ jobs:\n             --bazel_options=--copt=-g \\\n             --clang_path=/usr/bin/clang-18\n \n-\n           mkdir -p dist\n+          # Check whether we have numpy wheel or exit with error\n+          ls ${GITHUB_WORKSPACE}/wheelhouse/numpy/*.whl || exit 1\n           cp -v ${GITHUB_WORKSPACE}/wheelhouse/numpy/*.whl dist/\n-          cp -v ${GITHUB_WORKSPACE}/wheelhouse/scipy/*.whl dist/\n           if [ \"${{ matrix.python-version }}\" == \"3.14\" ]; then\n+            # Check whether we have scipy wheel or exit with error\n+            ls ${GITHUB_WORKSPACE}/wheelhouse/scipy/*.whl || exit 1\n+            cp -v ${GITHUB_WORKSPACE}/wheelhouse/scipy/*.whl dist/\n+\n             # Patch build/requirements_lock_3_14_ft.txt to use TSAN instrumented NumPy and Scipy\n             sed -i \"s|--extra-index-url.*|--extra-index-url file://${GITHUB_WORKSPACE}/wheelhouse/|\" build/${{ matrix.requirements_lock_name }}.txt\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Expose `GSPMDSharding` via `jex` as a temporary measure.\n\nPiperOrigin-RevId: 764791015",
        "output": [
            "```diff\nCommit: 605b8c0cc4216032e1aa9644fb7da39f04bafed5\nDate: 2025-05-29T17:39:04Z\nURL: https://github.com/jax-ml/jax/commit/605b8c0cc4216032e1aa9644fb7da39f04bafed5\nFiles changed: 2\nAdditions: +23, Deletions: -0\ndiff --git a/jax/extend/BUILD b/jax/extend/BUILD\nindex 6dc5d7d76311..c2a5c48bd2b0 100644\n--- a/jax/extend/BUILD\n+++ b/jax/extend/BUILD\n@@ -71,6 +71,12 @@ pytype_strict_library(\n     deps = [\"//jax\"],\n )\n \n+pytype_strict_library(\n+    name = \"sharding\",\n+    srcs = [\"sharding.py\"],\n+    deps = [\"//jax:sharding_impls\"],\n+)\n+\n pytype_strict_library(\n     name = \"source_info_util\",\n     srcs = [\"source_info_util.py\"],\ndiff --git a/jax/extend/sharding.py b/jax/extend/sharding.py\nnew file mode 100644\nindex 000000000000..8af2bf397249\n--- /dev/null\n+++ b/jax/extend/sharding.py\n@@ -0,0 +1,17 @@\n+# Copyright 2025 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# TODO(yashkatariya): Remove this after NamedSharding supports more complicated\n+# shardings like sub-axes, strided shardings, etc.\n+from jax._src.sharding_impls import GSPMDSharding as GSPMDSharding\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28985 from dfm:scan-fwd-ext-traceable\n\nPiperOrigin-RevId: 764784115",
        "output": [
            "```diff\nCommit: d1a1346597f1f32fd2257ca2abc8bf565f6f1f11\nDate: 2025-05-29T17:23:35Z\nURL: https://github.com/jax-ml/jax/commit/d1a1346597f1f32fd2257ca2abc8bf565f6f1f11\nFiles changed: 2\nAdditions: +82, Deletions: -1\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 83c31928d7cb..b9ce8ae09380 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -341,7 +341,7 @@ def _create_jaxpr(init):\n   # If the body forwards an input carry to an output carry, that input is\n   # read-only and can be moved to be a const. Doing so can lead to efficiency\n   # wins, e.g. if the scan is inside a cond with a batched predicate.\n-  carry_fwd, _ = split_list(pe._jaxpr_forwarding(jaxpr.jaxpr), [num_carry])\n+  carry_fwd, ext_fwd = split_list(pe._jaxpr_forwarding(jaxpr.jaxpr), [num_carry])\n   move_to_const = [len(consts) + i == f for i, f in enumerate(carry_fwd)]\n   if any(move_to_const):\n     jaxpr = pe.prune_closed_jaxpr_outputs(\n@@ -352,12 +352,32 @@ def _create_jaxpr(init):\n     consts = [*new_consts, *consts]\n     num_carry -= len(new_consts)\n \n+  # When an extensive output is forwarded from an extensive input, we can\n+  # avoid copying it by pruning it from the jaxpr and forwarding manually. We\n+  # don't need to update the indexing based on the optimization above since it\n+  # doesn't change the total number of consts and carries combined, and\n+  # `ext_fwd` already only includes the extensive outputs. But, we do remove\n+  # the number of consts from the index since we're going to use it to index\n+  # into `in_flat`, which doesn't include consts.\n+  ext_to_ext_fwd = [\n+      in_idx - len(consts) if in_idx is not None and\n+      in_idx >= num_carry + len(consts) else None for in_idx in ext_fwd]\n+  jaxpr = pe.prune_closed_jaxpr_outputs(\n+      jaxpr, [True] * num_carry + [i is None for i in ext_to_ext_fwd])\n+\n   out = scan_p.bind(*consts, *in_flat,\n                     reverse=reverse, length=length, jaxpr=jaxpr,\n                     num_consts=len(consts), num_carry=num_carry,\n                     linear=(False,) * (len(consts) + len(in_flat)),\n                     unroll=unroll, _split_transpose=_split_transpose)\n \n+  # Apply input to output forwarding that was computed above.\n+  carry_out, out = split_list(out, [num_carry])\n+  out_ = iter(out)\n+  out = [next(out_) if f is None else _maybe_put(in_flat[f]) for f in ext_to_ext_fwd]\n+  assert next(out_, None) is None\n+  out = [*carry_out, *out]\n+\n   if any(move_to_const):\n     out = pe.merge_lists(move_to_const + [False] * num_ys, out, new_consts)\n \ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex 54dff47fea32..2f1e154627f4 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -3299,6 +3299,59 @@ def body_fun(c, _):\n     outs_ref = body_fun(body_fun(init_vals, [x[0] for x in xs])[0], [x[1] for x in xs])[0]\n     self.assertAllClose(outs, outs_ref, check_dtypes=False)\n \n+  @parameterized.parameters(itertools.product(range(3), repeat=4))\n+  @jtu.run_on_devices(\"cpu\")\n+  def test_scan_forwarding_correctness(\n+      self,\n+      seed,\n+      num_body_consts,\n+      num_const_fwds,\n+      num_input_fwds):\n+\n+    num_carry = num_const_fwds + 4\n+    num_xs = num_input_fwds + 2\n+    num_ys = num_xs + 1\n+\n+    rng = np.random.RandomState(seed)\n+    carry_perm = rng.permutation(num_carry)\n+    carry_iperm = np.argsort(carry_perm)\n+\n+    xs_perm = rng.permutation(num_xs)\n+    ys_perm = rng.permutation(num_ys)\n+    f = np.arange(num_xs)\n+    f = [f[i] if idx < num_input_fwds else None for idx, i in enumerate(xs_perm)]\n+    f += [None]\n+    in_fwd = [f[i] for i in ys_perm]\n+\n+    body_consts = [rng.randn(3) for _ in range(num_body_consts)]\n+    init_vals = list(rng.uniform(size=num_carry))\n+\n+    def body_fun(c, x):\n+      c = [c[i] for i in carry_iperm]\n+      carry_fwds, carry_dont_fwd = split_list(c, [num_const_fwds])\n+      carry_dont_fwd = [jnp.sin(x) * sum(jnp.sum(c) for c in body_consts)\n+                        for x in carry_dont_fwd]\n+      new_c_perm = [*carry_fwds, *carry_dont_fwd]\n+      new_c = [new_c_perm[i] for i in carry_perm]\n+\n+      x = [x[i] for i in xs_perm]\n+      x_fwd, x_dont_fwd = split_list(x, [num_input_fwds])\n+      x_dont_fwd = [jnp.cos(x) * sum(jnp.sum(c) for c in body_consts)\n+                    for x in x_dont_fwd]\n+      y = [*x_fwd, *x_dont_fwd, 0]\n+      y = [y[i] for i in ys_perm]\n+\n+      return new_c, y\n+\n+    xs = list(rng.uniform(size=(num_xs, 2)))\n+    final, outs = jax.lax.scan(body_fun, init_vals, xs)\n+    for f, y in zip(in_fwd, outs):\n+      if f is not None:\n+        self.assertAllClose(y, xs[f])\n+\n+    final_ref = body_fun(body_fun(init_vals, [x[0] for x in xs])[0], [x[1] for x in xs])[0]\n+    self.assertAllClose(final, final_ref, check_dtypes=False)\n+\n   def test_scan_diff_of_print(self):\n     # ref: https://github.com/jax-ml/jax/issues/28738\n     def f(c, _):\n@@ -3311,6 +3364,14 @@ def g(x):\n     eqn_jaxpr = jaxpr.eqns[0].params[\"jaxpr\"]\n     self.assertIn(\"debug_callback\", [e.primitive.name for e in eqn_jaxpr.eqns])\n \n+  def test_scan_input_to_output_forwarding(self):\n+    def f(c, x):\n+      return c + 1, x\n+    def g(x):\n+      return jax.lax.scan(f, 0, x)\n+    jaxpr = jax.make_jaxpr(g)(jnp.arange(3.))\n+    self.assertLen(jaxpr.eqns[0].params[\"jaxpr\"].jaxpr.outvars, 1)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas/Fuser] Add custom_vjp_call rule for physicalize\n\nPiperOrigin-RevId: 764763254",
        "output": [
            "```diff\nCommit: 42977e51816b9eb42c7360abe05f56cad70e894a\nDate: 2025-05-29T16:33:08Z\nURL: https://github.com/jax-ml/jax/commit/42977e51816b9eb42c7360abe05f56cad70e894a\nFiles changed: 2\nAdditions: +21, Deletions: -0\ndiff --git a/jax/_src/pallas/fuser/BUILD b/jax/_src/pallas/fuser/BUILD\nindex a4c3402f5309..d5b5d128241d 100644\n--- a/jax/_src/pallas/fuser/BUILD\n+++ b/jax/_src/pallas/fuser/BUILD\n@@ -115,6 +115,7 @@ pytype_strict_library(\n         \"//jax\",\n         \"//jax:api_util\",\n         \"//jax:core\",\n+        \"//jax:custom_derivatives\",\n         \"//jax:dtypes\",\n         \"//jax:partial_eval\",\n         \"//jax:source_info_util\",\ndiff --git a/jax/_src/pallas/fuser/fusible_dtype.py b/jax/_src/pallas/fuser/fusible_dtype.py\nindex 7d9c2ca67855..09cd8f57dbc1 100644\n--- a/jax/_src/pallas/fuser/fusible_dtype.py\n+++ b/jax/_src/pallas/fuser/fusible_dtype.py\n@@ -22,6 +22,7 @@\n import jax\n from jax._src import api_util\n from jax._src import core\n+from jax._src import custom_derivatives\n from jax._src import dtypes\n from jax._src import linear_util as lu\n from jax._src import source_info_util\n@@ -312,6 +313,25 @@ def _cond_physicalize_rule(ctx: Context, *args, branches, **kwargs):\n _physicalize_rules[conditionals.cond_p] = _cond_physicalize_rule\n \n \n+def _custom_vjp_call_physicalize_rule(\n+    ctx: Context, *args, call_jaxpr, num_consts, fwd_jaxpr_thunk, bwd, **kwargs\n+):\n+  _assert_no_fusion_types(ctx.avals_out)\n+  new_jaxpr = physicalize_closed_jaxpr(call_jaxpr)\n+  fun = lu.wrap_init(core.jaxpr_as_fun(new_jaxpr),\n+                     debug_info=call_jaxpr.jaxpr.debug_info)\n+  fwd = custom_derivatives.lift_fwd(num_consts, fwd_jaxpr_thunk)\n+  new_fwd = lu.wrap_init(physicalize(fwd.f_transformed), debug_info=fwd.debug_info)\n+  const_avals, _ = util.split_list(new_jaxpr.in_avals, [num_consts])\n+  bwd = custom_derivatives._handle_consts_in_bwd(bwd, const_avals)\n+  new_bwd = lu.wrap_init(physicalize(bwd.f_transformed), debug_info=bwd.debug_info)\n+  return custom_derivatives.custom_vjp_call_p.bind(\n+      fun, new_fwd, new_bwd, *args, **kwargs\n+  )\n+\n+_physicalize_rules[custom_derivatives.custom_vjp_call_p] = _custom_vjp_call_physicalize_rule\n+\n+\n def _run_state_rule(ctx: Context, *args, jaxpr, which_linear, is_initialized):\n   _assert_no_fusion_types(ctx.avals_in)\n   _assert_no_fusion_types(ctx.avals_out)\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Apply extensive input to extensive output forwarding in scan.",
        "output": [
            "```diff\nCommit: 770eff03dfbc225011535cb32ab92ae40cff679b\nDate: 2025-05-29T14:59:39Z\nURL: https://github.com/jax-ml/jax/commit/770eff03dfbc225011535cb32ab92ae40cff679b\nFiles changed: 2\nAdditions: +82, Deletions: -1\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 83c31928d7cb..b9ce8ae09380 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -341,7 +341,7 @@ def _create_jaxpr(init):\n   # If the body forwards an input carry to an output carry, that input is\n   # read-only and can be moved to be a const. Doing so can lead to efficiency\n   # wins, e.g. if the scan is inside a cond with a batched predicate.\n-  carry_fwd, _ = split_list(pe._jaxpr_forwarding(jaxpr.jaxpr), [num_carry])\n+  carry_fwd, ext_fwd = split_list(pe._jaxpr_forwarding(jaxpr.jaxpr), [num_carry])\n   move_to_const = [len(consts) + i == f for i, f in enumerate(carry_fwd)]\n   if any(move_to_const):\n     jaxpr = pe.prune_closed_jaxpr_outputs(\n@@ -352,12 +352,32 @@ def _create_jaxpr(init):\n     consts = [*new_consts, *consts]\n     num_carry -= len(new_consts)\n \n+  # When an extensive output is forwarded from an extensive input, we can\n+  # avoid copying it by pruning it from the jaxpr and forwarding manually. We\n+  # don't need to update the indexing based on the optimization above since it\n+  # doesn't change the total number of consts and carries combined, and\n+  # `ext_fwd` already only includes the extensive outputs. But, we do remove\n+  # the number of consts from the index since we're going to use it to index\n+  # into `in_flat`, which doesn't include consts.\n+  ext_to_ext_fwd = [\n+      in_idx - len(consts) if in_idx is not None and\n+      in_idx >= num_carry + len(consts) else None for in_idx in ext_fwd]\n+  jaxpr = pe.prune_closed_jaxpr_outputs(\n+      jaxpr, [True] * num_carry + [i is None for i in ext_to_ext_fwd])\n+\n   out = scan_p.bind(*consts, *in_flat,\n                     reverse=reverse, length=length, jaxpr=jaxpr,\n                     num_consts=len(consts), num_carry=num_carry,\n                     linear=(False,) * (len(consts) + len(in_flat)),\n                     unroll=unroll, _split_transpose=_split_transpose)\n \n+  # Apply input to output forwarding that was computed above.\n+  carry_out, out = split_list(out, [num_carry])\n+  out_ = iter(out)\n+  out = [next(out_) if f is None else _maybe_put(in_flat[f]) for f in ext_to_ext_fwd]\n+  assert next(out_, None) is None\n+  out = [*carry_out, *out]\n+\n   if any(move_to_const):\n     out = pe.merge_lists(move_to_const + [False] * num_ys, out, new_consts)\n \ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex 54dff47fea32..2f1e154627f4 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -3299,6 +3299,59 @@ def body_fun(c, _):\n     outs_ref = body_fun(body_fun(init_vals, [x[0] for x in xs])[0], [x[1] for x in xs])[0]\n     self.assertAllClose(outs, outs_ref, check_dtypes=False)\n \n+  @parameterized.parameters(itertools.product(range(3), repeat=4))\n+  @jtu.run_on_devices(\"cpu\")\n+  def test_scan_forwarding_correctness(\n+      self,\n+      seed,\n+      num_body_consts,\n+      num_const_fwds,\n+      num_input_fwds):\n+\n+    num_carry = num_const_fwds + 4\n+    num_xs = num_input_fwds + 2\n+    num_ys = num_xs + 1\n+\n+    rng = np.random.RandomState(seed)\n+    carry_perm = rng.permutation(num_carry)\n+    carry_iperm = np.argsort(carry_perm)\n+\n+    xs_perm = rng.permutation(num_xs)\n+    ys_perm = rng.permutation(num_ys)\n+    f = np.arange(num_xs)\n+    f = [f[i] if idx < num_input_fwds else None for idx, i in enumerate(xs_perm)]\n+    f += [None]\n+    in_fwd = [f[i] for i in ys_perm]\n+\n+    body_consts = [rng.randn(3) for _ in range(num_body_consts)]\n+    init_vals = list(rng.uniform(size=num_carry))\n+\n+    def body_fun(c, x):\n+      c = [c[i] for i in carry_iperm]\n+      carry_fwds, carry_dont_fwd = split_list(c, [num_const_fwds])\n+      carry_dont_fwd = [jnp.sin(x) * sum(jnp.sum(c) for c in body_consts)\n+                        for x in carry_dont_fwd]\n+      new_c_perm = [*carry_fwds, *carry_dont_fwd]\n+      new_c = [new_c_perm[i] for i in carry_perm]\n+\n+      x = [x[i] for i in xs_perm]\n+      x_fwd, x_dont_fwd = split_list(x, [num_input_fwds])\n+      x_dont_fwd = [jnp.cos(x) * sum(jnp.sum(c) for c in body_consts)\n+                    for x in x_dont_fwd]\n+      y = [*x_fwd, *x_dont_fwd, 0]\n+      y = [y[i] for i in ys_perm]\n+\n+      return new_c, y\n+\n+    xs = list(rng.uniform(size=(num_xs, 2)))\n+    final, outs = jax.lax.scan(body_fun, init_vals, xs)\n+    for f, y in zip(in_fwd, outs):\n+      if f is not None:\n+        self.assertAllClose(y, xs[f])\n+\n+    final_ref = body_fun(body_fun(init_vals, [x[0] for x in xs])[0], [x[1] for x in xs])[0]\n+    self.assertAllClose(final, final_ref, check_dtypes=False)\n+\n   def test_scan_diff_of_print(self):\n     # ref: https://github.com/jax-ml/jax/issues/28738\n     def f(c, _):\n@@ -3311,6 +3364,14 @@ def g(x):\n     eqn_jaxpr = jaxpr.eqns[0].params[\"jaxpr\"]\n     self.assertIn(\"debug_callback\", [e.primitive.name for e in eqn_jaxpr.eqns])\n \n+  def test_scan_input_to_output_forwarding(self):\n+    def f(c, x):\n+      return c + 1, x\n+    def g(x):\n+      return jax.lax.scan(f, 0, x)\n+    jaxpr = jax.make_jaxpr(g)(jnp.arange(3.))\n+    self.assertLen(jaxpr.eqns[0].params[\"jaxpr\"].jaxpr.outvars, 1)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Merge pull request #29081 from DanisNone:main\n\nPiperOrigin-RevId: 764711492",
        "output": [
            "```diff\nCommit: 5f1105432285b479eb7566cecdba4aade3f1030e\nDate: 2025-05-29T14:00:42Z\nURL: https://github.com/jax-ml/jax/commit/5f1105432285b479eb7566cecdba4aade3f1030e\nFiles changed: 2\nAdditions: +33, Deletions: -2\ndiff --git a/jax/_src/lax/other.py b/jax/_src/lax/other.py\nindex 00e15ef6a91d..6da39b0c2405 100644\n--- a/jax/_src/lax/other.py\n+++ b/jax/_src/lax/other.py\n@@ -287,3 +287,35 @@ def _logaddexp_jvp(primals, tangents):\n   tangent_out = lax.add(lax.mul(t1, lax.exp(lax.sub(_replace_inf(x1), _replace_inf(primal_out)))),\n                         lax.mul(t2, lax.exp(lax.sub(_replace_inf(x2), _replace_inf(primal_out)))))\n   return primal_out, tangent_out\n+\n+\n+@custom_jvp\n+def logaddexp2(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n+  \"\"\"Compute log2(exp2(x1) + exp2(x2)) avoiding overflow.\"\"\"\n+  x1_arr = lax.asarray(x1)\n+  x2_arr = lax.asarray(x2)\n+  assert x1_arr.dtype == x2_arr.dtype\n+\n+  amax = lax.max(x1_arr, x2_arr)\n+  invln2 = lax._const(amax, 1/np.log(2))\n+  if dtypes.isdtype(x1_arr.dtype, \"real floating\"):\n+    delta = lax.sub(x1_arr, x2_arr)\n+    return lax.select(lax._isnan(delta),\n+                      lax.add(x1_arr, x2_arr),  # NaNs or infinities of the same sign.\n+                      lax.add(amax, lax.mul(invln2, lax.log1p(lax.exp2(lax.neg(lax.abs(delta)))))))\n+  elif dtypes.isdtype(x1_arr.dtype, \"complex floating\"):\n+    delta = lax.sub(lax.add(x1_arr, x2_arr), lax.mul(amax, lax._const(amax, 2)))\n+    out = lax.add(amax, lax.mul(invln2, lax.log1p(lax.exp2(delta))))\n+    return lax.complex(lax.real(out), _wrap_between(lax.imag(out), np.pi / np.log(2)))\n+  else:\n+    raise ValueError(f\"logaddexp2 requires floating-point or complex inputs; got {x1_arr.dtype}\")\n+\n+\n+@logaddexp2.defjvp\n+def _logaddexp2_jvp(primals, tangents):\n+  x1, x2 = primals\n+  t1, t2 = tangents\n+  primal_out = logaddexp2(x1, x2)\n+  tangent_out = lax.add(lax.mul(t1, lax.exp2(lax.sub(_replace_inf(x1), _replace_inf(primal_out)))),\n+                        lax.mul(t2, lax.exp2(lax.sub(_replace_inf(x2), _replace_inf(primal_out)))))\n+  return primal_out, tangent_out\ndiff --git a/jax/_src/numpy/ufuncs.py b/jax/_src/numpy/ufuncs.py\nindex 77b1220214ed..d722534e3136 100644\n--- a/jax/_src/numpy/ufuncs.py\n+++ b/jax/_src/numpy/ufuncs.py\n@@ -2782,8 +2782,7 @@ def logaddexp2(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n     Array(True, dtype=bool)\n   \"\"\"\n   x1, x2 = promote_args_inexact(\"logaddexp2\", x1, x2)\n-  ln2 = float(np.log(2))\n-  return logaddexp(x1 * ln2, x2 * ln2) / ln2\n+  return lax_other.logaddexp2(x1, x2)\n \n \n @export\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] `emit_pipeline` now allows specifying a carry\n\nPiperOrigin-RevId: 764672556",
        "output": [
            "```diff\nCommit: 50253f1acfe5434a7a50507fd940641984d9e7a7\nDate: 2025-05-29T11:35:25Z\nURL: https://github.com/jax-ml/jax/commit/50253f1acfe5434a7a50507fd940641984d9e7a7\nFiles changed: 2\nAdditions: +65, Deletions: -19\ndiff --git a/jax/_src/pallas/mosaic_gpu/pipeline.py b/jax/_src/pallas/mosaic_gpu/pipeline.py\nindex a7f8d32677b0..f85b73b6b946 100644\n--- a/jax/_src/pallas/mosaic_gpu/pipeline.py\n+++ b/jax/_src/pallas/mosaic_gpu/pipeline.py\n@@ -16,13 +16,12 @@\n \n from __future__ import annotations\n \n-from typing import Protocol, TypeVar\n from collections.abc import Callable, Sequence\n import dataclasses\n import functools\n import itertools as it\n import math\n-from typing import Any\n+from typing import Any, Protocol, TypeVar\n \n import jax\n from jax import api_util\n@@ -176,28 +175,44 @@ def __eq__(self, other: _Slice) -> jax.Array:  # type: ignore\n \n \n def emit_pipeline(\n-    body: Callable[..., None],\n+    body: Callable[..., T],\n     *,\n     grid: pallas_core.TupleGrid,\n     in_specs: Sequence[pallas_core.BlockSpec] = (),\n     out_specs: Sequence[pallas_core.BlockSpec] = (),\n     max_concurrent_steps: int = 1,\n     delay_release: int = 0,\n+    init_carry: T | None = None,\n ):\n-  \"\"\"Creates a function to emit a manual pipeline within a Pallas kernel.\n+  r\"\"\"Creates a function to emit a manual pipeline within a Pallas kernel.\n \n   Args:\n-    body: The pipeline body, called with the indices for the current step, the\n-      input refs, followed by the output refs.\n-    grid: The grid to use for the pipeline.\n-    in_specs: The block specs for the inputs.\n-    out_specs: The block specs for the outputs.\n-    max_concurrent_steps: The maximum number of sequential stages that are\n-      active concurrently. Defaults to 1.\n-    delay_release: The number of steps to wait before reusing the input/output\n-      references. Defaults to 0, and must be strictly smaller than\n-      ``max_concurrent_steps``. Generally, you'll want to set it to 1 if you\n-      don't await the WGMMA in the body.\n+    body: The pipeline body function, which is called with\n+\n+      - ``indices``: Tuple of current loop indices.\n+      - ``*input_refs``: SMEM refs for inputs.\n+      - ``*output_refs``: SMEM refs for outputs.\n+\n+      If ``init_carry`` is provided, ``body`` receives an additional argument\n+      ``carry`` -- the carry from the previous iteration. It must then return\n+      the next carry value.\n+    grid: The grid dimensions for the pipeline.\n+    in_specs: A sequence of :class:`~jax.experimental.pallas.BlockSpec`\\s\n+      for inputs.\n+    out_specs: A sequence of :class:`~jax.experimental.pallas.BlockSpec`\\s\n+      for outputs.\n+    max_concurrent_steps: Maximum concurrently active pipeline stages.\n+    delay_release: Number of steps to delay before reusing input/output\n+      references. Must be ``< max_concurrent_steps``. Useful for hiding WGMMA\n+      latency (typically set to 1).\n+    init_carry: Optional initial carry. If provided, ``body`` handles\n+      carry-over state between iterations, and the pipeline returns the\n+      final carry.\n+\n+  Returns:\n+    A function that, when called with GMEM input and output refs, executes the\n+    pipeline and returns the final carry value (if ``init_carry`` was used),\n+    otherwise it returns None.\n   \"\"\"\n   if max_concurrent_steps <= delay_release:\n     raise ValueError(\n@@ -278,7 +293,7 @@ def scoped_pipeline(\n \n     def loop_body(step, carry):\n       slot = lax.rem(step, max_concurrent_steps)\n-      indices, fetch_indices, last_store_slices = carry\n+      indices, fetch_indices, last_store_slices, prev_body_carry = carry\n \n       if barrier_ref is not None:\n         # Wait for the current GMEM->SMEM copy to complete, if any.\n@@ -289,12 +304,13 @@ def loop_body(step, carry):\n             max_concurrent_steps - (1 + delay_release), wait_read_only=True\n         )\n \n-      body(\n+      next_body_carry = body(\n           indices,\n           *(\n               bref.get_ref_for_slot(slot)\n               for bref in it.chain(in_brefs, out_brefs)\n           ),\n+          *(prev_body_carry,) if init_carry is not None else (),\n       )\n \n       if copies_out_in_loop:\n@@ -346,6 +362,7 @@ def do_fetch():\n           _inc_grid_by_1(indices, grid),\n           _inc_grid_by_1(fetch_indices, grid),\n           new_store_slices,\n+          next_body_carry if init_carry is not None else None,\n       )\n \n     # Invariant: ``indices`` and ``fetch_indices`` are always\n@@ -360,8 +377,11 @@ def do_fetch():\n         else (_Slice(-1, -1),) * len(bref.spec.block_shape)\n         for bref in out_brefs\n     ]\n-    last_indices, _, _ = lax.fori_loop(\n-        0, num_steps, loop_body, (indices, fetch_indices, last_store_slices)\n+    last_indices, _, _, final_carry = lax.fori_loop(\n+        0,\n+        num_steps,\n+        loop_body,\n+        (indices, fetch_indices, last_store_slices, init_carry),\n     )\n \n     # Outputs invariant to the sequential axis are never written from inside the\n@@ -378,6 +398,7 @@ def do_fetch():\n \n     # Finalize the pipeline.\n     gpu_primitives.wait_smem_to_gmem(0)\n+    return final_carry if init_carry is not None else None\n \n   return pipeline\n \ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 608cfcba2465..f9a23e7be9d0 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -2841,6 +2841,31 @@ def kernel_body(_, x_smem, o_smem):\n     )\n     np.testing.assert_array_equal(kernel_fn(x), x + 1.0)\n \n+  def test_emit_with_carry(self):\n+    num_steps = 4\n+\n+    def kernel(o_gmem):\n+      plgpu.emit_pipeline(\n+          kernel_body,\n+          out_specs=[pl.BlockSpec((64, 64), lambda i: (0, i))],\n+          grid=(num_steps,),\n+          max_concurrent_steps=2,\n+          init_carry=0,\n+      )(o_gmem)\n+\n+    def kernel_body(_, o_smem, carry):\n+      o_smem[...] = lax.broadcast(carry, o_smem.shape)\n+      return carry + 1\n+\n+    kernel_fn = self.pallas_call(\n+        kernel,\n+        out_specs=pl.BlockSpec(memory_space=plgpu.GMEM),\n+        out_shape=jax.ShapeDtypeStruct((64, num_steps * 64), jnp.int32),\n+    )\n+    np.testing.assert_array_equal(\n+        kernel_fn(), jnp.tile(jnp.repeat(jnp.arange(num_steps), 64), (64, 1))\n+    )\n+\n \n class PipelineWGTest(\n     PipelineTest, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[mosaic_gpu] Use `DIScopeForLLVMFuncOpPass` from MLIR instead of its Triton fork\n\nPiperOrigin-RevId: 764652343",
        "output": [
            "```diff\nCommit: 89828819a383911db365661e8c7b6203299c35fb\nDate: 2025-05-29T10:23:14Z\nURL: https://github.com/jax-ml/jax/commit/89828819a383911db365661e8c7b6203299c35fb\nFiles changed: 2\nAdditions: +8, Deletions: -15\ndiff --git a/jaxlib/mosaic/gpu/BUILD b/jaxlib/mosaic/gpu/BUILD\nindex fc1abb9397d5..d2abea0048d6 100644\n--- a/jaxlib/mosaic/gpu/BUILD\n+++ b/jaxlib/mosaic/gpu/BUILD\n@@ -151,6 +151,8 @@ cc_library(\n         \":nvshmem\",\n         \":passes\",\n         \":target\",\n+        \"//jaxlib/cuda:cuda_vendor\",\n+        \"//jaxlib/mosaic/dialect/gpu:mosaic_gpu\",\n         \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/cleanup\",\n@@ -181,6 +183,7 @@ cc_library(\n         \"@llvm-project//mlir:IR\",\n         \"@llvm-project//mlir:IndexToLLVM\",\n         \"@llvm-project//mlir:LLVMDialect\",\n+        \"@llvm-project//mlir:LLVMIRTransforms\",\n         \"@llvm-project//mlir:LLVMToLLVMIRTranslation\",\n         \"@llvm-project//mlir:MathDialect\",\n         \"@llvm-project//mlir:MathToLLVM\",\n@@ -200,16 +203,11 @@ cc_library(\n         \"@llvm-project//mlir:UBToLLVM\",\n         \"@llvm-project//mlir:VectorDialect\",\n         \"@llvm-project//mlir:VectorToLLVM\",\n-        \"//jaxlib/cuda:cuda_vendor\",\n-        \"//jaxlib/mosaic/dialect/gpu:mosaic_gpu\",\n+        \"@tsl//tsl/profiler/lib:traceme\",\n         \"@xla//xla/ffi\",\n         \"@xla//xla/ffi:ffi_api\",\n         \"@xla//xla/service:custom_call_status\",\n         \"@xla//xla/service:custom_call_target_registry\",\n-        \"@tsl//tsl/profiler/lib:traceme\",\n-        # TODO(slebedev): Remove once enable-line-info is merged into the upstream\n-        # ensure-debug-info-scope-on-llvm-func pass in MLIR.\n-        \"@triton//:TritonLLVMIR\",\n     ],\n     alwayslink = True,\n )\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 524d0ffe23ab..5253d4590658 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -68,6 +68,7 @@ limitations under the License.\n #include \"mlir/Dialect/GPU/Transforms/Passes.h\"\n #include \"mlir/Dialect/LLVMIR/LLVMDialect.h\"\n #include \"mlir/Dialect/LLVMIR/NVVMDialect.h\"\n+#include \"mlir/Dialect/LLVMIR/Transforms/Passes.h\"\n #include \"mlir/Dialect/Math/IR/Math.h\"\n #include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n #include \"mlir/Dialect/MemRef/Transforms/Passes.h\"\n@@ -102,7 +103,6 @@ limitations under the License.\n #include \"xla/service/custom_call_status.h\"\n #include \"xla/service/custom_call_target_registry.h\"\n #include \"tsl/profiler/lib/traceme.h\"\n-#include \"triton/Target/LLVMIR/Passes.h\"\n \n namespace {\n \n@@ -340,7 +340,7 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n     mosaic::gpu::registerConvertGpuToLLVMPass();\n     mosaic::gpu::registerByvalInsertionPass();\n     mlir::arith::registerArithExpandOpsPass();\n-    mlir::registerLLVMDIScopePass();\n+    mlir::LLVM::registerDIScopeForLLVMFuncOpPass();\n     return true;\n   });\n   bool emit_line_info = getenv(\"MOSAIC_GPU_LINE_INFO\") != nullptr;\n@@ -360,10 +360,7 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n         convert-scf-to-cf,\n         convert-nvvm-to-llvm,\n         expand-strided-metadata,\n-        nvvm-attach-target{)\",\n-      // TODO(slebedev): Always use O=3 once\n-      // https://github.com/llvm/llvm-project/pull/140146 is merged.\n-      emit_line_info ? \"O=0\" : \"O=3\", \" chip=\", sm, \" fast=false features=+\",\n+        nvvm-attach-target{O=3 chip=)\", sm, \" fast=false features=+\",\n       ptx_isa,\n       R\"( ftz=false  module= triple=nvptx64-nvidia-cuda},\n         lower-affine,\n@@ -381,9 +378,7 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n         gpu.module(reconcile-unrealized-casts),\n         mosaic-convert-gpu-to-llvm,\n         )\",\n-      // TODO(slebedev): Switch to the ensure-debug-info-scope-on-llvm-func\n-      // pass in MLIR once Triton upstreams its changes.\n-      emit_line_info ? \"enable-line-info,\" : \"\",\n+      emit_line_info ? \"ensure-debug-info-scope-on-llvm-func{emission-kind=DebugDirectivesOnly},\" : \"\",\n       \"gpu-module-to-binary{format=\",\n       mlir::gpu::stringifyCompilationTarget(target).str(),\n       (!nvshmem_path.empty() ? \" l=\" + nvshmem_path : \"\"),\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "A more numerically stable implementation of logaddexp2",
        "output": [
            "```diff\nCommit: a4a31ecd8476a4d10dedcf226d5a116af2d056b3\nDate: 2025-05-29T06:02:14Z\nURL: https://github.com/jax-ml/jax/commit/a4a31ecd8476a4d10dedcf226d5a116af2d056b3\nFiles changed: 2\nAdditions: +33, Deletions: -2\ndiff --git a/jax/_src/lax/other.py b/jax/_src/lax/other.py\nindex 00e15ef6a91d..6da39b0c2405 100644\n--- a/jax/_src/lax/other.py\n+++ b/jax/_src/lax/other.py\n@@ -287,3 +287,35 @@ def _logaddexp_jvp(primals, tangents):\n   tangent_out = lax.add(lax.mul(t1, lax.exp(lax.sub(_replace_inf(x1), _replace_inf(primal_out)))),\n                         lax.mul(t2, lax.exp(lax.sub(_replace_inf(x2), _replace_inf(primal_out)))))\n   return primal_out, tangent_out\n+\n+\n+@custom_jvp\n+def logaddexp2(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n+  \"\"\"Compute log2(exp2(x1) + exp2(x2)) avoiding overflow.\"\"\"\n+  x1_arr = lax.asarray(x1)\n+  x2_arr = lax.asarray(x2)\n+  assert x1_arr.dtype == x2_arr.dtype\n+\n+  amax = lax.max(x1_arr, x2_arr)\n+  invln2 = lax._const(amax, 1/np.log(2))\n+  if dtypes.isdtype(x1_arr.dtype, \"real floating\"):\n+    delta = lax.sub(x1_arr, x2_arr)\n+    return lax.select(lax._isnan(delta),\n+                      lax.add(x1_arr, x2_arr),  # NaNs or infinities of the same sign.\n+                      lax.add(amax, lax.mul(invln2, lax.log1p(lax.exp2(lax.neg(lax.abs(delta)))))))\n+  elif dtypes.isdtype(x1_arr.dtype, \"complex floating\"):\n+    delta = lax.sub(lax.add(x1_arr, x2_arr), lax.mul(amax, lax._const(amax, 2)))\n+    out = lax.add(amax, lax.mul(invln2, lax.log1p(lax.exp2(delta))))\n+    return lax.complex(lax.real(out), _wrap_between(lax.imag(out), np.pi / np.log(2)))\n+  else:\n+    raise ValueError(f\"logaddexp2 requires floating-point or complex inputs; got {x1_arr.dtype}\")\n+\n+\n+@logaddexp2.defjvp\n+def _logaddexp2_jvp(primals, tangents):\n+  x1, x2 = primals\n+  t1, t2 = tangents\n+  primal_out = logaddexp2(x1, x2)\n+  tangent_out = lax.add(lax.mul(t1, lax.exp2(lax.sub(_replace_inf(x1), _replace_inf(primal_out)))),\n+                        lax.mul(t2, lax.exp2(lax.sub(_replace_inf(x2), _replace_inf(primal_out)))))\n+  return primal_out, tangent_out\ndiff --git a/jax/_src/numpy/ufuncs.py b/jax/_src/numpy/ufuncs.py\nindex 77b1220214ed..d722534e3136 100644\n--- a/jax/_src/numpy/ufuncs.py\n+++ b/jax/_src/numpy/ufuncs.py\n@@ -2782,8 +2782,7 @@ def logaddexp2(x1: ArrayLike, x2: ArrayLike, /) -> Array:\n     Array(True, dtype=bool)\n   \"\"\"\n   x1, x2 = promote_args_inexact(\"logaddexp2\", x1, x2)\n-  ln2 = float(np.log(2))\n-  return logaddexp(x1 * ln2, x2 * ln2) / ln2\n+  return lax_other.logaddexp2(x1, x2)\n \n \n @export\n\n```"
        ],
        "from_id": [
            "DanisNone"
        ]
    },
    {
        "text_input": "[Pallas Fuser] Add support for basic PRNG op fusion\n\nPiperOrigin-RevId: 764490044",
        "output": [
            "```diff\nCommit: 37a9ac23681d85e5e5663d21fac4b88224715ba3\nDate: 2025-05-29T00:46:45Z\nURL: https://github.com/jax-ml/jax/commit/37a9ac23681d85e5e5663d21fac4b88224715ba3\nFiles changed: 4\nAdditions: +186, Deletions: -46\ndiff --git a/jax/_src/pallas/core.py b/jax/_src/pallas/core.py\nindex 7950f90bc377..a05f97eb122f 100644\n--- a/jax/_src/pallas/core.py\n+++ b/jax/_src/pallas/core.py\n@@ -272,6 +272,7 @@ class MemorySpace(enum.Enum):\n   ANY = \"any\"  # Unrestricted memory space (usually HBM)\n   ERROR = \"error\"  # Memory space for checkify errors.\n   INDEX = \"index\"  # Memory space for scalar prefetch arguments.\n+  KEY = \"key\"  # Memory space for PRNG keys.\n \n   def __str__(self) -> str:\n     return self.value\ndiff --git a/jax/_src/pallas/fuser/block_spec.py b/jax/_src/pallas/fuser/block_spec.py\nindex 3e9ff497bf1e..4f9d1c344429 100644\n--- a/jax/_src/pallas/fuser/block_spec.py\n+++ b/jax/_src/pallas/fuser/block_spec.py\n@@ -29,6 +29,7 @@\n from jax._src import core\n from jax._src import custom_derivatives\n from jax._src import pjit\n+from jax._src import prng\n from jax._src import state\n from jax._src import tree_util\n from jax._src import util\n@@ -215,7 +216,7 @@ def _wrap_block_spec_scalar_prefetch(\n     block_spec: pallas_core.BlockSpec,\n     num_grid_args: int,\n ) -> pallas_core.BlockSpec:\n-  if block_spec is pallas_core.no_block_spec:\n+  if block_spec is pallas_core.no_block_spec or block_spec.index_map is None:\n     return block_spec\n \n   def new_index_map(*args_and_scalar_prefetch):\n@@ -272,11 +273,12 @@ def wrapped(*args, **kwargs):\n     )\n     assert all(used_invars)\n     assert all(used_consts)\n+    read_usage_env = compute_usage(jaxpr, jaxpr_out_usages)\n     in_block_specs, env, read_usage_env = _pull_block_spec(\n         jaxpr,\n         tuple(flat_block_specs),\n-        jaxpr_out_usages,\n         scalar_prefetch_handler=scalar_prefetch_handler,\n+        read_usage_env=read_usage_env,\n         grid=grid,\n     )\n     kernel_fn = make_kernel_function(\n@@ -307,8 +309,8 @@ def wrapped(*args, **kwargs):\n def _pull_block_spec(\n     jaxpr: core.Jaxpr,\n     out_block_specs: tuple[pallas_core.BlockSpec, ...],\n-    out_usages,\n     *,\n+    read_usage_env: Callable[[core.Var], set[Usage]],\n     scalar_prefetch_handler: Any | None = None,\n     grid: tuple[int | jax.Array, ...],\n ) -> tuple[\n@@ -316,7 +318,6 @@ def _pull_block_spec(\n     tuple[dict[core.Var, pallas_core.BlockSpec], dict[int, Any]],\n     Any,\n ]:\n-  read_usage_env = compute_usage(jaxpr, out_usages)\n   jaxpr_invar_usages = util.safe_map(read_usage_env, jaxpr.invars)\n   env: dict[core.Var, pallas_core.BlockSpec] = {}\n   scalar_prefetch_fn_env = {}\n@@ -456,6 +457,8 @@ def _get_block_aval(bs, aval):\n       return aval\n     if bs is pallas_core.no_block_spec or bs is None:\n       return _no_aval\n+    if bs.block_shape is None:\n+      return aval\n     return aval.update(shape=_remove_nones(bs.block_shape))  # pytype: disable=attribute-error\n \n   in_block_avals = [\n@@ -830,7 +833,10 @@ def register_binop_rule(prim: core.Primitive):\n register_binop_rule(lax.eq_p)\n register_binop_rule(lax.gt_p)\n register_binop_rule(lax.ge_p)\n+register_binop_rule(lax.or_p)\n+register_binop_rule(lax.xor_p)\n register_binop_rule(lax.and_p)\n+register_binop_rule(lax.shift_right_logical_p)\n register_binop_rule(ad_util.add_any_p)\n \n \n@@ -1473,6 +1479,68 @@ def _convert_element_type_pull_rule(\n   return [block_spec]\n \n \n+@register_eval_rule(lax.bitcast_convert_type_p)\n+def _bitcast_convert_type_eval_rule(eval_ctx: KernelEvalContext, x, new_dtype):\n+  return jax.lax.bitcast_convert_type(x, new_dtype)\n+\n+\n+@register_pull_block_spec_rule(lax.bitcast_convert_type_p)\n+def _bitcast_convert_type_pull_rule(\n+    ctx: PullRuleContext,\n+    block_spec: pallas_core.BlockSpec,\n+    *,\n+    new_dtype: jnp.dtype,\n+):\n+  old_dtype = ctx.avals_in[0].dtype  # pytype: disable=attribute-error\n+  if old_dtype.itemsize != new_dtype.itemsize:\n+    raise NotImplementedError(\n+        'bitcast_convert_type with different bitwidths not supported yet:'\n+        f' {old_dtype=}, {new_dtype=}'\n+    )\n+  return [block_spec]\n+\n+\n+@register_eval_rule(prng.random_bits_p)\n+def _random_bits_eval_rule(eval_ctx: KernelEvalContext, key, bit_width, shape):\n+  del shape\n+  block_spec = eval_ctx.out_block_specs[0]\n+  indices = eval_ctx.get_out_block_indices()[0]\n+  block_shape = block_spec.block_shape\n+  # This is the important part here: we fold in block indices into the key so\n+  # each block gets different random numbers.\n+  for idx in indices:\n+    key = jax.random.fold_in(key, idx)\n+  return prng.random_bits(key, bit_width=bit_width, shape=block_shape)\n+\n+\n+@register_pull_block_spec_rule(prng.random_bits_p)\n+def _random_bits_pull_rule(\n+    ctx: PullRuleContext,\n+    block_spec: pallas_core.BlockSpec,\n+    **_,\n+):\n+  del ctx, block_spec\n+  key_block_spec = pallas_core.BlockSpec(\n+      block_shape=None, memory_space=pallas_core.MemorySpace.KEY\n+  )\n+  return [key_block_spec]\n+\n+@register_eval_rule(prng.random_wrap_p)\n+def _random_wrap_eval_rule(eval_ctx: KernelEvalContext, arr, *, impl):\n+  del eval_ctx\n+  return jax.random.wrap_key_data(arr, impl=impl)\n+\n+@register_pull_block_spec_rule(prng.random_wrap_p)\n+def _random_wrap_pull_rule(\n+    ctx: PullRuleContext,\n+    block_spec: pallas_core.BlockSpec,\n+    *,\n+    impl\n+):\n+  del ctx, block_spec, impl\n+  return [pallas_core.BlockSpec(block_shape=None)]\n+\n+\n @register_eval_rule(lax.iota_p)\n def _iota_eval_rule(\n     eval_ctx: KernelEvalContext, *, dimension, shape, dtype, sharding\n@@ -1599,12 +1667,13 @@ def _jit_eval_rule(ctx: KernelEvalContext, *args, jaxpr, **kwargs):\n     raise NotImplementedError('pjit with consts not supported yet')\n   out_tree = tree_util.tree_structure(tuple(jaxpr.outvars))\n   in_tree = tree_util.tree_structure((tuple(jaxpr.invars), {}))\n-  read_usage_env = compute_usage(jaxpr, ctx.out_usages)\n+  def read_usage_env(_: core.Var):\n+    return {Usage.REGULAR}\n   _, env, _ = _pull_block_spec(\n       jaxpr,\n       ctx.out_block_specs,\n-      ctx.out_usages,\n       scalar_prefetch_handler=ctx.scalar_prefetch_handler,\n+      read_usage_env=read_usage_env,\n       grid=ctx.grid,\n   )\n   kernel_fn = make_kernel_function(\n@@ -1628,11 +1697,13 @@ def _jit_pull_block_spec_rule(\n   jaxpr, consts = jaxpr.jaxpr, jaxpr.consts\n   if consts:\n     raise NotImplementedError('pjit with consts not supported yet')\n+  def read_usage_env(_: core.Var):\n+    return {Usage.REGULAR}\n   in_block_specs, _, _ = _pull_block_spec(\n       jaxpr,\n       out_block_specs,\n-      ctx.out_usages,\n       scalar_prefetch_handler=ctx.scalar_prefetch_handler,\n+      read_usage_env=read_usage_env,\n       grid=ctx.grid,\n   )\n   return in_block_specs\n@@ -1657,13 +1728,14 @@ def _custom_jvp_call_eval_rule(\n     raise NotImplementedError('custom_jvp_call with consts not supported yet')\n   out_tree = tree_util.tree_structure(tuple(jaxpr.outvars))\n   in_tree = tree_util.tree_structure((tuple(jaxpr.invars), {}))\n-  read_usage_env = compute_usage(jaxpr, ctx.out_usages)\n+  def read_usage_env(_: core.Var):\n+    return {Usage.REGULAR}\n   _, env, _ = _pull_block_spec(\n       jaxpr,\n       ctx.out_block_specs,\n-      ctx.out_usages,\n       scalar_prefetch_handler=ctx.scalar_prefetch_handler,\n       grid=ctx.grid,\n+      read_usage_env=read_usage_env,\n   )\n   kernel_fn = make_kernel_function(\n       jaxpr,\n@@ -1686,12 +1758,14 @@ def _custom_jvp_call_pull_block_spec_rule(\n   jaxpr, consts = call_jaxpr.jaxpr, call_jaxpr.consts\n   if consts:\n     raise NotImplementedError('custom_jvp_call with consts not supported yet')\n+  def read_usage_env(_: core.Var):\n+    return {Usage.REGULAR}\n   in_block_specs, _, _ = _pull_block_spec(\n       jaxpr,\n       out_block_specs,\n-      ctx.out_usages,\n       scalar_prefetch_handler=ctx.scalar_prefetch_handler,\n       grid=ctx.grid,\n+      read_usage_env=read_usage_env,\n   )\n   return in_block_specs\n \ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 635c473620c3..c6aaf77199b5 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -222,7 +222,11 @@ def _memory_space_to_tpu_memory_space(memory_space: MemorySpace | None\n     case pallas_core.MemorySpace.ANY:\n       # Map the general ANY memory space to TPU ANY memory space\n       return TPUMemorySpace.ANY\n-    case pallas_core.MemorySpace.ERROR | pallas_core.MemorySpace.INDEX:\n+    case (\n+        pallas_core.MemorySpace.ERROR\n+        | pallas_core.MemorySpace.INDEX\n+        | pallas_core.MemorySpace.KEY\n+    ):\n       return TPUMemorySpace.SMEM\n     case TPUMemorySpace():\n       # Leave the memory space unchanged\n@@ -365,7 +369,7 @@ def _get_arg_type(\n ):\n   memory_space = None\n   if isinstance(aval, pallas_core.AbstractMemoryRef):\n-    memory_space = aval.memory_space\n+    memory_space = _memory_space_to_tpu_memory_space(aval.memory_space)\n     # We assume unannotated memory refs are in VMEM\n     if memory_space is None:\n       memory_space = TPUMemorySpace.VMEM\n@@ -595,10 +599,10 @@ def _check_block_mappings(\n     rank = len(bm.block_shape)\n     # TODO(necula): add tests for SMEM blocks with trivial windowing\n     # We support scalars too\n-    if (bm.block_aval.memory_space == tpu_core.TPUMemorySpace.SMEM and\n-        bm.has_trivial_window()):\n+    memory_space = _memory_space_to_tpu_memory_space(bm.block_aval.memory_space)\n+    if memory_space == tpu_core.TPUMemorySpace.SMEM and bm.has_trivial_window():\n       continue\n-    if bm.block_aval.memory_space == tpu_core.TPUMemorySpace.SEMAPHORE:\n+    if memory_space == tpu_core.TPUMemorySpace.SEMAPHORE:\n       continue\n \n     def err_details():\n@@ -614,8 +618,10 @@ def err_details():\n           \"The Pallas TPU lowering currently supports only blocks of \"\n           \"rank >= 1. \" + err_details())\n \n-    if (bm.block_aval.memory_space == tpu_core.TPUMemorySpace.ANY and\n-        not bm.has_trivial_window()):\n+    if (\n+        memory_space == tpu_core.TPUMemorySpace.ANY\n+        and not bm.has_trivial_window()\n+    ):\n       raise ValueError(\n           \"The Pallas TPU lowering currently supports in memory space ANY \"\n           \"only blocks having the same block shape as the array shape \"\n@@ -3723,10 +3729,16 @@ def new_lowering(key, bit_width, shape):\n \n @register_lowering_rule(prng.random_fold_in_p)\n def random_fold_in_lowering(ctx, keys, msgs):\n-  keys_aval, _ = ctx.avals_in\n+  keys_aval, msgs_aval = ctx.avals_in\n   impl = keys_aval.dtype._impl\n   fold_in_lowering = lower_fun(impl.fold_in, multiple_results=False)\n-  return fold_in_lowering(ctx, keys, msgs)\n+  if pl_random.is_pallas_impl(impl):\n+    return fold_in_lowering(ctx, keys, msgs)\n+  else:\n+    ctx = dataclasses.replace(ctx,\n+                        avals_in=[jax_core.physical_aval(keys_aval), msgs_aval],\n+                        avals_out=map(jax_core.physical_aval, ctx.avals_out))\n+    return fold_in_lowering(ctx, keys, msgs)\n \n \n @register_lowering_rule(prng.random_unwrap_p)\ndiff --git a/tests/pallas/fuser_block_spec_test.py b/tests/pallas/fuser_block_spec_test.py\nindex 5c0ef0352b1c..b348ba971c38 100644\n--- a/tests/pallas/fuser_block_spec_test.py\n+++ b/tests/pallas/fuser_block_spec_test.py\n@@ -755,7 +755,9 @@ def f():\n     )(new_values)\n     self.assertLen(value_block_specs, 1)\n     self.assertEmpty(scalar_prefetch_values)\n-    self.assertEqual(value_block_specs[0].block_shape, (pl.Element(128, (0, 16)), 128))\n+    self.assertEqual(\n+        value_block_specs[0].block_shape, (pl.Element(128, (0, 16)), 128)\n+    )\n     self.assertEqual(value_block_specs[0].index_map(0, 1, 2), (16, 1))\n     self.assertEqual(value_block_specs[0].index_map(1, 1, 2), (128 + 16, 1))\n \n@@ -801,10 +803,13 @@ def f(x):\n   def test_basic_swap(self):\n     value = jnp.arange((512 * 1024), dtype=jnp.int32).reshape((512, 1024)) * 2\n     x = jnp.zeros((256, 512), dtype=jnp.int32)\n+\n     def outer(refs):\n       ref, y_ref = refs\n+\n       def f(x):\n         return ref.swap(x)\n+\n       in_type = jax.ShapeDtypeStruct((512, 1024), jnp.int32)\n       f2, new_values, scalar_prefetch_values = block_spec_lib.get_fusion_values(\n           f, in_type\n@@ -826,73 +831,121 @@ def f(x):\n       self.assertEqual(x_block_spec.index_map(3, 2, 1), (3, 1))\n \n       y_ref[...] = kernel_fn((0, 1, 1), scalar_prefetch_values, (ref,), x)\n+\n     y = jnp.zeros((256, 512), jnp.int32)\n     _, y = pl.run_state(outer)((value, y))\n     np.testing.assert_array_equal(y, value[:256, 512:1024])\n \n   def test_basic_get(self):\n     value = jnp.arange((512 * 1024), dtype=jnp.int32).reshape((512, 1024)) * 2\n+\n     def outer(refs):\n       ref, y_ref = refs\n+\n       def f():\n         return ref.get()\n \n       block_spec = pl.BlockSpec((256, 512), lambda i, j, k: (i, k))\n-      kernel_fn, (), _ = (\n-          block_spec_lib.pull_block_spec(\n-              f,\n-              block_spec,\n-              grid=(2, 3, 4),\n-              scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n-          )()\n-      )\n+      kernel_fn, (), _ = block_spec_lib.pull_block_spec(\n+          f,\n+          block_spec,\n+          grid=(2, 3, 4),\n+          scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+      )()\n       y_ref[...] = kernel_fn((0, 1, 1), ())\n+\n     y = jnp.zeros((256, 512), jnp.int32)\n     _, y = pl.run_state(outer)((value, y))\n     np.testing.assert_array_equal(y, value[:256, 512:1024])\n \n   def test_get_with_squeezed_block_spec(self):\n-    value = jnp.arange((4 * 512 * 1024), dtype=jnp.int32).reshape((4, 512, 1024)) * 2\n+    value = (\n+        jnp.arange((4 * 512 * 1024), dtype=jnp.int32).reshape((4, 512, 1024))\n+        * 2\n+    )\n+\n     def outer(refs):\n       ref, y_ref = refs\n+\n       def f():\n         return ref.get()\n \n-      block_spec = pl.BlockSpec((pl.Squeezed(), 256, 512), lambda i, j, k: (j, i, k))\n-      kernel_fn, (), _ = (\n-          block_spec_lib.pull_block_spec(\n-              f,\n-              block_spec,\n-              grid=(2, 3, 4),\n-              scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n-          )()\n+      block_spec = pl.BlockSpec(\n+          (pl.Squeezed(), 256, 512), lambda i, j, k: (j, i, k)\n       )\n+      kernel_fn, (), _ = block_spec_lib.pull_block_spec(\n+          f,\n+          block_spec,\n+          grid=(2, 3, 4),\n+          scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+      )()\n       y_ref[...] = kernel_fn((0, 3, 1), ())\n+\n     y = jnp.zeros((256, 512), jnp.int32)\n     _, y = pl.run_state(outer)((value, y))\n     np.testing.assert_array_equal(y, value[3, :256, 512:1024])\n \n   def test_get_with_squeezed_indexer(self):\n-    value = jnp.arange((4 * 512 * 1024), dtype=jnp.int32).reshape((4, 512, 1024)) * 2\n+    value = (\n+        jnp.arange((4 * 512 * 1024), dtype=jnp.int32).reshape((4, 512, 1024))\n+        * 2\n+    )\n+\n     def outer(refs):\n       ref, y_ref = refs\n+\n       def f():\n         return ref[3]\n \n       block_spec = pl.BlockSpec((256, 512), lambda i, j, k: (i, k))\n-      kernel_fn, (), _ = (\n-          block_spec_lib.pull_block_spec(\n-              f,\n-              block_spec,\n-              grid=(2, 3, 4),\n-              scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n-          )()\n-      )\n+      kernel_fn, (), _ = block_spec_lib.pull_block_spec(\n+          f,\n+          block_spec,\n+          grid=(2, 3, 4),\n+          scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+      )()\n       y_ref[...] = kernel_fn((0, 2, 1), ())\n+\n     y = jnp.zeros((256, 512), jnp.int32)\n     _, y = pl.run_state(outer)((value, y))\n     np.testing.assert_array_equal(y, value[3, :256, 512:1024])\n \n+  def test_random_noise(self):\n+    key = jax.random.key(0, impl='threefry2x32')\n+\n+    def f(key):\n+      return jax.random.uniform(key, (512, 512), dtype=jnp.float32)\n+\n+    f2, new_values, scalar_prefetch_values = block_spec_lib.get_fusion_values(\n+        f, key\n+    )\n+    self.assertEmpty(new_values)\n+    self.assertEmpty(scalar_prefetch_values)\n+\n+    block_spec = pl.BlockSpec((128, 256), lambda i, j: (i, j))\n+    kernel_fn, (value_block_specs, key_block_spec), _ = (\n+        block_spec_lib.pull_block_spec(\n+            f2,\n+            block_spec,\n+            grid=(4, 2),\n+            scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+        )(new_values, key)\n+    )\n+    self.assertEmpty(value_block_specs)\n+    self.assertEqual(key_block_spec.memory_space, pl.MemorySpace.KEY)\n+    self.assertIsNone(key_block_spec.block_shape)\n+    @jax.jit\n+    def gen(idx):\n+      k = key\n+      for i in idx:\n+        k = jax.random.fold_in(k, i)\n+      return jax.random.uniform(k, (128, 256), dtype=jnp.float32)\n+    for i in range(4):\n+      for j in range(2):\n+        out = kernel_fn((i, j), scalar_prefetch_values, (), key)\n+        out_ref = gen((i, j))\n+        np.testing.assert_array_equal(out, out_ref)\n+\n \n class PullBlockSpecHOPTest(jtu.JaxTestCase):\n \n\n```"
        ],
        "from_id": [
            "sharadmv",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29047 from mattjj:returning-non-jaxtype\n\nPiperOrigin-RevId: 764449037",
        "output": [
            "```diff\nCommit: 22b4f263b216a48e7db6c02cc8be8af258c18ef4\nDate: 2025-05-28T22:50:11Z\nURL: https://github.com/jax-ml/jax/commit/22b4f263b216a48e7db6c02cc8be8af258c18ef4\nFiles changed: 2\nAdditions: +38, Deletions: -1\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 6ea16ec8e8ba..3c499429a663 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -2243,7 +2243,7 @@ def trace_to_jaxpr_dynamic(\n     try:\n       with core.set_current_trace(trace):\n         ans = fun.call_wrapped(*in_tracers)\n-\n+      _check_returned_jaxtypes(fun.debug_info, ans)\n       out_tracers = map(partial(trace.to_jaxpr_tracer, source_info=source_info), ans)\n       _check_no_returned_refs(fun.debug_info, out_tracers)\n       jaxpr, consts, attrs_tracked = trace.to_jaxpr(out_tracers, fun.debug_info)\n@@ -2255,6 +2255,20 @@ def trace_to_jaxpr_dynamic(\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   return jaxpr, [v.aval for v in jaxpr.outvars], consts, attrs_tracked\n \n+def _check_returned_jaxtypes(dbg, out_tracers):\n+  for i, x in enumerate(out_tracers):\n+    try:\n+      core.typeof(x)\n+    except TypeError:\n+      if (dbg and len(paths := dbg.result_paths()) > i and\n+          (p := paths[i].removeprefix('result'))):\n+        extra = f' at output component {p}'\n+      else:\n+        extra = ''\n+      raise TypeError(\n+      f\"function {dbg.func_src_info} traced for {dbg.traced_for} returned a \"\n+      f\"value of type {type(x)}{extra}, which is not a valid JAX type\") from None\n+\n def _check_no_returned_refs(\n     dbg: core.DebugInfo,\n     out_tracers: Sequence[DynamicJaxprTracer]\ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex f5b74e1e10d6..584eb0eda496 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -5058,6 +5058,29 @@ def test_ensure_compile_time_eval_no_leaks(self):\n     with jax.ensure_compile_time_eval():\n       jnp.linalg.solve(jnp.eye(3), jnp.ones(3))  # doesn't crash\n \n+  def test_returned_non_jaxtype(self):\n+\n+    class TestEnum(enum.Enum):\n+      A = enum.auto()\n+\n+    @jax.tree_util.register_dataclass\n+    @dataclasses.dataclass\n+    class TestClass3:\n+      test_enum_field: TestEnum = dataclasses.field(metadata=dict(static=True))\n+      test_data_field: int\n+\n+    def test_jax_function(test_class: TestClass3) -> TestEnum:\n+      return test_class.test_enum_field\n+\n+    jitted_test_function = jax.jit(test_jax_function)\n+    with self.assertRaisesRegex(TypeError, \"returned a value of type\"):\n+        jitted_test_function(\n+            TestClass3(\n+                test_data_field=1,\n+                test_enum_field=TestEnum.A,\n+            )\n+        )\n+\n \n class RematTest(jtu.JaxTestCase):\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Integrate LLVM at llvm/llvm-project@2b8bff6f66fd\n\nUpdates LLVM usage to match\n[2b8bff6f66fd](https://github.com/llvm/llvm-project/commit/2b8bff6f66fd)\n\nPiperOrigin-RevId: 764439621",
        "output": [
            "```diff\nCommit: 2dc69daec8ed513668e155bc3c9973f2d5d32b05\nDate: 2025-05-28T22:24:52Z\nURL: https://github.com/jax-ml/jax/commit/2dc69daec8ed513668e155bc3c9973f2d5d32b05\nFiles changed: 4\nAdditions: +15, Deletions: -11\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu.td b/jaxlib/mosaic/dialect/tpu/tpu.td\nindex 505478b9ad72..766900cd07e4 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu.td\n+++ b/jaxlib/mosaic/dialect/tpu/tpu.td\n@@ -513,8 +513,8 @@ def TPU_FPToSIOp : TPU_Op<\"fptosi\", [Pure, ElementwiseMappable]> {\n // Internal operation. All arith.sitofp operations that change the bitwidth\n // must be canonicalized to this operation.\n def TPU_SIToFPOp : TPU_Op<\"sitofp\", [Pure, ElementwiseMappable]> {\n-  let arguments = (ins AnyVectorOfAnyRank:$in, TPU_RoundingModeEnum:$rounding_mode);\n-  let results = (outs AnyVectorOfAnyRank:$output);\n+  let arguments = (ins AnyType:$in, TPU_RoundingModeEnum:$rounding_mode);\n+  let results = (outs AnyType:$output);\n   let assemblyFormat = [{ $in attr-dict `:` type($in) `->` type($output) }];\n }\n \ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 6502a9c6682e..53d8712d5274 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -1161,10 +1161,10 @@ LogicalResult tpu_sitofp_rule(RewriteContext &ctx, Operation &op,\n         FAILUREOR_ASSIGN_OR_RETURN(\n             xla::Array<Value> vregs,\n             ext_op_rule_impl(ctx, builder, sitofp_op, layout_in, layout_out));\n-        sitofp_op.replaceAllUsesWith(assemble(builder, sitofp_op.getType(),\n-                                              layout_out, std::move(vregs),\n-                                              ctx.target_shape)\n-                                         .getResult());\n+        sitofp_op.replaceAllUsesWith(\n+            assemble(builder, cast<VectorType>(sitofp_op.getType()), layout_out,\n+                     std::move(vregs), ctx.target_shape)\n+                .getResult());\n         sitofp_op.erase();\n         return success();\n       }\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\nindex 1d8ea1299f04..c963cff0be50 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n@@ -715,10 +715,12 @@ LogicalResult canonicalize_sitofp(const CanonicalizeContext &ctx,\n     }\n   }\n   if (is_vector) {\n-    x = builder.create<arith::SIToFPOp>(\n-        VectorType::get(src_vty.getShape(), builder.getF32Type()), x);\n+    x = builder.create<tpu::SIToFPOp>(\n+        VectorType::get(src_vty.getShape(), builder.getF32Type()), x,\n+        tpu::RoundingMode::kToNearestEven);\n   } else {\n-    x = builder.create<arith::SIToFPOp>(builder.getF32Type(), x);\n+    x = builder.create<tpu::SIToFPOp>(builder.getF32Type(), x,\n+                                      tpu::RoundingMode::kToNearestEven);\n   }\n   if (dst_bitwidth < 32) {\n     x = builder.create<arith::TruncFOp>(op.getType(), x);\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex 976e31cb55f4..14d1fb2104fa 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -155,8 +155,10 @@ class VectorLayoutInferer {\n           return failure();\n         }\n       } else if (auto op = dyn_cast<tpu::SIToFPOp>(any_op);\n-                 op && op.getIn().getType().getElementTypeBitWidth() <\n-                           op.getType().getElementTypeBitWidth()) {\n+                 op &&\n+                 cast<VectorType>(op.getIn().getType())\n+                         .getElementTypeBitWidth() <\n+                     cast<VectorType>(op.getType()).getElementTypeBitWidth()) {\n         if (inferExt(&any_op).failed()) {\n           return failure();\n         }\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Change all us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build container to us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build.\n\nThese containers are the same (same build script), but they are just in a different repositories.\n\nPiperOrigin-RevId: 764435895",
        "output": [
            "```diff\nCommit: c5b908ceca710900e0e099b451b7c138ffcdf0ec\nDate: 2025-05-28T22:14:44Z\nURL: https://github.com/jax-ml/jax/commit/c5b908ceca710900e0e099b451b7c138ffcdf0ec\nFiles changed: 15\nAdditions: +18, Deletions: -18\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex d6173a809500..09c9d173e0d0 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -41,7 +41,7 @@ jobs:\n         # Explicitly set the shell to bash\n         shell: bash\n     runs-on: ${{ inputs.runner }}\n-    container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest') ||\n+    container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest') ||\n                    (contains(inputs.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') }}\n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: ${{ inputs.python }}\ndiff --git a/.github/workflows/bazel_cpu_rbe.yml b/.github/workflows/bazel_cpu_rbe.yml\nindex 2f8eb2c33cee..3eff0932adcb 100644\n--- a/.github/workflows/bazel_cpu_rbe.yml\n+++ b/.github/workflows/bazel_cpu_rbe.yml\n@@ -28,7 +28,7 @@ jobs:\n   run_tests:\n     if: github.event.repository.fork == false\n     runs-on: ${{ matrix.runner }}\n-    container: ${{ (contains(matrix.runner, 'linux-x86') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest') ||\n+    container: ${{ (contains(matrix.runner, 'linux-x86') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest') ||\n                    (contains(matrix.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') }}\n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: ${{ matrix.python }}\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 348d19763989..458589199c53 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -51,7 +51,7 @@ jobs:\n         # Explicitly set the shell to bash\n         shell: bash\n     runs-on: ${{ inputs.runner }}\n-    container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest\"\n+    container: \"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-cuda12.8-cudnn9.8:latest\"\n \n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: ${{ inputs.python }}\ndiff --git a/.github/workflows/bazel_cuda_rbe.yml b/.github/workflows/bazel_cuda_rbe.yml\nindex cd4e9a021cfc..2c57b35587fa 100644\n--- a/.github/workflows/bazel_cuda_rbe.yml\n+++ b/.github/workflows/bazel_cuda_rbe.yml\n@@ -28,7 +28,7 @@ jobs:\n   run_tests:\n     if: github.event.repository.fork == false\n     runs-on: ${{ matrix.runner }}\n-    container: 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest'\n+    container: 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest'\n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: ${{ matrix.python }}\n       JAXCI_ENABLE_X64: ${{ matrix.enable-x_64 }}\ndiff --git a/.github/workflows/bazel_optional_h100_b200.yml b/.github/workflows/bazel_optional_h100_b200.yml\nindex 68fea50857a5..16c7bb95c16b 100644\n--- a/.github/workflows/bazel_optional_h100_b200.yml\n+++ b/.github/workflows/bazel_optional_h100_b200.yml\n@@ -27,7 +27,7 @@ jobs:\n   run_tests:\n     if: ${{ github.event.repository.fork == false && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || contains(github.event.pull_request.labels.*.name, 'CI Optional GPU Presubmit')) }}\n     runs-on: linux-x86-a4-224-b200-1gpu\n-    container: 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest'\n+    container: 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-cuda12.8-cudnn9.8:latest'\n     name: \"Bazel single B200 CUDA tests\"\n # End Presubmit Naming Check github-cuda-presubmits\n     steps:\n@@ -72,7 +72,7 @@ jobs:\n   run_multiaccelerator_tests:\n     if: ${{ github.event.repository.fork == false && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || contains(github.event.pull_request.labels.*.name, 'CI Optional GPU Presubmit')) }}\n     runs-on: linux-x86-a3-8g-h100-8gpu\n-    container: 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest'\n+    container: 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-cuda12.8-cudnn9.8:latest'\n     name: \"Bazel multiple H100 CUDA tests\"\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 90c888471b73..72d554aa5d1b 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -100,7 +100,7 @@ jobs:\n \n     runs-on: ${{ inputs.runner }}\n \n-    container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest') ||\n+    container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest') ||\n                    (contains(inputs.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') ||\n                    (contains(inputs.runner, 'windows-x86') && null) }}\n \ndiff --git a/.github/workflows/cloud-tpu-ci-nightly.yml b/.github/workflows/cloud-tpu-ci-nightly.yml\nindex 3ed560b04a88..1f096ce48e2d 100644\n--- a/.github/workflows/cloud-tpu-ci-nightly.yml\n+++ b/.github/workflows/cloud-tpu-ci-nightly.yml\n@@ -47,7 +47,7 @@ jobs:\n       LIBTPU_OLDEST_VERSION_DATE: 20250228\n       PYTHON: python${{ matrix.python-version }}\n     runs-on: ${{ matrix.tpu.runner }}\n-    container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n+    container: \"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\"\n     timeout-minutes: 180\n     defaults:\n       run:\ndiff --git a/.github/workflows/jax-array-api.yml b/.github/workflows/jax-array-api.yml\nindex 16a72fe34714..41879a6f2e9f 100644\n--- a/.github/workflows/jax-array-api.yml\n+++ b/.github/workflows/jax-array-api.yml\n@@ -15,7 +15,7 @@ permissions: {}\n jobs:\n   build:\n     runs-on: linux-x86-n2-16\n-    container: us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\n+    container: us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\n     strategy:\n       matrix:\n         python-version: [3.11]\ndiff --git a/.github/workflows/numpy_nightly.yml b/.github/workflows/numpy_nightly.yml\nindex d9a858216857..c0036ccf8f7f 100644\n--- a/.github/workflows/numpy_nightly.yml\n+++ b/.github/workflows/numpy_nightly.yml\n@@ -33,7 +33,7 @@ jobs:\n     strategy:\n       matrix:\n             python: [\"3.13\",]\n-    container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n+    container: \"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\"\n     name: \"CI - jaxlib head with NumPy nightly\"\n \n     env:\ndiff --git a/.github/workflows/oldest_supported_numpy.yml b/.github/workflows/oldest_supported_numpy.yml\nindex c7f1f1e38a26..67fc9f10e5ce 100644\n--- a/.github/workflows/oldest_supported_numpy.yml\n+++ b/.github/workflows/oldest_supported_numpy.yml\n@@ -23,7 +23,7 @@ jobs:\n       run:\n         shell: bash\n     runs-on: \"linux-x86-n2-64\"\n-    container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n+    container: \"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\"\n # Begin Presubmit Naming Check - name modification requires internal check to be updated\n     name: \"CI - Oldest Supported NumPy (Python 3.10, x64=0)\"\n # End Presubmit Naming Check github-oldest-supported-numpy-presubmit\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex d08cb520eab4..a92f2d96dc89 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -52,7 +52,7 @@ jobs:\n         # Explicitly set the shell to bash to override Windows's default (cmd)\n         shell: bash\n     runs-on: ${{ inputs.runner }}\n-    container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest') ||\n+    container: ${{ (contains(inputs.runner, 'linux-x86') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest') ||\n                    (contains(inputs.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') ||\n                    (contains(inputs.runner, 'windows-x86') && null) }}\n \ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex d095dbfb9e80..6fa4e14f8b85 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -65,9 +65,9 @@ jobs:\n     # Test the oldest and newest supported CUDA versions.\n     # If testing the CUDA packages from PyPI, then use the ml-build image which does not have any\n     # CUDA pckages installed on the system.\n-    container:  ${{ !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.1') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.1-cudnn9.8:latest') ||\n-                !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.8') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest') ||\n-                inputs.use-nvidia-pip-wheels && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest'}}\n+    container:  ${{ !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.1') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-cuda12.1-cudnn9.8:latest') ||\n+                !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.8') && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build-cuda12.8-cudnn9.8:latest') ||\n+                inputs.use-nvidia-pip-wheels && 'us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest'}}\n     name: \"${{ (contains(inputs.runner, 'h100') && 'h100') ||\n         (contains(inputs.runner, 'b200') && 'b200') ||\n         (contains(inputs.runner, 'l4') && 'l4') }}, CUDA ${{ inputs.cuda-version }}, py ${{ inputs.python }}, x64=${{ inputs.enable-x64 }}\"\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex ce5dcf0c9fc8..5f56b165c295 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -75,7 +75,7 @@ jobs:\n       run:\n         shell: bash\n     runs-on: ${{ inputs.runner }}\n-    container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n+    container: \"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\"\n     # Begin Presubmit Naming Check - name modification requires internal check to be updated\n     name: \"${{ inputs.tpu-type }}, py ${{ inputs.python }}, libtpu=${{ inputs.libtpu-version-type }}\"\n     # End Presubmit Naming Check github-tpu-presubmits\ndiff --git a/.github/workflows/wheel_tests_nightly_release.yml b/.github/workflows/wheel_tests_nightly_release.yml\nindex c536466c7dcb..6d25ee281c7b 100644\n--- a/.github/workflows/wheel_tests_nightly_release.yml\n+++ b/.github/workflows/wheel_tests_nightly_release.yml\n@@ -154,7 +154,7 @@ jobs:\n         fail-fast: false # don't cancel all jobs on failure\n         matrix:\n           python: [\"3.10\", \"3.13\", \"3.13-nogil\"]\n-    container:  \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n+    container:  \"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\"\n \n     # Verifies that JAX's release wheels can be installed\n     name: \"Verify release wheels install (Python ${{ matrix.python }})\"\ndiff --git a/ci/envs/docker.env b/ci/envs/docker.env\nindex a0f558520d45..5135b61ac45b 100644\n--- a/ci/envs/docker.env\n+++ b/ci/envs/docker.env\n@@ -29,7 +29,7 @@ export JAXCI_DOCKER_ARGS=\"\"\n # Linux x86 image for building JAX artifacts, running Pytests CPU/TPU tests, and\n # Bazel tests\n if [[ $os == \"linux\" ]] && [[ $arch == \"x86_64\" ]]; then\n-  export JAXCI_DOCKER_IMAGE=\"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n+  export JAXCI_DOCKER_IMAGE=\"us-docker.pkg.dev/ml-oss-artifacts-published/ml-public-container/ml-build:latest\"\n fi\n \n # Linux Aarch64 image for building JAX artifacts, running Pytests CPU tests, and\n\n```"
        ],
        "from_id": [
            "quoctruong",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Rework CUDA_ROOT logic a bit\n\nPiperOrigin-RevId: 764419062",
        "output": [
            "```diff\nCommit: 5aa339561871ccf037f1216237cf4e5db937376c\nDate: 2025-05-28T21:32:19Z\nURL: https://github.com/jax-ml/jax/commit/5aa339561871ccf037f1216237cf4e5db937376c\nFiles changed: 1\nAdditions: +18, Deletions: -7\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex bf6a04783be7..524d0ffe23ab 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -45,6 +45,7 @@ limitations under the License.\n #include \"absl/strings/str_replace.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n+// Leave this comment here. Internal Google business.\n #include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/Support/CodeGen.h\"\n #include \"llvm/Support/TargetSelect.h\"\n@@ -130,12 +131,17 @@ class TemporaryDirectory {\n   std::string path;\n };\n \n+const char *GetCUDARoot() {\n+  return getenv(\"CUDA_ROOT\");\n+}\n+\n absl::StatusOr<std::string> RunCUDATool(const char* tool,\n                                         const std::vector<const char*>& args,\n                                         bool stderr_to_stdout = true) {\n   CHECK(!args.empty() && args.back() == nullptr);\n-  const char* cuda_path_ptr = getenv(\"CUDA_ROOT\");\n-  if (!cuda_path_ptr) return absl::InternalError(\"Failed to get CUDA_ROOT\");\n+  const char* cuda_path_ptr = GetCUDARoot();\n+  if (!cuda_path_ptr)\n+    return absl::InternalError(\"Failed to get the CUDA toolkit path\");\n   std::string tool_path(cuda_path_ptr);\n   tool_path += \"/bin/\";\n   tool_path += tool;\n@@ -338,6 +344,10 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n     return true;\n   });\n   bool emit_line_info = getenv(\"MOSAIC_GPU_LINE_INFO\") != nullptr;\n+  const char *cuda_root = GetCUDARoot();\n+  if (!cuda_root) {\n+    return mlir::failure();\n+  }\n   return mlir::parsePassPipeline(absl::StrCat(\n       R\"(\n       builtin.module(\n@@ -374,11 +384,12 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n       // TODO(slebedev): Switch to the ensure-debug-info-scope-on-llvm-func\n       // pass in MLIR once Triton upstreams its changes.\n       emit_line_info ? \"enable-line-info,\" : \"\",\n-      R\"(\n-        gpu-module-to-binary{format=)\" +\n-          mlir::gpu::stringifyCompilationTarget(target).str() +\n-          (!nvshmem_path.empty() ? R\"( l=)\" + nvshmem_path : \"\") +\n-          (emit_line_info ? \"  opts=-lineinfo\" : \"\") + R\"(},\n+      \"gpu-module-to-binary{format=\",\n+      mlir::gpu::stringifyCompilationTarget(target).str(),\n+      (!nvshmem_path.empty() ? \" l=\" + nvshmem_path : \"\"),\n+      (emit_line_info ? \"  opts=-lineinfo\" : \"\"),\n+      \" toolkit=\", cuda_root,\n+      R\"(},\n         convert-math-to-llvm{approximate-log1p=true},\n         canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true},\n         cse,\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29074 from MichaelHudgins:actions-fixes\n\nPiperOrigin-RevId: 764408327",
        "output": [
            "```diff\nCommit: ea4049f224cdedf7454660133f619c754c23fa16\nDate: 2025-05-28T21:06:07Z\nURL: https://github.com/jax-ml/jax/commit/ea4049f224cdedf7454660133f619c754c23fa16\nFiles changed: 24\nAdditions: +102, Deletions: -53\ndiff --git a/.github/actionlint.yaml b/.github/actionlint.yaml\nnew file mode 100644\nindex 000000000000..e7ee1a086558\n--- /dev/null\n+++ b/.github/actionlint.yaml\n@@ -0,0 +1,20 @@\n+# Configuration related to self-hosted runner.\n+self-hosted-runner:\n+  labels:\n+    - \"linux-x86-n2-32\" # Linux X86 runner using the 32 vcpu n2-standard-32 machine.\n+    - \"linux-x86-n2-64\" # Linux X86 runner using the 64 vcpu n2-standard-64 machine.\n+    - \"linux-x86-g2-16-l4-1gpu\" # Linux X86 GPU runner using g2-standard-16 machine with 1 NVIDIA L4 GPU attached.\n+    - \"linux-x86-g2-48-l4-4gpu\" # Linux X86 GPU runner using g2-standard-48 machine with 4 NVIDIA L4 GPUs attached.\n+    - \"linux-x86-ct5lp-224-8tpu\" # Linux X86 TPU runner using ct5lp-hightpu-8t machine with 2x4 topology.\n+    - \"linux-arm64-c4a-16\" # Linux ARM64 CPU Runner using the 16 vcpu c4a-standard-16 machine.\n+    - \"linux-arm64-c4a-64\" # Linux ARM64 CPU Runner using the 64 vcpu c4a-standard-64 machine.\n+    - \"windows-x86-n2-16\" # Windows X86 runner using n2-standard-16 machine.\n+    - \"windows-x86-n2-64\" # Windows X86 runner using n2-standard-64 machine.\n+    - \"linux-x86-a4-224-b200-1gpu\" # Linux X86 GPU runner using 1 B200 GPU and 1/8 the resources of a a4-highgpu-8g machine\n+    - \"linux-x86-a3-8g-h100-8gpu\" # Linux X86 GPU runner using a3-highgpu-8g machine with 8 NVIDIA H100 GPUs attached.\n+    - \"linux-x86-ct6e-180-8tpu\" # Linux X86 TPU runner using ct6e-hightpu-8t machine with 2x4 topology.\n+    - \"linux-x86-ct6e-180-4tpu\" # Linux X86 TPU runner using ct6e-hightpu-4t machine with 2x2 topology.\n+    - \"linux-x86-ct4p-240-4tpu\" # Linux X86 TPU runner using ct4p-hightpu-4t machine with 2x2x1 topology.\n+    - \"linux-x86-n2-128\" # Linux X86 runner using the 128 vcpu n2-standard-128 machine.\n+    - \"linux-x86-n2-16\" # Linux X86 runner using the 16 vcpu n2-standard-16 machine.\n+    - \"linux-x86_64-cirrascale-64-8gpu-amd-mi250\" # AMD runner\ndiff --git a/.github/workflows/asan.yaml b/.github/workflows/asan.yaml\nindex ea69d92e552e..533d4381f474 100644\n--- a/.github/workflows/asan.yaml\n+++ b/.github/workflows/asan.yaml\n@@ -13,7 +13,7 @@ on:\n       - main\n     paths:\n       - '**/workflows/asan.yaml'\n-\n+permissions: {}\n jobs:\n   asan:\n     # Don't execute in fork due to runner type\n@@ -41,11 +41,13 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           path: jax\n+          persist-credentials: false\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           repository: python/cpython\n           path: cpython\n           ref: v3.13.0\n+          persist-credentials: false\n       - name: Build CPython with ASAN enabled\n         env:\n           ASAN_OPTIONS: detect_leaks=0\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex 7eb6d2ed27b4..d6173a809500 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -9,9 +9,7 @@\n #    - Executes the `run_bazel_test_cpu_py_import_rbe.sh` script, which performs the following actions:\n #      - Runs the Bazel CPU tests with py_import dependency.\n name: CI - Bazel CPU tests with py_import (RBE)\n-permissions:\n-  contents: read\n-\n+permissions: {}\n on:\n   workflow_call:\n     inputs:\n@@ -54,6 +52,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       # Halt for testing\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\ndiff --git a/.github/workflows/bazel_cpu_rbe.yml b/.github/workflows/bazel_cpu_rbe.yml\nindex a8b40c260260..2f8eb2c33cee 100644\n--- a/.github/workflows/bazel_cpu_rbe.yml\n+++ b/.github/workflows/bazel_cpu_rbe.yml\n@@ -18,7 +18,7 @@ on:\n     branches:\n       - main\n       - 'release/**'\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n@@ -53,6 +53,8 @@ jobs:\n # End Presubmit Naming Check github-cpu-presubmits\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 5a3ceaa8a4e8..348d19763989 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -44,7 +44,6 @@ on:\n         type: string\n         required: false\n         default: 'no'\n-\n jobs:\n   run-tests:\n     defaults:\n@@ -67,6 +66,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/bazel_cuda_rbe.yml b/.github/workflows/bazel_cuda_rbe.yml\nindex 83f651c0ef95..cd4e9a021cfc 100644\n--- a/.github/workflows/bazel_cuda_rbe.yml\n+++ b/.github/workflows/bazel_cuda_rbe.yml\n@@ -23,7 +23,7 @@ concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n   cancel-in-progress: ${{ !contains(github.ref, 'release/') && github.ref != 'main' }}\n-\n+permissions: {}\n jobs:\n   run_tests:\n     if: github.event.repository.fork == false\n@@ -49,6 +49,8 @@ jobs:\n # End Presubmit Naming Check github-cuda-presubmits\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\ndiff --git a/.github/workflows/bazel_optional_h100_b200.yml b/.github/workflows/bazel_optional_h100_b200.yml\nindex ec907280938e..68fea50857a5 100644\n--- a/.github/workflows/bazel_optional_h100_b200.yml\n+++ b/.github/workflows/bazel_optional_h100_b200.yml\n@@ -32,6 +32,8 @@ jobs:\n # End Presubmit Naming Check github-cuda-presubmits\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n@@ -74,6 +76,8 @@ jobs:\n     name: \"Bazel multiple H100 CUDA tests\"\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 7bca28c3190d..90c888471b73 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -90,10 +90,7 @@ on:\n       gcs_upload_uri:\n         description: \"GCS location prefix to where the artifacts were uploaded\"\n         value: ${{ jobs.build-artifacts.outputs.gcs_upload_uri }}\n-\n-permissions:\n-  contents: read\n-\n+permissions: {}\n jobs:\n   build-artifacts:\n     defaults:\n@@ -122,6 +119,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Enable RBE if building on Linux x86 or Windows x86\n         if: contains(inputs.runner, 'linux-x86') || contains(inputs.runner, 'windows-x86')\n         run: echo \"JAXCI_BUILD_ARTIFACT_WITH_RBE=1\" >> $GITHUB_ENV\ndiff --git a/.github/workflows/ci-build.yaml b/.github/workflows/ci-build.yaml\nindex 0769c698d5fe..ada470526ef8 100644\n--- a/.github/workflows/ci-build.yaml\n+++ b/.github/workflows/ci-build.yaml\n@@ -16,10 +16,7 @@ on:\n     branches:\n       - main\n \n-permissions:\n-  contents: read  # to fetch code\n-  actions: write  # to cancel previous workflows\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n@@ -30,6 +27,8 @@ jobs:\n     timeout-minutes: 5\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set up Python 3.11\n         uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n         with:\n@@ -65,6 +64,8 @@ jobs:\n             num_generated_cases: 1\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Image Setup\n       run: |\n         apt update\n@@ -106,6 +107,8 @@ jobs:\n         python-version: ['3.10']\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Set up Python ${{ matrix.python-version }}\n       uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n       with:\n@@ -136,6 +139,8 @@ jobs:\n         python-version: ['3.10']\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Image Setup\n       run: |\n         apt update\n@@ -166,6 +171,8 @@ jobs:\n             num_generated_cases: 10\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Set up Python ${{ matrix.python-version }}\n       uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n       with:\n@@ -198,6 +205,8 @@ jobs:\n     timeout-minutes: 30\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Set up Python\n       uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n       with:\ndiff --git a/.github/workflows/cloud-tpu-ci-nightly.yml b/.github/workflows/cloud-tpu-ci-nightly.yml\nindex c7394a498dd6..3ed560b04a88 100644\n--- a/.github/workflows/cloud-tpu-ci-nightly.yml\n+++ b/.github/workflows/cloud-tpu-ci-nightly.yml\n@@ -57,6 +57,8 @@ jobs:\n       # mandates using a specific commit for non-Google actions. We use\n       # https://github.com/sethvargo/ratchet to pin specific versions.\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       # Checkout XLA at head, if we're building jaxlib at head.\n       - name: Checkout XLA at head\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n@@ -64,6 +66,7 @@ jobs:\n         with:\n           repository: openxla/xla\n           path: xla\n+          persist-credentials: false\n       # We need to mark the GitHub workspace as safe as otherwise git commands will fail.\n       - name: Mark GitHub workspace as safe\n         run: |\ndiff --git a/.github/workflows/cloud-tpu-ci-presubmit.yml b/.github/workflows/cloud-tpu-ci-presubmit.yml\nindex 090259c0f849..fe1f2820b338 100644\n--- a/.github/workflows/cloud-tpu-ci-presubmit.yml\n+++ b/.github/workflows/cloud-tpu-ci-presubmit.yml\n@@ -25,9 +25,7 @@ on:\n \n # This should also be set to read-only in the project settings, but it's nice to\n # document and enforce the permissions here.\n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\ndiff --git a/.github/workflows/jax-array-api.yml b/.github/workflows/jax-array-api.yml\nindex 6419cb730b71..16a72fe34714 100644\n--- a/.github/workflows/jax-array-api.yml\n+++ b/.github/workflows/jax-array-api.yml\n@@ -11,22 +11,21 @@ on:\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n+permissions: {}\n jobs:\n   build:\n-\n     runs-on: linux-x86-n2-16\n     container: us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\n     strategy:\n       matrix:\n         python-version: [3.11]\n-\n     env:\n       PYTHON: \"python${{ matrix.python-version }}\"\n-\n     steps:\n     - name: Checkout jax\n       uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Checkout array-api-tests\n       uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       with:\n@@ -35,6 +34,7 @@ jobs:\n         ref: 'c847143beb8d769bde5dbcc063fe19ed7acc2f9b'  # Latest commit as of 2025-05-12\n         submodules: 'true'\n         path: 'array-api-tests'\n+        persist-credentials: false\n     - name: Install dependencies\n       run: |\n         $PYTHON -m uv pip install --system .[ci] pytest-xdist -r array-api-tests/requirements.txt\ndiff --git a/.github/workflows/k8s.yaml b/.github/workflows/k8s.yaml\nindex 5756b1afbbd2..81552f9bb43b 100644\n--- a/.github/workflows/k8s.yaml\n+++ b/.github/workflows/k8s.yaml\n@@ -1,5 +1,4 @@\n name: Multi-process run using K8s\n-\n on:\n   push:\n     branches:\n@@ -16,19 +15,14 @@ on:\n       - 'jax/_src/distributed.py'\n       - 'jax/_src/clusters/**'\n \n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n defaults:\n   run:\n     shell: bash -ex -o pipefail {0}\n-\n jobs:\n-\n   distributed-initialize:\n     runs-on: ubuntu-22.04\n     strategy:\n@@ -40,6 +34,7 @@ jobs:\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # ratchet:actions/checkout@v4\n         with:\n           path: jax\n+          persist-credentials: false\n           \n       - name: Start Minikube cluster\n         uses: medyagh/setup-minikube@cea33675329b799adccc9526aa5daccc26cd5052 # ratchet:medyagh/setup-minikube@v0.0.19\n@@ -105,7 +100,7 @@ jobs:\n           done\n \n       - name: Examine individual pod outputs\n-        if: \"!cancelled()\"\n+        if: ${{ !cancelled() }}\n         run: |\n           set +x\n           kubectl get pods --no-headers | awk '{print $1}' | while read -s pod; do\ndiff --git a/.github/workflows/metal_plugin_ci.yml b/.github/workflows/metal_plugin_ci.yml\nindex 2135e473d6be..c76153d48f10 100644\n--- a/.github/workflows/metal_plugin_ci.yml\n+++ b/.github/workflows/metal_plugin_ci.yml\n@@ -14,7 +14,7 @@ on:\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n+permissions: {}\n jobs:\n   jax-metal-plugin-test:\n \n@@ -30,6 +30,7 @@ jobs:\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n         with:\n           path: jax\n+          persist-credentials: false\n       - name: Setup build and test enviroment\n         run: |\n           rm -rf ${GITHUB_WORKSPACE}/jax-metal-venv\ndiff --git a/.github/workflows/numpy_nightly.yml b/.github/workflows/numpy_nightly.yml\nindex 17357e9f1dd8..d9a858216857 100644\n--- a/.github/workflows/numpy_nightly.yml\n+++ b/.github/workflows/numpy_nightly.yml\n@@ -18,9 +18,7 @@ on:\n   schedule:\n     - cron: \"0 */3 * * *\" # Run once every 3 hours\n \n-permissions:\n-      contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n@@ -46,12 +44,15 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Checkout ml_dtypes\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683\n         with:\n           repository: jax-ml/ml_dtypes\n           ref: main\n           path: ml_dtypes\n+          persist-credentials: false\n       # Halt for testing\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\ndiff --git a/.github/workflows/oldest_supported_numpy.yml b/.github/workflows/oldest_supported_numpy.yml\nindex a63cb0b1c614..c7f1f1e38a26 100644\n--- a/.github/workflows/oldest_supported_numpy.yml\n+++ b/.github/workflows/oldest_supported_numpy.yml\n@@ -10,12 +10,7 @@ on:\n   push:\n     branches:\n       - main\n-\n-# This should also be set to read-only in the project settings, but it's nice to\n-# document and enforce the permissions here.\n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n@@ -23,7 +18,7 @@ concurrency:\n \n jobs:\n   test-oldest-supported-numpy:\n-    if: \"github.event.repository.fork == false && !startsWith(github.head_ref, 'release/')\"\n+    if: ${{ github.event.repository.fork == false && !startsWith(github.head_ref, 'release/') }}\n     defaults:\n       run:\n         shell: bash\n@@ -40,6 +35,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Install Python dependencies\n         run: |\n           $JAXCI_PYTHON -m uv pip install -r build/test-requirements.txt\n@@ -52,8 +49,6 @@ jobs:\n       # Halt for testing\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n-        with:\n-          halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Pytest CPU tests\n         timeout-minutes: 30\n         run: ./ci/run_pytest_cpu.sh\n\\ No newline at end of file\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex 263bfd7ec9a9..d08cb520eab4 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -67,6 +67,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 5f8888526aad..d095dbfb9e80 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -79,6 +79,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 8c2457208e12..ce5dcf0c9fc8 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -11,7 +11,6 @@\n #      - Installs the downloaded jaxlib wheel.\n #      - Runs the TPU tests with Pytest.\n name: CI - Pytest TPU\n-\n on:\n   workflow_call:\n     inputs:\n@@ -90,6 +89,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/release-notification.yml b/.github/workflows/release-notification.yml\nindex a4a342ef6de7..6d68bf922655 100644\n--- a/.github/workflows/release-notification.yml\n+++ b/.github/workflows/release-notification.yml\n@@ -2,14 +2,21 @@ name: Google Chat Release Notification\n on:\n   release:\n     types: [published]\n+permissions: {}\n jobs:\n   build:\n+    env:\n+      WEBHOOK_URL: ${{ secrets.RELEASES_WEBHOOK }}\n+      RELEASE_NAME: ${{github.event.release.name}}\n+      PUBLISHED_AT: ${{github.event.release.published_at}}\n+      AUTHOR_LOGIN: ${{github.event.release.author.login}}\n+      RELEASE_URL: ${{github.event.release.url}}\n     runs-on: ubuntu-latest\n     steps:\n       - name: Google Chat Notification\n         run: |\n-          curl --location --request POST '${{ secrets.RELEASES_WEBHOOK }}' \\\n+          curl --location --request POST '${WEBHOOK_URL}' \\\n           --header 'Content-Type: application/json' \\\n           --data-raw '{\n-              \"text\": \"Release ${{github.event.release.name}} at ${{github.event.release.published_at}} by ${{github.event.release.author.login}}. <${{github.event.release.url}}|[github]>\"\n+              \"text\": \"Release $RELEASE_NAME at $PUBLISHED_AT by $AUTHOR_LOGIN. <$RELEASE_URL|[github]>\"\n           }'\ndiff --git a/.github/workflows/rocm-ci.yml b/.github/workflows/rocm-ci.yml\nindex 0ce20726ce63..4bfb8cb50a5e 100644\n--- a/.github/workflows/rocm-ci.yml\n+++ b/.github/workflows/rocm-ci.yml\n@@ -6,9 +6,7 @@ on:\n     branches:\n       - main\n \n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n \n@@ -36,6 +34,7 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n         with:\n           path: ${{ env.WORKSPACE_DIR }}\n+          persist-credentials: false\n       - name: Build JAX\n         run: |\n           pushd $WORKSPACE_DIR\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex ce4130c31a30..67ff8dd93e3d 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -3,7 +3,6 @@ name: CI - Free-threading and Thread Sanitizer (nightly)\n concurrency:\n   group: ${{ github.workflow }}-${{ github.ref }}\n   cancel-in-progress: true\n-\n on:\n   schedule:\n     - cron: \"0 5 * * *\" # Daily at 05:00 UTC == 00:00 EST == 21:00 PST\n@@ -14,7 +13,7 @@ on:\n     paths:\n       - '**/workflows/tsan.yaml'\n       - '**/workflows/tsan-suppressions*.txt'\n-\n+permissions: {}\n jobs:\n   tsan:\n     runs-on: linux-x86-n2-64\n@@ -50,17 +49,20 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           path: jax\n+          persist-credentials: false\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           repository: numpy/numpy\n           path: numpy\n           submodules: true\n+          persist-credentials: false\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         if: ${{ matrix.python-version == '3.14' }}\n         with:\n           repository: scipy/scipy\n           path: scipy\n           submodules: true\n+          persist-credentials: false\n \n       - name: Get year & week number\n         id: get-date\n@@ -81,6 +83,8 @@ jobs:\n           repository: python/cpython\n           path: cpython\n           ref: ${{ matrix.github_branch }}\n+          persist-credentials: false\n+\n \n       - name: Build TSAN CPython ${{ matrix.python-version }}\n         if: steps.cache-cpython-tsan-restore.outputs.cache-hit != 'true'\ndiff --git a/.github/workflows/upstream-nightly.yml b/.github/workflows/upstream-nightly.yml\nindex 349ddf0d96a3..23b8ac32d844 100644\n--- a/.github/workflows/upstream-nightly.yml\n+++ b/.github/workflows/upstream-nightly.yml\n@@ -19,7 +19,7 @@ on:\n       - main\n     paths:\n       - '**workflows/upstream-nightly.yml'\n-\n+permissions: {}\n jobs:\n   upstream-dev:\n     runs-on: linux-x86-n2-64\n@@ -33,6 +33,8 @@ jobs:\n         python-version: [\"3.13\"]\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set up Python ${{ matrix.python-version }}\n         uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n         with:\ndiff --git a/.github/workflows/wheel_tests_nightly_release.yml b/.github/workflows/wheel_tests_nightly_release.yml\nindex 7bad41647e6b..c536466c7dcb 100644\n--- a/.github/workflows/wheel_tests_nightly_release.yml\n+++ b/.github/workflows/wheel_tests_nightly_release.yml\n@@ -33,7 +33,7 @@ on:\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n+permissions: {}\n jobs:\n   run-pytest-cpu:\n     uses: ./.github/workflows/pytest_cpu.yml\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update actions to adhere to best practices",
        "output": [
            "```diff\nCommit: 36eeceb5ef4263559ede1bac18b63306c2e2ddd6\nDate: 2025-05-28T20:50:03Z\nURL: https://github.com/jax-ml/jax/commit/36eeceb5ef4263559ede1bac18b63306c2e2ddd6\nFiles changed: 24\nAdditions: +102, Deletions: -53\ndiff --git a/.github/actionlint.yaml b/.github/actionlint.yaml\nnew file mode 100644\nindex 000000000000..e7ee1a086558\n--- /dev/null\n+++ b/.github/actionlint.yaml\n@@ -0,0 +1,20 @@\n+# Configuration related to self-hosted runner.\n+self-hosted-runner:\n+  labels:\n+    - \"linux-x86-n2-32\" # Linux X86 runner using the 32 vcpu n2-standard-32 machine.\n+    - \"linux-x86-n2-64\" # Linux X86 runner using the 64 vcpu n2-standard-64 machine.\n+    - \"linux-x86-g2-16-l4-1gpu\" # Linux X86 GPU runner using g2-standard-16 machine with 1 NVIDIA L4 GPU attached.\n+    - \"linux-x86-g2-48-l4-4gpu\" # Linux X86 GPU runner using g2-standard-48 machine with 4 NVIDIA L4 GPUs attached.\n+    - \"linux-x86-ct5lp-224-8tpu\" # Linux X86 TPU runner using ct5lp-hightpu-8t machine with 2x4 topology.\n+    - \"linux-arm64-c4a-16\" # Linux ARM64 CPU Runner using the 16 vcpu c4a-standard-16 machine.\n+    - \"linux-arm64-c4a-64\" # Linux ARM64 CPU Runner using the 64 vcpu c4a-standard-64 machine.\n+    - \"windows-x86-n2-16\" # Windows X86 runner using n2-standard-16 machine.\n+    - \"windows-x86-n2-64\" # Windows X86 runner using n2-standard-64 machine.\n+    - \"linux-x86-a4-224-b200-1gpu\" # Linux X86 GPU runner using 1 B200 GPU and 1/8 the resources of a a4-highgpu-8g machine\n+    - \"linux-x86-a3-8g-h100-8gpu\" # Linux X86 GPU runner using a3-highgpu-8g machine with 8 NVIDIA H100 GPUs attached.\n+    - \"linux-x86-ct6e-180-8tpu\" # Linux X86 TPU runner using ct6e-hightpu-8t machine with 2x4 topology.\n+    - \"linux-x86-ct6e-180-4tpu\" # Linux X86 TPU runner using ct6e-hightpu-4t machine with 2x2 topology.\n+    - \"linux-x86-ct4p-240-4tpu\" # Linux X86 TPU runner using ct4p-hightpu-4t machine with 2x2x1 topology.\n+    - \"linux-x86-n2-128\" # Linux X86 runner using the 128 vcpu n2-standard-128 machine.\n+    - \"linux-x86-n2-16\" # Linux X86 runner using the 16 vcpu n2-standard-16 machine.\n+    - \"linux-x86_64-cirrascale-64-8gpu-amd-mi250\" # AMD runner\ndiff --git a/.github/workflows/asan.yaml b/.github/workflows/asan.yaml\nindex ea69d92e552e..533d4381f474 100644\n--- a/.github/workflows/asan.yaml\n+++ b/.github/workflows/asan.yaml\n@@ -13,7 +13,7 @@ on:\n       - main\n     paths:\n       - '**/workflows/asan.yaml'\n-\n+permissions: {}\n jobs:\n   asan:\n     # Don't execute in fork due to runner type\n@@ -41,11 +41,13 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           path: jax\n+          persist-credentials: false\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           repository: python/cpython\n           path: cpython\n           ref: v3.13.0\n+          persist-credentials: false\n       - name: Build CPython with ASAN enabled\n         env:\n           ASAN_OPTIONS: detect_leaks=0\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex 7eb6d2ed27b4..d6173a809500 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -9,9 +9,7 @@\n #    - Executes the `run_bazel_test_cpu_py_import_rbe.sh` script, which performs the following actions:\n #      - Runs the Bazel CPU tests with py_import dependency.\n name: CI - Bazel CPU tests with py_import (RBE)\n-permissions:\n-  contents: read\n-\n+permissions: {}\n on:\n   workflow_call:\n     inputs:\n@@ -54,6 +52,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       # Halt for testing\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\ndiff --git a/.github/workflows/bazel_cpu_rbe.yml b/.github/workflows/bazel_cpu_rbe.yml\nindex a8b40c260260..2f8eb2c33cee 100644\n--- a/.github/workflows/bazel_cpu_rbe.yml\n+++ b/.github/workflows/bazel_cpu_rbe.yml\n@@ -18,7 +18,7 @@ on:\n     branches:\n       - main\n       - 'release/**'\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n@@ -53,6 +53,8 @@ jobs:\n # End Presubmit Naming Check github-cpu-presubmits\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 5a3ceaa8a4e8..348d19763989 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -44,7 +44,6 @@ on:\n         type: string\n         required: false\n         default: 'no'\n-\n jobs:\n   run-tests:\n     defaults:\n@@ -67,6 +66,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/bazel_cuda_rbe.yml b/.github/workflows/bazel_cuda_rbe.yml\nindex 83f651c0ef95..cd4e9a021cfc 100644\n--- a/.github/workflows/bazel_cuda_rbe.yml\n+++ b/.github/workflows/bazel_cuda_rbe.yml\n@@ -23,7 +23,7 @@ concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n   cancel-in-progress: ${{ !contains(github.ref, 'release/') && github.ref != 'main' }}\n-\n+permissions: {}\n jobs:\n   run_tests:\n     if: github.event.repository.fork == false\n@@ -49,6 +49,8 @@ jobs:\n # End Presubmit Naming Check github-cuda-presubmits\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\ndiff --git a/.github/workflows/bazel_optional_h100_b200.yml b/.github/workflows/bazel_optional_h100_b200.yml\nindex ec907280938e..68fea50857a5 100644\n--- a/.github/workflows/bazel_optional_h100_b200.yml\n+++ b/.github/workflows/bazel_optional_h100_b200.yml\n@@ -32,6 +32,8 @@ jobs:\n # End Presubmit Naming Check github-cuda-presubmits\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n@@ -74,6 +76,8 @@ jobs:\n     name: \"Bazel multiple H100 CUDA tests\"\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 7bca28c3190d..90c888471b73 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -90,10 +90,7 @@ on:\n       gcs_upload_uri:\n         description: \"GCS location prefix to where the artifacts were uploaded\"\n         value: ${{ jobs.build-artifacts.outputs.gcs_upload_uri }}\n-\n-permissions:\n-  contents: read\n-\n+permissions: {}\n jobs:\n   build-artifacts:\n     defaults:\n@@ -122,6 +119,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Enable RBE if building on Linux x86 or Windows x86\n         if: contains(inputs.runner, 'linux-x86') || contains(inputs.runner, 'windows-x86')\n         run: echo \"JAXCI_BUILD_ARTIFACT_WITH_RBE=1\" >> $GITHUB_ENV\ndiff --git a/.github/workflows/ci-build.yaml b/.github/workflows/ci-build.yaml\nindex 0769c698d5fe..ada470526ef8 100644\n--- a/.github/workflows/ci-build.yaml\n+++ b/.github/workflows/ci-build.yaml\n@@ -16,10 +16,7 @@ on:\n     branches:\n       - main\n \n-permissions:\n-  contents: read  # to fetch code\n-  actions: write  # to cancel previous workflows\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n@@ -30,6 +27,8 @@ jobs:\n     timeout-minutes: 5\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set up Python 3.11\n         uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n         with:\n@@ -65,6 +64,8 @@ jobs:\n             num_generated_cases: 1\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Image Setup\n       run: |\n         apt update\n@@ -106,6 +107,8 @@ jobs:\n         python-version: ['3.10']\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Set up Python ${{ matrix.python-version }}\n       uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n       with:\n@@ -136,6 +139,8 @@ jobs:\n         python-version: ['3.10']\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Image Setup\n       run: |\n         apt update\n@@ -166,6 +171,8 @@ jobs:\n             num_generated_cases: 10\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Set up Python ${{ matrix.python-version }}\n       uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n       with:\n@@ -198,6 +205,8 @@ jobs:\n     timeout-minutes: 30\n     steps:\n     - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Set up Python\n       uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n       with:\ndiff --git a/.github/workflows/cloud-tpu-ci-nightly.yml b/.github/workflows/cloud-tpu-ci-nightly.yml\nindex c7394a498dd6..3ed560b04a88 100644\n--- a/.github/workflows/cloud-tpu-ci-nightly.yml\n+++ b/.github/workflows/cloud-tpu-ci-nightly.yml\n@@ -57,6 +57,8 @@ jobs:\n       # mandates using a specific commit for non-Google actions. We use\n       # https://github.com/sethvargo/ratchet to pin specific versions.\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       # Checkout XLA at head, if we're building jaxlib at head.\n       - name: Checkout XLA at head\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n@@ -64,6 +66,7 @@ jobs:\n         with:\n           repository: openxla/xla\n           path: xla\n+          persist-credentials: false\n       # We need to mark the GitHub workspace as safe as otherwise git commands will fail.\n       - name: Mark GitHub workspace as safe\n         run: |\ndiff --git a/.github/workflows/cloud-tpu-ci-presubmit.yml b/.github/workflows/cloud-tpu-ci-presubmit.yml\nindex 090259c0f849..fe1f2820b338 100644\n--- a/.github/workflows/cloud-tpu-ci-presubmit.yml\n+++ b/.github/workflows/cloud-tpu-ci-presubmit.yml\n@@ -25,9 +25,7 @@ on:\n \n # This should also be set to read-only in the project settings, but it's nice to\n # document and enforce the permissions here.\n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\ndiff --git a/.github/workflows/jax-array-api.yml b/.github/workflows/jax-array-api.yml\nindex 6419cb730b71..16a72fe34714 100644\n--- a/.github/workflows/jax-array-api.yml\n+++ b/.github/workflows/jax-array-api.yml\n@@ -11,22 +11,21 @@ on:\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n+permissions: {}\n jobs:\n   build:\n-\n     runs-on: linux-x86-n2-16\n     container: us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\n     strategy:\n       matrix:\n         python-version: [3.11]\n-\n     env:\n       PYTHON: \"python${{ matrix.python-version }}\"\n-\n     steps:\n     - name: Checkout jax\n       uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+      with:\n+        persist-credentials: false\n     - name: Checkout array-api-tests\n       uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       with:\n@@ -35,6 +34,7 @@ jobs:\n         ref: 'c847143beb8d769bde5dbcc063fe19ed7acc2f9b'  # Latest commit as of 2025-05-12\n         submodules: 'true'\n         path: 'array-api-tests'\n+        persist-credentials: false\n     - name: Install dependencies\n       run: |\n         $PYTHON -m uv pip install --system .[ci] pytest-xdist -r array-api-tests/requirements.txt\ndiff --git a/.github/workflows/k8s.yaml b/.github/workflows/k8s.yaml\nindex 5756b1afbbd2..81552f9bb43b 100644\n--- a/.github/workflows/k8s.yaml\n+++ b/.github/workflows/k8s.yaml\n@@ -1,5 +1,4 @@\n name: Multi-process run using K8s\n-\n on:\n   push:\n     branches:\n@@ -16,19 +15,14 @@ on:\n       - 'jax/_src/distributed.py'\n       - 'jax/_src/clusters/**'\n \n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n defaults:\n   run:\n     shell: bash -ex -o pipefail {0}\n-\n jobs:\n-\n   distributed-initialize:\n     runs-on: ubuntu-22.04\n     strategy:\n@@ -40,6 +34,7 @@ jobs:\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # ratchet:actions/checkout@v4\n         with:\n           path: jax\n+          persist-credentials: false\n           \n       - name: Start Minikube cluster\n         uses: medyagh/setup-minikube@cea33675329b799adccc9526aa5daccc26cd5052 # ratchet:medyagh/setup-minikube@v0.0.19\n@@ -105,7 +100,7 @@ jobs:\n           done\n \n       - name: Examine individual pod outputs\n-        if: \"!cancelled()\"\n+        if: ${{ !cancelled() }}\n         run: |\n           set +x\n           kubectl get pods --no-headers | awk '{print $1}' | while read -s pod; do\ndiff --git a/.github/workflows/metal_plugin_ci.yml b/.github/workflows/metal_plugin_ci.yml\nindex 2135e473d6be..c76153d48f10 100644\n--- a/.github/workflows/metal_plugin_ci.yml\n+++ b/.github/workflows/metal_plugin_ci.yml\n@@ -14,7 +14,7 @@ on:\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n+permissions: {}\n jobs:\n   jax-metal-plugin-test:\n \n@@ -30,6 +30,7 @@ jobs:\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n         with:\n           path: jax\n+          persist-credentials: false\n       - name: Setup build and test enviroment\n         run: |\n           rm -rf ${GITHUB_WORKSPACE}/jax-metal-venv\ndiff --git a/.github/workflows/numpy_nightly.yml b/.github/workflows/numpy_nightly.yml\nindex 17357e9f1dd8..d9a858216857 100644\n--- a/.github/workflows/numpy_nightly.yml\n+++ b/.github/workflows/numpy_nightly.yml\n@@ -18,9 +18,7 @@ on:\n   schedule:\n     - cron: \"0 */3 * * *\" # Run once every 3 hours\n \n-permissions:\n-      contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n@@ -46,12 +44,15 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Checkout ml_dtypes\n         uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683\n         with:\n           repository: jax-ml/ml_dtypes\n           ref: main\n           path: ml_dtypes\n+          persist-credentials: false\n       # Halt for testing\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\ndiff --git a/.github/workflows/oldest_supported_numpy.yml b/.github/workflows/oldest_supported_numpy.yml\nindex a63cb0b1c614..c7f1f1e38a26 100644\n--- a/.github/workflows/oldest_supported_numpy.yml\n+++ b/.github/workflows/oldest_supported_numpy.yml\n@@ -10,12 +10,7 @@ on:\n   push:\n     branches:\n       - main\n-\n-# This should also be set to read-only in the project settings, but it's nice to\n-# document and enforce the permissions here.\n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   # Don't cancel in-progress jobs for main/release branches.\n@@ -23,7 +18,7 @@ concurrency:\n \n jobs:\n   test-oldest-supported-numpy:\n-    if: \"github.event.repository.fork == false && !startsWith(github.head_ref, 'release/')\"\n+    if: ${{ github.event.repository.fork == false && !startsWith(github.head_ref, 'release/') }}\n     defaults:\n       run:\n         shell: bash\n@@ -40,6 +35,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Install Python dependencies\n         run: |\n           $JAXCI_PYTHON -m uv pip install -r build/test-requirements.txt\n@@ -52,8 +49,6 @@ jobs:\n       # Halt for testing\n       - name: Wait For Connection\n         uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n-        with:\n-          halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Pytest CPU tests\n         timeout-minutes: 30\n         run: ./ci/run_pytest_cpu.sh\n\\ No newline at end of file\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex 263bfd7ec9a9..d08cb520eab4 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -67,6 +67,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 5f8888526aad..d095dbfb9e80 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -79,6 +79,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 8c2457208e12..ce5dcf0c9fc8 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -11,7 +11,6 @@\n #      - Installs the downloaded jaxlib wheel.\n #      - Runs the TPU tests with Pytest.\n name: CI - Pytest TPU\n-\n on:\n   workflow_call:\n     inputs:\n@@ -90,6 +89,8 @@ jobs:\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set env vars for use in artifact download URL\n         run: |\n           os=$(uname -s | awk '{print tolower($0)}')\ndiff --git a/.github/workflows/release-notification.yml b/.github/workflows/release-notification.yml\nindex a4a342ef6de7..6d68bf922655 100644\n--- a/.github/workflows/release-notification.yml\n+++ b/.github/workflows/release-notification.yml\n@@ -2,14 +2,21 @@ name: Google Chat Release Notification\n on:\n   release:\n     types: [published]\n+permissions: {}\n jobs:\n   build:\n+    env:\n+      WEBHOOK_URL: ${{ secrets.RELEASES_WEBHOOK }}\n+      RELEASE_NAME: ${{github.event.release.name}}\n+      PUBLISHED_AT: ${{github.event.release.published_at}}\n+      AUTHOR_LOGIN: ${{github.event.release.author.login}}\n+      RELEASE_URL: ${{github.event.release.url}}\n     runs-on: ubuntu-latest\n     steps:\n       - name: Google Chat Notification\n         run: |\n-          curl --location --request POST '${{ secrets.RELEASES_WEBHOOK }}' \\\n+          curl --location --request POST '${WEBHOOK_URL}' \\\n           --header 'Content-Type: application/json' \\\n           --data-raw '{\n-              \"text\": \"Release ${{github.event.release.name}} at ${{github.event.release.published_at}} by ${{github.event.release.author.login}}. <${{github.event.release.url}}|[github]>\"\n+              \"text\": \"Release $RELEASE_NAME at $PUBLISHED_AT by $AUTHOR_LOGIN. <$RELEASE_URL|[github]>\"\n           }'\ndiff --git a/.github/workflows/rocm-ci.yml b/.github/workflows/rocm-ci.yml\nindex 0ce20726ce63..4bfb8cb50a5e 100644\n--- a/.github/workflows/rocm-ci.yml\n+++ b/.github/workflows/rocm-ci.yml\n@@ -6,9 +6,7 @@ on:\n     branches:\n       - main\n \n-permissions:\n-  contents: read\n-\n+permissions: {}\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n \n@@ -36,6 +34,7 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n         with:\n           path: ${{ env.WORKSPACE_DIR }}\n+          persist-credentials: false\n       - name: Build JAX\n         run: |\n           pushd $WORKSPACE_DIR\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex ce4130c31a30..67ff8dd93e3d 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -3,7 +3,6 @@ name: CI - Free-threading and Thread Sanitizer (nightly)\n concurrency:\n   group: ${{ github.workflow }}-${{ github.ref }}\n   cancel-in-progress: true\n-\n on:\n   schedule:\n     - cron: \"0 5 * * *\" # Daily at 05:00 UTC == 00:00 EST == 21:00 PST\n@@ -14,7 +13,7 @@ on:\n     paths:\n       - '**/workflows/tsan.yaml'\n       - '**/workflows/tsan-suppressions*.txt'\n-\n+permissions: {}\n jobs:\n   tsan:\n     runs-on: linux-x86-n2-64\n@@ -50,17 +49,20 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           path: jax\n+          persist-credentials: false\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         with:\n           repository: numpy/numpy\n           path: numpy\n           submodules: true\n+          persist-credentials: false\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\n         if: ${{ matrix.python-version == '3.14' }}\n         with:\n           repository: scipy/scipy\n           path: scipy\n           submodules: true\n+          persist-credentials: false\n \n       - name: Get year & week number\n         id: get-date\n@@ -81,6 +83,8 @@ jobs:\n           repository: python/cpython\n           path: cpython\n           ref: ${{ matrix.github_branch }}\n+          persist-credentials: false\n+\n \n       - name: Build TSAN CPython ${{ matrix.python-version }}\n         if: steps.cache-cpython-tsan-restore.outputs.cache-hit != 'true'\ndiff --git a/.github/workflows/upstream-nightly.yml b/.github/workflows/upstream-nightly.yml\nindex 349ddf0d96a3..23b8ac32d844 100644\n--- a/.github/workflows/upstream-nightly.yml\n+++ b/.github/workflows/upstream-nightly.yml\n@@ -19,7 +19,7 @@ on:\n       - main\n     paths:\n       - '**workflows/upstream-nightly.yml'\n-\n+permissions: {}\n jobs:\n   upstream-dev:\n     runs-on: linux-x86-n2-64\n@@ -33,6 +33,8 @@ jobs:\n         python-version: [\"3.13\"]\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n+        with:\n+          persist-credentials: false\n       - name: Set up Python ${{ matrix.python-version }}\n         uses: actions/setup-python@a26af69be951a213d495a4c3e4e4022e16d87065  # v5.6.0\n         with:\ndiff --git a/.github/workflows/wheel_tests_nightly_release.yml b/.github/workflows/wheel_tests_nightly_release.yml\nindex 7bad41647e6b..c536466c7dcb 100644\n--- a/.github/workflows/wheel_tests_nightly_release.yml\n+++ b/.github/workflows/wheel_tests_nightly_release.yml\n@@ -33,7 +33,7 @@ on:\n concurrency:\n   group: ${{ github.workflow }}-${{ github.head_ref || github.ref }}\n   cancel-in-progress: true\n-\n+permissions: {}\n jobs:\n   run-pytest-cpu:\n     uses: ./.github/workflows/pytest_cpu.yml\n\n```"
        ],
        "from_id": [
            "MichaelHudgins"
        ]
    },
    {
        "text_input": "Make CI job names to be shorter\n\nThis strips away the redundant terms in job names to keep them shorter and easy to read. Actions displays job names that reuse workflows in the following format: `caller workflow name / called workflow name`. The changes here are done in the called workflow names as changing the caller workflow names seem to make the summary page hard to parse (see https://github.com/jax-ml/jax/actions/runs/15217612585).\n\nHere's how the continuous workflow's summary page looks like with this change: https://github.com/jax-ml/jax/actions/runs/15286609214/job/42998511666\n\nPiperOrigin-RevId: 764390866",
        "output": [
            "```diff\nCommit: 1994074023349c6f6c144a474f5b9c0fea8f9e16\nDate: 2025-05-28T20:25:14Z\nURL: https://github.com/jax-ml/jax/commit/1994074023349c6f6c144a474f5b9c0fea8f9e16\nFiles changed: 7\nAdditions: +18, Deletions: -7\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex c98bcee980e8..7eb6d2ed27b4 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -49,7 +49,8 @@ jobs:\n       JAXCI_HERMETIC_PYTHON_VERSION: ${{ inputs.python }}\n       JAXCI_ENABLE_X64: ${{ inputs.enable-x64 }}\n \n-    name: \"Bazel CPU tests with py_import (${{ inputs.runner }}, Python ${{ inputs.python }}, x64=${{ inputs.enable-x64 }})\"\n+    name: \"${{ (contains(inputs.runner, 'linux-x86') && 'linux x86') ||\n+        (contains(inputs.runner, 'linux-arm64') && 'linux arm64') }}, py ${{ inputs.python }}, x64=${{ inputs.enable-x64 }}\"\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 3e68034dfbf4..5a3ceaa8a4e8 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -60,7 +60,10 @@ jobs:\n       # Enable writing to the Bazel remote cache bucket.\n       JAXCI_WRITE_TO_BAZEL_REMOTE_CACHE: \"1\"\n \n-    name: \"Bazel single accelerator and multi-accelerator CUDA tests (jaxlib version=${{ inputs.jaxlib-version }}, ${{ inputs.runner }}, Python ${{ inputs.python }}, x64=${{ inputs.enable-x64 }})\"\n+    name: \"jaxlib=${{ inputs.jaxlib-version }},\n+          ${{ (contains(inputs.runner, 'h100') && 'h100') ||\n+          (contains(inputs.runner, 'b200') && 'b200') ||\n+          (contains(inputs.runner, 'l4') && 'l4') }}, py ${{ inputs.python }}, x64=${{ inputs.enable-x64 }}\"\n \n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex d5fc35a99cd5..7bca28c3190d 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -111,7 +111,10 @@ jobs:\n       JAXCI_HERMETIC_PYTHON_VERSION: \"${{ inputs.python }}\"\n       JAXCI_CLONE_MAIN_XLA: \"${{ inputs.clone_main_xla }}\"\n \n-    name: Build ${{ inputs.artifact }} (${{ inputs.runner }}, Python ${{ inputs.python }}, clone main XLA=${{ inputs.clone_main_xla }})\n+    name: \"${{ inputs.artifact }},\n+            ${{ (contains(inputs.runner, 'linux-x86') && 'linux x86') ||\n+            (contains(inputs.runner, 'linux-arm64') && 'linux arm64') ||\n+            (contains(inputs.runner, 'windows-x86') && 'windows x86') }}, py ${{ inputs.python }}, clone main XLA=${{ inputs.clone_main_xla }}\"\n \n     # Map the job outputs to step outputs\n     outputs:\ndiff --git a/.github/workflows/cloud-tpu-ci-presubmit.yml b/.github/workflows/cloud-tpu-ci-presubmit.yml\nindex 40c99735c2de..090259c0f849 100644\n--- a/.github/workflows/cloud-tpu-ci-presubmit.yml\n+++ b/.github/workflows/cloud-tpu-ci-presubmit.yml\n@@ -54,7 +54,7 @@ jobs:\n     needs: [build-jax-artifacts]\n     uses: ./.github/workflows/pytest_tpu.yml\n     # Begin Presubmit Naming Check - name modification requires internal check to be updated\n-    name: \"TPU test (jaxlib=head, v5e-8)\"\n+    name: \"TPU test (jaxlib=head)\"\n     with:\n       runner: \"linux-x86-ct5lp-224-8tpu\"\n       cores: \"8\"\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex 3af06fe8037e..263bfd7ec9a9 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -56,7 +56,9 @@ jobs:\n                    (contains(inputs.runner, 'linux-arm64') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-arm64:latest') ||\n                    (contains(inputs.runner, 'windows-x86') && null) }}\n \n-    name: \"Pytest CPU (${{ inputs.runner }}, Python ${{ inputs.python }}, x64=${{ inputs.enable-x64 }})\"\n+    name: \"${{ (contains(inputs.runner, 'linux-x86') && 'linux x86') ||\n+        (contains(inputs.runner, 'linux-arm64') && 'linux arm64') ||\n+        (contains(inputs.runner, 'windows-x86') && 'windows x86') }}, py ${{ inputs.python }}, x64=${{ inputs.enable-x64 }}\"\n \n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: \"${{ inputs.python }}\"\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 78f32cda672d..5f8888526aad 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -68,7 +68,9 @@ jobs:\n     container:  ${{ !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.1') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.1-cudnn9.8:latest') ||\n                 !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.8') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest') ||\n                 inputs.use-nvidia-pip-wheels && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest'}}\n-    name: \"Pytest CUDA (${{ inputs.runner }}, CUDA ${{ inputs.cuda }}, Python ${{ inputs.python }}, x64=${{ inputs.enable-x64 }})\"\n+    name: \"${{ (contains(inputs.runner, 'h100') && 'h100') ||\n+        (contains(inputs.runner, 'b200') && 'b200') ||\n+        (contains(inputs.runner, 'l4') && 'l4') }}, CUDA ${{ inputs.cuda-version }}, py ${{ inputs.python }}, x64=${{ inputs.enable-x64 }}\"\n \n     env:\n       JAXCI_HERMETIC_PYTHON_VERSION: \"${{ inputs.python }}\"\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 22cd64977dc5..8c2457208e12 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -78,7 +78,7 @@ jobs:\n     runs-on: ${{ inputs.runner }}\n     container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\n     # Begin Presubmit Naming Check - name modification requires internal check to be updated\n-    name: \"Pytest TPU (${{ inputs.tpu-type }}, Python ${{ inputs.python }}, libtpu=${{ inputs.libtpu-version-type }})\"\n+    name: \"${{ inputs.tpu-type }}, py ${{ inputs.python }}, libtpu=${{ inputs.libtpu-version-type }}\"\n     # End Presubmit Naming Check github-tpu-presubmits\n \n     env:\n\n```"
        ],
        "from_id": [
            "nitins17",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29069 from jenriver:skip_mds_for_cpu\n\nPiperOrigin-RevId: 764376257",
        "output": [
            "```diff\nCommit: da4ea8d97394ea9a1eb6043de4eef51a233171d5\nDate: 2025-05-28T19:48:42Z\nURL: https://github.com/jax-ml/jax/commit/da4ea8d97394ea9a1eb6043de4eef51a233171d5\nFiles changed: 1\nAdditions: +6, Deletions: -0\ndiff --git a/jax/_src/cloud_tpu_init.py b/jax/_src/cloud_tpu_init.py\nindex 0539e4253063..f42794db7696 100644\n--- a/jax/_src/cloud_tpu_init.py\n+++ b/jax/_src/cloud_tpu_init.py\n@@ -15,11 +15,14 @@\n import datetime\n import os\n import re\n+import logging\n import warnings\n from jax import version\n from jax._src import config\n from jax._src import hardware_utils\n \n+logger = logging.getLogger(__name__)\n+\n running_in_cloud_tpu_vm: bool = False\n \n \n@@ -74,6 +77,9 @@ def cloud_tpu_init() -> None:\n   # Exit early if we're not running on a Cloud TPU VM or libtpu isn't installed.\n   libtpu_path = get_tpu_library_path()\n   num_tpu_chips, tpu_id = hardware_utils.num_available_tpu_chips_and_device_id()\n+  if num_tpu_chips == 0:\n+    logger.info('Using LibTPU with a device other than TPU. Skipping TPU metadata query.')\n+    os.environ['TPU_SKIP_MDS_QUERY'] = '1'\n   if (\n       tpu_id is not None\n       and tpu_id >= hardware_utils.TpuVersion.v5e\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Skip TPU metadata server query when not using TPU.\n\nResolve an issue where `jax.devices()` hangs due to unwanted TPU\nmetadata query when using LibTPU with a device other than TPU (ex:\nCPU's).\nThis feature can be useful in cross [AOT](https://docs.jax.dev/en/latest/aot.html).",
        "output": [
            "```diff\nCommit: 68fcf154bdcce970ef8fae50b1876404726855bf\nDate: 2025-05-28T18:24:05Z\nURL: https://github.com/jax-ml/jax/commit/68fcf154bdcce970ef8fae50b1876404726855bf\nFiles changed: 1\nAdditions: +6, Deletions: -0\ndiff --git a/jax/_src/cloud_tpu_init.py b/jax/_src/cloud_tpu_init.py\nindex 0539e4253063..f42794db7696 100644\n--- a/jax/_src/cloud_tpu_init.py\n+++ b/jax/_src/cloud_tpu_init.py\n@@ -15,11 +15,14 @@\n import datetime\n import os\n import re\n+import logging\n import warnings\n from jax import version\n from jax._src import config\n from jax._src import hardware_utils\n \n+logger = logging.getLogger(__name__)\n+\n running_in_cloud_tpu_vm: bool = False\n \n \n@@ -74,6 +77,9 @@ def cloud_tpu_init() -> None:\n   # Exit early if we're not running on a Cloud TPU VM or libtpu isn't installed.\n   libtpu_path = get_tpu_library_path()\n   num_tpu_chips, tpu_id = hardware_utils.num_available_tpu_chips_and_device_id()\n+  if num_tpu_chips == 0:\n+    logger.info('Using LibTPU with a device other than TPU. Skipping TPU metadata query.')\n+    os.environ['TPU_SKIP_MDS_QUERY'] = '1'\n   if (\n       tpu_id is not None\n       and tpu_id >= hardware_utils.TpuVersion.v5e\n\n```"
        ],
        "from_id": [
            "jenriver"
        ]
    },
    {
        "text_input": "Merge pull request #28665 from mattjj:smap-systematic\n\nPiperOrigin-RevId: 764330041",
        "output": [
            "```diff\nCommit: bfc20eb30bb117760615e678631de10224e98936\nDate: 2025-05-28T17:50:59Z\nURL: https://github.com/jax-ml/jax/commit/bfc20eb30bb117760615e678631de10224e98936\nFiles changed: 1\nAdditions: +88, Deletions: -0\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex df69db7c9462..f473a4dc0547 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -3853,6 +3853,94 @@ def f(x):\n     self.assertAllClose(f(jnp.arange(8.)), jnp.array([1.,  5.,  9., 13.]))\n \n \n+def smap_ref(f, in_axes, out_axes, axis_name, axis_size):\n+  del axis_name  # no collectives\n+  def smapped(*args):\n+    split_args = zip(*[split_arg(x, d, axis_size) for x, d in zip(args, in_axes)])\n+    split_result = [f(*xs) for xs in split_args]\n+    return concat_result(split_result, out_axes)\n+  return smapped\n+\n+def split_arg(x, d, axis_size):\n+  if d is None:\n+    x = np.tile(x, [axis_size] + [1] * (x.ndim - 1))\n+  return np.split(x, axis_size, d or 0)\n+\n+def concat_result(results, out_axes):\n+  if not isinstance(results[0], (list, tuple)):\n+    return results[0] if out_axes is None else np.concatenate(results, out_axes)\n+  return [res[0] if d is None else np.concatenate(res, d)\n+          for res, d in zip(zip(*results), out_axes)]\n+\n+def sample_smap() -> Chooser:\n+  spec = yield fun_specs\n+  mesh_shape = yield mesh_shapes\n+  axis_names = ('i', 'j', 'k', 'l')[:len(mesh_shape)]\n+  mesh = SimpleNamespace(shape=dict(zip(axis_names, mesh_shape)),\n+                         axis_names=axis_names)\n+  axis_name = yield axis_names\n+  body_in_types = yield (tys for tys in it.product(input_shapes, repeat=spec.num_inputs)\n+                         if not spec.valid_types or spec.valid_types(*tys))\n+  in_axes = yield from sample_in_axes(body_in_types)\n+  out_rep = spec.out_rep(*[ax is None for ax in in_axes])\n+  body_out_type = jax.eval_shape(spec.fun, *body_in_types)\n+  out_axes = yield from sample_out_axes(out_rep, body_out_type)\n+  in_str = '(' + ','.join(jax.core.ShapedArray(t.shape, t.dtype).str_short()\n+                          for t in body_in_types) + ')'\n+  name = f'{spec.name}_{mesh.shape}_{in_axes}_{out_axes}_{axis_name}_{in_str}'\n+  in_types = [ty.update(shape=dilate_axis(ty.shape, d, mesh.shape[axis_name]))\n+              for ty, d in zip(body_in_types, in_axes)]\n+  args = [np.arange(ty.size, dtype=ty.dtype).reshape(ty.shape) / ty.size\n+          for ty in in_types]\n+  return name, spec, mesh.shape, in_axes, out_axes, axis_name, args\n+\n+def sample_in_axes(body_in_types) -> Chooser:\n+  in_axes = []\n+  for ty in body_in_types:\n+    in_axes.append((yield [None, *range(ty.ndim)]))\n+  return tuple(in_axes)\n+\n+def sample_out_axes(out_rep, body_out_type) -> Chooser:\n+  if not isinstance(body_out_type, (list, tuple)):\n+    out_axes = yield [None] * out_rep + list(range(body_out_type.ndim))\n+  else:\n+    out_axes_ = []\n+    for ty, r in zip(body_out_type, out_rep):\n+      out_axes_.append((yield [None] * r + list(range(ty.ndim))))\n+    out_axes = tuple(out_axes_)\n+  return out_axes\n+\n+def dilate_axis(shape: tuple[int, ...], i: int | None, size: int) -> tuple[int, ...]:\n+  if i is None:\n+    return shape\n+  shp = list(shape)\n+  shp[i] *= size\n+  return tuple(shp)\n+\n+class SmapSystematicTest(jtu.JaxTestCase):\n+\n+  @staticmethod\n+  def make_mesh(mesh_shape):\n+    return jtu.create_mesh(tuple(mesh_shape.values()), tuple(mesh_shape))\n+\n+  @parameterized.parameters(\n+      sample(jtu.NUM_GENERATED_CASES.value, sample_smap))\n+  def test_against_ref(self, fun_spec, mesh_shape, in_axes, out_axes, axis_name, args):\n+    fun = fun_spec.fun\n+    mesh = self.make_mesh(mesh_shape)\n+    args = map(jnp.array, args)\n+\n+    with jax.sharding.use_mesh(mesh):\n+      fun_ = smap(fun, in_axes=in_axes, out_axes=out_axes, axis_name=axis_name)\n+      out = jax.jit(fun_)(*args)\n+\n+    fun_ref = smap_ref(fun, in_axes=in_axes, out_axes=out_axes, axis_name=axis_name,\n+                       axis_size=mesh_shape[axis_name])\n+    expected = fun_ref(*args)\n+\n+    self.assertAllClose(out, expected, check_dtypes=False)\n+\n+\n @jtu.with_config(jax_use_shardy_partitioner=True)\n # TODO(phawkins): enable this test unconditionally once shardy is the default.\n @unittest.skipIf(sdy is None, \"shardy is not enabled\")\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29030 from dfm:custom-transpose-nones\n\nPiperOrigin-RevId: 764328992",
        "output": [
            "```diff\nCommit: f32ce04fd02ccfb5fbb79a0c7c84843acd7ff050\nDate: 2025-05-28T17:48:45Z\nURL: https://github.com/jax-ml/jax/commit/f32ce04fd02ccfb5fbb79a0c7c84843acd7ff050\nFiles changed: 2\nAdditions: +25, Deletions: -5\ndiff --git a/jax/_src/custom_transpose.py b/jax/_src/custom_transpose.py\nindex 21e607b5bff2..fb125e174122 100644\n--- a/jax/_src/custom_transpose.py\n+++ b/jax/_src/custom_transpose.py\n@@ -217,7 +217,6 @@ def custom_transpose_transpose_rule(\n   # Consider passing this information to the custom transpose rule?\n \n   res_arg, lin_arg = tree_unflatten(call_in_tree, args)\n-  del lin_arg\n   assert all(not ad.is_undefined_primal(x) for x in tree_leaves(res_arg))\n \n   cts = [ad_util.zeros_like_aval(ct.aval) if type(ct) is ad_util.Zero else ct\n@@ -225,10 +224,17 @@ def custom_transpose_transpose_rule(\n   ct_out = tree_unflatten(out_tree, cts)\n   ct_lin = transpose.call_wrapped(res_arg, ct_out)\n   check_transpose_rule_trees(transpose, lin_tree, tree_structure(ct_lin))\n-  ct_lin_flat, _ = tree_flatten(\n-      tree_broadcast(lin_tree, ct_lin, is_leaf=lambda x: x is None),\n-      is_leaf=lambda x: x is None)\n-  return [None] * len(tree_leaves(res_arg)) + ct_lin_flat\n+  ct_lin = tree_broadcast(lin_tree, ct_lin, is_leaf=lambda x: x is None)\n+\n+  # When the transpose returns None, we treat that as a Zero, except when the\n+  # input is also None. In that case, the cotangent corresponding to that input\n+  # should be dropped.\n+  zero = object()\n+  ct_lin = tree_map(lambda l, ct: zero if ct is None and l is not None else ct,\n+                    lin_arg, ct_lin, is_leaf=ad.is_undefined_primal)\n+\n+  ct_lin_flat, _ = tree_flatten(ct_lin)\n+  return [None] * res_tree.num_leaves + [None if ct is zero else ct for ct in ct_lin_flat]\n \n \n def custom_transpose_lowering(*args, call_jaxpr, **params):\ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 9d10b40c6030..bfe391797920 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3722,6 +3722,20 @@ def gt(x, t):\n     with config.use_direct_linearize(True):\n       self.assertAllClose(jax.grad(f)(0.5), jnp.cos(0.5))\n \n+  def test_input_none(self):\n+    # ref: https://github.com/jax-ml/jax/issues/29009\n+    @jax.custom_jvp\n+    def f(x, y): return y\n+    @f.defjvp\n+    def f_jvp(p, t): return f(*p), g(p, t)\n+\n+    @custom_transpose(jnp.float32(0))\n+    def g(r, x): return x[1]\n+    @g.def_transpose\n+    def gt(r, t): return None, jnp.zeros_like(r[1])\n+\n+    jax.grad(f, argnums=(1,))(None, jnp.float32(2))  # doesn't crash\n+\n \n class CustomDceTest(jtu.JaxTestCase):\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[better-errors] if a non-jaxtype is returned, say it's a return problem",
        "output": [
            "```diff\nCommit: ba64c02fb3baa283c5475a2b74b7ceed584c25c7\nDate: 2025-05-28T17:45:32Z\nURL: https://github.com/jax-ml/jax/commit/ba64c02fb3baa283c5475a2b74b7ceed584c25c7\nFiles changed: 2\nAdditions: +38, Deletions: -1\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 6ea16ec8e8ba..3c499429a663 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -2243,7 +2243,7 @@ def trace_to_jaxpr_dynamic(\n     try:\n       with core.set_current_trace(trace):\n         ans = fun.call_wrapped(*in_tracers)\n-\n+      _check_returned_jaxtypes(fun.debug_info, ans)\n       out_tracers = map(partial(trace.to_jaxpr_tracer, source_info=source_info), ans)\n       _check_no_returned_refs(fun.debug_info, out_tracers)\n       jaxpr, consts, attrs_tracked = trace.to_jaxpr(out_tracers, fun.debug_info)\n@@ -2255,6 +2255,20 @@ def trace_to_jaxpr_dynamic(\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   return jaxpr, [v.aval for v in jaxpr.outvars], consts, attrs_tracked\n \n+def _check_returned_jaxtypes(dbg, out_tracers):\n+  for i, x in enumerate(out_tracers):\n+    try:\n+      core.typeof(x)\n+    except TypeError:\n+      if (dbg and len(paths := dbg.result_paths()) > i and\n+          (p := paths[i].removeprefix('result'))):\n+        extra = f' at output component {p}'\n+      else:\n+        extra = ''\n+      raise TypeError(\n+      f\"function {dbg.func_src_info} traced for {dbg.traced_for} returned a \"\n+      f\"value of type {type(x)}{extra}, which is not a valid JAX type\") from None\n+\n def _check_no_returned_refs(\n     dbg: core.DebugInfo,\n     out_tracers: Sequence[DynamicJaxprTracer]\ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex f5b74e1e10d6..584eb0eda496 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -5058,6 +5058,29 @@ def test_ensure_compile_time_eval_no_leaks(self):\n     with jax.ensure_compile_time_eval():\n       jnp.linalg.solve(jnp.eye(3), jnp.ones(3))  # doesn't crash\n \n+  def test_returned_non_jaxtype(self):\n+\n+    class TestEnum(enum.Enum):\n+      A = enum.auto()\n+\n+    @jax.tree_util.register_dataclass\n+    @dataclasses.dataclass\n+    class TestClass3:\n+      test_enum_field: TestEnum = dataclasses.field(metadata=dict(static=True))\n+      test_data_field: int\n+\n+    def test_jax_function(test_class: TestClass3) -> TestEnum:\n+      return test_class.test_enum_field\n+\n+    jitted_test_function = jax.jit(test_jax_function)\n+    with self.assertRaisesRegex(TypeError, \"returned a value of type\"):\n+        jitted_test_function(\n+            TestClass3(\n+                test_data_field=1,\n+                test_enum_field=TestEnum.A,\n+            )\n+        )\n+\n \n class RematTest(jtu.JaxTestCase):\n \n\n```"
        ],
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Added the missing resource estimation rule for `pl.run_state`\n\nPiperOrigin-RevId: 764276682",
        "output": [
            "```diff\nCommit: de491b96c5df15464fb6410cf264c4d830bfe0f9\nDate: 2025-05-28T15:41:34Z\nURL: https://github.com/jax-ml/jax/commit/de491b96c5df15464fb6410cf264c4d830bfe0f9\nFiles changed: 1\nAdditions: +52, Deletions: -10\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 82a0a47c4a0d..a56733a89f60 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -176,11 +176,18 @@ def _estimate_resources(\n   rs = Resources(smem_scratch_bytes=0)\n   for eqn in jaxpr.eqns:\n     # TODO(slebedev): Add support for other primitives, notably control flow.\n-    rule = _resource_estimators.get(eqn.primitive)\n-    if rule is None:\n-      # Assume that unsupported primitives are neutral wrt resource usage.\n+    if rule := _resource_estimators.get(eqn.primitive):\n+      rs |= rule(ctx, *(invar.aval for invar in eqn.invars), **eqn.params)\n       continue\n-    rs |= rule(ctx, *(invar.aval for invar in eqn.invars), **eqn.params)\n+    # Assume that unsupported primitives are neutral wrt resource usage,\n+    # unless they have a jaxpr in their params.\n+    if any(\n+        isinstance(v, (jax_core.Jaxpr, jax_core.ClosedJaxpr))\n+        for v in eqn.params.values()\n+    ):\n+      raise NotImplementedError(\n+          f\"Resource estimation does not support {eqn.primitive}\"\n+      )\n \n   return rs\n \n@@ -188,7 +195,7 @@ def _estimate_resources(\n @_register_resource_estimator(lax.cond_p)\n def _cond_resource_estimator(\n     ctx: ResourceEstimatorContext, *args, branches\n-) -> int:\n+) -> Resources:\n   del args  # Unused.\n   return functools.reduce(\n       lambda a, b: a | b,\n@@ -199,7 +206,7 @@ def _cond_resource_estimator(\n @_register_resource_estimator(lax.scan_p)\n def _scan_resource_estimator(\n     ctx: ResourceEstimatorContext, *args, jaxpr: jax_core.ClosedJaxpr, **params\n-) -> int:\n+) -> Resources:\n   del args, params  # Unused.\n   return _estimate_resources(ctx, jaxpr)\n \n@@ -211,17 +218,52 @@ def _while_resource_estimator(\n     cond_jaxpr: jax_core.ClosedJaxpr,\n     body_jaxpr: jax_core.ClosedJaxpr,\n     **params,\n-) -> int:\n+) -> Resources:\n   del args, params  # Unused.\n   return _estimate_resources(ctx, cond_jaxpr) | _estimate_resources(\n       ctx, body_jaxpr\n   )\n \n \n+@_register_resource_estimator(pjit.pjit_p)\n+def _pjit_resource_estimator(\n+    ctx: ResourceEstimatorContext,\n+    *args,\n+    jaxpr: jax_core.ClosedJaxpr,\n+    **params,\n+) -> Resources:\n+  del args, params  # Unused.\n+  return _estimate_resources(ctx, jaxpr)\n+\n+\n+@_register_resource_estimator(pallas_core.core_map_p)\n+def _core_map_resource_estimator(\n+    ctx: ResourceEstimatorContext,\n+    *args,\n+    jaxpr: jax_core.ClosedJaxpr,\n+    **params,\n+) -> Resources:\n+  del args, params  # Unused.\n+  return _estimate_resources(ctx, jaxpr)\n+\n+\n+@_register_resource_estimator(discharge.run_state_p)\n+def _run_state_resource_estimator(\n+    ctx: ResourceEstimatorContext, *args, jaxpr: jax_core.Jaxpr, **params\n+) -> Resources:\n+  del args, params  # Unused.\n+  return _estimate_resources(ctx, jaxpr)\n+\n+\n @_register_resource_estimator(primitives.run_scoped_p)\n def _run_scoped_resource_estimator(\n-    ctx: ResourceEstimatorContext, *consts, jaxpr: jax_core.Jaxpr, collective_axes\n-) -> int:\n+    ctx: ResourceEstimatorContext,\n+    *consts,\n+    jaxpr: jax_core.Jaxpr,\n+    collective_axes,\n+) -> Resources:\n+  del collective_axes  # Unused.\n+\n   # NOTE: This rule assumes that the allocation happens collectively, although\n   # it can't be checked here due to limited context. We check this in the actual\n   # lowering rule.\n@@ -280,7 +322,7 @@ def _run_scoped_resource_estimator(\n @_register_resource_estimator(lax.reduce_sum_p)\n def _reduce_sum_resource_estimator(\n     ctx: ResourceEstimatorContext, x_aval: jax_core.ShapedArray, *, axes\n-) -> int:\n+) -> Resources:\n   del ctx, axes  # Unused.\n   # We don't need shmem for some reductons, but it depends on the layout, so we\n   # conservatively request some scratch space.\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:triton] Removed the `Triton` prefix from `TritonCompilerParams`\n\nAll Triton-specific APIs are always used qualified, e.g. `plgpu.TritonCompilerParams`,\nso the prefix is redundant.\n\nPiperOrigin-RevId: 764276165",
        "output": [
            "```diff\nCommit: 30eecf68052c2ee485c40a04f07b3fe2097a7f8a\nDate: 2025-05-28T15:39:42Z\nURL: https://github.com/jax-ml/jax/commit/30eecf68052c2ee485c40a04f07b3fe2097a7f8a\nFiles changed: 13\nAdditions: +52, Deletions: -28\ndiff --git a/docs/jax.experimental.pallas.triton.rst b/docs/jax.experimental.pallas.triton.rst\nindex 76b0896ccf17..023a33bb0909 100644\n--- a/docs/jax.experimental.pallas.triton.rst\n+++ b/docs/jax.experimental.pallas.triton.rst\n@@ -9,7 +9,7 @@ Classes\n .. autosummary::\n    :toctree: _autosummary\n \n-   TritonCompilerParams\n+   CompilerParams\n \n Functions\n ---------\n@@ -19,4 +19,4 @@ Functions\n \n    approx_tanh\n    debug_barrier\n-   elementwise_inline_asm\n\\ No newline at end of file\n+   elementwise_inline_asm\ndiff --git a/docs/pallas/CHANGELOG.md b/docs/pallas/CHANGELOG.md\nindex 2d8a83c897f1..40a30057354d 100644\n--- a/docs/pallas/CHANGELOG.md\n+++ b/docs/pallas/CHANGELOG.md\n@@ -13,6 +13,14 @@ Remember to align the itemized text with the first line of an item within a list\n \n ## Unreleased\n \n+* Deprecations\n+\n+  * {class}`jax.experimental.pallas.triton.TritonCompilerParams` has been\n+    renamed to {class}`jax.experimental.pallas.triton.CompilerParams`. The\n+    old name is deprecated and will be removed in a future release.\n+\n+## Released with jax 0.6.1\n+\n * Removals\n \n   * Removed previously deprecated {mod}`jax.experimental.pallas.gpu`. To use\ndiff --git a/jax/_src/pallas/pallas_call.py b/jax/_src/pallas/pallas_call.py\nindex 964709b4c915..6f8c96a4591c 100644\n--- a/jax/_src/pallas/pallas_call.py\n+++ b/jax/_src/pallas/pallas_call.py\n@@ -1499,7 +1499,7 @@ def pallas_call(\n     interpret: Any = False,\n     name: str | None = None,\n     compiler_params: (\n-        Mapping[Backend, CompilerParams] | CompilerParams | None\n+        Mapping[Backend, \"CompilerParams\"] | \"CompilerParams\" | None\n     ) = None,\n     cost_estimate: CostEstimate | None = None,\n     backend: Backend | None = None,\n@@ -1550,7 +1550,7 @@ def pallas_call(\n     compiler_params: Optional compiler parameters. The value should either be a\n       backend-specific dataclass\n       (:class:`jax.experimental.pallas.tpu.TPUCompilerParams`,\n-      :class:`jax.experimental.pallas.triton.TritonCompilerParams`,\n+      :class:`jax.experimental.pallas.triton.CompilerParams`,\n       :class:`jax.experimental.pallas.mosaic_gpu.CompilerParams`) or a dict\n       mapping backend name to the corresponding platform-specific dataclass.\n     backend: Optional string literal one of  ``\"mosaic_tpu\"``, ``\"triton\"`` or\n@@ -1600,13 +1600,13 @@ def _normalize_compiler_params(\n ) -> Mapping[Backend, CompilerParams]:\n   if compiler_params is None:\n     return {}\n-  if isinstance(compiler_params, pallas_core.CompilerParams):\n+  if isinstance(compiler_params, CompilerParams):\n     compiler_params = {compiler_params.BACKEND: compiler_params}\n   assert isinstance(compiler_params, Mapping)\n   for backend, params in compiler_params.items():\n     if backend not in [\"mosaic_tpu\", \"mosaic_gpu\", \"triton\"]:\n       raise ValueError(f\"Unknown backend in compiler_params: {backend}\")\n-    if not isinstance(params, pallas_core.CompilerParams):\n+    if not isinstance(params, CompilerParams):\n       raise ValueError(\n           f\"Unexpected compiler_params for backend {backend}: {params}\"\n       )\ndiff --git a/jax/_src/pallas/triton/core.py b/jax/_src/pallas/triton/core.py\nindex 6b3e10f2b018..7b6e69dc8dd8 100644\n--- a/jax/_src/pallas/triton/core.py\n+++ b/jax/_src/pallas/triton/core.py\n@@ -21,7 +21,7 @@\n from jax._src.pallas import core as pallas_core\n \n @dataclasses.dataclass(frozen=True)\n-class TritonCompilerParams(pallas_core.CompilerParams):\n+class CompilerParams(pallas_core.CompilerParams):\n   \"\"\"Compiler parameters for Triton.\n \n   Attributes:\ndiff --git a/jax/_src/pallas/triton/pallas_call_registration.py b/jax/_src/pallas/triton/pallas_call_registration.py\nindex e111cef0f924..9bb5c8f21628 100644\n--- a/jax/_src/pallas/triton/pallas_call_registration.py\n+++ b/jax/_src/pallas/triton/pallas_call_registration.py\n@@ -72,9 +72,9 @@ def pallas_call_lowering(\n   [lowering_platform] = ctx.platforms or ctx.module_context.platforms\n \n   if \"triton\" in compiler_params:\n-    params = cast(triton_core.TritonCompilerParams, compiler_params[\"triton\"])\n+    params = cast(triton_core.CompilerParams, compiler_params[\"triton\"])\n   else:\n-    params = triton_core.TritonCompilerParams()\n+    params = triton_core.CompilerParams()\n   num_warps = 4 if params.num_warps is None else params.num_warps\n   num_stages = params.num_stages\n   if num_stages is None:\ndiff --git a/jax/experimental/pallas/ops/gpu/attention.py b/jax/experimental/pallas/ops/gpu/attention.py\nindex 2442ed14f351..ae429be5d73a 100644\n--- a/jax/experimental/pallas/ops/gpu/attention.py\n+++ b/jax/experimental/pallas/ops/gpu/attention.py\n@@ -288,7 +288,7 @@ def mha(\n       grid=grid_,\n       in_specs=in_specs,\n       out_specs=out_specs,\n-      compiler_params=plgpu.TritonCompilerParams(\n+      compiler_params=plgpu.CompilerParams(\n           num_warps=num_warps_, num_stages=num_stages),\n       out_shape=out_shape,\n       debug=debug,\n@@ -351,7 +351,7 @@ def _preprocess_backward(out, do, lse, block_q: int,\n                        lambda i, j, k: (j, i, k, 0)),\n       ],\n       out_specs=pl.BlockSpec((None, None, block_q), lambda i, j, k: (j, k, i)),\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=4, num_stages=3),\n+      compiler_params=plgpu.CompilerParams(num_warps=4, num_stages=3),\n       out_shape=out_shape,\n       debug=debug,\n       interpret=interpret,\n@@ -634,7 +634,7 @@ def _mha_backward(sm_scale: float, causal: bool, block_sizes: BlockSizes,\n         name=\"mha_backward\",\n         debug=debug,\n         interpret=interpret,\n-        compiler_params=plgpu.TritonCompilerParams(\n+        compiler_params=plgpu.CompilerParams(\n             num_warps=num_warps_, num_stages=2\n         ),\n     )(q, k, v, segment_ids, out, do, lse, delta)\ndiff --git a/jax/experimental/pallas/ops/gpu/decode_attention.py b/jax/experimental/pallas/ops/gpu/decode_attention.py\nindex e2c19b3eaf2d..ee8c22d1b3a4 100644\n--- a/jax/experimental/pallas/ops/gpu/decode_attention.py\n+++ b/jax/experimental/pallas/ops/gpu/decode_attention.py\n@@ -193,7 +193,7 @@ def decode_attn_unbatched(\n       pl.BlockSpec((None, block_h), lambda i, j: (j, i)),  # l\n       pl.BlockSpec((None, block_h), lambda i, j: (j, i)),  # m\n     ],\n-    compiler_params=plgpu.TritonCompilerParams(\n+    compiler_params=plgpu.CompilerParams(\n       num_warps=num_warps_, num_stages=num_stages\n     ),\n     out_shape=[\ndiff --git a/jax/experimental/pallas/ops/gpu/layer_norm.py b/jax/experimental/pallas/ops/gpu/layer_norm.py\nindex 187d74ee1fd9..b838885a9136 100644\n--- a/jax/experimental/pallas/ops/gpu/layer_norm.py\n+++ b/jax/experimental/pallas/ops/gpu/layer_norm.py\n@@ -94,7 +94,7 @@ def layer_norm_forward(\n   ]\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=num_warps),\n+      compiler_params=plgpu.CompilerParams(num_warps=num_warps),\n       grid=(),\n       out_shape=out_shape,\n       debug=False,\n@@ -215,7 +215,7 @@ def layer_norm_backward(\n   out_shape_dx = jax.ShapeDtypeStruct(shape=(n,), dtype=x.dtype)\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=num_warps),\n+      compiler_params=plgpu.CompilerParams(num_warps=num_warps),\n       grid=(),\n       out_shape=out_shape_dx,\n       debug=False,\n@@ -247,7 +247,7 @@ def layer_norm_backward(\n   grid_ = (pl.cdiv(reshaped_x.shape[1], block_n),)\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=num_warps),\n+      compiler_params=plgpu.CompilerParams(num_warps=num_warps),\n       grid=grid_,\n       out_shape=out_shape_dwbias,\n       debug=False,\n@@ -283,7 +283,7 @@ def layer_norm(\n   out_shape = jax.ShapeDtypeStruct(shape=(n,), dtype=x.dtype)\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(\n+      compiler_params=plgpu.CompilerParams(\n           num_warps=num_warps, num_stages=num_stages),\n       grid=(),\n       out_shape=out_shape,\ndiff --git a/jax/experimental/pallas/ops/gpu/paged_attention.py b/jax/experimental/pallas/ops/gpu/paged_attention.py\nindex b30ef554fe12..fbf861f92412 100644\n--- a/jax/experimental/pallas/ops/gpu/paged_attention.py\n+++ b/jax/experimental/pallas/ops/gpu/paged_attention.py\n@@ -222,7 +222,7 @@ def paged_attention_unbatched(\n       ],\n       debug=debug,\n       interpret=interpret,\n-      compiler_params=plgpu.TritonCompilerParams(\n+      compiler_params=plgpu.CompilerParams(\n           num_warps=num_warps, num_stages=num_stages\n       ),\n       name=f\"paged_attention_{block_h=}_{pages_per_compute_block=}\",\ndiff --git a/jax/experimental/pallas/ops/gpu/rms_norm.py b/jax/experimental/pallas/ops/gpu/rms_norm.py\nindex baeaeb8a57b3..a1b2b582f7bb 100644\n--- a/jax/experimental/pallas/ops/gpu/rms_norm.py\n+++ b/jax/experimental/pallas/ops/gpu/rms_norm.py\n@@ -82,7 +82,7 @@ def rms_norm_forward(\n   ]\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=num_warps),\n+      compiler_params=plgpu.CompilerParams(num_warps=num_warps),\n       grid=(),\n       out_shape=out_shape,\n       debug=False,\n@@ -196,7 +196,7 @@ def rms_norm_backward(\n   out_shape_dx = jax.ShapeDtypeStruct(shape=(n,), dtype=x.dtype)\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=num_warps),\n+      compiler_params=plgpu.CompilerParams(num_warps=num_warps),\n       grid=(),\n       out_shape=out_shape_dx,\n       debug=False,\n@@ -228,7 +228,7 @@ def rms_norm_backward(\n   grid_ = (pl.cdiv(reshaped_x.shape[1], block_n),)\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(num_warps=num_warps),\n+      compiler_params=plgpu.CompilerParams(num_warps=num_warps),\n       grid=grid_,\n       out_shape=out_shape_dwbias,\n       debug=False,\n@@ -264,7 +264,7 @@ def rms_norm(\n   out_shape = jax.ShapeDtypeStruct(shape=(n,), dtype=x.dtype)\n   method = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(\n+      compiler_params=plgpu.CompilerParams(\n           num_warps=num_warps, num_stages=num_stages\n       ),\n       grid=(),\ndiff --git a/jax/experimental/pallas/ops/gpu/softmax.py b/jax/experimental/pallas/ops/gpu/softmax.py\nindex 7fc6a0f50cb4..68960081288e 100644\n--- a/jax/experimental/pallas/ops/gpu/softmax.py\n+++ b/jax/experimental/pallas/ops/gpu/softmax.py\n@@ -80,7 +80,7 @@ def softmax(\n   kernel = functools.partial(_vmappable_softmax_kernel, block_row=block_row)\n   f = pl.pallas_call(\n       kernel,\n-      compiler_params=plgpu.TritonCompilerParams(\n+      compiler_params=plgpu.CompilerParams(\n           num_warps=num_warps, num_stages=1),\n       grid=(),\n       out_shape=out_shape,\ndiff --git a/jax/experimental/pallas/triton.py b/jax/experimental/pallas/triton.py\nindex 06adb9e6da7e..1c512540adf2 100644\n--- a/jax/experimental/pallas/triton.py\n+++ b/jax/experimental/pallas/triton.py\n@@ -14,7 +14,23 @@\n \n \"\"\"Triton-specific Pallas APIs.\"\"\"\n \n-from jax._src.pallas.triton.core import TritonCompilerParams as TritonCompilerParams\n+from jax._src.pallas.triton.core import CompilerParams as CompilerParams\n from jax._src.pallas.triton.primitives import approx_tanh as approx_tanh\n from jax._src.pallas.triton.primitives import debug_barrier as debug_barrier\n from jax._src.pallas.triton.primitives import elementwise_inline_asm as elementwise_inline_asm\n+\n+import typing as _typing  # pylint: disable=g-import-not-at-top\n+if _typing.TYPE_CHECKING:\n+  TritonCompilerParams = CompilerParams\n+else:\n+  from jax._src.deprecations import deprecation_getattr as _deprecation_getattr\n+  _deprecations = {\n+      # Deprecated on May 27th 2025.\n+      \"TritonCompilerParams\": (\n+          \"TritonCompilerParams is deprecated, use CompilerParams instead.\",\n+          CompilerParams,\n+      ),\n+  }\n+  __getattr__ = _deprecation_getattr(__name__, _deprecations)\n+  del _deprecation_getattr\n+del _typing\ndiff --git a/tests/pallas/ops_test.py b/tests/pallas/ops_test.py\nindex 3e777ac7ea2c..c819d050c8a5 100644\n--- a/tests/pallas/ops_test.py\n+++ b/tests/pallas/ops_test.py\n@@ -1646,7 +1646,7 @@ def kernel(x_ref, o_ref):\n \n   @unittest.skipIf(\n       sys.platform == \"win32\",\n-      \"plgpu_triton.TritonCompilerParams unavailable on Windows\",\n+      \"plgpu_triton.CompilerParams unavailable on Windows\",\n   )\n   def test_debug_print(self):\n     self.skip_if_mosaic_gpu()\n@@ -1661,7 +1661,7 @@ def test_debug_print(self):\n     @functools.partial(\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct((2,), jnp.float32),\n-        compiler_params=plgpu_triton.TritonCompilerParams(\n+        compiler_params=plgpu_triton.CompilerParams(\n             num_warps=1, num_stages=1\n         ),\n     )\n@@ -1677,7 +1677,7 @@ def kernel(x_ref, o_ref):\n \n   @unittest.skipIf(\n       sys.platform == \"win32\",\n-      \"plgpu_triton.TritonCompilerParams unavailable on Windows\",\n+      \"plgpu_triton.CompilerParams unavailable on Windows\",\n   )\n   def test_debug_print_with_values(self):\n     if jtu.test_device_matches([\"tpu\"]):\n@@ -1690,7 +1690,7 @@ def test_debug_print_with_values(self):\n     @functools.partial(\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct((2,), jnp.float32),\n-        compiler_params=plgpu_triton.TritonCompilerParams(\n+        compiler_params=plgpu_triton.CompilerParams(\n             num_warps=1, num_stages=1\n         ),\n     )\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix JAX PGLE test\n\nXLA dumps one more HLO file by default, which leads to one more PGLE profile\nfile.\n\nPiperOrigin-RevId: 764274080",
        "output": [
            "```diff\nCommit: fd28b2f45953d5dfd01a5bc9e7795fe38ba22b72\nDate: 2025-05-28T15:34:04Z\nURL: https://github.com/jax-ml/jax/commit/fd28b2f45953d5dfd01a5bc9e7795fe38ba22b72\nFiles changed: 1\nAdditions: +6, Deletions: -4\ndiff --git a/tests/pgle_test.py b/tests/pgle_test.py\nindex fd55f0a392f0..e136c3ab8a5a 100644\n--- a/tests/pgle_test.py\n+++ b/tests/pgle_test.py\n@@ -167,8 +167,9 @@ def f(x):\n           self.assertArraysEqual(f(x), expected)\n         self.assertEqual(cache_miss_count(), 2)\n         fdo_profiles_before_pgle = self.get_fdo_profiles(dump_dir)\n-        # One for before and one for after optimization.\n-        self.assertLen(fdo_profiles_before_pgle, 2)\n+        # One for before optimizatiom, one after SPMD partitioning, and one\n+        # after optimization.\n+        self.assertLen(fdo_profiles_before_pgle, 3)\n         # The FDO profile file should be empty.\n         self.assertEqual(\n             os.path.getsize(os.path.join(dump_dir, fdo_profiles_before_pgle[0])), 0)\n@@ -178,8 +179,9 @@ def f(x):\n           self.assertArraysEqual(f(x), expected)\n         self.assertEqual(cache_miss_count(), 2)\n         fdo_profiles_after_pgle = self.get_fdo_profiles(dump_dir)\n-        # One for before and one for after optimization.\n-        self.assertLen(fdo_profiles_after_pgle, 4)\n+        # One more before optimizatiom, one more after SPMD partitioning, and\n+        # one more after optimization.\n+        self.assertLen(fdo_profiles_after_pgle, 6)\n \n         for fdo_profile in fdo_profiles_after_pgle:\n           if fdo_profile not in fdo_profiles_before_pgle:\n\n```"
        ],
        "from_id": [
            "frgossen",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Set the mesh in SPMDAxisContext to be a concrete mesh so that pallas/mosaic:GPU can get access to the device ids in the mesh\n\nPiperOrigin-RevId: 764263324",
        "output": [
            "```diff\nCommit: 0d0393fd39b44c0627616752df71bc7b97904b80\nDate: 2025-05-28T15:05:07Z\nURL: https://github.com/jax-ml/jax/commit/0d0393fd39b44c0627616752df71bc7b97904b80\nFiles changed: 1\nAdditions: +11, Deletions: -2\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 1ed831d8577f..c38312868163 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -783,6 +783,13 @@ def _shardy_shard_map_token_sharding(\n   return ns._to_sdy_sharding(0)\n \n \n+def _get_spmdaxis_ctx_mesh(mesh):\n+  if isinstance(mesh, AbstractMesh):\n+    concrete_mesh = get_concrete_mesh()\n+    return concrete_mesh if concrete_mesh is not None else mesh\n+  return mesh\n+\n+\n def _shard_map_lowering_shardy(\n     ctx, in_nodes, jaxpr, mesh, in_specs, out_specs, manual_axes, check_vma):\n   axis_ctx = ctx.module_context.axis_context\n@@ -793,7 +800,8 @@ def _shard_map_lowering_shardy(\n     shardy_manual_axes = frozenset(mesh.axis_names) - axis_ctx.manual_axes\n   else:\n     shardy_manual_axes = manual_axes\n-  new_axis_context = sharding_impls.SPMDAxisContext(mesh, manual_axes)\n+  new_axis_context = sharding_impls.SPMDAxisContext(\n+      _get_spmdaxis_ctx_mesh(mesh), manual_axes)\n   sub_ctx = ctx.module_context.replace(axis_context=new_axis_context)\n \n   tokens = [ctx.tokens_in.get(eff) for eff in ctx.tokens_in.effects()]\n@@ -868,7 +876,8 @@ def _shard_map_lowering(ctx, *in_nodes, jaxpr, mesh, in_specs, out_specs,\n   out_avals_ = [x.aval for x in jaxpr.outvars]\n   in_nodes_ = map(partial(_xla_shard, ctx, mesh, manual_axes), in_specs,\n                   ctx.avals_in, in_avals_, in_nodes)\n-  new_axis_context = sharding_impls.SPMDAxisContext(mesh, manual_axes)\n+  new_axis_context = sharding_impls.SPMDAxisContext(\n+      _get_spmdaxis_ctx_mesh(mesh), manual_axes)\n   sub_ctx = ctx.module_context.replace(axis_context=new_axis_context)\n   with _extend_axis_env(mesh, manual_axes), config._check_vma(check_vma):\n     out_nodes_, tokens_out = mlir.call_lowering(\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Implement a new MMA/TMEM read pipelined matmul kernel\n\nThis replaces the old scheme that still included a bit of a bubble at the\nend of each tile with a new scheme that should be entirely bubble-free, for\nas long as the MMA loop is long enough to hide the store latency (i.e. for big\nenough K dimensions). This also removes the problems with spills we had in the\nprevious version since the register footprint is relatively small now.\n\nPiperOrigin-RevId: 764256446",
        "output": [
            "```diff\nCommit: 98e6041a214af302e9945f066d1db56084584e6b\nDate: 2025-05-28T14:44:32Z\nURL: https://github.com/jax-ml/jax/commit/98e6041a214af302e9945f066d1db56084584e6b\nFiles changed: 6\nAdditions: +49, Deletions: -38\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 6d54e153a9a2..82a0a47c4a0d 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -258,8 +258,7 @@ def _run_scoped_resource_estimator(\n         packing = 4 // aval.dtype.itemsize\n       else:\n         packing = 1\n-      layout = tcgen05._infer_tmem_layout(\n-          aval.shape, collective=aval.collective, packing=packing)\n+      layout = tcgen05._infer_tmem_layout(aval.shape, packing=packing)\n       cols_used = layout.cols_in_shape(aval.shape)\n       cols_used = tcgen05._alloc_ncols(cols_used, exact=False)\n       rs += Resources(tmem_scratch_cols=cols_used)\n@@ -391,8 +390,7 @@ def alloc_tmem(\n     else:\n       packing = 1\n     if layout is None:\n-      layout = tcgen05._infer_tmem_layout(\n-          struct.shape, collective, packing=packing)\n+      layout = tcgen05._infer_tmem_layout(struct.shape, packing=packing)\n     unpadded_cols_used = layout.cols_in_shape(struct.shape)\n     cols_used = tcgen05._alloc_ncols(unpadded_cols_used, exact_cols)\n \ndiff --git a/jax/experimental/mosaic/gpu/core.py b/jax/experimental/mosaic/gpu/core.py\nindex 73403fccd595..4ed551654a0e 100644\n--- a/jax/experimental/mosaic/gpu/core.py\n+++ b/jax/experimental/mosaic/gpu/core.py\n@@ -351,7 +351,7 @@ def ref(member_thunks=member_thunks):\n         )\n         if layout is None:\n           layout = tcgen05._infer_tmem_layout(\n-              shape, collective, 1 if packing is None else packing\n+              shape, 1 if packing is None else packing\n           )\n         num_cols = layout.cols_in_shape(shape)\n         tmem_allocs.append(_TMEMAlloc(addr_ref, num_cols, collective))\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex 52909f6a6a2e..ac5a8985ebff 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -107,9 +107,8 @@ def compute_output(block_m_start, n_start, call_counter):\n       call_counter should be 0 the first time this function is called and\n       incremented by 1 before each subsequent call.\n       \"\"\"\n-      isnt_first_call = arith.cmpi(\n-          arith.CmpIPredicate.ne, call_counter, c(0, index)\n-      )\n+      acc_slot = arith.remui(call_counter, c(2, index))\n+      acc_slice = acc.slice(slice(None), mgpu.ds(arith.muli(acc_slot, c(tile_n, index)), tile_n))\n       # All blocks in the cluster share the same m_start -- align it!\n       m_start = arith.muli(arith.divui(block_m_start, c(tile_m, index)), c(tile_m, index))\n       with mgpu.when(is_leader_of(TMA_WARP)):\n@@ -119,6 +118,9 @@ def _tma_body(ki, _):\n           isnt_warmup = arith.cmpi(\n               arith.CmpIPredicate.uge, ki, c(max_concurrent_steps, index)\n           )\n+          isnt_first_call = arith.cmpi(\n+              arith.CmpIPredicate.ne, call_counter, c(0, index)\n+          )\n           with mgpu.when(arith.ori(isnt_first_call, isnt_warmup)):\n             ab_empty_barriers[slot].wait()\n           full_barrier = ab_full_barriers[slot]\n@@ -151,15 +153,16 @@ def _tma_body(ki, _):\n           )\n \n       # We wait in all blocks in the cluster to avoid double arrival errors.\n-      with mgpu.when(arith.andi(is_leader_of(MMA_WARP), isnt_first_call)):\n-        tmem_done_barrier.wait(for_tensor_core=True)\n+      reuses_tmem = arith.cmpi(arith.CmpIPredicate.uge, call_counter, c(2, index))\n+      with mgpu.when(arith.andi(is_leader_of(MMA_WARP), reuses_tmem)):\n+        tmem_done_barrier[acc_slot].wait(for_tensor_core=True)\n       with mgpu.when(arith.andi(is_leader_of(MMA_WARP), is_leader_block)):\n         @mgpu.fori(c(k_loop_iter, index), arith.constant(i1, 0))\n         def _mma_body(ki, accumulate):\n           slot = arith.remui(ki, c(max_concurrent_steps, index))\n           ab_full_barriers[slot].wait()\n           tcgen05.mma(\n-              acc,\n+              acc_slice,\n               mgpu.memref_slice(a_smem, slot),\n               mgpu.memref_transpose(mgpu.memref_slice(b_smem, slot), (1, 0, 3, 2)),\n               a_swizzle=swizzle,\n@@ -173,21 +176,17 @@ def _mma_body(ki, accumulate):\n               arith.CmpIPredicate.eq, ki, c(k_loop_iter - 1, index)\n           )\n           with mgpu.when(is_last_iter):\n-            tcgen05.commit_arrive(mma_done_barrier, collective=collective, ctx=ctx)\n+            tcgen05.commit_arrive(mma_done_barrier[acc_slot], collective=collective, ctx=ctx)\n           return accumulate\n \n       with mgpu.when(is_store_warpgroup):\n-        mma_done_barrier.wait(for_tensor_core=True)\n-        final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n+        mma_done_barrier[acc_slot].wait(for_tensor_core=True)\n+        final_acc = acc_slice.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n         assert tile_n % epilogue_tile_n == 0\n         for ni in range(tile_n // epilogue_tile_n):\n           n_slice = ds(ni * epilogue_tile_n, epilogue_tile_n)\n           final_acc[:, n_slice].store_tiled(d_smem, swizzle=128)\n           # We store the first tile before arriving to reduce register pressure.\n-          if ni == 0:\n-            # Make sure we've loaded all of TMEM before we arrive.\n-            tcgen05.wait_tmem_load()\n-            tmem_done_barrier.arrive(for_tensor_core=True)\n           mgpu.commit_shared()\n           store_n_start = arith.addi(n_start, c(ni * epilogue_tile_n, index))\n           ctx.async_copy(\n@@ -200,7 +199,8 @@ def _mma_body(ki, accumulate):\n               gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n               swizzle=128,\n           )\n-          ctx.await_async_copy(0)\n+          ctx.await_async_copy(0, await_read_only=True)\n+        tmem_done_barrier[acc_slot].arrive(for_tensor_core=True)\n \n     # We statically assign the tiles to SMs.\n     logical_grid_size = math.prod(logical_grid)\n@@ -246,9 +246,9 @@ def _mn_loop(local_mn_step, _):\n   smem = (\n       smem_buffers,\n       [mgpu.Barrier(arrival_count=1, num_barriers=max_concurrent_steps)] * 2,\n-      mgpu.Barrier(arrival_count=1),\n-      mgpu.ClusterBarrier(collective_dims=(gpu.Dimension.x,), num_barriers=1),\n-      mgpu.TMEM((128, tile_n), jnp.float32, collective=collective),\n+      mgpu.Barrier(arrival_count=1, num_barriers=2),\n+      mgpu.ClusterBarrier(collective_dims=(gpu.Dimension.x,), num_barriers=2),\n+      mgpu.TMEM((128, 2 * tile_n), jnp.float32, collective=collective),\n   )\n   num_sms = 148\n   return mgpu.as_gpu_kernel(\n@@ -273,7 +273,7 @@ def main(unused_argv):\n   b = jr.normal(key=kb, shape=(n, k), dtype=jnp.float16)\n \n   tile_m = (128,)\n-  tile_n = (128, 256, 512)\n+  tile_n = (128, 256)\n   max_concurrent_steps = (2, 4, 5, 6)\n   grid_tile_m = (1, 2, 4, 8, 16)\n   collective = (False, True)\n@@ -290,7 +290,7 @@ def main(unused_argv):\n       tile_n *= 2\n     if m < tile_m or n < tile_n:\n       continue\n-    if tile_n > 512:\n+    if 2 * tile_n > 512:\n       continue\n     if (m // tile_m) % kwargs[\"grid_tile_m\"]:\n       continue\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex 86fbd31ed56d..13d945249b69 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -173,9 +173,14 @@ def mma(\n     raise ValueError(\n         f\"Accumulator shape mismatch: expected {(m, n * num_cta)}, got {d.shape}\"\n     )\n-  if d.layout != (expected_layout := _infer_tmem_layout(d.shape, collective, packing=1)):\n+  expected_d_layout = (\n+      TMEM_COLLECTIVE_N512_LAYOUT\n+      if collective and n * num_cta == 512\n+      else TMEM_DEFAULT_LAYOUT\n+  )\n+  if d.layout != expected_d_layout:\n     raise ValueError(\n-        f\"Accumulator layout mismatch: expected {expected_layout}, got {d.layout}\"\n+        f\"Accumulator layout mismatch: expected {expected_d_layout}, got {d.layout}\"\n     )\n   f32 = ir.F32Type.get()\n   f16 = ir.F16Type.get()\n@@ -570,9 +575,7 @@ def cols_in_shape(self, shape: tuple[int, int]):\n     return num_tiles // tiles_in_row * cols_in_tile\n \n \n-def _infer_tmem_layout(\n-    shape: tuple[int, int], collective: bool, packing: int = 1\n-) -> TMEMLayout:\n+def _infer_tmem_layout(shape: tuple[int, int], packing: int = 1) -> TMEMLayout:\n   if shape[0] > TMEM_ROWS:\n     raise ValueError(\n         \"Can only infer TMEM layout for shapes with at most 128 rows, got:\"\n@@ -593,14 +596,14 @@ def _infer_tmem_layout(\n         \"Can only infer TMEM layout for shapes with column count that's a\"\n         f\" multiple of 8, got: {shape[1]}\"\n     )\n-  if collective and shape[1] == 512:\n-    return TMEMLayout(\n-        elements_in_tile=(shape[0], 128), column_tile_stride=2, packing=packing\n-    )\n-  else:\n-    return TMEMLayout(elements_in_tile=(shape[0], 8), packing=packing)\n+  return TMEMLayout(elements_in_tile=(shape[0], 8), packing=packing)\n \n \n+TMEM_DEFAULT_LAYOUT = TMEMLayout(elements_in_tile=(TMEM_ROWS, 8), packing=1)\n+TMEM_COLLECTIVE_N512_LAYOUT = TMEMLayout(\n+    elements_in_tile=(TMEM_ROWS, 128), column_tile_stride=2, packing=1\n+)\n+\n @dataclasses.dataclass(frozen=True)\n class TMEMRef:\n   address: ir.Value\n@@ -669,6 +672,8 @@ def slice(self, *idxs):\n     col_idx = base_idx[1]\n     if not isinstance(col_idx, ir.Value):\n       col_idx = arith.constant(i32, col_idx)\n+    if col_idx.type == ir.IndexType.get():\n+      col_idx = arith.index_cast(i32, col_idx)\n     if packing != 1:\n       col_idx = arith.divui(col_idx, arith.constant(i32, packing))\n     return TMEMRef(\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 232cb703d06a..99b7d67cd691 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -1398,11 +1398,12 @@ def quantize(x):\n     y_block_shape = (n_block_tile, k) if rhs_transpose else (k, n_block_tile)\n     y = quantize(self.prng.uniform(-1, 1, y_shape)).astype(in_jax_dtype)\n     out_shape = jax.ShapeDtypeStruct((m, n), out_jax_dtype)\n+    tmem_layout = tcgen05.TMEM_COLLECTIVE_N512_LAYOUT if n == 512 else None\n     scratch_shape = [\n         jax.ShapeDtypeStruct(tile_shape(x_block_shape, tiling), in_jax_dtype),\n         jax.ShapeDtypeStruct(tile_shape(y_block_shape, tiling), in_jax_dtype),\n         mgpu.TMABarrier(3),\n-        mgpu.TMEM((128, n), out_jax_dtype, collective=True),\n+        mgpu.TMEM((128, n), out_jax_dtype, collective=True, layout=tmem_layout),\n     ]\n     z = mgpu.as_gpu_kernel(\n         kernel, (2, 1, 1), (128, 1, 1), (x, y), out_shape, scratch_shape, cluster=(2, 1, 1)\ndiff --git a/tests/mosaic/matmul_test.py b/tests/mosaic/matmul_test.py\nindex 680e699c8972..13082885710e 100644\n--- a/tests/mosaic/matmul_test.py\n+++ b/tests/mosaic/matmul_test.py\n@@ -161,8 +161,15 @@ def test_matmul_sm100(self, data):\n     tile_m = data.draw(\n         hps.sampled_from([t for t in [128] if t * num_ctas <= m]), label=\"tile_m\"\n     )\n+    tmem_cols = 512\n     tile_n = data.draw(\n-        hps.sampled_from([t for t in [64, 128, 256] if t * num_ctas <= n]), label=\"tile_n\"\n+        hps.sampled_from([\n+            t\n+            for t in [64, 128, 256]\n+            # We're double buffering TMEM in the kernel, hence the 2x.\n+            if t * num_ctas <= n and 2 * t * num_ctas <= tmem_cols\n+        ]),\n+        label=\"tile_n\",\n     )\n     grid_m = m // (num_ctas * tile_m)\n     grid_tile_m = data.draw(hps.sampled_from([1, 2, 4, 8, 16]), label=\"grid_tile_m\")\n@@ -196,4 +203,4 @@ def test_matmul_sm100(self, data):\n \n \n if __name__ == \"__main__\":\n-  absltest.main(testLoader=jtu.JaxTestLoader())\n+  absltest.main(argv=[\"python\"], testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Reduce SMEM pressure of the GMEM store\n\nThis reworks the previous scheme by transferring all of TMEM to registers at once,\nand then doing RMEM->SMEM->GMEM in multiple phases, allowing us to use a smaller\nSMEM buffer. This, in turn, lets us bump max_concurrent_steps for the MMA pipeline\nwhich increases performance considerably.\n\nThe only downside of this scheme is that even though it should be technically feasible\nto perform the epilogue with 255 registers per thread, ptxas generates a number of spills\nthat might be lowering our performance. Either way, it's still better than the previous\nalternatives.\n\nPiperOrigin-RevId: 764249234",
        "output": [
            "```diff\nCommit: 360799e6405004a9d9a59e044122168314dff970\nDate: 2025-05-28T14:20:00Z\nURL: https://github.com/jax-ml/jax/commit/360799e6405004a9d9a59e044122168314dff970\nFiles changed: 3\nAdditions: +43, Deletions: -14\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex bf50ca702063..52909f6a6a2e 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -179,17 +179,28 @@ def _mma_body(ki, accumulate):\n       with mgpu.when(is_store_warpgroup):\n         mma_done_barrier.wait(for_tensor_core=True)\n         final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n-        final_acc.store_tiled(d_smem, swizzle=128)\n-        mgpu.commit_shared()\n-        tmem_done_barrier.arrive()\n-        ctx.async_copy(\n-            src_ref=d_smem,\n-            dst_ref=d,\n-            gmem_slice=(ds(block_m_start, block_tile_m), ds(n_start, tile_n)),\n-            gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n-            swizzle=128,\n-        )\n-        ctx.await_async_copy(0)\n+        assert tile_n % epilogue_tile_n == 0\n+        for ni in range(tile_n // epilogue_tile_n):\n+          n_slice = ds(ni * epilogue_tile_n, epilogue_tile_n)\n+          final_acc[:, n_slice].store_tiled(d_smem, swizzle=128)\n+          # We store the first tile before arriving to reduce register pressure.\n+          if ni == 0:\n+            # Make sure we've loaded all of TMEM before we arrive.\n+            tcgen05.wait_tmem_load()\n+            tmem_done_barrier.arrive(for_tensor_core=True)\n+          mgpu.commit_shared()\n+          store_n_start = arith.addi(n_start, c(ni * epilogue_tile_n, index))\n+          ctx.async_copy(\n+              src_ref=d_smem,\n+              dst_ref=d,\n+              gmem_slice=(\n+                  ds(block_m_start, block_tile_m),\n+                  ds(store_n_start, epilogue_tile_n),\n+              ),\n+              gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n+              swizzle=128,\n+          )\n+          ctx.await_async_copy(0)\n \n     # We statically assign the tiles to SMs.\n     logical_grid_size = math.prod(logical_grid)\n@@ -227,8 +238,9 @@ def _mn_loop(local_mn_step, _):\n         mgpu.tile_shape((max_concurrent_steps, block_tile_n, tile_k), tiling),\n         dtype),\n   )\n+  epilogue_tile_n = 64\n   epilogue_buffer = jax.ShapeDtypeStruct(\n-      mgpu.tile_shape((block_tile_m, tile_n), (128, swizzle_elems)),\n+      mgpu.tile_shape((block_tile_m, epilogue_tile_n), (128, swizzle_elems)),\n       dtype)\n   smem_buffers = [compute_buffers, epilogue_buffer]\n   smem = (\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex c4f670527a0c..86fbd31ed56d 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -477,6 +477,17 @@ def tmem_load(tmem_addr, shape, num, pack: bool):\n   return [llvm.extractvalue(i32, regs, [i]) for i in range(num_out_regs)]\n \n \n+def wait_tmem_load():\n+  llvm.inline_asm(\n+      ir.Type.parse(\"!llvm.void\"),\n+      [],\n+      \"tcgen05.wait::ld.sync.aligned;\",\n+      \"\",\n+      has_side_effects=True,\n+  )\n+  utils.warpgroup_barrier()\n+\n+\n def tmem_store(tmem_addr, shape, num, regs, unpack: bool):\n   num_out_regs, regs_vector = _tmem_access_helper(shape, num)\n   pack_mod = \".unpack::16b\" if unpack else \"\"\n@@ -832,7 +843,7 @@ def _transfer_32xcols(\n   regs_per_instr = atom_shape[0] * atom_shape[1] // (utils.WARP_SIZE * reg_packing)\n   # We artificially lower the instr_num compared to its limits, because higher\n   # values can lead to register spills..\n-  instr_num = min(total_num, 64 // regs_per_instr)\n+  instr_num = min(total_num, 32 // regs_per_instr)\n   assert 32 % atom_rows == 0\n   num_row_steps = 32 // atom_rows\n   for lane_step in range(num_row_steps):\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex e55a21442db0..51b6ed4612ca 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -993,11 +993,17 @@ def __iter__(self):\n   def __getitem__(self, offset):\n     return CollectiveBarrierRef(self.barrier[offset], self.cluster_mask)\n \n-  def arrive(self):\n+  def arrive(self, for_tensor_core: bool = False):\n     \"\"\"Arrives on a barrier in all blocks that share at least one of the coordinates along the collective dimensions.\n \n     Note that unlike in arrive, each warpgroup arrives once.\n     \"\"\"\n+    if for_tensor_core:\n+      llvm.inline_asm(\n+          ir.Type.parse(\"!llvm.void\"),\n+          [], \"tcgen05.fence::before_thread_sync;\", \"\",\n+          has_side_effects=True,\n+      )\n     if self.barrier.num_barriers != 1:\n       raise ValueError(\"Can only arrive on a single barrier\")\n     if self.cluster_mask is None:\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Use a second warpgroup to store the MMA outputs\n\nThis allows us to prime the GMEM->SMEM pipeline for the next tile\nwhile storing the SMEM->GMEM tile for the current one. However, this implies\nthat we can no longer share the same SMEM region for the MMA pipeline\nand the epilogue, which pushes the SMEM pressure so high that we can't fetch\ntoo many steps into the future. Overall the performance is slightly worse than\nfor the baseline kernel, but it recovers and improves upon it in the follow up.\n\nPiperOrigin-RevId: 764220403",
        "output": [
            "```diff\nCommit: 39f09066e35205335f4c9dd1013316200e77dc31\nDate: 2025-05-28T12:42:34Z\nURL: https://github.com/jax-ml/jax/commit/39f09066e35205335f4c9dd1013316200e77dc31\nFiles changed: 2\nAdditions: +39, Deletions: -21\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex 8bf8ca557496..bf50ca702063 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -86,7 +86,7 @@ def build_kernel(\n   logical_grid = (grid_tile_m, n // tile_n, m // (block_tile_m * grid_tile_m))\n \n   def kernel(ctx, a, b, d, smem):\n-    ((a_smem, b_smem), d_smem), barriers, mma_done_barrier, acc = smem\n+    ((a_smem, b_smem), d_smem), barriers, mma_done_barrier, tmem_done_barrier, acc = smem\n     (ab_full_barriers, ab_empty_barriers) = barriers\n \n     warp_idx = mgpu.warp_idx(sync=True)\n@@ -97,6 +97,9 @@ def kernel(ctx, a, b, d, smem):\n     is_leader_block = arith.cmpi(\n         arith.CmpIPredicate.eq, ctx.cluster_idx(gpu.Dimension.x), c(0, index)\n     )\n+    is_store_warpgroup = arith.cmpi(\n+        arith.CmpIPredicate.eq, mgpu.warpgroup_idx(sync=True), c(1, i32)\n+    )\n \n     def compute_output(block_m_start, n_start, call_counter):\n       \"\"\"Compute and store a single output tile.\n@@ -104,6 +107,9 @@ def compute_output(block_m_start, n_start, call_counter):\n       call_counter should be 0 the first time this function is called and\n       incremented by 1 before each subsequent call.\n       \"\"\"\n+      isnt_first_call = arith.cmpi(\n+          arith.CmpIPredicate.ne, call_counter, c(0, index)\n+      )\n       # All blocks in the cluster share the same m_start -- align it!\n       m_start = arith.muli(arith.divui(block_m_start, c(tile_m, index)), c(tile_m, index))\n       with mgpu.when(is_leader_of(TMA_WARP)):\n@@ -113,9 +119,6 @@ def _tma_body(ki, _):\n           isnt_warmup = arith.cmpi(\n               arith.CmpIPredicate.uge, ki, c(max_concurrent_steps, index)\n           )\n-          isnt_first_call = arith.cmpi(\n-              arith.CmpIPredicate.ne, call_counter, c(0, index)\n-          )\n           with mgpu.when(arith.ori(isnt_first_call, isnt_warmup)):\n             ab_empty_barriers[slot].wait()\n           full_barrier = ab_full_barriers[slot]\n@@ -147,6 +150,9 @@ def _tma_body(ki, _):\n               **common_args,\n           )\n \n+      # We wait in all blocks in the cluster to avoid double arrival errors.\n+      with mgpu.when(arith.andi(is_leader_of(MMA_WARP), isnt_first_call)):\n+        tmem_done_barrier.wait(for_tensor_core=True)\n       with mgpu.when(arith.andi(is_leader_of(MMA_WARP), is_leader_block)):\n         @mgpu.fori(c(k_loop_iter, index), arith.constant(i1, 0))\n         def _mma_body(ki, accumulate):\n@@ -170,20 +176,20 @@ def _mma_body(ki, accumulate):\n             tcgen05.commit_arrive(mma_done_barrier, collective=collective, ctx=ctx)\n           return accumulate\n \n-      gpu.barrier()\n-      mma_done_barrier.wait(for_tensor_core=True)\n-\n-      final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n-      final_acc.store_tiled(d_smem, swizzle=128)\n-      mgpu.commit_shared()\n-      ctx.async_copy(\n-          src_ref=d_smem,\n-          dst_ref=d,\n-          gmem_slice=(ds(block_m_start, block_tile_m), ds(n_start, tile_n)),\n-          gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n-          swizzle=swizzle,\n-      )\n-      ctx.await_async_copy(0)\n+      with mgpu.when(is_store_warpgroup):\n+        mma_done_barrier.wait(for_tensor_core=True)\n+        final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n+        final_acc.store_tiled(d_smem, swizzle=128)\n+        mgpu.commit_shared()\n+        tmem_done_barrier.arrive()\n+        ctx.async_copy(\n+            src_ref=d_smem,\n+            dst_ref=d,\n+            gmem_slice=(ds(block_m_start, block_tile_m), ds(n_start, tile_n)),\n+            gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n+            swizzle=128,\n+        )\n+        ctx.await_async_copy(0)\n \n     # We statically assign the tiles to SMs.\n     logical_grid_size = math.prod(logical_grid)\n@@ -224,18 +230,19 @@ def _mn_loop(local_mn_step, _):\n   epilogue_buffer = jax.ShapeDtypeStruct(\n       mgpu.tile_shape((block_tile_m, tile_n), (128, swizzle_elems)),\n       dtype)\n-  smem_buffers = mgpu.Union([compute_buffers, epilogue_buffer])\n+  smem_buffers = [compute_buffers, epilogue_buffer]\n   smem = (\n       smem_buffers,\n       [mgpu.Barrier(arrival_count=1, num_barriers=max_concurrent_steps)] * 2,\n       mgpu.Barrier(arrival_count=1),\n+      mgpu.ClusterBarrier(collective_dims=(gpu.Dimension.x,), num_barriers=1),\n       mgpu.TMEM((128, tile_n), jnp.float32, collective=collective),\n   )\n   num_sms = 148\n   return mgpu.as_gpu_kernel(\n       kernel,\n       (num_sms, 1, 1),  # This is a persistent kernel.\n-      (128, 1, 1),\n+      (2 * 128, 1, 1),\n       (\n           jax.ShapeDtypeStruct((m, k), dtype),\n           jax.ShapeDtypeStruct((n, k), dtype),\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex 224f5a09cfe5..e55a21442db0 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -817,8 +817,19 @@ def update_parities(self, parities: ir.Value) -> tuple[ir.Value, ir.Value]:\n     )\n     return parity, arith.xori(parities, bitmask)\n \n-  def arrive(self, arrival_count: int = 1, can_complete: bool = True):\n+  def arrive(\n+      self,\n+      arrival_count: int = 1,\n+      can_complete: bool = True,\n+      for_tensor_core: bool = False,\n+  ):\n     i64 = ir.IntegerType.get_signless(64)\n+    if for_tensor_core:\n+      llvm.inline_asm(\n+          ir.Type.parse(\"!llvm.void\"),\n+          [], \"tcgen05.fence::before_thread_sync;\", \"\",\n+          has_side_effects=True,\n+      )\n     if can_complete:\n       if arrival_count > 1:\n         count = c(arrival_count - 1, ir.IntegerType.get_signless(32))\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Implement FragmentedArray.__getitem__ for arbitrary tiled layouts\n\nAny tile-aligned slicing is easy to handle.\n\nPiperOrigin-RevId: 764189366",
        "output": [
            "```diff\nCommit: 0b17f6ce59e893940b16cb8d2aa7b39661e587df\nDate: 2025-05-28T10:49:44Z\nURL: https://github.com/jax-ml/jax/commit/0b17f6ce59e893940b16cb8d2aa7b39661e587df\nFiles changed: 2\nAdditions: +108, Deletions: -27\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex 77584b5f0dd4..04dd30023293 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -1376,26 +1376,26 @@ def bitcast(self, elt: ir.Type, *, output_is_signed: bool | None = None):\n     )\n \n   def __getitem__(self, idx):\n-    if self.layout !=  WGMMA_LAYOUT:\n-      raise NotImplementedError(\"Only WGMMA layouts support slicing\")\n+    if not isinstance(self.layout, TiledLayout):\n+      raise NotImplementedError(\"Only arrays with tiled layouts can be sliced\")\n     base_idx, slice_shape, is_squeezed = utils.parse_indices(idx, self.shape)\n+    if any(isinstance(idx, ir.Value) for idx in base_idx):\n+      raise ValueError(\"Only static slicing allowed\")\n     if any(is_squeezed):\n       raise NotImplementedError(\"Only slicing implemented\")\n-    if (\n-        base_idx[0] % 64\n-        or slice_shape[0] % 64\n-        or base_idx[1] % 8\n-        or slice_shape[1] % 8\n+    base_tile_shape = self.layout.base_tile_shape\n+    if len(base_tile_shape) != len(self.shape):\n+      raise NotImplementedError(\"Tiling has different rank than array\")\n+    if any(\n+        b % t or l % t\n+        for b, l, t in zip(base_idx, slice_shape, base_tile_shape, strict=True)\n     ):\n       raise NotImplementedError(\"Only tile aligned slicing supported\")\n-    base_idx[0] //= 64\n-    slice_shape[0] //= 64\n-    base_idx[1] //= 8\n-    slice_shape[1] //= 8\n-    new_regs = self.registers[\n-        base_idx[0] : base_idx[0] + slice_shape[0],\n-        base_idx[1] : base_idx[1] + slice_shape[1],\n-    ]\n+    register_slices = tuple(\n+        slice(b // t, (b + l) // t)\n+        for b, l, t in zip(base_idx, slice_shape, base_tile_shape, strict=True)\n+    )\n+    new_regs = self.registers[register_slices]\n     return FragmentedArray(\n         _registers=new_regs, _layout=self.layout, _is_signed=self.is_signed\n     )\n@@ -1882,6 +1882,21 @@ def select(self, on_true, on_false):\n         lambda t, p, f: arith.select(p, t, f), self, on_false,\n     )\n \n+  @classmethod\n+  def build(\n+      cls,\n+      shape: tuple[int, ...],\n+      layout: FragmentedLayout,\n+      fn: Callable[..., ir.Value],  # ir.Value varargs, one for each dim\n+      *,\n+      is_signed: bool | None = None,\n+  ):\n+    undef = llvm.mlir_undef(ir.IntegerType.get_signless(32))\n+    dummy = cls.splat(undef, shape, layout, is_signed=False)\n+    return dummy.foreach(\n+        lambda _, idx: fn(*idx), create_array=True, is_signed=is_signed\n+    )\n+\n   def foreach(\n       self,\n       fn: Callable[[ir.Value, tuple[ir.Value, ...]], ir.Value | None],\n@@ -1892,8 +1907,19 @@ def foreach(\n     \"\"\"Call a function for each value and index.\"\"\"\n     index = ir.IndexType.get()\n     new_regs = None\n-    if create_array:\n-      new_regs = np.full_like(self.registers, llvm.mlir_undef(self.registers.flat[0].type))\n+    orig_fn = fn\n+    def fn(*args):\n+      nonlocal new_regs\n+      result = orig_fn(*args)\n+      old_reg_type = self.registers.flat[0].type\n+      # Lazily create new_regs once we know the desired output type.\n+      if create_array and new_regs is None:\n+        if ir.VectorType.isinstance(old_reg_type):\n+          new_reg_type = ir.VectorType.get(old_reg_type.shape, result.type)\n+        else:\n+          new_reg_type = result.type\n+        new_regs = np.full_like(self.registers, llvm.mlir_undef(new_reg_type))\n+      return result\n     for mlir_idx, reg_idx in zip(self.layout.thread_idxs(self.shape), np.ndindex(self.registers.shape), strict=True):\n       reg = self.registers[reg_idx]\n       assert len(mlir_idx) == len(self.shape), (mlir_idx, self.shape)\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 0c79d26782c7..232cb703d06a 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -3544,7 +3544,9 @@ def test_pass_is_registered(self):\n \n if hp is not None:\n   @hps.composite\n-  def tiled_layouts(draw, initial_tile, vector_transfer: bool = False):\n+  def tiled_layouts(\n+      draw, initial_tile, vector_transfer: bool = False\n+  ) -> fa.TiledLayout:\n     assert all(t.bit_count() == 1 for t in initial_tile)\n     assert math.prod(initial_tile) >= 128\n     tiles = [initial_tile]\n@@ -3605,20 +3607,28 @@ def tiled_layouts(draw, initial_tile, vector_transfer: bool = False):\n         vector_dim=vector_dim,\n     )\n \n+  @hps.composite\n+  def shape_and_tiled_layout(\n+      draw, vector_transfer: bool = False\n+  ) -> tuple[tuple[int, ...], fa.TiledLayout]:\n+    rank = draw(hps.integers(2, 3))\n+    initial_tile = tuple(\n+        draw(hps.sampled_from([1, 2, 4, 8, 16, 32, 64, 128]))\n+        for _ in range(rank)\n+    )\n+    hp.assume(128 <= math.prod(initial_tile) < 128 * 32)\n+    shape = tuple(t * draw(hps.integers(1, 5)) for t in initial_tile)\n+    hp.assume(math.prod(shape) <= 128 * 128)\n+    layout = draw(tiled_layouts(initial_tile, vector_transfer=vector_transfer))\n+    return shape, layout\n+\n   class HypothesisTest(TestCase):\n \n     def test_reduce(self):\n       @hps.composite\n       def strategy(draw):\n-        rank = draw(hps.integers(2, 3))\n-        initial_tile = tuple(\n-            draw(hps.sampled_from([1, 2, 4, 8, 16, 32, 64, 128]))\n-            for _ in range(rank)\n-        )\n-        hp.assume(128 <= math.prod(initial_tile) < 128 * 32)\n-        shape = tuple(t * draw(hps.integers(1, 5)) for t in initial_tile)\n-        hp.assume(math.prod(shape) <= 128 * 128)\n-        layout = draw(tiled_layouts(initial_tile, vector_transfer=True))\n+        shape, layout = draw(shape_and_tiled_layout(vector_transfer=True))\n+        rank = len(shape)\n         reduced_dims = draw(hps.sets(hps.integers(0, rank - 1), min_size=1))\n         return shape, layout, tuple(reduced_dims)\n \n@@ -3645,6 +3655,51 @@ def kernel(ctx, src, dst, scratch):\n         np.testing.assert_array_equal(result, x.max(reduced_dims))\n       run()\n \n+    def test_slice(self):\n+      i32 = ir.IntegerType.get_signless(32)\n+      index = ir.IndexType.get()\n+\n+      @hps.composite\n+      def strategy(draw):\n+        shape, layout = draw(shape_and_tiled_layout(vector_transfer=True))\n+        tiling = layout.base_tile_shape\n+        tiled_shape = mgpu.tile_shape(shape, tiling)[:len(shape)]\n+        def draw_slice(size, tile):\n+          start = draw(hps.integers(0, size - 1))\n+          length = draw(hps.integers(1, size - start))\n+          return slice(start * tile, (start + length) * tile)\n+        slices = tuple(map(draw_slice, tiled_shape, tiling))\n+        return shape, layout, slices\n+\n+      basic_slices = (slice(128, 256), slice(16, 16 + 32))\n+      @hp.given(strategy())\n+      @hp.example(((256, 256), fa.WGMMA_LAYOUT, basic_slices))\n+      @hp.example(((256, 256), tcgen05.LAYOUT, basic_slices))\n+      @hp.example(((256, 256), tcgen05.TMEM_NATIVE_LAYOUT, basic_slices))\n+      def run(args):\n+        shape, layout, slices = args\n+        def kernel(ctx, dst, _):\n+          def linear_index(*idxs):\n+            total = arith.constant(index, 0)\n+            stride = 1\n+            for i, size in zip(idxs[::-1], shape[::-1]):\n+              total = arith.addi(total, arith.muli(i, c(stride, index)))\n+              stride *= size\n+            return arith.index_cast(i32, total)\n+          x = mgpu.FragmentedArray.build(\n+              shape, layout, linear_index, is_signed=True\n+          )\n+          x[slices].store_untiled(dst, optimized=False)\n+\n+        slice_shape = tuple(len(range(size)[s]) for s, size in zip(slices, shape))\n+        out_shape = jax.ShapeDtypeStruct(shape=slice_shape, dtype=jnp.int32)\n+        result = mgpu.as_gpu_kernel(\n+            kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ()\n+        )()\n+        iota = np.arange(np.prod(shape), dtype=jnp.int32).reshape(*shape)\n+        np.testing.assert_array_equal(result, iota[slices])\n+      run()\n+\n \n if __name__ == \"__main__\":\n   absltest.main(argv=[\"python\"], testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Improve the error message when PTX version inference fails\n\nPiperOrigin-RevId: 764182705",
        "output": [
            "```diff\nCommit: f7adde5227514094beb725766fc4648f115a0aa5\nDate: 2025-05-28T10:25:24Z\nURL: https://github.com/jax-ml/jax/commit/f7adde5227514094beb725766fc4648f115a0aa5\nFiles changed: 1\nAdditions: +3, Deletions: -2\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 54fef13a8521..bf6a04783be7 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -232,8 +232,9 @@ absl::StatusOr<int> GetLatestPtxasPtxIsaVersion() {\n   // Unsupported .version 99.99; current version is '8.8'\n   std::vector<std::string> chunks = absl::StrSplit(status.message(), '\\'');\n   if (chunks.size() != 3) {\n-    return absl::InternalError(\n-        \"Failed to locate PTX ISA version in ptxas error message\");\n+    return absl::InternalError(absl::StrCat(\n+        \"Failed to locate PTX ISA version in ptxas error message: \",\n+        status.message()));\n   }\n   std::vector<std::string> major_minor = absl::StrSplit(chunks[1], '.');\n   if (major_minor.size() != 2) {\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move jax/_src/attrs.py to its own BUILD rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nThis required moving a couple `jax.numpy` imports into local functions. These could probably be addressed by moving the registrations elsewhere.\n\nPiperOrigin-RevId: 764170653",
        "output": [
            "```diff\nCommit: 27e4a7486246879a16946ed18a9ba964d72d97c2\nDate: 2025-05-28T09:41:51Z\nURL: https://github.com/jax-ml/jax/commit/27e4a7486246879a16946ed18a9ba964d72d97c2\nFiles changed: 2\nAdditions: +23, Deletions: -5\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 396e6fdf6ed4..2e2d7902577d 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -297,7 +297,6 @@ py_library_providing_imports_info(\n         \"_src/ad_checkpoint.py\",\n         \"_src/api.py\",\n         \"_src/array.py\",\n-        \"_src/attrs.py\",\n         \"_src/blocked_sampler.py\",\n         \"_src/buffer_callback.py\",\n         \"_src/callback.py\",\n@@ -377,6 +376,7 @@ py_library_providing_imports_info(\n         \":ad\",\n         \":ad_util\",\n         \":api_util\",\n+        \":attrs\",\n         \":basearray\",\n         \":batching\",\n         \":cloud_tpu_init\",\n@@ -465,6 +465,22 @@ pytype_strict_library(\n     ] + py_deps(\"numpy\"),\n )\n \n+pytype_strict_library(\n+    name = \"attrs\",\n+    srcs = [\"_src/attrs.py\"],\n+    deps = [\n+        \":ad\",\n+        \":ad_util\",\n+        \":api_util\",\n+        \":core\",\n+        \":dtypes\",\n+        \":partial_eval\",\n+        \":source_info_util\",\n+        \":tree_util\",\n+        \":util\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"basearray\",\n     srcs = [\"_src/basearray.py\"],\ndiff --git a/jax/_src/attrs.py b/jax/_src/attrs.py\nindex db738ee6368d..7ad6f0e52d32 100644\n--- a/jax/_src/attrs.py\n+++ b/jax/_src/attrs.py\n@@ -16,7 +16,6 @@\n \n from typing import Any, Callable\n \n-import jax\n from jax._src import core\n from jax._src import source_info_util\n from jax._src import api_util\n@@ -53,7 +52,8 @@ def jax_setattr(obj: Any, attr: str, val: PyTree) -> None:\n     return t.process_setattr(obj, attr, val)\n \n def jax_appendattr(obj: Any, attr: str, val: Array) -> None:\n-  return jax_extendattr(obj, attr, jax.numpy.expand_dims(val, 0))\n+  import jax.numpy as jnp  # pytype: disable=import-error\n+  return jax_extendattr(obj, attr, jnp.expand_dims(val, 0))\n \n def jax_extendattr(obj: Any, attr: str, val: Array) -> None:\n   with core.take_current_trace() as t:\n@@ -68,12 +68,13 @@ def _setattr_impl(_, obj, attr, val):\n core.EvalTrace.process_setattr = _setattr_impl\n \n def _extendattr_impl(_, obj, attr, val):\n+  import jax.numpy as jnp  # pytype: disable=import-error\n   cur = getattr(obj, attr, dne_sentinel)\n   if cur is dne_sentinel:\n     new = val\n   else:\n     _check_append_type_agreement(obj, attr, core.typeof(cur), core.typeof(val))\n-    new = jax.numpy.concatenate([cur, val])\n+    new = jnp.concatenate([cur, val])\n   setattr(obj, attr, new)\n core.EvalTrace.process_extendattr = _extendattr_impl\n \n@@ -122,6 +123,7 @@ def _setattr_staging(trace, obj, attr, val):\n pe.DynamicJaxprTrace.process_setattr = _setattr_staging\n \n def _extendattr_staging(trace, obj, attr, val):\n+  import jax.numpy as jnp  # pytype: disable=import-error\n   frame = trace.frame\n \n   if (obj, attr, ReadWrite) in frame.attrs_tracked:\n@@ -138,7 +140,7 @@ def _extendattr_staging(trace, obj, attr, val):\n   else:\n     assert init_val is not dne_sentinel\n     with core.set_current_trace(trace):\n-      tracer = jax.numpy.concatenate([init_val, val])\n+      tracer = jnp.concatenate([init_val, val])\n   setattr(obj, attr, tracer)\n pe.DynamicJaxprTrace.process_extendattr = _extendattr_staging\n \n\n```"
        ],
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Make the Blackwell matmul kernel persistent\n\nThis helps with performance a bit (we only allocate and deallocate TMEM once in each\nSM), and opens up the opportunity for better overlapping of the epilogue.\n\nPiperOrigin-RevId: 764168230",
        "output": [
            "```diff\nCommit: 6004c7b6daf68650bec2595ab86bc1efc2eb8845\nDate: 2025-05-28T09:34:54Z\nURL: https://github.com/jax-ml/jax/commit/6004c7b6daf68650bec2595ab86bc1efc2eb8845\nFiles changed: 1\nAdditions: +48, Deletions: -20\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex 929c7c498986..8bf8ca557496 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -15,6 +15,7 @@\n \"\"\"Matmul kernel for Blackwell.\"\"\"\n \n import itertools\n+import math\n \n import jax\n from jax._src.interpreters import mlir\n@@ -81,6 +82,9 @@ def build_kernel(\n   if (m // block_tile_m) % grid_tile_m:\n     raise ValueError(f\"{m=} // {tile_m=} must be divisible by {grid_tile_m=}\")\n \n+  # We intend this to be iterated in column-major order.\n+  logical_grid = (grid_tile_m, n // tile_n, m // (block_tile_m * grid_tile_m))\n+\n   def kernel(ctx, a, b, d, smem):\n     ((a_smem, b_smem), d_smem), barriers, mma_done_barrier, acc = smem\n     (ab_full_barriers, ab_empty_barriers) = barriers\n@@ -94,17 +98,25 @@ def kernel(ctx, a, b, d, smem):\n         arith.CmpIPredicate.eq, ctx.cluster_idx(gpu.Dimension.x), c(0, index)\n     )\n \n-    # This function executes the kernel for a single output tile.\n-    def compute_output(block_m_start, n_start):\n-      \"\"\"Compute and store a single output tile.\"\"\"\n+    def compute_output(block_m_start, n_start, call_counter):\n+      \"\"\"Compute and store a single output tile.\n+\n+      call_counter should be 0 the first time this function is called and\n+      incremented by 1 before each subsequent call.\n+      \"\"\"\n       # All blocks in the cluster share the same m_start -- align it!\n       m_start = arith.muli(arith.divui(block_m_start, c(tile_m, index)), c(tile_m, index))\n       with mgpu.when(is_leader_of(TMA_WARP)):\n         @mgpu.fori(c(k_loop_iter, index), None)\n         def _tma_body(ki, _):\n           slot = arith.remui(ki, c(max_concurrent_steps, index))\n-          # TODO(apaszke): Use a predicate instead of a conditional.\n-          with mgpu.when(arith.cmpi(arith.CmpIPredicate.uge, ki, c(max_concurrent_steps, index))):\n+          isnt_warmup = arith.cmpi(\n+              arith.CmpIPredicate.uge, ki, c(max_concurrent_steps, index)\n+          )\n+          isnt_first_call = arith.cmpi(\n+              arith.CmpIPredicate.ne, call_counter, c(0, index)\n+          )\n+          with mgpu.when(arith.ori(isnt_first_call, isnt_warmup)):\n             ab_empty_barriers[slot].wait()\n           full_barrier = ab_full_barriers[slot]\n           with mgpu.when(is_leader_block):\n@@ -150,15 +162,12 @@ def _mma_body(ki, accumulate):\n               collective=collective,\n           )\n           accumulate = arith.constant(i1, 1)\n+          tcgen05.commit_arrive(ab_empty_barriers[slot], collective=collective, ctx=ctx)\n           is_last_iter = arith.cmpi(\n               arith.CmpIPredicate.eq, ki, c(k_loop_iter - 1, index)\n           )\n-          barrier_ptr = arith.select(\n-              is_last_iter,\n-              mma_done_barrier.get_ptr(),\n-              ab_empty_barriers[slot].get_ptr(),\n-          )\n-          tcgen05.commit_arrive(barrier_ptr, collective=collective, ctx=ctx)\n+          with mgpu.when(is_last_iter):\n+            tcgen05.commit_arrive(mma_done_barrier, collective=collective, ctx=ctx)\n           return accumulate\n \n       gpu.barrier()\n@@ -176,15 +185,33 @@ def _mma_body(ki, accumulate):\n       )\n       ctx.await_async_copy(0)\n \n-    m_idx = arith.addi(\n-        gpu.block_id(gpu.Dimension.x),\n-        arith.muli(gpu.block_id(gpu.Dimension.z), c(grid_tile_m, index)),\n+    # We statically assign the tiles to SMs.\n+    logical_grid_size = math.prod(logical_grid)\n+    sm_id = gpu.block_id(gpu.Dimension.x)\n+    extra_step = arith.cmpi(\n+        arith.CmpIPredicate.slt, sm_id, c(logical_grid_size % num_sms, index)\n+    )  # Some SMs do an extra step when grid size isn't divisible by SM count.\n+    mn_steps = arith.addi(\n+        mgpu.c(logical_grid_size // num_sms, index),\n+        arith.index_castui(index, extra_step),\n     )\n-    n_idx = gpu.block_id(gpu.Dimension.y)\n-    block_m_start = arith.muli(m_idx, c(block_tile_m, index))\n-    n_start = arith.muli(n_idx, c(tile_n,index))\n-    # This is not a persistent kernel, so we only process one tile.\n-    compute_output(block_m_start, n_start)\n+\n+    @mgpu.fori(mn_steps, None)\n+    def _mn_loop(local_mn_step, _):\n+      global_mn_step = arith.addi(\n+          sm_id, arith.muli(local_mn_step, mgpu.c(num_sms, index))\n+      )\n+      logical_idxs = []\n+      for dim_size in logical_grid:\n+        logical_idxs.append(arith.remui(global_mn_step, mgpu.c(dim_size, index)))\n+        global_mn_step = arith.divui(global_mn_step, mgpu.c(dim_size, index))\n+      lx, ly, lz = logical_idxs\n+      m_idx = arith.addi(lx, arith.muli(lz, c(grid_tile_m, index)))\n+      n_idx = ly\n+\n+      block_m_start = arith.muli(m_idx, c(block_tile_m, index))\n+      n_start = arith.muli(n_idx, c(tile_n,index))\n+      compute_output(block_m_start, n_start, local_mn_step)\n \n   compute_buffers = (\n     jax.ShapeDtypeStruct(\n@@ -204,9 +231,10 @@ def _mma_body(ki, accumulate):\n       mgpu.Barrier(arrival_count=1),\n       mgpu.TMEM((128, tile_n), jnp.float32, collective=collective),\n   )\n+  num_sms = 148\n   return mgpu.as_gpu_kernel(\n       kernel,\n-      (grid_tile_m, n // tile_n, m // (block_tile_m * grid_tile_m)),\n+      (num_sms, 1, 1),  # This is a persistent kernel.\n       (128, 1, 1),\n       (\n           jax.ShapeDtypeStruct((m, k), dtype),\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] Fix `broadcast_in_dim` fuser eval rule.\n\nPiperOrigin-RevId: 764019664",
        "output": [
            "```diff\nCommit: 69c431759123b4db3de9578837d4dbe0b58db74b\nDate: 2025-05-28T00:53:15Z\nURL: https://github.com/jax-ml/jax/commit/69c431759123b4db3de9578837d4dbe0b58db74b\nFiles changed: 2\nAdditions: +77, Deletions: -34\ndiff --git a/jax/_src/pallas/fuser/block_spec.py b/jax/_src/pallas/fuser/block_spec.py\nindex 3d4df549949c..3e9ff497bf1e 100644\n--- a/jax/_src/pallas/fuser/block_spec.py\n+++ b/jax/_src/pallas/fuser/block_spec.py\n@@ -1364,14 +1364,15 @@ def _broadcast_in_dim_usage_rule(ctx, used_out: set[Usage], **params):\n def _broadcast_in_dim_eval_rule(\n     eval_ctx: KernelEvalContext, x, broadcast_dimensions, **params\n ):\n-  if not eval_ctx.avals_in[0].shape:  # pytype: disable=attribute-error\n-    # Scalar -> Array broadcast\n-    block_spec = eval_ctx.out_block_specs[0]\n-    shape = tuple(\n-        _block_size(s) for s in block_spec.block_shape if s is not None\n-    )\n-    return jax.lax.broadcast_in_dim(x, broadcast_dimensions=(), shape=shape)\n-  return x\n+  del params  # Unused.\n+  shape = tuple(map(_block_size, eval_ctx.out_block_specs[0].block_shape))\n+  dims = tuple(\n+      d - sum(s is None for s in shape[:d])\n+      for d in broadcast_dimensions\n+      if shape[d] is not None\n+  )\n+  shape = tuple(s for s in shape if s is not None)\n+  return jax.lax.broadcast_in_dim(x, broadcast_dimensions=dims, shape=shape)\n \n \n @register_pull_block_spec_rule(lax.broadcast_in_dim_p)\n@@ -1385,15 +1386,20 @@ def _broadcast_in_dim_pull_rule(\n ):\n   del shape, sharding\n \n-  if not ctx.avals_in[0].shape:  # pytype: disable=attribute-error\n+  shape = ctx.avals_in[0].shape  # pytype: disable=attribute-error\n+  if not shape:\n     return [pallas_core.no_block_spec]\n \n   def new_index_map(*args):\n     idx = block_spec.index_map(*args)\n-    return tuple(idx[i] for i in broadcast_dimensions)\n+    return tuple(\n+        0 if (d == 1) else idx[i]\n+        for i, d in zip(broadcast_dimensions, shape, strict=True)\n+    )\n \n   new_block_shape = tuple(\n-      block_spec.block_shape[i] for i in broadcast_dimensions\n+      b if ((b := block_spec.block_shape[i]) is None) or (d != 1) else 1\n+      for i, d in zip(broadcast_dimensions, shape, strict=True)\n   )\n   return [pallas_core.BlockSpec(new_block_shape, new_index_map)]\n \ndiff --git a/tests/pallas/fuser_block_spec_test.py b/tests/pallas/fuser_block_spec_test.py\nindex f7e70ec1d708..5c0ef0352b1c 100644\n--- a/tests/pallas/fuser_block_spec_test.py\n+++ b/tests/pallas/fuser_block_spec_test.py\n@@ -653,9 +653,12 @@ def f():\n         kernel_fn((0, 0, 3, 0), scalar_prefetch_values, ()), x\n     )\n \n-  def test_broadcast_array(self):\n+  @parameterized.parameters(\n+      (False, False), (False, True), (True, False), (True, True)\n+  )\n+  def test_broadcast_array(self, bcast0, bcast1):\n \n-    x = jnp.ones((512, 512))\n+    x = jnp.ones((1 if bcast0 else 512, 1 if bcast1 else 512))\n \n     def f():\n       return jax.lax.broadcast_in_dim(x, (2, 2, 512, 512), (2, 3))\n@@ -664,9 +667,8 @@ def f():\n     self.assertLen(new_values, 1)\n     self.assertEmpty(scalar_prefetch_values)\n \n-    block_spec = pl.BlockSpec(\n-        (None, 1, 128, 128), lambda i, j, k, l: (i, j, k, l)\n-    )\n+    block_shape = (None, 1, 128, 128)\n+    block_spec = pl.BlockSpec(block_shape, lambda i, j, k, l: (i, j, k, l))\n     kernel_fn, (value_block_specs,), _ = block_spec_lib.pull_block_spec(\n         f2,\n         block_spec,\n@@ -674,27 +676,62 @@ def f():\n         scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n     )(new_values)\n     self.assertLen(value_block_specs, 1)\n-    x_block_spec = value_block_specs[0]\n-    self.assertEqual(x_block_spec.index_map(0, 0, 1, 2), (1, 2))\n-    self.assertEqual(x_block_spec.index_map(1, 2, 3, 3), (3, 3))\n-\n-    x = jnp.full((128, 128), fill_value=1.2345, dtype=jnp.float32)\n-    np.testing.assert_array_equal(\n-        kernel_fn((0, 0, 0, 0), scalar_prefetch_values, (x,)), x\n-    )\n-    np.testing.assert_array_equal(\n-        kernel_fn((1, 1, 0, 0), scalar_prefetch_values, (x,)), x\n-    )\n-    np.testing.assert_array_equal(\n-        kernel_fn((0, 0, 0, 1), scalar_prefetch_values, (x,)), x\n-    )\n-    np.testing.assert_array_equal(\n-        kernel_fn((0, 0, 1, 0), scalar_prefetch_values, (x,)), x\n+    x_index_map = value_block_specs[0].index_map\n+    self.assertEqual(\n+        x_index_map(0, 0, 1, 2), (0 if bcast0 else 1, 0 if bcast1 else 2)\n     )\n-    np.testing.assert_array_equal(\n-        kernel_fn((0, 0, 3, 0), scalar_prefetch_values, (x,)), x\n+    self.assertEqual(\n+        x_index_map(1, 2, 3, 3), (0 if bcast0 else 3, 0 if bcast1 else 3)\n     )\n \n+    block_shape = (1 if bcast0 else 128, 1 if bcast1 else 128)\n+    self.assertEqual(block_shape, value_block_specs[0].block_shape)\n+    x = jnp.full(block_shape, fill_value=1.2345, dtype=jnp.float32)\n+    y = jax.lax.broadcast_in_dim(x, (1, 128, 128), (1, 2))\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 0, 0), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((1, 1, 0, 0), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 0, 1), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 1, 0), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 3, 0), (), (x,)), y)\n+\n+  @parameterized.parameters(0, 1, 2, 3)\n+  def test_broadcast_1d_array(self, bcast_dim):\n+    full_shape = (2, 2, 512, 512)\n+    x = jnp.ones((full_shape[bcast_dim],))\n+\n+    def f():\n+      return jax.lax.broadcast_in_dim(x, full_shape, (bcast_dim,))\n+\n+    f2, new_values, scalar_prefetch_values = block_spec_lib.get_fusion_values(f)\n+    self.assertLen(new_values, 1)\n+    self.assertEmpty(scalar_prefetch_values)\n+\n+    block_shape = (None, 1, 128, 128)\n+    block_spec = pl.BlockSpec(block_shape, lambda i, j, k, l: (i, j, k, l))\n+    kernel_fn, (value_block_specs,), _ = block_spec_lib.pull_block_spec(\n+        f2,\n+        block_spec,\n+        grid=(2, 2, 4, 4),\n+        scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+    )(new_values)\n+    self.assertLen(value_block_specs, 1)\n+    x_index_map = value_block_specs[0].index_map\n+    self.assertEqual(x_index_map(0, 0, 1, 2), ((0, 0, 1, 2)[bcast_dim],))\n+    self.assertEqual(x_index_map(1, 2, 3, 3), ((1, 2, 3, 3)[bcast_dim],))\n+\n+    if block_shape[bcast_dim] is None:\n+      x = jnp.ones(())\n+      y = jax.lax.broadcast_in_dim(x, (1, 128, 128), ())\n+    else:\n+      x = jnp.arange(block_shape[bcast_dim] or 1, dtype=jnp.float32)\n+      y = jax.lax.broadcast_in_dim(x, (1, 128, 128), (bcast_dim - 1,))\n+\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 0, 0), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((1, 1, 0, 0), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 0, 1), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 1, 0), (), (x,)), y)\n+    np.testing.assert_array_equal(kernel_fn((0, 0, 3, 0), (), (x,)), y)\n+\n   def test_element_indexing(self):\n \n     x = np.zeros((512, 512), dtype=np.float32)\n\n```"
        ],
        "from_id": [
            "chr1sj0nes",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reshape ragged_all_to_all to correct shape before concatenating\n\nPreviously the result of vmapped RA2A was concatenating a flattened result.\n\nPiperOrigin-RevId: 763958632",
        "output": [
            "```diff\nCommit: 669f08a8276bda81fb851a2242158802fcbc5f47\nDate: 2025-05-27T21:54:40Z\nURL: https://github.com/jax-ml/jax/commit/669f08a8276bda81fb851a2242158802fcbc5f47\nFiles changed: 2\nAdditions: +64, Deletions: -4\ndiff --git a/jax/_src/lax/parallel.py b/jax/_src/lax/parallel.py\nindex c5f8d3988144..a5bb7222143d 100644\n--- a/jax/_src/lax/parallel.py\n+++ b/jax/_src/lax/parallel.py\n@@ -1448,13 +1448,15 @@ def _ragged_all_to_all_batched_collective(axis_data, vals_in, dims_in,\n   sliced_results = []\n   for i in range(operand.shape[operand_dim]):\n     sliced_operand = slicing.slice_in_dim(operand, start_index=i, limit_index=i+1, axis=operand_dim).flatten()\n-    sliced_output = slicing.slice_in_dim(output, start_index=i, limit_index=i+1, axis=output_dim).flatten()\n+    sliced_output = slicing.slice_in_dim(output, start_index=i, limit_index=i+1, axis=output_dim)\n+    sliced_output_shape = sliced_output.shape\n+    sliced_output = sliced_output.flatten()\n     sliced_input_offsets = slicing.slice_in_dim(input_offsets, start_index=i, limit_index=i+1, axis=input_offsets_dim).flatten()\n     sliced_send_sizes = slicing.slice_in_dim(send_sizes, start_index=i, limit_index=i+1, axis=send_sizes_dim).flatten()\n     sliced_output_offsets = slicing.slice_in_dim(output_offsets, start_index=i, limit_index=i+1, axis=output_offsets_dim).flatten()\n     sliced_recv_sizes = slicing.slice_in_dim(recv_sizes, start_index=i, limit_index=i+1, axis=recv_sizes_dim).flatten()\n     sliced_result = ragged_all_to_all(sliced_operand, sliced_output, sliced_input_offsets, sliced_send_sizes, sliced_output_offsets, sliced_recv_sizes, axis_name=axis_name, axis_index_groups=axis_index_groups)\n-    sliced_result = lax.expand_dims(sliced_result, dimensions=(output_dim,))\n+    sliced_result = lax.expand_dims(sliced_result.reshape(sliced_output_shape), dimensions=(output_dim,))\n     sliced_results.append(sliced_result)\n \n   concat_result = lax.concatenate(sliced_results, dimension=output_dim)\ndiff --git a/tests/ragged_collective_test.py b/tests/ragged_collective_test.py\nindex 1734f67ff063..8b94b862419c 100644\n--- a/tests/ragged_collective_test.py\n+++ b/tests/ragged_collective_test.py\n@@ -382,6 +382,66 @@ def fwd(\n         c, jnp.array([[0, 0, 1, 0], [0, 2, 3, 4]], dtype=jnp.int32)\n     )\n \n+  def test_ragged_all_to_all_vmap_multi_dim_operand(self):\n+    device_type = jax.devices()[0].platform\n+    if device_type == 'tpu' and jtu.get_tpu_version() < 4:\n+      raise unittest.SkipTest(\n+          'UNSUPPORTED: HLO opcode `ragged-all-to-all` is not supported by TPU'\n+          f' v{jtu.get_tpu_version()}'\n+      )\n+\n+    axis_name = 'x'\n+    mesh_axes = dict(x=2)\n+    mesh = jtu.create_mesh(tuple(mesh_axes.values()), tuple(mesh_axes.keys()))\n+    data_sharding = P(axis_name, None, None)\n+    operand_data = jnp.zeros((2, 2, 3), dtype=jnp.int32)\n+    output_data = jnp.zeros((2, 2, 4), dtype=jnp.int32)\n+    input_offsets_data = jnp.zeros((2, 2, 2), dtype=jnp.int32)\n+    send_sizes_data = jnp.zeros((2, 2, 2), dtype=jnp.int32)\n+    output_offsets_data = jnp.zeros((2, 2, 2), dtype=jnp.int32)\n+    recv_sizes_data = jnp.zeros((2, 2, 2), dtype=jnp.int32)\n+\n+    operand = jax.device_put(operand_data, jax.sharding.NamedSharding(mesh, data_sharding))\n+    output = jax.device_put(output_data, jax.sharding.NamedSharding(mesh, data_sharding))\n+    input_offsets = jax.device_put(input_offsets_data, jax.sharding.NamedSharding(mesh, data_sharding))\n+    send_sizes = jax.device_put(send_sizes_data, jax.sharding.NamedSharding(mesh, data_sharding))\n+    output_offsets = jax.device_put(output_offsets_data, jax.sharding.NamedSharding(mesh, data_sharding))\n+    recv_sizes = jax.device_put(recv_sizes_data, jax.sharding.NamedSharding(mesh, data_sharding))\n+\n+    @partial(\n+        shard_map,\n+        mesh=mesh,\n+        in_specs=(\n+            P(axis_name, None),\n+            P(axis_name, None),\n+            P(axis_name, None),\n+            P(axis_name, None),\n+            P(axis_name, None),\n+            P(axis_name, None),\n+        ),\n+        out_specs=P(axis_name),\n+        check_vma=False,\n+    )\n+    def fwd(\n+        operand, output, input_offsets, send_sizes, output_offsets, recv_sizes\n+    ):\n+      return lax.ragged_all_to_all(\n+          operand=operand.reshape(operand.shape[1:]),\n+          output=output.reshape(output.shape[1:]),\n+          input_offsets=input_offsets.reshape(input_offsets.shape[1:]),\n+          send_sizes=send_sizes.reshape(send_sizes.shape[1:]),\n+          output_offsets=output_offsets.reshape(output_offsets.shape[1:]),\n+          recv_sizes=recv_sizes.reshape(recv_sizes.shape[1:]),\n+          axis_name=axis_name,\n+      )\n+\n+    res = vmap(\n+        fwd, in_axes=0, out_axes=0, axis_name='x'\n+    )(\n+        operand, output, input_offsets, send_sizes, output_offsets, recv_sizes\n+    )\n+    self.assertEqual(res.shape, (2, 2, 4))\n+\n   @parameterized.named_parameters(\n     dict(\n         testcase_name='_batch_0_data_shard_axis_0_input_0',\n@@ -510,8 +570,6 @@ def fwd(\n         fwd, in_axes=vmap_batch_axis, out_axes=0, axis_name=vmap_axis_name\n     )(\n         operand, output, input_offsets, send_sizes, output_offsets, recv_sizes\n-    ).reshape(\n-        (2, 2, 4)\n     )\n     expected_res = jnp.array([[[1, 4, 0, 0], [2, 3, 5, 0]],\n                               [[1, 4, 0, 0], [2, 3, 5, 0]]], dtype=jnp.int32)\n\n```"
        ],
        "from_id": [
            "ghpvnist",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29043 from hawkinsp:locks\n\nPiperOrigin-RevId: 763950695",
        "output": [
            "```diff\nCommit: 0caeb982a4dec6b98096a21cdf218dece9f86bd6\nDate: 2025-05-27T21:33:54Z\nURL: https://github.com/jax-ml/jax/commit/0caeb982a4dec6b98096a21cdf218dece9f86bd6\nFiles changed: 6\nAdditions: +183, Deletions: -173\ndiff --git a/build/requirements.in b/build/requirements.in\nindex c5ce2ea279bd..c1be7a250bff 100644\n--- a/build/requirements.in\n+++ b/build/requirements.in\n@@ -16,11 +16,11 @@ wheel\n # JAX's own libraries. We include these in the requirements so you can\n # bazel test without building jaxlib and without manually updating the\n # the requirements files.\n-jaxlib\n+jaxlib==0.6.1\n \n # The with-cuda extra also includes NVIDIA's pip packages.\n-jax-cuda12-plugin[with-cuda] ; sys_platform == \"linux\"\n-jax-cuda12-pjrt ; sys_platform == \"linux\"\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\"\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\"\n \n # TPU dependencies\n libtpu ; sys_platform == \"linux\" and platform_machine == \"x86_64\"\ndiff --git a/build/requirements_lock_3_10.txt b/build/requirements_lock_3_10.txt\nindex a4c6b1bf2b77..832c801ced63 100644\n--- a/build/requirements_lock_3_10.txt\n+++ b/build/requirements_lock_3_10.txt\n@@ -160,43 +160,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.5 \\\n     --hash=sha256:00bd361b903dc4bbf4eb165f24d1acbee754fce22ded24c3d56eec268658a5cf \\\n@@ -494,7 +494,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549\ndiff --git a/build/requirements_lock_3_11.txt b/build/requirements_lock_3_11.txt\nindex 0633e733414b..de3c35ed3c02 100644\n--- a/build/requirements_lock_3_11.txt\n+++ b/build/requirements_lock_3_11.txt\n@@ -154,43 +154,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.5 \\\n     --hash=sha256:00bd361b903dc4bbf4eb165f24d1acbee754fce22ded24c3d56eec268658a5cf \\\n@@ -489,7 +489,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549\ndiff --git a/build/requirements_lock_3_12.txt b/build/requirements_lock_3_12.txt\nindex 1ab77a6ec36e..04c6990da696 100644\n--- a/build/requirements_lock_3_12.txt\n+++ b/build/requirements_lock_3_12.txt\n@@ -154,43 +154,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.5 \\\n     --hash=sha256:00bd361b903dc4bbf4eb165f24d1acbee754fce22ded24c3d56eec268658a5cf \\\n@@ -489,7 +489,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549\ndiff --git a/build/requirements_lock_3_13.txt b/build/requirements_lock_3_13.txt\nindex c20068b732e6..965cb3bc9672 100644\n--- a/build/requirements_lock_3_13.txt\n+++ b/build/requirements_lock_3_13.txt\n@@ -181,43 +181,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.7 \\\n     --hash=sha256:073a36c8273647592ea332e816e75ef8da5c303236ec0167196793eb1e34657a \\\n@@ -544,7 +544,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.4.0 \\\n     --hash=sha256:69bb92469f86a1565195ece4ac0323943e83477171b91d24c35afe028a90d7cd \\\n     --hash=sha256:96ca72f1b886d148241348783498194c577fa30a8faac108586b14f1ba4473ac\ndiff --git a/build/requirements_lock_3_13_ft.txt b/build/requirements_lock_3_13_ft.txt\nindex 3795343df0cb..e7d111c3b3e9 100644\n--- a/build/requirements_lock_3_13_ft.txt\n+++ b/build/requirements_lock_3_13_ft.txt\n@@ -172,43 +172,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.8 \\\n     --hash=sha256:01c3d31902c7db5fb6182832713d3b4122ad9317c2c5877d0539227d96bb2e50 \\\n@@ -495,7 +495,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.4.0 \\\n     --hash=sha256:69bb92469f86a1565195ece4ac0323943e83477171b91d24c35afe028a90d7cd \\\n     --hash=sha256:96ca72f1b886d148241348783498194c577fa30a8faac108586b14f1ba4473ac\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update lock files for jaxlib 0.6.1",
        "output": [
            "```diff\nCommit: c09b1bb763d846a694f919e5a5adda9575ce66d6\nDate: 2025-05-27T20:44:50Z\nURL: https://github.com/jax-ml/jax/commit/c09b1bb763d846a694f919e5a5adda9575ce66d6\nFiles changed: 6\nAdditions: +183, Deletions: -173\ndiff --git a/build/requirements.in b/build/requirements.in\nindex c5ce2ea279bd..c1be7a250bff 100644\n--- a/build/requirements.in\n+++ b/build/requirements.in\n@@ -16,11 +16,11 @@ wheel\n # JAX's own libraries. We include these in the requirements so you can\n # bazel test without building jaxlib and without manually updating the\n # the requirements files.\n-jaxlib\n+jaxlib==0.6.1\n \n # The with-cuda extra also includes NVIDIA's pip packages.\n-jax-cuda12-plugin[with-cuda] ; sys_platform == \"linux\"\n-jax-cuda12-pjrt ; sys_platform == \"linux\"\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\"\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\"\n \n # TPU dependencies\n libtpu ; sys_platform == \"linux\" and platform_machine == \"x86_64\"\ndiff --git a/build/requirements_lock_3_10.txt b/build/requirements_lock_3_10.txt\nindex a4c6b1bf2b77..832c801ced63 100644\n--- a/build/requirements_lock_3_10.txt\n+++ b/build/requirements_lock_3_10.txt\n@@ -160,43 +160,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.5 \\\n     --hash=sha256:00bd361b903dc4bbf4eb165f24d1acbee754fce22ded24c3d56eec268658a5cf \\\n@@ -494,7 +494,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549\ndiff --git a/build/requirements_lock_3_11.txt b/build/requirements_lock_3_11.txt\nindex 0633e733414b..de3c35ed3c02 100644\n--- a/build/requirements_lock_3_11.txt\n+++ b/build/requirements_lock_3_11.txt\n@@ -154,43 +154,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.5 \\\n     --hash=sha256:00bd361b903dc4bbf4eb165f24d1acbee754fce22ded24c3d56eec268658a5cf \\\n@@ -489,7 +489,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549\ndiff --git a/build/requirements_lock_3_12.txt b/build/requirements_lock_3_12.txt\nindex 1ab77a6ec36e..04c6990da696 100644\n--- a/build/requirements_lock_3_12.txt\n+++ b/build/requirements_lock_3_12.txt\n@@ -154,43 +154,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.5 \\\n     --hash=sha256:00bd361b903dc4bbf4eb165f24d1acbee754fce22ded24c3d56eec268658a5cf \\\n@@ -489,7 +489,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.3.0 \\\n     --hash=sha256:2455e59e3947d3c275477df7f5205b30635e266fe6dc300e3d9f9646bfcea147 \\\n     --hash=sha256:59f6475f77bbc37dcf7cd748519c0ec60722e91e63ca114e68821c0c54a46549\ndiff --git a/build/requirements_lock_3_13.txt b/build/requirements_lock_3_13.txt\nindex c20068b732e6..965cb3bc9672 100644\n--- a/build/requirements_lock_3_13.txt\n+++ b/build/requirements_lock_3_13.txt\n@@ -181,43 +181,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.7 \\\n     --hash=sha256:073a36c8273647592ea332e816e75ef8da5c303236ec0167196793eb1e34657a \\\n@@ -544,7 +544,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.4.0 \\\n     --hash=sha256:69bb92469f86a1565195ece4ac0323943e83477171b91d24c35afe028a90d7cd \\\n     --hash=sha256:96ca72f1b886d148241348783498194c577fa30a8faac108586b14f1ba4473ac\ndiff --git a/build/requirements_lock_3_13_ft.txt b/build/requirements_lock_3_13_ft.txt\nindex 3795343df0cb..e7d111c3b3e9 100644\n--- a/build/requirements_lock_3_13_ft.txt\n+++ b/build/requirements_lock_3_13_ft.txt\n@@ -172,43 +172,43 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n-    --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n+jax-cuda12-pjrt==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:4c97d10a5a9ac09fa001568cac3b715014e8dbbc2cd86763753f58e5a730c333 \\\n+    --hash=sha256:967076cfb6f2e33959e7376663599aa0c11cc0ede8f2f51a206da0a1d422c6bb\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n-    --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n-    --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n-    --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n-    --hash=sha256:581f9468c6394f572a9ef0b25cf28b4a8d099abc26ee5da981dd5b680d0a00df \\\n-    --hash=sha256:7cd1b488a54a3089e89588ccaf677089952c82529e7d0403e0b050199e525418 \\\n-    --hash=sha256:a2a3af5f98880d86f8d246abb46a552e5a2ef49d767bfc4a74c8c357752007c6 \\\n-    --hash=sha256:a342f2ce7c4b1f59d403f665a35a86b8650253bb25de34647fb225c45ceb0a04 \\\n-    --hash=sha256:a700e171823ce255102002e40c94788fa868f216257b7d3f0568d09fe75c107b \\\n-    --hash=sha256:e70eb4f084696c3e3be12b5e909ef1205c9f56efe3dcecf2621bd9b5ab5954d5 \\\n-    --hash=sha256:e96f3dd4a942516ae878c9f697e6aefed78e148f09018ca73ee28b23426a7d8a\n+jax-cuda12-plugin[with-cuda]==0.6.1 ; sys_platform == \"linux\" \\\n+    --hash=sha256:1885f15be38faecccfbf24b184ffdc1d0d363717eadd2534d5759c0d3d0af523 \\\n+    --hash=sha256:1fbf8d4b42455443a089afd1a88fb106a51ba1075fc6884b339dc96571c5b617 \\\n+    --hash=sha256:2a3578dc0b7d44cc1b0233b0fe7ad764265381095d7eac64c56bd01b34be76f2 \\\n+    --hash=sha256:425ccf13cbdd4678b1109f843988157a59e4f4d9bc298205acb16df048a31c38 \\\n+    --hash=sha256:b77804e0e4d923ad39909095ff7c1b723eac6f3ee5f9ffcb80597ba867b572b8 \\\n+    --hash=sha256:b8bff7a5fc7a416717e1d59da9728a1f7aad07a8b65afa0f86962d43ed0e654f \\\n+    --hash=sha256:ba09bad8d5c9c33326e6374b0669dc325e7a4fb0d57798df3dcd560693c877dc \\\n+    --hash=sha256:bb64a0c801f93a718a654dfc69742f2fd60a26074312204ebdf4fe403d9e2bc4 \\\n+    --hash=sha256:d9c2be8ebb4ef6ae11dd7345ae864ac49d00bd455d06fff925a5d1eb266b02f1 \\\n+    --hash=sha256:da9f7dc9243ec28e03c0e3a39852b4246fa9cfc3dcd51e4286d82097f5c695c0\n     # via -r build/requirements.in\n-jaxlib==0.6.0 \\\n-    --hash=sha256:1597e972ff0e99abbb5bd376167b0b1d565554da54de94f12a5f5c574082f9c6 \\\n-    --hash=sha256:189729639762050c1780b050e98ff620480b1ea32bf167533e000a5cf4c5738e \\\n-    --hash=sha256:2536fa93ec148d5016da8b2077ba66325b0d86aae2289a61c126877f042b3d1c \\\n-    --hash=sha256:541a418b98b28df5bd3a1e93c62b2d3f64d44b0c70b7b608f7fe2b4aa452b2af \\\n-    --hash=sha256:554512c1445ee69c566ef097c3dbdd09e9d9908523eef222c589a559f4220370 \\\n-    --hash=sha256:63106d4e38aec5e4285c8de85e8cddcbb40084c077d07ac03778d3a2bcfa3aae \\\n-    --hash=sha256:64a82f8eb40fdb7ba1d46ef907300d42e4f98cbda9602a2ed8e70db1a9ac4a60 \\\n-    --hash=sha256:7e3ce2ef0edc9b48b36e2704c36181f1ece7a12ac114df753db4286ea2c6e8b8 \\\n-    --hash=sha256:9494cf32c5894669d785c9e2311d2ac0794b29a1a8e9822593211ab43517e657 \\\n-    --hash=sha256:a4d4254c713388887a321379d3c5b1a20213a8dcdc903faf15139ba81e3ecd61 \\\n-    --hash=sha256:b6d85b8d1fd79248b04503517201e72fcbcd3980cf791d37e814709ea50a3c82 \\\n-    --hash=sha256:bed45525e3bb5ec08630bfd207c09af9d62e9ff13f5f07c2ee2cfd8ed8411ba1 \\\n-    --hash=sha256:c0ae959899802e1329cc8ec5a2b4d4be9a076b5beb2052eb49ba37514e623ebc \\\n-    --hash=sha256:c4e97934cbaf5172343aa5ae8ef0c58462ce26154dfda754202b3034160cac7b \\\n-    --hash=sha256:d0fb122dc7830ca2a5ca3c874a087363a00532b644509c219c3bfd1d54515e8d \\\n-    --hash=sha256:d7ab9eaa6e4db3dc6bfba8a061b660147bcd5a1b9d777fde3d729c794f274ab9 \\\n-    --hash=sha256:ec61ca368d0708e1a7543eae620823025bfd405fa9ab331302f209833e970107 \\\n-    --hash=sha256:ef163cf07de00bc5690169e97fafaadc378f1c381f0287e8a473e78ab5bab1b5\n+jaxlib==0.6.1 \\\n+    --hash=sha256:02bac5153389f01616516a9fd1dcd6038d23ee50681dac14e4ddbc43ccb3133a \\\n+    --hash=sha256:11fcc4b1c741a1e0057f2ffa77d5a82bfe7ee97c3864ed88df67493e789b9173 \\\n+    --hash=sha256:2168217ec37bf951ca33377d3e0953178ba5cade95f194211d9ab2d53dcd2201 \\\n+    --hash=sha256:277cc7e9d657d0893a559261277b3eae916ad7fa73e300a629261fb537dca0f1 \\\n+    --hash=sha256:3301addee156f55d1f8079f80b314d89b80094740b7d64e5ec6e7ef2e1febbd7 \\\n+    --hash=sha256:5a90ee7c59b2c00773026fbf918269c7a8676a6a81a34a03af919f7d7bdce9a8 \\\n+    --hash=sha256:5e4f49113a527bcbac70c9e7074e95d8abfa35c3d67c2fed01f77a7abfd317aa \\\n+    --hash=sha256:76d6f65f3153ffb70e20a76b915d4431823cf70a786d86ba1b76a9c5bf66a0a4 \\\n+    --hash=sha256:7ae5815ada71b69532ce443a11160a3ed25c67e82a294a0d89af9d4d27429434 \\\n+    --hash=sha256:8106dc316eb440d07b9d4628a0c8e2acf76da5606742c9f5c33104aaa77b0ac2 \\\n+    --hash=sha256:acfe91eb44c29dbbd1f1f65f9bd66e1aef4483f57ad5e3d645129f3ec9ecde2a \\\n+    --hash=sha256:b12c8842b2dfc0770ca3785e183f7bed3fa1c2596c720591dbfbe29a05045108 \\\n+    --hash=sha256:b58c29fe747622b70946ea87823ad39202cc83da3d93a5293b432173b738a868 \\\n+    --hash=sha256:d039124468565bbf39363b1504c190e6719e6af89a7948dee256f1dee813bb94 \\\n+    --hash=sha256:d0c343c51b1052593edb603ddf58cf7f98812b2951ae6c45bd6e93e3e1f2f621 \\\n+    --hash=sha256:e14195c23eecd559a61c31027b4172e912e5a50f630320918ffdfae83090ca5a \\\n+    --hash=sha256:e734be70fe3e1fa2a31415362721189d974d10a66b0f5396c84585587d101b15 \\\n+    --hash=sha256:f4ca75d9d47a2e90099adfede0e9c926b83ef703d349b3289b8c88e861c09e5d\n     # via -r build/requirements.in\n kiwisolver==1.4.8 \\\n     --hash=sha256:01c3d31902c7db5fb6182832713d3b4122ad9317c2c5877d0539227d96bb2e50 \\\n@@ -495,7 +495,9 @@ nvidia-nvjitlink-cu12==12.8.61 \\\n nvidia-nvshmem-cu12==3.2.5 ; sys_platform == \"linux\" \\\n     --hash=sha256:2f5798d65f1a08f9878aae17cf4d3dcbfe884d1f12cf170556cd40f2be90ca96 \\\n     --hash=sha256:e076957d5cc72e51061a04f2d46f55df477be53e8a55d0d621be08f7aefe1d00\n-    # via -r build/requirements.in\n+    # via\n+    #   -r build/requirements.in\n+    #   jax-cuda12-plugin\n opt-einsum==3.4.0 \\\n     --hash=sha256:69bb92469f86a1565195ece4ac0323943e83477171b91d24c35afe028a90d7cd \\\n     --hash=sha256:96ca72f1b886d148241348783498194c577fa30a8faac108586b14f1ba4473ac\n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Merge pull request #28955 from jax-ml:prevent-partial-eval-dce-effects\n\nPiperOrigin-RevId: 763886950",
        "output": [
            "```diff\nCommit: 0f4da0c921964fd977b7c2bf96e386e84a7613dd\nDate: 2025-05-27T18:52:32Z\nURL: https://github.com/jax-ml/jax/commit/0f4da0c921964fd977b7c2bf96e386e84a7613dd\nFiles changed: 12\nAdditions: +52, Deletions: -36\ndiff --git a/jax/_src/ad_checkpoint.py b/jax/_src/ad_checkpoint.py\nindex 2d743bf06c6b..2a056d5c94f0 100644\n--- a/jax/_src/ad_checkpoint.py\n+++ b/jax/_src/ad_checkpoint.py\n@@ -578,7 +578,7 @@ def remat_partial_eval(trace: pe.JaxprTrace, *tracers: core.Tracer,\n   out_jaxpr_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(x.aval), None)\n                        for x in jaxpr_unknown.outvars]\n   new_params = dict(params, jaxpr=jaxpr_unknown, differentiated=True)\n-  recipe = pe.new_eqn_recipe(in_jaxpr_tracers, out_jaxpr_tracers, remat_p,\n+  recipe = pe.new_eqn_recipe(trace, in_jaxpr_tracers, out_jaxpr_tracers, remat_p,\n                              new_params, jaxpr_unknown.effects,\n                              source_info_util.current())\n \ndiff --git a/jax/_src/debugging.py b/jax/_src/debugging.py\nindex e931a6edb9b3..3490de5118e1 100644\n--- a/jax/_src/debugging.py\n+++ b/jax/_src/debugging.py\n@@ -127,7 +127,7 @@ def debug_callback_jvp_rule(primals, tangents, **params):\n ad.primitive_jvps[debug_callback_p] = debug_callback_jvp_rule\n \n def debug_callback_transpose_rule(*flat_args, callback: Callable[..., Any],\n-    effect: DebugEffect):\n+                                  effect: DebugEffect, partitioned):\n   del flat_args, callback, effect\n   raise ValueError(\"Transpose doesn't support debugging callbacks.\")\n ad.primitive_transposes[debug_callback_p] = debug_callback_transpose_rule\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 9366b91f8022..7cbdfff01462 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -885,7 +885,7 @@ def make_zero(aval):\n     out_nz_tracers = [trace.to_jaxpr_tracer(r)\n                       for (r, nz) in zip(out_tangents, out_nzs) if nz]\n     in_tracers = [t for t, nz in zip(tangent_args, nonzeros) if nz]\n-    jaxpr, out_consts, _ = pe.tracers_to_jaxpr(in_tracers, out_nz_tracers, jvp.debug_info)\n+    jaxpr, out_consts, _ = pe.tracers_to_jaxpr(in_tracers, out_nz_tracers, [], jvp.debug_info)\n     jaxpr, used_consts, _ = pe.dce_jaxpr_consts(\n         jaxpr, [True] * len(jaxpr.outvars),\n         [False] * len(jaxpr.constvars) + [True] * len(jaxpr.invars))\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex f77db5443a86..6ea16ec8e8ba 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -16,6 +16,7 @@\n from collections import namedtuple\n from collections.abc import Callable, Sequence, Hashable\n import contextlib\n+from dataclasses import dataclass\n from functools import partial\n import itertools as it\n import operator as op\n@@ -42,7 +43,7 @@\n                            mapped_aval, unmapped_aval, DBIdx, InDBIdx, OutDBIdx,\n                            InputType, OutputType, get_referent, JaxprEqnContext)\n from jax._src.source_info_util import SourceInfo\n-from jax._src.state.types import AbstractRef, ReadEffect\n+from jax._src.state.types import AbstractRef, ReadEffect, RefEffect\n from jax._src.tree_util import (PyTreeDef, treedef_tuple, tree_flatten,\n                                 tree_structure, register_static)\n from jax._src.util import (unzip2, safe_zip, safe_map, toposort, split_list,\n@@ -147,6 +148,10 @@ def get_aval(self) -> AbstractValue:\n     else:\n       return self[0]\n \n+@dataclass(frozen=True)\n+class EffectHandle:\n+  parents : list[Tracer]\n+  recipe : JaxprEqnRecipe\n \n class JaxprTrace(Trace['JaxprTracer']):\n \n@@ -156,6 +161,8 @@ def __init__(self, parent_trace:Trace, name_stack: source_info_util.NameStack, t\n     self.tag = tag\n     self.parent_trace = parent_trace\n     self.requires_low = False\n+    self.effect_handles : list[EffectHandle] = []\n+    self.counter = it.count()\n \n   def to_jaxpr_tracer(self, x):\n     if isinstance(x, JaxprTracer) and x._trace.tag is self.tag:\n@@ -239,14 +246,19 @@ def default_process_primitive(self, primitive, tracers, params):\n     if primitive.multiple_results:\n       out_tracers = [JaxprTracer(self, PartialVal.unknown(aval), None)\n                      for aval in out_aval]\n-      eqn = new_eqn_recipe(tracers, out_tracers, primitive, params, effects,\n+      eqn = new_eqn_recipe(self, tracers, out_tracers, primitive, params, effects,\n                            source)\n+      if any(isinstance(e, RefEffect) for e in effects):\n+        self.effect_handles.append(EffectHandle(tracers, eqn))\n       for t in out_tracers: t.recipe = eqn\n       return out_tracers\n     else:\n       out_tracer = JaxprTracer(self, PartialVal.unknown(out_aval), None)\n-      out_tracer.recipe = new_eqn_recipe(tracers, [out_tracer], primitive,\n-                                         params, effects, source)\n+      eqn = new_eqn_recipe(self, tracers, [out_tracer], primitive,\n+                           params, effects, source)\n+      if any(isinstance(e, RefEffect) for e in effects):\n+        self.effect_handles.append(EffectHandle(tracers, eqn))\n+      out_tracer.recipe = eqn\n       return out_tracer\n \n   def process_call(self, primitive, f: lu.WrappedFun, tracers, params):\n@@ -321,7 +333,7 @@ def process_call(self, primitive, f: lu.WrappedFun, tracers, params):\n                      for a in out_type]\n     name_stack = self._current_truncated_name_stack()\n     source = source_info_util.current().replace(name_stack=name_stack)\n-    eqn = new_eqn_recipe((*res_tracers, *env_tracers, *unknown_arg_tracers),\n+    eqn = new_eqn_recipe(self, (*res_tracers, *env_tracers, *unknown_arg_tracers),\n                          out_tracers, primitive, staged_params, jaxpr.effects,\n                          source)\n     for t in out_tracers: t.recipe = eqn\n@@ -390,7 +402,7 @@ def const_out_axes_thunk():\n                    for a in out_avals]\n     effs = core.filter_named_axis_effects(jaxpr.effects, {params['axis_name']})\n     src_info = source_info_util.current()\n-    eqn = new_eqn_recipe((*const_tracers, *env_tracers, *unknown_arg_tracers),\n+    eqn = new_eqn_recipe(self, (*const_tracers, *env_tracers, *unknown_arg_tracers),\n                          out_tracers, primitive, staged_params, effs, src_info)\n     for t in out_tracers: t.recipe = eqn\n \n@@ -425,7 +437,7 @@ def process_custom_transpose(self, prim, call, tracers, **params):\n                      for aval in params['out_types']]\n       in_tracers = map(self.instantiate_const, tracers)\n       new_params = dict(params, call=call)\n-      eqn = new_eqn_recipe(in_tracers, out_tracers, prim, new_params,\n+      eqn = new_eqn_recipe(self, in_tracers, out_tracers, prim, new_params,\n           core.no_effects, source_info_util.current())\n       for t in out_tracers: t.recipe = eqn\n       return out_tracers\n@@ -470,7 +482,7 @@ def fwd_jaxpr_thunk(*zeros):\n         out_trees=out_trees,\n         symbolic_zeros=symbolic_zeros\n     )\n-    eqn = new_eqn_recipe((*res_tracers, *env_tracers, *tracers),\n+    eqn = new_eqn_recipe(self, (*res_tracers, *env_tracers, *tracers),\n                          out_tracers, prim, params, jaxpr.effects, source)\n     for t in out_tracers: t.recipe = eqn\n     return out_tracers\n@@ -657,7 +669,7 @@ def _trace_to_subjaxpr_nounits(f: Callable, trace: JaxprTrace,\n   out_tracers = [trace.instantiate_const(t) if inst else t\n                  for inst, t in zip(instantiate, out_tracers)]\n   out_tracers_ = [t for t in out_tracers if not t.is_known()]\n-  jaxpr, out_consts, env = tracers_to_jaxpr(in_tracers, out_tracers_, debug_info)\n+  jaxpr, out_consts, env = tracers_to_jaxpr(in_tracers, out_tracers_, trace.effect_handles, debug_info)\n   return out_tracers, jaxpr, out_consts, env\n \n # The below variant implements an optimization where residuals which are also\n@@ -739,7 +751,8 @@ class JaxprEqnRecipe(NamedTuple):\n   source_info: source_info_util.SourceInfo\n   ctx: JaxprEqnContext\n \n-def new_eqn_recipe(in_tracers: Sequence[JaxprTracer],\n+def new_eqn_recipe(trace: JaxprTrace,\n+                   in_tracers: Sequence[JaxprTracer],\n                    out_tracers: Sequence[JaxprTracer],\n                    primitive: Primitive,\n                    params: dict[str, Any],\n@@ -762,7 +775,7 @@ def new_eqn_recipe(in_tracers: Sequence[JaxprTracer],\n       config.threefry_partitionable.value,\n       xla_metadata_lib.current_xla_metadata(),\n   )\n-  return JaxprEqnRecipe(object(), tuple(in_tracers), map(ref, out_tracers),\n+  return JaxprEqnRecipe(next(trace.counter), tuple(in_tracers), map(ref, out_tracers),\n                         out_avals, primitive, params, effects, source_info,\n                         ctx)\n \n@@ -780,6 +793,7 @@ def recipe_to_eqn(getvar: Callable[[JaxprTracer], Atom],\n def tracers_to_jaxpr(\n   in_tracers: Sequence[JaxprTracer],\n   out_tracers: Sequence[JaxprTracer],\n+  effect_handles: Sequence[Any],\n   debug_info: core.DebugInfo,\n   ) -> tuple[Jaxpr, tuple[Any, ...], tuple[Any, ...]]:\n   \"\"\"Constructs Jaxpr given tracers for inputs and outputs.\n@@ -821,7 +835,15 @@ def type_substitute(aval: AbstractValue) -> AbstractValue:\n \n   processed_eqn_ids = set()\n   eqns: list[core.JaxprEqn] = []\n-  for t in toposort((*in_tracers, *out_tracers)):\n+\n+  reachable = toposort\n+  tracers = reachable((*in_tracers, *out_tracers, *effect_handles))\n+  def sort_key(t):\n+    r = t.recipe\n+    return r.eqn_id if isinstance(r, JaxprEqnRecipe) else -1\n+  tracers = sorted(tracers, key=sort_key)\n+\n+  for t in tracers:\n     r = t.recipe\n     if isinstance(r, JaxprEqnRecipe):\n       # TODO broadcast_in_dim can create a new tracer, not present in parents\ndiff --git a/jax/_src/lax/control_flow/conditionals.py b/jax/_src/lax/control_flow/conditionals.py\nindex 741636c47e31..4e8368341d9f 100644\n--- a/jax/_src/lax/control_flow/conditionals.py\n+++ b/jax/_src/lax/control_flow/conditionals.py\n@@ -617,7 +617,7 @@ def _cond_partial_eval(trace, *tracers, branches, **params):\n   name_stack = source_info_util.current_name_stack()[len(trace.name_stack):]\n   source = source_info_util.current().replace(name_stack=name_stack)\n   eqn = pe.new_eqn_recipe(\n-      [index_tracer] + res_tracers + ops_tracers, out_tracers, cond_p, params,\n+      trace, [index_tracer] + res_tracers + ops_tracers, out_tracers, cond_p, params,\n       core.join_effects(*(j.effects for j in branches_unknown)), source)\n   for t in out_tracers: t.recipe = eqn\n   return util.merge_lists(out_uks, out_consts, out_tracers)\ndiff --git a/jax/_src/lax/control_flow/for_loop.py b/jax/_src/lax/control_flow/for_loop.py\nindex fc7ebde4cbea..90b81ae367aa 100644\n--- a/jax/_src/lax/control_flow/for_loop.py\n+++ b/jax/_src/lax/control_flow/for_loop.py\n@@ -498,7 +498,7 @@ def _for_partial_eval(trace: pe.JaxprTrace, *tracers: pe.JaxprTracer,\n \n   assert len(unknown_inputs) == len(res_ref_unknown_outputs)\n   assert len(unknown_inputs) == len(jaxpr_unknown.invars) - 1\n-  eqn = pe.new_eqn_recipe(unknown_inputs, res_ref_unknown_outputs,\n+  eqn = pe.new_eqn_recipe(trace, unknown_inputs, res_ref_unknown_outputs,\n                           for_p, dict(jaxpr=jaxpr_unknown, nsteps=nsteps,\n                                       reverse=reverse,\n                                       which_linear=which_linear_unknown,\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 7efe3294fdca..83c31928d7cb 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -920,7 +920,7 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n   name_stack = source_info_util.current_name_stack()[len(trace.name_stack):]\n   source = source_info_util.current().replace(name_stack=name_stack)\n   assert len(out_tracers) == len(jaxpr_unknown.out_avals)\n-  eqn = pe.new_eqn_recipe([*intensive_res, *unknown_inputs, *extensive_res],\n+  eqn = pe.new_eqn_recipe(trace, [*intensive_res, *unknown_inputs, *extensive_res],\n                           out_tracers, scan_p,\n                           dict(reverse=reverse, length=length, unroll=unroll,\n                                jaxpr=jaxpr_unknown, linear=linear_unknown,\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex a9d81c684297..68363d10bc04 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -6550,7 +6550,7 @@ def _broadcast_in_dim_partial_eval(\n   out_aval = core.DShapedArray(tuple(shape_), operand.dtype, operand.weak_type)\n   out_tracer = pe.JaxprTracer(trace, pe.PartialVal.unknown(out_aval), None)\n   eqn = pe.new_eqn_recipe(\n-      [operand_tracer, *dyn_shape_tracers], [out_tracer], broadcast_in_dim_p,\n+      trace, [operand_tracer, *dyn_shape_tracers], [out_tracer], broadcast_in_dim_p,\n       dict(shape=shape, broadcast_dimensions=broadcast_dimensions,\n            sharding=None),\n       core.no_effects, source_info_util.current())\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 0c55f3fe30ab..d5286be8e0c9 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -2324,18 +2324,8 @@ def _pjit_partial_eval(trace: pe.JaxprTrace,\n \n   known_ins = tuple(pv.is_known() for pv in in_pvals)\n   unknown_ins = tuple(not k for k in known_ins)\n-  if any(isinstance(e, (RefEffect, core.InternalMutableArrayEffect))\n-         for e in jaxpr.effects):\n-    known_jaxpr_, unknown_jaxpr_, unknown_outs, _, num_res_val, num_res_ref = \\\n-        pe.partial_eval_jaxpr_stateful(jaxpr.jaxpr, unknown_ins, unknown_ins,\n-                                       False, False, None)\n-    if num_res_ref: raise NotImplementedError\n-    known_jaxpr = pe.ClosedJaxpr(known_jaxpr_, jaxpr.consts)\n-    unknown_jaxpr = pe.ClosedJaxpr(unknown_jaxpr_, jaxpr.consts)\n-    res_avals = unknown_jaxpr.in_avals[:num_res_val]\n-  else:\n-    known_jaxpr, unknown_jaxpr, unknown_outs, res_avals = \\\n-        pe.partial_eval_jaxpr_nounits(jaxpr, unknown_ins, instantiate=False)\n+  known_jaxpr, unknown_jaxpr, unknown_outs, res_avals = \\\n+      pe.partial_eval_jaxpr_nounits(jaxpr, unknown_ins, instantiate=False)\n   unknown_outs = tuple(unknown_outs)  # type: ignore[assignment]\n   known_outs = tuple(not uk for uk in unknown_outs)\n   num_residuals = len(res_avals)\n@@ -2431,7 +2421,7 @@ def keep_where(l, should_keep):\n       pe.JaxprTracer(trace, pe.PartialVal.unknown(aval), None)\n       for aval in unknown_out_avals\n   ]\n-  eqn = pe.new_eqn_recipe((*unknown_tracers_in, *residual_tracers),\n+  eqn = pe.new_eqn_recipe(trace, (*unknown_tracers_in, *residual_tracers),\n                           unknown_tracers_out,\n                           pjit_p,\n                           unknown_params,\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex bc6bda9c16de..1ed831d8577f 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -1392,7 +1392,7 @@ def known_out_specs():\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(a), None)\n                  for a in out_avals]\n   effs = core.filter_named_axis_effects(jaxpr.effects, mesh.axis_names)\n-  eqn = pe.new_eqn_recipe((*const_tracers, *env_tracers, *unk_arg_tracers),\n+  eqn = pe.new_eqn_recipe(trace, (*const_tracers, *env_tracers, *unk_arg_tracers),\n                           out_tracers, shard_map_p, unk_params,\n                           effs, source_info_util.current())\n   for t in out_tracers: t.recipe = eqn\ndiff --git a/jax/_src/state/discharge.py b/jax/_src/state/discharge.py\nindex bc6a20a0a76e..100447f12d18 100644\n--- a/jax/_src/state/discharge.py\n+++ b/jax/_src/state/discharge.py\n@@ -828,7 +828,7 @@ def _run_state_partial_eval(trace: pe.JaxprTrace, *tracers: pe.JaxprTracer,\n                    is_initialized=(True,) * len(jaxpr_unknown.invars))\n   _, eqn_effects = run_state_p.abstract_eval(*[v.aval for v in unknown_inputs],\n                                              **uk_params)\n-  eqn = pe.new_eqn_recipe(unknown_inputs, res_ref_unknown_outputs,\n+  eqn = pe.new_eqn_recipe(trace, unknown_inputs, res_ref_unknown_outputs,\n                           run_state_p, uk_params,\n                           eqn_effects, source)\n   for t in res_ref_unknown_outputs: t.recipe = eqn\ndiff --git a/tests/mutable_array_test.py b/tests/mutable_array_test.py\nindex 865d4f8520f1..0da335e2fac5 100644\n--- a/tests/mutable_array_test.py\n+++ b/tests/mutable_array_test.py\n@@ -192,14 +192,18 @@ def f():\n     x = f()\n     self.assertArraysEqual(x, jnp.zeros(8))\n \n-  def test_grad_mutable_array(self):\n-    @jax.jit\n+  @parameterized.parameters([False, True])\n+  def test_grad_mutable_array(self, jit):\n+\n     def f(x):\n       x_ = core.mutable_array(x)\n       x_[()] = x_[()] + x_[()]\n       y = core.freeze(x_)\n       return y\n \n+    if jit:\n+      f = jax.jit(f)\n+\n     ans = jax.grad(f)(1.)\n     expected = 2.0\n     self.assertAllClose(ans, expected, check_dtypes=False)\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #29001 from johannahaffner:test-clip\n\nPiperOrigin-RevId: 763865376",
        "output": [
            "```diff\nCommit: 1d10a488d524b3ad1562f3ef0c0c74030913a5f9\nDate: 2025-05-27T18:00:42Z\nURL: https://github.com/jax-ml/jax/commit/1d10a488d524b3ad1562f3ef0c0c74030913a5f9\nFiles changed: 2\nAdditions: +9, Deletions: -0\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex 0bd287dadd51..ad2b3ad6aa75 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -3410,6 +3410,7 @@ def clip(\n   Returns:\n     An array containing values from ``arr``, with values smaller than ``min`` set\n     to ``min``, and values larger than ``max`` set to ``max``.\n+    Wherever ``min`` is larger than ``max``, the value of ``max`` is returned.\n \n   See also:\n     - :func:`jax.numpy.minimum`: Compute the element-wise minimum value of two arrays.\ndiff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py\nindex 875024617b5f..29e6586ffa18 100644\n--- a/tests/lax_numpy_test.py\n+++ b/tests/lax_numpy_test.py\n@@ -1065,6 +1065,14 @@ def testClipDeprecatedArgs(self):\n                                              \"Passing arguments 'a', 'a_min' or 'a_max' to jax.numpy.clip is deprecated\"):\n       jnp.clip(jnp.arange(4), a_min=2, a_max=3)\n \n+  def testClipUpperPrecedence(self):\n+    a_min = 3 * np.ones(1)\n+    a_max = 2 * np.ones(1)\n+    x = 4 * np.ones(1)\n+    y = jnp.clip(x, min=a_min, max=a_max)\n+    assert y == a_max, f\"Expected {y} to equal {a_max} when a_min > a_max.\"\n+    assert y == jnp.asarray(np.clip(x, a_min=a_min, a_max=a_max))\n+\n   def testHypotComplexInputError(self):\n     rng = jtu.rand_default(self.rng())\n     x = rng((5,), dtype=jnp.complex64)\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix sempahore typo in JAX\n\nPiperOrigin-RevId: 763862020",
        "output": [
            "```diff\nCommit: e258708fc74da6b5757b0996f0fe4bdab07bc526\nDate: 2025-05-27T17:53:33Z\nURL: https://github.com/jax-ml/jax/commit/e258708fc74da6b5757b0996f0fe4bdab07bc526\nFiles changed: 3\nAdditions: +5, Deletions: -5\ndiff --git a/docs/pallas/tpu/distributed.ipynb b/docs/pallas/tpu/distributed.ipynb\nindex f2b9562c0db2..75aeeb92ca43 100644\n--- a/docs/pallas/tpu/distributed.ipynb\n+++ b/docs/pallas/tpu/distributed.ipynb\n@@ -178,7 +178,7 @@\n     \"\\n\",\n     \"`send_sem` and `recv_sem` are instances of a special type of semaphore reserved exclusively for use with DMAs. They must be allocated with the `tpu.SemaphoreType.DMA` type when specifying input specs to `pallas_call`.\\n\",\n     \"\\n\",\n-    \"Internally, DMA semaphores can be thought of as integer-valued progress trackers. On DMA start, the local device will begin to increment the value of `send_sem` and the receiver's `recv_sem` asynchronously. Waiting on a semaphore will block until the value of the semaphore reaches the total bytes of data sent/received; when the value is reached, waiting threads are released and the sempahore's value is decremented by the same amount. This means that either all data has been sent (for `send_sem`) or all data has been received (for `dst_sem`). The value of the semaphore can be read with `pl.semaphore_read`, but note that the underlying semantics of the value could change between hardware generations (e.g. the value may not represent exactly the number of bytes sent, although this is a useful mental model to have when reasoning about the behavior of the semaphore).\\n\",\n+    \"Internally, DMA semaphores can be thought of as integer-valued progress trackers. On DMA start, the local device will begin to increment the value of `send_sem` and the receiver's `recv_sem` asynchronously. Waiting on a semaphore will block until the value of the semaphore reaches the total bytes of data sent/received; when the value is reached, waiting threads are released and the semaphore's value is decremented by the same amount. This means that either all data has been sent (for `send_sem`) or all data has been received (for `dst_sem`). The value of the semaphore can be read with `pl.semaphore_read`, but note that the underlying semantics of the value could change between hardware generations (e.g. the value may not represent exactly the number of bytes sent, although this is a useful mental model to have when reasoning about the behavior of the semaphore).\\n\",\n     \"\\n\",\n     \"### Routing\\n\",\n     \"\\n\",\n@@ -531,7 +531,7 @@\n     \"\\n\",\n     \"Semaphores must be zero at the end of a Pallas program to complete succesfully. There are two error cases where this may happen:\\n\",\n     \" - If a semaphore is over-signaled, the program will end with non-zero (>0) semaphores. In this case, the program will crash upon completion. This is useful for debugging as non-zero semaphores typically means there is a bug somewhere inside of the program.\\n\",\n-    \" - If a semaphore is over-waited, the program will hang on the blocking `semaphore_wait` call while it waits for the sempahore to be incremented. In this case the device or program will need to be restarted.\\n\",\n+    \" - If a semaphore is over-waited, the program will hang on the blocking `semaphore_wait` call while it waits for the semaphore to be incremented. In this case the device or program will need to be restarted.\\n\",\n     \"\\n\",\n     \"#### Barrier Semaphores\\n\",\n     \"\\n\",\ndiff --git a/docs/pallas/tpu/distributed.md b/docs/pallas/tpu/distributed.md\nindex 36528bfbddec..7b1f26bccf89 100644\n--- a/docs/pallas/tpu/distributed.md\n+++ b/docs/pallas/tpu/distributed.md\n@@ -163,7 +163,7 @@ def example_kernel(input_ref, output_ref, send_sem, recv_sem):\n \n `send_sem` and `recv_sem` are instances of a special type of semaphore reserved exclusively for use with DMAs. They must be allocated with the `tpu.SemaphoreType.DMA` type when specifying input specs to `pallas_call`.\n \n-Internally, DMA semaphores can be thought of as integer-valued progress trackers. On DMA start, the local device will begin to increment the value of `send_sem` and the receiver's `recv_sem` asynchronously. Waiting on a semaphore will block until the value of the semaphore reaches the total bytes of data sent/received; when the value is reached, waiting threads are released and the sempahore's value is decremented by the same amount. This means that either all data has been sent (for `send_sem`) or all data has been received (for `dst_sem`). The value of the semaphore can be read with `pl.semaphore_read`, but note that the underlying semantics of the value could change between hardware generations (e.g. the value may not represent exactly the number of bytes sent, although this is a useful mental model to have when reasoning about the behavior of the semaphore).\n+Internally, DMA semaphores can be thought of as integer-valued progress trackers. On DMA start, the local device will begin to increment the value of `send_sem` and the receiver's `recv_sem` asynchronously. Waiting on a semaphore will block until the value of the semaphore reaches the total bytes of data sent/received; when the value is reached, waiting threads are released and the semaphore's value is decremented by the same amount. This means that either all data has been sent (for `send_sem`) or all data has been received (for `dst_sem`). The value of the semaphore can be read with `pl.semaphore_read`, but note that the underlying semantics of the value could change between hardware generations (e.g. the value may not represent exactly the number of bytes sent, although this is a useful mental model to have when reasoning about the behavior of the semaphore).\n \n ### Routing\n \n@@ -453,7 +453,7 @@ In order to use regular semaphores, they can be allocated in the same way as a D\n \n Semaphores must be zero at the end of a Pallas program to complete succesfully. There are two error cases where this may happen:\n  - If a semaphore is over-signaled, the program will end with non-zero (>0) semaphores. In this case, the program will crash upon completion. This is useful for debugging as non-zero semaphores typically means there is a bug somewhere inside of the program.\n- - If a semaphore is over-waited, the program will hang on the blocking `semaphore_wait` call while it waits for the sempahore to be incremented. In this case the device or program will need to be restarted.\n+ - If a semaphore is over-waited, the program will hang on the blocking `semaphore_wait` call while it waits for the semaphore to be incremented. In this case the device or program will need to be restarted.\n \n #### Barrier Semaphores\n \ndiff --git a/jax/_src/pallas/mosaic/interpret.py b/jax/_src/pallas/mosaic/interpret.py\nindex f3a165105640..401ed02288bc 100644\n--- a/jax/_src/pallas/mosaic/interpret.py\n+++ b/jax/_src/pallas/mosaic/interpret.py\n@@ -746,7 +746,7 @@ def _allocate_semaphores(\n ):\n   \"\"\"Allocates semaphores on the device with id `device_id` and core with id `local_core_id`.\n \n-  The number of sempahores allocated is given by the product of the entries in\n+  The number of semaphores allocated is given by the product of the entries in\n   `shape`.\n \n   Since for each semaphore id there is really only one global `Semaphore`\n\n```"
        ],
        "from_id": [
            "apivovarov",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Add an unsafe flag that disables automatic WG-barrier insertion\n\nEnabling this flag can introduce races into certain kernels, which is why it's\nFalse by default. Still, there's plenty of kernels where it's unnecessary and\na few of those suffer performance regressions when it is on. So it makes sense\nto at least allow users to opt out.\n\nPiperOrigin-RevId: 763853668",
        "output": [
            "```diff\nCommit: 6f0b99356d4b95d5547c11c8ab12aa4f224e4181\nDate: 2025-05-27T17:32:55Z\nURL: https://github.com/jax-ml/jax/commit/6f0b99356d4b95d5547c11c8ab12aa4f224e4181\nFiles changed: 3\nAdditions: +29, Deletions: -7\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 2fca1464ee0b..7fb933f5623d 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -87,6 +87,16 @@ class CompilerParams(pallas_core.CompilerParams):\n       references. Defaults to 0, and must be strictly smaller than\n       max_concurrent_steps. Generally, you'll want to set it to 1 if you don't\n       await the WGMMA in the body.\n+    unsafe_no_auto_barriers: If True, Pallas will never automatically insert\n+      barrier instructions that ensure synchronous semantics of loads and stores.\n+      At the moment, the insertion is done conservatively and might regress\n+      performance. There are (at least) two conditions that must be satisfied\n+      for the use of this flag to be safe. First, no memory region is ever read\n+      *and* written to by the same thread (async copies are performed by\n+      background threads and do not count towards this rule). Secondly, no\n+      thread ever calls commit_smem(), reads from the committed SMEM and then\n+      issues an async copy overwriting that region (this is a very artificial\n+      and highly unlikely scenario).\n     profile_space: The number of profiler events that can be collected in a\n       single invocation. It is undefined behavior if a thread collects more\n       events than this.\n@@ -97,6 +107,7 @@ class CompilerParams(pallas_core.CompilerParams):\n   dimension_semantics: Sequence[DimensionSemantics] | None = None\n   max_concurrent_steps: int = 1\n   delay_release: int = 0\n+  unsafe_no_auto_barriers: bool = False\n   profile_space: int = 0\n   profile_dir: str = \"\"\n   lowering_semantics: mgpu.core.LoweringSemantics = mgpu.core.LoweringSemantics.Lane\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex cf867e55f4c9..6d54e153a9a2 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -327,6 +327,8 @@ class ModuleContext:\n   lowering_semantics: mgpu.LoweringSemantics\n   primitive_semantics: gpu_core.PrimitiveSemantics\n   mesh: mesh_lib.Mesh | None\n+  # See the documentation of unsafe_no_auto_barriers in CompilerParams.\n+  auto_barriers: bool\n   warp_axis_name: str | None = None\n \n   @property\n@@ -822,6 +824,7 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n         lowering_semantics=lowering_semantics,\n         primitive_semantics=gpu_core.PrimitiveSemantics.Warpgroup,\n         mesh=jax_mesh,\n+        auto_barriers=not params.unsafe_no_auto_barriers,\n     )\n     del runtime_smem, grouped_barriers, runtime_barriers\n     _ = lower_jaxpr_to_mosaic_gpu(\n@@ -1389,7 +1392,8 @@ def _swap_lowering_rule(\n       ctx, x_ref, transforms, handle_transposes=not transposed_value,\n       allow_peer_refs=True\n   )\n-  mgpu.warpgroup_barrier()  # Make sure reads have completed before we write.\n+  if ctx.module_ctx.auto_barriers:\n+    mgpu.warpgroup_barrier()  # Make sure reads have completed before we write.\n   match transforms:\n     case (\n         gpu_core.UnswizzleRef(swizzle),\n@@ -1443,7 +1447,8 @@ def _swap_lowering_rule(\n           value.store_untiled(x_smem)\n     case _:\n       raise NotImplementedError(f\"Unsupported transforms: {transforms}\")\n-  mgpu.warpgroup_barrier()  # Make sure the writes have completed.\n+  if ctx.module_ctx.auto_barriers:\n+    mgpu.warpgroup_barrier()  # Make sure the writes have completed.\n   return old_value\n \n \n@@ -2796,7 +2801,8 @@ def _core_map_lowering_rule(\n     # We allow the warps to schedule async copies without synchronizing with\n     # other warps, so we need to add a barrier here to make sure all reads and\n     # writes have completed.\n-    mgpu.warpgroup_barrier()\n+    if ctx.module_ctx.auto_barriers:\n+      mgpu.warpgroup_barrier()\n     _ = lower_jaxpr_to_mosaic_gpu(\n         module_ctx,\n         ctx.launch_ctx,\n@@ -2804,8 +2810,9 @@ def _core_map_lowering_rule(\n         args=(),\n         consts=args,\n     )\n-    # TODO(apaszke,justinfu): Do we really need this barrier?\n-    mgpu.warpgroup_barrier()\n+    if ctx.module_ctx.auto_barriers:\n+      # TODO(apaszke,justinfu): Do we really need this barrier?\n+      mgpu.warpgroup_barrier()\n     return []\n   raise ValueError(f\"Unsupported mesh: {mesh}\")\n \n@@ -3052,7 +3059,8 @@ def _semaphore_signal_lowering_rule(\n   # anything about the state of the other three warps in the warpgroup (they\n   # might still be e.g. reading memory that someone will overwrite once they\n   # receive a signal).\n-  mgpu.utils.warpgroup_barrier()\n+  if ctx.module_ctx.auto_barriers:\n+    mgpu.utils.warpgroup_barrier()\n   pred = ctx.module_ctx.single_wg_lane_predicate\n   llvm_dialect.inline_asm(\n     i32,\n@@ -3098,6 +3106,8 @@ def _semaphore_wait_lowering_rule(ctx: LoweringRuleContext, *args, args_tree):\n     after_block = while_op.after.blocks.append(i32_ty)\n     with ir.InsertionPoint.at_block_begin(after_block):\n       scf_dialect.yield_(after_block.arguments)\n+  # NOTE: This barrier is necessary for a correct lowering of this op and can't\n+  # be removed even if auto_barriers is False.\n   mgpu_utils.warpgroup_barrier()\n   return ()\n \ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 61be6e35cc55..9e40d046af13 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -530,7 +530,8 @@ def _copy_gmem_to_smem_lowering(\n       # arrive with the whole transfer size, while everyone else arrives with 0.\n       # But we should continue using this scheme as it's likely to be faster.\n       bytes //= WARPGROUP_SIZE\n-      mgpu.warpgroup_barrier()  # Make sure all reads have completed.\n+      if ctx.module_ctx.auto_barriers:\n+        mgpu.warpgroup_barrier()  # Make sure all reads have completed.\n       barrier.arrive_expect_tx(bytes)\n     else:\n       # In Warp-level lowering, we arrive on each CUDA thread in a warp, but\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "#sdy Remove redundant sdy export since it's now done as part of `MlirToXlaComputation`.\n\nPiperOrigin-RevId: 763837933",
        "output": [
            "```diff\nCommit: 3b3c3385e8c51c81fea111707ff107a5a40edca3\nDate: 2025-05-27T16:58:14Z\nURL: https://github.com/jax-ml/jax/commit/3b3c3385e8c51c81fea111707ff107a5a40edca3\nFiles changed: 3\nAdditions: +0, Deletions: -17\ndiff --git a/jaxlib/mlir.cc b/jaxlib/mlir.cc\nindex a632cac71d10..4c8188b04a7f 100644\n--- a/jaxlib/mlir.cc\n+++ b/jaxlib/mlir.cc\n@@ -106,8 +106,6 @@ absl::StatusOr<XlaComputation> PyMlirModuleToXlaComputation(\n   TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n                       ParseMlirModuleString(mlir_module, context));\n   XlaComputation computation;\n-  // SDY dialect may be part of the module which XLA doesn't know about.\n-  TF_RETURN_IF_ERROR(ExportShardyForHloRoundTrip(*module));\n   TF_RETURN_IF_ERROR(MlirToXlaComputation(*module, computation, use_tuple_args,\n                                           return_tuple,\n                                           /*use_shardy=*/false));\ndiff --git a/jaxlib/py_client.cc b/jaxlib/py_client.cc\nindex 842bdfecad3d..98bde8c27396 100644\n--- a/jaxlib/py_client.cc\n+++ b/jaxlib/py_client.cc\n@@ -459,11 +459,6 @@ PyClient::CompileAndLoad(nb_class_ptr<PyClient> client, std::string mlir_module,\n   mlir::MLIRContext context;\n   TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n                       ParseMlirModuleString(mlir_module, context));\n-  if (options.executable_build_options.use_shardy_partitioner()) {\n-    // Since Shardy is located in the middle of the XLA pipeline, we need to\n-    // export it before going to HLO while preserving Shardy ops and attrs.\n-    TF_RETURN_IF_ERROR(ExportShardyForHloRoundTrip(*module));\n-  }\n   return CompileAndLoadIfrtProgram(\n       client, std::make_unique<xla::ifrt::HloProgram>(module.get()),\n       MakeIfrtCompileOptions(std::move(options), std::move(executable_devices),\n@@ -478,11 +473,6 @@ PyClient::CompileAndLoad(nb_class_ptr<PyClient> client, std::string mlir_module,\n   mlir::MLIRContext context;\n   TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n                       ParseMlirModuleString(mlir_module, context));\n-  if (options.executable_build_options.use_shardy_partitioner()) {\n-    // Since Shardy is located in the middle of the XLA pipeline, we need to\n-    // export it before going to HLO while preserving Shardy ops and attrs.\n-    TF_RETURN_IF_ERROR(ExportShardyForHloRoundTrip(*module));\n-  }\n \n   std::vector<tsl::RCReference<ifrt::LoadedHostCallback>>\n       ifrt_loaded_host_callbacks;\ndiff --git a/jaxlib/py_compile_only_client.cc b/jaxlib/py_compile_only_client.cc\nindex 2de896d80bef..274f57acba00 100644\n--- a/jaxlib/py_compile_only_client.cc\n+++ b/jaxlib/py_compile_only_client.cc\n@@ -82,11 +82,6 @@ class CompileOnlyPyClient : public PyClient {\n     mlir::MLIRContext context;\n     TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n                         ParseMlirModuleString(mlir_module, context));\n-    if (options.executable_build_options.use_shardy_partitioner()) {\n-      // Since Shardy is located in the middle of the XLA pipeline, we need to\n-      // export it before going to HLO while preserving Shardy ops and attrs.\n-      TF_RETURN_IF_ERROR(ExportShardyForHloRoundTrip(*module));\n-    }\n     auto* ifrt_client =\n         llvm::dyn_cast_or_null<CompileOnlyIfRtClient>(this->ifrt_client());\n     CHECK(ifrt_client) << \"CompileOnlyPyClient requires ifrt_client be a \"\n\n```"
        ],
        "from_id": [
            "tomnatan30",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix handling of input None in custom_transpose.",
        "output": [
            "```diff\nCommit: fce93d2f829ae20b4be49451fb3511df91627679\nDate: 2025-05-27T14:58:45Z\nURL: https://github.com/jax-ml/jax/commit/fce93d2f829ae20b4be49451fb3511df91627679\nFiles changed: 2\nAdditions: +25, Deletions: -5\ndiff --git a/jax/_src/custom_transpose.py b/jax/_src/custom_transpose.py\nindex 21e607b5bff2..fb125e174122 100644\n--- a/jax/_src/custom_transpose.py\n+++ b/jax/_src/custom_transpose.py\n@@ -217,7 +217,6 @@ def custom_transpose_transpose_rule(\n   # Consider passing this information to the custom transpose rule?\n \n   res_arg, lin_arg = tree_unflatten(call_in_tree, args)\n-  del lin_arg\n   assert all(not ad.is_undefined_primal(x) for x in tree_leaves(res_arg))\n \n   cts = [ad_util.zeros_like_aval(ct.aval) if type(ct) is ad_util.Zero else ct\n@@ -225,10 +224,17 @@ def custom_transpose_transpose_rule(\n   ct_out = tree_unflatten(out_tree, cts)\n   ct_lin = transpose.call_wrapped(res_arg, ct_out)\n   check_transpose_rule_trees(transpose, lin_tree, tree_structure(ct_lin))\n-  ct_lin_flat, _ = tree_flatten(\n-      tree_broadcast(lin_tree, ct_lin, is_leaf=lambda x: x is None),\n-      is_leaf=lambda x: x is None)\n-  return [None] * len(tree_leaves(res_arg)) + ct_lin_flat\n+  ct_lin = tree_broadcast(lin_tree, ct_lin, is_leaf=lambda x: x is None)\n+\n+  # When the transpose returns None, we treat that as a Zero, except when the\n+  # input is also None. In that case, the cotangent corresponding to that input\n+  # should be dropped.\n+  zero = object()\n+  ct_lin = tree_map(lambda l, ct: zero if ct is None and l is not None else ct,\n+                    lin_arg, ct_lin, is_leaf=ad.is_undefined_primal)\n+\n+  ct_lin_flat, _ = tree_flatten(ct_lin)\n+  return [None] * res_tree.num_leaves + [None if ct is zero else ct for ct in ct_lin_flat]\n \n \n def custom_transpose_lowering(*args, call_jaxpr, **params):\ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 9d10b40c6030..bfe391797920 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3722,6 +3722,20 @@ def gt(x, t):\n     with config.use_direct_linearize(True):\n       self.assertAllClose(jax.grad(f)(0.5), jnp.cos(0.5))\n \n+  def test_input_none(self):\n+    # ref: https://github.com/jax-ml/jax/issues/29009\n+    @jax.custom_jvp\n+    def f(x, y): return y\n+    @f.defjvp\n+    def f_jvp(p, t): return f(*p), g(p, t)\n+\n+    @custom_transpose(jnp.float32(0))\n+    def g(r, x): return x[1]\n+    @g.def_transpose\n+    def gt(r, t): return None, jnp.zeros_like(r[1])\n+\n+    jax.grad(f, argnums=(1,))(None, jnp.float32(2))  # doesn't crash\n+\n \n class CustomDceTest(jtu.JaxTestCase):\n \n\n```"
        ],
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "[Mosaic GPU][NFC] Refactor the body of the matmul kernel\n\nThis will make it much simpler to make the kernel persistent.\n\nPiperOrigin-RevId: 763782577",
        "output": [
            "```diff\nCommit: ee727f98746e77a0cd7f8ad5b63debfd0ba00053\nDate: 2025-05-27T14:26:58Z\nURL: https://github.com/jax-ml/jax/commit/ee727f98746e77a0cd7f8ad5b63debfd0ba00053\nFiles changed: 1\nAdditions: +84, Deletions: -79\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex 3653d9be8d8d..929c7c498986 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -94,92 +94,97 @@ def kernel(ctx, a, b, d, smem):\n         arith.CmpIPredicate.eq, ctx.cluster_idx(gpu.Dimension.x), c(0, index)\n     )\n \n+    # This function executes the kernel for a single output tile.\n+    def compute_output(block_m_start, n_start):\n+      \"\"\"Compute and store a single output tile.\"\"\"\n+      # All blocks in the cluster share the same m_start -- align it!\n+      m_start = arith.muli(arith.divui(block_m_start, c(tile_m, index)), c(tile_m, index))\n+      with mgpu.when(is_leader_of(TMA_WARP)):\n+        @mgpu.fori(c(k_loop_iter, index), None)\n+        def _tma_body(ki, _):\n+          slot = arith.remui(ki, c(max_concurrent_steps, index))\n+          # TODO(apaszke): Use a predicate instead of a conditional.\n+          with mgpu.when(arith.cmpi(arith.CmpIPredicate.uge, ki, c(max_concurrent_steps, index))):\n+            ab_empty_barriers[slot].wait()\n+          full_barrier = ab_full_barriers[slot]\n+          with mgpu.when(is_leader_block):\n+            full_barrier.arrive_expect_tx(\n+                bytecount((tile_m, tile_k), dtype) + bytecount((tile_n, tile_k), dtype)\n+            )\n+          k_start = arith.muli(ki, c(tile_k, index))\n+          common_args = dict(\n+              swizzle=swizzle,\n+              barrier=full_barrier,\n+              arrive=False,\n+              predicate=None,\n+              collective=gpu.Dimension.x,\n+              partitioned=0,  # Non-contracting dim is always 0.\n+          )\n+          ctx.async_copy(\n+              src_ref=a,\n+              dst_ref=mgpu.memref_slice(a_smem, slot),\n+              gmem_slice=(ds(m_start, tile_m), ds(k_start, tile_k)),\n+              gmem_transform=mgpu.TileTransform(tiling),\n+              **common_args,\n+          )\n+          ctx.async_copy(\n+              src_ref=b,\n+              dst_ref=mgpu.memref_slice(b_smem, slot),\n+              gmem_slice=(ds(n_start, tile_n), ds(k_start, tile_k)),\n+              gmem_transform=mgpu.TileTransform(tiling),\n+              **common_args,\n+          )\n+\n+      with mgpu.when(arith.andi(is_leader_of(MMA_WARP), is_leader_block)):\n+        @mgpu.fori(c(k_loop_iter, index), arith.constant(i1, 0))\n+        def _mma_body(ki, accumulate):\n+          slot = arith.remui(ki, c(max_concurrent_steps, index))\n+          ab_full_barriers[slot].wait()\n+          tcgen05.mma(\n+              acc,\n+              mgpu.memref_slice(a_smem, slot),\n+              mgpu.memref_transpose(mgpu.memref_slice(b_smem, slot), (1, 0, 3, 2)),\n+              a_swizzle=swizzle,\n+              b_swizzle=swizzle,\n+              accumulate=accumulate,\n+              collective=collective,\n+          )\n+          accumulate = arith.constant(i1, 1)\n+          is_last_iter = arith.cmpi(\n+              arith.CmpIPredicate.eq, ki, c(k_loop_iter - 1, index)\n+          )\n+          barrier_ptr = arith.select(\n+              is_last_iter,\n+              mma_done_barrier.get_ptr(),\n+              ab_empty_barriers[slot].get_ptr(),\n+          )\n+          tcgen05.commit_arrive(barrier_ptr, collective=collective, ctx=ctx)\n+          return accumulate\n+\n+      gpu.barrier()\n+      mma_done_barrier.wait(for_tensor_core=True)\n+\n+      final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n+      final_acc.store_tiled(d_smem, swizzle=128)\n+      mgpu.commit_shared()\n+      ctx.async_copy(\n+          src_ref=d_smem,\n+          dst_ref=d,\n+          gmem_slice=(ds(block_m_start, block_tile_m), ds(n_start, tile_n)),\n+          gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n+          swizzle=swizzle,\n+      )\n+      ctx.await_async_copy(0)\n+\n     m_idx = arith.addi(\n         gpu.block_id(gpu.Dimension.x),\n         arith.muli(gpu.block_id(gpu.Dimension.z), c(grid_tile_m, index)),\n     )\n     n_idx = gpu.block_id(gpu.Dimension.y)\n     block_m_start = arith.muli(m_idx, c(block_tile_m, index))\n-    # All blocks in the cluster share the same m_start -- align it!\n-    m_start = arith.muli(arith.divui(block_m_start, c(tile_m, index)), c(tile_m, index))\n     n_start = arith.muli(n_idx, c(tile_n,index))\n-\n-    with mgpu.when(is_leader_of(TMA_WARP)):\n-      @mgpu.fori(c(k_loop_iter, index), None)\n-      def _tma_body(ki, _):\n-        slot = arith.remui(ki, c(max_concurrent_steps, index))\n-        # TODO(apaszke): Use a predicate instead of a conditional.\n-        with mgpu.when(arith.cmpi(arith.CmpIPredicate.uge, ki, c(max_concurrent_steps, index))):\n-          ab_empty_barriers[slot].wait()\n-        full_barrier = ab_full_barriers[slot]\n-        with mgpu.when(is_leader_block):\n-          full_barrier.arrive_expect_tx(\n-              bytecount((tile_m, tile_k), dtype) + bytecount((tile_n, tile_k), dtype)\n-          )\n-        k_start = arith.muli(ki, c(tile_k, index))\n-        common_args = dict(\n-            swizzle=swizzle,\n-            barrier=full_barrier,\n-            arrive=False,\n-            predicate=None,\n-            collective=gpu.Dimension.x,\n-            partitioned=0,  # Non-contracting dim is always 0.\n-        )\n-        ctx.async_copy(\n-            src_ref=a,\n-            dst_ref=mgpu.memref_slice(a_smem, slot),\n-            gmem_slice=(ds(m_start, tile_m), ds(k_start, tile_k)),\n-            gmem_transform=mgpu.TileTransform(tiling),\n-            **common_args,\n-        )\n-        ctx.async_copy(\n-            src_ref=b,\n-            dst_ref=mgpu.memref_slice(b_smem, slot),\n-            gmem_slice=(ds(n_start, tile_n), ds(k_start, tile_k)),\n-            gmem_transform=mgpu.TileTransform(tiling),\n-            **common_args,\n-        )\n-\n-    with mgpu.when(arith.andi(is_leader_of(MMA_WARP), is_leader_block)):\n-      @mgpu.fori(c(k_loop_iter, index), arith.constant(i1, 0))\n-      def _mma_body(ki, accumulate):\n-        slot = arith.remui(ki, c(max_concurrent_steps, index))\n-        ab_full_barriers[slot].wait()\n-        tcgen05.mma(\n-            acc,\n-            mgpu.memref_slice(a_smem, slot),\n-            mgpu.memref_transpose(mgpu.memref_slice(b_smem, slot), (1, 0, 3, 2)),\n-            a_swizzle=swizzle,\n-            b_swizzle=swizzle,\n-            accumulate=accumulate,\n-            collective=collective,\n-        )\n-        accumulate = arith.constant(i1, 1)\n-        is_last_iter = arith.cmpi(\n-            arith.CmpIPredicate.eq, ki, c(k_loop_iter - 1, index)\n-        )\n-        barrier_ptr = arith.select(\n-            is_last_iter,\n-            mma_done_barrier.get_ptr(),\n-            ab_empty_barriers[slot].get_ptr(),\n-        )\n-        tcgen05.commit_arrive(barrier_ptr, collective=collective, ctx=ctx)\n-        return accumulate\n-\n-    gpu.barrier()\n-    mma_done_barrier.wait(for_tensor_core=True)\n-\n-    final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n-    final_acc.store_tiled(d_smem, swizzle=128)\n-    mgpu.commit_shared()\n-    ctx.async_copy(\n-        src_ref=d_smem,\n-        dst_ref=d,\n-        gmem_slice=(ds(block_m_start, block_tile_m), ds(n_start, tile_n)),\n-        gmem_transform=mgpu.TileTransform((128, swizzle_elems)),\n-        swizzle=swizzle,\n-    )\n-    ctx.await_async_copy(0)\n+    # This is not a persistent kernel, so we only process one tile.\n+    compute_output(block_m_start, n_start)\n \n   compute_buffers = (\n     jax.ShapeDtypeStruct(\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Fix missing symbol errors in OSS collective kernels\n\nWe sometimes access NVSHMEM functions from the host code too, which means\nwe should include the NVSHMEM host library in the context of the ExecutionEngine.\n\nPiperOrigin-RevId: 763777731",
        "output": [
            "```diff\nCommit: f5ffd7fc7c717b9e1aa91d3887bfb1c93189c73e\nDate: 2025-05-27T14:10:34Z\nURL: https://github.com/jax-ml/jax/commit/f5ffd7fc7c717b9e1aa91d3887bfb1c93189c73e\nFiles changed: 1\nAdditions: +7, Deletions: -4\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 7df185cffaf1..54fef13a8521 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -555,9 +555,12 @@ absl::StatusOr<std::pair<std::unique_ptr<mlir::ExecutionEngine>, bool>> Compile(\n     return absl::InternalError(\"Pass pipeline failed\");\n   }\n \n-  llvm::SmallVector<llvm::StringRef> runtime_lib;\n-  if (const char* lib_path = getenv(\"MOSAIC_GPU_RUNTIME_LIB_PATH\")) {\n-    runtime_lib.emplace_back(lib_path);\n+  llvm::SmallVector<llvm::StringRef> runtime_libs;\n+  if (const char* runtime_lib_path = getenv(\"MOSAIC_GPU_RUNTIME_LIB_PATH\")) {\n+    runtime_libs.emplace_back(runtime_lib_path);\n+  }\n+  if (const char* nvshmem_path = getenv(\"MOSAIC_GPU_NVSHMEM_SO_PATH\")) {\n+    runtime_libs.emplace_back(nvshmem_path);\n   }\n   // Create a transformer to run all LLVM optimization passes at the\n   // specified optimization level.\n@@ -566,7 +569,7 @@ absl::StatusOr<std::pair<std::unique_ptr<mlir::ExecutionEngine>, bool>> Compile(\n   mlir::ExecutionEngineOptions options;\n   options.transformer = transformer;\n   options.jitCodeGenOptLevel = llvm::CodeGenOptLevel::Aggressive;\n-  options.sharedLibPaths = runtime_lib;\n+  options.sharedLibPaths = runtime_libs;\n   auto maybe_execution_engine = mlir::ExecutionEngine::create(module, options);\n   if (!maybe_execution_engine) {\n     return absl::InternalError(\"Failed to compile kernel\");\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add tests for the Blackwell matmul kernel\n\nJust to give us extra confidence while we make changes.\n\nPiperOrigin-RevId: 763767275",
        "output": [
            "```diff\nCommit: 487eeb4c0fa518b4055f2d154ffd49153809e845\nDate: 2025-05-27T13:36:12Z\nURL: https://github.com/jax-ml/jax/commit/487eeb4c0fa518b4055f2d154ffd49153809e845\nFiles changed: 5\nAdditions: +113, Deletions: -24\ndiff --git a/jax/experimental/mosaic/gpu/examples/BUILD b/jax/experimental/mosaic/gpu/examples/BUILD\nindex fe1a7e9180ac..b24c38b34235 100644\n--- a/jax/experimental/mosaic/gpu/examples/BUILD\n+++ b/jax/experimental/mosaic/gpu/examples/BUILD\n@@ -39,6 +39,15 @@ py_library(\n     ],\n )\n \n+py_library(\n+    name = \"matmul_blackwell\",\n+    srcs = [\"matmul_blackwell.py\"],\n+    deps = [\n+        \"//jax\",\n+        \"//jax:mosaic_gpu\",\n+    ],\n+)\n+\n py_library(\n     name = \"flash_attention\",\n     srcs = [\"flash_attention.py\"],\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex f771c8bc1ef1..3653d9be8d8d 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -41,7 +41,8 @@ def bytecount(shape, dtype):\n \n \n def build_kernel(\n-    m, n, k,\n+    m, k, n,\n+    dtype: jnp.dtype,\n     tile_m: int = 128,\n     tile_n: int = 128,\n     grid_tile_m: int = 1,\n@@ -51,12 +52,15 @@ def build_kernel(\n   i1 = ir.IntegerType.get_signless(1)\n   i32 = ir.IntegerType.get_signless(32)\n   index = ir.IndexType.get()\n+  if jnp.dtype(dtype).itemsize != 2:\n+    raise NotImplementedError(f\"Only tested with 16-bit dtypes, but got {dtype}\")\n+  if tile_m != 128:\n+    raise NotImplementedError(f\"Only tile_m=128 supported, but got {tile_m}\")\n \n   swizzle = 128\n-  swizzle_elems = tile_k = swizzle // 2\n+  swizzle_elems = tile_k = 8 * swizzle // jnp.finfo(dtype).bits\n   tiling = (8, swizzle_elems)\n \n-  in_dtype = jnp.float16\n   k_loop_iter = k // tile_k\n   max_concurrent_steps = min(max_concurrent_steps, k_loop_iter)\n \n@@ -74,7 +78,7 @@ def build_kernel(\n     raise ValueError(f\"{n=} must be divisible by {tile_n=}\")\n   if k % tile_k != 0:\n     raise ValueError(f\"{k=} must be divisible by {tile_k=}\")\n-  if (m // tile_m) % grid_tile_m:\n+  if (m // block_tile_m) % grid_tile_m:\n     raise ValueError(f\"{m=} // {tile_m=} must be divisible by {grid_tile_m=}\")\n \n   def kernel(ctx, a, b, d, smem):\n@@ -83,8 +87,12 @@ def kernel(ctx, a, b, d, smem):\n \n     warp_idx = mgpu.warp_idx(sync=True)\n     is_warp_leader = nvvm.elect_sync(i1)\n-    is_leader_of = lambda i: arith.andi(arith.cmpi(arith.CmpIPredicate.eq, warp_idx, c(i, i32)), is_warp_leader)\n-    is_leader_block = arith.cmpi(arith.CmpIPredicate.eq, ctx.cluster_idx(gpu.Dimension.x), c(0, index))\n+    is_leader_of = lambda i: arith.andi(\n+        arith.cmpi(arith.CmpIPredicate.eq, warp_idx, c(i, i32)), is_warp_leader\n+    )\n+    is_leader_block = arith.cmpi(\n+        arith.CmpIPredicate.eq, ctx.cluster_idx(gpu.Dimension.x), c(0, index)\n+    )\n \n     m_idx = arith.addi(\n         gpu.block_id(gpu.Dimension.x),\n@@ -96,7 +104,6 @@ def kernel(ctx, a, b, d, smem):\n     m_start = arith.muli(arith.divui(block_m_start, c(tile_m, index)), c(tile_m, index))\n     n_start = arith.muli(n_idx, c(tile_n,index))\n \n-\n     with mgpu.when(is_leader_of(TMA_WARP)):\n       @mgpu.fori(c(k_loop_iter, index), None)\n       def _tma_body(ki, _):\n@@ -107,7 +114,7 @@ def _tma_body(ki, _):\n         full_barrier = ab_full_barriers[slot]\n         with mgpu.when(is_leader_block):\n           full_barrier.arrive_expect_tx(\n-              bytecount((tile_m, tile_k), in_dtype) + bytecount((tile_n, tile_k), in_dtype)\n+              bytecount((tile_m, tile_k), dtype) + bytecount((tile_n, tile_k), dtype)\n           )\n         k_start = arith.muli(ki, c(tile_k, index))\n         common_args = dict(\n@@ -162,7 +169,8 @@ def _mma_body(ki, accumulate):\n     gpu.barrier()\n     mma_done_barrier.wait(for_tensor_core=True)\n \n-    acc.load().astype(ir.F16Type.get()).store_tiled(d_smem, swizzle=128)\n+    final_acc = acc.load().astype(mlir.dtype_to_ir_type(jnp.dtype(dtype)))\n+    final_acc.store_tiled(d_smem, swizzle=128)\n     mgpu.commit_shared()\n     ctx.async_copy(\n         src_ref=d_smem,\n@@ -176,14 +184,14 @@ def _mma_body(ki, accumulate):\n   compute_buffers = (\n     jax.ShapeDtypeStruct(\n         mgpu.tile_shape((max_concurrent_steps, block_tile_m, tile_k), tiling),\n-        jnp.float16),\n+        dtype),\n     jax.ShapeDtypeStruct(\n-         mgpu.tile_shape((max_concurrent_steps, block_tile_n, tile_k), tiling),\n-         jnp.float16),\n+        mgpu.tile_shape((max_concurrent_steps, block_tile_n, tile_k), tiling),\n+        dtype),\n   )\n   epilogue_buffer = jax.ShapeDtypeStruct(\n       mgpu.tile_shape((block_tile_m, tile_n), (128, swizzle_elems)),\n-      jnp.float16)\n+      dtype)\n   smem_buffers = mgpu.Union([compute_buffers, epilogue_buffer])\n   smem = (\n       smem_buffers,\n@@ -196,10 +204,10 @@ def _mma_body(ki, accumulate):\n       (grid_tile_m, n // tile_n, m // (block_tile_m * grid_tile_m)),\n       (128, 1, 1),\n       (\n-          jax.ShapeDtypeStruct((m, k), jnp.float16),\n-          jax.ShapeDtypeStruct((n, k), jnp.float16),\n+          jax.ShapeDtypeStruct((m, k), dtype),\n+          jax.ShapeDtypeStruct((n, k), dtype),\n       ),\n-      jax.ShapeDtypeStruct((m, n), jnp.float16),\n+      jax.ShapeDtypeStruct((m, n), dtype),\n       smem,\n       cluster=(2 if collective else 1, 1, 1),\n   )\n@@ -236,7 +244,7 @@ def main(unused_argv):\n       continue\n     try:\n       with mlir.make_ir_context(), ir.Location.unknown():\n-        f = build_kernel(m, n, k, **kwargs)\n+        f = build_kernel(m, k, n, jnp.float16, **kwargs)\n         _, runtime = profiler.measure(f)(a, b)\n     except ValueError as e:\n       if \"Mosaic GPU kernel exceeds available shared memory\" not in str(e):\n@@ -251,7 +259,7 @@ def main(unused_argv):\n     raise ValueError(\"No valid configuration found\")\n \n   with mlir.make_ir_context(), ir.Location.unknown():\n-    d, runtime = profiler.measure(build_kernel(m, n, k, **best_kwargs))(a, b)\n+    d, runtime = profiler.measure(build_kernel(m, k, n, jnp.float16, **best_kwargs))(a, b)\n   d_ref, ref_runtime = profiler.measure(jax.jit(lambda a, b: a @ b.T))(a, b)\n \n   tflops = float(2 * k * m * n) / (runtime / 1e3) / 1e12\ndiff --git a/tests/mosaic/BUILD b/tests/mosaic/BUILD\nindex 9b6a7f79d099..ffaa0c3c843f 100644\n--- a/tests/mosaic/BUILD\n+++ b/tests/mosaic/BUILD\n@@ -126,6 +126,7 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:mosaic_gpu\",\n         \"//jax/experimental/mosaic/gpu/examples:matmul\",\n+        \"//jax/experimental/mosaic/gpu/examples:matmul_blackwell\",\n     ] + py_deps([\n         \"absl/testing\",\n         \"numpy\",\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 42a3f0fc83c1..0c79d26782c7 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -228,8 +228,7 @@ def setUp(self):\n     super().setUp()\n     self.prng = np.random.default_rng(1234)\n     self.context = mlir.make_ir_context()\n-    if mgpu_dialect is not None:\n-      mgpu_dialect.register_dialect(self.context)\n+    mgpu_dialect.register_dialect(self.context)\n     self.enter_context(config.traceback_filtering(\"off\"))\n     self.enter_context(self.context)\n     self.enter_context(ir.Location.unknown())\ndiff --git a/tests/mosaic/matmul_test.py b/tests/mosaic/matmul_test.py\nindex 9634718d2d44..680e699c8972 100644\n--- a/tests/mosaic/matmul_test.py\n+++ b/tests/mosaic/matmul_test.py\n@@ -19,7 +19,11 @@\n from absl.testing import absltest, parameterized\n from jax._src import config\n from jax._src import test_util as jtu\n+from jax._src.interpreters import mlir\n+from jax._src.lib.mlir import ir\n+from jax.experimental.mosaic.gpu import dialect as mgpu_dialect  # pylint: disable=g-importing-member\n import jax.numpy as jnp\n+import numpy as np\n \n import hypothesis as hp\n import hypothesis.strategies as hps\n@@ -31,6 +35,7 @@\n   matmul = None\n else:\n   from jax.experimental.mosaic.gpu.examples import matmul\n+  from jax.experimental.mosaic.gpu.examples import matmul_blackwell\n \n \n config.parse_flags_with_absl()\n@@ -53,9 +58,13 @@ def setUp(self):\n     super().setUp()\n     if matmul is None:\n       self.skipTest(\"Mosaic GPU not available.\")\n-    if (not jtu.test_device_matches([\"cuda\"]) or\n-        not jtu.is_cuda_compute_capability_equal(\"9.0\")):\n-      self.skipTest(\"Only works on GPU with capability sm90a\")\n+    if not jtu.test_device_matches([\"cuda\"]):\n+      self.skipTest(\"Test needs a GPU device\")\n+    self.context = mlir.make_ir_context()\n+    mgpu_dialect.register_dialect(self.context)\n+    self.enter_context(config.traceback_filtering(\"off\"))\n+    self.enter_context(self.context)\n+    self.enter_context(ir.Location.unknown())\n \n   @parameterized.named_parameters(\n       (f\"_shard{i}\", i) for i in range(5)\n@@ -63,7 +72,10 @@ def setUp(self):\n   @seed_hypothesis\n   @hp.settings(max_examples=100)  # Add verbosity=hp.Verbosity.verbose to debug\n   @hp.given(hps.data())\n-  def test_matmul(self, data):\n+  def test_matmul_sm90(self, data):\n+    if not jtu.is_cuda_compute_capability_equal(\"9.0\"):\n+      self.skipTest(\"Only works on GPU with capability sm90a\")\n+\n     in_dtype = data.draw(\n         hps.sampled_from([jnp.float16, jnp.bfloat16, jnp.float32]),\n         label=\"in_dtype\",\n@@ -122,6 +134,66 @@ def test_matmul(self, data):\n         hp.assume(False)\n       raise e\n \n+  @parameterized.named_parameters(\n+      # TODO(apaszke): Increase shard count once we have more B200s in CI.\n+      (f\"_shard{i}\", i) for i in range(1)\n+  )\n+  @seed_hypothesis\n+  @hp.settings(max_examples=100)  # Add verbosity=hp.Verbosity.verbose to debug\n+  @hp.given(hps.data())\n+  def test_matmul_sm100(self, data):\n+    if not jtu.is_cuda_compute_capability_equal(\"10.0\"):\n+      self.skipTest(\"Only works on GPU with capability sm100a\")\n+\n+    dtype = data.draw(\n+        hps.sampled_from([jnp.float16, jnp.bfloat16]),\n+        label=\"dtype\",\n+    )\n+    m, n, k = (\n+        data.draw(hps.sampled_from([128, 256, 512, 2048, 8192]), label=d) for d in \"mnk\"\n+    )\n+    max_concurrent_steps = data.draw(\n+        hps.integers(2, 5), label=\"max_concurrent_steps\"\n+    )\n+    collective = data.draw(hps.booleans(), label=\"collective\")\n+    num_ctas = 2 if collective else 1\n+    hp.assume(not (m == 128 and collective))  # Too small for collective MMA.\n+    tile_m = data.draw(\n+        hps.sampled_from([t for t in [128] if t * num_ctas <= m]), label=\"tile_m\"\n+    )\n+    tile_n = data.draw(\n+        hps.sampled_from([t for t in [64, 128, 256] if t * num_ctas <= n]), label=\"tile_n\"\n+    )\n+    grid_m = m // (num_ctas * tile_m)\n+    grid_tile_m = data.draw(hps.sampled_from([1, 2, 4, 8, 16]), label=\"grid_tile_m\")\n+    hp.assume(grid_m % grid_tile_m == 0)\n+\n+    try:\n+      kernel = matmul_blackwell.build_kernel(\n+          m,\n+          k,\n+          n,\n+          dtype=dtype,\n+          tile_m=tile_m,\n+          tile_n=tile_n,\n+          grid_tile_m=grid_tile_m,\n+          max_concurrent_steps=max_concurrent_steps,\n+          collective=collective,\n+      )\n+    except ValueError as e:\n+      if \"Mosaic GPU kernel exceeds available shared memory\" in str(e):\n+        hp.assume(False)\n+      raise\n+\n+    ka, kb = jax.random.split(jax.random.key(0), 2)\n+    a = jax.random.normal(key=ka, shape=(m, k), dtype=dtype)\n+    b = jax.random.normal(key=kb, shape=(n, k), dtype=dtype)\n+    out = kernel(a, b)\n+    out_ref = jnp.dot(a, b.T)\n+    np.testing.assert_allclose(\n+        out, out_ref, atol=1e-3, rtol=1e-3 if k < 512 else 1e-2\n+    )\n+\n \n if __name__ == \"__main__\":\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "#sdy remove redundant call to sdy-round-trip-export in JAX export.\n\nWe already call `xla::sdy::addSdyRoundTripExportPipeline` in `xla::SerializeUsingVersionedStablehlo` so no need for this anymore.\n\nPiperOrigin-RevId: 763762358",
        "output": [
            "```diff\nCommit: a57b4a1583e9b67f2520b76f7e5c466f3c5ff99b\nDate: 2025-05-27T13:18:26Z\nURL: https://github.com/jax-ml/jax/commit/a57b4a1583e9b67f2520b76f7e5c466f3c5ff99b\nFiles changed: 1\nAdditions: +3, Deletions: -7\ndiff --git a/jax/_src/export/_export.py b/jax/_src/export/_export.py\nindex 189818541a2c..b390574c0a79 100644\n--- a/jax/_src/export/_export.py\n+++ b/jax/_src/export/_export.py\n@@ -694,7 +694,7 @@ def _export_lowered(\n   shardy_enabled = _jax.sdy.lowered_with_shardy(\n       mlir.module_to_bytecode(mlir_module))\n \n-  mlir_module_serialized = _module_to_bytecode(mlir_module, shardy_enabled)\n+  mlir_module_serialized = _module_to_bytecode(mlir_module)\n \n   # Figure out the result types and shapes\n   if \"global_out_avals\" in lowering.compile_args:\n@@ -808,12 +808,8 @@ def _get_exported_vjp(exp_primal: Exported) -> Exported:\n       calling_convention_version=version,\n       _get_vjp=_get_exported_vjp)\n \n-def _module_to_bytecode(module: ir.Module, shardy_enabled: bool) -> bytes:\n-  if shardy_enabled:\n-    mlir_str = _jax.sdy.sdy_round_trip_export_pipeline(\n-        mlir.module_to_bytecode(module))\n-  else:\n-    mlir_str = mlir.module_to_bytecode(module)\n+def _module_to_bytecode(module: ir.Module) -> bytes:\n+  mlir_str = mlir.module_to_bytecode(module)\n   # `target_version` is used to manage situations when a StableHLO producer\n   # and a StableHLO consumer were built using different versions of StableHLO.\n   #\n\n```"
        ],
        "from_id": [
            "bartchr808",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas] Require parallel dimensions to form a prefix of the grid in TPU interpret mode.\n\nSince dimensions with parallel semantics must now appear as the leading dimensions of the grid, this CL also makes the sequential iteration over cores in the simulation never re-visit a core after the simulation has moved on to the next core. This enables the simulation to correctly omit loads and stores of kernel buffers if the same (slice of a) buffer is processed by multiple kernel invocations on the same core.\n\nPiperOrigin-RevId: 763737647",
        "output": [
            "```diff\nCommit: 8124cb64dc7329fa348da82a0e92f0731dccae0a\nDate: 2025-05-27T11:56:59Z\nURL: https://github.com/jax-ml/jax/commit/8124cb64dc7329fa348da82a0e92f0731dccae0a\nFiles changed: 2\nAdditions: +385, Deletions: -109\ndiff --git a/jax/_src/pallas/mosaic/interpret.py b/jax/_src/pallas/mosaic/interpret.py\nindex 4258e52e6541..f3a165105640 100644\n--- a/jax/_src/pallas/mosaic/interpret.py\n+++ b/jax/_src/pallas/mosaic/interpret.py\n@@ -86,18 +86,23 @@ class TPUInterpretParams:\n       replaced with arrays all of `jnp.inf`. Additionaly any floating point\n       operands to any operation will be replaced with (arrays of) `jnp.inf`.\n       Default: False.\n-    uninitialized_memory: If \"nan\", allocated buffers are initialized to\n-      contain all NaNs (or to their maximum possible value for integers). If\n-      \"zero\", allocated buffers are initialized to all zeros.\n+    uninitialized_memory: If \"nan\", allocated buffers are initialized to contain\n+      all NaNs (or to their maximum possible value for integers). If \"zero\",\n+      allocated buffers are initialized to all zeros.\n       Default: \"nan\".\n     random_seed: Seed for random number generator used during interpretation.\n       Currently random numbers are used to randomize the grid coordinates along\n       dimensions with 'parallel' semantics.\n       Default: None.\n     grid_point_recorder: Callback that is invoked by the interpreter for each\n-      grid point in the order in which the grid points are traversed. This is\n-      intended for inspecting the randomization of coordinates along grid\n-      dimensions with 'parallel' semantics.\n+      grid point in the order in which the grid points are traversed. The\n+      callback is invoked with two arguments:\n+        - A tuple of grid coordinates.\n+        - The local core ID of the core that is processing the grid point.\n+      This callback is intended for inspecting\n+        - the randomization of coordinates along grid dimensions with 'parallel'\n+          semantics and\n+        - the mapping of grid points to local (i.e. per-device) cores.\n       Default: None.\n     num_cores_per_device: The number of cores per device.\n       Default: 1.\n@@ -107,7 +112,9 @@ class TPUInterpretParams:\n   skip_floating_point_ops: bool = False\n   uninitialized_memory: Literal[\"nan\", \"zero\"] = \"nan\"\n   random_seed: int | None = None\n-  grid_point_recorder: Callable[[tuple[jnp.int32, ...]], None] | None = None\n+  grid_point_recorder: (\n+      Callable[[tuple[np.int32, ...], np.int32], None] | None\n+  ) = None\n   num_cores_per_device: int = 1\n \n \n@@ -1752,11 +1759,45 @@ def _get_mosaic_params(compiler_params: dict[str, pallas_core.CompilerParams]) -\n def _get_parallel_dim_semantics(\n     compiler_params: dict[str, Any], num_dimensions_in_grid: int,\n ) -> tuple[bool, ...]:\n-  \"\"\"Returns a tuple of booleans indicating whether the corresponding dimension in the grid is parallel.\"\"\"\n+  \"\"\"Returns a tuple indicating which grid dimensions have parallel semantics.\n+\n+  Args:\n+    compiler_params: Representation of a `mosaic_core.TPUCompilerParams` object\n+      as a dictionary.\n+    num_dimensions_in_grid: The number of dimensions in the grid.\n+\n+  Returns:\n+    A tuple of booleans where the entry at index `i` is `True` precisely if the\n+    `i`-th dimension in the grid has parallel semantics.\n+\n+  Raises:\n+    ValueError: If the dimensions with parallel semantics do not form a prefix\n+      of the grid.\n+  \"\"\"\n   mosaic_params = _get_mosaic_params(compiler_params)\n   if mosaic_params.dimension_semantics is None:\n     return (False,) * num_dimensions_in_grid\n-  return tuple(ds == 'parallel' for ds in mosaic_params.dimension_semantics)\n+  result = tuple(ds == 'parallel' for ds in mosaic_params.dimension_semantics)\n+  for ds0, ds1 in zip(result[:-1], result[1:]):\n+    if ds1 and not ds0:\n+      raise ValueError(\n+          'Dimensions with parallel semantics must form a prefix of the grid.'\n+      )\n+  return result\n+\n+\n+def _get_parallel_subgrid_size(\n+    parallel_semantics_per_dim: tuple[bool, ...], grid: tuple[int, ...]\n+) -> int:\n+  \"\"\"Returns the size of the subgrid along the parallel dimensions.\"\"\"\n+  return functools.reduce(\n+      lambda x, y: x * y,\n+      (\n+          dim_size if parallel_dim else 1\n+          for dim_size, parallel_dim in zip(grid, parallel_semantics_per_dim)\n+      ),\n+      1,\n+  )\n \n _GridPointCoordinatesPerDim = tuple[Array, ...]\n \n@@ -1836,24 +1877,6 @@ def _get_grid_point(\n     grid_point.append(li if jnp.size(coords) == 0 else coords[li])\n   return jnp.array(grid_point, dtype=np.int32)\n \n-\n-def _get_next_local_core_id(\n-    local_core_id: int,\n-    parallel_semantics_per_dim: tuple[bool, ...],\n-    grid_point: Array,\n-    next_grid_point: Array,\n-    interpret_params: TPUInterpretParams,\n-) -> int:\n-  delta = next_grid_point - grid_point\n-  assert delta.shape == (len(parallel_semantics_per_dim),)\n-  parallel_semantics_per_dim = jnp.array(parallel_semantics_per_dim)\n-  deltas_along_parallel_dims = jnp.where(parallel_semantics_per_dim, delta, 0)\n-  return jax.lax.cond(\n-      jnp.any(deltas_along_parallel_dims),\n-      lambda: (local_core_id + 1) % interpret_params.num_cores_per_device,\n-      lambda: local_core_id,\n-  )\n-\n def _uninitialized_value(shape, dtype, interpret_params):\n   if interpret_params.uninitialized_memory == 'nan':\n     if jnp.issubdtype(dtype, jnp.floating):\n@@ -2078,13 +2101,28 @@ def interpret_pallas_call(\n     # Base case is always one iteration when grid is ()\n     num_iterations = 1\n \n-  parallel_semantics_per_dim = _get_parallel_dim_semantics(\n-      compiler_params, len(grid)\n-  )\n   randomized_grid_coordinates = _get_randomized_grid_coordinates(\n       grid, compiler_params, interpret_params.random_seed  # type: ignore[arg-type]\n   )\n \n+  parallel_dim_semantics = _get_parallel_dim_semantics(\n+      compiler_params, len(grid)\n+  )\n+  parallel_subgrid_size = _get_parallel_subgrid_size(\n+      parallel_dim_semantics, grid  # type: ignore[arg-type]\n+  )\n+  num_points_in_parallel_subgrid_per_core = (\n+      parallel_subgrid_size + interpret_params.num_cores_per_device - 1\n+  ) // interpret_params.num_cores_per_device  # We round up here.\n+  num_iterations_per_point_in_parallel_subgrid = (\n+      # This is evenly divisible.\n+      num_iterations // parallel_subgrid_size  # type: ignore[operator]\n+  )\n+  num_iterations_per_core = (\n+      num_points_in_parallel_subgrid_per_core\n+      * num_iterations_per_point_in_parallel_subgrid\n+  )\n+\n   def _get_local_grid_env(loop_idx):\n     if grid_mapping.local_grid_env is not None:\n       return grid_mapping.local_grid_env(loop_idx, grid)\n@@ -2153,20 +2191,20 @@ def body(\n         cur_start_indices,\n     ) = carry\n     if interpret_params.grid_point_recorder is not None:\n-      callback.io_callback(interpret_params.grid_point_recorder, (), grid_point)\n+      callback.io_callback(\n+          interpret_params.grid_point_recorder,\n+          (),\n+          grid_point,\n+          cur_local_core_id,\n+      )\n+\n+    next_local_core_id = (iteration_idx + 1) // num_iterations_per_core\n \n     with pallas_core.grid_env(_get_local_grid_env(loop_idx)):\n       next_loop_idx = _get_next_indices(grid, loop_idx)\n       next_grid_point = _get_grid_point(\n           next_loop_idx, randomized_grid_coordinates\n       )\n-      next_local_core_id = _get_next_local_core_id(\n-          cur_local_core_id,\n-          parallel_semantics_per_dim,\n-          grid_point,\n-          next_grid_point,\n-          interpret_params,\n-      )\n       next_start_indices = [\n           _compute_start_indices(\n               bm,\n@@ -2178,8 +2216,8 @@ def body(\n           )\n           for bm in grid_mapping.block_mappings\n       ]\n-      # Copy slices of the input to the kernel buffers.\n \n+      # Copy slices of the input to the kernel buffers.\n       def _store_slice_to_kernel_input(index, input_var):\n         # Copy from the HBM buffer for the pallas_call input to the kernel\n         # input buffer.\ndiff --git a/tests/pallas/tpu_pallas_interpret_test.py b/tests/pallas/tpu_pallas_interpret_test.py\nindex 014667d5f948..47d4ba3e1acf 100644\n--- a/tests/pallas/tpu_pallas_interpret_test.py\n+++ b/tests/pallas/tpu_pallas_interpret_test.py\n@@ -18,6 +18,8 @@\n contains only tests that do not use shard_map.\n \"\"\"\n \n+from collections.abc import Callable\n+import dataclasses\n import functools\n \n from absl.testing import absltest\n@@ -59,11 +61,18 @@ def num_stores(self):\n     return self._num_stores\n \n \n+@dataclasses.dataclass(frozen=True)\n+class ProcessedGridPoint():\n+  \"\"\"Represents a grid point and the ID of the core that has processed it.\"\"\"\n+  grid_point: tuple[int, ...]\n+  core_id: int\n+\n+\n class GridPointRecorderContext(object):\n-  \"\"\"Records grid points in the order in which they are traversed.\"\"\"\n+  \"\"\"Records grid points in the order in which they are procsessed.\"\"\"\n \n   def __init__(self):\n-    self._grid_points = []\n+    self._grid_points: list[ProcessedGridPoint] = []\n \n   def __enter__(self):\n     return self\n@@ -71,14 +80,17 @@ def __enter__(self):\n   def __exit__(self, ty, value, traceback):\n     ...\n \n-  def get_recorder(self):\n-    def _recorder(grid_point):\n-      self._grid_points.append(grid_point)\n+  def get_recorder(self) -> Callable[[tuple[np.int32, ...], np.int32], None]:\n+    def _recorder(grid_point, core_id):\n+      processed_grid_point = ProcessedGridPoint(\n+          tuple(int(coord) for coord in grid_point), int(core_id)\n+      )\n+      self._grid_points.append(processed_grid_point)\n \n     return _recorder\n \n   @property\n-  def grid_points(self):\n+  def grid_points(self) -> list[ProcessedGridPoint]:\n     return self._grid_points\n \n \n@@ -359,7 +371,7 @@ def kernel(s_ref, o_ref):\n       s_ref[0] = s + 1\n       o_ref[:] = jax.lax.full_like(o_ref, s)\n \n-    def kernel_call_dimensions_arbitrary_parallel(s, grid_point_recorder):\n+    def kernel_call_dimensions_parallel_arbitrary(s, grid_point_recorder):\n       return pl.pallas_call(\n           kernel,\n           out_shape=jax.ShapeDtypeStruct((32, 512), jnp.float32),\n@@ -370,13 +382,13 @@ def kernel_call_dimensions_arbitrary_parallel(s, grid_point_recorder):\n               random_seed=12345, grid_point_recorder=grid_point_recorder\n           ),\n           compiler_params=pltpu.TPUCompilerParams(\n-              dimension_semantics=('arbitrary', 'parallel')\n+              dimension_semantics=('parallel', 'arbitrary')\n           ),\n       )(s)\n \n     with GridPointRecorderContext() as grid_point_recorder:\n       result = jax.jit(\n-          kernel_call_dimensions_arbitrary_parallel, static_argnums=1\n+          kernel_call_dimensions_parallel_arbitrary, static_argnums=1\n       )(\n           jnp.zeros((1,), jnp.int32),\n           grid_point_recorder.get_recorder(),\n@@ -384,85 +396,55 @@ def kernel_call_dimensions_arbitrary_parallel(s, grid_point_recorder):\n       np.testing.assert_allclose(\n           result[::8, ::128],\n           [\n-              [ 2.0,  3.0,  0.0,  1.0],\n-              [ 6.0,  7.0,  4.0,  5.0],\n-              [10.0, 11.0,  8.0,  9.0],\n-              [14.0, 15.0, 12.0, 13.0],\n+              [ 8.0,  9.0, 10.0, 11.0],\n+              [12.0, 13.0, 14.0, 15.0],\n+              [ 0.0,  1.0,  2.0,  3.0],\n+              [ 4.0,  5.0,  6.0,  7.0],\n           ],\n       )\n-      np.testing.assert_array_equal(\n+      self.assertListEqual(\n           grid_point_recorder.grid_points,\n           [\n-              [0, 2],\n-              [0, 3],\n-              [0, 0],\n-              [0, 1],\n-              [1, 2],\n-              [1, 3],\n-              [1, 0],\n-              [1, 1],\n-              [2, 2],\n-              [2, 3],\n-              [2, 0],\n-              [2, 1],\n-              [3, 2],\n-              [3, 3],\n-              [3, 0],\n-              [3, 1],\n+              ProcessedGridPoint((2, 0), 0),\n+              ProcessedGridPoint((2, 1), 0),\n+              ProcessedGridPoint((2, 2), 0),\n+              ProcessedGridPoint((2, 3), 0),\n+              ProcessedGridPoint((3, 0), 0),\n+              ProcessedGridPoint((3, 1), 0),\n+              ProcessedGridPoint((3, 2), 0),\n+              ProcessedGridPoint((3, 3), 0),\n+              ProcessedGridPoint((0, 0), 0),\n+              ProcessedGridPoint((0, 1), 0),\n+              ProcessedGridPoint((0, 2), 0),\n+              ProcessedGridPoint((0, 3), 0),\n+              ProcessedGridPoint((1, 0), 0),\n+              ProcessedGridPoint((1, 1), 0),\n+              ProcessedGridPoint((1, 2), 0),\n+              ProcessedGridPoint((1, 3), 0),\n           ],\n       )\n \n-    def kernel_call_dimensions_parallel_arbitrary(s, grid_point_recorder):\n+  def test_dimensions_arbitrary_parallel_raises(self):\n+    def kernel_call(s):\n+      def kernel(s_ref, o_ref):\n+        s = s_ref[0]\n+        o_ref[0] = s\n+\n       return pl.pallas_call(\n           kernel,\n           out_shape=jax.ShapeDtypeStruct((32, 512), jnp.float32),\n           grid=(4, 4),\n           in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n           out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n-          interpret=mosaic_interpret.TPUInterpretParams(\n-              random_seed=12345, grid_point_recorder=grid_point_recorder\n-          ),\n+          interpret=mosaic_interpret.TPUInterpretParams(random_seed=12345),\n           compiler_params=pltpu.TPUCompilerParams(\n-              dimension_semantics=('parallel', 'arbitrary')\n+              dimension_semantics=('arbitrary', 'parallel')\n           ),\n       )(s)\n \n-    with GridPointRecorderContext() as grid_point_recorder:\n-      result = jax.jit(\n-          kernel_call_dimensions_parallel_arbitrary, static_argnums=1\n-      )(\n+    with self.assertRaises(ValueError):\n+      jax.jit(kernel_call)(\n           jnp.zeros((1,), jnp.int32),\n-          grid_point_recorder.get_recorder(),\n-      )\n-      np.testing.assert_allclose(\n-          result[::8, ::128],\n-          [\n-              [ 8.0,  9.0, 10.0, 11.0],\n-              [12.0, 13.0, 14.0, 15.0],\n-              [ 0.0,  1.0,  2.0,  3.0],\n-              [ 4.0,  5.0,  6.0,  7.0],\n-          ],\n-      )\n-      np.testing.assert_array_equal(\n-          grid_point_recorder.grid_points,\n-          [\n-              [2, 0],\n-              [2, 1],\n-              [2, 2],\n-              [2, 3],\n-              [3, 0],\n-              [3, 1],\n-              [3, 2],\n-              [3, 3],\n-              [0, 0],\n-              [0, 1],\n-              [0, 2],\n-              [0, 3],\n-              [1, 0],\n-              [1, 1],\n-              [1, 2],\n-              [1, 3],\n-          ],\n       )\n \n   def test_dynamic_parallel_dimension_raises(self):\n@@ -583,6 +565,262 @@ def kernel(x_ref, o_ref, vmem_ref):\n     self.assertFalse(mosaic_interpret.races.races_found)\n     np.testing.assert_allclose(y, 2.0 * x)\n \n+  def test_parallel_dimension_and_multiple_cores(self):\n+    def kernel(s_ref, o_ref):\n+      s = s_ref[0]\n+      s_ref[0] = s + 1\n+      o_ref[:] = jax.lax.full_like(o_ref, s)\n+\n+    def kernel_call(s, num_cores_per_device, grid_point_recorder):\n+      return pl.pallas_call(\n+          kernel,\n+          out_shape=jax.ShapeDtypeStruct((32, 512), jnp.float32),\n+          grid=(4, 4),\n+          in_specs=[pl.BlockSpec(memory_space=pltpu.SMEM)],\n+          out_specs=pl.BlockSpec((8, 128), lambda i, j: (i, j)),\n+          interpret=mosaic_interpret.TPUInterpretParams(\n+              random_seed=12345,\n+              num_cores_per_device=num_cores_per_device,\n+              grid_point_recorder=grid_point_recorder,\n+          ),\n+          compiler_params=pltpu.TPUCompilerParams(\n+              dimension_semantics=('parallel', 'arbitrary')\n+          ),\n+      )(s)\n+\n+    with self.subTest('num_cores_per_device=1'):\n+      with GridPointRecorderContext() as grid_point_recorder:\n+        result = jax.jit(kernel_call, static_argnums=(1, 2))(\n+            jnp.zeros((1,), jnp.int32), 1, grid_point_recorder.get_recorder()\n+        )\n+        np.testing.assert_allclose(\n+            result[::8, ::128],\n+            [\n+                [8.0, 9.0, 10.0, 11.0],\n+                [12.0, 13.0, 14.0, 15.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [4.0, 5.0, 6.0, 7.0],\n+            ],\n+        )\n+        self.assertListEqual(\n+            grid_point_recorder.grid_points,\n+            # parallel_subgrid_size = 4\n+            # num_parallel_points_per_core = (4 + 1 - 1) // 1 = 4\n+            # num_iterations_per_core = 4 * (16 // 4) = 16\n+            [\n+                ProcessedGridPoint((2, 0), 0),\n+                ProcessedGridPoint((2, 1), 0),\n+                ProcessedGridPoint((2, 2), 0),\n+                ProcessedGridPoint((2, 3), 0),\n+                ProcessedGridPoint((3, 0), 0),\n+                ProcessedGridPoint((3, 1), 0),\n+                ProcessedGridPoint((3, 2), 0),\n+                ProcessedGridPoint((3, 3), 0),\n+                ProcessedGridPoint((0, 0), 0),\n+                ProcessedGridPoint((0, 1), 0),\n+                ProcessedGridPoint((0, 2), 0),\n+                ProcessedGridPoint((0, 3), 0),\n+                ProcessedGridPoint((1, 0), 0),\n+                ProcessedGridPoint((1, 1), 0),\n+                ProcessedGridPoint((1, 2), 0),\n+                ProcessedGridPoint((1, 3), 0),\n+            ],\n+        )\n+\n+    with self.subTest('num_cores_per_device=2'):\n+      with GridPointRecorderContext() as grid_point_recorder:\n+        result = jax.jit(kernel_call, static_argnums=(1, 2))(\n+            jnp.zeros((1,), jnp.int32), 2, grid_point_recorder.get_recorder()\n+        )\n+        np.testing.assert_allclose(\n+            result[::8, ::128],\n+            [\n+                [0.0, 1.0, 2.0, 3.0],\n+                [4.0, 5.0, 6.0, 7.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [4.0, 5.0, 6.0, 7.0],\n+            ],\n+        )\n+        self.assertListEqual(\n+            grid_point_recorder.grid_points,\n+            # parallel_subgrid_size = 4\n+            # num_parallel_points_per_core = (4 + 2 - 1) // 2 = 2\n+            # num_iterations_per_core = 2 * (16 // 4) = 8\n+            [\n+                ProcessedGridPoint((2, 0), 0),\n+                ProcessedGridPoint((2, 1), 0),\n+                ProcessedGridPoint((2, 2), 0),\n+                ProcessedGridPoint((2, 3), 0),\n+                ProcessedGridPoint((3, 0), 0),\n+                ProcessedGridPoint((3, 1), 0),\n+                ProcessedGridPoint((3, 2), 0),\n+                ProcessedGridPoint((3, 3), 0),\n+                ProcessedGridPoint((0, 0), 1),\n+                ProcessedGridPoint((0, 1), 1),\n+                ProcessedGridPoint((0, 2), 1),\n+                ProcessedGridPoint((0, 3), 1),\n+                ProcessedGridPoint((1, 0), 1),\n+                ProcessedGridPoint((1, 1), 1),\n+                ProcessedGridPoint((1, 2), 1),\n+                ProcessedGridPoint((1, 3), 1),\n+            ],\n+        )\n+\n+    with self.subTest('num_cores_per_device=3'):\n+      with GridPointRecorderContext() as grid_point_recorder:\n+        result = jax.jit(kernel_call, static_argnums=(1, 2))(\n+            jnp.zeros((1,), jnp.int32), 3, grid_point_recorder.get_recorder()\n+        )\n+        np.testing.assert_allclose(\n+            result[::8, ::128],\n+            [\n+                [0.0, 1.0, 2.0, 3.0],\n+                [4.0, 5.0, 6.0, 7.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [4.0, 5.0, 6.0, 7.0],\n+            ],\n+        )\n+        self.assertListEqual(\n+            grid_point_recorder.grid_points,\n+            # parallel_subgrid_size = 4\n+            # num_parallel_points_per_core = (4 + 3 - 1) // 3 = 2\n+            # num_iterations_per_core = 2 * (16 // 4) = 8\n+            [\n+                ProcessedGridPoint((2, 0), 0),\n+                ProcessedGridPoint((2, 1), 0),\n+                ProcessedGridPoint((2, 2), 0),\n+                ProcessedGridPoint((2, 3), 0),\n+                ProcessedGridPoint((3, 0), 0),\n+                ProcessedGridPoint((3, 1), 0),\n+                ProcessedGridPoint((3, 2), 0),\n+                ProcessedGridPoint((3, 3), 0),\n+                ProcessedGridPoint((0, 0), 1),\n+                ProcessedGridPoint((0, 1), 1),\n+                ProcessedGridPoint((0, 2), 1),\n+                ProcessedGridPoint((0, 3), 1),\n+                ProcessedGridPoint((1, 0), 1),\n+                ProcessedGridPoint((1, 1), 1),\n+                ProcessedGridPoint((1, 2), 1),\n+                ProcessedGridPoint((1, 3), 1),\n+            ],\n+        )\n+\n+    with self.subTest('num_cores_per_device=4'):\n+      with GridPointRecorderContext() as grid_point_recorder:\n+        result = jax.jit(kernel_call, static_argnums=(1, 2))(\n+            jnp.zeros((1,), jnp.int32), 4, grid_point_recorder.get_recorder()\n+        )\n+        np.testing.assert_allclose(\n+            result[::8, ::128],\n+            [\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+            ],\n+        )\n+        self.assertListEqual(\n+            grid_point_recorder.grid_points,\n+            # parallel_subgrid_size = 4\n+            # num_parallel_points_per_core = (4 + 4 - 1) // 4 = 1\n+            # num_iterations_per_core = 1 * (16 // 4) = 4\n+            [\n+                ProcessedGridPoint((2, 0), 0),\n+                ProcessedGridPoint((2, 1), 0),\n+                ProcessedGridPoint((2, 2), 0),\n+                ProcessedGridPoint((2, 3), 0),\n+                ProcessedGridPoint((3, 0), 1),\n+                ProcessedGridPoint((3, 1), 1),\n+                ProcessedGridPoint((3, 2), 1),\n+                ProcessedGridPoint((3, 3), 1),\n+                ProcessedGridPoint((0, 0), 2),\n+                ProcessedGridPoint((0, 1), 2),\n+                ProcessedGridPoint((0, 2), 2),\n+                ProcessedGridPoint((0, 3), 2),\n+                ProcessedGridPoint((1, 0), 3),\n+                ProcessedGridPoint((1, 1), 3),\n+                ProcessedGridPoint((1, 2), 3),\n+                ProcessedGridPoint((1, 3), 3),\n+            ],\n+        )\n+\n+    with self.subTest('num_cores_per_device=5'):\n+      with GridPointRecorderContext() as grid_point_recorder:\n+        result = jax.jit(kernel_call, static_argnums=(1, 2))(\n+            jnp.zeros((1,), jnp.int32), 5, grid_point_recorder.get_recorder()\n+        )\n+        np.testing.assert_allclose(\n+            result[::8, ::128],\n+            [\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+            ],\n+        )\n+        self.assertListEqual(\n+            grid_point_recorder.grid_points,\n+            # parallel_subgrid_size = 4\n+            # num_parallel_points_per_core = (4 + 5 - 1) // 5 = 1\n+            # num_iterations_per_core = 1 * (16 // 4) = 4\n+            [\n+                ProcessedGridPoint((2, 0), 0),\n+                ProcessedGridPoint((2, 1), 0),\n+                ProcessedGridPoint((2, 2), 0),\n+                ProcessedGridPoint((2, 3), 0),\n+                ProcessedGridPoint((3, 0), 1),\n+                ProcessedGridPoint((3, 1), 1),\n+                ProcessedGridPoint((3, 2), 1),\n+                ProcessedGridPoint((3, 3), 1),\n+                ProcessedGridPoint((0, 0), 2),\n+                ProcessedGridPoint((0, 1), 2),\n+                ProcessedGridPoint((0, 2), 2),\n+                ProcessedGridPoint((0, 3), 2),\n+                ProcessedGridPoint((1, 0), 3),\n+                ProcessedGridPoint((1, 1), 3),\n+                ProcessedGridPoint((1, 2), 3),\n+                ProcessedGridPoint((1, 3), 3),\n+            ],\n+        )\n+\n+    with self.subTest('num_cores_per_device=6'):\n+      with GridPointRecorderContext() as grid_point_recorder:\n+        result = jax.jit(kernel_call, static_argnums=(1, 2))(\n+            jnp.zeros((1,), jnp.int32), 6, grid_point_recorder.get_recorder()\n+        )\n+        np.testing.assert_allclose(\n+            result[::8, ::128],\n+            [\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+                [0.0, 1.0, 2.0, 3.0],\n+            ],\n+        )\n+        self.assertListEqual(\n+            grid_point_recorder.grid_points,\n+            # parallel_subgrid_size = 4\n+            # num_parallel_points_per_core = (4 + 6 - 1) // 6 = 1\n+            # num_iterations_per_core = 1 * (16 // 4) = 4\n+            [\n+                ProcessedGridPoint((2, 0), 0),\n+                ProcessedGridPoint((2, 1), 0),\n+                ProcessedGridPoint((2, 2), 0),\n+                ProcessedGridPoint((2, 3), 0),\n+                ProcessedGridPoint((3, 0), 1),\n+                ProcessedGridPoint((3, 1), 1),\n+                ProcessedGridPoint((3, 2), 1),\n+                ProcessedGridPoint((3, 3), 1),\n+                ProcessedGridPoint((0, 0), 2),\n+                ProcessedGridPoint((0, 1), 2),\n+                ProcessedGridPoint((0, 2), 2),\n+                ProcessedGridPoint((0, 3), 2),\n+                ProcessedGridPoint((1, 0), 3),\n+                ProcessedGridPoint((1, 1), 3),\n+                ProcessedGridPoint((1, 2), 3),\n+                ProcessedGridPoint((1, 3), 3),\n+            ],\n+        )\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move jax/_src/custom_derivatives.py to its own BUILD rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nThis was unblocked by moving ad, batching, and custom_transpose to their own rules in prior changes. It required one small code refactoring: moving an effects registration to the location where the effect is defined.\n\nPiperOrigin-RevId: 763736189",
        "output": [
            "```diff\nCommit: c13de5cb83c33c7c4d9a3bbbdfd35478e5bc91eb\nDate: 2025-05-27T11:50:50Z\nURL: https://github.com/jax-ml/jax/commit/c13de5cb83c33c7c4d9a3bbbdfd35478e5bc91eb\nFiles changed: 8\nAdditions: +31, Deletions: -4\ndiff --git a/jax/BUILD b/jax/BUILD\nindex a236cd206a55..396e6fdf6ed4 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -303,7 +303,6 @@ py_library_providing_imports_info(\n         \"_src/callback.py\",\n         \"_src/checkify.py\",\n         \"_src/custom_batching.py\",\n-        \"_src/custom_derivatives.py\",\n         \"_src/custom_partitioning.py\",\n         \"_src/custom_partitioning_sharding_rule.py\",\n         \"_src/debugging.py\",\n@@ -388,6 +387,7 @@ py_library_providing_imports_info(\n         \":core\",\n         \":custom_api_util\",\n         \":custom_dce\",\n+        \":custom_derivatives\",\n         \":custom_transpose\",\n         \":deprecations\",\n         \":dtypes\",\n@@ -613,6 +613,30 @@ pytype_strict_library(\n     ],\n )\n \n+pytype_strict_library(\n+    name = \"custom_derivatives\",\n+    srcs = [\"_src/custom_derivatives.py\"],\n+    deps = [\n+        \":ad\",\n+        \":ad_util\",\n+        \":api_util\",\n+        \":batching\",\n+        \":config\",\n+        \":core\",\n+        \":custom_api_util\",\n+        \":custom_transpose\",\n+        \":dtypes\",\n+        \":effects\",\n+        \":mlir\",\n+        \":partial_eval\",\n+        \":state_types\",\n+        \":traceback_util\",\n+        \":tree_util\",\n+        \":util\",\n+        \":xla\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"custom_transpose\",\n     srcs = [\"_src/custom_transpose.py\"],\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex d76d145fd0a6..2a09665f6285 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -40,7 +40,6 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.interpreters import xla\n from jax._src.interpreters.batching import not_mapped\n-from jax._src.lax import lax\n from jax._src.tree_util import (\n     tree_flatten, tree_unflatten, tree_map, treedef_is_leaf, treedef_tuple,\n     register_pytree_node_class, tree_leaves, tree_flatten_with_path,\n@@ -410,8 +409,6 @@ def jvp(*xs):\n     return [*out_primals, *out_tangents]\n   return lu.wrap_init(jvp, debug_info=jvp_jaxpr_fun.debug_info)\n \n-effects.custom_derivatives_allowed_effects.add_type(lax.InOutFeedEffect)\n-\n custom_jvp_call_p = CustomJVPCallPrimitive('custom_jvp_call')\n \n def _custom_jvp_call_typecheck(_, *in_avals, call_jaxpr, jvp_jaxpr_fun,\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex a49c27d06eee..a9d81c684297 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -8188,6 +8188,7 @@ class InOutFeedEffect(effects.Effect):\n infeed_effect = InOutFeedEffect()\n outfeed_effect = InOutFeedEffect()\n \n+effects.custom_derivatives_allowed_effects.add_type(InOutFeedEffect)\n \n def infeed(token, shape=None, partitions=None):\n   \"\"\"Consumes an infeed value of `shape` from the host. Experimental.\ndiff --git a/jax/_src/pallas/fuser/BUILD b/jax/_src/pallas/fuser/BUILD\nindex a62a9937d91d..a4c3402f5309 100644\n--- a/jax/_src/pallas/fuser/BUILD\n+++ b/jax/_src/pallas/fuser/BUILD\n@@ -50,6 +50,7 @@ pytype_strict_library(\n         \"//jax:ad_util\",\n         \"//jax:api_util\",\n         \"//jax:core\",\n+        \"//jax:custom_derivatives\",\n         \"//jax:partial_eval\",\n         \"//jax:tree_util\",\n         \"//jax:util\",\ndiff --git a/jax/_src/pallas/mosaic/BUILD b/jax/_src/pallas/mosaic/BUILD\nindex fdd3a56ac7c8..83525f11d3cf 100644\n--- a/jax/_src/pallas/mosaic/BUILD\n+++ b/jax/_src/pallas/mosaic/BUILD\n@@ -103,6 +103,7 @@ py_library(\n         \"//jax\",\n         \"//jax:ad_util\",\n         \"//jax:core\",\n+        \"//jax:custom_derivatives\",\n         \"//jax:dtypes\",\n         \"//jax:mesh\",\n         \"//jax:mlir\",\ndiff --git a/jax/_src/pallas/triton/BUILD b/jax/_src/pallas/triton/BUILD\nindex 2b8ee4eaa8f2..acbc11a60039 100644\n--- a/jax/_src/pallas/triton/BUILD\n+++ b/jax/_src/pallas/triton/BUILD\n@@ -63,6 +63,7 @@ pytype_strict_library(\n         \"//jax:api_util\",\n         \"//jax:config\",\n         \"//jax:core\",\n+        \"//jax:custom_derivatives\",\n         \"//jax:mlir\",\n         \"//jax:partial_eval\",\n         \"//jax:source_info_util\",\ndiff --git a/jax/extend/BUILD b/jax/extend/BUILD\nindex 06fb8e671120..6dc5d7d76311 100644\n--- a/jax/extend/BUILD\n+++ b/jax/extend/BUILD\n@@ -46,6 +46,7 @@ py_library_providing_imports_info(\n         \"//jax:ad\",\n         \"//jax:ad_util\",\n         \"//jax:core\",\n+        \"//jax:custom_derivatives\",\n     ],\n )\n \ndiff --git a/tests/BUILD b/tests/BUILD\nindex 6fac61933d59..e3672eb73f48 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -49,6 +49,7 @@ jax_multiplatform_test(\n     srcs = [\"custom_api_test.py\"],\n     shard_count = 10,\n     deps = [\n+        \"//jax:custom_derivatives\",\n         \"//jax:experimental\",\n     ] + py_deps([\n         \"absl/testing\",\n\n```"
        ],
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] `Barrier` and `ClusterBarrier` are now `kw_only=True`\n\nPiperOrigin-RevId: 763730217",
        "output": [
            "```diff\nCommit: 4f717d31c5b94bcfb742d1a9aaf0024cca7ccd6c\nDate: 2025-05-27T11:30:22Z\nURL: https://github.com/jax-ml/jax/commit/4f717d31c5b94bcfb742d1a9aaf0024cca7ccd6c\nFiles changed: 4\nAdditions: +35, Deletions: -35\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 08e47cec4b2e..2fca1464ee0b 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -850,7 +850,7 @@ def __str__(self):\n     return self.name\n \n \n-@dataclasses.dataclass(frozen=True)\n+@dataclasses.dataclass(frozen=True, kw_only=True)\n class Barrier:\n   \"\"\"Describes a barrier Ref.\n \n@@ -862,9 +862,9 @@ class Barrier:\n       the tensor core. This should be set to True when waiting on Blackwell\n       (TC Gen 5) asynchoronous matmul instructions.\n   \"\"\"\n-  num_arrivals: int\n+  num_arrivals: int = 1\n   num_barriers: int = 1\n-  for_tensor_core: bool = dataclasses.field(default=False, kw_only=True)\n+  for_tensor_core: bool = False\n \n   def get_ref_aval(self) -> AbstractMemoryRef:\n     aval = jax_core.ShapedArray(\n@@ -879,7 +879,7 @@ def __post_init__(self):\n           f\"Num arrivals must be at least 1, but got {self.num_arrivals}\"\n       )\n \n-@dataclasses.dataclass(frozen=True)\n+@dataclasses.dataclass(frozen=True, kw_only=True)\n class ClusterBarrier:\n   collective_axes: tuple[str | tuple[str, ...], ...]\n   num_barriers: int = 1\ndiff --git a/jax/_src/pallas/mosaic_gpu/pipeline.py b/jax/_src/pallas/mosaic_gpu/pipeline.py\nindex 4966eec7fa6d..a7f8d32677b0 100644\n--- a/jax/_src/pallas/mosaic_gpu/pipeline.py\n+++ b/jax/_src/pallas/mosaic_gpu/pipeline.py\n@@ -230,7 +230,7 @@ def pipeline(*gmem_refs: pallas_core.AbstractMemoryRef):\n         ],\n         [len(in_specs)],\n     )\n-    arrival_count = sum(map(_in_smem, in_specs))\n+    num_arrivals = sum(map(_in_smem, in_specs))\n     return pl.run_scoped(\n         functools.partial(\n             scoped_pipeline,\n@@ -240,10 +240,10 @@ def pipeline(*gmem_refs: pallas_core.AbstractMemoryRef):\n         in_smem_refs=in_smem_refs,\n         out_smem_refs=out_smem_refs,\n         barrier_ref=None\n-        if arrival_count == 0\n+        if num_arrivals == 0\n         else gpu_core.Barrier(\n             # TODO(slebedev): Change this to arrive only once.\n-            arrival_count,\n+            num_arrivals=num_arrivals,\n             num_barriers=max_concurrent_steps,\n         ),\n     )\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex c7e9f95e3f99..447e3affd7c1 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -278,9 +278,9 @@ def entry(q_ref, k_ref, v_ref, out_ref, lse_ref):\n         lambda *args: kernel(q_ref, k_ref, v_ref, out_ref, lse_ref, args),\n         scratch,\n         (\n-            plgpu.Barrier(1, num_barriers=max_concurrent_steps),\n-            plgpu.Barrier(1, num_barriers=max_concurrent_steps),\n-            plgpu.Barrier(1, num_barriers=compute_wgs),\n+            plgpu.Barrier(num_barriers=max_concurrent_steps),\n+            plgpu.Barrier(num_barriers=max_concurrent_steps),\n+            plgpu.Barrier(num_barriers=compute_wgs),\n         ),\n         (plgpu.Barrier(num_arrivals=compute_wgs, num_barriers=max_concurrent_steps),) * 2,\n         plgpu.Barrier(num_arrivals=compute_wgs),\n@@ -587,7 +587,7 @@ def compute_dk(acc_ref):\n       out_shape=q,\n       scratch_shapes=[\n           (q_scratch, do_scratch, lse_scratch, delta_scratch),  # type: ignore\n-          (plgpu.Barrier(1, num_barriers=compute_wgs),) * 4  # type: ignore\n+          (plgpu.Barrier(num_barriers=compute_wgs),) * 4  # type: ignore\n       ],\n       compiler_params=plgpu.CompilerParams(approx_math=True),\n       grid=(batch_size, num_q_tiles, num_q_heads),\n@@ -608,7 +608,7 @@ def compute_dk(acc_ref):\n     out_shape=[out_shape_kv, out_shape_kv],\n     scratch_shapes=[\n         (k_scratch, v_scratch),  # type: ignore\n-        (plgpu.Barrier(1, num_barriers=compute_wgs),) * 2  # type: ignore\n+        (plgpu.Barrier(num_barriers=compute_wgs),) * 2  # type: ignore\n   ],\n     compiler_params=plgpu.CompilerParams(approx_math=True),\n     grid=(batch_size, num_kv_tiles, num_q_heads),\n@@ -776,7 +776,7 @@ def compute_pv(acc_ref):\n             out_shape=out_shape,\n       scratch_shapes=(\n           tuple(smem_scratch),  # type: ignore\n-          plgpu.Barrier(1, num_barriers=compute_wgs),  # type: ignore\n+          plgpu.Barrier(num_barriers=compute_wgs),  # type: ignore\n           plgpu.Barrier(num_arrivals=compute_wgs),),  # type: ignore\n       compiler_params=plgpu.CompilerParams(\n           approx_math=True, lowering_semantics=plgpu.LoweringSemantics.Warpgroup,\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 7430fcc1ca53..608cfcba2465 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -408,7 +408,7 @@ def test_inline_mgpu(self):\n                 dtype,\n                 transforms=transforms,\n             ),\n-            plgpu.Barrier(num_arrivals=1),\n+            plgpu.Barrier(),\n         ],\n         out_specs=pl.BlockSpec(memory_space=plgpu.GMEM),\n     )\n@@ -540,7 +540,7 @@ def test_copy_gmem_to_smem(self, indexer):\n         in_specs=(pl.BlockSpec(memory_space=plgpu.GMEM),),\n         scratch_shapes=[\n             plgpu.SMEM((256,), jnp.float32),\n-            plgpu.Barrier(num_arrivals=1),\n+            plgpu.Barrier(),\n         ],\n     )\n     def kernel(x_ref_gmem, o_ref, scratch_ref, barrier_ref):\n@@ -586,7 +586,7 @@ def test_copy_gmem_to_smem_with_multiple_gmem_indexers(self, shape, indexers):\n         in_specs=(pl.BlockSpec(memory_space=plgpu.GMEM),),\n         scratch_shapes=[\n             plgpu.SMEM(shape, jnp.float32),\n-            plgpu.Barrier(num_arrivals=1),\n+            plgpu.Barrier(),\n         ],\n         grid=(1,),\n     )\n@@ -617,7 +617,7 @@ def test_gmem_to_smem_with_multiple_smem_indexers(self):\n         in_specs=(pl.BlockSpec(memory_space=plgpu.GMEM),),\n         scratch_shapes=[\n             plgpu.SMEM(x.shape, jnp.float32),\n-            plgpu.Barrier(num_arrivals=1),\n+            plgpu.Barrier(),\n         ],\n     )\n     def extract_x0(x_ref_gmem, o_ref, scratch_ref, barrier_ref):\n@@ -672,7 +672,7 @@ def test_copy_gmem_to_smem_with_indexed_barrier(self, indexer):\n         in_specs=(pl.BlockSpec(memory_space=plgpu.GMEM),),\n         scratch_shapes=[\n             plgpu.SMEM((128,), jnp.float32),\n-            plgpu.Barrier(num_arrivals=1, num_barriers=4),\n+            plgpu.Barrier(num_barriers=4),\n         ],\n     )\n     def kernel(x_ref_gmem, o_ref, scratch_ref, barrier_ref):\n@@ -713,7 +713,7 @@ def kernel(x_ref, o_ref, barrier_ref):\n         out_shape=jax.ShapeDtypeStruct([128, 128], jnp.float32),\n         in_specs=(in_spec,),\n         out_specs=out_spec,\n-        scratch_shapes=[plgpu.Barrier(num_arrivals=1)],\n+        scratch_shapes=[plgpu.Barrier()],\n     )\n     x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n     np.testing.assert_array_equal(f(x), x)\n@@ -736,7 +736,7 @@ def body(tmp_ref):\n         out_shape=jax.ShapeDtypeStruct([128, 128], jnp.float32),\n         in_specs=(in_spec,),\n         out_specs=out_spec,\n-        scratch_shapes=[plgpu.Barrier(num_arrivals=1)],\n+        scratch_shapes=[plgpu.Barrier()],\n     )\n     x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n     np.testing.assert_array_equal(f(x), x * 2)\n@@ -756,7 +756,7 @@ def body(tmp_ref):\n         kernel,\n         out_shape=jax.ShapeDtypeStruct([128, 128], jnp.float32),\n         in_specs=(in_spec,),\n-        scratch_shapes=[plgpu.Barrier(num_arrivals=1)],\n+        scratch_shapes=[plgpu.Barrier()],\n     )\n     x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n     np.testing.assert_array_equal(f(x), x * 2)\n@@ -783,7 +783,7 @@ def kernel(x_ref, o_ref, barrier_ref):\n         out_shape=jax.ShapeDtypeStruct([2, 128, 128], jnp.float32),\n         in_specs=(in_spec,),\n         out_specs=out_spec,\n-        scratch_shapes=[plgpu.Barrier(num_arrivals=1)],\n+        scratch_shapes=[plgpu.Barrier()],\n     )\n     x = jnp.arange(128 * 128, dtype=jnp.float32).reshape(128, 128)\n     np.testing.assert_array_equal(f(x), np.stack([x, x], axis=0))\n@@ -827,7 +827,7 @@ def kernel(x_ref, o_ref, barrier_ref):\n         out_shape=jax.ShapeDtypeStruct([2, 64, 2, 128], jnp.float32),\n         in_specs=(in_spec,),\n         out_specs=out_spec,\n-        scratch_shapes=[plgpu.Barrier(num_arrivals=1)],\n+        scratch_shapes=[plgpu.Barrier()],\n     )\n     x = jnp.arange(2 * 64 * 128, dtype=jnp.float32).reshape(2, 64, 128)\n     xt = x.transpose((1, 0, 2))\n@@ -847,7 +847,7 @@ def inner_body(scratch_ref):\n           plgpu.barrier_wait(barrier_ref)\n           o_ref[...] = scratch_ref[...] + 1\n         pl.run_scoped(inner_body, plgpu.SMEM((256,), jnp.float32))\n-      pl.run_scoped(body, plgpu.Barrier(num_arrivals=1))\n+      pl.run_scoped(body, plgpu.Barrier())\n \n     x = jnp.arange(256).astype(jnp.float32)\n     np.testing.assert_array_equal(kernel(x), x + 1.0)\n@@ -1016,7 +1016,7 @@ def test_get_swap_with_transforms(self, *transforms):\n         out_shape=jax.ShapeDtypeStruct(shape, jnp.int32),\n         scratch_shapes=[\n             plgpu.SMEM(shape, jnp.int32, transforms=tuple(transforms)),\n-            plgpu.Barrier(num_arrivals=1),\n+            plgpu.Barrier(),\n         ]\n     )\n     def kernel(x_ref, o_ref, scratch_ref, barrier_ref):\n@@ -1089,7 +1089,7 @@ def scoped_kernel(barrier_ref):\n         plgpu.barrier_wait(barrier_ref)\n \n       def branch():\n-        pl.run_scoped(scoped_kernel, plgpu.Barrier(num_arrivals=1))\n+        pl.run_scoped(scoped_kernel, plgpu.Barrier())\n \n       jax.lax.cond(x_ref_gmem[0] % 2 == 0, branch, branch)\n \n@@ -1698,7 +1698,7 @@ def kernel(x_gmem, o_gmem):\n               plgpu.SMEM(shape, large_ty, transforms=(tiling, large_swizzle)),\n               plgpu.SMEM(shape, small_ty, transforms=(tiling, small_swizzle))\n           ),\n-          plgpu.Barrier(1, num_barriers=1),\n+          plgpu.Barrier(num_barriers=1),\n       )\n \n     def scoped_kernel(x_gmem, o_gmem, aliased_ref, barrier):\n@@ -1780,7 +1780,7 @@ def body(x_ref, y_ref, barrier):\n           in_specs=[pl.BlockSpec(memory_space=plgpu.GMEM)],\n           out_specs=pl.BlockSpec(memory_space=plgpu.SMEM),\n           out_shape=x,\n-          scratch_shapes=[plgpu.Barrier(1)],\n+          scratch_shapes=[plgpu.Barrier()],\n       )(x)\n     except:\n       # assertRaisesRegex raises does not let us match the traceback.\n@@ -1921,7 +1921,7 @@ def _():\n         plgpu.wait_smem_to_gmem(0)\n       pl.run_scoped(scope,\n                     smem_ref=plgpu.SMEM((32, 32), jnp.float32),\n-                    tma_barrier=plgpu.Barrier(num_arrivals=1))\n+                    tma_barrier=plgpu.Barrier())\n     x = jax.random.uniform(jax.random.key(42), (64, 32), jnp.float32)\n     result = kernel(x)\n     np.testing.assert_array_equal(result, x[32:64])\n@@ -2345,7 +2345,7 @@ def test_tmem(self):\n             plgpu.TMEM((128, 128), jnp.float32),\n             plgpu.TMEM((128, 128), jnp.float32),\n             plgpu.SMEM((128, 128), jnp.float32, transforms=transforms),\n-            plgpu.Barrier(num_arrivals=1),\n+            plgpu.Barrier(),\n         ],\n         num_threads=1,\n         thread_name=\"x\",\n@@ -2416,7 +2416,7 @@ def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref,\n     scratch_shapes = [\n         plgpu.TMEM(shape, jnp.float32, packed=False),\n         plgpu.SMEM(shape, dtype, transforms=transforms),\n-        plgpu.Barrier(num_arrivals=1, for_tensor_core=True),\n+        plgpu.Barrier(for_tensor_core=True),\n     ]\n     if lhs_tmem:\n       scratch_shapes.append(plgpu.TMEM(shape, dtype, packed=True))\n@@ -2478,8 +2478,8 @@ def kernel(a_gmem, b_gmem, out_gmem):\n         b_smem=plgpu.SMEM(_rhs_shape, dtype, transforms=transforms),\n         acc_tmem=plgpu.TMEM(_acc_shape, jnp.float32, collective=True),\n         scratch_smem=plgpu.SMEM(_acc_shape, dtype, transforms=transforms),\n-        tma_barrier=plgpu.Barrier(num_arrivals=1),\n-        mma_barrier=plgpu.Barrier(num_arrivals=1, for_tensor_core=True),\n+        tma_barrier=plgpu.Barrier(),\n+        mma_barrier=plgpu.Barrier(for_tensor_core=True),\n         cluster_barrier=plgpu.ClusterBarrier(collective_axes=(\"x\",)),\n       )\n       def _scoped(a_smem, b_smem,\n@@ -2546,7 +2546,7 @@ def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref):\n     scratch_shapes = [\n         plgpu.TMEM(shape, jnp.float32, packed=False),\n         plgpu.SMEM(shape, dtype, transforms=transforms),\n-        plgpu.Barrier(num_arrivals=1, num_barriers=2, for_tensor_core=True),\n+        plgpu.Barrier(num_barriers=2, for_tensor_core=True),\n     ]\n     f = self.pallas_call(\n         kernel,\n@@ -2612,7 +2612,7 @@ def kernel(x_gmem, o_gmem):\n           functools.partial(scoped_kernel, x_gmem, o_gmem),\n           plgpu.SMEM((max_concurrent_steps, 32, 16), jnp.float32),\n           plgpu.SMEM((max_concurrent_steps, 32, 16), jnp.float32),\n-          plgpu.Barrier(1, num_barriers=max_concurrent_steps),\n+          plgpu.Barrier(num_barriers=max_concurrent_steps),\n       )\n \n     def scoped_kernel(x_gmem, o_gmem, x_smem, o_smem, barrier):\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Add a missing warpgroup barrier before warp core_map\n\nIf we don't synchronize the warps, some of them can go on and schedule\ne.g. async copies without waiting for the memory transactions of other\nwarps in the warpgroup to complete.\n\nPiperOrigin-RevId: 763721411",
        "output": [
            "```diff\nCommit: 9a7f9f13efdb5129a861b44263ab3024fa95ceb6\nDate: 2025-05-27T11:00:55Z\nURL: https://github.com/jax-ml/jax/commit/9a7f9f13efdb5129a861b44263ab3024fa95ceb6\nFiles changed: 1\nAdditions: +5, Deletions: -0\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex b9a3aa17c39d..cf867e55f4c9 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -2793,6 +2793,10 @@ def _core_map_lowering_rule(\n           \"Can only close over scalars and Refs when using core_map with \"\n           f\"WarpMesh. Found array of shape {aval_in}.\"\n         )\n+    # We allow the warps to schedule async copies without synchronizing with\n+    # other warps, so we need to add a barrier here to make sure all reads and\n+    # writes have completed.\n+    mgpu.warpgroup_barrier()\n     _ = lower_jaxpr_to_mosaic_gpu(\n         module_ctx,\n         ctx.launch_ctx,\n@@ -2800,6 +2804,7 @@ def _core_map_lowering_rule(\n         args=(),\n         consts=args,\n     )\n+    # TODO(apaszke,justinfu): Do we really need this barrier?\n     mgpu.warpgroup_barrier()\n     return []\n   raise ValueError(f\"Unsupported mesh: {mesh}\")\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Make sure that lowering errors mention the offending line\n\nI thought this doesn't work, but it does! Still, adding a test to make sure\nwe don't regress it.\n\nPiperOrigin-RevId: 763717665",
        "output": [
            "```diff\nCommit: b44b9634ec4c1613b87dcf1278b9437ce9ab8004\nDate: 2025-05-27T10:46:15Z\nURL: https://github.com/jax-ml/jax/commit/b44b9634ec4c1613b87dcf1278b9437ce9ab8004\nFiles changed: 1\nAdditions: +22, Deletions: -0\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 3c0b463ba1c7..7430fcc1ca53 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -21,6 +21,7 @@\n import re\n import sys\n import tempfile\n+import traceback\n from typing import ClassVar\n \n from absl.testing import absltest\n@@ -1766,6 +1767,27 @@ def body(idx, _):\n           jnp.tile((132 * sm_step + jnp.arange(132))[:, None], 128),\n       )\n \n+  def test_lowering_error_context(self):\n+    def body(x_ref, y_ref, barrier):\n+      plgpu.copy_gmem_to_smem(x_ref, y_ref, barrier)\n+      plgpu.barrier_wait(barrier)\n+\n+    x = jnp.arange(127, dtype=jnp.int4)  # Size is not a multiple of bytes\n+    offending_line = \"plgpu.copy_gmem_to_smem(x_ref, y_ref, barrier)\"\n+    try:\n+      pl.pallas_call(\n+          body,\n+          in_specs=[pl.BlockSpec(memory_space=plgpu.GMEM)],\n+          out_specs=pl.BlockSpec(memory_space=plgpu.SMEM),\n+          out_shape=x,\n+          scratch_shapes=[plgpu.Barrier(1)],\n+      )(x)\n+    except:\n+      # assertRaisesRegex raises does not let us match the traceback.\n+      self.assertIn(offending_line, traceback.format_exc())\n+    else:\n+      self.fail(\"Should have raised an exception\")\n+\n \n class PallasCallWarpPrimitiveSemanticsTest(PallasTest):\n   def setUp(self):\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28595 from andportnoy:mosaic-gpu-ptx-isa-from-ptxas-and-llvm\n\nPiperOrigin-RevId: 763701410",
        "output": [
            "```diff\nCommit: 2cbec58cf7d330f04356fd31f1a7c31f8f4a70fd\nDate: 2025-05-27T09:46:03Z\nURL: https://github.com/jax-ml/jax/commit/2cbec58cf7d330f04356fd31f1a7c31f8f4a70fd\nFiles changed: 3\nAdditions: +206, Deletions: -86\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 27175c3773e6..7df185cffaf1 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -42,6 +42,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/strings/str_replace.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"llvm/ADT/SmallVector.h\"\n@@ -109,6 +110,106 @@ namespace ffi = xla::ffi;\n using MosaicInitFunc = void(void****);\n using MosaicHostFunc = void(void**);\n \n+class TemporaryDirectory {\n+ private:\n+  TemporaryDirectory(std::string path) : path(std::move(path)) {}\n+  // TODO(apaszke): Unlink in destructor.\n+\n+ public:\n+  static absl::StatusOr<TemporaryDirectory> Create() {\n+    std::string pattern = \"/tmp/mosaic-gpu-XXXXXX\";\n+    if (mkdtemp(pattern.data()) == NULL) {\n+      return absl::InternalError(\"Failed to create temporary directory\");\n+    }\n+    return TemporaryDirectory(std::move(pattern));\n+  }\n+\n+  std::string_view GetPath() { return path; }\n+\n+ private:\n+  std::string path;\n+};\n+\n+absl::StatusOr<std::string> RunCUDATool(const char* tool,\n+                                        const std::vector<const char*>& args,\n+                                        bool stderr_to_stdout = true) {\n+  CHECK(!args.empty() && args.back() == nullptr);\n+  const char* cuda_path_ptr = getenv(\"CUDA_ROOT\");\n+  if (!cuda_path_ptr) return absl::InternalError(\"Failed to get CUDA_ROOT\");\n+  std::string tool_path(cuda_path_ptr);\n+  tool_path += \"/bin/\";\n+  tool_path += tool;\n+  int stdout_pipe[2] = {-1, -1};\n+  pid_t child_pid;\n+  posix_spawn_file_actions_t file_actions;\n+  if (posix_spawn_file_actions_init(&file_actions)) {\n+    return absl::InternalError(\"Failed to initialize spawn file actions\");\n+  }\n+  absl::Cleanup file_actions_destroyer = [&file_actions] {\n+    posix_spawn_file_actions_destroy(&file_actions);\n+  };\n+  if (pipe(stdout_pipe) == -1) {\n+    return absl::InternalError(\"Failed to set up pipe\");\n+  }\n+  absl::Cleanup pipe_closer = [&stdout_pipe] {\n+    if (stdout_pipe[0] != -1) close(stdout_pipe[0]);\n+    if (stdout_pipe[1] != -1) close(stdout_pipe[1]);\n+  };\n+  // close read end in child\n+  if (posix_spawn_file_actions_addclose(&file_actions, stdout_pipe[0])) {\n+    return absl::InternalError(\"Failed to close read end of the pipe in child\");\n+  }\n+  if (posix_spawn_file_actions_adddup2(&file_actions, stdout_pipe[1],\n+                                       STDOUT_FILENO)) {\n+    return absl::InternalError(\"Failed to redirect stdout to pipe\");\n+  }\n+  if (stderr_to_stdout && posix_spawn_file_actions_adddup2(\n+                              &file_actions, STDOUT_FILENO, STDERR_FILENO)) {\n+    return absl::InternalError(\"Failed to redirect stderr to stdout\");\n+  }\n+  // execv is guaranteed by POSIX to not modify the args (other than\n+  // replacing the whole process image), so the const_cast is valid.\n+  if (int status =\n+          posix_spawn(&child_pid, tool_path.c_str(), &file_actions, nullptr,\n+                      const_cast<char* const*>(args.data()), environ)) {\n+    return absl::InternalError(\n+        absl::StrCat(\"Process spawn failed: \", strerror(status)));\n+  }\n+  // Proactively close write end in parent. If we don't do this, read\n+  // will block since the pipe will have an open write end in the\n+  // parent process.\n+  if (close(stdout_pipe[1]) == -1) {\n+    return absl::InternalError(\n+        absl::StrCat(\"Failed to close write end of pipe in parent process: \",\n+                     strerror(errno)));\n+  }\n+  // Mark the write end as successfully closed, so it doesn't get\n+  // closed a second time by the deferred pipe_closer.\n+  stdout_pipe[1] = -1;\n+  std::string stdout;\n+  char buf[1024];\n+  while (int bytes_read = read(stdout_pipe[0], buf, sizeof buf)) {\n+    if (bytes_read == -1) {\n+      return absl::InternalError(\n+          absl::StrCat(\"Failed to read from pipe: \", strerror(errno)));\n+    }\n+    stdout.append(buf, bytes_read);\n+  }\n+  int status;\n+  if (waitpid(child_pid, &status, 0) == -1) {\n+    return absl::InternalError(\"Failed to wait for CUDA tool invocation\");\n+  }\n+  if (status != 0) {\n+    std::string error_message = \"CUDA tool failed\";\n+    if (!stdout.empty()) {\n+      error_message += \": \";\n+      error_message += stdout;\n+    }\n+    return absl::InternalError(error_message);\n+  }\n+  return stdout;\n+}\n+\n void EnsureLLVMNVPTXTargetIsRegistered() {\n   static absl::once_flag register_nvptx_target_flag;\n   absl::call_once(register_nvptx_target_flag, []() {\n@@ -119,7 +220,65 @@ void EnsureLLVMNVPTXTargetIsRegistered() {\n   });\n }\n \n-absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion() {\n+absl::StatusOr<int> GetLatestPtxasPtxIsaVersion() {\n+  std::vector<const char*> ptxas_args = {\"ptxas\", \"--input-as-string\",\n+                                         \".version 99.99\", nullptr};\n+  auto status = RunCUDATool(\"ptxas\", ptxas_args).status();\n+  if (status.ok()) {\n+    return absl::InternalError(\"ptxas succeeded where it was expected to fail\");\n+  }\n+  // Output message is of the form:\n+  // ptxas application ptx input, line 1; fatal   :\n+  // Unsupported .version 99.99; current version is '8.8'\n+  std::vector<std::string> chunks = absl::StrSplit(status.message(), '\\'');\n+  if (chunks.size() != 3) {\n+    return absl::InternalError(\n+        \"Failed to locate PTX ISA version in ptxas error message\");\n+  }\n+  std::vector<std::string> major_minor = absl::StrSplit(chunks[1], '.');\n+  if (major_minor.size() != 2) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"Expected PTX ISA version to be formatted as \"\n+                        \"MAJOR.MINOR, instead got: %s\",\n+                        chunks[1]));\n+  }\n+  int major;\n+  if (!absl::SimpleAtoi(major_minor[0], &major)) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"Failed to parse PTX ISA major version, expected a \"\n+                        \"parsable integer, instead got: %s\",\n+                        major_minor[0]));\n+  }\n+  int minor;\n+  if (!absl::SimpleAtoi(major_minor[1], &minor)) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"Failed to parse PTX ISA minor version, expected a \"\n+                        \"parsable integer, instead got: %s\",\n+                        major_minor[1]));\n+  }\n+  if (minor >= 10) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"PTX ISA minor version %d is not less than or equal to \"\n+                        \"9, which is assumed for version comparison\",\n+                        minor));\n+  }\n+  return major * 10 + minor;\n+}\n+\n+absl::StatusOr<std::string> GetPtxIsaVersion() {\n+  TF_ASSIGN_OR_RETURN(int ptxas_latest_version, GetLatestPtxasPtxIsaVersion());\n+  // We'd like to target the latest PTX ISA version supported by\n+  // ptxas. However, it doesn't make sense to ask LLVM to target a PTX\n+  // ISA that it isn't aware of yet. Find the latest version supported\n+  // by LLVM and return the minimum of the two versions, one from\n+  // ptxas and the other from LLVM.\n+  TF_ASSIGN_OR_RETURN(int llvm_latest_version,\n+                      mosaic::gpu::GetLatestLlvmPtxIsaVersion());\n+  int final_version = std::min(ptxas_latest_version, llvm_latest_version);\n+  return absl::StrFormat(\"ptx%d\", final_version);\n+}\n+\n+absl::StatusOr<std::string> GetSmVersion() {\n   // Assumes driver has been initialized and a context exists. XLA already has\n   // some utilities to query this, but we try to stay runtime-agnostic, so we\n   // build our own here.\n@@ -138,10 +297,9 @@ absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion() {\n     return absl::InternalError(\"Failed to get minor compute capability\");\n   }\n   EnsureLLVMNVPTXTargetIsRegistered();\n-  return mosaic::gpu::GetSmAndPtxIsaVersion(major, minor);\n+  return mosaic::gpu::GetSmVersion(major, minor);\n }\n \n-\n mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n     mlir::MLIRContext* ctx, mlir::gpu::CompilationTarget target,\n     const std::string& sm, const std::string& ptx_isa, const std::string& nvshmem_path) {\n@@ -272,61 +430,6 @@ void InitContext(mlir::MLIRContext* context) {\n   context->loadAllAvailableDialects();\n }\n \n-absl::Status RunCUDATool(const char* tool,\n-                         const std::vector<const char*>& args,\n-                         bool stderr_to_stdout = false) {\n-  CHECK(!args.empty() && args.back() == nullptr);\n-  const char * cuda_path_ptr = getenv(\"CUDA_ROOT\");\n-  if (!cuda_path_ptr) return absl::InternalError(\"Failed to get CUDA_ROOT\");\n-  std::string tool_path(cuda_path_ptr);\n-  tool_path += \"/bin/\";\n-  tool_path += tool;\n-  pid_t child_pid;\n-  posix_spawn_file_actions_t file_actions;\n-  if (posix_spawn_file_actions_init(&file_actions)) {\n-    return absl::InternalError(\"Failed to initialize spawn file actions\");\n-  }\n-  if (posix_spawn_file_actions_adddup2(&file_actions, STDOUT_FILENO,\n-                                       STDERR_FILENO)) {\n-    return absl::InternalError(\"Failed to set up spawn file actions\");\n-  }\n-  // execv is guaranteed by POSIX to not modify the args (other than\n-  // replacing the whole process image), so the const_cast is valid.\n-  if (posix_spawn(&child_pid, tool_path.c_str(), &file_actions, nullptr,\n-                  const_cast<char* const*>(args.data()), environ)) {\n-    return absl::InternalError(\"Process spawn failed\");\n-  }\n-  int status;\n-  if (waitpid(child_pid, &status, 0) == -1) {\n-    return absl::InternalError(\"Failed to wait for CUDA tool invocation\");\n-  }\n-  if (status != 0) return absl::InternalError(\"CUDA tool failed\");\n-  if (posix_spawn_file_actions_destroy(&file_actions) != 0) {\n-    return absl::InternalError(\"Failed to clean up after posix_spawn\");\n-  }\n-  return absl::OkStatus();\n-}\n-\n-class TemporaryDirectory {\n- private:\n-  TemporaryDirectory(std::string path) : path(std::move(path)) {}\n-  // TODO(apaszke): Unlink in destructor.\n-\n- public:\n-  static absl::StatusOr<TemporaryDirectory> Create() {\n-    std::string pattern = \"/tmp/mosaic-gpu-XXXXXX\";\n-    if (mkdtemp(pattern.data()) == NULL) {\n-      return absl::InternalError(\"Failed to create temporary directory\");\n-    }\n-    return TemporaryDirectory(std::move(pattern));\n-  }\n-\n-  std::string_view GetPath() { return path; }\n-\n- private:\n-  std::string path;\n-};\n-\n void DumpCompilationOutput(mlir::ModuleOp module, const std::string& sm,\n                            const std::string& ptx_isa, const std::string& nvshmem_path) {\n   bool dump_ptx = getenv(\"MOSAIC_GPU_DUMP_PTX\") != nullptr;\n@@ -382,19 +485,23 @@ void DumpCompilationOutput(mlir::ModuleOp module, const std::string& sm,\n       ptxas_args.push_back(\"-v\");\n     }\n     ptxas_args.push_back(nullptr);\n-    if (auto status = RunCUDATool(\"ptxas\", ptxas_args); !status.ok()) {\n-      std::cerr << \"ptxas invocation failed: \" << status.message() << std::endl;\n+    if (auto result = RunCUDATool(\"ptxas\", ptxas_args); !result.ok()) {\n+      std::cerr << \"ptxas invocation failed: \" << result.status() << std::endl;\n       continue;\n+    } else if (dump_ptxas) {\n+      std::cout << *result << std::endl;\n     }\n     if (!dump_sass) { continue; }  // We're done.\n     // Call nvdisasm to pretty-print SASS.\n-    if (auto status = RunCUDATool(\n-            \"nvdisasm\", {\"nvdisasm\", \"-ndf\", \"-c\", elf_path.c_str(), nullptr});\n-        !status.ok()) {\n-      std::cerr << \"nvdisasm invocation failed: \" << status.message()\n+    auto result = RunCUDATool(\n+        \"nvdisasm\", {\"nvdisasm\", \"-ndf\", \"-c\", elf_path.c_str(), nullptr});\n+    if (!result.ok()) {\n+      std::cerr << \"nvdisasm invocation failed: \" << result.status()\n                 << std::endl;\n       continue;\n     }\n+    // Dump SASS.\n+    std::cout << *result << std::endl;\n   }\n }\n \n@@ -424,12 +531,8 @@ absl::StatusOr<std::string> get_nvshmem_llvm_lib_path() {\n absl::StatusOr<std::pair<std::unique_ptr<mlir::ExecutionEngine>, bool>> Compile(\n     mlir::ModuleOp module) {\n   tsl::profiler::TraceMe trace(\"Compile\");\n-  auto sm_and_ptx_isa = GetSmAndPtxIsaVersion();\n-  if (!sm_and_ptx_isa.ok()) {\n-    return sm_and_ptx_isa.status();\n-  }\n-  const std::string sm = sm_and_ptx_isa.value().first;\n-  const std::string ptx_isa = sm_and_ptx_isa.value().second;\n+  TF_ASSIGN_OR_RETURN(std::string sm, GetSmVersion());\n+  TF_ASSIGN_OR_RETURN(std::string ptx_isa, GetPtxIsaVersion());\n   bool is_comm_used = is_nvshmem_used(module);\n   std::string nvshmem_path = \"\";\n   if (is_comm_used) {\ndiff --git a/jaxlib/mosaic/gpu/target.cc b/jaxlib/mosaic/gpu/target.cc\nindex a259b3dead7b..dfb119b410af 100644\n--- a/jaxlib/mosaic/gpu/target.cc\n+++ b/jaxlib/mosaic/gpu/target.cc\n@@ -21,15 +21,16 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/match.h\"\n+#include \"absl/strings/numbers.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/strings/strip.h\"\n #include \"llvm/MC/MCSubtargetInfo.h\"\n #include \"llvm/MC/TargetRegistry.h\"\n \n namespace mosaic::gpu {\n \n-absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion(\n-    int major, int minor) {\n+absl::StatusOr<std::string> GetSmVersion(int major, int minor) {\n   // \"base\" compute capability as reported by the driver.\n   // For example for a Hopper H200 GPU this would return sm_90, and never\n   // sm_90a.\n@@ -64,25 +65,41 @@ absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion(\n       }\n     }\n   }\n+  return sm_arch_specific ? sm_arch_specific : sm_base;\n+}\n \n-  const std::string sm = sm_arch_specific ? sm_arch_specific : sm_base;\n-\n+absl::StatusOr<int> GetLatestLlvmPtxIsaVersion() {\n+  const std::string triple = \"nvptx64-nvidia-cuda\";\n+  std::string error;\n+  const llvm::Target* target =\n+      llvm::TargetRegistry::lookupTarget(triple, error);\n+  if (target == nullptr) {\n+    return absl::InternalError(absl::StrFormat(\n+        \"Failed to lookup LLVM target based on triple %s: %s\", triple, error));\n+  }\n+  // generic subtarget\n   std::unique_ptr<const llvm::MCSubtargetInfo> subtarget_info{\n-      target->createMCSubtargetInfo(triple, sm, \"\")};\n+      target->createMCSubtargetInfo(triple, \"\", \"\")};\n   if (subtarget_info == nullptr) {\n-    return absl::InternalError(\n-        absl::StrFormat(\"Failed to get LLVM subtarget info for sm %s\", sm));\n+    return absl::InternalError(absl::StrFormat(\n+        \"Failed to get generic LLVM subtarget info for triple %s\", triple));\n   }\n-\n+  int llvm_latest_version = 0;\n   for (const llvm::SubtargetFeatureKV& feature :\n-       subtarget_info->getEnabledProcessorFeatures()) {\n-    if (absl::StartsWith(feature.Key, \"ptx\")) {\n-      std::string ptx_isa = feature.Key;\n-      return std::make_pair(sm, ptx_isa);\n+       subtarget_info->getAllProcessorFeatures()) {\n+    absl::string_view version_string = feature.Key;\n+    if (absl::ConsumePrefix(&version_string, \"ptx\")) {\n+      int version;\n+      if (!absl::SimpleAtoi(version_string, &version)) {\n+        return absl::InternalError(\n+            absl::StrFormat(\"Failed to convert PTX ISA version to integer: %s\",\n+                            version_string));\n+      }\n+      llvm_latest_version =\n+          version > llvm_latest_version ? version : llvm_latest_version;\n     }\n   }\n-  return absl::InternalError(absl::StrFormat(\n-      \"Failed to find a PTX ISA LLVM subtarget feature for %s\", sm));\n+  return llvm_latest_version;\n }\n \n }  // namespace mosaic::gpu\ndiff --git a/jaxlib/mosaic/gpu/target.h b/jaxlib/mosaic/gpu/target.h\nindex 070ecedebd01..5a2a240d8db1 100644\n--- a/jaxlib/mosaic/gpu/target.h\n+++ b/jaxlib/mosaic/gpu/target.h\n@@ -22,8 +22,8 @@ limitations under the License.\n \n namespace mosaic::gpu {\n \n-absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion(\n-    int major, int minor);\n+absl::StatusOr<std::string> GetSmVersion(int major, int minor);\n+absl::StatusOr<int> GetLatestLlvmPtxIsaVersion();\n \n }  // namespace mosaic::gpu\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] The `cf` dialect is now always available\n\nPiperOrigin-RevId: 763697379",
        "output": [
            "```diff\nCommit: f35d708503c7c0b401fe55e3716ad2f3ce8396cc\nDate: 2025-05-27T09:33:34Z\nURL: https://github.com/jax-ml/jax/commit/f35d708503c7c0b401fe55e3716ad2f3ce8396cc\nFiles changed: 3\nAdditions: +2, Deletions: -13\ndiff --git a/jax/_src/lib/mlir/dialects/__init__.py b/jax/_src/lib/mlir/dialects/__init__.py\nindex b49154e7936a..eccd40104dc1 100644\n--- a/jax/_src/lib/mlir/dialects/__init__.py\n+++ b/jax/_src/lib/mlir/dialects/__init__.py\n@@ -19,6 +19,7 @@\n if TYPE_CHECKING:\n   from jaxlib.mlir.dialects import arith as arith\n   from jaxlib.mlir.dialects import builtin as builtin\n+  from jaxlib.mlir.dialects import cf as cf\n   from jaxlib.mlir.dialects import chlo as chlo\n   from jaxlib.mlir.dialects import func as func\n   from jaxlib.mlir.dialects import gpu as gpu\n@@ -36,6 +37,7 @@\n   __getattr__, __dir__, __all__ = _lazy.attach(\"jaxlib.mlir.dialects\", [\n       \"arith\",\n       \"builtin\",\n+      \"cf\",\n       \"chlo\",\n       \"func\",\n       \"gpu\",\n@@ -57,4 +59,3 @@\n from jaxlib.mlir.dialects import stablehlo as hlo\n \n from jax._src import lib\n-from jaxlib.mlir.dialects import cf\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 4e8827401e0c..635c473620c3 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -3780,12 +3780,6 @@ def _check_lowering_rule(\n   if not pallas_helpers.debug_checks_enabled():\n     return []\n \n-  if cf is None:\n-    # TODO(slebedev): Remove once the minimal jaxlib version is 0.6.1.\n-    raise ValueError(\n-        \"cf dialect is not available. Make sure you have jaxlib 0.6.1 or later.\"\n-    )\n-\n   error = jax.tree.unflatten(err_tree, err_args)\n   [pred] = error._pred.values()\n   [exception_tree] = error._metadata.values()\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 9e396167f610..b9a3aa17c39d 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -3109,12 +3109,6 @@ def _check_lowering_rule(ctx: LoweringRuleContext, *err_args, err_tree, debug):\n   if not pallas_helpers.debug_checks_enabled():\n     return []\n \n-  if cf_dialect is None:\n-    # TODO(slebedev): Remove once the minimal jaxlib version is 0.6.1.\n-    raise ValueError(\n-        \"cf dialect is not available. Make sure you have jaxlib 0.6.1 or later.\"\n-    )\n-\n   error = jax.tree.unflatten(err_tree, err_args)\n   [pred] = error._pred.values()\n   [exception_tree] = error._metadata.values()\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Use PTX ISA version = min(ptxas, LLVM)",
        "output": [
            "```diff\nCommit: c1e8f250b53e2c9636fd095ead21b64f13c1d422\nDate: 2025-05-26T15:17:49Z\nURL: https://github.com/jax-ml/jax/commit/c1e8f250b53e2c9636fd095ead21b64f13c1d422\nFiles changed: 3\nAdditions: +206, Deletions: -86\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 27175c3773e6..d109520582c9 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -42,6 +42,7 @@ limitations under the License.\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n+#include \"absl/strings/str_replace.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/synchronization/mutex.h\"\n #include \"llvm/ADT/SmallVector.h\"\n@@ -109,6 +110,106 @@ namespace ffi = xla::ffi;\n using MosaicInitFunc = void(void****);\n using MosaicHostFunc = void(void**);\n \n+class TemporaryDirectory {\n+ private:\n+  TemporaryDirectory(std::string path) : path(std::move(path)) {}\n+  // TODO(apaszke): Unlink in destructor.\n+\n+ public:\n+  static absl::StatusOr<TemporaryDirectory> Create() {\n+    std::string pattern = \"/tmp/mosaic-gpu-XXXXXX\";\n+    if (mkdtemp(pattern.data()) == NULL) {\n+      return absl::InternalError(\"Failed to create temporary directory\");\n+    }\n+    return TemporaryDirectory(std::move(pattern));\n+  }\n+\n+  std::string_view GetPath() { return path; }\n+\n+ private:\n+  std::string path;\n+};\n+\n+absl::StatusOr<std::string> RunCUDATool(const char* tool,\n+                                        const std::vector<const char*>& args,\n+                                        bool stderr_to_stdout = true) {\n+  CHECK(!args.empty() && args.back() == nullptr);\n+  const char * cuda_path_ptr = getenv(\"CUDA_ROOT\");\n+  if (!cuda_path_ptr) return absl::InternalError(\"Failed to get CUDA_ROOT\");\n+  std::string tool_path(cuda_path_ptr);\n+  tool_path += \"/bin/\";\n+  tool_path += tool;\n+  int stdout_pipe[2] = {-1, -1};\n+  pid_t child_pid;\n+  posix_spawn_file_actions_t file_actions;\n+  if (posix_spawn_file_actions_init(&file_actions)) {\n+    return absl::InternalError(\"Failed to initialize spawn file actions\");\n+  }\n+  absl::Cleanup file_actions_destroyer = [&file_actions] {\n+    posix_spawn_file_actions_destroy(&file_actions);\n+  };\n+  if (pipe(stdout_pipe) == -1) {\n+    return absl::InternalError(\"Failed to set up pipe\");\n+  }\n+  absl::Cleanup pipe_closer = [&stdout_pipe] {\n+    if (stdout_pipe[0] != -1) close(stdout_pipe[0]);\n+    if (stdout_pipe[1] != -1) close(stdout_pipe[1]);\n+  };\n+  // close read end in child\n+  if (posix_spawn_file_actions_addclose(&file_actions, stdout_pipe[0])) {\n+    return absl::InternalError(\"Failed to close read end of the pipe in child\");\n+  }\n+  if (posix_spawn_file_actions_adddup2(&file_actions, stdout_pipe[1],\n+                                       STDOUT_FILENO)) {\n+    return absl::InternalError(\"Failed to redirect stdout to pipe\");\n+  }\n+  if (stderr_to_stdout && posix_spawn_file_actions_adddup2(\n+                              &file_actions, STDOUT_FILENO, STDERR_FILENO)) {\n+    return absl::InternalError(\"Failed to redirect stderr to stdout\");\n+  }\n+  // execv is guaranteed by POSIX to not modify the args (other than\n+  // replacing the whole process image), so the const_cast is valid.\n+  if (int status =\n+          posix_spawn(&child_pid, tool_path.c_str(), &file_actions, nullptr,\n+                      const_cast<char *const *>(args.data()), environ)) {\n+    return absl::InternalError(\n+        absl::StrCat(\"Process spawn failed: \", strerror(status)));\n+  }\n+  // Proactively close write end in parent. If we don't do this, read\n+  // will block since the pipe will have an open write end in the\n+  // parent process.\n+  if (close(stdout_pipe[1]) == -1) {\n+    return absl::InternalError(\n+        absl::StrCat(\"Failed to close write end of pipe in parent process: \",\n+                     strerror(errno)));\n+  }\n+  // Mark the write end as successfully closed, so it doesn't get\n+  // closed a second time by the deferred pipe_closer.\n+  stdout_pipe[1] = -1;\n+  std::string stdout;\n+  char buf[1024];\n+  while (int bytes_read = read(stdout_pipe[0], buf, sizeof buf)) {\n+    if (bytes_read == -1) {\n+      return absl::InternalError(\n+          absl::StrCat(\"Failed to read from pipe: \", strerror(errno)));\n+    }\n+    stdout.append(buf, bytes_read);\n+  }\n+  int status;\n+  if (waitpid(child_pid, &status, 0) == -1) {\n+    return absl::InternalError(\"Failed to wait for CUDA tool invocation\");\n+  }\n+  if (status != 0) {\n+    std::string error_message = \"CUDA tool failed\";\n+    if (!stdout.empty()) {\n+      error_message += \": \";\n+      error_message += stdout;\n+    }\n+    return absl::InternalError(error_message);\n+  }\n+  return stdout;\n+}\n+\n void EnsureLLVMNVPTXTargetIsRegistered() {\n   static absl::once_flag register_nvptx_target_flag;\n   absl::call_once(register_nvptx_target_flag, []() {\n@@ -119,7 +220,65 @@ void EnsureLLVMNVPTXTargetIsRegistered() {\n   });\n }\n \n-absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion() {\n+absl::StatusOr<int> GetLatestPtxasPtxIsaVersion() {\n+  std::vector<const char*> ptxas_args = {\"ptxas\", \"--input-as-string\",\n+                                         \".version 99.99\", nullptr};\n+  auto status = RunCUDATool(\"ptxas\", ptxas_args).status();\n+  if (status.ok()) {\n+    return absl::InternalError(\"ptxas succeeded where it was expected to fail\");\n+  }\n+  // Output message is of the form:\n+  // ptxas application ptx input, line 1; fatal   : Unsupported .version 99.99; current version is '8.8'\n+  std::vector<std::string> chunks =\n+      absl::StrSplit(status.message(), '\\'');\n+  if (chunks.size() != 3) {\n+    return absl::InternalError(\n+        \"Failed to locate PTX ISA version in ptxas error message\");\n+  }\n+  std::vector<std::string> major_minor = absl::StrSplit(chunks[1], '.');\n+  if (major_minor.size() != 2) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"Expected PTX ISA version to be formatted as \"\n+                        \"MAJOR.MINOR, instead got: %s\",\n+                        chunks[1]));\n+  }\n+  int major;\n+  if (!absl::SimpleAtoi(major_minor[0], &major)) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"Failed to parse PTX ISA major version, expected a \"\n+                        \"parsable integer, instead got: %s\",\n+                        major_minor[0]));\n+  }\n+  int minor;\n+  if (!absl::SimpleAtoi(major_minor[1], &minor)) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"Failed to parse PTX ISA minor version, expected a \"\n+                        \"parsable integer, instead got: %s\",\n+                        major_minor[1]));\n+  }\n+  if (minor >= 10) {\n+    return absl::InternalError(\n+        absl::StrFormat(\"PTX ISA minor version %d is not less than or equal to \"\n+                        \"9, which is assumed for version comparison\",\n+                        minor));\n+  }\n+  return major * 10 + minor;\n+}\n+\n+absl::StatusOr<std::string> GetPtxIsaVersion() {\n+  TF_ASSIGN_OR_RETURN(int ptxas_latest_version, GetLatestPtxasPtxIsaVersion());\n+  // We'd like to target the latest PTX ISA version supported by\n+  // ptxas. However, it doesn't make sense to ask LLVM to target a PTX\n+  // ISA that it isn't aware of yet. Find the latest version supported\n+  // by LLVM and return the minimum of the two versions, one from\n+  // ptxas and the other from LLVM.\n+  TF_ASSIGN_OR_RETURN(int llvm_latest_version,\n+                      mosaic::gpu::GetLatestLlvmPtxIsaVersion());\n+  int final_version = std::min(ptxas_latest_version, llvm_latest_version);\n+  return absl::StrFormat(\"ptx%d\", final_version);\n+}\n+\n+absl::StatusOr<std::string> GetSmVersion() {\n   // Assumes driver has been initialized and a context exists. XLA already has\n   // some utilities to query this, but we try to stay runtime-agnostic, so we\n   // build our own here.\n@@ -138,10 +297,9 @@ absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion() {\n     return absl::InternalError(\"Failed to get minor compute capability\");\n   }\n   EnsureLLVMNVPTXTargetIsRegistered();\n-  return mosaic::gpu::GetSmAndPtxIsaVersion(major, minor);\n+  return mosaic::gpu::GetSmVersion(major, minor);\n }\n \n-\n mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n     mlir::MLIRContext* ctx, mlir::gpu::CompilationTarget target,\n     const std::string& sm, const std::string& ptx_isa, const std::string& nvshmem_path) {\n@@ -272,61 +430,6 @@ void InitContext(mlir::MLIRContext* context) {\n   context->loadAllAvailableDialects();\n }\n \n-absl::Status RunCUDATool(const char* tool,\n-                         const std::vector<const char*>& args,\n-                         bool stderr_to_stdout = false) {\n-  CHECK(!args.empty() && args.back() == nullptr);\n-  const char * cuda_path_ptr = getenv(\"CUDA_ROOT\");\n-  if (!cuda_path_ptr) return absl::InternalError(\"Failed to get CUDA_ROOT\");\n-  std::string tool_path(cuda_path_ptr);\n-  tool_path += \"/bin/\";\n-  tool_path += tool;\n-  pid_t child_pid;\n-  posix_spawn_file_actions_t file_actions;\n-  if (posix_spawn_file_actions_init(&file_actions)) {\n-    return absl::InternalError(\"Failed to initialize spawn file actions\");\n-  }\n-  if (posix_spawn_file_actions_adddup2(&file_actions, STDOUT_FILENO,\n-                                       STDERR_FILENO)) {\n-    return absl::InternalError(\"Failed to set up spawn file actions\");\n-  }\n-  // execv is guaranteed by POSIX to not modify the args (other than\n-  // replacing the whole process image), so the const_cast is valid.\n-  if (posix_spawn(&child_pid, tool_path.c_str(), &file_actions, nullptr,\n-                  const_cast<char* const*>(args.data()), environ)) {\n-    return absl::InternalError(\"Process spawn failed\");\n-  }\n-  int status;\n-  if (waitpid(child_pid, &status, 0) == -1) {\n-    return absl::InternalError(\"Failed to wait for CUDA tool invocation\");\n-  }\n-  if (status != 0) return absl::InternalError(\"CUDA tool failed\");\n-  if (posix_spawn_file_actions_destroy(&file_actions) != 0) {\n-    return absl::InternalError(\"Failed to clean up after posix_spawn\");\n-  }\n-  return absl::OkStatus();\n-}\n-\n-class TemporaryDirectory {\n- private:\n-  TemporaryDirectory(std::string path) : path(std::move(path)) {}\n-  // TODO(apaszke): Unlink in destructor.\n-\n- public:\n-  static absl::StatusOr<TemporaryDirectory> Create() {\n-    std::string pattern = \"/tmp/mosaic-gpu-XXXXXX\";\n-    if (mkdtemp(pattern.data()) == NULL) {\n-      return absl::InternalError(\"Failed to create temporary directory\");\n-    }\n-    return TemporaryDirectory(std::move(pattern));\n-  }\n-\n-  std::string_view GetPath() { return path; }\n-\n- private:\n-  std::string path;\n-};\n-\n void DumpCompilationOutput(mlir::ModuleOp module, const std::string& sm,\n                            const std::string& ptx_isa, const std::string& nvshmem_path) {\n   bool dump_ptx = getenv(\"MOSAIC_GPU_DUMP_PTX\") != nullptr;\n@@ -382,19 +485,23 @@ void DumpCompilationOutput(mlir::ModuleOp module, const std::string& sm,\n       ptxas_args.push_back(\"-v\");\n     }\n     ptxas_args.push_back(nullptr);\n-    if (auto status = RunCUDATool(\"ptxas\", ptxas_args); !status.ok()) {\n-      std::cerr << \"ptxas invocation failed: \" << status.message() << std::endl;\n+    if (auto result = RunCUDATool(\"ptxas\", ptxas_args); !result.ok()) {\n+      std::cerr << \"ptxas invocation failed: \" << result.status() << std::endl;\n       continue;\n+    } else if (dump_ptxas) {\n+      std::cout << *result << std::endl;\n     }\n     if (!dump_sass) { continue; }  // We're done.\n     // Call nvdisasm to pretty-print SASS.\n-    if (auto status = RunCUDATool(\n-            \"nvdisasm\", {\"nvdisasm\", \"-ndf\", \"-c\", elf_path.c_str(), nullptr});\n-        !status.ok()) {\n-      std::cerr << \"nvdisasm invocation failed: \" << status.message()\n+    auto result = RunCUDATool(\n+        \"nvdisasm\", {\"nvdisasm\", \"-ndf\", \"-c\", elf_path.c_str(), nullptr});\n+    if (!result.ok()) {\n+      std::cerr << \"nvdisasm invocation failed: \" << result.status()\n                 << std::endl;\n       continue;\n     }\n+    // Dump SASS.\n+    std::cout << *result << std::endl;\n   }\n }\n \n@@ -424,12 +531,8 @@ absl::StatusOr<std::string> get_nvshmem_llvm_lib_path() {\n absl::StatusOr<std::pair<std::unique_ptr<mlir::ExecutionEngine>, bool>> Compile(\n     mlir::ModuleOp module) {\n   tsl::profiler::TraceMe trace(\"Compile\");\n-  auto sm_and_ptx_isa = GetSmAndPtxIsaVersion();\n-  if (!sm_and_ptx_isa.ok()) {\n-    return sm_and_ptx_isa.status();\n-  }\n-  const std::string sm = sm_and_ptx_isa.value().first;\n-  const std::string ptx_isa = sm_and_ptx_isa.value().second;\n+  TF_ASSIGN_OR_RETURN(std::string sm, GetSmVersion());\n+  TF_ASSIGN_OR_RETURN(std::string ptx_isa, GetPtxIsaVersion());\n   bool is_comm_used = is_nvshmem_used(module);\n   std::string nvshmem_path = \"\";\n   if (is_comm_used) {\ndiff --git a/jaxlib/mosaic/gpu/target.cc b/jaxlib/mosaic/gpu/target.cc\nindex a259b3dead7b..4c1866fdaea9 100644\n--- a/jaxlib/mosaic/gpu/target.cc\n+++ b/jaxlib/mosaic/gpu/target.cc\n@@ -21,6 +21,8 @@ limitations under the License.\n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/match.h\"\n+#include \"absl/strings/numbers.h\"\n+#include \"absl/strings/strip.h\"\n #include \"absl/strings/str_cat.h\"\n #include \"absl/strings/str_format.h\"\n #include \"llvm/MC/MCSubtargetInfo.h\"\n@@ -28,8 +30,7 @@ limitations under the License.\n \n namespace mosaic::gpu {\n \n-absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion(\n-    int major, int minor) {\n+absl::StatusOr<std::string> GetSmVersion(int major, int minor) {\n   // \"base\" compute capability as reported by the driver.\n   // For example for a Hopper H200 GPU this would return sm_90, and never\n   // sm_90a.\n@@ -64,25 +65,41 @@ absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion(\n       }\n     }\n   }\n+  return sm_arch_specific ? sm_arch_specific : sm_base;\n+}\n \n-  const std::string sm = sm_arch_specific ? sm_arch_specific : sm_base;\n-\n+absl::StatusOr<int> GetLatestLlvmPtxIsaVersion() {\n+  const std::string triple = \"nvptx64-nvidia-cuda\";\n+  std::string error;\n+  const llvm::Target* target =\n+      llvm::TargetRegistry::lookupTarget(triple, error);\n+  if (target == nullptr) {\n+    return absl::InternalError(absl::StrFormat(\n+        \"Failed to lookup LLVM target based on triple %s: %s\", triple, error));\n+  }\n+  // generic subtarget\n   std::unique_ptr<const llvm::MCSubtargetInfo> subtarget_info{\n-      target->createMCSubtargetInfo(triple, sm, \"\")};\n+      target->createMCSubtargetInfo(triple, \"\", \"\")};\n   if (subtarget_info == nullptr) {\n-    return absl::InternalError(\n-        absl::StrFormat(\"Failed to get LLVM subtarget info for sm %s\", sm));\n+    return absl::InternalError(absl::StrFormat(\n+        \"Failed to get generic LLVM subtarget info for triple %s\", triple));\n   }\n-\n+  int llvm_latest_version = 0;\n   for (const llvm::SubtargetFeatureKV& feature :\n-       subtarget_info->getEnabledProcessorFeatures()) {\n-    if (absl::StartsWith(feature.Key, \"ptx\")) {\n-      std::string ptx_isa = feature.Key;\n-      return std::make_pair(sm, ptx_isa);\n+       subtarget_info->getAllProcessorFeatures()) {\n+    absl::string_view version_string = feature.Key;\n+    if (absl::ConsumePrefix(&version_string, \"ptx\")) {\n+      int version;\n+      if (!absl::SimpleAtoi(version_string, &version)) {\n+        return absl::InternalError(\n+            absl::StrFormat(\"Failed to convert PTX ISA version to integer: %s\",\n+                            version_string));\n+      }\n+      llvm_latest_version =\n+          version > llvm_latest_version ? version : llvm_latest_version;\n     }\n   }\n-  return absl::InternalError(absl::StrFormat(\n-      \"Failed to find a PTX ISA LLVM subtarget feature for %s\", sm));\n+  return llvm_latest_version;\n }\n \n }  // namespace mosaic::gpu\ndiff --git a/jaxlib/mosaic/gpu/target.h b/jaxlib/mosaic/gpu/target.h\nindex 070ecedebd01..5a2a240d8db1 100644\n--- a/jaxlib/mosaic/gpu/target.h\n+++ b/jaxlib/mosaic/gpu/target.h\n@@ -22,8 +22,8 @@ limitations under the License.\n \n namespace mosaic::gpu {\n \n-absl::StatusOr<std::pair<std::string, std::string>> GetSmAndPtxIsaVersion(\n-    int major, int minor);\n+absl::StatusOr<std::string> GetSmVersion(int major, int minor);\n+absl::StatusOr<int> GetLatestLlvmPtxIsaVersion();\n \n }  // namespace mosaic::gpu\n \n\n```"
        ],
        "from_id": [
            "andportnoy"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Support remote async copies and use them in the collective matmul\n\nPiperOrigin-RevId: 763353415",
        "output": [
            "```diff\nCommit: fae05bd8592b3508143179602b25dcd609a630b9\nDate: 2025-05-26T10:07:57Z\nURL: https://github.com/jax-ml/jax/commit/fae05bd8592b3508143179602b25dcd609a630b9\nFiles changed: 3\nAdditions: +35, Deletions: -5\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 1ec22bff3f6d..61be6e35cc55 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -37,6 +37,7 @@\n from jax._src.lib.mlir.dialects import gpu as gpu_dialect\n from jax._src.lib.mlir.dialects import nvvm as nvvm_dialect\n from jax._src.pallas import core as pallas_core\n+from jax._src.pallas import primitives as pallas_primitives\n from jax._src.pallas.mosaic_gpu import core as gpu_core\n from jax._src.pallas.mosaic_gpu import lowering\n from jax._src.pallas.mosaic_gpu.core import state_types\n@@ -282,6 +283,10 @@ def _copy_smem_to_gmem_lowering(\n   else:\n     indices, slice_lengths = _split_gmem_slice(copy_params[\"gmem_slice\"])\n   assert copy_params.get(\"swizzle\") is None\n+  if copy_params.get(\"gmem_peer_id\", None) is not None:\n+    raise NotImplementedError(\n+        \"GMEM refs with peer ids are not supported in warpgroup lowering.\"\n+    )\n   assert not copy_params.get(\"gmem_transform\")\n   mgpu.dialect.async_store(\n       src,\n@@ -317,13 +322,25 @@ def _split_gmem_slice(gmem_slice):\n def _extract_gmem_copy_params(transforms):\n   if not transforms:\n     return {}\n+  peer_id = None\n+  indexers = []\n   for transform in transforms:\n-    if not isinstance(transform, indexing.NDIndexer):\n+    if isinstance(transform, gpu_core.PeerMemRef):\n+      if transform.device_id_type != pallas_primitives.DeviceIdType.LOGICAL:\n+        raise NotImplementedError(\n+            \"Only logical device ids are supported for GMEM refs.\"\n+        )\n+      peer_id = lowering._ensure_ir_value(transform.device_id, jnp.int32)\n+      continue\n+    elif isinstance(transform, indexing.NDIndexer):\n+      indexers.append(transform)\n+    else:\n       raise NotImplementedError(\n           \"Non-indexing transforms on GMEM refs are not implemented.\")\n-  indexer = lowering.merge_indexers(transforms)\n+  indexer = lowering.merge_indexers(indexers)\n   return dict(\n       gmem_slice=lowering._ndindexer_indices(indexer),\n+      gmem_peer_id=peer_id,\n   )\n \n \n@@ -542,6 +559,10 @@ def _copy_gmem_to_smem_lowering(\n     indices, slice_lengths = _split_gmem_slice(copy_params[\"gmem_slice\"])\n   assert copy_params.get(\"swizzle\") is None\n   assert not copy_params.get(\"gmem_transform\")\n+  if copy_params.get(\"gmem_peer_id\", None) is not None:\n+    raise NotImplementedError(\n+        \"GMEM refs with peer ids are not supported in warpgroup lowering.\"\n+    )\n   barrier_ref = barrier.as_barrier_memref()\n   mgpu.dialect.arrive_expect_tx(barrier_ref, bytes)\n   mgpu.dialect.async_load(\ndiff --git a/jax/experimental/mosaic/gpu/launch_context.py b/jax/experimental/mosaic/gpu/launch_context.py\nindex e4f0c4efa22c..2a5bb96f4708 100644\n--- a/jax/experimental/mosaic/gpu/launch_context.py\n+++ b/jax/experimental/mosaic/gpu/launch_context.py\n@@ -407,7 +407,10 @@ def _get_tma_desc(\n         \"add\",\"min\",\"max\",\"inc\",\"dec\",\"and\",\"or\",\"xor\"\n       ] | None,\n   ):\n-    tma_desc_key = (gmem_ref, transformed_slice_shape, swizzle, gmem_transform)\n+    # Using ir.Values in cache keys is a little sketchy, but I think it should\n+    # be fine. Having it in the key will keep it alive, and if comparison and\n+    # hashing is by identity then it should work out.\n+    tma_desc_key = (gmem_ref, transformed_slice_shape, swizzle, gmem_transform, gmem_peer_id)\n     if (tma_desc := self.tma_descriptors.get(tma_desc_key, None)) is None:\n       i32 = ir.IntegerType.get_signless(32)\n       i64 = ir.IntegerType.get_signless(64)\ndiff --git a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\nindex a6c372f2cee7..854d75dbf6a3 100644\n--- a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n@@ -140,9 +140,15 @@ def k_loop(idxs, lhs_smem, rhs_smem):\n             plgpu.wgmma(acc_ref, lhs_smem, rhs_smem)\n             k_slice = pl.ds(ki * block_k, block_k)\n             # TODO(apaszke): No need to send on the last step\n-            # TODO(apaszke): Use an async copy. This is uncoalesced.\n-            send_scratch_ref[next_scratch_slot, :, k_slice] = lhs_smem[...]\n+            plgpu.copy_smem_to_gmem(\n+                lhs_smem, send_scratch_ref.at[next_scratch_slot, :, k_slice]\n+            )\n+            # We only delay release by 1 step, so we need to wait for the\n+            # previous copies.\n+            plgpu.wait_smem_to_gmem(1, wait_read_only=True)\n           k_loop(scratch_ref.at[scratch_slot], rhs_ref)\n+          # Make sure the copy is fully done.\n+          plgpu.wait_smem_to_gmem(0, wait_read_only=False)\n           # TODO(apaszke): Both of those semaphores perform a .sys release.\n           # This is very expensive and we should only do a single .sys fence.\n           pl.semaphore_signal(capacity_sem, device_id=recv_dev_id, device_id_type=pl.DeviceIdType.LOGICAL)\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add missing allocator config and skips in one of our distributed tests\n\nI added them in all other files, but forgot about this one.\n\nPiperOrigin-RevId: 763352483",
        "output": [
            "```diff\nCommit: f9c7a1421571ceb441011775af41ea75410ceeeb\nDate: 2025-05-26T10:05:04Z\nURL: https://github.com/jax-ml/jax/commit/f9c7a1421571ceb441011775af41ea75410ceeeb\nFiles changed: 1\nAdditions: +9, Deletions: -0\ndiff --git a/tests/mosaic/gpu_test_distributed.py b/tests/mosaic/gpu_test_distributed.py\nindex fee2ce5b03a6..cf3913771983 100644\n--- a/tests/mosaic/gpu_test_distributed.py\n+++ b/tests/mosaic/gpu_test_distributed.py\n@@ -13,6 +13,8 @@\n # limitations under the License.\n # ==============================================================================\n \n+import os\n+\n from absl.testing import parameterized\n import jax\n from jax._src import config\n@@ -50,6 +52,8 @@ def setUp(self):\n       self.skipTest(\"Only works on GPU with capability >= sm90\")\n     if not mgpu.supports_cross_device_collectives():\n       self.skipTest(\"NVSHMEM library unavailable.\")\n+    if os.environ.get(\"XLA_PYTHON_CLIENT_ALLOCATOR\", \"\") == \"platform\":\n+      self.skipTest(\"NVSHMEM doesn't work with the platform allocator.\")\n     if jax.process_count() == 1:\n       self.skipTest(\"Test requires multiple processes.\")\n     if jax.device_count() != jax.process_count():\n@@ -97,4 +101,9 @@ def kernel(ctx, src, dst, scratch):\n \n \n if __name__ == \"__main__\":\n+  # This test doesn't work with the platform allocator, so we override it\n+  # if it's ran alone. If it's part of a larger test suite and the platform\n+  # allocator is used, setUp will skip the test.\n+  os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.01'\n+  os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'default'\n   jt_multiprocess.main()\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Clarify that upper bound takes precedence in jnp.clip where bounds are incongruent",
        "output": [
            "```diff\nCommit: c22bba2f9c237feb8743caf5378ee3b7966209bd\nDate: 2025-05-25T17:07:35Z\nURL: https://github.com/jax-ml/jax/commit/c22bba2f9c237feb8743caf5378ee3b7966209bd\nFiles changed: 2\nAdditions: +9, Deletions: -0\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex 0bd287dadd51..ad2b3ad6aa75 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -3410,6 +3410,7 @@ def clip(\n   Returns:\n     An array containing values from ``arr``, with values smaller than ``min`` set\n     to ``min``, and values larger than ``max`` set to ``max``.\n+    Wherever ``min`` is larger than ``max``, the value of ``max`` is returned.\n \n   See also:\n     - :func:`jax.numpy.minimum`: Compute the element-wise minimum value of two arrays.\ndiff --git a/tests/lax_numpy_test.py b/tests/lax_numpy_test.py\nindex 875024617b5f..29e6586ffa18 100644\n--- a/tests/lax_numpy_test.py\n+++ b/tests/lax_numpy_test.py\n@@ -1065,6 +1065,14 @@ def testClipDeprecatedArgs(self):\n                                              \"Passing arguments 'a', 'a_min' or 'a_max' to jax.numpy.clip is deprecated\"):\n       jnp.clip(jnp.arange(4), a_min=2, a_max=3)\n \n+  def testClipUpperPrecedence(self):\n+    a_min = 3 * np.ones(1)\n+    a_max = 2 * np.ones(1)\n+    x = 4 * np.ones(1)\n+    y = jnp.clip(x, min=a_min, max=a_max)\n+    assert y == a_max, f\"Expected {y} to equal {a_max} when a_min > a_max.\"\n+    assert y == jnp.asarray(np.clip(x, a_min=a_min, a_max=a_max))\n+\n   def testHypotComplexInputError(self):\n     rng = jtu.rand_default(self.rng())\n     x = rng((5,), dtype=jnp.complex64)\n\n```"
        ],
        "from_id": [
            "johanna.haffner@bsse.ethz.ch"
        ]
    },
    {
        "text_input": "Move jax/_src/sourcemap to its own build rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, discourages private imports downstream, and leads to improved build and iteration times.\n\nPiperOrigin-RevId: 762621491",
        "output": [
            "```diff\nCommit: d0195f2240fdf081e011e193585593df2e060b26\nDate: 2025-05-24T00:07:02Z\nURL: https://github.com/jax-ml/jax/commit/d0195f2240fdf081e011e193585593df2e060b26\nFiles changed: 2\nAdditions: +8, Deletions: -1\ndiff --git a/jax/BUILD b/jax/BUILD\nindex de187b4ce597..a236cd206a55 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -321,7 +321,6 @@ py_library_providing_imports_info(\n         \"_src/random.py\",\n         \"_src/shard_alike.py\",\n         \"_src/shard_map.py\",\n-        \"_src/sourcemap.py\",\n     ] + glob(\n         [\n             \"*.py\",\n@@ -413,6 +412,7 @@ py_library_providing_imports_info(\n         \":sharding_impls\",\n         \":sharding_specs\",\n         \":source_info_util\",\n+        \":sourcemap\",\n         \":stages\",\n         \":traceback_util\",\n         \":tree\",\n@@ -795,6 +795,11 @@ pytype_strict_library(\n     ],\n )\n \n+pytype_strict_library(\n+    name = \"sourcemap\",\n+    srcs = [\"_src/sourcemap.py\"],\n+)\n+\n pytype_strict_library(\n     name = \"source_mapper\",\n     srcs = glob(include = [\"experimental/source_mapper/**/*.py\"]),\n@@ -806,6 +811,7 @@ pytype_strict_library(\n         \":core\",\n         \":jax\",\n         \":source_info_util\",\n+        \":sourcemap\",\n     ] + py_deps(\"absl/flags\"),\n )\n \ndiff --git a/tests/BUILD b/tests/BUILD\nindex 6c9f3f74b56a..6fac61933d59 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -2109,6 +2109,7 @@ jax_py_test(\n     srcs = [\"sourcemap_test.py\"],\n     deps = [\n         \"//jax\",\n+        \"//jax:sourcemap\",\n         \"//jax:test_util\",\n     ] + py_deps([\n         \"absl/testing\",\n\n```"
        ],
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move jax/_src/tree.py to its own build rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nPiperOrigin-RevId: 762589488",
        "output": [
            "```diff\nCommit: 2b9d7c80a84401c39b2ae7b9083b1db9d0bbc22a\nDate: 2025-05-23T22:29:02Z\nURL: https://github.com/jax-ml/jax/commit/2b9d7c80a84401c39b2ae7b9083b1db9d0bbc22a\nFiles changed: 1\nAdditions: +9, Deletions: -1\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 5fb96d34d91e..de187b4ce597 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -322,7 +322,6 @@ py_library_providing_imports_info(\n         \"_src/shard_alike.py\",\n         \"_src/shard_map.py\",\n         \"_src/sourcemap.py\",\n-        \"_src/tree.py\",\n     ] + glob(\n         [\n             \"*.py\",\n@@ -416,6 +415,7 @@ py_library_providing_imports_info(\n         \":source_info_util\",\n         \":stages\",\n         \":traceback_util\",\n+        \":tree\",\n         \":tree_util\",\n         \":typing\",\n         \":util\",\n@@ -1207,6 +1207,14 @@ pytype_strict_library(\n     ] + py_deps(\"numpy\"),\n )\n \n+pytype_strict_library(\n+    name = \"tree\",\n+    srcs = [\"_src/tree.py\"],\n+    deps = [\n+        \":tree_util\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"tree_util\",\n     srcs = [\"_src/tree_util.py\"],\n\n```"
        ],
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[ragged-paged-attn] Implement static kv cache quantization. (The scale of kv cache is a scalar float value)\n\nPiperOrigin-RevId: 762576286",
        "output": [
            "```diff\nCommit: 966bcb932ea962133d77ea2ad29d65a85127b402\nDate: 2025-05-23T21:49:28Z\nURL: https://github.com/jax-ml/jax/commit/966bcb932ea962133d77ea2ad29d65a85127b402\nFiles changed: 3\nAdditions: +171, Deletions: -49\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 586e9c2dd6e6..5fb96d34d91e 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -908,6 +908,7 @@ pytype_strict_library(\n         \":pallas_tpu_users\",\n     ],\n     deps = [\n+        \":dtypes\",\n         \":jax\",\n         \":pallas\",\n         \":pallas_tpu\",\ndiff --git a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\nindex d9d952d5a378..67c0b376ecc6 100644\n--- a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n+++ b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n@@ -22,11 +22,13 @@\n import functools\n import jax\n from jax import lax\n+from jax._src import dtypes\n from jax.experimental import pallas as pl\n from jax.experimental.pallas import tpu as pltpu\n from jax.experimental.pallas.ops.tpu.ragged_paged_attention.tuned_block_sizes import get_tuned_block_sizes\n import jax.numpy as jnp\n \n+\n DEFAULT_MASK_VALUE = -0.7 * float(jnp.finfo(jnp.dtype(\"float32\")).max)\n \n \n@@ -80,6 +82,8 @@ def ref_ragged_paged_attention(\n     sliding_window: int | None = None,\n     soft_cap: float | None = None,\n     mask_value: float | None = DEFAULT_MASK_VALUE,\n+    k_scale: float | None = None,\n+    v_scale: float | None = None,\n ):\n   static_validate_inputs(\n       queries,\n@@ -89,6 +93,8 @@ def ref_ragged_paged_attention(\n       cu_q_lens,\n       num_seqs,\n       sm_scale=sm_scale,\n+      k_scale=k_scale,\n+      v_scale=v_scale,\n       sliding_window=sliding_window,\n       soft_cap=soft_cap,\n       mask_value=mask_value,\n@@ -115,6 +121,12 @@ def ref_ragged_paged_attention(\n     v = kv_pages[indices, :, 1::2, :].reshape(-1, num_kv_heads, head_dim)[\n         :kv_len\n     ]\n+    if k_scale is not None:\n+      k = k.astype(jnp.float32) * k_scale\n+      k = k.astype(q.dtype)\n+    if v_scale is not None:\n+      v = v.astype(jnp.float32) * v_scale\n+      v = v.astype(q.dtype)\n     k = jnp.repeat(k, num_query_per_kv, axis=1)\n     v = jnp.repeat(v, num_query_per_kv, axis=1)\n     attn = jnp.einsum(\"qhd,khd->hqk\", q, k, preferred_element_type=jnp.float32)\n@@ -150,7 +162,9 @@ def dynamic_validate_inputs(\n     sliding_window: int | None = None,\n     soft_cap: float | None = None,\n     mask_value: float | None = None,\n-    # Kernel specific params.\n+    k_scale: float | None = None,\n+    v_scale: float | None = None,\n+    # Kernel tuning params.\n     num_kv_pages_per_block: int | None = None,\n     num_queries_per_block: int | None = None,\n     vmem_limit_bytes: int | None = None,\n@@ -166,6 +180,8 @@ def dynamic_validate_inputs(\n       sliding_window=sliding_window,\n       soft_cap=soft_cap,\n       mask_value=mask_value,\n+      k_scale=k_scale,\n+      v_scale=v_scale,\n       num_kv_pages_per_block=num_kv_pages_per_block,\n       num_queries_per_block=num_queries_per_block,\n       vmem_limit_bytes=vmem_limit_bytes,\n@@ -210,7 +226,9 @@ def static_validate_inputs(\n     sliding_window: int | None = None,\n     soft_cap: float | None = None,\n     mask_value: float | None = None,\n-    # Kernel specific params.\n+    k_scale: float | None = None,\n+    v_scale: float | None = None,\n+    # Kernel tuning params.\n     num_kv_pages_per_block: int | None = None,\n     num_queries_per_block: int | None = None,\n     vmem_limit_bytes: int | None = None,\n@@ -218,6 +236,8 @@ def static_validate_inputs(\n   _, num_q_heads, head_dim = q.shape\n   _, _, num_combined_kv_heads, head_dim_k = kv_pages.shape\n   assert num_combined_kv_heads % 2 == 0\n+  assert isinstance(k_scale, float) or k_scale is None\n+  assert isinstance(v_scale, float) or v_scale is None\n   num_kv_heads = num_combined_kv_heads // 2\n   max_num_seqs, pages_per_seq = page_indices.shape\n   if num_seqs.shape != (1,):\n@@ -291,6 +311,8 @@ def ragged_paged_attention_kernel(\n     sliding_window: int | None = None,\n     soft_cap: float | None = None,\n     mask_value: float | None = DEFAULT_MASK_VALUE,\n+    k_scale: float | None = None,\n+    v_scale: float | None = None,\n ):\n   if mask_value is None:\n     mask_value = DEFAULT_MASK_VALUE\n@@ -334,23 +356,41 @@ def create_kv_async_copy_descriptors(\n     return async_copy_kv\n \n   # TODO(jevinjiang): Add these to Mosaic:\n-  # 1. Support arbitrary strided load/store for any dtype.\n+  # 1. Support arbitrary strided load/store for int4 and int8 dtype.\n   # 2. Support arbitrary strided load/store for any last dimension.\n   def strided_load_kv(ref, start, step):\n-    if ref.dtype == jnp.float32:\n-      return ref[start::step, :], ref[start + 1 :: step, :]\n     packing = get_dtype_packing(ref.dtype)\n-    assert ref.dtype == jnp.bfloat16\n+    if packing == 1:\n+      return [ref[start::step, :]], [ref[start + 1 :: step, :]]\n+    assert packing in (2, 4, 8)\n     assert step % packing == 0\n+    k_list, v_list = [], []\n     b_start = start // packing\n     b_step = step // packing\n     b_ref = ref.bitcast(jnp.uint32)\n     b = b_ref[b_start::b_step, :]\n-    bk = b << 16\n-    bv = b & jnp.uint32(0xffff0000)\n-    k = pltpu.bitcast(bk, jnp.float32).astype(jnp.bfloat16)\n-    v = pltpu.bitcast(bv, jnp.float32).astype(jnp.bfloat16)\n-    return k, v\n+\n+    # TODO(chengjiyao): use the general strided loading logic for bf16 after\n+    # fixing the issue in mosaic's infer vector layout pass\n+    if ref.dtype == jnp.bfloat16:\n+      bk = b << 16\n+      bv = b & jnp.uint32(0xFFFF0000)\n+      k = pltpu.bitcast(bk, jnp.float32).astype(jnp.bfloat16)\n+      v = pltpu.bitcast(bv, jnp.float32).astype(jnp.bfloat16)\n+      k_list.append(k)\n+      v_list.append(v)\n+    else:\n+      bitwidth = 32 // packing\n+      bitcast_dst_dtype = jnp.dtype(f\"uint{bitwidth}\")\n+      for i in range(0, packing, 2):\n+        bk = b >> (i * bitwidth)\n+        k = pltpu.bitcast(bk.astype(bitcast_dst_dtype), ref.dtype)\n+        k_list.append(k)\n+        bv = b >> ((i + 1) * bitwidth)\n+        v = pltpu.bitcast(bv.astype(bitcast_dst_dtype), ref.dtype)\n+        v_list.append(v)\n+\n+    return k_list, v_list\n \n   def fold_on_2nd_minor(vec):\n     assert vec.dtype == jnp.bfloat16 or vec.dtype == jnp.float32\n@@ -578,25 +618,42 @@ def prefetch_next_kv_blk():\n           num_kv_pages_per_blk * page_size * num_combined_kv_heads_per_blk,\n           head_dim,\n       )\n-      for kv_head_idx in range(num_kv_heads_per_blk):\n-        q_head_idx = kv_head_idx * num_q_heads_per_kv_head\n-        # TODO(jevinjiang): extra handlig for packed type that can start at\n-        # unaligned position!\n-        q = fold_on_2nd_minor(\n-            q_ref[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :]\n-        )\n-        k, v = strided_load_kv(\n-            kv_ref, kv_head_idx * 2, num_combined_kv_heads_per_blk\n-        )\n-        flash_attention(\n-            q,\n-            k,\n-            v,\n-            l_ref.at[kv_head_idx],\n-            m_ref.at[kv_head_idx],\n-            acc_ref.at[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :],\n-            kv_blk_idx=kv_blk_idx,\n+      kv_packing = get_dtype_packing(kv_ref.dtype)\n+      # NOTE: kv_packing is divided by 2 because k and v are packed together.\n+      kv_load_step = max(1, kv_packing // 2)\n+      for kv_head_chunk_idx in range(0, num_kv_heads_per_blk, kv_load_step):\n+        k_list, v_list = strided_load_kv(\n+            kv_ref, kv_head_chunk_idx * 2, num_combined_kv_heads_per_blk\n         )\n+        for step_idx in range(kv_load_step):\n+          k = k_list[step_idx]\n+          v = v_list[step_idx]\n+          if k_scale is not None:\n+            # NOTE: Conversion between arbitrary data types is not supported.\n+            # That's why it is converted to float32 first.\n+            k = k.astype(jnp.float32) * k_scale\n+            k = k.astype(q_ref.dtype)\n+          if v_scale is not None:\n+            v = v.astype(jnp.float32) * v_scale\n+            v = v.astype(q_ref.dtype)\n+          kv_head_idx = kv_head_chunk_idx + step_idx\n+          q_head_idx = kv_head_idx * num_q_heads_per_kv_head\n+          # TODO(jevinjiang): extra handlig for packed type that can start at\n+          # unaligned position!\n+          q = fold_on_2nd_minor(\n+              q_ref[:, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :]\n+          )\n+          flash_attention(\n+              q,\n+              k,\n+              v,\n+              l_ref.at[kv_head_idx],\n+              m_ref.at[kv_head_idx],\n+              acc_ref.at[\n+                  :, q_head_idx : q_head_idx + num_q_heads_per_kv_head, :\n+              ],\n+              kv_blk_idx=kv_blk_idx,\n+          )\n       return kv_blk_idx + 1, next_buf_idx\n \n     _, next_buf_idx = lax.while_loop(\n@@ -625,15 +682,8 @@ def cdiv(a, b):\n \n \n def get_dtype_packing(dtype):\n-  if dtype == jnp.float32:\n-    return 1\n-  if dtype == jnp.bfloat16:\n-    return 2\n-  if dtype == jnp.int8:\n-    return 4\n-  if dtype == jnp.int4:\n-    return 8\n-  raise ValueError(f\"Not implemented: unsupported {dtype=}\")\n+  bits = dtypes.bit_width(dtype)\n+  return 32 // bits\n \n \n def get_min_heads_per_blk(\n@@ -681,6 +731,8 @@ def can_be_xla_fully_tiled(x, packing):\n         \"vmem_limit_bytes\",\n         \"sliding_window\",\n         \"soft_cap\",\n+        \"k_scale\",\n+        \"v_scale\",\n     ],\n )\n def ragged_paged_attention(\n@@ -696,6 +748,8 @@ def ragged_paged_attention(\n     sliding_window: int | None = None,\n     soft_cap: float | None = None,\n     mask_value: float | None = DEFAULT_MASK_VALUE,\n+    k_scale: float | None = None,\n+    v_scale: float | None = None,\n     num_kv_pages_per_block: int | None = None,\n     num_queries_per_block: int | None = None,\n     vmem_limit_bytes: int | None = None,\n@@ -715,6 +769,8 @@ def ragged_paged_attention(\n     sliding_window: the sliding window size for the attention.\n     soft_cap: the logit soft cap for the attention.\n     mask_value: mask value for causal mask.\n+    k_scale: the scale for the key cache.\n+    v_scale: the scale for the value cache.\n     num_kv_pages_per_block: number of kv pages to be processed in one flash\n       attention block in the pallas kernel.\n     num_queries_per_block: number of kv pages to be processed in one flash\n@@ -735,6 +791,8 @@ def ragged_paged_attention(\n       sliding_window=sliding_window,\n       soft_cap=soft_cap,\n       mask_value=mask_value,\n+      k_scale=k_scale,\n+      v_scale=v_scale,\n       num_kv_pages_per_block=num_kv_pages_per_block,\n       num_queries_per_block=num_queries_per_block,\n       vmem_limit_bytes=vmem_limit_bytes,\n@@ -823,6 +881,8 @@ def q_index_map(heads_blk_idx, q_blk_idx, *_):\n           sliding_window=sliding_window,\n           soft_cap=soft_cap,\n           mask_value=mask_value,\n+          k_scale=k_scale,\n+          v_scale=v_scale,\n       ),\n       grid_spec=pltpu.PrefetchScalarGridSpec(\n           num_scalar_prefetch=len(scalar_prefetches),\ndiff --git a/tests/pallas/tpu_ragged_paged_attention_test.py b/tests/pallas/tpu_ragged_paged_attention_test.py\nindex 4265445c69c7..eebc292ce3ab 100644\n--- a/tests/pallas/tpu_ragged_paged_attention_test.py\n+++ b/tests/pallas/tpu_ragged_paged_attention_test.py\n@@ -17,6 +17,7 @@\n from absl.testing import absltest\n from absl.testing import parameterized\n import jax\n+from jax._src import dtypes\n from jax._src import test_util as jtu\n from jax.experimental.pallas.ops.tpu.ragged_paged_attention import (\n     cdiv,\n@@ -39,7 +40,8 @@ def _test_ragged_paged_attention(\n       num_heads,  # [num_q_heads, num_kv_heads]\n       head_dim,\n       page_size,\n-      dtype,\n+      q_dtype,\n+      kv_dtype,\n       num_pages,\n       *,\n       num_kv_pages_per_block=8,\n@@ -49,6 +51,8 @@ def _test_ragged_paged_attention(\n       max_num_seq=8,\n       sliding_window: int | None = None,\n       soft_cap: float | None = None,\n+      k_scale: float | None = None,\n+      v_scale: float | None = None,\n   ):\n     if not jtu.is_device_tpu_at_least(version=4):\n       self.skipTest(\"Expect TPUv4+\")\n@@ -70,17 +74,27 @@ def _test_ragged_paged_attention(\n     q = jax.random.normal(\n         k0,\n         (max_num_batched_tokens, num_q_heads, head_dim),\n-        dtype=dtype,\n+        dtype=q_dtype,\n     )\n     page_cnt = 0\n     page_indices_list = []\n     kv_pages_list = []\n     for kv_len in kv_lens:\n-      kv = jax.random.normal(\n-          k1,\n-          (kv_len, num_kv_heads * 2, head_dim),\n-          dtype=dtype,\n-      )\n+      if jnp.issubdtype(kv_dtype, jnp.integer):\n+        # random.randint doesn't support int4, so we use jnp.int32 here and then\n+        # convert to the desired dtype.\n+        kv = jax.random.normal(\n+            k1,\n+            (kv_len, num_kv_heads * 2, head_dim),\n+            dtype=jnp.int32,\n+        )\n+        kv = kv.astype(kv_dtype)\n+      else:\n+        kv = jax.random.normal(\n+            k1,\n+            (kv_len, num_kv_heads * 2, head_dim),\n+            dtype=kv_dtype,\n+        )\n       kv = jnp.pad(\n           kv,\n           ((0, cdiv(kv_len, page_size) * page_size - kv_len), (0, 0), (0, 0)),\n@@ -138,7 +152,9 @@ def _test_ragged_paged_attention(\n         vmem_limit_bytes=vmem_limit_bytes,\n         sliding_window=sliding_window,\n         soft_cap=soft_cap,\n-    )[: actual_num_q_tokens]\n+        k_scale=k_scale,\n+        v_scale=v_scale,\n+    )[:actual_num_q_tokens]\n \n     expected = ref_ragged_paged_attention(\n         q,\n@@ -149,12 +165,17 @@ def _test_ragged_paged_attention(\n         num_seqs=num_seqs,\n         sliding_window=sliding_window,\n         soft_cap=soft_cap,\n+        k_scale=k_scale,\n+        v_scale=v_scale,\n     )\n+    dtype_bits = dtypes.bit_width(jnp.dtype(kv_dtype))\n     tols = {\n-        \"float32\": 0.15,\n-        \"bfloat16\": 0.2,\n+        32: 0.15,\n+        16: 0.2,\n+        8: 0.2,\n+        4: 0.2,\n     }\n-    tol = tols[jnp.dtype(dtype).name]\n+    tol = tols[dtype_bits]\n     self.assertAllClose(output, expected, atol=tol, rtol=tol)\n \n   @parameterized.product(\n@@ -173,9 +194,40 @@ def test_ragged_paged_attention_basic(self, dtype):\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n     )\n \n+  # TODO: support int4 and int8\n+  @parameterized.product(\n+      q_dtype=[jnp.bfloat16],\n+      kv_dtype=[jnp.float8_e5m2, jnp.float8_e4m3fn],\n+      kv_scales=[(0.5, 0.5), (None, None)],\n+  )\n+  def test_ragged_paged_attention_quantized_kv_cache(\n+      self, q_dtype, kv_dtype, kv_scales\n+  ):\n+    if not jtu.is_device_tpu_at_least(version=5):\n+      self.skipTest(\"Expect TPUv5+\")\n+    seq_lens = [(192, 328), (128, 180), (64, 255)]\n+    num_heads = (32, 8)\n+    head_dim = 128\n+    page_size = 16\n+    num_pages = 1000\n+    k_scale, v_scale = kv_scales\n+\n+    self._test_ragged_paged_attention(\n+        seq_lens,\n+        num_heads,\n+        head_dim,\n+        page_size,\n+        q_dtype,\n+        kv_dtype,\n+        num_pages,\n+        k_scale=k_scale,\n+        v_scale=v_scale,\n+    )\n+\n   @parameterized.product(\n       dtype=[jnp.float32, jnp.bfloat16],\n   )\n@@ -209,6 +261,7 @@ def test_ragged_paged_attention_decode_only(self, dtype):\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n     )\n \n@@ -245,6 +298,7 @@ def test_ragged_paged_attention_prefill_only(self, dtype):\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n     )\n \n@@ -281,6 +335,7 @@ def test_ragged_paged_attention_mixed(self, dtype):\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n     )\n \n@@ -316,6 +371,7 @@ def test_ragged_paged_attention_complex(\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n         num_kv_pages_per_block=num_kv_pages_per_block,\n         num_queries_per_block=num_queries_per_block,\n@@ -351,6 +407,7 @@ def test_ragged_paged_attention_sliding_window(\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n         num_kv_pages_per_block=num_kv_pages_per_block,\n         num_queries_per_block=num_queries_per_block,\n@@ -386,6 +443,7 @@ def test_ragged_paged_attention_logit_soft_capping(\n         head_dim,\n         page_size,\n         dtype,\n+        dtype,\n         num_pages,\n         num_kv_pages_per_block=num_kv_pages_per_block,\n         num_queries_per_block=num_queries_per_block,\n@@ -407,6 +465,7 @@ def test_ragged_paged_attention_sliding_window_should_be_positive(self):\n           head_dim,\n           page_size,\n           dtype,\n+          dtype,\n           num_pages,\n           sliding_window=0,\n       )\n@@ -418,6 +477,7 @@ def test_ragged_paged_attention_sliding_window_should_be_positive(self):\n           head_dim,\n           page_size,\n           dtype,\n+          dtype,\n           num_pages,\n           sliding_window=-1,\n       )\n@@ -437,6 +497,7 @@ def test_ragged_paged_attention_soft_cap_cannot_be_zero(self):\n           head_dim,\n           page_size,\n           dtype,\n+          dtype,\n           num_pages,\n           soft_cap=0.0,\n       )\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Simplify attention VJP definition\n\nPiperOrigin-RevId: 762567722",
        "output": [
            "```diff\nCommit: 292dea67fa3f550b8add78ea1840b566d95069b1\nDate: 2025-05-23T21:25:49Z\nURL: https://github.com/jax-ml/jax/commit/292dea67fa3f550b8add78ea1840b566d95069b1\nFiles changed: 2\nAdditions: +54, Deletions: -73\ndiff --git a/jax/experimental/pallas/ops/gpu/attention.py b/jax/experimental/pallas/ops/gpu/attention.py\nindex ccb3ae8fd3b7..2442ed14f351 100644\n--- a/jax/experimental/pallas/ops/gpu/attention.py\n+++ b/jax/experimental/pallas/ops/gpu/attention.py\n@@ -152,7 +152,7 @@ def body(start_k, carry):\n       # Apply mask to qk.\n       qk = jnp.where(mask, qk, DEFAULT_MASK_VALUE)\n \n-    m_curr = qk.max(axis=-1)\n+    m_curr = jnp.max(qk, axis=-1)\n     m_next = jnp.maximum(m_prev, m_curr)\n     correction = jnp.exp2(m_prev - m_next)\n     l_prev_corr = correction * l_prev\n@@ -201,7 +201,7 @@ def segment_mask(\n \n \n @functools.partial(\n-    jax.custom_vjp, nondiff_argnums=[4, 5, 6, 7, 8, 9, 10, 11, 12]\n+    jax.custom_vjp, nondiff_argnums=[4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n )\n @functools.partial(\n     jax.jit,\n@@ -215,6 +215,7 @@ def segment_mask(\n         \"grid\",\n         \"interpret\",\n         \"debug\",\n+        \"return_residuals\",\n     ],\n )\n def mha(\n@@ -231,6 +232,7 @@ def mha(\n     grid: tuple[int, ...] | None = None,\n     interpret: bool = False,\n     debug: bool = False,\n+    return_residuals: bool = False,\n ):\n   del backward_pass_impl\n   batch_size, q_seq_len, num_heads, head_dim = q.shape\n@@ -273,14 +275,19 @@ def mha(\n       if segment_ids is None\n       else pl.BlockSpec((None, kv_seq_len), lambda _, j, k: (j, 0))\n   )\n-  out_shape = jax.ShapeDtypeStruct(shape=q.shape, dtype=q.dtype)\n-  return pl.pallas_call(\n+  out_shape = [q]\n+  out_specs = [pl.BlockSpec((None, block_q, None, head_dim_padded),\n+                            lambda i, j, k: (j, i, k, 0))]\n+  if return_residuals:\n+    out_shape.append(jax.ShapeDtypeStruct(\n+        shape=(batch_size, num_heads, q_seq_len), dtype=jnp.float32))  # lse\n+    out_specs.append(\n+        pl.BlockSpec((None, None, block_q), lambda i, j, k: (j, k, i)))  # lse\n+  out = pl.pallas_call(\n       kernel,\n       grid=grid_,\n       in_specs=in_specs,\n-      out_specs=pl.BlockSpec(\n-          (None, block_q, None, head_dim_padded), lambda i, j, k: (j, i, k, 0)\n-      ),\n+      out_specs=out_specs,\n       compiler_params=plgpu.TritonCompilerParams(\n           num_warps=num_warps_, num_stages=num_stages),\n       out_shape=out_shape,\n@@ -288,6 +295,7 @@ def mha(\n       interpret=interpret,\n       name=\"mha_forward\",\n   )(q, k, v, segment_ids)\n+  return out if return_residuals else out[0]\n \n \n def _mha_forward(\n@@ -304,71 +312,17 @@ def _mha_forward(\n     grid: Any,\n     interpret: bool,\n     debug: bool,\n+    return_residuals: bool,\n ):\n-  del backward_pass_impl\n-  batch_size, q_seq_len, num_heads, head_dim = q.shape\n-  kv_seq_len = k.shape[1]\n-  block_q = min(block_sizes.block_q, q_seq_len)\n-  block_k = min(block_sizes.block_k, kv_seq_len)\n-  if (q.shape[-1] != k.shape[-1]) or (q.shape[-1] != v.shape[-1]):\n-    raise ValueError(\n-        f\"This kernel expects q, k, and v to have the same head dimension, but\"\n-        f\" found {q.shape=}, {k.shape=}, {v.shape=}.\"\n-    )\n-  if q_seq_len % block_q != 0:\n-    raise ValueError(f\"{q_seq_len=} must be a multiple of {block_q=}\")\n-  if kv_seq_len % block_k != 0:\n-    raise ValueError(f\"{kv_seq_len=} must be a multiple of {block_k=}\")\n-  head_dim_padded = pl.next_power_of_2(head_dim)\n-\n-  # Heuristics.\n-  grid_ = grid\n-  if grid_ is None:\n-    grid_ = (pl.cdiv(q_seq_len, block_q), batch_size, num_heads)\n-\n-  num_warps_ = num_warps\n-  if num_warps_ is None:\n-    num_warps_ = 4 if head_dim <= 64 else 8\n-  kernel = functools.partial(mha_forward_kernel, sm_scale=sm_scale,\n-                             causal=causal, block_q=block_q, block_k=block_k,\n-                             head_dim=head_dim)\n-  out_shape = [\n-      jax.ShapeDtypeStruct(shape=q.shape, dtype=q.dtype),  # out\n-      jax.ShapeDtypeStruct(\n-          shape=(batch_size, num_heads, q_seq_len), dtype=jnp.float32  # lse\n-      ),\n-  ]\n-  in_specs = [\n-      pl.BlockSpec((None, block_q, None, head_dim_padded),\n-                   lambda i, j, k: (j, i, k, 0)),\n-      pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n-                   lambda _, j, k: (j, 0, k, 0)),\n-      pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n-                   lambda _, j, k: (j, 0, k, 0)),\n-  ]\n-  in_specs.append(\n-      None  # type: ignore[arg-type]\n-      if segment_ids is None\n-      else pl.BlockSpec((None, kv_seq_len), lambda _, j, k: (j, 0))\n-  )\n-  out, lse = pl.pallas_call(\n-      kernel,\n-      grid=grid_,\n-      in_specs=in_specs,\n-      out_specs=[\n-          pl.BlockSpec((None, block_q, None, head_dim_padded),\n-                       lambda i, j, k: (j, i, k, 0)),\n-          pl.BlockSpec((None, None, block_q), lambda i, j, k: (j, k, i)),\n-      ],\n-      compiler_params=plgpu.TritonCompilerParams(\n-          num_warps=num_warps_, num_stages=num_stages\n-      ),\n-      out_shape=out_shape,\n-      debug=debug,\n-      interpret=interpret,\n-      name=\"mha_forward\",\n-  )(q, k, v, segment_ids)\n-  return out, (q, k, v, segment_ids, out, lse)\n+  out, lse = mha(q, k, v, segment_ids=segment_ids, sm_scale=sm_scale,\n+                 causal=causal, block_sizes=block_sizes,\n+                 backward_pass_impl=backward_pass_impl,\n+                 num_warps=num_warps, num_stages=num_stages,\n+                 grid=grid, interpret=interpret, debug=debug,\n+                 return_residuals=True)\n+  residuals = (q, k, v, segment_ids, out, lse)\n+  ret = (out, lse) if return_residuals else out\n+  return ret, residuals\n \n \n def _preprocess_backward_kernel(out_ref, dout_ref, delta_ref, head_dim: int):\n@@ -576,9 +530,12 @@ def inner_loop_dq(start_k, dq):\n def _mha_backward(sm_scale: float, causal: bool, block_sizes: BlockSizes,\n                   backward_pass_impl: str, num_warps: int | None,\n                   num_stages: int, grid: Any, interpret: bool,\n-                  debug: bool, res, do):\n-  del num_stages, grid\n+                  debug: bool, return_residuals: bool, res, do):\n+  if return_residuals:\n+    raise ValueError(\n+        \"Kernel differentiation is not supported if return_residuals is True.\")\n   q, k, v, segment_ids, out, lse = res\n+  del num_stages, grid, return_residuals\n \n   if backward_pass_impl == \"xla\":\n     return jax.vjp(\ndiff --git a/tests/pallas/gpu_ops_test.py b/tests/pallas/gpu_ops_test.py\nindex 1637686365e1..cc2d15a8fdee 100644\n--- a/tests/pallas/gpu_ops_test.py\n+++ b/tests/pallas/gpu_ops_test.py\n@@ -313,6 +313,30 @@ def f_ref(q, k, v):\n     self.assertAllClose(dk, dk_ref, atol=5e-2)\n     self.assertAllClose(dv, dv_ref, atol=5e-2)\n \n+  def test_return_residuals_not_differentiable(self):\n+    batch_size, seq_len, num_heads, head_dim = 2, 128, 2, 128\n+    causal = False\n+    k1, k2, k3 = random.split(random.key(0), 3)\n+    q = random.normal(\n+        k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n+    )\n+    k = random.normal(\n+        k2, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n+    )\n+    v = random.normal(\n+        k3, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n+    )\n+    segment_ids = None\n+\n+    def f(q, k, v):\n+      return attention.mha(q, k, v, causal=causal, segment_ids=segment_ids,\n+                           interpret=self.INTERPRET,\n+                           return_residuals=True)[0].sum()\n+\n+    with self.assertRaisesRegex(ValueError, \"Kernel differentiation is not\"\n+                                \" supported if return_residuals is True.\"):\n+      _ = jax.grad(f, argnums=(0, 1, 2))(q, k, v)\n+\n \n class FusedAttentionInterpretTest(FusedAttentionTest):\n   INTERPRET = True\n\n```"
        ],
        "from_id": [
            "rdyro",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28979 from jakevdp:pytree-err\n\nPiperOrigin-RevId: 762562570",
        "output": [
            "```diff\nCommit: 704c3c625979362e984ff9dd1c415867f8910db2\nDate: 2025-05-23T21:10:25Z\nURL: https://github.com/jax-ml/jax/commit/704c3c625979362e984ff9dd1c415867f8910db2\nFiles changed: 3\nAdditions: +32, Deletions: -3\ndiff --git a/jaxlib/pytree.cc b/jaxlib/pytree.cc\nindex 2700ac9e6c9a..bd845c47ec1e 100644\n--- a/jaxlib/pytree.cc\n+++ b/jaxlib/pytree.cc\n@@ -281,8 +281,16 @@ bool PyTreeDef::operator==(const PyTreeDef& other) const {\n         a.custom != b.custom) {\n       return false;\n     }\n-    if (a.node_data && a.node_data.not_equal(b.node_data)) {\n-      return false;\n+    try {\n+      if (a.node_data && a.node_data.not_equal(b.node_data)) {\n+        return false;\n+      }\n+    } catch (nb::python_error& e) {\n+      nb::raise_from(e, PyExc_ValueError,\n+                     \"Exception raised while checking equality of metadata \"\n+                     \"fields of pytree. Make sure that metadata fields are \"\n+                     \"hashable and have simple equality semantics. (Note: \"\n+                     \"arrays cannot be passed as metadata fields!)\");\n     }\n     if (!IsSortedPyDictKeysEqual(a.sorted_dict_keys, b.sorted_dict_keys)) {\n       return false;\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 24861bad81de..b9497b71dcb1 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 345\n+_version = 346\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\ndiff --git a/tests/tree_util_test.py b/tests/tree_util_test.py\nindex 8d4cd5854e7d..0d92156b2530 100644\n--- a/tests/tree_util_test.py\n+++ b/tests/tree_util_test.py\n@@ -1050,6 +1050,27 @@ def testPickle(self):\n       unpickled = pickle.loads(pickle.dumps(key))\n       self.assertEqual(key, unpickled)\n \n+  def testEqualityErrorWithArrayAsStaticArg(self):\n+    # Regression test for https://github.com/jax-ml/jax/issues/28659\n+    @tree_util.register_dataclass\n+    @dataclasses.dataclass\n+    class Tree:\n+      x : jnp.ndarray = dataclasses.field(metadata={'static': True})\n+\n+    f = jax.jit(lambda x: x)\n+\n+    if jax._src.lib.jaxlib_extension_version < 346:\n+      msg = \"The truth value of an array with more than one element is ambiguous.\"\n+    else:\n+      msg = \"Exception raised while checking equality of metadata fields of pytree.\"\n+\n+    # First call succeeds, because there is no equality check.\n+    f(Tree(jnp.arange(4)))\n+\n+    # Second fall fails, because arrays are marked static and compared for equality.\n+    with self.assertRaisesRegex(ValueError, msg):\n+      f(Tree(jnp.arange(4)))\n+\n \n class StaticTest(parameterized.TestCase):\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "TSAN CI, make jax buid/test step fail if missing deps wheels",
        "output": [
            "```diff\nCommit: ae2f943b54bd353ad3731fc07b6ef3d723f83446\nDate: 2025-05-23T20:43:44Z\nURL: https://github.com/jax-ml/jax/commit/ae2f943b54bd353ad3731fc07b6ef3d723f83446\nFiles changed: 1\nAdditions: +7, Deletions: -3\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex ce4130c31a30..6cd502050344 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -116,6 +116,7 @@ jobs:\n       - name: Build TSAN Numpy wheel\n         if: steps.cache-numpy-tsan-restore.outputs.cache-hit != 'true'\n         run: |\n+          set -eux\n           cd numpy\n \n           # If we restored cpython from cache, we need to get python interpreter from python-tsan.tgz\n@@ -131,7 +132,6 @@ jobs:\n           export PATH=${GITHUB_WORKSPACE}/cpython-tsan/bin/:$PATH\n \n           python3 -m pip install uv~=0.5.30\n-\n           python3 -m uv pip install -r requirements/build_requirements.txt\n \n           CC=clang-18 CXX=clang++-18 python3 -m pip wheel --wheel-dir dist -v . --no-build-isolation -Csetup-args=-Db_sanitize=thread -Csetup-args=-Dbuildtype=debugoptimized\n@@ -268,11 +268,15 @@ jobs:\n             --bazel_options=--copt=-g \\\n             --clang_path=/usr/bin/clang-18\n \n-\n           mkdir -p dist\n+          # Check whether we have numpy wheel or exit with error\n+          ls ${GITHUB_WORKSPACE}/wheelhouse/numpy/*.whl || exit 1\n           cp -v ${GITHUB_WORKSPACE}/wheelhouse/numpy/*.whl dist/\n-          cp -v ${GITHUB_WORKSPACE}/wheelhouse/scipy/*.whl dist/\n           if [ \"${{ matrix.python-version }}\" == \"3.14\" ]; then\n+            # Check whether we have scipy wheel or exit with error\n+            ls ${GITHUB_WORKSPACE}/wheelhouse/scipy/*.whl || exit 1\n+            cp -v ${GITHUB_WORKSPACE}/wheelhouse/scipy/*.whl dist/\n+\n             # Patch build/requirements_lock_3_14_ft.txt to use TSAN instrumented NumPy and Scipy\n             sed -i \"s|--extra-index-url.*|--extra-index-url file://${GITHUB_WORKSPACE}/wheelhouse/|\" build/${{ matrix.requirements_lock_name }}.txt\n \n\n```"
        ],
        "from_id": [
            "vfdev-5"
        ]
    },
    {
        "text_input": "This is a change to patch some internal Google builds while we complete a refactor.\n\nPiperOrigin-RevId: 762547026",
        "output": [
            "```diff\nCommit: 57d07e195b4643fd134abc175f46e108ea667875\nDate: 2025-05-23T20:32:14Z\nURL: https://github.com/jax-ml/jax/commit/57d07e195b4643fd134abc175f46e108ea667875\nFiles changed: 1\nAdditions: +7, Deletions: -0\ndiff --git a/jax/profiler.py b/jax/profiler.py\nindex 31f3ea186d79..d776791e9200 100644\n--- a/jax/profiler.py\n+++ b/jax/profiler.py\n@@ -14,6 +14,7 @@\n \n # Note: import <name> as <name> is required for names to be exported.\n # See PEP 484 & https://github.com/jax-ml/jax/issues/7570\n+from typing import Any\n \n from jax._src.profiler import (\n     ProfileOptions as ProfileOptions,\n@@ -28,3 +29,9 @@\n     stop_trace as stop_trace,\n     trace as trace,\n )\n+\n+# this is a temporary shim to please pytype in the meantime before the migration\n+# is complete for cl/760646494\n+ProfileData: Any = None\n+ProfileEvent: Any = None\n+ProfilePlane: Any = None\n\n```"
        ],
        "from_id": [
            "ZacCranko",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Transfer library: poison outstanding buffer fetches upon connection failure.\n\nPiperOrigin-RevId: 762546985",
        "output": [
            "```diff\nCommit: 9153ab760bc146945f572a79d86fc345286d5f46\nDate: 2025-05-23T20:31:51Z\nURL: https://github.com/jax-ml/jax/commit/9153ab760bc146945f572a79d86fc345286d5f46\nFiles changed: 1\nAdditions: +7, Deletions: -0\ndiff --git a/jaxlib/py_socket_transfer.cc b/jaxlib/py_socket_transfer.cc\nindex 8086196b9df8..c7cc7c496b0b 100644\n--- a/jaxlib/py_socket_transfer.cc\n+++ b/jaxlib/py_socket_transfer.cc\n@@ -163,6 +163,8 @@ class PyTransferServerConnection {\n     }\n   }\n \n+  SocketServer::Connection& conn() { return *conn_; }\n+\n  private:\n   tsl::RCReference<SocketServer::Connection> conn_;\n };\n@@ -257,6 +259,11 @@ struct CopyDests {\n \n void RegisterTransferServerTypes(nanobind::module_& m) {\n   nb::class_<PyTransferServerConnection>(m, \"TransferConnection\")\n+#if JAX_IFRT_VERSION_NUMBER > 9\n+      .def(\n+          \"_testonly_inject_failure\",\n+          [](PyTransferServerConnection& self) { self.conn().InjectFailure(); })\n+#endif\n       .def(\"_pull_flat\", [](PyTransferServerConnection& self, uint64_t uuid,\n                             xla::nb_class_ptr<xla::PyClient> py_client,\n                             std::vector<nb::object> py_avals) {\n\n```"
        ],
        "from_id": [
            "pschuh",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move jax/_src/custom_dce.py to its own BUILD rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nThis was unblocked by moving batching & ad to their own rules in prior changes.\n\nPiperOrigin-RevId: 762527517",
        "output": [
            "```diff\nCommit: f5a9d460723b417ca2032057f8b2ef5ad9e6fdf9\nDate: 2025-05-23T19:32:40Z\nURL: https://github.com/jax-ml/jax/commit/f5a9d460723b417ca2032057f8b2ef5ad9e6fdf9\nFiles changed: 1\nAdditions: +19, Deletions: -1\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 7c8847e2e94b..586e9c2dd6e6 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -303,7 +303,6 @@ py_library_providing_imports_info(\n         \"_src/callback.py\",\n         \"_src/checkify.py\",\n         \"_src/custom_batching.py\",\n-        \"_src/custom_dce.py\",\n         \"_src/custom_derivatives.py\",\n         \"_src/custom_partitioning.py\",\n         \"_src/custom_partitioning_sharding_rule.py\",\n@@ -390,6 +389,7 @@ py_library_providing_imports_info(\n         \":config\",\n         \":core\",\n         \":custom_api_util\",\n+        \":custom_dce\",\n         \":custom_transpose\",\n         \":deprecations\",\n         \":dtypes\",\n@@ -595,6 +595,24 @@ pytype_strict_library(\n     srcs = [\"_src/custom_api_util.py\"],\n )\n \n+pytype_strict_library(\n+    name = \"custom_dce\",\n+    srcs = [\"_src/custom_dce.py\"],\n+    deps = [\n+        \":ad\",\n+        \":api_util\",\n+        \":batching\",\n+        \":core\",\n+        \":custom_api_util\",\n+        \":mlir\",\n+        \":partial_eval\",\n+        \":source_info_util\",\n+        \":traceback_util\",\n+        \":tree_util\",\n+        \":util\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"custom_transpose\",\n     srcs = [\"_src/custom_transpose.py\"],\n\n```"
        ],
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add barrier transformation support to tcgen05_mma.\n\nAlso fix accumulator argument when it's dynamic.\n\nPiperOrigin-RevId: 762509416",
        "output": [
            "```diff\nCommit: c4a90c193473a686c08a31650f23f4dc436c801e\nDate: 2025-05-23T18:43:10Z\nURL: https://github.com/jax-ml/jax/commit/c4a90c193473a686c08a31650f23f4dc436c801e\nFiles changed: 2\nAdditions: +106, Deletions: -16\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 379a972be9b0..1ec22bff3f6d 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -554,6 +554,7 @@ def _copy_gmem_to_smem_lowering(\n   )\n   return ()\n \n+\n lowering.register_lowering_rule(\n     copy_gmem_to_smem_p,\n     mgpu.LoweringSemantics.Lane,\n@@ -722,9 +723,14 @@ def _barrier_wait_pp_eqn(\n \n \n @lowering.register_lowering_rule(barrier_wait_p, mgpu.LoweringSemantics.Lane)\n-@lowering.register_lowering_rule(barrier_wait_p, mgpu.LoweringSemantics.Lane,\n-                                 gpu_core.PrimitiveSemantics.Warp)\n-@lowering.register_lowering_rule(barrier_wait_p, mgpu.LoweringSemantics.Warpgroup)\n+@lowering.register_lowering_rule(\n+    barrier_wait_p,\n+    mgpu.LoweringSemantics.Lane,\n+    gpu_core.PrimitiveSemantics.Warp,\n+)\n+@lowering.register_lowering_rule(\n+    barrier_wait_p, mgpu.LoweringSemantics.Warpgroup\n+)\n def _barrier_wait_lowering(\n     ctx: lowering.LoweringRuleContext,\n     barrier,\n@@ -1198,18 +1204,31 @@ def tcgen05_mma(acc: _Ref,\n   else:\n     b_transforms_leaves, b_transforms_tree = [], None\n \n+  if isinstance(barrier, pallas_core.TransformedRef):\n+    barrier_transforms_leaves, barrier_transforms_tree = jax.tree.flatten(\n+        barrier.transforms\n+    )\n+    barrier = barrier.ref\n+  else:\n+    barrier_transforms_leaves, barrier_transforms_tree = [], None\n+\n   tcgen05_mma_p.bind(acc, a, b, barrier, accumulate,\n                       *a_transforms_leaves, *b_transforms_leaves,\n+                      *barrier_transforms_leaves,\n                       a_transforms_tree=a_transforms_tree,\n                       b_transforms_tree=b_transforms_tree,\n+                      barrier_transforms_tree=barrier_transforms_tree,\n                       collective_axis=collective_axis)\n \n+\n @tcgen05_mma_p.def_abstract_eval\n def _tcgen05_mma_abstract_eval(acc, a, b, barrier, accumulate,\n                                *transforms_leaves,\n                                a_transforms_tree, b_transforms_tree,\n+                               barrier_transforms_tree,\n                                collective_axis):\n-  del (accumulate, transforms_leaves, a_transforms_tree, b_transforms_tree)\n+  del (accumulate, transforms_leaves, a_transforms_tree, b_transforms_tree,\n+       barrier_transforms_tree)\n \n   if acc.memory_space != gpu_core.TMEM:\n     raise ValueError(\"Accumulator must be a TMEM Ref.\")\n@@ -1233,6 +1252,7 @@ def _tcgen05_mma_abstract_eval(acc, a, b, barrier, accumulate,\n \n   return []\n \n+\n @lowering.register_lowering_rule(tcgen05_mma_p, *gpu_core.LANExWG_SEMANTICS)\n @lowering.register_lowering_rule(tcgen05_mma_p, *gpu_core.LANExWARP_SEMANTICS)\n def _tcgen05_mma_lowering(\n@@ -1245,16 +1265,26 @@ def _tcgen05_mma_lowering(\n     *transforms_leaves,\n     a_transforms_tree,\n     b_transforms_tree,\n+    barrier_transforms_tree,\n     collective_axis,\n ):\n   _, a_aval, b_aval, *_ = ctx.avals_in\n   lhs_swizzle: int | None = None\n   lhs_transpose: bool = False\n-  if a_transforms_tree is not None:\n-    a_transforms_leaves, b_transforms_leaves = util.split_list(\n-        transforms_leaves, [a_transforms_tree.num_leaves]\n-    )\n \n+  transforms_trees = (\n+      a_transforms_tree,\n+      b_transforms_tree,\n+      barrier_transforms_tree,\n+  )\n+  (a_transforms_leaves, b_transforms_leaves, barrier_transforms_leaves, _) = (\n+      util.split_list(\n+          transforms_leaves,\n+          [getattr(tree, \"num_leaves\", 0) for tree in transforms_trees],\n+      )\n+  )\n+\n+  if a_transforms_tree is not None:\n     a_transforms = a_transforms_tree.unflatten(a_transforms_leaves)\n     a_ref, a_transforms = lowering._handle_transforms(\n         ctx, a_ref, a_transforms, handle_transposes=False, handle_reshapes=True\n@@ -1276,9 +1306,8 @@ def _tcgen05_mma_lowering(\n     if lhs_tiling != (8, swizzle_elems):\n       raise ValueError(\"MMA lhs tiling does not fit swizzle. \"\n                        f\"{lhs_tiling=} expected={(8, swizzle_elems)}\")\n-  else:\n-    b_transforms_leaves = transforms_leaves  # type: ignore\n \n+  assert b_transforms_tree is not None\n   b_transforms = b_transforms_tree.unflatten(b_transforms_leaves)\n   b_ref, b_transforms = lowering._handle_transforms(\n       ctx, b_ref, b_transforms, handle_transposes=False, handle_reshapes=True\n@@ -1296,16 +1325,28 @@ def _tcgen05_mma_lowering(\n       raise NotImplementedError(\n           f\"Unsupported transforms: {b_transforms}.\"\n       )\n-\n   swizzle_elems = rhs_swizzle // b_aval.dtype.itemsize\n+  if rhs_tiling != (8, swizzle_elems):\n+    raise ValueError(\n+        \"MMA rhs tiling does not fit swizzle\"\n+        f\" {rhs_tiling=} expected={(8, swizzle_elems)}\"\n+    )\n+\n+  if barrier_transforms_tree is not None:\n+    barrier_transforms = barrier_transforms_tree.unflatten(\n+        barrier_transforms_leaves\n+    )\n+    indexer = _extract_barrier_indexer(barrier_transforms)\n+    if indexer is not None:\n+      barrier_ref = barrier_ref.__getitem__(\n+          *map(lowering._as_index, indexer.indices)\n+      )\n+\n   if lhs_swizzle is None:\n     lhs_swizzle = rhs_swizzle\n   elif rhs_swizzle != lhs_swizzle:\n     raise ValueError(\"MMA rhs swizzle must match lhs swizzle.\"\n                       f\" {lhs_swizzle=} {rhs_swizzle=}\")\n-  if rhs_tiling != (8, swizzle_elems):\n-    raise ValueError(\"MMA rhs tiling does not fit swizzle\"\n-                      f\" {rhs_tiling=} expected={(8, swizzle_elems)}\")\n   if lhs_transpose:\n     if isinstance(a_ref, tcgen05.TMEMRef):\n       raise ValueError(\"TMEM transpose not allowed.\")\n@@ -1314,6 +1355,9 @@ def _tcgen05_mma_lowering(\n     b_ref = mgpu.memref_transpose(b_ref, (1, 0, 3, 2))\n   if isinstance(accumulate, bool):\n     accumulate = mgpu.c(accumulate, ir.IntegerType.get_signless(1))\n+  elif isinstance(accumulate, mgpu.FragmentedArray):\n+    accumulate = accumulate.registers.item()\n+    assert isinstance(accumulate, ir.Value)\n \n   predicate = ctx.module_ctx.single_lane_predicate\n   collective = False\n@@ -1341,8 +1385,8 @@ def _tcgen05_mma_lowering(\n               acc,\n               a_ref,\n               b_ref,\n-              a_swizzle=lhs_swizzle,\n-              b_swizzle=rhs_swizzle,\n+              a_swizzle=int(lhs_swizzle),\n+              b_swizzle=int(rhs_swizzle),\n               accumulate=accumulate,\n               collective=collective,\n           )\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 029445143a0f..3c0b463ba1c7 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -2496,6 +2496,52 @@ def _scoped(a_smem, b_smem,\n     expected = x @ y\n     np.testing.assert_allclose(result, expected, rtol=1e-3)\n \n+  @parameterized.parameters((0,), (1,))\n+  def test_mma_barrier_indexing(\n+      self, barrier_index, shape=(128, 128), swizzle=128, dtype=jnp.float16\n+  ):\n+    self.skip_if_wg_semantics()\n+    swizzle_elems = swizzle // jnp.dtype(dtype).itemsize\n+    transforms = (\n+        plgpu.TilingTransform((8, swizzle_elems)),\n+        plgpu.SwizzleTransform(swizzle),\n+    )\n+\n+    def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref):\n+      plgpu.tcgen05_mma(\n+          acc_tmem,\n+          a_smem,\n+          b_smem,\n+          barrier_ref.at[barrier_index],\n+          accumulate=False,\n+      )\n+      plgpu.barrier_wait(barrier_ref.at[barrier_index])\n+      scratch_smem[...] = acc_tmem[...].astype(dtype)\n+      plgpu.commit_smem()\n+      plgpu.copy_smem_to_gmem(scratch_smem, out_ref)\n+      plgpu.wait_smem_to_gmem(0)\n+\n+    scratch_shapes = [\n+        plgpu.TMEM(shape, jnp.float32, packed=False),\n+        plgpu.SMEM(shape, dtype, transforms=transforms),\n+        plgpu.Barrier(num_arrivals=1, num_barriers=2, for_tensor_core=True),\n+    ]\n+    f = self.pallas_call(\n+        kernel,\n+        in_specs=(\n+            plgpu.BlockSpec(transforms=transforms, memory_space=plgpu.SMEM),\n+            plgpu.BlockSpec(transforms=transforms, memory_space=plgpu.SMEM),\n+        ),\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.GMEM),\n+        out_shape=jax.ShapeDtypeStruct(shape, dtype),\n+        scratch_shapes=scratch_shapes,\n+    )\n+    x = jax.random.uniform(jax.random.key(0), shape=shape, dtype=dtype)\n+    y = jax.random.uniform(jax.random.key(1), shape=shape, dtype=dtype)\n+    result = f(x, y)\n+    expected = x @ y\n+    np.testing.assert_allclose(result, expected, rtol=1e-3)\n+\n \n class PallasCallSm100AWGTest(\n     PallasCallSm100ATest, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n\n```"
        ],
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add support for copy_gmem_to_smem in Warp semantics.\n\nPiperOrigin-RevId: 762475094",
        "output": [
            "```diff\nCommit: d4ab82637a9200752d30726d46281e97e5987427\nDate: 2025-05-23T17:17:09Z\nURL: https://github.com/jax-ml/jax/commit/d4ab82637a9200752d30726d46281e97e5987427\nFiles changed: 3\nAdditions: +56, Deletions: -12\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 53c890932e38..379a972be9b0 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -49,6 +49,7 @@\n import jax.numpy as jnp\n \n \n+WARP_SIZE = 32\n WARPGROUP_SIZE = 128\n \n \n@@ -464,7 +465,7 @@ def _copy_gmem_to_smem_lowering(\n     dst_transforms_treedef,\n     barrier_transforms_treedef,\n     collective_axes,\n-    warpgroup_sync: bool = True,\n+    for_warpgroup: bool = True,\n ):\n   flat_src_transforms, flat_dst_transforms, flat_barrier_transforms = (\n       util.split_list(\n@@ -505,15 +506,23 @@ def _copy_gmem_to_smem_lowering(\n   if ctx.module_ctx.lowering_semantics == mgpu.LoweringSemantics.Lane:\n     if bytes % WARPGROUP_SIZE:\n       raise NotImplementedError(\"Only aligned copies are supported\")\n-    # We arrive uniformly from each thread in the WG, so we need to divide the\n-    # number of bytes by the number of threads in the WG.\n-    # TODO: apaszke - Relax this. We can just select the WG leader and have it\n-    # arrive with the whole transfer size, while everyone else arrives with 0.\n-    # But we should continue using this scheme as it's likely to be faster.\n-    bytes //= WARPGROUP_SIZE\n-    if warpgroup_sync:\n+    if for_warpgroup:\n+      # We arrive uniformly from each thread in the WG, so we need to divide the\n+      # number of bytes by the number of threads in the WG.\n+      # TODO: apaszke - Relax this. We can just select the WG leader and have it\n+      # arrive with the whole transfer size, while everyone else arrives with 0.\n+      # But we should continue using this scheme as it's likely to be faster.\n+      bytes //= WARPGROUP_SIZE\n       mgpu.warpgroup_barrier()  # Make sure all reads have completed.\n-    barrier.arrive_expect_tx(bytes)\n+      barrier.arrive_expect_tx(bytes)\n+    else:\n+      # In Warp-level lowering, we arrive on each CUDA thread in a warp, but\n+      # the barrier still expects a full 128 arrivals so we arrive 4 times\n+      # on each CUDA thread instead.\n+      bytes //= WARP_SIZE\n+      barrier.arrive(arrival_count=3, can_complete=False)\n+      barrier.arrive_expect_tx(bytes)\n+\n     ctx.launch_ctx.async_copy(\n         src_ref=src,\n         dst_ref=dst,\n@@ -549,7 +558,7 @@ def _copy_gmem_to_smem_lowering(\n     copy_gmem_to_smem_p,\n     mgpu.LoweringSemantics.Lane,\n     primitive_semantics=gpu_core.PrimitiveSemantics.Warp,\n-)(functools.partial(_copy_gmem_to_smem_lowering, warpgroup_sync=False))\n+)(functools.partial(_copy_gmem_to_smem_lowering, for_warpgroup=False))\n \n \n def copy_gmem_to_smem(\n@@ -713,6 +722,8 @@ def _barrier_wait_pp_eqn(\n \n \n @lowering.register_lowering_rule(barrier_wait_p, mgpu.LoweringSemantics.Lane)\n+@lowering.register_lowering_rule(barrier_wait_p, mgpu.LoweringSemantics.Lane,\n+                                 gpu_core.PrimitiveSemantics.Warp)\n @lowering.register_lowering_rule(barrier_wait_p, mgpu.LoweringSemantics.Warpgroup)\n def _barrier_wait_lowering(\n     ctx: lowering.LoweringRuleContext,\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex 4aeb3358b97a..1915b0b45f11 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -816,9 +816,16 @@ def update_parities(self, parities: ir.Value) -> tuple[ir.Value, ir.Value]:\n     )\n     return parity, arith.xori(parities, bitmask)\n \n-  def arrive(self):\n+  def arrive(self, arrival_count: int = 1, can_complete: bool = True):\n     i64 = ir.IntegerType.get_signless(64)\n-    nvvm.mbarrier_arrive_shared(i64, self.get_ptr())\n+    if can_complete:\n+      if arrival_count > 1:\n+        count = c(arrival_count - 1, ir.IntegerType.get_signless(32))\n+        nvvm.mbarrier_arrive_nocomplete_shared(i64, self.get_ptr(), count)\n+      nvvm.mbarrier_arrive_shared(i64, self.get_ptr())\n+    else:\n+      count = c(arrival_count, ir.IntegerType.get_signless(32))\n+      nvvm.mbarrier_arrive_nocomplete_shared(i64, self.get_ptr(), count)\n \n   def arrive_expect_tx(\n       self, bytes: int | ir.Value, predicate: ir.Value | None = None\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex b3c2f11ee43f..029445143a0f 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1878,6 +1878,32 @@ def _():\n         },\n     )\n \n+  def test_copy_gmem_to_smem_from_different_warps(self):\n+    # In this test, we issue a copy from from warp 0 and await it in warp 1.\n+    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n+    @functools.partial(plgpu.kernel,\n+                       out_shape=jax.ShapeDtypeStruct((32, 32), jnp.float32))\n+    def kernel(x_ref, y_ref):\n+      def scope(smem_ref, tma_barrier):\n+        @pl.core_map(warp_mesh)\n+        def _():\n+          warp_id = lax.axis_index(\"warp\")\n+          @pl.when(warp_id == 0)\n+          def _():\n+            plgpu.copy_gmem_to_smem(x_ref.at[32:64], smem_ref, tma_barrier)\n+\n+          @pl.when(warp_id == 1)\n+          def _():\n+            plgpu.barrier_wait(tma_barrier)\n+            plgpu.copy_smem_to_gmem(smem_ref, y_ref)\n+        plgpu.wait_smem_to_gmem(0)\n+      pl.run_scoped(scope,\n+                    smem_ref=plgpu.SMEM((32, 32), jnp.float32),\n+                    tma_barrier=plgpu.Barrier(num_arrivals=1))\n+    x = jax.random.uniform(jax.random.key(42), (64, 32), jnp.float32)\n+    result = kernel(x)\n+    np.testing.assert_array_equal(result, x[32:64])\n+\n \n class PallasCallWGTest(\n     PallasCallTest, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n\n```"
        ],
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[tree_util] raise more informative error when pytree equality check fails",
        "output": [
            "```diff\nCommit: aa63a159e5a2ef6145f8b87538e16c6bc42970b8\nDate: 2025-05-23T17:11:39Z\nURL: https://github.com/jax-ml/jax/commit/aa63a159e5a2ef6145f8b87538e16c6bc42970b8\nFiles changed: 3\nAdditions: +32, Deletions: -3\ndiff --git a/jaxlib/pytree.cc b/jaxlib/pytree.cc\nindex 2700ac9e6c9a..bd845c47ec1e 100644\n--- a/jaxlib/pytree.cc\n+++ b/jaxlib/pytree.cc\n@@ -281,8 +281,16 @@ bool PyTreeDef::operator==(const PyTreeDef& other) const {\n         a.custom != b.custom) {\n       return false;\n     }\n-    if (a.node_data && a.node_data.not_equal(b.node_data)) {\n-      return false;\n+    try {\n+      if (a.node_data && a.node_data.not_equal(b.node_data)) {\n+        return false;\n+      }\n+    } catch (nb::python_error& e) {\n+      nb::raise_from(e, PyExc_ValueError,\n+                     \"Exception raised while checking equality of metadata \"\n+                     \"fields of pytree. Make sure that metadata fields are \"\n+                     \"hashable and have simple equality semantics. (Note: \"\n+                     \"arrays cannot be passed as metadata fields!)\");\n     }\n     if (!IsSortedPyDictKeysEqual(a.sorted_dict_keys, b.sorted_dict_keys)) {\n       return false;\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 24861bad81de..b9497b71dcb1 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 345\n+_version = 346\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\ndiff --git a/tests/tree_util_test.py b/tests/tree_util_test.py\nindex 8d4cd5854e7d..0d92156b2530 100644\n--- a/tests/tree_util_test.py\n+++ b/tests/tree_util_test.py\n@@ -1050,6 +1050,27 @@ def testPickle(self):\n       unpickled = pickle.loads(pickle.dumps(key))\n       self.assertEqual(key, unpickled)\n \n+  def testEqualityErrorWithArrayAsStaticArg(self):\n+    # Regression test for https://github.com/jax-ml/jax/issues/28659\n+    @tree_util.register_dataclass\n+    @dataclasses.dataclass\n+    class Tree:\n+      x : jnp.ndarray = dataclasses.field(metadata={'static': True})\n+\n+    f = jax.jit(lambda x: x)\n+\n+    if jax._src.lib.jaxlib_extension_version < 346:\n+      msg = \"The truth value of an array with more than one element is ambiguous.\"\n+    else:\n+      msg = \"Exception raised while checking equality of metadata fields of pytree.\"\n+\n+    # First call succeeds, because there is no equality check.\n+    f(Tree(jnp.arange(4)))\n+\n+    # Second fall fails, because arrays are marked static and compared for equality.\n+    with self.assertRaisesRegex(ValueError, msg):\n+      f(Tree(jnp.arange(4)))\n+\n \n class StaticTest(parameterized.TestCase):\n \n\n```"
        ],
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Rename backend.compile to backend.compile_and_load.\n\nPart of a larger refactor. Today, `compile` returns a loaded executable i.e., fuses the compile and load functions. Eventually, `compile` should return an unloaded executable and `load` should return a loaded exectuable; the default jit path will still return a loaded executable.\n\nPiperOrigin-RevId: 762457830",
        "output": [
            "```diff\nCommit: 9928409798fbdf4b9a0b811e78a7bb1698caeda3\nDate: 2025-05-23T16:37:44Z\nURL: https://github.com/jax-ml/jax/commit/9928409798fbdf4b9a0b811e78a7bb1698caeda3\nFiles changed: 1\nAdditions: +48, Deletions: -11\ndiff --git a/jax/_src/compiler.py b/jax/_src/compiler.py\nindex 343f747efbd7..4f805034e99c 100644\n--- a/jax/_src/compiler.py\n+++ b/jax/_src/compiler.py\n@@ -34,7 +34,9 @@\n from jax._src import profiler\n from jax._src import traceback_util\n from jax._src.interpreters import mlir\n+from jax._src.lib import jaxlib_extension_version\n from jax._src.lib import xla_client as xc\n+from jax._src.lib import _jax\n from jax._src.lib.mlir import ir\n import numpy as np\n \n@@ -291,6 +293,19 @@ def backend_compile(\n     executable_devices: xc.DeviceList,\n     options: xc.CompileOptions,\n     host_callbacks: Sequence[Any],\n+) -> xc.LoadedExecutable:\n+  return backend_compile_and_load(\n+      backend, module, executable_devices, options, host_callbacks\n+  )\n+\n+\n+@profiler.annotate_function\n+def backend_compile_and_load(\n+    backend: xc.Client,\n+    module: ir.Module,\n+    executable_devices: xc.DeviceList,\n+    options: xc.CompileOptions,\n+    host_callbacks: Sequence[Any],\n ) -> xc.LoadedExecutable:\n   sym_name = module.operation.attributes['sym_name']\n   module_name = ir.StringAttr(sym_name).value\n@@ -315,18 +330,40 @@ def backend_compile(\n   try:\n     # we use a separate function call to ensure that XLA compilation appears\n     # separately in Python profiling results\n-    if host_callbacks:\n+    # TODO(dsuo): Simplify this logic once backend_compile actually returns an\n+    # unloaded executable.\n+    if jaxlib_extension_version < 345 or (\n+        jaxlib_extension_version >= 345\n+        and isinstance(backend, _jax.CompileOnlyPyClient)\n+    ):\n+      if host_callbacks:\n+        return backend.compile(\n+            built_c,\n+            executable_devices=executable_devices,  # type: ignore\n+            compile_options=options,\n+            host_callbacks=host_callbacks,\n+        )\n+      # Some backends don't have `host_callbacks` option yet\n+      # TODO(sharadmv): remove this fallback when all backends allow `compile`\n+      # to take in `host_callbacks`\n       return backend.compile(\n+          built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n+    else:\n+      if host_callbacks:\n+        return backend.compile_and_load(\n+            built_c,\n+            executable_devices=executable_devices,\n+            compile_options=options,\n+            host_callbacks=host_callbacks,\n+        )\n+      # Some backends don't have `host_callbacks` option yet\n+      # TODO(sharadmv): remove this fallback when all backends allow `compile`\n+      # to take in `host_callbacks`\n+      return backend.compile_and_load(\n           built_c,\n-          executable_devices=executable_devices,  # type: ignore\n+          executable_devices=executable_devices,\n           compile_options=options,\n-          host_callbacks=host_callbacks,\n       )\n-    # Some backends don't have `host_callbacks` option yet\n-    # TODO(sharadmv): remove this fallback when all backends allow `compile`\n-    # to take in `host_callbacks`\n-    return backend.compile(\n-        built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n   except xc.XlaRuntimeError as e:\n     for error_handler in _XLA_RUNTIME_ERROR_HANDLERS:\n       handler_result = error_handler(e)\n@@ -391,7 +428,7 @@ def compile_or_get_cached(\n   )\n \n   if cache_key is None:\n-    return backend_compile(\n+    return backend_compile_and_load(\n         backend, computation, executable_devices, compile_options,\n         host_callbacks)\n \n@@ -419,7 +456,7 @@ def compile_or_get_cached(\n       config.share_binary_between_hosts.value\n       and is_multi_process\n       and distributed.global_state.client is not None\n-      # Host callbacks are currently baked into the HLO module so we cant share\n+      # Host callbacks are currently baked into the HLO module so we can't share\n       # them.\n       and len(host_callbacks) == 0\n   ):\n@@ -705,7 +742,7 @@ def _compile_and_write_cache(\n     cache_key: str,\n ) -> xc.LoadedExecutable:\n   start_time = time.monotonic()\n-  executable = backend_compile(\n+  executable = backend_compile_and_load(\n       backend, computation, executable_devices, compile_options, host_callbacks\n   )\n   compile_time = time.monotonic() - start_time\n\n```"
        ],
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move jax/_src/interpreters/batching.py into its own BUILD rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nUnfortunately this is not a clean build refactor, because batching depends on jax.lax, which in turn depends on batching. However, the problematic functions are only called within contexts where jax.lax is available for import.\n\nWe have a few options here:\n\n1. Continue to bundle the batching.py source with the main build.\n2. Build separately, but do the local import workaround in this CL (a pattern we use elsewhere).\n3. Build this separately, but move some batching definitions into jax.lax for a more strict dependency graph. Or pass the `lax` namespace explicitly to the function at the call site.\n\nI opted for (2) here because I judged the benefits of a refactored build to be worth the cost of localized impure dependencies, and the kind of refactoring in (3) would affect some downstream users.\n\nPiperOrigin-RevId: 762447323",
        "output": [
            "```diff\nCommit: c2c55aef522abcff80fbcf9b11dd8faaa28924be\nDate: 2025-05-23T16:09:27Z\nURL: https://github.com/jax-ml/jax/commit/c2c55aef522abcff80fbcf9b11dd8faaa28924be\nFiles changed: 2\nAdditions: +38, Deletions: -10\ndiff --git a/jax/BUILD b/jax/BUILD\nindex e7f1fad3121d..7c8847e2e94b 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -315,7 +315,6 @@ py_library_providing_imports_info(\n         \"_src/ffi.py\",\n         \"_src/flatten_util.py\",\n         \"_src/interpreters/__init__.py\",\n-        \"_src/interpreters/batching.py\",\n         \"_src/interpreters/pxla.py\",\n         \"_src/pjit.py\",\n         \"_src/prng.py\",\n@@ -383,6 +382,7 @@ py_library_providing_imports_info(\n         \":ad_util\",\n         \":api_util\",\n         \":basearray\",\n+        \":batching\",\n         \":cloud_tpu_init\",\n         \":compilation_cache_internal\",\n         \":compiler\",\n@@ -707,6 +707,24 @@ pytype_strict_library(\n     ],\n )\n \n+pytype_strict_library(\n+    name = \"batching\",\n+    srcs = [\"_src/interpreters/batching.py\"],\n+    deps = [\n+        \":ad_util\",\n+        \":config\",\n+        \":core\",\n+        \":mesh\",\n+        \":partial_eval\",\n+        \":partition_spec\",\n+        \":sharding_impls\",\n+        \":source_info_util\",\n+        \":tree_util\",\n+        \":typing\",\n+        \":util\",\n+    ] + py_deps(\"numpy\"),\n+)\n+\n pytype_strict_library(\n     name = \"mlir\",\n     srcs = [\"_src/interpreters/mlir.py\"],\ndiff --git a/jax/_src/interpreters/batching.py b/jax/_src/interpreters/batching.py\nindex 0fbe54a30672..55769aa307fc 100644\n--- a/jax/_src/interpreters/batching.py\n+++ b/jax/_src/interpreters/batching.py\n@@ -21,7 +21,6 @@\n \n import numpy as np\n \n-import jax\n from jax._src import config\n from jax._src import core\n from jax._src import source_info_util\n@@ -301,11 +300,14 @@ def _cont(axis_size, elt, axis):\n from_elt_handlers: dict[type, FromEltHandler] = {}\n \n def make_iota(axis_size: AxisSize) -> Array:\n+  # Callers of this utility, via batch() or vtile(), must be in a context\n+  # where lax is importable.\n+  from jax import lax  # pytype: disable=import-error\n   handler = make_iota_handlers.get(type(axis_size))\n   if handler:\n     return handler(axis_size)\n   else:\n-    return jax.lax.iota('int32', int(axis_size))\n+    return lax.iota('int32', int(axis_size))\n make_iota_handlers: dict[type, MakeIotaHandler] = {}\n \n def register_vmappable(data_type: type, spec_type: type, axis_size_type: type,\n@@ -1019,10 +1021,13 @@ def broadcast_batcher(prim, args, dims, **params):\n     return (out, (0,) * len(out)) if prim.multiple_results else (out, 0)\n \n def _handle_scalar_broadcasting(nd, x, d):\n+  # Callers of this utility, via broadcast_batcher() or defbroadcasting(),\n+  # must be in a context where lax is importable.\n+  from jax import lax  # pytype: disable=import-error\n   if d is not_mapped or nd == np.ndim(x):\n     return x\n   else:\n-    return jax.lax.expand_dims(x, tuple(range(np.ndim(x), nd)))\n+    return lax.expand_dims(x, tuple(range(np.ndim(x), nd)))\n \n def defreducer(prim, ident):\n   primitive_batchers[prim] = partial(reducer_batcher, prim, ident)\n@@ -1078,17 +1083,20 @@ def mask_ragged_axes(operand: Array, ident, axis_spec: RaggedAxis) -> Array:\n \n def _mask_one_ragged_axis(\n     operand: Array, ident, axis_spec: RaggedAxis) -> Array:\n+  # Callers of this utility, via reducer_batcher() or defreducer(),\n+  # must be in a context where lax is importable.\n+  from jax import lax  # pytype: disable=import-error\n   assert len(axis_spec.ragged_axes) == 1, \"Mask just one ragged axis at a time\"\n   ragged_axis, segment_lengths = axis_spec.ragged_axes[0]\n   value = ident(operand.dtype)\n-  positions = jax.lax.broadcasted_iota('int32', operand.shape, ragged_axis)\n+  positions = lax.broadcasted_iota('int32', operand.shape, ragged_axis)\n   # TODO(mattjj, axch) can't get ._data, need to convert it\n-  # lengths = jax.lax.convert_element_type(segment_lengths._data, 'int32')\n-  lengths = jax.lax.convert_element_type(segment_lengths, 'int32')\n-  limits = jax.lax.broadcast_in_dim(\n+  # lengths = lax.convert_element_type(segment_lengths._data, 'int32')\n+  lengths = lax.convert_element_type(segment_lengths, 'int32')\n+  limits = lax.broadcast_in_dim(\n       lengths, operand.shape, [axis_spec.stacked_axis])\n   mask = positions < limits\n-  return jax.lax.select(mask, operand, jax.lax.broadcast(value, operand.shape))\n+  return lax.select(mask, operand, lax.broadcast(value, operand.shape))\n \n def move_stacked_axis(operand, bdim, dst):\n   dst = canonicalize_axis(dst, operand.ndim)\n@@ -1103,6 +1111,8 @@ def move_stacked_axis(operand, bdim, dst):\n ### general utilities for manipulating axes on jaxpr types (not vmappables)\n \n def broadcast(x, sz, axis, mesh_axis=None):\n+  # Callers of this utility must be in a context where lax is importable.\n+  from jax import lax  # pytype: disable=import-error\n   shape = list(np.shape(x))\n   shape.insert(axis, sz)\n   broadcast_dims = tuple(np.delete(np.arange(len(shape)), axis))\n@@ -1114,7 +1124,7 @@ def broadcast(x, sz, axis, mesh_axis=None):\n   # TODO(dougalm, yashkatariya): Delete this context manager once we figure\n   # out how to ensure jaxpr arguments always have the context mesh.\n   with mesh_lib.use_abstract_mesh(sharding.mesh):\n-    x = jax.lax.broadcast_in_dim(x, shape, broadcast_dims, out_sharding=sharding)\n+    x = lax.broadcast_in_dim(x, shape, broadcast_dims, out_sharding=sharding)\n     if config._check_vma.value:\n       # TODO(yashkatariya,parkers): don't do this, fix during fixit week 2026\n       spmd_names = core.get_axis_env().spmd_axis_names\n\n```"
        ],
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[CI] Pin the ML Connect action to a specific sha\n\nPiperOrigin-RevId: 762441171",
        "output": [
            "```diff\nCommit: 9d6553815f12d6903d4ccfd810519e31e2a97810\nDate: 2025-05-23T15:51:15Z\nURL: https://github.com/jax-ml/jax/commit/9d6553815f12d6903d4ccfd810519e31e2a97810\nFiles changed: 11\nAdditions: +12, Deletions: -12\ndiff --git a/.github/workflows/bazel_cpu_py_import_rbe.yml b/.github/workflows/bazel_cpu_py_import_rbe.yml\nindex 14d6b95b4347..c98bcee980e8 100644\n--- a/.github/workflows/bazel_cpu_py_import_rbe.yml\n+++ b/.github/workflows/bazel_cpu_py_import_rbe.yml\n@@ -55,7 +55,7 @@ jobs:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Bazel CPU tests with py_import (RBE)\ndiff --git a/.github/workflows/bazel_cpu_rbe.yml b/.github/workflows/bazel_cpu_rbe.yml\nindex ef5084960b30..a8b40c260260 100644\n--- a/.github/workflows/bazel_cpu_rbe.yml\n+++ b/.github/workflows/bazel_cpu_rbe.yml\n@@ -54,7 +54,7 @@ jobs:\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       # Since we do not have a Linux Arm64 RBE pool, we do not run the tests on Arm64. Instead, we\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 677d8d869a22..3e68034dfbf4 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -106,7 +106,7 @@ jobs:\n           exit 1\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Bazel CUDA tests (Non-RBE)\ndiff --git a/.github/workflows/bazel_cuda_rbe.yml b/.github/workflows/bazel_cuda_rbe.yml\nindex 5a2c94c4db47..83f651c0ef95 100644\n--- a/.github/workflows/bazel_cuda_rbe.yml\n+++ b/.github/workflows/bazel_cuda_rbe.yml\n@@ -50,7 +50,7 @@ jobs:\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Bazel CUDA Tests with RBE\ndiff --git a/.github/workflows/bazel_optional_h100_b200.yml b/.github/workflows/bazel_optional_h100_b200.yml\nindex 7381ce6d80bf..ec907280938e 100644\n--- a/.github/workflows/bazel_optional_h100_b200.yml\n+++ b/.github/workflows/bazel_optional_h100_b200.yml\n@@ -33,7 +33,7 @@ jobs:\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Bazel single B200 CUDA Tests\n@@ -75,7 +75,7 @@ jobs:\n     steps:\n       - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683  # v4.2.2\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Bazel multiple H100 CUDA Tests\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 1b534ee3b6fc..d5fc35a99cd5 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -127,7 +127,7 @@ jobs:\n         run: echo \"JAXCI_WRITE_TO_BAZEL_REMOTE_CACHE=1\" >> $GITHUB_ENV\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Build ${{ inputs.artifact }}\ndiff --git a/.github/workflows/numpy_nightly.yml b/.github/workflows/numpy_nightly.yml\nindex 51876a7eb71d..17357e9f1dd8 100644\n--- a/.github/workflows/numpy_nightly.yml\n+++ b/.github/workflows/numpy_nightly.yml\n@@ -54,7 +54,7 @@ jobs:\n           path: ml_dtypes\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Install numpy & scipy development versions\ndiff --git a/.github/workflows/oldest_supported_numpy.yml b/.github/workflows/oldest_supported_numpy.yml\nindex 06e7cf6230df..a63cb0b1c614 100644\n--- a/.github/workflows/oldest_supported_numpy.yml\n+++ b/.github/workflows/oldest_supported_numpy.yml\n@@ -51,7 +51,7 @@ jobs:\n           $JAXCI_PYTHON -m uv pip install -e .[minimum-jaxlib]\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Pytest CPU tests\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex fc4633110667..3af06fe8037e 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -140,7 +140,7 @@ jobs:\n           fi\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Pytest CPU tests\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 2f22901e661a..78f32cda672d 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -138,7 +138,7 @@ jobs:\n         run: $JAXCI_PYTHON -m uv pip install -r build/test-requirements.txt\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Pytest CUDA tests\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex ae0250884831..22cd64977dc5 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -152,7 +152,7 @@ jobs:\n           fi\n       # Halt for testing\n       - name: Wait For Connection\n-        uses: google-ml-infra/actions/ci_connection@main\n+        uses: google-ml-infra/actions/ci_connection@7f5ca0c263a81ed09ea276524c1b9192f1304e3c\n         with:\n           halt-dispatch-input: ${{ inputs.halt-for-connection }}\n       - name: Run Pytest TPU tests\n\n```"
        ],
        "from_id": [
            "MichaelHudgins",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[jaxlib] Add CompileOnlyPyClient to xla_client.\n\nWe have users of CompileOnlyPyClient that use `backend.compile` as we eventually intend it (i.e., return `ExecutableRef`, possibly `PyExecutable` eventually, instead of `PyLoadedExectuable`).\n\nPiperOrigin-RevId: 762440439",
        "output": [
            "```diff\nCommit: 7d13c56570072565ce244bf6ff77c2a55f4d5e66\nDate: 2025-05-23T15:49:07Z\nURL: https://github.com/jax-ml/jax/commit/7d13c56570072565ce244bf6ff77c2a55f4d5e66\nFiles changed: 2\nAdditions: +12, Deletions: -1\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex ed0089a3dd88..67dc9ffc6001 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -553,6 +553,17 @@ class Client:\n   ) -> PjRtLayout: ...\n   def __getattr__(self, name: str) -> Any: ...\n \n+\n+class CompileOnlyPyClient(Client):\n+  def compile(\n+      self,\n+      computation: str | bytes,\n+      executable_devices: DeviceList | Sequence[Device],\n+      compile_options: CompileOptions = ...,\n+      host_callbacks: Sequence[Any] = ...,\n+  ) -> LoadedExecutable: ...\n+\n+\n class CpuCollectives: ...\n \n def make_gloo_tcp_collectives(\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex ac816e72bebe..24861bad81de 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 344\n+_version = 345\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\n\n```"
        ],
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] Slightly revamped how checkify is exposed in Pallas\n\n* We now re-export a restricted version of `debug_check` under `pl`.\n  Unlike the original, the `pl` version only allows a static message, i.e.\n  string interpolation is not supported.\n* Only debug checks are supported, which means that by default no checking\n  is done -- `debug_check` is lowered to a noop.\n* The context manager enabling debug checks is called `enable_debug_checks`.\n  I would very much like to drop the `enable_` prefix, but without it the\n  context manager reads too similar to `debug_check`.\n\nPiperOrigin-RevId: 762433258",
        "output": [
            "```diff\nCommit: e989e23d6bb5895215864371e25724910a05fb0b\nDate: 2025-05-23T15:28:14Z\nURL: https://github.com/jax-ml/jax/commit/e989e23d6bb5895215864371e25724910a05fb0b\nFiles changed: 10\nAdditions: +72, Deletions: -61\ndiff --git a/docs/pallas/CHANGELOG.md b/docs/pallas/CHANGELOG.md\nindex 476cc54673a1..2d8a83c897f1 100644\n--- a/docs/pallas/CHANGELOG.md\n+++ b/docs/pallas/CHANGELOG.md\n@@ -26,6 +26,11 @@ Remember to align the itemized text with the first line of an item within a list\n     `block_shape` for each entry that needs unblocked indexing.\n   * {func}`jax.experimental.pallas.pallas_call` now requires `compiler_params`\n     to be a backend-specific dataclass instead of a param to value mapping.\n+  * {func}`jax.experimental.pallas.debug_check` is now supported both on\n+    TPU and Mosaic GPU. Previously, this functionality was only supported\n+    on TPU and required using the APIs from {mod}`jax.experimental.checkify`.\n+    Note that debug checks are not executed unless\n+    {data}`jax.experimental.pallas.enable_debug_checks` is set.\n \n ## Released with jax 0.5.0\n \ndiff --git a/jax/_src/pallas/core.py b/jax/_src/pallas/core.py\nindex 13c634eb395f..7950f90bc377 100644\n--- a/jax/_src/pallas/core.py\n+++ b/jax/_src/pallas/core.py\n@@ -44,22 +44,6 @@\n from jax._src.state.types import TransformedRef\n import jax.numpy as jnp\n \n-# TODO(slebedev): Rename to --jax_pallas_debug_assertions.\n-_ENABLE_RUNTIME_ASSERT = config.bool_state(\n-    \"jax_pallas_enable_runtime_assert\",\n-    default=False,\n-    help=(\n-        \"If set, enables runtime assertions in the kernel via checkify.check.\"\n-        \" Otherwise, runtime asserts will be ignored unless functionalized\"\n-        \" using checkify.checkify.\"\n-    ),\n-)\n-\n-\n-def runtime_assert_enabled() -> bool:\n-  \"\"\"Returns whether runtime asserts are enabled.\"\"\"\n-  return _ENABLE_RUNTIME_ASSERT.value\n-\n \n class DynamicGridDim:\n   def __repr__(self):\ndiff --git a/jax/_src/pallas/helpers.py b/jax/_src/pallas/helpers.py\nindex 6b274c0b6cce..5c77d0a04f09 100644\n--- a/jax/_src/pallas/helpers.py\n+++ b/jax/_src/pallas/helpers.py\n@@ -14,8 +14,10 @@\n \"\"\"Pallas helper functions.\"\"\"\n \n import jax\n-from jax._src.pallas import pallas_call\n+from jax._src import checkify\n+from jax._src import config\n from jax._src.pallas import core as pl_core\n+from jax._src.pallas import pallas_call\n \n \n @jax.named_call\n@@ -65,3 +67,29 @@ def _wrapped(f):\n     else:\n       jax.lax.cond(condition, f, lambda: None)\n   return _wrapped\n+\n+\n+_ENABLE_DEBUG_CHECKS = config.bool_state(\n+    \"jax_pallas_enable_debug_checks\",\n+    default=False,\n+    help=(\n+        \"If set, ``pl.debug_check`` calls are checked at runtime. Otherwise,\"\n+        \" they are a noop.\"\n+    ),\n+)\n+\n+\n+enable_debug_checks = _ENABLE_DEBUG_CHECKS\n+\n+\n+def debug_checks_enabled() -> bool:\n+  \"\"\"Returns runtime checks are enabled.\"\"\"\n+  return _ENABLE_DEBUG_CHECKS.value\n+\n+\n+def debug_check(condition, message):\n+  \"\"\"Check the condition if\n+  :func:`~jax.experimental.pallas.enable_debug_checks` is set, otherwise\n+  do nothing.\n+  \"\"\"\n+  return checkify.debug_check(condition, message)\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex d1222c16ac96..4e8827401e0c 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -61,6 +61,7 @@\n from jax._src.pallas import pallas_call\n from jax._src.pallas import primitives\n from jax._src.pallas import utils as pallas_utils\n+from jax._src.pallas import helpers as pallas_helpers\n from jax._src.pallas.mosaic import core as tpu_core\n from jax._src.pallas.mosaic import error_handling\n from jax._src.pallas.mosaic import primitives as tpu_primitives\n@@ -3766,17 +3767,18 @@ def _join_key_lowering_rule(ctx: LoweringRuleContext, *scalars, impl):\n \n \n @register_lowering_rule(checkify.check_p)\n-def _checkify_lowering_rule(\n-    ctx: LoweringRuleContext, *err_args, err_tree, debug):\n-  if not pallas_core.runtime_assert_enabled():\n-    if debug:\n-      return []\n-    else:\n-      raise LoweringException(\n-          \"Non-debug check must be functionalized. Enable runtime asserts via\"\n-          \" ``pl.enable_runtime_assert`` or --jax_pallas_enable_runtime_assert\"\n-          \" or, alternatively, functionalize with ``checkify.check``.\"\n-      )\n+def _check_lowering_rule(\n+    ctx: LoweringRuleContext, *err_args, err_tree, debug\n+):\n+  del ctx  # Unused.\n+\n+  if not debug:\n+    raise NotImplementedError(\n+        \"Non-debug checks are not supported by the Mosaic backend.\"\n+        \" Functionalize them via `jax.experimental.checkify`.\"\n+    )\n+  if not pallas_helpers.debug_checks_enabled():\n+    return []\n \n   if cf is None:\n     # TODO(slebedev): Remove once the minimal jaxlib version is 0.6.1.\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 00716bb1c675..9e396167f610 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -53,6 +53,7 @@\n from jax._src.pallas import pallas_call\n from jax._src.pallas import primitives\n from jax._src.pallas import utils as pallas_utils\n+from jax._src.pallas import helpers as pallas_helpers\n from jax._src.pallas.mosaic_gpu import core as gpu_core\n from jax._src.state import discharge\n from jax._src.state import indexing\n@@ -3097,18 +3098,16 @@ def _semaphore_wait_lowering_rule(ctx: LoweringRuleContext, *args, args_tree):\n \n \n @register_lowering_rule(checkify.check_p, mgpu.LoweringSemantics.Lane)\n-def _checkify_lowering_rule(\n-    ctx: LoweringRuleContext, *err_args, err_tree, debug\n-):\n-  if not pallas_core.runtime_assert_enabled():\n-    if debug:\n-      return []\n-    else:\n-      raise LoweringError(\n-          \"Non-debug check must be functionalized. Enable runtime asserts via\"\n-          \" ``pl.enable_runtime_assert`` or --jax_pallas_enable_runtime_assert\"\n-          \" or, alternatively, functionalize with ``checkify.check``.\"\n-      )\n+def _check_lowering_rule(ctx: LoweringRuleContext, *err_args, err_tree, debug):\n+  del ctx  # Unused.\n+\n+  if not debug:\n+    raise NotImplementedError(\n+        \"Non-debug checks are not supported by the Mosaic GPU backend.\"\n+        \" Functionalize them via `jax.experimental.checkify`.\"\n+    )\n+  if not pallas_helpers.debug_checks_enabled():\n+    return []\n \n   if cf_dialect is None:\n     # TODO(slebedev): Remove once the minimal jaxlib version is 0.6.1.\ndiff --git a/jax/experimental/pallas/__init__.py b/jax/experimental/pallas/__init__.py\nindex 406d6e965322..caf77a3c4fce 100644\n--- a/jax/experimental/pallas/__init__.py\n+++ b/jax/experimental/pallas/__init__.py\n@@ -18,7 +18,6 @@\n https://docs.jax.dev/en/latest/pallas.html.\n \"\"\"\n \n-from jax._src.pallas.core import _ENABLE_RUNTIME_ASSERT as enable_runtime_assert  # noqa: F401\n from jax._src.pallas.core import BlockDim as BlockDim\n from jax._src.pallas.core import Blocked as Blocked\n from jax._src.pallas.core import BlockSpec as BlockSpec\n@@ -33,7 +32,6 @@\n from jax._src.pallas.core import MemoryRef as MemoryRef\n from jax._src.pallas.core import MemorySpace as MemorySpace\n from jax._src.pallas.core import no_block_spec as no_block_spec\n-from jax._src.pallas.core import runtime_assert_enabled as runtime_assert_enabled\n from jax._src.pallas.core import semaphore as semaphore\n from jax._src.pallas.core import Squeezed as Squeezed\n from jax._src.pallas.core import squeezed as squeezed\n@@ -41,6 +39,9 @@\n from jax._src.pallas.helpers import empty as empty\n from jax._src.pallas.helpers import empty_like as empty_like\n from jax._src.pallas.helpers import when as when\n+from jax._src.pallas.helpers import debug_check as debug_check\n+from jax._src.pallas.helpers import debug_checks_enabled as debug_checks_enabled\n+from jax._src.pallas.helpers import enable_debug_checks as enable_debug_checks\n from jax._src.pallas.pallas_call import pallas_call as pallas_call\n from jax._src.pallas.pallas_call import pallas_call_p as pallas_call_p\n from jax._src.pallas.primitives import atomic_add as atomic_add\ndiff --git a/jax/experimental/pallas/g3doc/debugging.md b/jax/experimental/pallas/g3doc/debugging.md\nindex 6dfa95eb16fa..791705d00d30 100644\n--- a/jax/experimental/pallas/g3doc/debugging.md\n+++ b/jax/experimental/pallas/g3doc/debugging.md\n@@ -3,7 +3,7 @@\n <!--internal:0-->\n \n <!--*\n-freshness: { owner: 'justinfu' reviewed: '2024-11-19' }\n+freshness: { owner: 'slebedev' reviewed: '2025-05-22' }\n *-->\n \n [TOC]\n@@ -45,16 +45,14 @@ as a Python error after the kernel has successfully executed.\n \n #### Hard assertion\n \n-Hard assertions can be inserted with `checkify.check`\n-and running your program with the `--jax_pallas_enable_runtime_assert` flag.\n+Hard assertions can be inserted with `pl.debug_check`\n+and running your program with the `--jax_pallas_enable_debug_checks` flag.\n \n Your code will look like the following:\n \n ```python\n-from jax.experimental import checkify\n-\n def kernel(...):\n-  checkify.check(x > y, \"Check x > y failed\")  # Will halt if x <= y\n+  pl.debug_check(x > y, \"Check x > y failed\")  # Will halt if x <= y\n ```\n \n This will print a relatively lengthy dump which resembles the following:\n@@ -76,11 +74,10 @@ Functionalized asserts can be performed by checkify-ing the `pl.pallas_call` op\n from jax.experimental import checkify\n \n def kernel(...):\n-  checkify.check(x > y, \"Check x > y failed\")  # Will throw an error if x <= y\n+  pl.debug_check(x > y, \"Check x > y failed\")  # Will throw an error if x <= y\n \n kernel = pl.pallas_call(...)\n-checkified_kernel = checkify.checkify(kernel,\n-  errors=checkify.all_checks)\n+checkified_kernel = checkify.checkify(kernel, errors=checkify.all_checks)\n error, result = checkified_kernel(x)\n error.throw()\n ```\n@@ -203,5 +200,3 @@ In most cases the error message should hint at what is wrong.\n For specific errors:\n \n * `Mixed dtype operands in cmp` when using `jnp.mod`: Use lax.rem instead of jnp.mod\n-\n-\ndiff --git a/jax/experimental/pallas/tpu.py b/jax/experimental/pallas/tpu.py\nindex c8e2ba131a9b..401b2fe66c45 100644\n--- a/jax/experimental/pallas/tpu.py\n+++ b/jax/experimental/pallas/tpu.py\n@@ -51,8 +51,6 @@\n # Those primitives got moved to Pallas core. Keeping the updated imports\n # here for backward compatibility.\n from jax._src.pallas.core import semaphore as semaphore\n-from jax._src.pallas.core import runtime_assert_enabled as runtime_assert_enabled\n-from jax._src.pallas.core import _ENABLE_RUNTIME_ASSERT as enable_runtime_assert  # noqa: F401\n from jax._src.pallas.primitives import DeviceIdType as DeviceIdType\n from jax._src.pallas.primitives import semaphore_read as semaphore_read\n from jax._src.pallas.primitives import semaphore_signal as semaphore_signal\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 4ef2fa8096ee..b3c2f11ee43f 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1031,14 +1031,14 @@ def kernel(x_ref, o_ref, scratch_ref, barrier_ref):\n   def test_check(self):\n     self.skip_if_wg_semantics()\n \n-    self.enter_context(pallas_core._ENABLE_RUNTIME_ASSERT(True))\n+    self.enter_context(pl.enable_debug_checks(True))\n \n     @functools.partial(\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct([256], jnp.int32),\n     )\n     def kernel(x_ref, o_ref):\n-      checkify.check(_sum_same_dtype(x_ref[...]) > 0, \"x.sum() is negative\")\n+      pl.debug_check(_sum_same_dtype(x_ref[...]) > 0, \"x.sum() is negative\")\n       o_ref[...] = x_ref[...]\n \n     x = jnp.arange(256, dtype=jnp.int32)\ndiff --git a/tests/pallas/pallas_test.py b/tests/pallas/pallas_test.py\nindex 1114153b16c2..03399e12b609 100644\n--- a/tests/pallas/pallas_test.py\n+++ b/tests/pallas/pallas_test.py\n@@ -2276,7 +2276,7 @@ def kernel(x_ref, y_ref):\n       checkify.check(False, \"second check failed\")\n     input_ = jnp.arange(4, dtype=jnp.int32)\n     out_shape = jax.ShapeDtypeStruct(input_.shape, input_.dtype)\n-    with pltpu.enable_runtime_assert(True):\n+    with pl.enable_debug_checks(True):\n       pallas_call = pl.pallas_call(kernel, out_shape=out_shape)\n       pallas_call(input_)  # This should log \"second check failed\"\n \n@@ -2286,11 +2286,10 @@ def test_runtime_assert_is_noop_when_not_enabled(self):\n       self.skipTest(\"Runtime check only implemented on TPU.\")\n     def kernel(x_ref, y_ref):\n       y_ref[...] = x_ref[...]\n-      checkify.check(False, \"failed check\",\n-                     debug=True)  # This check always fails.\n+      pl.debug_check(False, \"failed check\")  # This check always fails.\n     input_ = jnp.arange(4, dtype=jnp.int32)\n     out_shape = jax.ShapeDtypeStruct(input_.shape, input_.dtype)\n-    with pltpu.enable_runtime_assert(False):\n+    with pl.enable_debug_checks(False):\n       pallas_call = pl.pallas_call(kernel, out_shape=out_shape)\n       result = pallas_call(input_)\n     np.testing.assert_allclose(result, input_)\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "pytest: use importlib mode by default.\n\nThis is an attempt to re-land https://github.com/jax-ml/jax/pull/28650, fixing build failures.\n\n**Motivation**\n\nhttps://github.com/jax-ml/jax/pull/28650 was motivated by recent changes in setuptools, which caused test failures with editable installs, but it also exposes a potentially larger issue with our approach to testing with pytest. As far as I understand it, the default pytest behavior is to prepend the working directory to `sys.path`, meaning that `jax` is imported from the source directory, rather than the installed version. Switching to the `importlib` import mode means that we correctly test against the installed version of `jax`, which seems like what we typically want to do.\n\nThe catch is that then we need to explicitly package any test utilities into the distribution. We don't currently package test-specific utilities like `internal_test_util` with JAX, but these utilities were still available to tests since they live within the `jax` source tree. This breaks when using `importlib` import mode, and a non-editable install of JAX.\n\n**Solutions**\n\nThe approach that I've taken here is to explicitly package everything needed by the tests into the `jax` distribution. This means that we can correctly test against the _installed_ version of JAX when using pytest.\n\nThis solution isn't ideal because it means that we're distributing `jax` submodules that aren't actually required except when running the test suite, but this seems like a small price to pay to me.\n\n**Alternatives**\n\nOne different approach that we could take would be to only support using pytest with _editable_ installs of JAX. This would work because the required files would still be discoverable in an editable install because they live within the source tree. In fact, before this change, most of our CI jobs actually did install an editable distribution (which is why the failures in https://github.com/jax-ml/jax/pull/28650 weren't caught in pre-submit!). The problem with this approach is that we're not actually testing JAX as it is used when installed from a distribution, and it wouldn't catch things like missing submodules. I think it's much better to test against the installed distribution!\n\nA more extreme approach would be to switch JAX to a `src/jax` and `src/jaxlib` layout (i.e. moving `jax` and `jaxlib` out of the root directory) as recommended by the Python packaging docs. Unfortunately this would be complicated with the way JAX is distributed internally at Google, so I think that's probably a non-starter.\n\nPiperOrigin-RevId: 762419160",
        "output": [
            "```diff\nCommit: 9cbf4934936043d472bb7b81cf02c49af77a97b1\nDate: 2025-05-23T14:45:29Z\nURL: https://github.com/jax-ml/jax/commit/9cbf4934936043d472bb7b81cf02c49af77a97b1\nFiles changed: 7\nAdditions: +51, Deletions: -4\ndiff --git a/.github/workflows/ci-build.yaml b/.github/workflows/ci-build.yaml\nindex 09f169548796..0769c698d5fe 100644\n--- a/.github/workflows/ci-build.yaml\n+++ b/.github/workflows/ci-build.yaml\n@@ -88,7 +88,6 @@ jobs:\n         JAX_SKIP_SLOW_TESTS: true\n         PY_COLORS: 1\n       run: |\n-        uv pip install --system -e .\n         echo \"JAX_NUM_GENERATED_CASES=$JAX_NUM_GENERATED_CASES\"\n         echo \"JAX_ENABLE_X64=$JAX_ENABLE_X64\"\n         echo \"JAX_ENABLE_CUSTOM_PRNG=$JAX_ENABLE_CUSTOM_PRNG\"\n@@ -185,7 +184,6 @@ jobs:\n         JAX_SKIP_SLOW_TESTS: true\n         PY_COLORS: 1\n       run: |\n-        uv pip install --system -e .\n         echo \"JAX_NUM_GENERATED_CASES=$JAX_NUM_GENERATED_CASES\"\n         echo \"JAX_ENABLE_X64=$JAX_ENABLE_X64\"\n         echo \"JAX_ENABLE_CHECKS=$JAX_ENABLE_CHECKS\"\ndiff --git a/BUILD.bazel b/BUILD.bazel\nindex 887f28d4583e..44885124797f 100644\n--- a/BUILD.bazel\n+++ b/BUILD.bazel\n@@ -42,12 +42,19 @@ wheel_sources(\n         \"//jax:pallas_triton\",\n         \"//jax:source_mapper\",\n         \"//jax:sparse_test_util\",\n+        \"//jax:test_multiprocess\",\n         \"//jax:test_util\",\n+        \"//jax:internal_export_back_compat_test_util\",\n+        \"//jax:internal_export_back_compat_test_data\",\n+        \"//jax:internal_test_harnesses\",\n+        \"//jax:internal_test_util\",\n         \"//jax/_src/lib\",\n         \"//jax/_src/pallas/fuser\",\n         \"//jax/_src/pallas/mosaic_gpu\",\n         \"//jax/experimental/array_serialization:serialization\",\n         \"//jax/experimental/jax2tf\",\n+        \"//jax/experimental/mosaic/gpu/examples:flash_attention\",\n+        \"//jax/experimental/mosaic/gpu/examples:matmul\",\n         \"//jax/extend\",\n         \"//jax/extend:ifrt_programs\",\n         \"//jax/extend/mlir\",\ndiff --git a/jax/_src/internal_test_util/export_back_compat_test_data/__init__.py b/jax/_src/internal_test_util/export_back_compat_test_data/__init__.py\nnew file mode 100644\nindex 000000000000..3da0dd1fa3ca\n--- /dev/null\n+++ b/jax/_src/internal_test_util/export_back_compat_test_data/__init__.py\n@@ -0,0 +1,14 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\ndiff --git a/jax/_src/internal_test_util/export_back_compat_test_data/pallas/__init__.py b/jax/_src/internal_test_util/export_back_compat_test_data/pallas/__init__.py\nnew file mode 100644\nindex 000000000000..3da0dd1fa3ca\n--- /dev/null\n+++ b/jax/_src/internal_test_util/export_back_compat_test_data/pallas/__init__.py\n@@ -0,0 +1,14 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\ndiff --git a/jax/experimental/mosaic/gpu/examples/__init__.py b/jax/experimental/mosaic/gpu/examples/__init__.py\nnew file mode 100644\nindex 000000000000..3da0dd1fa3ca\n--- /dev/null\n+++ b/jax/experimental/mosaic/gpu/examples/__init__.py\n@@ -0,0 +1,14 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\ndiff --git a/pyproject.toml b/pyproject.toml\nindex 83b85b0271f5..d48351197b54 100644\n--- a/pyproject.toml\n+++ b/pyproject.toml\n@@ -88,7 +88,7 @@ doctest_optionflags = [\n     \"NUMBER\",\n     \"NORMALIZE_WHITESPACE\"\n ]\n-addopts = \"--doctest-glob='*.rst' --ignore='examples/ffi'\"\n+addopts = \"--doctest-glob='*.rst' --ignore='examples/ffi' --import-mode=importlib\"\n \n [tool.ruff]\n preview = true\ndiff --git a/setup.py b/setup.py\nindex ef78b8f6e7ff..2b50b041008d 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -57,7 +57,7 @@ def load_version_module(pkg_path):\n     long_description_content_type='text/markdown',\n     author='JAX team',\n     author_email='jax-dev@google.com',\n-    packages=find_packages(exclude=[\"*examples*\", \"*internal_test_util*\"]),\n+    packages=find_packages(exclude=[\"examples\"]),\n     package_data={'jax': ['py.typed', \"*.pyi\", \"**/*.pyi\"]},\n     python_requires='>=3.10',\n     install_requires=[\n\n```"
        ],
        "from_id": [
            "dfm",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move _src/custom_transpose.py into its own BUILD rule\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nPiperOrigin-RevId: 762414305",
        "output": [
            "```diff\nCommit: 0a970e43eceeb5aa076d1b981c49dfcf796b041c\nDate: 2025-05-23T14:32:08Z\nURL: https://github.com/jax-ml/jax/commit/0a970e43eceeb5aa076d1b981c49dfcf796b041c\nFiles changed: 2\nAdditions: +21, Deletions: -1\ndiff --git a/jax/BUILD b/jax/BUILD\nindex e820bd06fe89..e7f1fad3121d 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -307,7 +307,6 @@ py_library_providing_imports_info(\n         \"_src/custom_derivatives.py\",\n         \"_src/custom_partitioning.py\",\n         \"_src/custom_partitioning_sharding_rule.py\",\n-        \"_src/custom_transpose.py\",\n         \"_src/debugging.py\",\n         \"_src/dispatch.py\",\n         \"_src/dlpack.py\",\n@@ -391,6 +390,7 @@ py_library_providing_imports_info(\n         \":config\",\n         \":core\",\n         \":custom_api_util\",\n+        \":custom_transpose\",\n         \":deprecations\",\n         \":dtypes\",\n         \":effects\",\n@@ -595,6 +595,25 @@ pytype_strict_library(\n     srcs = [\"_src/custom_api_util.py\"],\n )\n \n+pytype_strict_library(\n+    name = \"custom_transpose\",\n+    srcs = [\"_src/custom_transpose.py\"],\n+    deps = [\n+        \":ad\",\n+        \":ad_util\",\n+        \":api_util\",\n+        \":core\",\n+        \":custom_api_util\",\n+        \":mlir\",\n+        \":partial_eval\",\n+        \":source_info_util\",\n+        \":traceback_util\",\n+        \":tree_util\",\n+        \":util\",\n+        \":xla\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"deprecations\",\n     srcs = [\"_src/deprecations.py\"],\ndiff --git a/tests/BUILD b/tests/BUILD\nindex 2418c8224869..6c9f3f74b56a 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -61,6 +61,7 @@ jax_multiplatform_test(\n     srcs = [\"debug_info_test.py\"],\n     enable_configs = [\"tpu_v3_x4\"],\n     deps = [\n+        \"//jax:custom_transpose\",\n         \"//jax:experimental\",\n         \"//jax:pallas\",\n         \"//jax:pallas_gpu\",\n\n```"
        ],
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Pass the right number of immediate values in the wgmma inline asm.\n\nBefore this change, in the int32 case, we pass two extra immediate args compared to the number of parameters in the ASM string. Running tests with `-UNDEBUG` detects the error.\n\nI think this likely broke with the special-casing of `int32` in cl/761489756.\n\nI've added an assert that should prevent mismatches in the future.\n\nPiperOrigin-RevId: 762394530",
        "output": [
            "```diff\nCommit: c1c0c0fa27f52de19ddd5112580c8da2f711e1e8\nDate: 2025-05-23T13:21:44Z\nURL: https://github.com/jax-ml/jax/commit/c1c0c0fa27f52de19ddd5112580c8da2f711e1e8\nFiles changed: 1\nAdditions: +8, Deletions: -1\ndiff --git a/jax/experimental/mosaic/gpu/wgmma.py b/jax/experimental/mosaic/gpu/wgmma.py\nindex abbd517fb37d..2fe826e173e5 100644\n--- a/jax/experimental/mosaic/gpu/wgmma.py\n+++ b/jax/experimental/mosaic/gpu/wgmma.py\n@@ -226,11 +226,18 @@ def lc(x):\n     return llvm.ConstantOp(i32, ir.IntegerAttr.get(i32, x)).result\n \n   use_out = scale_a = scale_b = lc(1)\n-  imms = [use_out, scale_a, scale_b]\n+  if out_ty == i32:\n+    imms = [use_out]\n+  else:\n+    imms = [use_out, scale_a, scale_b]\n+\n   if supports_transpose and a_transpose is not None:\n     imms += [lc(int(a_transpose)), lc(int(b_transpose))]\n   elif supports_transpose:\n     imms += [lc(int(b_transpose))]\n+\n+  assert len(imms) == num_imm_regs + 1  # +1 for the use_out_reg in setp.ne.b32\n+\n   if acc.ndim != 10 or acc.shape[0] != 1 or math.prod(acc.shape[2:]) != 2:\n     raise ValueError(acc.shape)\n   acc_struct_type = ir.Type.parse(\n\n```"
        ],
        "from_id": [
            "dimitar-asenov",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add checks for argument shapes and types\n\nApparently we never checked it and it's been quite easy to\nget this wrong.\n\nPiperOrigin-RevId: 762394139",
        "output": [
            "```diff\nCommit: 6cc627a4dc194d91e2ea3920ddd51e310b88920e\nDate: 2025-05-23T13:19:53Z\nURL: https://github.com/jax-ml/jax/commit/6cc627a4dc194d91e2ea3920ddd51e310b88920e\nFiles changed: 2\nAdditions: +16, Deletions: -2\ndiff --git a/jax/experimental/mosaic/gpu/core.py b/jax/experimental/mosaic/gpu/core.py\nindex 5e1ed6b88412..9464bb587c71 100644\n--- a/jax/experimental/mosaic/gpu/core.py\n+++ b/jax/experimental/mosaic/gpu/core.py\n@@ -677,7 +677,7 @@ def as_gpu_kernel(\n   if launch_ctx.is_device_collective and not supports_cross_device_collectives():\n     raise RuntimeError(\"Kernel is a cross-device collective but no support is available.\")\n \n-  expected_arg_treedef = jax.tree.structure(in_shape)\n+  expected_arg_tys, expected_arg_treedef = jax.tree.flatten(in_shape)\n   def _check_args(*args):\n     arg_treedef = jax.tree.structure(args)\n     if arg_treedef != expected_arg_treedef:\n@@ -685,6 +685,20 @@ def _check_args(*args):\n           f\"Invalid argument structure: expected {expected_arg_treedef}, got\"\n           f\" {arg_treedef}, ({args=})\"\n       )\n+    for arg, expected_ty in zip(args, expected_arg_tys):\n+      if arg.shape != expected_ty.shape:\n+        raise ValueError(\n+            f\"Argument shape mismatch: expected {expected_ty.shape}, got\"\n+            f\" {arg.shape}\"\n+        )\n+      if arg.dtype != expected_ty.dtype:\n+        hint = \"\"\n+        if not arg.shape:\n+          hint = f\". Hint: cast the scalar to {expected_ty.dtype} explicitly.\"\n+        raise ValueError(\n+            f\"Argument dtype mismatch: expected {expected_ty.dtype}, got\"\n+            f\" {arg.dtype}{hint}\"\n+        )\n \n   def bind(*args) -> Any:\n     return mosaic_gpu_p.bind(*args, module=module, out_types=out_shape)\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 6ea27eb42878..42a3f0fc83c1 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -460,7 +460,7 @@ def test_scalar_argument(self, dtype):\n         \" values read from the 32-bit input buffer to sometimes\"\n         \" (nondeterministically) contain garbage.\")\n \n-    scalar = 42\n+    scalar = dtype(42)\n     expected = np.full((128, 128), scalar, dtype=dtype)\n \n     def kernel(ctx, inp, out, _):\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add support for async copies to peer devices\n\nPiperOrigin-RevId: 762370447",
        "output": [
            "```diff\nCommit: 5a448b867cf9c91d99472b51f78c66749ce98e62\nDate: 2025-05-23T11:47:34Z\nURL: https://github.com/jax-ml/jax/commit/5a448b867cf9c91d99472b51f78c66749ce98e62\nFiles changed: 4\nAdditions: +177, Deletions: -1\ndiff --git a/jax/experimental/mosaic/gpu/launch_context.py b/jax/experimental/mosaic/gpu/launch_context.py\nindex aaae007a67f0..e4f0c4efa22c 100644\n--- a/jax/experimental/mosaic/gpu/launch_context.py\n+++ b/jax/experimental/mosaic/gpu/launch_context.py\n@@ -400,6 +400,7 @@ def _get_tma_desc(\n       self,\n       gmem_ref,\n       gmem_transform: tuple[MemRefTransform, ...],\n+      gmem_peer_id: int | ir.Value | None,\n       transformed_slice_shape: tuple[int, ...],\n       swizzle: int | None,\n       reduction_op: Literal[\n@@ -408,6 +409,7 @@ def _get_tma_desc(\n   ):\n     tma_desc_key = (gmem_ref, transformed_slice_shape, swizzle, gmem_transform)\n     if (tma_desc := self.tma_descriptors.get(tma_desc_key, None)) is None:\n+      i32 = ir.IntegerType.get_signless(32)\n       i64 = ir.IntegerType.get_signless(64)\n       ptr_ty = ir.Type.parse(\"!llvm.ptr\")\n       def init_tma_desc(host_ptr):\n@@ -432,6 +434,25 @@ def init_tma_desc(host_ptr):\n         base_ptr = llvm.getelementptr(\n             ptr_ty, alloc_ptr, [as_i64(offset)], [llvm_dyn], ref_ty.element_type, llvm.GEPNoWrapFlags.none,\n         )\n+        if gmem_peer_id is not None:\n+          if not isinstance(gmem_peer_id, ir.Value):\n+            peer_id = c(gmem_peer_id, i32)\n+          else:\n+            try:\n+              # We try to reproduce the gmem_peer_id computation on the host.\n+              peer_id = _recompute_peer_id(gmem_peer_id)\n+            except ReplicationError as e:\n+              raise ValueError(\n+                  \"Failed to recompute the async_copy peer id on the host\"\n+              ) from e\n+          self._ensure_nvshmem_decls()\n+          base_ptr = llvm.call(\n+              base_ptr.type,\n+              [base_ptr, peer_id],\n+              [],\n+              [],\n+              callee=\"nvshmem_ptr\",\n+          )\n         rank = ref_ty.rank\n         assert rank * 2 == len(sizes_and_strides)\n         swizzle_arg = (\n@@ -507,6 +528,7 @@ def async_copy(\n       dst_ref,\n       gmem_slice: Any = (),\n       gmem_transform: MemRefTransform | tuple[MemRefTransform, ...] = (),\n+      gmem_peer_id: int | ir.Value | None = None,\n       barrier: utils.BarrierRef | None = None,\n       swizzle: int | None = None,\n       arrive: bool | None = None,\n@@ -750,7 +772,8 @@ def partition_dim(dim: int, idx: ir.Value, num_chunks: int):\n       multicast_mask = None\n \n     tma_desc = self._get_tma_desc(\n-        gmem_ref, gmem_transform, tuple(slice_shape), swizzle, reduction_op,\n+        gmem_ref, gmem_transform, gmem_peer_id,\n+        tuple(slice_shape), swizzle, reduction_op,\n     )\n \n     # We constuct TMA descriptors in column-major order.\n@@ -893,3 +916,33 @@ def device_id(self) -> ir.Value:\n     self._ensure_nvshmem_decls()\n     i32 = ir.IntegerType.get_signless(32)\n     return llvm.call(i32, [], [], [], callee=\"nvshmem_my_pe\")\n+\n+\n+class ReplicationError(Exception):\n+  pass\n+\n+def _recompute_peer_id(peer_id: ir.Value, fuel=8) -> ir.Value:\n+  if fuel == 0:\n+    raise ReplicationError(\n+        \"gmem_peer_id computation is too complicated to recompute on the host\"\n+    )\n+  if isinstance(peer_id, ir.BlockArgument):\n+    raise ReplicationError(\"Can't recompute a value that's a block argument\")\n+  op = peer_id.owner.opview\n+  # We accept all arith ops\n+  if op.OPERATION_NAME.startswith(\"arith.\"):\n+    new_operands = [_recompute_peer_id(x, fuel - 1) for x in op.operands]\n+    result_types = [r.type for r in op.results]\n+    new_attributes = {na.name: na.attr for na in op.attributes}\n+    new_op = ir.Operation.create(\n+        op.OPERATION_NAME, result_types, new_operands, new_attributes\n+    )\n+    return new_op.results if len(new_op.results) > 1 else new_op.result\n+  # nvshmem_my_pe queries the device id of the current process and works on both\n+  # the host and the device.\n+  if isinstance(op, llvm.CallOp) and op.callee.value == \"nvshmem_my_pe\":\n+    i32 = ir.IntegerType.get_signless(32)\n+    return llvm.call(i32, [], [], [], callee=\"nvshmem_my_pe\")\n+  raise ReplicationError(\n+      f\"Unrecognized op can't be recomputed on the host: {op}\"\n+  )\ndiff --git a/jaxlib/mosaic/gpu/BUILD b/jaxlib/mosaic/gpu/BUILD\nindex 66f13bdac7f5..fc1abb9397d5 100644\n--- a/jaxlib/mosaic/gpu/BUILD\n+++ b/jaxlib/mosaic/gpu/BUILD\n@@ -122,6 +122,8 @@ cc_library(\n     # Linker may prune these symbols if they are not explicitly exported.\n     linkopts = [\n         \"-Wl,--export-dynamic-symbol='mosaic_gpu_*'\",\n+        \"-Wl,--export-dynamic-symbol='nvshmem_my_pe'\",\n+        \"-Wl,--export-dynamic-symbol='nvshmem_ptr'\",\n         \"-Wl,--export-dynamic-symbol='nvshmemx_barrier_all_on_stream'\",\n         \"-Wl,--export-dynamic-symbol='nvshmemx_cumodule_init'\",\n         \"-Wl,--export-dynamic-symbol='nvshmemx_init_status'\",\ndiff --git a/tests/mosaic/BUILD b/tests/mosaic/BUILD\nindex 75e1df335f6f..9b6a7f79d099 100644\n--- a/tests/mosaic/BUILD\n+++ b/tests/mosaic/BUILD\n@@ -63,6 +63,27 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"gpu_test_distributed\",\n+    srcs = [\"gpu_test_distributed.py\"],\n+    args = [\n+        \"--num_processes=2\",\n+        \"--gpus_per_process=1\",\n+    ],\n+    enable_backends = [],\n+    enable_configs = [\"gpu_h100x2\"],\n+    env = {\"XLA_FLAGS\": \"--xla_gpu_autotune_level=0 --xla_gpu_experimental_enable_nvshmem=true\"},\n+    tags = [\"multiaccelerator\"],\n+    deps = [\n+        \"//jax:experimental\",\n+        \"//jax:mosaic_gpu\",\n+        \"//jax:test_multiprocess\",\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n+)\n+\n jax_py_test(\n     name = \"gpu_dialect_test\",\n     srcs = [\"gpu_dialect_test.py\"],\ndiff --git a/tests/mosaic/gpu_test_distributed.py b/tests/mosaic/gpu_test_distributed.py\nnew file mode 100644\nindex 000000000000..fee2ce5b03a6\n--- /dev/null\n+++ b/tests/mosaic/gpu_test_distributed.py\n@@ -0,0 +1,100 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+from absl.testing import parameterized\n+import jax\n+from jax._src import config\n+from jax._src import test_util as jtu\n+from jax._src import test_multiprocess as jt_multiprocess\n+from jax._src.interpreters import mlir\n+from jax._src.lib.mlir import ir\n+from jax._src.lib.mlir.dialects import arith\n+from jax.experimental.mosaic.gpu import dialect as mgpu_dialect  # pylint: disable=g-importing-member\n+from jax.experimental import shard\n+from jax.experimental import multihost_utils\n+import jax.numpy as jnp\n+import numpy as np\n+try:\n+  import jax._src.lib.mosaic_gpu  # noqa: F401\n+  HAS_MOSAIC_GPU = True\n+except ImportError:\n+  HAS_MOSAIC_GPU = False\n+else:\n+  import jax.experimental.mosaic.gpu as mgpu\n+\n+\n+# ruff: noqa: F405\n+# pylint: disable=g-complex-comprehension\n+P = jax.sharding.PartitionSpec\n+\n+\n+class TestCase(parameterized.TestCase):\n+\n+  def setUp(self):\n+    if not HAS_MOSAIC_GPU:\n+      self.skipTest(\"jaxlib built without Mosaic GPU\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability >= sm90\")\n+    if not mgpu.supports_cross_device_collectives():\n+      self.skipTest(\"NVSHMEM library unavailable.\")\n+    if jax.process_count() == 1:\n+      self.skipTest(\"Test requires multiple processes.\")\n+    if jax.device_count() != jax.process_count():\n+      self.skipTest(\"Need 1 device per process\")\n+    super().setUp()\n+    self.prng = np.random.default_rng(1234)\n+    self.context = mlir.make_ir_context()\n+    if mgpu_dialect is not None:\n+      mgpu_dialect.register_dialect(self.context)\n+    self.enter_context(config.traceback_filtering(\"off\"))\n+    self.enter_context(self.context)\n+    self.enter_context(ir.Location.unknown())\n+\n+\n+class ProfilerTest(TestCase):\n+\n+  def test_remote_async_copy(self):\n+    i32 = ir.IntegerType.get_signless(32)\n+    def kernel(ctx, src, dst, scratch):\n+      tmp, barrier = scratch\n+      other_device = arith.subi(arith.constant(i32, 1), ctx.device_id())\n+      ctx.async_copy(src_ref=src, dst_ref=tmp, barrier=barrier)\n+      barrier.wait()\n+      ctx.async_copy(src_ref=tmp, dst_ref=dst, gmem_peer_id=other_device)\n+      ctx.await_async_copy(0)\n+    mesh = jax.make_mesh(\n+        (2,), (\"x\",), axis_types=(jax.sharding.AxisType.Explicit,)\n+    )\n+    with jax.sharding.use_mesh(mesh):\n+      x_np = np.arange(64 * 64, dtype=jnp.float32).reshape(64, 64)\n+      x = shard.reshard(x_np, P(\"x\"))\n+      y = jax.jit(\n+          jax.shard_map(\n+              lambda x: mgpu.as_gpu_kernel(\n+                  kernel, (1, 1, 1), (128, 1, 1), x, x, (x, mgpu.TMABarrier())\n+              )(x),\n+              out_specs=P(\"x\"),\n+              check_vma=False,\n+          )\n+      )(x)\n+      y_np = multihost_utils.process_allgather(y, tiled=True)\n+      np.testing.assert_array_equal(\n+          y_np, np.concatenate(np.split(x_np, 2)[::-1], axis=0)\n+      )\n+\n+\n+if __name__ == \"__main__\":\n+  jt_multiprocess.main()\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Properly handle single-element outputs in inline assembly.\n\nThis issue was discovered when enabling the ragged dot example kernel to run using warpgroup semantics.\n\nThe new test requires `-UNDEBUG`.\n\nPiperOrigin-RevId: 762365352",
        "output": [
            "```diff\nCommit: dc0cdf720bd3e3747d702a29cb1e63ef529eba80\nDate: 2025-05-23T11:28:14Z\nURL: https://github.com/jax-ml/jax/commit/dc0cdf720bd3e3747d702a29cb1e63ef529eba80\nFiles changed: 2\nAdditions: +37, Deletions: -11\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex f69d3f33fe7c..77584b5f0dd4 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -2591,17 +2591,28 @@ def _repack(regs_it, reg_ty):\n   all_reg_constraints = \",\".join(\n       [*(\"=\" + c for c in reg_constraints), *reg_constraints]\n   )\n-  struct_ty = ir.Type.parse(\n-      f\"!llvm.struct<({','.join(map(str, reg_dtypes))})>\"\n-  )\n-  result_struct = llvm.inline_asm(\n-      struct_ty, regs, ptx, all_reg_constraints,\n-      asm_dialect=0, has_side_effects=True,\n-  )\n-  regs = [\n-      llvm.extractvalue(dtype, result_struct, [i])\n-      for i, dtype in enumerate(reg_dtypes)\n-  ]\n+\n+  if len(reg_dtypes) == 1:\n+    # The InlineAsm::verify() function doesn't allow a struct output when there\n+    # is only one element (even though that seems to work for the case below).\n+    result_elem = llvm.inline_asm(\n+        reg_dtypes[0], regs, ptx, all_reg_constraints,\n+        asm_dialect=0, has_side_effects=True,\n+    )\n+    regs = [result_elem]\n+  else:\n+    struct_ty = ir.Type.parse(\n+        f\"!llvm.struct<({','.join(map(str, reg_dtypes))})>\"\n+    )\n+    result_struct = llvm.inline_asm(\n+        struct_ty, regs, ptx, all_reg_constraints,\n+        asm_dialect=0, has_side_effects=True,\n+    )\n+    regs = [\n+        llvm.extractvalue(dtype, result_struct, [i])\n+        for i, dtype in enumerate(reg_dtypes)\n+    ]\n+\n   i32 = ir.IntegerType.get_signless(32)\n   results = []\n   regs_it = iter(regs)\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex e7fc9723347b..6ea27eb42878 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -2451,6 +2451,21 @@ def kernel(ctx, inp, out, smem):\n     f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, None)\n     np.testing.assert_array_equal(f(x), x * 3)\n \n+  def test_optimization_barrier_with_single_value(self):\n+    shape = (64, 64)\n+    value = 5.0\n+    dtype = jnp.float32\n+    def kernel(ctx, out, smem):\n+      del ctx, smem\n+      mlir_type = utils.dtype_to_ir_type(dtype)\n+      arr = mgpu.FragmentedArray.splat(c(value, mlir_type), shape)\n+      arr = mgpu.optimization_barrier(arr)\n+      arr.store_untiled(out)\n+\n+    out_shape = jax.ShapeDtypeStruct(shape, dtype)\n+    f = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), out_shape, ())\n+    np.testing.assert_array_equal(f(), jnp.full(shape, value, dtype=dtype))\n+\n   def test_convert_bool_to_u8(self):\n     m, n = 128, 128\n     def kernel(ctx, dst, _):\n\n```"
        ],
        "from_id": [
            "dimitar-asenov",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix psum_invariant transpose rule where we were binding `pvary` with `ad_util.Zero` which lead to errors like this: `TypeError: Argument 'Zero(float32[1,1,512])' of type '<class 'jax._src.ad_util.Zero'>' is not a valid JAX type`\n\nPiperOrigin-RevId: 762264425",
        "output": [
            "```diff\nCommit: 199d9f73cba51e940a69c3e90f666e869c13f413\nDate: 2025-05-23T05:15:21Z\nURL: https://github.com/jax-ml/jax/commit/199d9f73cba51e940a69c3e90f666e869c13f413\nFiles changed: 2\nAdditions: +33, Deletions: -5\ndiff --git a/jax/_src/lax/parallel.py b/jax/_src/lax/parallel.py\nindex bf27261a2c8e..c5f8d3988144 100644\n--- a/jax/_src/lax/parallel.py\n+++ b/jax/_src/lax/parallel.py\n@@ -2056,13 +2056,25 @@ def _pgather_collective_batcher(axis_size, frame_name, _, vals_in, dims_in, *, a\n batching.skippable_batchers[psum_invariant_p] = partial(_names_in_param, 'axes')\n \n def _psum_invariant_transpose_rule(cts, *args, axes, axis_index_groups):\n-  del args\n-  return core.pvary_p.bind(*cts, axes=axes, axis_index_groups=axis_index_groups)\n+  def f(ct, arg):\n+    assert ad.is_undefined_primal(arg)\n+    return ad.Zero(arg.aval) if type(ct) is ad.Zero else ct\n+  cts = map(f, cts, args)\n+  nonzero_out_cts, treedef = tree_util.tree_flatten(cts)\n+  nonzero_in_cts = core.pvary_p.bind(*nonzero_out_cts, axes=axes,\n+                                     axis_index_groups=axis_index_groups)\n+  return tree_util.tree_unflatten(treedef, nonzero_in_cts)\n ad.deflinear2(psum_invariant_p, _psum_invariant_transpose_rule)\n \n ########################### pvary ##################################\n \n-def _pvary_transpose_rule(cts, *_, axes, axis_index_groups):\n-  return psum_invariant_p.bind(\n-      *cts, axes=axes, axis_index_groups=axis_index_groups)\n+def _pvary_transpose_rule(cts, *args, axes, axis_index_groups):\n+  def f(ct, arg):\n+    assert ad.is_undefined_primal(arg)\n+    return ad.Zero(arg.aval) if type(ct) is ad.Zero else ct\n+  cts = map(f, cts, args)\n+  nonzero_out_cts, treedef = tree_util.tree_flatten(cts)\n+  nonzero_in_cts = psum_invariant_p.bind(*nonzero_out_cts, axes=axes,\n+                                         axis_index_groups=axis_index_groups)\n+  return tree_util.tree_unflatten(treedef, nonzero_in_cts)\n ad.deflinear2(core.pvary_p, _pvary_transpose_rule)\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 90360989f13f..df69db7c9462 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -989,6 +989,22 @@ def f(x):\n     for i in range(len(jax.devices())):\n       self.assertIn(f'instance {i} has value', output())\n \n+  def test_psum_transpose_non_zero_cts(self):\n+    mesh = jtu.create_mesh((8,), 'x')\n+    @shard_map(mesh=mesh, in_specs=P('x'), out_specs=(P('x'), P()))\n+    def f1(x_block):\n+      return x_block, jax.lax.psum(x_block, axis_name='x')\n+\n+    x1 = jnp.arange(16.)\n+    f1(x1)  # doesn't crash\n+\n+    def f2(x_block):\n+      y, _ = f1(x_block)\n+      return y.sum()\n+\n+    jax.jit(jax.grad(f2))(x1)  # doesn't crash\n+    jax.grad(f2)(x1)  # doesn't crash\n+\n   @jtu.run_on_devices('cpu', 'gpu', 'tpu')\n   @jtu.thread_unsafe_test()\n   def test_debug_print_jit_partial_auto(self):\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Implement `_to_sdy_sharding` for GSPMDSharding.\n\nPiperOrigin-RevId: 762220186",
        "output": [
            "```diff\nCommit: 82d76099b448a1ea9da1ae475d426abed2d57f90\nDate: 2025-05-23T02:22:41Z\nURL: https://github.com/jax-ml/jax/commit/82d76099b448a1ea9da1ae475d426abed2d57f90\nFiles changed: 5\nAdditions: +70, Deletions: -5\ndiff --git a/jax/_src/sharding_impls.py b/jax/_src/sharding_impls.py\nindex 982af82c5c4d..4703e6403079 100644\n--- a/jax/_src/sharding_impls.py\n+++ b/jax/_src/sharding_impls.py\n@@ -638,8 +638,25 @@ def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n     return self._hlo_sharding\n \n   def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n-    raise NotImplementedError(\n-        \"GSPMDSharding can't be converted to SdyArray.\")\n+    if self._hlo_sharding.tuple_elements():\n+      raise TypeError(\n+          f'Cannot convert GSPMDSharding {self._hlo_sharding} into SdyArray.')\n+    elif self._hlo_sharding.is_replicated():\n+      empty_mesh = mesh_lib.AbstractMesh((), ())\n+      return NamedSharding(empty_mesh, PartitionSpec())._to_sdy_sharding(\n+          num_dimensions)\n+    elif self._hlo_sharding.is_tiled():\n+      if not self._hlo_sharding.is_tile_assignment_iota():\n+        raise TypeError(\n+            f'Cannot convert GSPMDSharding {self._hlo_sharding} into SdyArray.')\n+      axis_sizes = tuple(self._hlo_sharding.get_axis_sizes())\n+      axis_names = tuple(f'_axis_{i}' for i in range(len(axis_sizes)))\n+      mesh = mesh_lib.AbstractMesh(axis_sizes, axis_names)\n+      return _gspmd_to_named_sharding_via_mesh(self, mesh)._to_sdy_sharding(\n+          num_dimensions)\n+    else:\n+      raise TypeError(\n+          f'Cannot convert GSPMDSharding {self._hlo_sharding} into SdyArray.')\n \n   @functools.cached_property\n   def is_fully_replicated(self) -> bool:\n@@ -1241,11 +1258,13 @@ def create_mesh_pspec_sharding(\n \n \n def _gspmd_to_named_sharding_via_mesh(\n-    out_s: GSPMDSharding, mesh: mesh_lib.Mesh) -> NamedSharding:\n+    out_s: GSPMDSharding, mesh: mesh_lib.Mesh | mesh_lib.AbstractMesh\n+) -> NamedSharding:\n   spec = parse_flatten_op_sharding(out_s._hlo_sharding, mesh)[0]\n   return create_mesh_pspec_sharding(\n       mesh, spec, memory_kind=out_s.memory_kind)\n \n+\n def flatten_spec(spec):\n   out = []\n   for s in spec:\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 898a4d5f2d22..ed0089a3dd88 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -417,6 +417,7 @@ class HloSharding:\n   def tuple_elements(self) -> list[HloSharding]: ...\n   def num_devices(self) -> int: ...\n   def num_dimensions(self) -> int: ...\n+  def is_tile_assignment_iota(self) -> bool: ...\n   def tile_assignment_dimensions(self) -> Sequence[int]: ...\n   def tile_assignment_devices(self) -> Sequence[int]: ...\n   def subgroup_types(self) -> Sequence[OpSharding_Type]: ...\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex c97ebf9d7c0a..ac816e72bebe 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 343\n+_version = 344\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\ndiff --git a/jaxlib/xla_compiler.cc b/jaxlib/xla_compiler.cc\nindex 1066c8137d32..f9ec134793ed 100644\n--- a/jaxlib/xla_compiler.cc\n+++ b/jaxlib/xla_compiler.cc\n@@ -1425,6 +1425,10 @@ void BuildXlaCompilerSubmodule(nb::module_& m) {\n             return self.tile_assignment().num_dimensions();\n           },\n           nb::lock_self())\n+      .def(\"is_tile_assignment_iota\",\n+           [](const xla::HloSharding& self) {\n+             return self.tile_assignment().iota().has_value();\n+           })\n       .def(\n           \"tile_assignment_dimensions\",\n           [](const xla::HloSharding& self) {\ndiff --git a/tests/array_test.py b/tests/array_test.py\nindex 0a04e7bd4bc9..10a17b557dea 100644\n--- a/tests/array_test.py\n+++ b/tests/array_test.py\n@@ -28,6 +28,7 @@\n from jax._src import op_shardings\n from jax._src import test_util as jtu\n from jax._src import xla_bridge as xb\n+from jax._src.lib import jaxlib_extension_version\n from jax._src.lib import xla_client as xc\n from jax._src.lib.mlir import dialects, ir\n from jax._src.util import safe_zip\n@@ -1467,7 +1468,7 @@ def test_named_sharding_unreduced_error(self):\n       NamedSharding(mesh, P('x', unreduced=('y', None)))\n \n   def test_hlo_sharding_get_axis_sizes(self):\n-    if jax._src.lib.jaxlib_extension_version < 343:\n+    if jaxlib_extension_version < 343:\n       self.skipTest('Requires jaxlib_extension_version >= 343')\n \n     op = xc.OpSharding()\n@@ -1479,6 +1480,46 @@ def test_hlo_sharding_get_axis_sizes(self):\n     self.assertIn('{devices=[6,35]<=[7,10,3]T(2,1,0)}', repr(s))\n     self.assertEqual(s._to_xla_hlo_sharding(2).get_axis_sizes(), [7, 2, 5, 3])\n \n+  @parameterized.named_parameters(\n+      ('2d_mesh_x_y', (4, 2), P('x', 'y')),\n+      ('2d_mesh_x', (4, 2), P('x')),\n+      ('2d_mesh_y', (4, 2), P('y')),\n+      ('2d_mesh_none_y', (4, 2), P(None, 'y')),\n+      ('2d_mesh_none_x', (4, 2), P(None, 'x')),\n+      ('2d_mesh_xy', (4, 2), P(('x', 'y'))),\n+      ('2d_mesh_none_xy', (4, 2), P(None, ('x', 'y'))),\n+      ('2d_mesh_fully_replicated', (4, 2), P()),\n+      ('2d_mesh_x_none', (2, 1), P(('x',), None)),\n+      ('3d_mesh_none_none_z', (2, 2, 2), P(None, None, 'z')),\n+      ('3d_mesh_none_y_none', (2, 2, 2), P(None, 'y', None)),\n+      ('3d_mesh_x_y_none', (2, 2, 2), P('x', 'y', None)),\n+      ('3d_mesh_none_yz', (2, 2, 2), P(None, ('y', 'z'))),\n+      ('3d_mesh_x_none_yz', (2, 2, 2), P('x', None, ('y', 'z'))),\n+      ('3d_mesh_none_x_yz', (2, 2, 2), P(None, 'x', ('y', 'z'))),\n+      ('3d_mesh_xy_z', (2, 2, 2), P(('x', 'y'), 'z')),\n+      ('3d_mesh_xy_none_z', (2, 2, 2), P(('x', 'y'), None, 'z')),\n+      ('3d_mesh_x_y_z', (2, 2, 2), P('x', 'y', 'z')),\n+      ('3d_mesh_xz_y', (2, 2, 2), P(('x', 'z'), 'y')),\n+      ('3d_mesh_xz_none_y', (2, 2, 2), P(('x', 'z'), None, 'y')),\n+      ('3d_mesh_y_none_xz', (2, 2, 2), P('y', None, ('x', 'z'))),\n+      ('3d_mesh_none_y_xz', (2, 2, 2), P(None, 'y', ('x', 'z'))),\n+      ('3d_mesh2_none_none_z', (1, 2, 4), P(None, None, 'z')),\n+      ('3d_mesh2_x_none_none', (1, 2, 4), P('x', None, None)),\n+      ('3d_mesh_x_none_none', (2, 1, 1), P('x', None, None)),\n+  )\n+  def test_gspmd_sharding_shardy_lowering(self, mesh_shape, pspec):\n+    if jaxlib_extension_version < 344:\n+      self.skipTest('Requires jaxlib_extension_version >= 344')\n+\n+    ndim = len(mesh_shape)\n+    mesh = jtu.create_mesh(\n+        mesh_shape, ('x', 'y') if ndim == 2 else ('x', 'y', 'z')\n+    )\n+    ns = jax.sharding.NamedSharding(mesh, pspec)\n+    gs = GSPMDSharding(ns._device_assignment, ns._to_xla_hlo_sharding(ndim))\n+    out_sdy_sharding = gs._to_sdy_sharding(ndim)\n+    self.assertTrue(out_sdy_sharding, ns._to_sdy_sharding(ndim))\n+\n \n @jtu.with_config(jax_use_shardy_partitioner=True)\n class ShardyShardingTest(jtu.JaxTestCase):\n\n```"
        ],
        "from_id": [
            "ZixuanJiang",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Introduce `get_axis_sizes` API for `HloSharding` in JAX.\n\nPiperOrigin-RevId: 762213931",
        "output": [
            "```diff\nCommit: 326361828d20d9cc295edf2546c97696f446f1df\nDate: 2025-05-23T01:55:42Z\nURL: https://github.com/jax-ml/jax/commit/326361828d20d9cc295edf2546c97696f446f1df\nFiles changed: 5\nAdditions: +27, Deletions: -2\ndiff --git a/jaxlib/BUILD b/jaxlib/BUILD\nindex 363218f4a9f3..dd96b7d23a8e 100644\n--- a/jaxlib/BUILD\n+++ b/jaxlib/BUILD\n@@ -1212,6 +1212,7 @@ cc_library(\n         \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@com_google_absl//absl/types:span\",\n+        \"@llvm-project//mlir:Support\",\n         \"@nanobind\",\n         \"@xla//xla:array\",\n         \"@xla//xla:debug_options_flags\",\n@@ -1242,6 +1243,7 @@ cc_library(\n         \"@xla//xla/service:hlo_module_config\",\n         \"@xla//xla/service:hlo_proto_cc\",\n         \"@xla//xla/service:name_uniquer\",\n+        \"@xla//xla/service/spmd/shardy/stablehlo_round_trip:stablehlo_import\",\n         \"@xla//xla/tsl/lib/strings:proto_serialization\",\n         \"@xla//xla/tsl/platform:env\",\n         \"@xla//xla/tsl/platform:errors\",\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 1d7f3042e8a3..898a4d5f2d22 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -422,6 +422,7 @@ class HloSharding:\n   def subgroup_types(self) -> Sequence[OpSharding_Type]: ...\n   def replicate_on_last_tile_dim(self) -> bool: ...\n   def to_proto(self) -> OpSharding: ...\n+  def get_axis_sizes(self) -> list[int]: ...\n \n # === END xla_compiler.cc\n \ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 8f8c829ee6c7..c97ebf9d7c0a 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 342\n+_version = 343\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\ndiff --git a/jaxlib/xla_compiler.cc b/jaxlib/xla_compiler.cc\nindex 73007530c27b..1066c8137d32 100644\n--- a/jaxlib/xla_compiler.cc\n+++ b/jaxlib/xla_compiler.cc\n@@ -32,6 +32,7 @@ limitations under the License.\n #include \"absl/strings/str_split.h\"\n #include \"absl/strings/string_view.h\"\n #include \"absl/types/span.h\"\n+#include \"mlir/Support/LLVM.h\"\n #include \"nanobind/nanobind.h\"\n #include \"nanobind/ndarray.h\"\n #include \"nanobind/stl/optional.h\"  // IWYU pragma: keep\n@@ -71,6 +72,7 @@ limitations under the License.\n #include \"xla/service/hlo.pb.h\"\n #include \"xla/service/hlo_graph_dumper.h\"\n #include \"xla/service/hlo_module_config.h\"\n+#include \"xla/service/spmd/shardy/stablehlo_round_trip/stablehlo_import.h\"\n #include \"xla/shape.h\"\n #include \"xla/shape_util.h\"\n #include \"xla/tsl/lib/strings/proto_serialization.h\"\n@@ -1447,6 +1449,13 @@ void BuildXlaCompilerSubmodule(nb::module_& m) {\n       .def(\"subgroup_types\", &xla::HloSharding::subgroup_types)\n       .def(\"__repr__\",\n            [](const xla::HloSharding& self) { return self.ToString(); })\n-      .def(\"to_proto\", &xla::HloSharding::ToProto);\n+      .def(\"to_proto\", &xla::HloSharding::ToProto)\n+      .def(\"get_axis_sizes\", [](const xla::HloSharding& self) {\n+        // If returning the SmallVector, we encounter the error \"unable to\n+        // convert function return value to a Python type!\".\n+        mlir::SmallVector<int64_t> mesh_shape =\n+            xla::sdy::getAxisSizes(self.tile_assignment());\n+        return std::vector<int64_t>(mesh_shape.begin(), mesh_shape.end());\n+      });\n }  // NOLINT(readability/fn_size)\n }  // namespace xla\ndiff --git a/tests/array_test.py b/tests/array_test.py\nindex 1691c3acc749..0a04e7bd4bc9 100644\n--- a/tests/array_test.py\n+++ b/tests/array_test.py\n@@ -1466,6 +1466,19 @@ def test_named_sharding_unreduced_error(self):\n         ValueError, \"unreduced cannot contain None.*\"):\n       NamedSharding(mesh, P('x', unreduced=('y', None)))\n \n+  def test_hlo_sharding_get_axis_sizes(self):\n+    if jax._src.lib.jaxlib_extension_version < 343:\n+      self.skipTest('Requires jaxlib_extension_version >= 343')\n+\n+    op = xc.OpSharding()\n+    op.type = xc.OpSharding.Type.OTHER\n+    op.tile_assignment_dimensions = [6, 35]\n+    op.iota_reshape_dims = [7, 10, 3]\n+    op.iota_transpose_perm = [2, 1, 0]\n+    s = GSPMDSharding(jax.devices(), op)\n+    self.assertIn('{devices=[6,35]<=[7,10,3]T(2,1,0)}', repr(s))\n+    self.assertEqual(s._to_xla_hlo_sharding(2).get_axis_sizes(), [7, 2, 5, 3])\n+\n \n @jtu.with_config(jax_use_shardy_partitioner=True)\n class ShardyShardingTest(jtu.JaxTestCase):\n\n```"
        ],
        "from_id": [
            "ZixuanJiang",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Adds explicit-axis handling to shard_map batching rule.\n\nWithout this handling, in explicit sharding mode vmap of a function with an internal shmap can introduce unnecessary replication.\n\nPiperOrigin-RevId: 762175189",
        "output": [
            "```diff\nCommit: 7cc9053f9d09aff6df2917a0804d861a04be0260\nDate: 2025-05-22T23:39:47Z\nURL: https://github.com/jax-ml/jax/commit/7cc9053f9d09aff6df2917a0804d861a04be0260\nFiles changed: 2\nAdditions: +70, Deletions: -7\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 66df2505100c..0bae2da15272 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -1228,6 +1228,15 @@ def _device_put_eager_rule(mesh, *xs, srcs, devices, copy_semantics):\n \n # Batching\n \n+def _modify_specs_axis_data(trace, name, mesh, in_specs, in_dims):\n+  new_in_specs = [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, name)\n+                  for sp, d in zip(in_specs, in_dims)]\n+  new_size = trace.axis_data.size // prod(mesh.shape[n] for n in name)\n+  new_axis_data = batching.AxisData(\n+      trace.axis_data.name, new_size, trace.axis_data.spmd_name,\n+      trace.axis_data.explicit_mesh_axis)\n+  return new_in_specs, new_axis_data\n+\n def _shard_map_batch(\n     trace: batching.BatchTrace, prim: core.Primitive, fun: lu.WrappedFun,\n     in_tracers: Sequence[batching.BatchTracer], mesh: Mesh,\n@@ -1237,15 +1246,20 @@ def _shard_map_batch(\n   if any(isinstance(d, batching.RaggedAxis) for d in in_dims):\n     raise NotImplementedError\n   spmd_axis_name = trace.axis_data.spmd_name\n+  explicit_mesh_axis = trace.axis_data.explicit_mesh_axis\n   if spmd_axis_name is not None:\n     used = {n for spec in in_specs for n in _spec_to_vma(spec)}\n     if not config.disable_vmap_shmap_error.value and set(spmd_axis_name) & used:\n       raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map in_specs\")\n-    new_in_specs = [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, spmd_axis_name)\n-                    for sp, d in zip(in_specs, in_dims)]\n-    new_size = trace.axis_data.size // prod(mesh.shape[n] for n in spmd_axis_name)\n-    new_axis_data = batching.AxisData(trace.axis_data.name, new_size,\n-                                      trace.axis_data.spmd_name, None)\n+    new_in_specs, new_axis_data = _modify_specs_axis_data(\n+        trace, spmd_axis_name, mesh, in_specs, in_dims)\n+  elif explicit_mesh_axis is not None:\n+    used = {n for spec in in_specs for n in _spec_to_vma(spec)}\n+    if set(explicit_mesh_axis) & used:\n+      raise ValueError(\"vmapped away explicit mesh axis cannot appear in \"\n+                       \"shard_map in_specs\")\n+    new_in_specs, new_axis_data = _modify_specs_axis_data(\n+        trace, explicit_mesh_axis, mesh, in_specs, in_dims)\n   else:\n     new_in_specs = [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, None)\n                     for sp, d in zip(in_specs, in_dims)]\n@@ -1254,7 +1268,8 @@ def _shard_map_batch(\n \n   @as_hashable_function(closure=out_specs_thunk)\n   def new_out_specs_thunk():\n-    return _batch_out_specs(spmd_axis_name, out_dims(), out_specs_thunk())\n+    return _batch_out_specs(spmd_axis_name, explicit_mesh_axis, out_dims(),\n+                            out_specs_thunk())\n \n   new_params = dict(mesh=mesh, in_specs=new_in_specs,\n                     out_specs_thunk=new_out_specs_thunk, check_vma=check_vma,\n@@ -1266,13 +1281,21 @@ def new_out_specs_thunk():\n   return map(make_tracer, out_vals, out_dims())\n batching.BatchTrace.process_shard_map = _shard_map_batch\n \n-def _batch_out_specs(spmd_name, dims, out_specs):\n+def _batch_out_specs(spmd_name, explicit_mesh_axis, dims, out_specs):\n   if spmd_name is not None:\n     used = {n for spec in out_specs for n in _spec_to_vma(spec)}\n     if not config.disable_vmap_shmap_error.value and set(spmd_name) & used:\n       raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map out_specs\")\n     return [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, spmd_name)\n             for sp, d in zip(out_specs, dims)]\n+  elif explicit_mesh_axis is not None:\n+    used = {n for spec in out_specs for n in _spec_to_vma(spec)}\n+    if set(explicit_mesh_axis) & used:\n+      raise ValueError(\"vmapped away explicit mesh axis cannot appear in \"\n+                       \"shard_map out_specs\")\n+    return [sp if d is batching.not_mapped else\n+            pxla.batch_spec(sp, d, explicit_mesh_axis)\n+            for sp, d in zip(out_specs, dims)]\n   else:\n     return [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, None)\n             for sp, d in zip(out_specs, dims)]\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 5fbace3c98e1..90360989f13f 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -767,6 +767,46 @@ def f(x):\n     self.assertIn('out_specs', e.params)\n     self.assertEqual(e.params['out_specs'], (P('y', 'x'),))\n \n+  def test_vmap_explicit_mesh_axis(self):\n+    mesh = jtu.create_mesh(\n+        (1, 2, 2), ('z', 'x', 'y'), axis_types=(AxisType.Explicit,) * 3)\n+\n+    @shard_map(mesh=mesh, in_specs=P('y'), out_specs=P('y'))\n+    def f(x):\n+      return x\n+\n+    x = jnp.arange(4 * 4).reshape(4, 4)\n+    s = NamedSharding(mesh, P(('z', 'x'), 'y'))\n+    x = jax.device_put(x, s)\n+\n+    f = jax.jit(jax.vmap(f))\n+    out = f(x)\n+    self.assertEqual(out.sharding, s)\n+\n+  def test_vmap_explicit_mesh_axis_error(self):\n+    mesh = jtu.create_mesh((2, 2), ('x', 'y'),\n+                           axis_types=(AxisType.Explicit,) * 2)\n+\n+    @shard_map(mesh=mesh, in_specs=P('x'), out_specs=P('x'))\n+    def f(x):\n+      return x\n+\n+    x = jnp.arange(4 * 4).reshape(4, 4)\n+    s = NamedSharding(mesh, P('x', 'y'))\n+    x = jax.device_put(x, s)\n+\n+    f = jax.jit(jax.vmap(f))\n+    with self.assertRaisesRegex(\n+        ValueError, \"vmapped away explicit mesh axis cannot appear\"):\n+      f(x)\n+\n+    f = jax.jit(jax.vmap(f, spmd_axis_name='y'))\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'Only one of spmd_axis_name or arrays sharded on `Explicit` mesh axis'\n+        ' type is allowed'):\n+      f(x)\n+\n   def test_vmap_of_grad_spmd_axis_name(self):\n     mesh = jtu.create_mesh((2, 2), ('x', 'y'))\n \n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Avoid doing DCE of effectful ops and reordering in partial eval.",
        "output": [
            "```diff\nCommit: 859e120fdd79575da26da4ea561c4bb135492b3c\nDate: 2025-05-22T23:23:26Z\nURL: https://github.com/jax-ml/jax/commit/859e120fdd79575da26da4ea561c4bb135492b3c\nFiles changed: 12\nAdditions: +52, Deletions: -36\ndiff --git a/jax/_src/ad_checkpoint.py b/jax/_src/ad_checkpoint.py\nindex 2d743bf06c6b..2a056d5c94f0 100644\n--- a/jax/_src/ad_checkpoint.py\n+++ b/jax/_src/ad_checkpoint.py\n@@ -578,7 +578,7 @@ def remat_partial_eval(trace: pe.JaxprTrace, *tracers: core.Tracer,\n   out_jaxpr_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(x.aval), None)\n                        for x in jaxpr_unknown.outvars]\n   new_params = dict(params, jaxpr=jaxpr_unknown, differentiated=True)\n-  recipe = pe.new_eqn_recipe(in_jaxpr_tracers, out_jaxpr_tracers, remat_p,\n+  recipe = pe.new_eqn_recipe(trace, in_jaxpr_tracers, out_jaxpr_tracers, remat_p,\n                              new_params, jaxpr_unknown.effects,\n                              source_info_util.current())\n \ndiff --git a/jax/_src/debugging.py b/jax/_src/debugging.py\nindex e931a6edb9b3..3490de5118e1 100644\n--- a/jax/_src/debugging.py\n+++ b/jax/_src/debugging.py\n@@ -127,7 +127,7 @@ def debug_callback_jvp_rule(primals, tangents, **params):\n ad.primitive_jvps[debug_callback_p] = debug_callback_jvp_rule\n \n def debug_callback_transpose_rule(*flat_args, callback: Callable[..., Any],\n-    effect: DebugEffect):\n+                                  effect: DebugEffect, partitioned):\n   del flat_args, callback, effect\n   raise ValueError(\"Transpose doesn't support debugging callbacks.\")\n ad.primitive_transposes[debug_callback_p] = debug_callback_transpose_rule\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 9366b91f8022..7cbdfff01462 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -885,7 +885,7 @@ def make_zero(aval):\n     out_nz_tracers = [trace.to_jaxpr_tracer(r)\n                       for (r, nz) in zip(out_tangents, out_nzs) if nz]\n     in_tracers = [t for t, nz in zip(tangent_args, nonzeros) if nz]\n-    jaxpr, out_consts, _ = pe.tracers_to_jaxpr(in_tracers, out_nz_tracers, jvp.debug_info)\n+    jaxpr, out_consts, _ = pe.tracers_to_jaxpr(in_tracers, out_nz_tracers, [], jvp.debug_info)\n     jaxpr, used_consts, _ = pe.dce_jaxpr_consts(\n         jaxpr, [True] * len(jaxpr.outvars),\n         [False] * len(jaxpr.constvars) + [True] * len(jaxpr.invars))\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex f77db5443a86..6ea16ec8e8ba 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -16,6 +16,7 @@\n from collections import namedtuple\n from collections.abc import Callable, Sequence, Hashable\n import contextlib\n+from dataclasses import dataclass\n from functools import partial\n import itertools as it\n import operator as op\n@@ -42,7 +43,7 @@\n                            mapped_aval, unmapped_aval, DBIdx, InDBIdx, OutDBIdx,\n                            InputType, OutputType, get_referent, JaxprEqnContext)\n from jax._src.source_info_util import SourceInfo\n-from jax._src.state.types import AbstractRef, ReadEffect\n+from jax._src.state.types import AbstractRef, ReadEffect, RefEffect\n from jax._src.tree_util import (PyTreeDef, treedef_tuple, tree_flatten,\n                                 tree_structure, register_static)\n from jax._src.util import (unzip2, safe_zip, safe_map, toposort, split_list,\n@@ -147,6 +148,10 @@ def get_aval(self) -> AbstractValue:\n     else:\n       return self[0]\n \n+@dataclass(frozen=True)\n+class EffectHandle:\n+  parents : list[Tracer]\n+  recipe : JaxprEqnRecipe\n \n class JaxprTrace(Trace['JaxprTracer']):\n \n@@ -156,6 +161,8 @@ def __init__(self, parent_trace:Trace, name_stack: source_info_util.NameStack, t\n     self.tag = tag\n     self.parent_trace = parent_trace\n     self.requires_low = False\n+    self.effect_handles : list[EffectHandle] = []\n+    self.counter = it.count()\n \n   def to_jaxpr_tracer(self, x):\n     if isinstance(x, JaxprTracer) and x._trace.tag is self.tag:\n@@ -239,14 +246,19 @@ def default_process_primitive(self, primitive, tracers, params):\n     if primitive.multiple_results:\n       out_tracers = [JaxprTracer(self, PartialVal.unknown(aval), None)\n                      for aval in out_aval]\n-      eqn = new_eqn_recipe(tracers, out_tracers, primitive, params, effects,\n+      eqn = new_eqn_recipe(self, tracers, out_tracers, primitive, params, effects,\n                            source)\n+      if any(isinstance(e, RefEffect) for e in effects):\n+        self.effect_handles.append(EffectHandle(tracers, eqn))\n       for t in out_tracers: t.recipe = eqn\n       return out_tracers\n     else:\n       out_tracer = JaxprTracer(self, PartialVal.unknown(out_aval), None)\n-      out_tracer.recipe = new_eqn_recipe(tracers, [out_tracer], primitive,\n-                                         params, effects, source)\n+      eqn = new_eqn_recipe(self, tracers, [out_tracer], primitive,\n+                           params, effects, source)\n+      if any(isinstance(e, RefEffect) for e in effects):\n+        self.effect_handles.append(EffectHandle(tracers, eqn))\n+      out_tracer.recipe = eqn\n       return out_tracer\n \n   def process_call(self, primitive, f: lu.WrappedFun, tracers, params):\n@@ -321,7 +333,7 @@ def process_call(self, primitive, f: lu.WrappedFun, tracers, params):\n                      for a in out_type]\n     name_stack = self._current_truncated_name_stack()\n     source = source_info_util.current().replace(name_stack=name_stack)\n-    eqn = new_eqn_recipe((*res_tracers, *env_tracers, *unknown_arg_tracers),\n+    eqn = new_eqn_recipe(self, (*res_tracers, *env_tracers, *unknown_arg_tracers),\n                          out_tracers, primitive, staged_params, jaxpr.effects,\n                          source)\n     for t in out_tracers: t.recipe = eqn\n@@ -390,7 +402,7 @@ def const_out_axes_thunk():\n                    for a in out_avals]\n     effs = core.filter_named_axis_effects(jaxpr.effects, {params['axis_name']})\n     src_info = source_info_util.current()\n-    eqn = new_eqn_recipe((*const_tracers, *env_tracers, *unknown_arg_tracers),\n+    eqn = new_eqn_recipe(self, (*const_tracers, *env_tracers, *unknown_arg_tracers),\n                          out_tracers, primitive, staged_params, effs, src_info)\n     for t in out_tracers: t.recipe = eqn\n \n@@ -425,7 +437,7 @@ def process_custom_transpose(self, prim, call, tracers, **params):\n                      for aval in params['out_types']]\n       in_tracers = map(self.instantiate_const, tracers)\n       new_params = dict(params, call=call)\n-      eqn = new_eqn_recipe(in_tracers, out_tracers, prim, new_params,\n+      eqn = new_eqn_recipe(self, in_tracers, out_tracers, prim, new_params,\n           core.no_effects, source_info_util.current())\n       for t in out_tracers: t.recipe = eqn\n       return out_tracers\n@@ -470,7 +482,7 @@ def fwd_jaxpr_thunk(*zeros):\n         out_trees=out_trees,\n         symbolic_zeros=symbolic_zeros\n     )\n-    eqn = new_eqn_recipe((*res_tracers, *env_tracers, *tracers),\n+    eqn = new_eqn_recipe(self, (*res_tracers, *env_tracers, *tracers),\n                          out_tracers, prim, params, jaxpr.effects, source)\n     for t in out_tracers: t.recipe = eqn\n     return out_tracers\n@@ -657,7 +669,7 @@ def _trace_to_subjaxpr_nounits(f: Callable, trace: JaxprTrace,\n   out_tracers = [trace.instantiate_const(t) if inst else t\n                  for inst, t in zip(instantiate, out_tracers)]\n   out_tracers_ = [t for t in out_tracers if not t.is_known()]\n-  jaxpr, out_consts, env = tracers_to_jaxpr(in_tracers, out_tracers_, debug_info)\n+  jaxpr, out_consts, env = tracers_to_jaxpr(in_tracers, out_tracers_, trace.effect_handles, debug_info)\n   return out_tracers, jaxpr, out_consts, env\n \n # The below variant implements an optimization where residuals which are also\n@@ -739,7 +751,8 @@ class JaxprEqnRecipe(NamedTuple):\n   source_info: source_info_util.SourceInfo\n   ctx: JaxprEqnContext\n \n-def new_eqn_recipe(in_tracers: Sequence[JaxprTracer],\n+def new_eqn_recipe(trace: JaxprTrace,\n+                   in_tracers: Sequence[JaxprTracer],\n                    out_tracers: Sequence[JaxprTracer],\n                    primitive: Primitive,\n                    params: dict[str, Any],\n@@ -762,7 +775,7 @@ def new_eqn_recipe(in_tracers: Sequence[JaxprTracer],\n       config.threefry_partitionable.value,\n       xla_metadata_lib.current_xla_metadata(),\n   )\n-  return JaxprEqnRecipe(object(), tuple(in_tracers), map(ref, out_tracers),\n+  return JaxprEqnRecipe(next(trace.counter), tuple(in_tracers), map(ref, out_tracers),\n                         out_avals, primitive, params, effects, source_info,\n                         ctx)\n \n@@ -780,6 +793,7 @@ def recipe_to_eqn(getvar: Callable[[JaxprTracer], Atom],\n def tracers_to_jaxpr(\n   in_tracers: Sequence[JaxprTracer],\n   out_tracers: Sequence[JaxprTracer],\n+  effect_handles: Sequence[Any],\n   debug_info: core.DebugInfo,\n   ) -> tuple[Jaxpr, tuple[Any, ...], tuple[Any, ...]]:\n   \"\"\"Constructs Jaxpr given tracers for inputs and outputs.\n@@ -821,7 +835,15 @@ def type_substitute(aval: AbstractValue) -> AbstractValue:\n \n   processed_eqn_ids = set()\n   eqns: list[core.JaxprEqn] = []\n-  for t in toposort((*in_tracers, *out_tracers)):\n+\n+  reachable = toposort\n+  tracers = reachable((*in_tracers, *out_tracers, *effect_handles))\n+  def sort_key(t):\n+    r = t.recipe\n+    return r.eqn_id if isinstance(r, JaxprEqnRecipe) else -1\n+  tracers = sorted(tracers, key=sort_key)\n+\n+  for t in tracers:\n     r = t.recipe\n     if isinstance(r, JaxprEqnRecipe):\n       # TODO broadcast_in_dim can create a new tracer, not present in parents\ndiff --git a/jax/_src/lax/control_flow/conditionals.py b/jax/_src/lax/control_flow/conditionals.py\nindex 741636c47e31..4e8368341d9f 100644\n--- a/jax/_src/lax/control_flow/conditionals.py\n+++ b/jax/_src/lax/control_flow/conditionals.py\n@@ -617,7 +617,7 @@ def _cond_partial_eval(trace, *tracers, branches, **params):\n   name_stack = source_info_util.current_name_stack()[len(trace.name_stack):]\n   source = source_info_util.current().replace(name_stack=name_stack)\n   eqn = pe.new_eqn_recipe(\n-      [index_tracer] + res_tracers + ops_tracers, out_tracers, cond_p, params,\n+      trace, [index_tracer] + res_tracers + ops_tracers, out_tracers, cond_p, params,\n       core.join_effects(*(j.effects for j in branches_unknown)), source)\n   for t in out_tracers: t.recipe = eqn\n   return util.merge_lists(out_uks, out_consts, out_tracers)\ndiff --git a/jax/_src/lax/control_flow/for_loop.py b/jax/_src/lax/control_flow/for_loop.py\nindex fc7ebde4cbea..90b81ae367aa 100644\n--- a/jax/_src/lax/control_flow/for_loop.py\n+++ b/jax/_src/lax/control_flow/for_loop.py\n@@ -498,7 +498,7 @@ def _for_partial_eval(trace: pe.JaxprTrace, *tracers: pe.JaxprTracer,\n \n   assert len(unknown_inputs) == len(res_ref_unknown_outputs)\n   assert len(unknown_inputs) == len(jaxpr_unknown.invars) - 1\n-  eqn = pe.new_eqn_recipe(unknown_inputs, res_ref_unknown_outputs,\n+  eqn = pe.new_eqn_recipe(trace, unknown_inputs, res_ref_unknown_outputs,\n                           for_p, dict(jaxpr=jaxpr_unknown, nsteps=nsteps,\n                                       reverse=reverse,\n                                       which_linear=which_linear_unknown,\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 7efe3294fdca..83c31928d7cb 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -920,7 +920,7 @@ def _scan_partial_eval(trace, *tracers, reverse: bool,\n   name_stack = source_info_util.current_name_stack()[len(trace.name_stack):]\n   source = source_info_util.current().replace(name_stack=name_stack)\n   assert len(out_tracers) == len(jaxpr_unknown.out_avals)\n-  eqn = pe.new_eqn_recipe([*intensive_res, *unknown_inputs, *extensive_res],\n+  eqn = pe.new_eqn_recipe(trace, [*intensive_res, *unknown_inputs, *extensive_res],\n                           out_tracers, scan_p,\n                           dict(reverse=reverse, length=length, unroll=unroll,\n                                jaxpr=jaxpr_unknown, linear=linear_unknown,\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex a49c27d06eee..0e3695ba4506 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -6550,7 +6550,7 @@ def _broadcast_in_dim_partial_eval(\n   out_aval = core.DShapedArray(tuple(shape_), operand.dtype, operand.weak_type)\n   out_tracer = pe.JaxprTracer(trace, pe.PartialVal.unknown(out_aval), None)\n   eqn = pe.new_eqn_recipe(\n-      [operand_tracer, *dyn_shape_tracers], [out_tracer], broadcast_in_dim_p,\n+      trace, [operand_tracer, *dyn_shape_tracers], [out_tracer], broadcast_in_dim_p,\n       dict(shape=shape, broadcast_dimensions=broadcast_dimensions,\n            sharding=None),\n       core.no_effects, source_info_util.current())\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 0c55f3fe30ab..d5286be8e0c9 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -2324,18 +2324,8 @@ def _pjit_partial_eval(trace: pe.JaxprTrace,\n \n   known_ins = tuple(pv.is_known() for pv in in_pvals)\n   unknown_ins = tuple(not k for k in known_ins)\n-  if any(isinstance(e, (RefEffect, core.InternalMutableArrayEffect))\n-         for e in jaxpr.effects):\n-    known_jaxpr_, unknown_jaxpr_, unknown_outs, _, num_res_val, num_res_ref = \\\n-        pe.partial_eval_jaxpr_stateful(jaxpr.jaxpr, unknown_ins, unknown_ins,\n-                                       False, False, None)\n-    if num_res_ref: raise NotImplementedError\n-    known_jaxpr = pe.ClosedJaxpr(known_jaxpr_, jaxpr.consts)\n-    unknown_jaxpr = pe.ClosedJaxpr(unknown_jaxpr_, jaxpr.consts)\n-    res_avals = unknown_jaxpr.in_avals[:num_res_val]\n-  else:\n-    known_jaxpr, unknown_jaxpr, unknown_outs, res_avals = \\\n-        pe.partial_eval_jaxpr_nounits(jaxpr, unknown_ins, instantiate=False)\n+  known_jaxpr, unknown_jaxpr, unknown_outs, res_avals = \\\n+      pe.partial_eval_jaxpr_nounits(jaxpr, unknown_ins, instantiate=False)\n   unknown_outs = tuple(unknown_outs)  # type: ignore[assignment]\n   known_outs = tuple(not uk for uk in unknown_outs)\n   num_residuals = len(res_avals)\n@@ -2431,7 +2421,7 @@ def keep_where(l, should_keep):\n       pe.JaxprTracer(trace, pe.PartialVal.unknown(aval), None)\n       for aval in unknown_out_avals\n   ]\n-  eqn = pe.new_eqn_recipe((*unknown_tracers_in, *residual_tracers),\n+  eqn = pe.new_eqn_recipe(trace, (*unknown_tracers_in, *residual_tracers),\n                           unknown_tracers_out,\n                           pjit_p,\n                           unknown_params,\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 66df2505100c..4f60a833429a 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -1369,7 +1369,7 @@ def known_out_specs():\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(a), None)\n                  for a in out_avals]\n   effs = core.filter_named_axis_effects(jaxpr.effects, mesh.axis_names)\n-  eqn = pe.new_eqn_recipe((*const_tracers, *env_tracers, *unk_arg_tracers),\n+  eqn = pe.new_eqn_recipe(trace, (*const_tracers, *env_tracers, *unk_arg_tracers),\n                           out_tracers, shard_map_p, unk_params,\n                           effs, source_info_util.current())\n   for t in out_tracers: t.recipe = eqn\ndiff --git a/jax/_src/state/discharge.py b/jax/_src/state/discharge.py\nindex bc6a20a0a76e..100447f12d18 100644\n--- a/jax/_src/state/discharge.py\n+++ b/jax/_src/state/discharge.py\n@@ -828,7 +828,7 @@ def _run_state_partial_eval(trace: pe.JaxprTrace, *tracers: pe.JaxprTracer,\n                    is_initialized=(True,) * len(jaxpr_unknown.invars))\n   _, eqn_effects = run_state_p.abstract_eval(*[v.aval for v in unknown_inputs],\n                                              **uk_params)\n-  eqn = pe.new_eqn_recipe(unknown_inputs, res_ref_unknown_outputs,\n+  eqn = pe.new_eqn_recipe(trace, unknown_inputs, res_ref_unknown_outputs,\n                           run_state_p, uk_params,\n                           eqn_effects, source)\n   for t in res_ref_unknown_outputs: t.recipe = eqn\ndiff --git a/tests/mutable_array_test.py b/tests/mutable_array_test.py\nindex 865d4f8520f1..0da335e2fac5 100644\n--- a/tests/mutable_array_test.py\n+++ b/tests/mutable_array_test.py\n@@ -192,14 +192,18 @@ def f():\n     x = f()\n     self.assertArraysEqual(x, jnp.zeros(8))\n \n-  def test_grad_mutable_array(self):\n-    @jax.jit\n+  @parameterized.parameters([False, True])\n+  def test_grad_mutable_array(self, jit):\n+\n     def f(x):\n       x_ = core.mutable_array(x)\n       x_[()] = x_[()] + x_[()]\n       y = core.freeze(x_)\n       return y\n \n+    if jit:\n+      f = jax.jit(f)\n+\n     ans = jax.grad(f)(1.)\n     expected = 2.0\n     self.assertAllClose(ans, expected, check_dtypes=False)\n\n```"
        ],
        "from_id": [
            "dougalm"
        ]
    },
    {
        "text_input": "Use additional semaphores to avoid data races in TPU paged_attention_kernel.\n\nAlso prevents an out-of-bounds read of SMEM.  And re-enables tests for the TPU paged_attention_kernel.\n\n@apaszke confirmed the presence of data races using the race detector in the new TPU interpret mode.  With the additional semaphores, the race detector no longer detects any races in the this kernel and I no longer see any test failures in 20+ test runs on a TPU.\n\nDetails on the data races:\n\n - In each iteration, the kernel:\n   (a) Starts copying data for `k` and `v` for the next iteration.\n   (b) Waits for the copy of `k` for the current iteration to finish.\n   (c) Waits for the copy of `v` for the current iteration to finish.\n\n - It is possible for these copies to happen out of order -- that is:\n   (a) The copies for the next iteration can finish before the copies\n       for the current iteration.\n   (b) And the copies for `v` for the current iteration can finish\n       before the copies for `k` for the current iteration.\n\n - If the same DMA semaphore is used for everything, then out-of-order\n   copies can lead to:\n   (a) `k = async_copy_k.wait_and_get_loaded()` returns but the data\n       isn't all available because the underlying semaphore was\n       signaled by the completion of copies of `v` for the current\n       iteration or copies of `k` or `v` for the next iteration.\n   (a) `v = async_copy_v.wait_and_get_loaded()` returns but the data\n       isn't all available because the underlying semaphore was\n       signaled by the completion of copies of `k` or `v` for the\n       next iteration.\n\nPiperOrigin-RevId: 762136079",
        "output": [
            "```diff\nCommit: 0e690c1a88049787f8371d462d674a94f9b85c73\nDate: 2025-05-22T21:53:42Z\nURL: https://github.com/jax-ml/jax/commit/0e690c1a88049787f8371d462d674a94f9b85c73\nFiles changed: 2\nAdditions: +15, Deletions: -10\ndiff --git a/jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py b/jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py\nindex 6280064f29d3..9c02679c45ea 100644\n--- a/jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py\n+++ b/jax/experimental/pallas/ops/tpu/paged_attention/paged_attention_kernel.py\n@@ -127,7 +127,8 @@ def paged_flash_attention_kernel(\n     k_scales_vmem_buffer,\n     v_vmem_buffer,\n     v_scales_vmem_buffer,\n-    sem,\n+    k_sems,\n+    v_sems,\n     *,\n     batch_size: int,\n     pages_per_compute_block: int,\n@@ -176,7 +177,9 @@ def advance_to_next_non_zero_length():\n \n       return (\n           lax.cond(\n-              jnp.logical_and(next_b < batch_size, lengths_ref[next_b] == 0),\n+              jnp.logical_and(\n+                  next_b < batch_size,\n+                  lengths_ref[lax.clamp(0, next_b, batch_size - 1)] == 0),\n               advance_to_next_non_zero_length,\n               lambda: next_b,\n           ),\n@@ -200,7 +203,7 @@ def create_kv_async_copy_descriptors(b, h, i, buffer_index):\n         k_scales_vmem_buffer.at[buffer_index]\n         if k_scales_vmem_buffer is not None\n         else None,\n-        sem,\n+        k_sems.at[buffer_index],\n         page_indices_ref,\n         page_offset,\n         pages_to_load,\n@@ -213,7 +216,7 @@ def create_kv_async_copy_descriptors(b, h, i, buffer_index):\n         v_scales_vmem_buffer.at[buffer_index]\n         if v_scales_vmem_buffer is not None\n         else None,\n-        sem,\n+        v_sems.at[buffer_index],\n         page_indices_ref,\n         page_offset,\n         pages_to_load,\n@@ -301,7 +304,8 @@ def paged_flash_attention_kernel_inline_seq_dim(\n     k_scales_vmem_buffer,\n     v_vmem_buffer,\n     v_scales_vmem_buffer,\n-    sem,\n+    k_sems,\n+    v_sems,\n     *,\n     batch_size: int,\n     pages_per_compute_block: int,\n@@ -336,7 +340,8 @@ def body(i, _):\n         k_scales_vmem_buffer,\n         v_vmem_buffer,\n         v_scales_vmem_buffer,\n-        sem,\n+        k_sems,\n+        v_sems,\n         batch_size=batch_size,\n         pages_per_compute_block=pages_per_compute_block,\n         pages_per_sequence=pages_per_sequence,\n@@ -584,7 +589,8 @@ def paged_attention(\n             ),\n             v_scales_pages.dtype,  # pytype: disable=attribute-error\n         ),  # v_scales_pages buffer\n-        pltpu.SemaphoreType.DMA,\n+        pltpu.SemaphoreType.DMA((2,)),\n+        pltpu.SemaphoreType.DMA((2,)),\n     )\n   else:\n     in_specs = [\n@@ -615,7 +621,8 @@ def paged_attention(\n             v_pages.dtype,\n         ),  # v_pages buffer\n         None,\n-        pltpu.SemaphoreType.DMA,\n+        pltpu.SemaphoreType.DMA((2,)),\n+        pltpu.SemaphoreType.DMA((2,)),\n     )\n \n   out, _, _ = pl.pallas_call(\ndiff --git a/tests/pallas/tpu_paged_attention_kernel_test.py b/tests/pallas/tpu_paged_attention_kernel_test.py\nindex 9886e7943f6f..ac24fea1b45a 100644\n--- a/tests/pallas/tpu_paged_attention_kernel_test.py\n+++ b/tests/pallas/tpu_paged_attention_kernel_test.py\n@@ -265,8 +265,6 @@ def test_paged_attention(\n       attn_logits_soft_cap,\n       are_kv_quantized,\n   ):\n-    # TODO(mvoz, skyewm): Re-enable this test once the data race is fixed.\n-    self.skipTest(\"This kernel has data races that need to be fixed.\")\n     if not jtu.is_device_tpu_at_least(4):\n       self.skipTest(\"Only supports TPU generation 4 or above\")\n     if jtu.is_device_tpu(version=4) and are_kv_quantized:\n\n```"
        ],
        "from_id": [
            "jburnim",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Raises an explicit error in reshard.\n\nHit this while working with sharding in types -- passing a sharding that had an empty mesh. (I think this was in a test). This failed trying to acces with `with_spec` attribute on None -- so just catching this case early.\n\nPiperOrigin-RevId: 762135310",
        "output": [
            "```diff\nCommit: fc683368fa457bbffd3a693fc80ce880c1796e18\nDate: 2025-05-22T21:51:14Z\nURL: https://github.com/jax-ml/jax/commit/fc683368fa457bbffd3a693fc80ce880c1796e18\nFiles changed: 2\nAdditions: +15, Deletions: -0\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex ecdcf3e17332..0c55f3fe30ab 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -2987,6 +2987,11 @@ def reshard(xs, out_shardings):\n   out_flat = []\n   for x, x_aval, s in safe_zip(x_flat, x_avals_flat, shardings_flat):\n     ds = canonicalize_sharding(s, 'reshard')\n+    if ds is None:\n+      raise ValueError(\n+          'Reshard should only be used with out_shardings which are non-None '\n+          'and have a nonempty mesh. Got sharding {s}.'\n+      )\n     ds = ds.with_spec(ds.spec._normalized_spec_for_aval(x_aval.ndim))  # pytype: disable=attribute-error\n     out_flat.append(reshard_p.bind(x, dst_sharding=ds))\n   return tree_unflatten(treedef, out_flat)\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 92190dc6bf3c..d37b21bd2460 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -8781,6 +8781,16 @@ def f(x):\n                                 \"to Shardy\"):\n       jax.jit(f)(x)\n \n+  def test_reshard_empty_mesh_error(self):\n+    arr = jax.device_put(np.arange(8), jax.devices()[0])\n+    with self.assertRaisesRegex(ValueError, \"nonempty mesh\"):\n+      reshard(arr, NamedSharding(mesh_lib.empty_abstract_mesh, P(None)))\n+\n+  def test_reshard_none_sharding_error(self):\n+    arr = jax.device_put(np.arange(8), jax.devices()[0])\n+    with self.assertRaisesRegex(ValueError, \"non-None\"):\n+      reshard(arr, None)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "jkr26",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add CUDA pytest job strategy to test CUDA packages downloaded from PyPI\n\nContinuous and Nightly/Release workflows will now run the CUDA 12.8 test runs by using the Nvidia CUDA packages from PyPI instead of those on the system.\n\nPiperOrigin-RevId: 762125356",
        "output": [
            "```diff\nCommit: 0e24e98032fef09e31a8e9219967aa92cf370b86\nDate: 2025-05-22T21:26:09Z\nURL: https://github.com/jax-ml/jax/commit/0e24e98032fef09e31a8e9219967aa92cf370b86\nFiles changed: 7\nAdditions: +56, Deletions: -31\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex a20be5b1dbcf..2f22901e661a 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -24,11 +24,16 @@ on:\n         type: string\n         required: true\n         default: \"3.12\"\n-      cuda:\n+      cuda-version:\n         description: \"Which CUDA version to test?\"\n         type: string\n         required: true\n-        default: \"12.3\"\n+        default: \"12.8\"\n+      use-nvidia-pip-wheels:\n+        description: \"Whether to download CUDA packages from PyPI?\"\n+        type: boolean\n+        required: false\n+        default: false\n       enable-x64:\n         description: \"Should x64 mode be enabled?\"\n         type: string\n@@ -58,8 +63,11 @@ jobs:\n         shell: bash\n     runs-on: ${{ inputs.runner }}\n     # Test the oldest and newest supported CUDA versions.\n-    container:  ${{ (contains(inputs.cuda, '12.1') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.1-cudnn9.8:latest') ||\n-                (contains(inputs.cuda, '12.8') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest') }}\n+    # If testing the CUDA packages from PyPI, then use the ml-build image which does not have any\n+    # CUDA pckages installed on the system.\n+    container:  ${{ !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.1') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.1-cudnn9.8:latest') ||\n+                !inputs.use-nvidia-pip-wheels && (contains(inputs.cuda-version, '12.8') && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build-cuda12.8-cudnn9.8:latest') ||\n+                inputs.use-nvidia-pip-wheels && 'us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest'}}\n     name: \"Pytest CUDA (${{ inputs.runner }}, CUDA ${{ inputs.cuda }}, Python ${{ inputs.python }}, x64=${{ inputs.enable-x64 }})\"\n \n     env:\n@@ -100,13 +108,24 @@ jobs:\n           if [[ \"${{ inputs.download-jax-only-from-gcs }}\" == \"1\" ]]; then\n             echo \"JAX only release. Only downloading the jax wheel from the release bucket.\"\n \n-            # Set the env var to install the CUDA plugin and PJRT packages from PyPI. jaxlib is\n-            # required dependency of jax so that gets installed automatically.\n-            echo \"JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI=jax_cuda_pypi\">> $GITHUB_ENV\n+            if [[ \"${{ inputs.use-nvidia-pip-wheels }}\" == false ]]; then\n+              # Install only the PJRT and JAX CUDA Plugin packages from PyPI. Nvidia CUDA packages\n+              # are used from the system.\n+              echo \"JAXCI_JAX_PYPI_EXTRAS=cuda12-local\">> $GITHUB_ENV\n+            else\n+             # Install the PJRT, JAX CUDA Plugin, and Nvidia CUDA packages from PyPI.\n+              echo \"JAXCI_JAX_PYPI_EXTRAS=cuda12\">> $GITHUB_ENV\n+            fi\n           else\n             gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n             gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*plugin*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n             gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*pjrt*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n+\n+             if [[ \"${{ inputs.use-nvidia-pip-wheels }}\" == true ]]; then\n+              # Install the Nvidia CUDA packages from PyPI. The wheels downloaded in the previous\n+              # step will be used for the PJRT and JAX CUDA Plugin packages.\n+              echo \"JAXCI_JAX_PYPI_EXTRAS=cuda12\">> $GITHUB_ENV\n+             fi\n           fi\n       - name: Skip the test run if the wheel artifacts were not downloaded successfully\n         if: steps.download-wheel-artifacts.outcome == 'failure'\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex d1af90283001..ae0250884831 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -137,9 +137,9 @@ jobs:\n             $JAXCI_PYTHON -m uv pip install --pre libtpu -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n           elif [[ \"${{ inputs.libtpu-version-type }}\" == \"pypi_latest\" ]]; then\n             echo \"Using latest libtpu from PyPI\"\n-            # Set JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI to \"tpu_pypi\". The `run_pytest_tpu.sh`\n-            # script will install the latest libtpu wheel from PyPI.\n-            echo \"JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI=tpu_pypi\" >> $GITHUB_ENV\n+            # Set JAXCI_JAX_PYPI_EXTRAS to \"tpu\". The `run_pytest_tpu.sh` script will install the\n+            # latest libtpu wheel from PyPI.\n+            echo \"JAXCI_JAX_PYPI_EXTRAS=tpu\" >> $GITHUB_ENV\n           elif [[ \"${{ inputs.libtpu-version-type }}\" == \"oldest_supported_libtpu\" ]]; then\n             echo \"Using oldest supported libtpu\"\n             $JAXCI_PYTHON -m uv pip install --pre libtpu-nightly==0.1.dev${{ env.LIBTPU_OLDEST_VERSION_DATE }} \\\ndiff --git a/.github/workflows/wheel_tests_continuous.yml b/.github/workflows/wheel_tests_continuous.yml\nindex 207075fd0340..91662ff51f3e 100644\n--- a/.github/workflows/wheel_tests_continuous.yml\n+++ b/.github/workflows/wheel_tests_continuous.yml\n@@ -117,25 +117,31 @@ jobs:\n           # See exlusions for what is fully tested\n           runner: [\"linux-x86-g2-48-l4-4gpu\", \"linux-x86-a3-8g-h100-8gpu\", \"linux-x86-a4-224-b200-1gpu\"]\n           python: [\"3.10\",]\n-          cuda: [\"12.1\", \"12.8\"]\n+          cuda: [\n+            {version: \"12.1\", use-nvidia-pip-wheels: false},\n+            {version: \"12.8\", use-nvidia-pip-wheels: true},\n+            ]\n           enable-x64: [1, 0]\n           exclude:\n             # H100 runs only a single config, CUDA 12.8 Enable x64 1\n             - runner: \"linux-x86-a3-8g-h100-8gpu\"\n-              cuda: \"12.1\"\n+              cuda:\n+                version: \"12.1\"\n             - runner: \"linux-x86-a3-8g-h100-8gpu\"\n               enable-x64: \"0\"\n             # B200 runs only a single config, CUDA 12.8 Enable x64 1\n             - runner: \"linux-x86-a4-224-b200-1gpu\"\n-              cuda: \"12.1\"\n+              cuda:\n+                version: \"12.1\"\n             - runner: \"linux-x86-a4-224-b200-1gpu\"\n               enable-x64: \"0\"\n \n-    name: \"Pytest CUDA (JAX artifacts version =  ${{ format('{0}', 'head') }})\"\n+    name: \"Pytest CUDA (JAX artifacts version =  ${{ format('{0}', 'head') }}, CUDA Pip packages = ${{ matrix.cuda.use-nvidia-pip-wheels }})\"\n     with:\n       runner: ${{ matrix.runner }}\n       python: ${{ matrix.python }}\n-      cuda:  ${{ matrix.cuda }}\n+      cuda-version:  ${{ matrix.cuda.version }}\n+      use-nvidia-pip-wheels: ${{ matrix.cuda.use-nvidia-pip-wheels }}\n       enable-x64:  ${{ matrix.enable-x64 }}\n       # GCS upload URI is the same for both artifact build jobs\n       gcs_download_uri: ${{ needs.build-jaxlib-artifact.outputs.gcs_upload_uri }}\ndiff --git a/.github/workflows/wheel_tests_nightly_release.yml b/.github/workflows/wheel_tests_nightly_release.yml\nindex 3e616a894d13..7bad41647e6b 100644\n--- a/.github/workflows/wheel_tests_nightly_release.yml\n+++ b/.github/workflows/wheel_tests_nightly_release.yml\n@@ -66,13 +66,17 @@ jobs:\n           # that build the wheels.\n           runner: [\"linux-x86-g2-48-l4-4gpu\"]\n           python: [\"3.10\",\"3.11\", \"3.12\", \"3.13\", \"3.13-nogil\"]\n-          cuda: [\"12.1\", \"12.8\"]\n+          cuda: [\n+            {cuda-version: \"12.1\", use-nvidia-pip-wheels: false},\n+            {cuda-version: \"12.8\", use-nvidia-pip-wheels: true}\n+          ]\n           enable-x64: [0]\n-    name: \"Pytest CUDA (JAX artifacts version = ${{ startsWith(github.ref_name, 'release/') && 'latest release' || 'nightly' }})\"\n+    name: \"Pytest CUDA (JAX artifacts version = ${{ startsWith(github.ref_name, 'release/') && 'latest release' || 'nightly' }}, CUDA Pip packages = ${{ matrix.cuda.use-nvidia-pip-wheels }})\"\n     with:\n       runner: ${{ matrix.runner }}\n       python: ${{ matrix.python }}\n-      cuda:  ${{ matrix.cuda }}\n+      cuda-version:  ${{ matrix.cuda.cuda-version }}\n+      use-nvidia-pip-wheels: ${{ matrix.cuda.use-nvidia-pip-wheels }}\n       enable-x64:  ${{ matrix.enable-x64 }}\n       download-jax-only-from-gcs: ${{inputs.download-jax-only-from-gcs}}\n       gcs_download_uri: ${{inputs.gcs_download_uri}}\ndiff --git a/ci/envs/README.md b/ci/envs/README.md\nindex 6b5dc554d824..cf7a0c12fc9f 100644\n--- a/ci/envs/README.md\n+++ b/ci/envs/README.md\n@@ -21,7 +21,7 @@ Name                                        | Default Value\n `JAXCI_ENABLE_X64`                          | 0                                        | By default, JAX enforces single-precision numbers to mitigate the Numpy API’s tendency to aggressively promote operands to `double`. When set to 1, the tests will use double-precision numbers.                                                                                                                                                             | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_ENABLE_X64&type=code)\n `JAXCI_TPU_CORES`                           | Unset                                    | Sets the number of TPU cores for the TPU machine type. Values are set in the workflow files.                                                                                                                                                                                                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_TPU_CORES&type=code)\n `JAXCI_RUN_FULL_TPU_TEST_SUITE`             | 0                                        | When set to 1, the full TPU test suite is run. Otherwise, a subset of tests is run.                                                                                                                                                                                                                                                                          | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_RUN_FULL_TPU_TEST_SUITE&type=code)\n-`JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI` | Unset                                    | Used to control the installation of JAX [extras](https://github.com/jax-ml/jax/blob/7e42539653d33ec995487b683794c0bc86f7199b/setup.py#L64) from PyPI. See [ci/utilities/install_wheels_locally.sh](https://github.com/jax-ml/jax/blob/main/ci/utilities/install_wheels_locally.sh) for the list of valid values and their behavior.                                                                                                                    | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI&type=code)\n+`JAXCI_JAX_PYPI_EXTRAS` | Unset                                    | Used to control the installation of JAX extras from PyPI. See JAX's [setup.py](https://github.com/jax-ml/jax/blob/c9934912885bb7c4b72c5a9271598235a6789a81/setup.py#L71) for the list of valid values.                                                                                                                 | [Usage](https://github.com/search?q=repo%3Ajax-ml%2Fjax%20JAXCI_JAX_PYPI_EXTRAS&type=code)\n \n ## Docker Specific Environment Variables\n \ndiff --git a/ci/envs/default.env b/ci/envs/default.env\nindex 774464724646..09594af89cbe 100644\n--- a/ci/envs/default.env\n+++ b/ci/envs/default.env\n@@ -58,7 +58,7 @@ export JAXCI_ENABLE_X64=${JAXCI_ENABLE_X64:-0}\n # Sets the number of TPU cores for the TPU machine type.\n export JAXCI_TPU_CORES=${JAXCI_TPU_CORES:-}\n \n-# JAXCI_PYTHON points to the Python binary on the system that should be used \n+# JAXCI_PYTHON points to the Python binary on the system that should be used\n # for installing the JAX wheels on the system and running Pytest scripts.\n export JAXCI_PYTHON=${JAXCI_PYTHON:-python${JAXCI_HERMETIC_PYTHON_VERSION}}\n \n@@ -66,5 +66,5 @@ export JAXCI_PYTHON=${JAXCI_PYTHON:-python${JAXCI_HERMETIC_PYTHON_VERSION}}\n # is run.\n export JAXCI_RUN_FULL_TPU_TEST_SUITE=${JAXCI_RUN_FULL_TPU_TEST_SUITE:-0}\n \n-# Controls which additional extras to install from PyPI.\n-export JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI=${JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI:-\"\"}\n\\ No newline at end of file\n+# Controls which additional extras for JAX to install from PyPI.\n+export JAXCI_JAX_PYPI_EXTRAS=${JAXCI_JAX_PYPI_EXTRAS:-\"\"}\n\\ No newline at end of file\ndiff --git a/ci/utilities/install_wheels_locally.sh b/ci/utilities/install_wheels_locally.sh\nindex 53f070d1e0e6..b1472d765c08 100644\n--- a/ci/utilities/install_wheels_locally.sh\n+++ b/ci/utilities/install_wheels_locally.sh\n@@ -22,15 +22,11 @@ WHEELS=( $(/usr/bin/find \"$JAXCI_OUTPUT_DIR/\" -type f \\(  -name \"*jax*py3*\" -o -\n \n for i in \"${!WHEELS[@]}\"; do\n   if [[ \"${WHEELS[$i]}\" == *jax*py3*none*any.whl ]]; then\n-    if [[ \"$JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI\" == \"tpu_pypi\" ]]; then\n-      # Append [tpu] to the jax wheel name to download the latest libtpu wheel\n-      # from PyPI.\n-      WHEELS[$i]=\"${WHEELS[$i]}[tpu]\"\n-    elif [[ \"$JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI\" == \"jax_cuda_pypi\" ]]; then\n-      # Append [cuda12-local] to the jax wheel name to download the latest\n-      # release of JAX's CUDA plugin and PJRT packages from PyPI. This is used\n-      # when running CUDA tests for a \"jax\" only release.\n-      WHEELS[$i]=\"${WHEELS[$i]}[cuda12-local]\"\n+    # Apppend an extra to the end of the JAX wheel path to install those\n+    # packages as well from PyPI. E.g. jax[tpu] will install the libtpu package\n+    # from PyPI. See ci/envs/README.md for more details.\n+    if [[ -n \"$JAXCI_JAX_PYPI_EXTRAS\" ]]; then\n+      WHEELS[$i]=\"${WHEELS[$i]}[$JAXCI_JAX_PYPI_EXTRAS]\"\n     fi\n   fi\n done\n\n```"
        ],
        "from_id": [
            "nitins17",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28899 from jenriver:absl_logging_fix\n\nPiperOrigin-RevId: 762099728",
        "output": [
            "```diff\nCommit: e71d5d5051e511bc09e1d5a08d30c298420aa169\nDate: 2025-05-22T20:23:06Z\nURL: https://github.com/jax-ml/jax/commit/e71d5d5051e511bc09e1d5a08d30c298420aa169\nFiles changed: 3\nAdditions: +16, Deletions: -18\ndiff --git a/docs/export/export.md b/docs/export/export.md\nindex 63c0db14f905..95e47385997c 100644\n--- a/docs/export/export.md\n+++ b/docs/export/export.md\n@@ -710,10 +710,7 @@ total 32\n -rw-rw-r--@ 1 necula  wheel  2333 Jun 19 11:04 jax_ir3_jit_my_fun_export.mlir\n ```\n \n-Inside Google, you can turn on logging by using the `--vmodule` argument to\n-specify the logging levels for different modules,\n-e.g., `--vmodule=_export=3`.\n-\n+Set [`JAX_DEBUG_LOG_MODULES=jax._src.export`](https://docs.jax.dev/en/latest/config_options.html#jax_debug_log_modules) to enable extra debugging logging.\n \n (export_ensuring_compat)=\n ### Ensuring forward and backward compatibility\ndiff --git a/jax/_src/compiler.py b/jax/_src/compiler.py\nindex e1b8e7c35697..343f747efbd7 100644\n--- a/jax/_src/compiler.py\n+++ b/jax/_src/compiler.py\n@@ -241,7 +241,7 @@ def get_compile_options(\n   else:\n     compile_options.profile_version = _NO_PROFILE_DONT_RETRIEVE\n     if backend is None:\n-      logging.info(\"get_compile_options: no backend supplied; \"\n+      logger.info(\"get_compile_options: no backend supplied; \"\n                    \"disabling XLA-AutoFDO profile\")\n     else:\n       fdo_profile_version = get_latest_profile_version(backend)\n@@ -369,7 +369,7 @@ def compile_or_get_cached(\n   module_name = ir.StringAttr(sym_name).value\n \n   if dumped_to := mlir.dump_module_to_file(computation, \"compile\"):\n-    logging.info(\"Dumped the module to %s.\", dumped_to)\n+    logger.info(\"Dumped the module to %s.\", dumped_to)\n \n   is_multi_process = (\n       len({device.process_index for device in devices.flatten()}) > 1\n@@ -514,7 +514,7 @@ def _resolve_compilation_strategy(\n     # The compilation cache is enabled and AutoPGLE is enabled/expected\n     if _is_executable_in_cache(backend, pgle_optimized_cache_key):\n       if config.compilation_cache_expect_pgle.value:\n-        logging.info(f\"PGLE-optimized {module_name} loaded from compilation cache\")\n+        logger.info(f\"PGLE-optimized {module_name} loaded from compilation cache\")\n       # No need to record N profiles in this case\n       if pgle_profiler is not None:\n         pgle_profiler.disable()\ndiff --git a/jax/_src/export/_export.py b/jax/_src/export/_export.py\nindex c0ca1e108590..189818541a2c 100644\n--- a/jax/_src/export/_export.py\n+++ b/jax/_src/export/_export.py\n@@ -25,7 +25,7 @@\n import re\n from typing import Any, Protocol, TypeVar, Union, cast\n \n-from absl import logging\n+import logging\n import numpy as np\n \n import jax\n@@ -55,6 +55,8 @@\n \n from jax._src.export import shape_poly\n \n+logger = logging.getLogger(__name__)\n+\n map = util.safe_map\n zip = util.safe_zip\n \n@@ -704,16 +706,15 @@ def _export_lowered(\n     out_avals_flat = lowered.compile_args[\"out_avals\"]  # type: ignore\n \n   # Log and then check the module.\n-  if logging.vlog_is_on(3):\n-    logmsg = (f\"fun_name={fun_name} version={version} \"\n-              f\"lowering_platforms={lowering._platforms} \"  # type: ignore[unused-ignore,attribute-error]\n-              f\"disabled_checks={disabled_checks}\")\n-    logging.info(\"Exported JAX function: %s\\n\", logmsg)\n-    logging.info(mlir.dump_module_message(mlir_module, \"export\"))\n-    logging.info(\n-        \"Size of mlir_module_serialized: %d byte\",\n-        len(mlir_module_serialized),\n-    )\n+  logmsg = (f\"fun_name={fun_name} version={version} \"\n+            f\"lowering_platforms={lowering._platforms} \"  # type: ignore[unused-ignore,attribute-error]\n+            f\"disabled_checks={disabled_checks}\")\n+  logger.debug(\"Exported JAX function: %s\\n\", logmsg)\n+  logger.debug(mlir.dump_module_message(mlir_module, \"export\"))\n+  logger.debug(\n+      \"Size of mlir_module_serialized: %d byte\",\n+      len(mlir_module_serialized),\n+  )\n \n   _check_module(mlir_module,\n                 disabled_checks=disabled_checks,\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28946 from jakevdp:random-mode\n\nPiperOrigin-RevId: 762092464",
        "output": [
            "```diff\nCommit: fdf6e1fac02824109994cdb455c1037979d062e5\nDate: 2025-05-22T20:05:44Z\nURL: https://github.com/jax-ml/jax/commit/fdf6e1fac02824109994cdb455c1037979d062e5\nFiles changed: 2\nAdditions: +32, Deletions: -10\ndiff --git a/jax/_src/random.py b/jax/_src/random.py\nindex 6f139dd9665c..60dad3a82021 100644\n--- a/jax/_src/random.py\n+++ b/jax/_src/random.py\n@@ -633,7 +633,8 @@ def choice(key: ArrayLike,\n            shape: Shape = (),\n            replace: bool = True,\n            p: RealArray | None = None,\n-           axis: int = 0) -> Array:\n+           axis: int = 0,\n+           mode: str | None = None) -> Array:\n   \"\"\"Generates a random sample from a given array.\n \n   .. warning::\n@@ -656,6 +657,12 @@ def choice(key: ArrayLike,\n       entries in a.\n     axis: int, optional. The axis along which the selection is performed.\n       The default, 0, selects by row.\n+    mode: optional, \"high\" or \"low\" for how many bits to use in the gumbel sampler\n+      when `p is None` and `replace = False`. The default is determined by the\n+      ``use_high_dynamic_range_gumbel`` config, which defaults to \"low\". With mode=\"low\",\n+      in float32 sampling will be biased for choices with probability less than about\n+      1E-7; with mode=\"high\" this limit is pushed down to about 1E-14. mode=\"high\"\n+      approximately doubles the cost of sampling.\n \n   Returns:\n     An array of shape `shape` containing samples from `a`.\n@@ -701,7 +708,7 @@ def choice(key: ArrayLike,\n       ind = jnp.searchsorted(p_cuml, r).astype(int)\n     else:\n       # Gumbel top-k trick: https://timvieira.github.io/blog/post/2019/09/16/algorithms-for-sampling-without-replacement/\n-      g = gumbel(key, (n_inputs,), dtype=p_arr.dtype) + jnp.log(p_arr)\n+      g = gumbel(key, (n_inputs,), dtype=p_arr.dtype, mode=mode) + jnp.log(p_arr)\n       ind = lax.top_k(g, k=n_draws)[1].astype(int)\n     result = ind if arr.ndim == 0 else jnp.take(arr, ind, axis)\n \n@@ -940,7 +947,8 @@ def bernoulli(key: ArrayLike,\n     mode: optional, \"high\" or \"low\" for how many bits to use when sampling.\n       default='low'. Set to \"high\" for correct sampling at small values of\n       `p`. When sampling in float32, bernoulli samples with mode='low' produce\n-      incorrect results for p < ~1E-7.\n+      incorrect results for p < ~1E-7. mode=\"high\" approximately doubles the\n+      cost of sampling.\n \n   Returns:\n     A random array with boolean dtype and shape given by ``shape`` if ``shape``\n@@ -1544,7 +1552,7 @@ def poisson(key: ArrayLike,\n def gumbel(key: ArrayLike,\n            shape: Shape = (),\n            dtype: DTypeLikeFloat = float,\n-           mode: str | None =None) -> Array:\n+           mode: str | None = None) -> Array:\n   \"\"\"Sample Gumbel random values with given shape and float dtype.\n \n   The values are distributed according to the probability density function:\n@@ -1559,6 +1567,11 @@ def gumbel(key: ArrayLike,\n     dtype: optional, a float dtype for the returned values (default float64 if\n       jax_enable_x64 is true, otherwise float32).\n     mode: optional, \"high\" or \"low\" for how many bits to use when sampling.\n+      The default is determined by the ``use_high_dynamic_range_gumbel`` config,\n+      which defaults to \"low\". When drawing float32 samples, with mode=\"low\" the\n+      uniform resolution is such that the largest possible gumbel logit is ~16;\n+      with mode=\"high\" this is increased to ~32, at approximately double the\n+      computational cost.\n \n   Returns:\n     A random array with the specified shape and dtype.\n@@ -1599,6 +1612,7 @@ def categorical(\n   axis: int = -1,\n   shape: Shape | None = None,\n   replace: bool = True,\n+  mode: str | None = None,\n ) -> Array:\n   \"\"\"Sample random values from categorical distributions.\n \n@@ -1615,6 +1629,12 @@ def categorical(\n       The default (None) produces a result shape equal to ``np.delete(logits.shape, axis)``.\n     replace: If True (default), perform sampling with replacement. If False, perform\n       sampling without replacement.\n+    mode: optional, \"high\" or \"low\" for how many bits to use in the gumbel sampler.\n+      The default is determined by the ``use_high_dynamic_range_gumbel`` config,\n+      which defaults to \"low\". With mode=\"low\", in float32 sampling will be biased\n+      for events with probability less than about 1E-7; with mode=\"high\" this limit\n+      is pushed down to about 1E-14. mode=\"high\" approximately doubles the cost of\n+      sampling.\n \n   Returns:\n     A random array with int dtype and shape given by ``shape`` if ``shape``\n@@ -1644,11 +1664,11 @@ def categorical(\n     logits_shape = list(shape[len(shape) - len(batch_shape):])\n     logits_shape.insert(axis % len(logits_arr.shape), logits_arr.shape[axis])\n     return jnp.argmax(\n-        gumbel(key, (*shape_prefix, *logits_shape), logits_arr.dtype) +\n+        gumbel(key, (*shape_prefix, *logits_shape), logits_arr.dtype, mode=mode) +\n         lax.expand_dims(logits_arr, tuple(range(len(shape_prefix)))),\n         axis=axis)\n   else:\n-    logits_arr += gumbel(key, logits_arr.shape, logits_arr.dtype)\n+    logits_arr += gumbel(key, logits_arr.shape, logits_arr.dtype, mode=mode)\n     k = math.prod(shape_prefix)\n     if k > logits_arr.shape[axis]:\n       raise ValueError(\ndiff --git a/tests/random_lax_test.py b/tests/random_lax_test.py\nindex f87b079b759c..9fe4d2ecbda3 100644\n--- a/tests/random_lax_test.py\n+++ b/tests/random_lax_test.py\n@@ -286,8 +286,9 @@ def testTruncatedNormal(self, dtype):\n     ],\n     dtype=jtu.dtypes.floating + jtu.dtypes.integer,\n     weighted=[True, False],\n+    mode=[None, 'low', 'high']\n   )\n-  def testChoice(self, dtype, input_range_or_shape, shape, replace, weighted, axis):\n+  def testChoice(self, dtype, input_range_or_shape, shape, replace, weighted, axis, mode):\n     # This is the function API that we test against (note that self.rng().choice differs)\n     np_choice = np.random.default_rng(0).choice\n     p_dtype = dtypes.to_inexact_dtype(dtype)\n@@ -303,7 +304,7 @@ def testChoice(self, dtype, input_range_or_shape, shape, replace, weighted, axis\n       p /= p.sum()\n     else:\n       p = None\n-    rand = lambda key, x: random.choice(key, x, shape, replace, p, axis)\n+    rand = lambda key, x: random.choice(key, x, shape, replace, p, axis, mode=mode)\n     sample = rand(key(), x)\n     if not is_range:\n       self.assertEqual(dtype, sample.dtype)\n@@ -397,15 +398,16 @@ def testBernoulli(self, p, dtype, mode):\n       ]\n     ],\n     sample_shape=[(10000,), (5000, 2)],\n+    mode=[None, 'low', 'high'],\n     dtype=jtu.dtypes.floating,\n   )\n-  def testCategorical(self, p, axis, dtype, sample_shape):\n+  def testCategorical(self, p, axis, dtype, sample_shape, mode):\n     key = lambda: self.make_key(0)\n     p = np.array(p, dtype=dtype)\n     logits = np.log(p) - 42 # test unnormalized\n     out_shape = tuple(np.delete(logits.shape, axis))\n     shape = sample_shape + out_shape\n-    rand = partial(random.categorical, shape=shape, axis=axis)\n+    rand = partial(random.categorical, shape=shape, axis=axis, mode=mode)\n     crand = jax.jit(rand)\n \n     uncompiled_samples = rand(key(), logits)\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Use the default python logging instead of absl log.\n\nUpdate use cases in export / compiler as well as the documentation.",
        "output": [
            "```diff\nCommit: 7cf4f35442743b01f4685ca906d282586428b3d0\nDate: 2025-05-22T19:06:57Z\nURL: https://github.com/jax-ml/jax/commit/7cf4f35442743b01f4685ca906d282586428b3d0\nFiles changed: 3\nAdditions: +16, Deletions: -18\ndiff --git a/docs/export/export.md b/docs/export/export.md\nindex 63c0db14f905..95e47385997c 100644\n--- a/docs/export/export.md\n+++ b/docs/export/export.md\n@@ -710,10 +710,7 @@ total 32\n -rw-rw-r--@ 1 necula  wheel  2333 Jun 19 11:04 jax_ir3_jit_my_fun_export.mlir\n ```\n \n-Inside Google, you can turn on logging by using the `--vmodule` argument to\n-specify the logging levels for different modules,\n-e.g., `--vmodule=_export=3`.\n-\n+Set [`JAX_DEBUG_LOG_MODULES=jax._src.export`](https://docs.jax.dev/en/latest/config_options.html#jax_debug_log_modules) to enable extra debugging logging.\n \n (export_ensuring_compat)=\n ### Ensuring forward and backward compatibility\ndiff --git a/jax/_src/compiler.py b/jax/_src/compiler.py\nindex 04f993fed799..cbde6fdb3366 100644\n--- a/jax/_src/compiler.py\n+++ b/jax/_src/compiler.py\n@@ -242,7 +242,7 @@ def get_compile_options(\n   else:\n     compile_options.profile_version = _NO_PROFILE_DONT_RETRIEVE\n     if backend is None:\n-      logging.info(\"get_compile_options: no backend supplied; \"\n+      logger.info(\"get_compile_options: no backend supplied; \"\n                    \"disabling XLA-AutoFDO profile\")\n     else:\n       fdo_profile_version = get_latest_profile_version(backend)\n@@ -376,7 +376,7 @@ def compile_or_get_cached(\n   module_name = ir.StringAttr(sym_name).value\n \n   if dumped_to := mlir.dump_module_to_file(computation, \"compile\"):\n-    logging.info(\"Dumped the module to %s.\", dumped_to)\n+    logger.info(\"Dumped the module to %s.\", dumped_to)\n \n   is_multi_process = (\n       len({device.process_index for device in devices.flatten()}) > 1\n@@ -521,7 +521,7 @@ def _resolve_compilation_strategy(\n     # The compilation cache is enabled and AutoPGLE is enabled/expected\n     if _is_executable_in_cache(backend, pgle_optimized_cache_key):\n       if config.compilation_cache_expect_pgle.value:\n-        logging.info(f\"PGLE-optimized {module_name} loaded from compilation cache\")\n+        logger.info(f\"PGLE-optimized {module_name} loaded from compilation cache\")\n       # No need to record N profiles in this case\n       if pgle_profiler is not None:\n         pgle_profiler.disable()\ndiff --git a/jax/_src/export/_export.py b/jax/_src/export/_export.py\nindex c0ca1e108590..189818541a2c 100644\n--- a/jax/_src/export/_export.py\n+++ b/jax/_src/export/_export.py\n@@ -25,7 +25,7 @@\n import re\n from typing import Any, Protocol, TypeVar, Union, cast\n \n-from absl import logging\n+import logging\n import numpy as np\n \n import jax\n@@ -55,6 +55,8 @@\n \n from jax._src.export import shape_poly\n \n+logger = logging.getLogger(__name__)\n+\n map = util.safe_map\n zip = util.safe_zip\n \n@@ -704,16 +706,15 @@ def _export_lowered(\n     out_avals_flat = lowered.compile_args[\"out_avals\"]  # type: ignore\n \n   # Log and then check the module.\n-  if logging.vlog_is_on(3):\n-    logmsg = (f\"fun_name={fun_name} version={version} \"\n-              f\"lowering_platforms={lowering._platforms} \"  # type: ignore[unused-ignore,attribute-error]\n-              f\"disabled_checks={disabled_checks}\")\n-    logging.info(\"Exported JAX function: %s\\n\", logmsg)\n-    logging.info(mlir.dump_module_message(mlir_module, \"export\"))\n-    logging.info(\n-        \"Size of mlir_module_serialized: %d byte\",\n-        len(mlir_module_serialized),\n-    )\n+  logmsg = (f\"fun_name={fun_name} version={version} \"\n+            f\"lowering_platforms={lowering._platforms} \"  # type: ignore[unused-ignore,attribute-error]\n+            f\"disabled_checks={disabled_checks}\")\n+  logger.debug(\"Exported JAX function: %s\\n\", logmsg)\n+  logger.debug(mlir.dump_module_message(mlir_module, \"export\"))\n+  logger.debug(\n+      \"Size of mlir_module_serialized: %d byte\",\n+      len(mlir_module_serialized),\n+  )\n \n   _check_module(mlir_module,\n                 disabled_checks=disabled_checks,\n\n```"
        ],
        "from_id": [
            "jenriver"
        ]
    },
    {
        "text_input": "Add support for non-power-of-2 head size in flash attention\nIntroduce checks on sequences being divisible by block sizes to address https://github.com/jax-ml/jax/issues/27224\n\nPiperOrigin-RevId: 762051831",
        "output": [
            "```diff\nCommit: 1aaec81f22a0dde3f7da56bc54bdd71f212076c9\nDate: 2025-05-22T18:36:42Z\nURL: https://github.com/jax-ml/jax/commit/1aaec81f22a0dde3f7da56bc54bdd71f212076c9\nFiles changed: 2\nAdditions: +104, Deletions: -84\ndiff --git a/jax/experimental/pallas/ops/gpu/attention.py b/jax/experimental/pallas/ops/gpu/attention.py\nindex 8b83d24ea199..ccb3ae8fd3b7 100644\n--- a/jax/experimental/pallas/ops/gpu/attention.py\n+++ b/jax/experimental/pallas/ops/gpu/attention.py\n@@ -86,28 +86,29 @@ def mha_forward_kernel(\n     segment_ids_ref: jax.Array | None,  # segment_id arrays\n     o_ref: Any,  # Output\n     *residual_refs: Any,  # Residual outputs\n-    num_heads: int,\n     sm_scale: float,\n     causal: bool,\n     block_q: int,\n-    block_d: int,\n     block_k: int,\n+    head_dim: int,\n ):\n   seq_len = k_ref.shape[0]\n   start_q = pl.program_id(0)\n+  head_dim_padded = q_ref.shape[-1]\n \n   # o is the buffer where we accumulate the output on sram.\n   # m_i and l_i (see FlashAttention paper) are updated during the k,v loop.\n   m_i = jnp.zeros(block_q, dtype=jnp.float32) - float('inf')\n   l_i = jnp.zeros(block_q, dtype=jnp.float32)\n   # acc is the buffer where we accumulate the output on sram.\n-  o = jnp.zeros((block_q, block_d), dtype=jnp.float32)\n+  o = jnp.zeros((block_q, head_dim_padded), dtype=jnp.float32)\n \n   # Load q: it will stay in L1 throughout. Indices form a matrix because we\n   # read, compute, and write all in 2d chunks. 1 element ~= 1 CUDA thread index.\n-  # q tile has shape [block_q, block_d], block_d == head_dim.\n+  # q tile has shape [block_q, head_dim_padded], head_dim_padded >= head_dim.\n   curr_q_slice = pl.dslice(start_q * block_q, block_q)\n-  q = q_ref[...]\n+  head_mask = (jnp.arange(head_dim_padded) < head_dim)[None, :]\n+  q = pl.load(q_ref, (slice(None), slice(None)), mask=head_mask, other=0.0)\n   q_segment_ids = (\n       None\n       if segment_ids_ref is None\n@@ -121,7 +122,7 @@ def body(start_k, carry):\n     o_prev, m_prev, l_prev = carry\n     curr_k_slice = pl.dslice(start_k * block_k, block_k)\n \n-    k = pl.load(k_ref, (curr_k_slice, slice(None)))\n+    k = pl.load(k_ref, (curr_k_slice, slice(None)), mask=head_mask, other=0.0)\n     qk = pl.dot(q, k.T)   # [block_q, block_k]\n \n     # Scale logits to convert from base-2 to the natural log domain.\n@@ -161,7 +162,7 @@ def body(start_k, carry):\n     l_curr = s_curr.sum(axis=-1)\n     l_next = l_prev_corr + l_curr\n     o_prev_corr = correction[:, None] * o_prev\n-    v = pl.load(v_ref, (curr_k_slice, pl.dslice(block_d)))\n+    v = pl.load(v_ref, (curr_k_slice, slice(None)), mask=head_mask)\n     o_curr = pl.dot(s_curr.astype(v.dtype), v)\n \n     o_next = o_prev_corr + o_curr\n@@ -182,7 +183,8 @@ def body(start_k, carry):\n     lse_ref = residual_refs[0]\n     lse_ref[...] = m_i + jnp.log2(l_i)\n   # Write output to dram.\n-  o_ref[...] = o.astype(o_ref.dtype)\n+  pl.store(o_ref, (slice(None), slice(o.shape[-1])), o.astype(o_ref.dtype),\n+           mask=head_mask)\n \n def segment_mask(\n     q_segment_ids: jax.Array,\n@@ -235,6 +237,17 @@ def mha(\n   kv_seq_len = k.shape[1]\n   block_q = min(block_sizes.block_q, q_seq_len)\n   block_k = min(block_sizes.block_k, kv_seq_len)\n+  head_dim_padded = pl.next_power_of_2(head_dim)\n+  if (q.shape[-1] != k.shape[-1]) or (q.shape[-1] != v.shape[-1]):\n+    raise ValueError(\n+        f\"This kernel expects q, k, and v to have the same head dimension, but\"\n+        f\" found {q.shape=}, {k.shape=}, {v.shape=}.\"\n+    )\n+  if q_seq_len % block_q != 0:\n+    raise ValueError(f\"{q_seq_len=} must be a multiple of {block_q=}\")\n+  if kv_seq_len % block_k != 0:\n+    raise ValueError(f\"{kv_seq_len=} must be a multiple of {block_k=}\")\n+\n   # Heuristics.\n   grid_ = grid\n   if grid_ is None:\n@@ -243,21 +256,17 @@ def mha(\n   num_warps_ = num_warps\n   if num_warps_ is None:\n     num_warps_ = 4 if head_dim <= 64 else 8\n-  kernel = functools.partial(mha_forward_kernel, num_heads=num_heads,\n-                             sm_scale=sm_scale, block_q=block_q,\n-                             block_k=block_k, block_d=head_dim,\n-                             causal=causal)\n+  kernel = functools.partial(mha_forward_kernel, sm_scale=sm_scale,\n+                             block_q=block_q, block_k=block_k,\n+                             head_dim=head_dim, causal=causal)\n \n   in_specs = [\n-      pl.BlockSpec(\n-          (None, block_q, None, head_dim), lambda i, j, k: (j, i, k, 0)\n-      ),\n-      pl.BlockSpec(\n-          (None, kv_seq_len, None, head_dim), lambda _, j, k: (j, 0, k, 0)\n-      ),\n-      pl.BlockSpec(\n-          (None, kv_seq_len, None, head_dim), lambda _, j, k: (j, 0, k, 0)\n-      ),\n+      pl.BlockSpec((None, block_q, None, head_dim_padded),\n+                   lambda i, j, k: (j, i, k, 0)),\n+      pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n+                   lambda _, j, k: (j, 0, k, 0)),\n+      pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n+                   lambda _, j, k: (j, 0, k, 0)),\n   ]\n   in_specs.append(\n       None  # type: ignore[arg-type]\n@@ -270,7 +279,7 @@ def mha(\n       grid=grid_,\n       in_specs=in_specs,\n       out_specs=pl.BlockSpec(\n-          (None, block_q, None, head_dim), lambda i, j, k: (j, i, k, 0)\n+          (None, block_q, None, head_dim_padded), lambda i, j, k: (j, i, k, 0)\n       ),\n       compiler_params=plgpu.TritonCompilerParams(\n           num_warps=num_warps_, num_stages=num_stages),\n@@ -301,6 +310,17 @@ def _mha_forward(\n   kv_seq_len = k.shape[1]\n   block_q = min(block_sizes.block_q, q_seq_len)\n   block_k = min(block_sizes.block_k, kv_seq_len)\n+  if (q.shape[-1] != k.shape[-1]) or (q.shape[-1] != v.shape[-1]):\n+    raise ValueError(\n+        f\"This kernel expects q, k, and v to have the same head dimension, but\"\n+        f\" found {q.shape=}, {k.shape=}, {v.shape=}.\"\n+    )\n+  if q_seq_len % block_q != 0:\n+    raise ValueError(f\"{q_seq_len=} must be a multiple of {block_q=}\")\n+  if kv_seq_len % block_k != 0:\n+    raise ValueError(f\"{kv_seq_len=} must be a multiple of {block_k=}\")\n+  head_dim_padded = pl.next_power_of_2(head_dim)\n+\n   # Heuristics.\n   grid_ = grid\n   if grid_ is None:\n@@ -309,9 +329,9 @@ def _mha_forward(\n   num_warps_ = num_warps\n   if num_warps_ is None:\n     num_warps_ = 4 if head_dim <= 64 else 8\n-  kernel = functools.partial(mha_forward_kernel, num_heads=num_heads,\n-                             sm_scale=sm_scale, causal=causal, block_q=block_q,\n-                             block_k=block_k, block_d=head_dim)\n+  kernel = functools.partial(mha_forward_kernel, sm_scale=sm_scale,\n+                             causal=causal, block_q=block_q, block_k=block_k,\n+                             head_dim=head_dim)\n   out_shape = [\n       jax.ShapeDtypeStruct(shape=q.shape, dtype=q.dtype),  # out\n       jax.ShapeDtypeStruct(\n@@ -319,15 +339,12 @@ def _mha_forward(\n       ),\n   ]\n   in_specs = [\n-      pl.BlockSpec(\n-          (None, block_q, None, head_dim), lambda i, j, k: (j, i, k, 0)\n-      ),\n-      pl.BlockSpec(\n-          (None, kv_seq_len, None, head_dim), lambda _, j, k: (j, 0, k, 0)\n-      ),\n-      pl.BlockSpec(\n-          (None, kv_seq_len, None, head_dim), lambda _, j, k: (j, 0, k, 0)\n-      ),\n+      pl.BlockSpec((None, block_q, None, head_dim_padded),\n+                   lambda i, j, k: (j, i, k, 0)),\n+      pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n+                   lambda _, j, k: (j, 0, k, 0)),\n+      pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n+                   lambda _, j, k: (j, 0, k, 0)),\n   ]\n   in_specs.append(\n       None  # type: ignore[arg-type]\n@@ -339,9 +356,8 @@ def _mha_forward(\n       grid=grid_,\n       in_specs=in_specs,\n       out_specs=[\n-          pl.BlockSpec(\n-              (None, block_q, None, head_dim), lambda i, j, k: (j, i, k, 0)\n-          ),\n+          pl.BlockSpec((None, block_q, None, head_dim_padded),\n+                       lambda i, j, k: (j, i, k, 0)),\n           pl.BlockSpec((None, None, block_q), lambda i, j, k: (j, k, i)),\n       ],\n       compiler_params=plgpu.TritonCompilerParams(\n@@ -355,10 +371,11 @@ def _mha_forward(\n   return out, (q, k, v, segment_ids, out, lse)\n \n \n-def _preprocess_backward_kernel(out_ref, dout_ref, delta_ref):\n+def _preprocess_backward_kernel(out_ref, dout_ref, delta_ref, head_dim: int):\n   # load\n-  o = out_ref[...].astype(jnp.float32)\n-  do = dout_ref[...].astype(jnp.float32)\n+  head_mask = (jnp.arange(out_ref.shape[-1]) < head_dim)[None, :]\n+  o = pl.load(out_ref, (slice(None), slice(None)), mask=head_mask, other=0.0)\n+  do = pl.load(dout_ref, (slice(None), slice(None)), mask=head_mask, other=0.0)\n   # compute\n   delta = jnp.sum(o * do, axis=1)\n   # write-back\n@@ -368,17 +385,16 @@ def _preprocess_backward_kernel(out_ref, dout_ref, delta_ref):\n def _preprocess_backward(out, do, lse, block_q: int,\n                          debug: bool, interpret: bool):\n   batch_size, seq_len, num_heads, head_dim = out.shape\n+  head_dim_padded = pl.next_power_of_2(head_dim)\n   out_shape = jax.ShapeDtypeStruct(lse.shape, lse.dtype)\n   delta = pl.pallas_call(\n-      _preprocess_backward_kernel,\n+      functools.partial(_preprocess_backward_kernel, head_dim=head_dim),\n       grid=(pl.cdiv(seq_len, block_q), batch_size, num_heads),\n       in_specs=[\n-          pl.BlockSpec(\n-              (None, block_q, None, head_dim), lambda i, j, k: (j, i, k, 0)\n-          ),\n-          pl.BlockSpec(\n-              (None, block_q, None, head_dim), lambda i, j, k: (j, i, k, 0)\n-          ),\n+          pl.BlockSpec((None, block_q, None, head_dim_padded),\n+                       lambda i, j, k: (j, i, k, 0)),\n+          pl.BlockSpec((None, block_q, None, head_dim_padded),\n+                       lambda i, j, k: (j, i, k, 0)),\n       ],\n       out_specs=pl.BlockSpec((None, None, block_q), lambda i, j, k: (j, k, i)),\n       compiler_params=plgpu.TritonCompilerParams(num_warps=4, num_stages=3),\n@@ -414,7 +430,7 @@ def mha_backward_kernel(\n     block_kv_dkv: int,\n     block_q_dq: int,\n     block_kv_dq: int,\n-    block_d: int,\n+    head_dim: int,\n ):\n   del out_ref  # Not needed\n   q_seq_len = q_ref.shape[0]\n@@ -427,11 +443,13 @@ def mha_backward_kernel(\n   start_k = pl.program_id(2)\n   curr_k_slice = pl.dslice(start_k * block_kv_dkv, block_kv_dkv)\n \n-  dv = jnp.zeros([block_kv_dkv, block_d], dtype=jnp.float32)\n-  dk = jnp.zeros([block_kv_dkv, block_d], dtype=jnp.float32)\n+  head_dim_padded = q_ref.shape[-1]\n+  dv = jnp.zeros([block_kv_dkv, head_dim_padded], dtype=jnp.float32)\n+  dk = jnp.zeros([block_kv_dkv, head_dim_padded], dtype=jnp.float32)\n \n-  v = pl.load(v_ref, (curr_k_slice, slice(None)))\n-  k = pl.load(k_ref, (curr_k_slice, slice(None)))\n+  head_mask = (jnp.arange(head_dim_padded) < head_dim)[None, :]\n+  v = pl.load(v_ref, (curr_k_slice, slice(None)), mask=head_mask, other=0.0)\n+  k = pl.load(k_ref, (curr_k_slice, slice(None)), mask=head_mask, other=0.0)\n   span_k = start_k * block_kv_dkv + jnp.arange(block_kv_dkv)\n   kv_segment_ids = (\n       None\n@@ -443,7 +461,7 @@ def inner_loop_dkdv(start_q, carry):\n     dv, dk = carry\n     curr_q_slice = pl.dslice(start_q * block_q_dkv, block_q_dkv)\n \n-    q = pl.load(q_ref, (curr_q_slice, slice(None)))\n+    q = pl.load(q_ref, (curr_q_slice, slice(None)), mask=head_mask, other=0.0)\n     qk = pl.dot(q, k.T)\n     qk_scale = math.log2(math.e)\n     if sm_scale != 1.:\n@@ -466,7 +484,8 @@ def inner_loop_dkdv(start_q, carry):\n \n     lse = pl.load(lse_ref, (curr_q_slice,))\n     di = pl.load(delta_ref, (curr_q_slice,))\n-    do = pl.load(do_scaled_ref, (curr_q_slice, slice(None)))\n+    do = pl.load(do_scaled_ref, (curr_q_slice, slice(None)), mask=head_mask,\n+                 other=0.0)\n \n     p = jnp.exp2(qk - lse[:, None])\n     dv = dv + pl.dot(p.astype(do.dtype).T, do)\n@@ -483,8 +502,10 @@ def inner_loop_dkdv(start_q, carry):\n   dv, dk = lax.fori_loop(\n       lower_bound, pl.cdiv(q_seq_len, block_q_dkv), inner_loop_dkdv, (dv, dk)\n   )\n-  dv_ref[...] = dv.astype(dv_ref.dtype)\n-  dk_ref[...] = dk.astype(dk_ref.dtype)\n+  pl.store(dv_ref, (slice(None), slice(dv.shape[-1])), dv.astype(dv_ref.dtype),\n+           mask=head_mask)\n+  pl.store(dk_ref, (slice(None), slice(dk.shape[-1])), dk.astype(dk_ref.dtype),\n+           mask=head_mask)\n \n   # Scan #2: dQ\n   #   1. Load a block of Q of size (block_q_dq, head_dim) in SMEM.\n@@ -493,22 +514,23 @@ def inner_loop_dkdv(start_q, carry):\n   start_q = pl.program_id(2)\n   curr_q_slice = pl.ds(start_q * block_q_dq, block_q_dq)\n   span_q = start_q * block_q_dq + jnp.arange(block_q_dq)\n-  dq = jnp.zeros([block_q_dq, block_d], dtype=jnp.float32)\n+  dq = jnp.zeros([block_q_dq, head_dim_padded], dtype=jnp.float32)\n \n-  q = pl.load(q_ref, (curr_q_slice, slice(None)))\n+  q = pl.load(q_ref, (curr_q_slice, slice(None)), mask=head_mask, other=0.0)\n   q_segment_ids = (\n       None\n       if segment_ids_ref is None\n       else pl.load(segment_ids_ref, (curr_q_slice,))\n   )\n   lse = pl.load(lse_ref, (curr_q_slice,))\n-  do = pl.load(do_scaled_ref, (curr_q_slice, slice(None)))\n+  do = pl.load(do_scaled_ref, (curr_q_slice, slice(None)), mask=head_mask,\n+               other=0.0)\n   di = pl.load(delta_ref, (curr_q_slice,))\n \n   def inner_loop_dq(start_k, dq):\n     curr_k_slice = pl.dslice(start_k * block_kv_dq, block_kv_dq)\n-    k = pl.load(k_ref, (curr_k_slice, slice(None)))\n-    v = pl.load(v_ref, (curr_k_slice, slice(None)))\n+    k = pl.load(k_ref, (curr_k_slice, slice(None)), mask=head_mask, other=0.0)\n+    v = pl.load(v_ref, (curr_k_slice, slice(None)), mask=head_mask, other=0.0)\n \n     qk = pl.dot(q, k.T)\n     qk_scale = math.log2(math.e)\n@@ -547,7 +569,8 @@ def inner_loop_dq(start_k, dq):\n     upper_bound = pl.cdiv(kv_seq_len, block_kv_dq)\n \n   dq = lax.fori_loop(0, upper_bound, inner_loop_dq, (dq))\n-  dq_ref[...] = dq.astype(dq_ref.dtype)\n+  pl.store(dq_ref, (slice(None), slice(dq.shape[-1])), dq.astype(dq_ref.dtype),\n+           mask=head_mask)\n \n \n def _mha_backward(sm_scale: float, causal: bool, block_sizes: BlockSizes,\n@@ -576,6 +599,7 @@ def _mha_backward(sm_scale: float, causal: bool, block_sizes: BlockSizes,\n     block_kv_dkv = min(block_sizes.block_kv_dkv, kv_seq_len)\n     block_q_dq = min(block_sizes.block_q_dq, q_seq_len)\n     block_kv_dq = min(block_sizes.block_kv_dq, kv_seq_len)\n+    head_dim_padded = pl.next_power_of_2(head_dim)\n \n     if q_seq_len // block_q_dq != kv_seq_len // block_kv_dkv:\n       raise ValueError(\n@@ -591,28 +615,24 @@ def _mha_backward(sm_scale: float, causal: bool, block_sizes: BlockSizes,\n     ]\n \n     in_specs = [\n-        pl.BlockSpec(\n-            (None, q_seq_len, None, head_dim), lambda i, j, _: (i, 0, j, 0)\n-        ),\n-        pl.BlockSpec(\n-            (None, kv_seq_len, None, head_dim), lambda i, j, _: (i, 0, j, 0)\n-        ),\n-        pl.BlockSpec(\n-            (None, kv_seq_len, None, head_dim), lambda i, j, _: (i, 0, j, 0)\n-        ),\n-        pl.BlockSpec(\n-            (None, q_seq_len, None, head_dim), lambda i, j, _: (i, 0, j, 0)\n-        ),\n-        pl.BlockSpec(\n-            (None, q_seq_len, None, head_dim), lambda i, j, _: (i, 0, j, 0)\n-        ),\n+        pl.BlockSpec((None, q_seq_len, None, head_dim_padded),\n+                     lambda i, j, _: (i, 0, j, 0)),\n+        pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n+                     lambda i, j, _: (i, 0, j, 0)),\n+        pl.BlockSpec((None, kv_seq_len, None, head_dim_padded),\n+                     lambda i, j, _: (i, 0, j, 0)),\n+        pl.BlockSpec((None, q_seq_len, None, head_dim_padded),\n+                     lambda i, j, _: (i, 0, j, 0)),\n+        pl.BlockSpec((None, q_seq_len, None, head_dim_padded),\n+                     lambda i, j, _: (i, 0, j, 0)),\n         pl.BlockSpec((None, None, q_seq_len), lambda i, j, _: (i, j, 0)),\n         pl.BlockSpec((None, None, q_seq_len), lambda i, j, _: (i, j, 0)),\n     ]\n     if segment_ids is None:\n       in_specs.insert(3, None)  # type: ignore[arg-type]\n     else:\n-      in_specs.insert(3, pl.BlockSpec((None, kv_seq_len), lambda i, j, _: (i, 0)))\n+      in_specs.insert(3, pl.BlockSpec((None, kv_seq_len),\n+                                      lambda i, j, _: (i, 0)))\n \n     grid = (batch_size, num_heads, pl.cdiv(kv_seq_len, block_kv_dkv))\n     num_warps_ = num_warps\n@@ -635,22 +655,22 @@ def _mha_backward(sm_scale: float, causal: bool, block_sizes: BlockSizes,\n             block_kv_dkv=block_kv_dkv,\n             block_q_dq=block_q_dq,\n             block_kv_dq=block_kv_dq,\n-            block_d=head_dim,\n+            head_dim=head_dim,\n         ),\n         out_shape=out_shapes,\n         in_specs=in_specs,\n         grid=grid,\n         out_specs=[\n             pl.BlockSpec(\n-                (None, block_q_dq, None, head_dim),\n+                (None, block_q_dq, None, head_dim_padded),\n                 lambda i, j, k: (i, k, j, 0),  # dq\n             ),\n             pl.BlockSpec(\n-                (None, block_kv_dkv, None, head_dim),\n+                (None, block_kv_dkv, None, head_dim_padded),\n                 lambda i, j, k: (i, k, j, 0),  # dk\n             ),\n             pl.BlockSpec(\n-                (None, block_kv_dkv, None, head_dim),\n+                (None, block_kv_dkv, None, head_dim_padded),\n                 lambda i, j, k: (i, k, j, 0),  # dv\n             ),\n         ],\ndiff --git a/tests/pallas/gpu_ops_test.py b/tests/pallas/gpu_ops_test.py\nindex 1b758cdd0a58..1637686365e1 100644\n--- a/tests/pallas/gpu_ops_test.py\n+++ b/tests/pallas/gpu_ops_test.py\n@@ -153,7 +153,7 @@ def setUp(self):\n       batch_size=(1, 2),\n       seq_len=(128, 384),\n       num_heads=(1, 2, 8),\n-      head_dim=(32, 64, 128),\n+      head_dim=(32, 64, 72, 128),\n       block_sizes=(\n         ((\"block_q\", 128), (\"block_k\", 128)),\n         ((\"block_q\", 64), (\"block_k\", 64)),\n@@ -226,7 +226,7 @@ def impl(q, k, v):\n       batch_size=(1, 2),\n       seq_len=(128, 384),\n       num_heads=(1, 2),\n-      head_dim=(32, 64, 128,),\n+      head_dim=(32, 64, 72, 128,),\n       block_sizes=(\n           (\n               (\"block_q\", 128),\n\n```"
        ],
        "from_id": [
            "rdyro",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add support for loops, debug_print, and unary ops to Warp semantics.\n\nPiperOrigin-RevId: 762036132",
        "output": [
            "```diff\nCommit: a827a274baf18ece651d37b7933bee3cb2f8760e\nDate: 2025-05-22T17:57:53Z\nURL: https://github.com/jax-ml/jax/commit/a827a274baf18ece651d37b7933bee3cb2f8760e\nFiles changed: 3\nAdditions: +139, Deletions: -60\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 2f81ecb969ac..00716bb1c675 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -1698,6 +1698,19 @@ def convert(ty, x):\n     lax.not_p: lambda ctx, x: ~x,\n })\n \n+def _unary_warp_lowering_rule(impl):\n+  def _lowering_rule(ctx: LoweringRuleContext, x):\n+    if not all(aval_in.shape == () for aval_in in ctx.avals_in):\n+      raise NotImplementedError(\n+          \"Non-scalar arithmetic is not supported in warp-level lowering.\")\n+    return impl(x)\n+  return _lowering_rule\n+\n+mosaic_lowering_rules[gpu_core.LANExWARP_SEMANTICS].update({\n+    lax.neg_p: _unary_warp_lowering_rule(lambda x: -x),\n+    lax.not_p: _unary_warp_lowering_rule(lambda x: ~x)\n+})\n+\n mosaic_lowering_rules[gpu_core.WGxWG_SEMANTICS].update({\n     lax.neg_p: _lower_fun(lambda x: jnp.subtract(0, x), multiple_results=False),\n     lax.not_p: _lower_fun(\n@@ -2163,6 +2176,8 @@ def _axis_index_warp_rule(ctx: LoweringRuleContext, *, axis_name: Hashable):\n \n \n @register_lowering_rule(primitives.debug_print_p, mgpu.LoweringSemantics.Lane)\n+@register_lowering_rule(primitives.debug_print_p, mgpu.LoweringSemantics.Lane,\n+                        gpu_core.PrimitiveSemantics.Warp)\n def _debug_print_lowering_rule(\n     ctx: LoweringRuleContext,\n     *args,\n@@ -2171,6 +2186,9 @@ def _debug_print_lowering_rule(\n ):\n   del has_placeholders  # Unused.\n   primitives.check_debug_print_format(fmt, *args)\n+  scope = mgpu.ThreadSubset.WARPGROUP\n+  if ctx.module_ctx.primitive_semantics == gpu_core.PrimitiveSemantics.Warp:\n+    scope = mgpu.ThreadSubset.WARP\n   if not any(aval.shape for aval in ctx.avals_in):\n     mgpu.debug_print(\n         fmt,\n@@ -2178,6 +2196,7 @@ def _debug_print_lowering_rule(\n             _ensure_ir_value(arg, aval.dtype)\n             for arg, aval in zip(args, ctx.avals_in)\n         ),\n+        scope=scope\n     )\n   elif len(ctx.avals_in) == 1:\n     [arg] = args\n@@ -2461,6 +2480,8 @@ def loop(loop_index, body_args):\n \n @register_lowering_rule(lax.scan_p, mgpu.LoweringSemantics.Lane)\n @register_lowering_rule(lax.scan_p, mgpu.LoweringSemantics.Warpgroup)\n+@register_lowering_rule(lax.scan_p, mgpu.LoweringSemantics.Lane,\n+                        gpu_core.PrimitiveSemantics.Warp)\n def _scan_lowering_rule(\n     ctx: LoweringRuleContext,\n     *args,\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex 1e20675f7909..4aeb3358b97a 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -144,7 +144,11 @@ def _debug_scalar_ty_format(arg):\n     return \"%f\", arg\n   raise NotImplementedError(f\"Can't print the type {arg.type}\")\n \n-def debug_print(fmt, *args, uniform=True):\n+def debug_print(fmt, *args, uniform=True, scope=None):\n+  if not uniform and scope is not None:\n+    raise ValueError(\"Cannot specify scope to a non-uniform debug_print.\")\n+  if scope is None:\n+    scope = ThreadSubset.WARPGROUP\n   type_formats = []\n   new_args = []\n   for arg in args:\n@@ -168,7 +172,7 @@ def debug_print(fmt, *args, uniform=True):\n       raise NotImplementedError(arg.type)\n     type_formats.append(ty_format)\n   ctx = (\n-      functools.partial(single_thread, scope=ThreadSubset.WARPGROUP)\n+      functools.partial(single_thread, scope=scope)\n       if uniform\n       else contextlib.nullcontext\n   )\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 7173639b879f..4ef2fa8096ee 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1569,64 +1569,6 @@ def kernel(x_ref, y_ref, o_ref):\n     y = jax.lax.iota(jnp.float32, 128) * 3\n     np.testing.assert_array_equal(kernel(x, y), x + y)\n \n-  def test_warp_specialization_axis_index(self):\n-    if self.LOWERING_SEMANTICS != plgpu.LoweringSemantics.Lane:\n-      self.skipTest(\"Test only works on Lane semantics\")\n-    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n-    @functools.partial(plgpu.kernel,\n-                       out_shape=jax.ShapeDtypeStruct((2, 128), jnp.int32))\n-    def kernel(y_ref):\n-      def scope(ones_smem_ref, threes_smem_ref):\n-        # Prepare data to copy.\n-        ones_smem_ref[:] = jnp.ones((1, 128), jnp.int32)\n-        threes_smem_ref[:] = jnp.ones((1, 128), jnp.int32) * 3\n-        plgpu.commit_smem()\n-        @pl.core_map(warp_mesh)\n-        def _():\n-          warp_id = lax.axis_index(\"warp\")\n-          # We cannot load/store inside of core_map, so we issue async\n-          # copies instead to produce a testable result.\n-          @pl.when(warp_id == 1)\n-          def _():\n-            plgpu.copy_smem_to_gmem(ones_smem_ref, y_ref.at[0:1])\n-          @pl.when(warp_id == 3)\n-          def _():\n-            plgpu.copy_smem_to_gmem(threes_smem_ref, y_ref.at[1:2])\n-        plgpu.wait_smem_to_gmem(0)\n-      pl.run_scoped(scope,\n-                    plgpu.SMEM((1, 128), jnp.int32),\n-                    plgpu.SMEM((1, 128), jnp.int32)\n-                    )\n-    result = kernel()\n-    expected = jnp.stack((jnp.ones((128,), jnp.int32),\n-                          jnp.ones((128,), jnp.int32) * 3), axis=0)\n-    np.testing.assert_array_equal(result, expected)\n-\n-  def test_warp_mesh_errors_when_closing_over_array(self):\n-    if self.LOWERING_SEMANTICS != plgpu.LoweringSemantics.Lane:\n-      self.skipTest(\"Test only works on Lane semantics\")\n-    # We currently do not allow closing over arrays when mapping over\n-    # a mesh, since we would need to present a view of the array local\n-    # to each warp.\n-    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n-    @functools.partial(plgpu.kernel,\n-                       out_shape=jax.ShapeDtypeStruct((32, 32), jnp.float32),\n-                       scratch_shapes=[plgpu.SMEM((32, 32), jnp.float32)])\n-    def kernel(out_ref, smem_ref):\n-      arr = jnp.ones((32, 32), dtype=jnp.float32)\n-      @pl.core_map(warp_mesh)\n-      def _():\n-        smem_ref[...] = arr + 1\n-      plgpu.commit_smem()\n-      plgpu.copy_smem_to_gmem(smem_ref, out_ref)\n-      plgpu.wait_smem_to_gmem(0)\n-    with self.assertRaisesRegex(\n-        mgpu_lowering.LoweringError,\n-        \"Can only close over scalars and Refs when using core_map with \"\n-        \"WarpMesh\",\n-    ):\n-      kernel()\n-\n   def test_smem_aliasing_works(self):\n     self.skip_if_wg_semantics()\n \n@@ -1825,6 +1767,118 @@ def body(idx, _):\n       )\n \n \n+class PallasCallWarpPrimitiveSemanticsTest(PallasTest):\n+  def setUp(self):\n+    super().setUp()\n+    if self.LOWERING_SEMANTICS != plgpu.LoweringSemantics.Lane:\n+      self.skipTest(\"Test only works on Lane semantics\")\n+\n+  def test_axis_index(self):\n+    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n+    @functools.partial(plgpu.kernel,\n+                       out_shape=jax.ShapeDtypeStruct((2, 128), jnp.int32))\n+    def kernel(y_ref):\n+      def scope(ones_smem_ref, threes_smem_ref):\n+        # Prepare data to copy.\n+        ones_smem_ref[:] = jnp.ones((1, 128), jnp.int32)\n+        threes_smem_ref[:] = jnp.ones((1, 128), jnp.int32) * 3\n+        plgpu.commit_smem()\n+        @pl.core_map(warp_mesh)\n+        def _():\n+          warp_id = lax.axis_index(\"warp\")\n+          # We cannot load/store inside of core_map, so we issue async\n+          # copies instead to produce a testable result.\n+          @pl.when(warp_id == 1)\n+          def _():\n+            plgpu.copy_smem_to_gmem(ones_smem_ref, y_ref.at[0:1])\n+          @pl.when(warp_id == 3)\n+          def _():\n+            plgpu.copy_smem_to_gmem(threes_smem_ref, y_ref.at[1:2])\n+        plgpu.wait_smem_to_gmem(0)\n+      pl.run_scoped(scope,\n+                    plgpu.SMEM((1, 128), jnp.int32),\n+                    plgpu.SMEM((1, 128), jnp.int32)\n+                    )\n+    result = kernel()\n+    expected = jnp.stack((jnp.ones((128,), jnp.int32),\n+                          jnp.ones((128,), jnp.int32) * 3), axis=0)\n+    np.testing.assert_array_equal(result, expected)\n+\n+  def test_errors_when_closing_over_array(self):\n+    # We currently do not allow closing over arrays when mapping over\n+    # a mesh, since we would need to present a view of the array local\n+    # to each warp.\n+    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n+    @functools.partial(plgpu.kernel,\n+                       out_shape=jax.ShapeDtypeStruct((32, 32), jnp.float32),\n+                       scratch_shapes=[plgpu.SMEM((32, 32), jnp.float32)])\n+    def kernel(out_ref, smem_ref):\n+      arr = jnp.ones((32, 32), dtype=jnp.float32)\n+      @pl.core_map(warp_mesh)\n+      def _():\n+        smem_ref[...] = arr + 1\n+      plgpu.commit_smem()\n+      plgpu.copy_smem_to_gmem(smem_ref, out_ref)\n+      plgpu.wait_smem_to_gmem(0)\n+    with self.assertRaisesRegex(\n+        mgpu_lowering.LoweringError,\n+        \"Can only close over scalars and Refs when using core_map with \"\n+        \"WarpMesh\",\n+    ):\n+      kernel()\n+\n+  def test_single_warp_scan(self):\n+    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n+    @functools.partial(plgpu.kernel,\n+                       out_shape=jax.ShapeDtypeStruct((10, 128), jnp.int32))\n+    def kernel(y_ref):\n+      def scope(smem_ref):\n+        # Prepare data to copy.\n+        for i in range(10):\n+          smem_ref[i, :] = jnp.ones_like(smem_ref.at[i]) * i\n+        plgpu.commit_smem()\n+        @pl.core_map(warp_mesh)\n+        def _():\n+          warp_id = lax.axis_index(\"warp\")\n+          @pl.when(warp_id == 0)\n+          def _():\n+            def loop_body(i, _):\n+              _slice = pl.ds(i, 1)\n+              plgpu.copy_smem_to_gmem(smem_ref.at[_slice], y_ref.at[_slice])\n+            lax.fori_loop(0, 10, loop_body, None)\n+        plgpu.wait_smem_to_gmem(0)\n+      pl.run_scoped(scope, plgpu.SMEM((10, 128), jnp.int32))\n+    result = kernel()\n+    expected = jnp.stack(\n+        [jnp.ones((128,), jnp.int32) * i for i in range(10)], axis=0)\n+    np.testing.assert_array_equal(result, expected)\n+\n+  def test_debug_print(self):\n+    warp_mesh = plgpu.WarpMesh(axis_name=\"warp\")\n+    @functools.partial(\n+        plgpu.kernel,\n+        out_shape=jnp.zeros(128, np.int32),\n+    )\n+    def kernel(ref):\n+      ref[...] = ref[...]  # Prevent kernel from being DCE'd\n+      @pl.core_map(warp_mesh)\n+      def _():\n+        warp_id = lax.axis_index(\"warp\")\n+        pl.debug_print(\"warp: {}\", warp_id)\n+\n+    with self.capture_stdout() as output:\n+      jax.block_until_ready(kernel())\n+    self.assertEqual(\n+        set(output().splitlines()),\n+        {\n+            \"warp: 0\",\n+            \"warp: 1\",\n+            \"warp: 2\",\n+            \"warp: 3\",\n+        },\n+    )\n+\n+\n class PallasCallWGTest(\n     PallasCallTest, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n ):\n\n```"
        ],
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Error out if wsc(x, P()) is called in a Explicit mesh context\n\nPiperOrigin-RevId: 762021159",
        "output": [
            "```diff\nCommit: 210b5fc8674e4993254c804720144c570992984e\nDate: 2025-05-22T17:24:17Z\nURL: https://github.com/jax-ml/jax/commit/210b5fc8674e4993254c804720144c570992984e\nFiles changed: 2\nAdditions: +12, Deletions: -3\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 0624dad88a2b..ecdcf3e17332 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -2703,7 +2703,13 @@ def check_shardings_are_auto(shardings_flat):\n       raise ValueError(\n           'The spec of NamedSharding passed to with_sharding_constraint can'\n           f' only refer to Auto axes of the mesh. Got spec={s.spec} and'\n-          f' mesh={mesh}')\n+          f' mesh={mesh}. You probably meant to use `reshard` API?')\n+\n+  cur_mesh = mesh_lib.get_abstract_mesh()\n+  if cur_mesh._are_all_axes_explicit:\n+    raise ValueError(\n+        'with_sharding_constraint cannot be used when all axes of the mesh are'\n+        ' of type `Explicit`. Please use the `reshard` API.')\n \n \n def with_sharding_constraint(x, shardings):\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 024901b746a8..48339bb2a519 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7069,8 +7069,11 @@ def test_wsc_error(self, mesh):\n         \"The spec of NamedSharding passed to with_sharding_constraint\"):\n       jax.lax.with_sharding_constraint(np.arange(8).reshape(4, 2), s)\n \n-    s = NamedSharding(mesh, P())\n-    jax.lax.with_sharding_constraint(np.arange(8), s)\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        'with_sharding_constraint cannot be used when all axes of the mesh are'\n+        ' of type `Explicit`'):\n+      jax.lax.with_sharding_constraint(np.arange(8), NamedSharding(mesh, P()))\n \n     s = NamedSharding(Mesh(mesh.devices, mesh.axis_names,\n                            axis_types=(AxisType.Explicit, AxisType.Auto)),\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "jax.random: thread mode parameter through categorical and choice",
        "output": [
            "```diff\nCommit: 437e32bfddabe6c4b5ff5682ca944cb6a4cbaf89\nDate: 2025-05-22T17:22:03Z\nURL: https://github.com/jax-ml/jax/commit/437e32bfddabe6c4b5ff5682ca944cb6a4cbaf89\nFiles changed: 2\nAdditions: +32, Deletions: -10\ndiff --git a/jax/_src/random.py b/jax/_src/random.py\nindex 6f139dd9665c..60dad3a82021 100644\n--- a/jax/_src/random.py\n+++ b/jax/_src/random.py\n@@ -633,7 +633,8 @@ def choice(key: ArrayLike,\n            shape: Shape = (),\n            replace: bool = True,\n            p: RealArray | None = None,\n-           axis: int = 0) -> Array:\n+           axis: int = 0,\n+           mode: str | None = None) -> Array:\n   \"\"\"Generates a random sample from a given array.\n \n   .. warning::\n@@ -656,6 +657,12 @@ def choice(key: ArrayLike,\n       entries in a.\n     axis: int, optional. The axis along which the selection is performed.\n       The default, 0, selects by row.\n+    mode: optional, \"high\" or \"low\" for how many bits to use in the gumbel sampler\n+      when `p is None` and `replace = False`. The default is determined by the\n+      ``use_high_dynamic_range_gumbel`` config, which defaults to \"low\". With mode=\"low\",\n+      in float32 sampling will be biased for choices with probability less than about\n+      1E-7; with mode=\"high\" this limit is pushed down to about 1E-14. mode=\"high\"\n+      approximately doubles the cost of sampling.\n \n   Returns:\n     An array of shape `shape` containing samples from `a`.\n@@ -701,7 +708,7 @@ def choice(key: ArrayLike,\n       ind = jnp.searchsorted(p_cuml, r).astype(int)\n     else:\n       # Gumbel top-k trick: https://timvieira.github.io/blog/post/2019/09/16/algorithms-for-sampling-without-replacement/\n-      g = gumbel(key, (n_inputs,), dtype=p_arr.dtype) + jnp.log(p_arr)\n+      g = gumbel(key, (n_inputs,), dtype=p_arr.dtype, mode=mode) + jnp.log(p_arr)\n       ind = lax.top_k(g, k=n_draws)[1].astype(int)\n     result = ind if arr.ndim == 0 else jnp.take(arr, ind, axis)\n \n@@ -940,7 +947,8 @@ def bernoulli(key: ArrayLike,\n     mode: optional, \"high\" or \"low\" for how many bits to use when sampling.\n       default='low'. Set to \"high\" for correct sampling at small values of\n       `p`. When sampling in float32, bernoulli samples with mode='low' produce\n-      incorrect results for p < ~1E-7.\n+      incorrect results for p < ~1E-7. mode=\"high\" approximately doubles the\n+      cost of sampling.\n \n   Returns:\n     A random array with boolean dtype and shape given by ``shape`` if ``shape``\n@@ -1544,7 +1552,7 @@ def poisson(key: ArrayLike,\n def gumbel(key: ArrayLike,\n            shape: Shape = (),\n            dtype: DTypeLikeFloat = float,\n-           mode: str | None =None) -> Array:\n+           mode: str | None = None) -> Array:\n   \"\"\"Sample Gumbel random values with given shape and float dtype.\n \n   The values are distributed according to the probability density function:\n@@ -1559,6 +1567,11 @@ def gumbel(key: ArrayLike,\n     dtype: optional, a float dtype for the returned values (default float64 if\n       jax_enable_x64 is true, otherwise float32).\n     mode: optional, \"high\" or \"low\" for how many bits to use when sampling.\n+      The default is determined by the ``use_high_dynamic_range_gumbel`` config,\n+      which defaults to \"low\". When drawing float32 samples, with mode=\"low\" the\n+      uniform resolution is such that the largest possible gumbel logit is ~16;\n+      with mode=\"high\" this is increased to ~32, at approximately double the\n+      computational cost.\n \n   Returns:\n     A random array with the specified shape and dtype.\n@@ -1599,6 +1612,7 @@ def categorical(\n   axis: int = -1,\n   shape: Shape | None = None,\n   replace: bool = True,\n+  mode: str | None = None,\n ) -> Array:\n   \"\"\"Sample random values from categorical distributions.\n \n@@ -1615,6 +1629,12 @@ def categorical(\n       The default (None) produces a result shape equal to ``np.delete(logits.shape, axis)``.\n     replace: If True (default), perform sampling with replacement. If False, perform\n       sampling without replacement.\n+    mode: optional, \"high\" or \"low\" for how many bits to use in the gumbel sampler.\n+      The default is determined by the ``use_high_dynamic_range_gumbel`` config,\n+      which defaults to \"low\". With mode=\"low\", in float32 sampling will be biased\n+      for events with probability less than about 1E-7; with mode=\"high\" this limit\n+      is pushed down to about 1E-14. mode=\"high\" approximately doubles the cost of\n+      sampling.\n \n   Returns:\n     A random array with int dtype and shape given by ``shape`` if ``shape``\n@@ -1644,11 +1664,11 @@ def categorical(\n     logits_shape = list(shape[len(shape) - len(batch_shape):])\n     logits_shape.insert(axis % len(logits_arr.shape), logits_arr.shape[axis])\n     return jnp.argmax(\n-        gumbel(key, (*shape_prefix, *logits_shape), logits_arr.dtype) +\n+        gumbel(key, (*shape_prefix, *logits_shape), logits_arr.dtype, mode=mode) +\n         lax.expand_dims(logits_arr, tuple(range(len(shape_prefix)))),\n         axis=axis)\n   else:\n-    logits_arr += gumbel(key, logits_arr.shape, logits_arr.dtype)\n+    logits_arr += gumbel(key, logits_arr.shape, logits_arr.dtype, mode=mode)\n     k = math.prod(shape_prefix)\n     if k > logits_arr.shape[axis]:\n       raise ValueError(\ndiff --git a/tests/random_lax_test.py b/tests/random_lax_test.py\nindex f87b079b759c..9fe4d2ecbda3 100644\n--- a/tests/random_lax_test.py\n+++ b/tests/random_lax_test.py\n@@ -286,8 +286,9 @@ def testTruncatedNormal(self, dtype):\n     ],\n     dtype=jtu.dtypes.floating + jtu.dtypes.integer,\n     weighted=[True, False],\n+    mode=[None, 'low', 'high']\n   )\n-  def testChoice(self, dtype, input_range_or_shape, shape, replace, weighted, axis):\n+  def testChoice(self, dtype, input_range_or_shape, shape, replace, weighted, axis, mode):\n     # This is the function API that we test against (note that self.rng().choice differs)\n     np_choice = np.random.default_rng(0).choice\n     p_dtype = dtypes.to_inexact_dtype(dtype)\n@@ -303,7 +304,7 @@ def testChoice(self, dtype, input_range_or_shape, shape, replace, weighted, axis\n       p /= p.sum()\n     else:\n       p = None\n-    rand = lambda key, x: random.choice(key, x, shape, replace, p, axis)\n+    rand = lambda key, x: random.choice(key, x, shape, replace, p, axis, mode=mode)\n     sample = rand(key(), x)\n     if not is_range:\n       self.assertEqual(dtype, sample.dtype)\n@@ -397,15 +398,16 @@ def testBernoulli(self, p, dtype, mode):\n       ]\n     ],\n     sample_shape=[(10000,), (5000, 2)],\n+    mode=[None, 'low', 'high'],\n     dtype=jtu.dtypes.floating,\n   )\n-  def testCategorical(self, p, axis, dtype, sample_shape):\n+  def testCategorical(self, p, axis, dtype, sample_shape, mode):\n     key = lambda: self.make_key(0)\n     p = np.array(p, dtype=dtype)\n     logits = np.log(p) - 42 # test unnormalized\n     out_shape = tuple(np.delete(logits.shape, axis))\n     shape = sample_shape + out_shape\n-    rand = partial(random.categorical, shape=shape, axis=axis)\n+    rand = partial(random.categorical, shape=shape, axis=axis, mode=mode)\n     crand = jax.jit(rand)\n \n     uncompiled_samples = rand(key(), logits)\n\n```"
        ],
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Merge pull request #28945 from hawkinsp:trove\n\nPiperOrigin-RevId: 762016836",
        "output": [
            "```diff\nCommit: 3622c928bc3ec9647023355db2e788c6a3012ee4\nDate: 2025-05-22T17:14:34Z\nURL: https://github.com/jax-ml/jax/commit/3622c928bc3ec9647023355db2e788c6a3012ee4\nFiles changed: 4\nAdditions: +9, Deletions: -2\ndiff --git a/jax_plugins/cuda/plugin_setup.py b/jax_plugins/cuda/plugin_setup.py\nindex c8b70408471c..fc467824fe5f 100644\n--- a/jax_plugins/cuda/plugin_setup.py\n+++ b/jax_plugins/cuda/plugin_setup.py\n@@ -78,10 +78,12 @@ def has_ext_modules(self):\n     url=\"https://github.com/jax-ml/jax\",\n     license=\"Apache-2.0\",\n     classifiers=[\n-        \"Development Status :: 3 - Alpha\",\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Programming Language :: Python :: 3.12\",\n+        \"Programming Language :: Python :: 3.13\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     package_data={\n         package_name: [\ndiff --git a/jax_plugins/cuda/setup.py b/jax_plugins/cuda/setup.py\nindex 1ce555978dac..b2c89285e7fd 100644\n--- a/jax_plugins/cuda/setup.py\n+++ b/jax_plugins/cuda/setup.py\n@@ -51,8 +51,9 @@ def load_version_module(pkg_path):\n     url=\"https://github.com/jax-ml/jax\",\n     license=\"Apache-2.0\",\n     classifiers=[\n-        \"Development Status :: 3 - Alpha\",\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     package_data={\n         package_name: [\"xla_cuda_plugin.so\"],\ndiff --git a/jaxlib/setup.py b/jaxlib/setup.py\nindex 8d7933953851..30e81c9ad671 100644\n--- a/jaxlib/setup.py\n+++ b/jaxlib/setup.py\n@@ -68,10 +68,12 @@ def has_ext_modules(self):\n     url='https://github.com/jax-ml/jax',\n     license='Apache-2.0',\n     classifiers=[\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Programming Language :: Python :: 3.12\",\n         \"Programming Language :: Python :: 3.13\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     package_data={\n         'jaxlib': [\ndiff --git a/setup.py b/setup.py\nindex 4c5c86f588c3..ef78b8f6e7ff 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -118,10 +118,12 @@ def load_version_module(pkg_path):\n     url='https://github.com/jax-ml/jax',\n     license='Apache-2.0',\n     classifiers=[\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Programming Language :: Python :: 3.12\",\n         \"Programming Language :: Python :: 3.13\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     zip_safe=False,\n )\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update the trove classifiers for JAX packages.\n\n* Don't tag packages as alpha.\n* Tag free-threading as supported.\n* Update some python version lists.",
        "output": [
            "```diff\nCommit: 0dc70b93f2e13fae5b097837760bd621e746dae7\nDate: 2025-05-22T16:59:16Z\nURL: https://github.com/jax-ml/jax/commit/0dc70b93f2e13fae5b097837760bd621e746dae7\nFiles changed: 4\nAdditions: +9, Deletions: -2\ndiff --git a/jax_plugins/cuda/plugin_setup.py b/jax_plugins/cuda/plugin_setup.py\nindex c8b70408471c..fc467824fe5f 100644\n--- a/jax_plugins/cuda/plugin_setup.py\n+++ b/jax_plugins/cuda/plugin_setup.py\n@@ -78,10 +78,12 @@ def has_ext_modules(self):\n     url=\"https://github.com/jax-ml/jax\",\n     license=\"Apache-2.0\",\n     classifiers=[\n-        \"Development Status :: 3 - Alpha\",\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Programming Language :: Python :: 3.12\",\n+        \"Programming Language :: Python :: 3.13\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     package_data={\n         package_name: [\ndiff --git a/jax_plugins/cuda/setup.py b/jax_plugins/cuda/setup.py\nindex 1ce555978dac..b2c89285e7fd 100644\n--- a/jax_plugins/cuda/setup.py\n+++ b/jax_plugins/cuda/setup.py\n@@ -51,8 +51,9 @@ def load_version_module(pkg_path):\n     url=\"https://github.com/jax-ml/jax\",\n     license=\"Apache-2.0\",\n     classifiers=[\n-        \"Development Status :: 3 - Alpha\",\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     package_data={\n         package_name: [\"xla_cuda_plugin.so\"],\ndiff --git a/jaxlib/setup.py b/jaxlib/setup.py\nindex 8d7933953851..30e81c9ad671 100644\n--- a/jaxlib/setup.py\n+++ b/jaxlib/setup.py\n@@ -68,10 +68,12 @@ def has_ext_modules(self):\n     url='https://github.com/jax-ml/jax',\n     license='Apache-2.0',\n     classifiers=[\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Programming Language :: Python :: 3.12\",\n         \"Programming Language :: Python :: 3.13\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     package_data={\n         'jaxlib': [\ndiff --git a/setup.py b/setup.py\nindex 4c5c86f588c3..ef78b8f6e7ff 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -118,10 +118,12 @@ def load_version_module(pkg_path):\n     url='https://github.com/jax-ml/jax',\n     license='Apache-2.0',\n     classifiers=[\n+        \"Development Status :: 5 - Production/Stable\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Programming Language :: Python :: 3.12\",\n         \"Programming Language :: Python :: 3.13\",\n+        \"Programming Language :: Python :: Free Threading :: 3 - Stable\",\n     ],\n     zip_safe=False,\n )\n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Don't require MOSAIC_GPU_NVSHMEM_SO_PATH to be set\n\nPiperOrigin-RevId: 762008659",
        "output": [
            "```diff\nCommit: 0e77a1617343886387369aa46b21cf8afbc7870c\nDate: 2025-05-22T16:55:12Z\nURL: https://github.com/jax-ml/jax/commit/0e77a1617343886387369aa46b21cf8afbc7870c\nFiles changed: 1\nAdditions: +7, Deletions: -7\ndiff --git a/jax/experimental/mosaic/gpu/core.py b/jax/experimental/mosaic/gpu/core.py\nindex 48a877f8c67a..5e1ed6b88412 100644\n--- a/jax/experimental/mosaic/gpu/core.py\n+++ b/jax/experimental/mosaic/gpu/core.py\n@@ -102,15 +102,15 @@\n def supports_cross_device_collectives():\n   try:\n     nvshmem_bc_path = os.environ[\"MOSAIC_GPU_NVSHMEM_BC_PATH\"]\n-    nvshmem_so_path = os.environ[\"MOSAIC_GPU_NVSHMEM_SO_PATH\"]\n   except KeyError:\n     return False\n-  try:\n-    # This both ensures that the file exists, and it populates the dlopen cache\n-    # helping XLA find the library even if the RPATH is not exactly right...\n-    ctypes.CDLL(nvshmem_so_path)\n-  except OSError:\n-    return False\n+  if nvshmem_so_path := os.environ.get(\"MOSAIC_GPU_NVSHMEM_SO_PATH\", \"\"):\n+    try:\n+      # This both ensures that the file exists, and it populates the dlopen\n+      # cache, helping XLA find the library even if the RPATH is not right...\n+      ctypes.CDLL(nvshmem_so_path)\n+    except OSError:\n+      return False\n   xla_flags = os.environ.get(\"XLA_FLAGS\", \"\")\n   return (\n       os.path.exists(nvshmem_bc_path)\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] (Slightly) Clean up the collective matmul test by using explicit sharding\n\nNot a huge difference, but it's a bit nicer.\n\nPiperOrigin-RevId: 761984854",
        "output": [
            "```diff\nCommit: 44d2cc927989b8a6a4681ba49c49f6ac8efba910\nDate: 2025-05-22T15:54:43Z\nURL: https://github.com/jax-ml/jax/commit/44d2cc927989b8a6a4681ba49c49f6ac8efba910\nFiles changed: 2\nAdditions: +12, Deletions: -16\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex cf0d46639559..e4a308c2f10b 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -864,6 +864,7 @@ jax_multiplatform_test(\n         \"notap\",\n     ],\n     deps = [\n+        \"//jax:experimental\",\n         \"//jax:pallas\",\n         \"//jax:pallas_experimental_gpu_ops\",\n         \"//jax:pallas_mosaic_gpu\",\ndiff --git a/tests/pallas/mgpu_collective_matmul_test.py b/tests/pallas/mgpu_collective_matmul_test.py\nindex e0ced79801d8..386162b1992c 100644\n--- a/tests/pallas/mgpu_collective_matmul_test.py\n+++ b/tests/pallas/mgpu_collective_matmul_test.py\n@@ -27,6 +27,7 @@\n from jax._src.pallas import pallas_call\n from jax.experimental.mosaic import gpu as mgpu\n from jax.experimental.pallas.ops.gpu import collective_matmul_mgpu\n+from jax.experimental import shard\n import jax.numpy as jnp\n import numpy as np\n \n@@ -51,8 +52,13 @@ def setUp(self):\n     if os.environ.get(\"XLA_PYTHON_CLIENT_ALLOCATOR\", \"\") == \"platform\":\n       self.skipTest(\"NVSHMEM doesn't work with the platform allocator.\")\n     context_stack = contextlib.ExitStack()\n-    context_stack.enter_context(pallas_call._PALLAS_USE_MOSAIC_GPU(True))\n     self.addCleanup(context_stack.close)\n+    context_stack.enter_context(pallas_call._PALLAS_USE_MOSAIC_GPU(True))\n+    num_devices = jax.device_count()\n+    mesh = jax.make_mesh(\n+        (num_devices,), (\"x\",), axis_types=(jax.sharding.AxisType.Explicit,)\n+    )\n+    context_stack.enter_context(jax.sharding.use_mesh(mesh))\n \n   @parameterized.product(\n       m_shard=(1024, 8192),\n@@ -90,28 +96,17 @@ def test_all_gather_lhs_matmul(\n     k1, k2 = random.split(random.key(1234), num=2)\n     lhs = random.normal(k1, (num_devices * m_shard, k), dtype)\n     rhs = random.normal(k2, (k, num_devices * n_shard), dtype)\n-\n-    mesh = jax.sharding.Mesh(jax.devices(), [\"x\"])\n-    lhs = jax.device_put(lhs, jax.sharding.NamedSharding(mesh, P(\"x\", None)))\n-    rhs = jax.device_put(rhs, jax.sharding.NamedSharding(mesh, P(None, \"x\")))\n+    lhs = shard.reshard(lhs, P(\"x\", None))\n+    rhs = shard.reshard(rhs, P(None, \"x\"))\n \n     def run(body):\n       out = jax.jit(\n-          jax.shard_map(\n-              body,\n-              mesh=mesh,\n-              in_specs=(P(\"x\", None), P(None, \"x\")),\n-              out_specs=P(None, \"x\"),\n-              check_vma=False,\n-          )\n+          jax.shard_map(body, out_specs=P(None, \"x\"), check_vma=False)\n       )(lhs, rhs)\n       # Gather output, for NumPy comparison on the host.\n       out = jax.shard_map(\n           lambda x: lax.all_gather(x, \"x\", axis=1, tiled=True),\n-          mesh=mesh,\n-          in_specs=P(None, \"x\"),\n-          out_specs=P(None),\n-          check_vma=False,\n+          out_specs=P(None), check_vma=False,\n       )(out)\n       return out\n \n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Disable the GPU distributed test when the platform allocator is used\n\nAnd, if we're only running this test, try to override the flag.\n\nPiperOrigin-RevId: 761950583",
        "output": [
            "```diff\nCommit: 4c83b08980f33f208d55667135e9fbe2cc3e8938\nDate: 2025-05-22T14:13:02Z\nURL: https://github.com/jax-ml/jax/commit/4c83b08980f33f208d55667135e9fbe2cc3e8938\nFiles changed: 4\nAdditions: +24, Deletions: -1\ndiff --git a/.github/workflows/bazel_optional_h100_b200.yml b/.github/workflows/bazel_optional_h100_b200.yml\nindex 0c73b238505e..7381ce6d80bf 100644\n--- a/.github/workflows/bazel_optional_h100_b200.yml\n+++ b/.github/workflows/bazel_optional_h100_b200.yml\n@@ -48,6 +48,7 @@ jobs:\n             --test_env=XLA_PYTHON_CLIENT_ALLOCATOR=platform \\\n             --run_under \"$(pwd)/build/parallel_accelerator_execute.sh\" \\\n             --test_output=errors \\\n+            --test_tag_filters=-multiaccelerator \\\n             --test_env=JAX_ACCELERATOR_COUNT=1 \\\n             --test_env=JAX_TESTS_PER_ACCELERATOR=8 \\\n             --strategy=TestRunner=local \\\n@@ -102,7 +103,6 @@ jobs:\n             //tests/pallas:gpu_tests \\\n             //tests:array_interoperability_test_gpu \\\n             //tests:cudnn_fusion_test_gpu \\\n-            //tests:fused_attention_stablehlo_test_gpu\n             //tests:fused_attention_stablehlo_test_gpu \\\n             //tests:gpu_tests \\\n             //tests:python_callback_test_gpu \\\ndiff --git a/jax/experimental/mosaic/gpu/core.py b/jax/experimental/mosaic/gpu/core.py\nindex c20c5252a27f..48a877f8c67a 100644\n--- a/jax/experimental/mosaic/gpu/core.py\n+++ b/jax/experimental/mosaic/gpu/core.py\n@@ -102,8 +102,15 @@\n def supports_cross_device_collectives():\n   try:\n     nvshmem_bc_path = os.environ[\"MOSAIC_GPU_NVSHMEM_BC_PATH\"]\n+    nvshmem_so_path = os.environ[\"MOSAIC_GPU_NVSHMEM_SO_PATH\"]\n   except KeyError:\n     return False\n+  try:\n+    # This both ensures that the file exists, and it populates the dlopen cache\n+    # helping XLA find the library even if the RPATH is not exactly right...\n+    ctypes.CDLL(nvshmem_so_path)\n+  except OSError:\n+    return False\n   xla_flags = os.environ.get(\"XLA_FLAGS\", \"\")\n   return (\n       os.path.exists(nvshmem_bc_path)\ndiff --git a/tests/pallas/gpu_pallas_distributed_test.py b/tests/pallas/gpu_pallas_distributed_test.py\nindex d862e6b9b819..3aeee352ff6d 100644\n--- a/tests/pallas/gpu_pallas_distributed_test.py\n+++ b/tests/pallas/gpu_pallas_distributed_test.py\n@@ -15,6 +15,8 @@\n \"\"\"Tests for distributed pallas GPU operations.\"\"\"\n \n import functools\n+import os\n+\n import jax\n from jax import lax\n from jax._src import test_util as jtu\n@@ -41,6 +43,8 @@ def setUp(self):\n       self.skipTest(\"NVSHMEM library unavailable.\")\n     if jax.process_count() == 1:\n       self.skipTest(\"Test requires multiple processes.\")\n+    if os.environ.get(\"XLA_PYTHON_CLIENT_ALLOCATOR\", \"\") == \"platform\":\n+      self.skipTest(\"NVSHMEM doesn't work with the platform allocator.\")\n     super().setUp()\n \n   def test_basic_remote_dma(self):\n@@ -114,4 +118,9 @@ def kernel(y_ref, sem):\n \n \n if __name__ == '__main__':\n+  # This test doesn't work with the platform allocator, so we override it\n+  # if it's ran alone. If it's part of a larger test suite and the platform\n+  # allocator is used, setUp will skip the test.\n+  os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.01'\n+  os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'default'\n   jt_multiprocess.main()\ndiff --git a/tests/pallas/mgpu_collective_matmul_test.py b/tests/pallas/mgpu_collective_matmul_test.py\nindex bbc50d39d7f6..e0ced79801d8 100644\n--- a/tests/pallas/mgpu_collective_matmul_test.py\n+++ b/tests/pallas/mgpu_collective_matmul_test.py\n@@ -48,6 +48,8 @@ def setUp(self):\n       self.skipTest(\"NVSHMEM library unavailable.\")\n     if jax.process_count() == 1:\n       self.skipTest(\"Test requires multiple processes.\")\n+    if os.environ.get(\"XLA_PYTHON_CLIENT_ALLOCATOR\", \"\") == \"platform\":\n+      self.skipTest(\"NVSHMEM doesn't work with the platform allocator.\")\n     context_stack = contextlib.ExitStack()\n     context_stack.enter_context(pallas_call._PALLAS_USE_MOSAIC_GPU(True))\n     self.addCleanup(context_stack.close)\n@@ -128,6 +130,11 @@ def run(body):\n \n \n if __name__ == \"__main__\":\n+  # This test doesn't work with the platform allocator, so we override it\n+  # if it's ran alone. If it's part of a larger test suite and the platform\n+  # allocator is used, setUp will skip the test.\n+  os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.01\"\n+  os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"default\"\n   os.environ[\"XLA_FLAGS\"] = (\n       os.environ.get(\"XLA_FLAGS\", \"\") + \" --xla_gpu_autotune_level=0\"\n   )\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Add a first prototype of an all_gather collective matmul kernel\n\nIt's not very optimized at the moment and is unlikely to outperform the baseline\nof raw all_gather + matmul, but it computes the right numbers. We are already aware\nof a few places that could be optimized and we'll start rolling them out soon.\n\nPiperOrigin-RevId: 761939624",
        "output": [
            "```diff\nCommit: 7014bde5a57931054ea623ce5a0d3dc85090ce4e\nDate: 2025-05-22T13:36:46Z\nURL: https://github.com/jax-ml/jax/commit/7014bde5a57931054ea623ce5a0d3dc85090ce4e\nFiles changed: 4\nAdditions: +342, Deletions: -1\ndiff --git a/jax/_src/pallas/core.py b/jax/_src/pallas/core.py\nindex fe755d61a310..13c634eb395f 100644\n--- a/jax/_src/pallas/core.py\n+++ b/jax/_src/pallas/core.py\n@@ -971,7 +971,7 @@ def _convert_block_spec_to_block_mapping(\n class ScratchShape(Protocol):\n   def get_array_aval(self) -> jax_core.AbstractValue:\n     ...\n-  def get_ref_aval(self) -> state.AbstractRef:\n+  def get_ref_aval(self) -> state.AbstractRef | TransformedRef:\n     ...\n \n \ndiff --git a/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\nnew file mode 100644\nindex 000000000000..a6c372f2cee7\n--- /dev/null\n+++ b/jax/experimental/pallas/ops/gpu/collective_matmul_mgpu.py\n@@ -0,0 +1,178 @@\n+# Copyright 2025 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"A collective matmul kernel implemented using Mosaic GPU.\"\"\"\n+\n+import functools\n+import jax\n+from jax import lax\n+from jax.experimental import pallas as pl\n+from jax.experimental.pallas import mosaic_gpu as plgpu\n+import jax.numpy as jnp\n+\n+\n+def _find_swizzle(dim_size_bits: int, what: str):\n+  for swizzle_bytes in (128, 64, 32, 16):\n+    if dim_size_bits % (swizzle_bytes * 8) == 0:\n+      return swizzle_bytes\n+  raise ValueError(\n+      f\"No valid out swizzle for {what}: its minor dimension has\"\n+      f\" {dim_size_bits} bits, which is not a multiple of 128\"\n+  )\n+\n+\n+# TODO(apaszke): Add grid tiling\n+def all_gather_lhs_matmul(\n+    lhs: jax.Array,\n+    rhs: jax.Array,\n+    axis_name,\n+    *,\n+    block_m: int,\n+    block_n: int,\n+    block_k: int,\n+    max_concurrent_steps: int,\n+) -> jax.Array:\n+  if (num_devices := jax.device_count()) != jax.process_count():\n+    raise ValueError(\"The kernel only supports one device per process\")\n+  if (axis_size := lax.axis_size(axis_name)) != num_devices:\n+    raise ValueError(\"The kernel can only work over all devices in a Mesh.\")\n+  if max_concurrent_steps < 2:\n+    raise ValueError(\"max_concurrent_steps must be >= 2\")\n+\n+  num_sms = 132  # There are 132 SMs on a H100 SXM GPU.\n+\n+  m_shard, k = lhs.shape\n+  k2, n_shard = rhs.shape\n+  if k != k2:\n+    raise ValueError(\n+        f\"lhs and rhs must have the same contraction size, got {k} and {k2}.\"\n+    )\n+  if (element_type := lhs.dtype) != rhs.dtype:\n+    raise ValueError(\n+        f\"lhs and rhs must have the same element type, got {element_type} and\"\n+        f\" {rhs.dtype}.\"\n+    )\n+  if k % block_k != 0:\n+    raise NotImplementedError(f\"k={k} must be a multiple of block_k={block_k}\")\n+  if m_shard % block_m != 0:\n+    raise NotImplementedError(f\"m_shard={m_shard} must be a multiple of block_m={block_m}\")\n+  if n_shard % block_n != 0:\n+    raise NotImplementedError(f\"n_shard={n_shard} must be a multiple of block_n={block_n}\")\n+  if n_shard != block_n:\n+    raise NotImplementedError(\n+        f\"n_shard={n_shard} must be equal to block_n={block_n}\"\n+    )\n+\n+  swizzle = min(\n+      _find_swizzle(block_k * jnp.finfo(element_type).bits, \"lhs\"),\n+      _find_swizzle(block_n * jnp.finfo(element_type).bits, \"rhs\"),\n+  )\n+  transforms = (\n+      plgpu.TilingTransform((8, swizzle // jnp.dtype(element_type).itemsize)),\n+      plgpu.SwizzleTransform(swizzle),\n+  )\n+\n+  def kernel_body(lhs_ref, rhs_ref, out_ref, scratch_ref, capacity_sem, received_sem):\n+    sm_id = lax.axis_index('sm')\n+    scratch_ref = scratch_ref.at[sm_id]\n+\n+    dev_id = lax.axis_index(axis_name)\n+    send_dev_id = lax.rem(dev_id + axis_size - 1, axis_size)\n+    recv_dev_id = lax.rem(dev_id + 1, axis_size)\n+    # NOTE: Technically we should signal the recv_dev_id (and our signal would\n+    # be received from send_dev_id), but if everyone signals in a ring after a\n+    # barrier then it's equivalent to a local signal.\n+    pl.semaphore_signal(capacity_sem)\n+    send_scratch_ref = plgpu.remote_ref(\n+        scratch_ref, send_dev_id, device_id_type=pl.DeviceIdType.LOGICAL\n+    )\n+\n+    def m_loop(mi, _):\n+      mi = mi * lax.axis_size('sm') + sm_id\n+      m_tile_slice = pl.ds(mi * block_m, block_m)\n+\n+      # For some reason ptxas spills if we unroll the loop over k\n+      copy_block = 32\n+      def k_copy_loop(ki, _):\n+        k_slice = pl.ds(ki * copy_block, copy_block)\n+        scratch_ref[0, :, k_slice] = lhs_ref[m_tile_slice, k_slice]\n+      jax.lax.fori_loop(0, k // copy_block, k_copy_loop, None)\n+\n+      def device_loop(device_offset, _):\n+        # Loop invariant: scratch_ref.at[scratch_slot] is ready to be used\n+        # We're double buffering the scratch space. At each step, we read from\n+        # scratch_ref.at[scratch_slot] and write to scratch_ref.at[next_scratch_slot]\n+        # located on the send_dev_id. We swap the slots after completing a step,\n+        # which lets us overlap the copy with compute.\n+        scratch_slot = lax.rem(device_offset, 2)\n+        next_scratch_slot = 1 - scratch_slot\n+\n+        @functools.partial(\n+            pl.run_scoped,\n+            acc_ref=plgpu.ACC((block_m, block_n)),\n+            out_smem=plgpu.SMEM((block_m, block_n), jnp.float16, transforms=transforms),\n+        )\n+        def _(acc_ref, out_smem):\n+          pl.semaphore_wait(capacity_sem)\n+          @functools.partial(\n+              plgpu.emit_pipeline,\n+              grid=(k // block_k,),\n+              in_specs=[\n+                  plgpu.BlockSpec((block_m, block_k), lambda k: (0, k), transforms=transforms),\n+                  plgpu.BlockSpec((block_k, block_n), lambda k: (k, 0), transforms=transforms),\n+              ],\n+              max_concurrent_steps=max_concurrent_steps,\n+              delay_release=1,\n+          )\n+          def k_loop(idxs, lhs_smem, rhs_smem):\n+            (ki,) = idxs\n+            plgpu.wgmma(acc_ref, lhs_smem, rhs_smem)\n+            k_slice = pl.ds(ki * block_k, block_k)\n+            # TODO(apaszke): No need to send on the last step\n+            # TODO(apaszke): Use an async copy. This is uncoalesced.\n+            send_scratch_ref[next_scratch_slot, :, k_slice] = lhs_smem[...]\n+          k_loop(scratch_ref.at[scratch_slot], rhs_ref)\n+          # TODO(apaszke): Both of those semaphores perform a .sys release.\n+          # This is very expensive and we should only do a single .sys fence.\n+          pl.semaphore_signal(capacity_sem, device_id=recv_dev_id, device_id_type=pl.DeviceIdType.LOGICAL)\n+          pl.semaphore_signal(received_sem, device_id=send_dev_id, device_id_type=pl.DeviceIdType.LOGICAL)\n+          # Make sure all TMAs have read SMEM before we overwrite it.\n+          plgpu.wait_smem_to_gmem(0, wait_read_only=True)\n+          out_smem[...] = acc_ref[...].astype(out_smem.dtype)\n+          plgpu.commit_smem()\n+          device_m_slice = pl.ds(\n+              lax.rem(device_offset + dev_id, num_devices) * m_shard, block_m\n+          )\n+          plgpu.copy_smem_to_gmem(\n+              out_smem, out_ref.at[device_m_slice].at[m_tile_slice]\n+          )\n+          # Wait for the next scratch to arrive --- see the loop invariant.\n+          pl.semaphore_wait(received_sem)\n+      jax.lax.fori_loop(0, num_devices, device_loop, None)\n+    grid_size = m_shard // block_m\n+    m_steps = grid_size // num_sms + jnp.int32(sm_id < grid_size % num_sms)\n+    # TODO(apaszke): Use the ND-loop helper.\n+    jax.lax.fori_loop(0, m_steps, m_loop, None)\n+\n+  result, _ = plgpu.kernel(\n+      kernel_body,\n+      out_shape=[jax.ShapeDtypeStruct((axis_size * m_shard, n_shard), jnp.float16),\n+                  jax.ShapeDtypeStruct((num_sms, 2, block_m, k), jnp.float16)],\n+      scratch_shapes=[\n+          plgpu.SemaphoreType.REGULAR, plgpu.SemaphoreType.REGULAR,\n+      ],\n+      grid=(num_sms,),\n+      grid_names=('sm',),\n+  )(lhs, rhs)\n+  return result\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex c45e52b1fe88..cf0d46639559 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -842,6 +842,35 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"mgpu_collective_matmul_test\",\n+    srcs = [\"mgpu_collective_matmul_test.py\"],\n+    args = [\n+        \"--num_processes=2\",\n+        \"--gpus_per_process=1\",\n+    ],\n+    enable_backends = [],\n+    enable_configs = [\n+        \"gpu_h100x2\",\n+    ],\n+    env = {\n+        \"XLA_FLAGS\": \"--xla_gpu_experimental_enable_nvshmem=true\",\n+        \"JAX_PALLAS_USE_MOSAIC_GPU\": \"1\",\n+    },\n+    shard_count = 4,\n+    tags = [\n+        \"manual\",\n+        \"multiaccelerator\",\n+        \"notap\",\n+    ],\n+    deps = [\n+        \"//jax:pallas\",\n+        \"//jax:pallas_experimental_gpu_ops\",\n+        \"//jax:pallas_mosaic_gpu\",\n+        \"//jax:test_multiprocess\",\n+    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+)\n+\n jax_multiplatform_test(\n     name = \"fuser_block_spec_test\",\n     srcs = [\ndiff --git a/tests/pallas/mgpu_collective_matmul_test.py b/tests/pallas/mgpu_collective_matmul_test.py\nnew file mode 100644\nindex 000000000000..bbc50d39d7f6\n--- /dev/null\n+++ b/tests/pallas/mgpu_collective_matmul_test.py\n@@ -0,0 +1,134 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Test different parameterizations of our Mosaic GPU collective matmul.\"\"\"\n+\n+import contextlib\n+import functools\n+import os\n+\n+from absl.testing import parameterized  # pylint: disable=g-multiple-import\n+import jax\n+from jax import lax\n+from jax import random\n+from jax._src import test_multiprocess as jt_multiprocess\n+from jax._src import test_util as jtu\n+from jax._src.pallas import pallas_call\n+from jax.experimental.mosaic import gpu as mgpu\n+from jax.experimental.pallas.ops.gpu import collective_matmul_mgpu\n+import jax.numpy as jnp\n+import numpy as np\n+\n+\n+P = jax.sharding.PartitionSpec\n+\n+\n+@jtu.with_config(jax_traceback_filtering=\"off\")\n+class CollectiveMatmulTestCase(jtu.JaxTestCase):\n+\n+  def setUp(self):\n+    super().setUp()\n+    if collective_matmul_mgpu is None:\n+      self.skipTest(\"Mosaic GPU not available.\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_equal(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability sm90a\")\n+    if not mgpu.supports_cross_device_collectives():\n+      self.skipTest(\"NVSHMEM library unavailable.\")\n+    if jax.process_count() == 1:\n+      self.skipTest(\"Test requires multiple processes.\")\n+    context_stack = contextlib.ExitStack()\n+    context_stack.enter_context(pallas_call._PALLAS_USE_MOSAIC_GPU(True))\n+    self.addCleanup(context_stack.close)\n+\n+  @parameterized.product(\n+      m_shard=(1024, 8192),\n+      n_shard=(64, 128, 192),\n+      k=(256, 8192),\n+      block_m=(64, 128, 192),\n+      block_n=(64, 128, 192),\n+      block_k=(64, 128),\n+      max_concurrent_steps=(2, 4),\n+  )\n+  def test_all_gather_lhs_matmul(\n+      self,\n+      m_shard,\n+      n_shard,\n+      k,\n+      block_m,\n+      block_n,\n+      block_k,\n+      max_concurrent_steps,\n+  ):\n+    num_devices = jax.device_count()\n+    dtype = jnp.float16\n+    lhs_smem_size = block_m * block_k * max_concurrent_steps * 2\n+    rhs_smem_size = block_k * block_n * max_concurrent_steps * 2\n+    # H100 SMEM limit is 228kB.\n+    if lhs_smem_size + rhs_smem_size > 228_000:\n+      self.skipTest(\"This configuration requires too much SMEM.\")\n+    if n_shard != block_n:\n+      self.skipTest(\"n_shard must be equal to block_n for now.\")\n+    if n_shard % block_n:\n+      self.skipTest(\"n_shard must be divisble by block_n for now.\")\n+    if m_shard % block_m:\n+      self.skipTest(\"m_shard must be divisible by block_m for now.\")\n+\n+    k1, k2 = random.split(random.key(1234), num=2)\n+    lhs = random.normal(k1, (num_devices * m_shard, k), dtype)\n+    rhs = random.normal(k2, (k, num_devices * n_shard), dtype)\n+\n+    mesh = jax.sharding.Mesh(jax.devices(), [\"x\"])\n+    lhs = jax.device_put(lhs, jax.sharding.NamedSharding(mesh, P(\"x\", None)))\n+    rhs = jax.device_put(rhs, jax.sharding.NamedSharding(mesh, P(None, \"x\")))\n+\n+    def run(body):\n+      out = jax.jit(\n+          jax.shard_map(\n+              body,\n+              mesh=mesh,\n+              in_specs=(P(\"x\", None), P(None, \"x\")),\n+              out_specs=P(None, \"x\"),\n+              check_vma=False,\n+          )\n+      )(lhs, rhs)\n+      # Gather output, for NumPy comparison on the host.\n+      out = jax.shard_map(\n+          lambda x: lax.all_gather(x, \"x\", axis=1, tiled=True),\n+          mesh=mesh,\n+          in_specs=P(None, \"x\"),\n+          out_specs=P(None),\n+          check_vma=False,\n+      )(out)\n+      return out\n+\n+    out = run(\n+        functools.partial(\n+            collective_matmul_mgpu.all_gather_lhs_matmul,\n+            axis_name=\"x\",\n+            block_m=block_m,\n+            block_n=block_n,\n+            block_k=block_k,\n+            max_concurrent_steps=max_concurrent_steps,\n+        )\n+    )\n+    ref_out = run(lambda x, y: lax.all_gather(x, \"x\", axis=0, tiled=True) @ y)\n+    np.testing.assert_allclose(out, ref_out)\n+\n+\n+if __name__ == \"__main__\":\n+  os.environ[\"XLA_FLAGS\"] = (\n+      os.environ.get(\"XLA_FLAGS\", \"\") + \" --xla_gpu_autotune_level=0\"\n+  )\n+  jt_multiprocess.main()\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas:MGPU] Make semaphores compatible with plgpu.kernel and the profiler\n\nThe previous code was overly specific to pl.pallas_call and did not work with\nplgpu.kernel at all. Now, semaphores can be allocated using run_scoped, which also\nhas the interesting side effect of the allocations being collective within\neach program. For a persistent kernel that means that a program/block can communicate\nwith programs/blocks on other devices that have the same program ID, but it is\ncurrently impossible to e.g. synchronize all programs in a grid. We don't have a\nuse case for it now, so we can add it later.\n\nPiperOrigin-RevId: 761885876",
        "output": [
            "```diff\nCommit: 925e705186ccc5412213c42901099511915096a8\nDate: 2025-05-22T10:15:05Z\nURL: https://github.com/jax-ml/jax/commit/925e705186ccc5412213c42901099511915096a8\nFiles changed: 4\nAdditions: +138, Deletions: -66\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex e212f9770a94..2f81ecb969ac 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -105,6 +105,7 @@ class Resources:\n   barrier_counts: collections.Counter[AnyBarrier] = dataclasses.field(\n       default_factory=collections.Counter\n   )\n+  gmem_semaphores: int = 0\n \n   def __post_init__(self):\n     object.__setattr__(\n@@ -132,6 +133,7 @@ def __add__(self, other: Resources) -> Resources:\n         smem_scratch_bytes=self.smem_scratch_bytes + other.smem_scratch_bytes,\n         tmem_scratch_cols=self.tmem_scratch_cols + other.tmem_scratch_cols,\n         barrier_counts=self.barrier_counts + other.barrier_counts,\n+        gmem_semaphores=self.gmem_semaphores + other.gmem_semaphores,\n     )\n \n   def __or__(self, other: Resources) -> Resources:\n@@ -143,6 +145,7 @@ def __or__(self, other: Resources) -> Resources:\n             self.tmem_scratch_cols, other.tmem_scratch_cols\n         ),\n         barrier_counts=self.barrier_counts | other.barrier_counts,\n+        gmem_semaphores=max(self.gmem_semaphores, other.gmem_semaphores),\n     )\n \n \n@@ -266,6 +269,8 @@ def _run_scoped_resource_estimator(\n     elif aval.memory_space == gpu_core.REGS:\n       # Don't need to allocate anything.\n       pass\n+    elif aval.memory_space == gpu_core.GMEM and jnp.issubdtype(aval.dtype, pallas_core.semaphore):\n+      rs += Resources(gmem_semaphores=math.prod(aval.shape))\n     else:\n       raise NotImplementedError(\n           f\"Unsupported memory space: {aval.memory_space}\")\n@@ -312,6 +317,8 @@ class ModuleContext:\n   tmem_requested_cols: int\n   tmem_used_cols: int\n   tmem_base_ptr: ir.Value\n+  gmem_used_semaphores: int\n+  gmem_semaphore_base_ptr: ir.Value | None\n   runtime_barriers: MutableMapping[AnyBarrier, MutableSequence[AnyBarrierRef]]\n   name_stack: source_info_util.NameStack\n   traceback_caches: mlir.TracebackCaches\n@@ -351,6 +358,21 @@ def reserve_barrier(\n     yield barrier\n     available.append(barrier)\n \n+  @contextlib.contextmanager\n+  def reserve_semaphores(\n+      self, shape: tuple[int, ...]\n+  ):\n+    allocated_sems = math.prod(shape)\n+    ref = mgpu.memref_slice(\n+        self.gmem_semaphore_base_ptr,\n+        mgpu.ds(self.gmem_used_semaphores, allocated_sems),\n+    )\n+    ref = mgpu.memref_reshape(ref, shape)\n+    self.gmem_used_semaphores += allocated_sems\n+    yield ref\n+    # TODO: In debug mode verify the values of all semaphores are again 0\n+    self.gmem_used_semaphores -= allocated_sems\n+\n   @contextlib.contextmanager\n   def alloc_tmem(\n       self,\n@@ -640,42 +662,15 @@ def ref_for_aval(aval: jax_core.AbstractValue):\n     else:\n       return gpu_core.SMEM(aval.shape, aval.dtype)\n \n-  sem_placeholder = None\n-  semaphore_ref_avals = []\n-  scratch_avals = []\n-  # Need to unzip semaphores\n-  for v in jaxpr.invars[grid_mapping.slice_scratch_ops]:\n-    aval = v.aval\n-    if (isinstance(aval, pallas_core.AbstractMemoryRef) and\n-        jnp.issubdtype(aval.dtype, pallas_core.semaphore_dtype)):\n-      if aval.memory_space != gpu_core.GMEM:\n-        raise ValueError(\n-            \"Only GMEM memory space is supported for semaphores in Mosaic GPU.\"\n-        )\n-      semaphore_ref_avals.append(aval)\n-      scratch_avals.append(sem_placeholder)\n-    else:\n-      scratch_avals.append(aval)\n-\n   def pipeline_fn(*refs):\n-    sem_refs = []\n-    if semaphore_ref_avals:\n-      refs, sem_refs = util.split_list(refs, [-len(semaphore_ref_avals)])\n     primitives.run_scoped(\n-        functools.partial(scoped_pipeline_fn, *refs, sem_refs=sem_refs),\n-        scratch_refs=[\n-            ref_for_aval(aval) if aval is not sem_placeholder else aval\n-            for aval in scratch_avals\n-        ],\n+        functools.partial(scoped_pipeline_fn, *refs),\n+        scratch_refs=[ref_for_aval(v.aval) for v in jaxpr.invars[grid_mapping.slice_scratch_ops]],\n         collective_axes=thread_axis,  # scratch_refs are shared across threads\n     )\n     return ()  # ``wrap_init`` does not support functions returning None.\n \n-  def scoped_pipeline_fn(*refs, sem_refs, scratch_refs):\n-    sem_refs_it = iter(sem_refs)\n-    scratch_refs = [\n-        next(sem_refs_it) if r is sem_placeholder else r for r in scratch_refs\n-    ]\n+  def scoped_pipeline_fn(*refs, scratch_refs):\n     def body_fn(indices, *refs):\n       program_ids_template = util.merge_lists(\n           which_parallel, indices, [None] * sum(which_parallel)\n@@ -708,7 +703,7 @@ def body_fn(indices, *refs):\n                 bm.array_shape_dtype.shape, bm.array_shape_dtype.dtype\n             ).get_ref_aval()\n             for bm in block_mappings\n-        ] + semaphore_ref_avals,\n+        ],\n     )\n     assert not new_consts\n \n@@ -726,10 +721,6 @@ def body_fn(indices, *refs):\n         gpu_mesh.cluster if gpu_mesh is not None else (),\n         [bm.array_shape_dtype for bm in in_block_mappings],\n         [bm.array_shape_dtype for bm in out_block_mappings],\n-        [\n-            jax.ShapeDtypeStruct(r.shape, np.dtype(np.int32))\n-            for r in semaphore_ref_avals\n-        ],\n         new_jaxpr,\n         params,\n         new_consts,\n@@ -744,7 +735,6 @@ def lower_jaxpr_to_module(\n     cluster: Sequence[int],\n     in_shapes: Sequence[jax.ShapeDtypeStruct],\n     out_shapes: Sequence[jax.ShapeDtypeStruct],\n-    gmem_scratch_shapes: Sequence[jax.ShapeDtypeStruct],\n     jaxpr: jax_core.Jaxpr,\n     params: gpu_core.CompilerParams,\n     consts=(),\n@@ -767,13 +757,31 @@ def lower_jaxpr_to_module(\n     squashed_dims = grid[:-2]\n     parallel_grid = (math.prod(grid[:-2]), *grid[-2:])\n \n+  rs = _estimate_resources(\n+      ResourceEstimatorContext(\n+          axis_names=axis_names, lowering_semantics=lowering_semantics\n+      ),\n+      jaxpr,\n+  )\n+\n   def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n     *buffers_gmem, (runtime_smem, runtime_barriers, runtime_tmem) = buffers\n-    if gmem_scratch_shapes:\n-      in_buffers, _, out_scratch_buffers = util.split_list(\n-          buffers_gmem, [len(in_shapes), len(gmem_scratch_shapes)]\n+    gmem_semaphores = None\n+    if rs.gmem_semaphores:\n+      # Extract the semaphores local to the current block.\n+      index = ir.IndexType.get()\n+      block_idx = arith_dialect.index_castui(index, mgpu_utils.block_idx())\n+      gmem_semaphores = mgpu.memref_slice(\n+          buffers_gmem[-1],\n+          mgpu.ds(\n+              arith_dialect.muli(\n+                  block_idx, arith_dialect.constant(index, rs.gmem_semaphores)\n+              ),\n+              rs.gmem_semaphores,\n+          ),\n       )\n-      buffers_gmem = in_buffers + out_scratch_buffers\n+      # The semaphore buffer is an aliased input/output, so we need to skip it twice.\n+      buffers_gmem = buffers_gmem[:len(in_shapes)] + buffers_gmem[-len(out_shapes) - 1:-1]\n \n     grouped_barriers = collections.defaultdict(list)\n     for barrier, barrier_ref in zip(rs.barriers, runtime_barriers):\n@@ -804,6 +812,8 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n         tmem_requested_cols=tmem_cols,\n         tmem_used_cols=0,\n         tmem_base_ptr=runtime_tmem.address if runtime_tmem else None,\n+        gmem_used_semaphores=0,\n+        gmem_semaphore_base_ptr=gmem_semaphores,\n         runtime_barriers=grouped_barriers,\n         name_stack=source_info_util.NameStack(),\n         traceback_caches=mlir.TracebackCaches(),\n@@ -817,13 +827,6 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n         module_ctx, launch_ctx, jaxpr, buffers_gmem, consts\n     )\n \n-  rs = _estimate_resources(\n-      ResourceEstimatorContext(\n-          axis_names=axis_names, lowering_semantics=lowering_semantics\n-      ),\n-      jaxpr,\n-  )\n-\n   scratch_buffers = [\n       jax.ShapeDtypeStruct(shape=[rs.smem_scratch_bytes], dtype=np.int8),\n       rs.barriers,\n@@ -842,14 +845,24 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n     # Each range is 2 events, each event is 4 bytes.\n     prof_spec = mgpu_profiler.ProfilerSpec(params.profile_space * 2 * 4)\n     prof_ctx = ProfilerContext(params.profile_dir, prof_spec)\n+  mgpu_grid = tuple(map(operator.mul, parallel_grid, cluster))\n+  semaphores_shape = ()\n+  if rs.gmem_semaphores:\n+    semaphores_shape = (\n+        jax.ShapeDtypeStruct(\n+            shape=(math.prod(mgpu_grid) * rs.gmem_semaphores,), dtype=np.int32\n+        ),\n+    )\n+  # NOTE: new_out_shapes has out_shapes, then semaphores_shape and\n+  # optionally the profiler buffer.\n   module, new_out_shapes, _, launch_ctx = (\n       mgpu_core._lower_as_gpu_kernel(\n           body,\n-          grid=tuple(map(operator.mul, parallel_grid, cluster)),\n+          grid=mgpu_grid,\n           cluster=cluster,\n           block=block,\n-          in_shapes=(*in_shapes, *gmem_scratch_shapes),\n-          out_shape=(*out_shapes, *gmem_scratch_shapes),\n+          in_shapes=(*in_shapes, *semaphores_shape),\n+          out_shape=(*out_shapes, *semaphores_shape),\n           smem_scratch_shape=scratch_buffers,\n           lowering_semantics=lowering_semantics,\n           module_name=mlir.sanitize_name(debug_info.func_name),\n@@ -871,11 +884,8 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n \n   launch_ctx.scratch.finalize_size()\n \n-  if gmem_scratch_shapes:\n-    new_out_shapes = new_out_shapes[:-len(gmem_scratch_shapes)]\n-\n   return LoweringResult(\n-      module, parallel_grid, block, new_out_shapes, prof_ctx, tuple(gmem_scratch_shapes)\n+      module, parallel_grid, block, new_out_shapes, prof_ctx, semaphores_shape\n   )\n \n \n@@ -2283,6 +2293,12 @@ def _run_scoped_lowering_rule(\n         )\n         input_refs.append(input_ref)\n         should_discharge.append(False)\n+      elif aval.memory_space == gpu_core.GMEM and jnp.issubdtype(aval.dtype, pallas_core.semaphore):\n+        input_ref = alloc_stack.enter_context(\n+            ctx.module_ctx.reserve_semaphores(aval.shape)\n+        )\n+        input_refs.append(input_ref)\n+        should_discharge.append(False)\n       else:\n         raise ValueError(f\"Can't convert to ref: {aval}\")\n \ndiff --git a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\nindex ef1ba37f0f5c..a14ccbb7daa9 100644\n--- a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n+++ b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n@@ -85,12 +85,16 @@ def pallas_call_lowering(\n   new_avals_out = list(map(_as_shaped_array, lowering_result.new_out_shapes))\n   scratch_args = ()\n   if lowering_result.gmem_scratch_shapes:\n+    # The new_out_shapes contain the original outputs first, followed by the\n+    # GMEM scratch shapes, and optionally the profiler buffer.\n     input_output_aliases += tuple(\n-        (len(new_avals_in) + i, len(new_avals_out) + i)\n+        (len(ctx.avals_in) + i, len(ctx.avals_out) + i)\n         for i in range(len(lowering_result.gmem_scratch_shapes))\n     )\n+    # The GMEM scratch is an aliased kernel input/output.\n     new_avals_in.extend(map(_as_shaped_array, lowering_result.gmem_scratch_shapes))\n-    new_avals_out.extend(map(_as_shaped_array, lowering_result.gmem_scratch_shapes))\n+    # We guarantee zero-initialization of the GMEM scratch at the moment, which\n+    # is important for semaphores.\n     def zero_init_gmem_scratch():\n       return [lax.zeros_like_array(s) for s in lowering_result.gmem_scratch_shapes]\n     scratch_args = mlir.lower_fun(\n@@ -100,12 +104,10 @@ def zero_init_gmem_scratch():\n       ctx.replace(avals_in=new_avals_in, avals_out=new_avals_out),\n       *args, *scratch_args,\n       module=module,\n-      out_types=(*lowering_result.new_out_shapes, *lowering_result.gmem_scratch_shapes),\n+      out_types=lowering_result.new_out_shapes,\n       input_output_aliases=input_output_aliases,\n       use_custom_barrier=False, # False until we add get_barrier_semaphore() feature\n   )\n-  if lowering_result.gmem_scratch_shapes:  # Drop the GMEM scratch.\n-    outs = outs[:-len(lowering_result.gmem_scratch_shapes)]\n   if (prof_ctx := lowering_result.profiler_context) is not None:\n     *outs, prof_buffer = outs\n     if (dump_path := prof_ctx.dump_path) == \"sponge\":\n@@ -133,6 +135,8 @@ def do_callback(prof_buffer):\n     mlir.lower_fun(do_callback, multiple_results=True)(\n         ctx.replace(avals_in=(new_avals_out[-1],)), prof_buffer\n     )\n+  if lowering_result.gmem_scratch_shapes:  # Drop the GMEM scratch.\n+    outs = outs[:-len(lowering_result.gmem_scratch_shapes)]\n   return outs\n \n \ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex 9eedc3402579..1e20675f7909 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -226,15 +226,19 @@ def when(cond):\n     scf.yield_([])\n \n \n-def thread_idx():\n+def _3d_to_1d_idx(dim_idx_fn, dim_size_fn):\n   i32 = ir.IntegerType.get_signless(32)\n   as_i32 = lambda x: arith.index_cast(i32, x)\n-  tidx = as_i32(gpu.thread_id(gpu.Dimension.x))\n-  stride = as_i32(gpu.block_dim(gpu.Dimension.x))\n+  idx = as_i32(dim_idx_fn(gpu.Dimension.x))\n+  stride = as_i32(dim_size_fn(gpu.Dimension.x))\n   for dim in (gpu.Dimension.y, gpu.Dimension.z):\n-    tidx = arith.addi(tidx, arith.muli(as_i32(gpu.thread_id(dim)), stride))\n-    stride = arith.muli(stride, as_i32(gpu.block_dim(dim)))\n-  return tidx\n+    idx = arith.addi(idx, arith.muli(as_i32(dim_idx_fn(dim)), stride))\n+    stride = arith.muli(stride, as_i32(dim_size_fn(dim)))\n+  return idx\n+\n+\n+thread_idx = functools.partial(_3d_to_1d_idx, gpu.thread_id, gpu.block_dim)\n+block_idx = functools.partial(_3d_to_1d_idx, gpu.block_id, gpu.grid_dim)\n \n \n def _warp_bcast(val, lane_idx=0):\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex aedd79e23194..7173639b879f 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -3394,7 +3394,10 @@ def compute(_, l_smem, r_smem, o_smem):\n \n     np.testing.assert_allclose(kernel(x, x), x + x)\n \n-  def test_semaphore_lowering(self):\n+\n+class SemaphoreTest(PallasTest):\n+\n+  def test_lowering(self):\n     # This is a smoke test until we add support for lowering of semaphore ops.\n     def body(i_ref1, i_ref2, o_ref, sem_ref):\n       del i_ref2  # Only here to have a different number of inputs and outputs.\n@@ -3420,6 +3423,51 @@ def body(i_ref1, i_ref2, o_ref, sem_ref):\n         text,\n     )\n \n+  def test_basic(self):\n+    def body(o_ref, sem_ref):\n+      assert jnp.issubdtype(sem_ref.dtype, pl.semaphore)\n+      pl.semaphore_signal(sem_ref)\n+      o_ref[...] = jnp.ones_like(o_ref)\n+      pl.semaphore_wait(sem_ref)\n+    kernel = plgpu.kernel(\n+        body,\n+        out_shape=jax.ShapeDtypeStruct((128,), jnp.float32),\n+        scratch_shapes=[plgpu.SemaphoreType.REGULAR],\n+        grid=(2,),\n+        grid_names=(\"x\",),\n+    )\n+    text = jax.jit(kernel).lower().as_text()\n+    np.testing.assert_array_equal(kernel(), jnp.ones((128,), jnp.float32))\n+    # The semaphore array is scaled up by the grid size.\n+    self.assertIn(\n+        r\"(tensor<128xf32>, tensor<2xi32>) -> (tensor<128xf32>, tensor<2xi32>)\",\n+        text,\n+    )\n+\n+  def test_with_profiler(self):\n+    # Dealing with profiler and semaphores together is tricky because they both\n+    # add extra outputs to the HLO op.\n+    def body(o_ref, sem_ref):\n+      assert jnp.issubdtype(sem_ref.dtype, pl.semaphore)\n+      with jax.named_scope(\"output\"):\n+        o_ref[...] = jnp.ones_like(o_ref)\n+    with tempfile.TemporaryDirectory() as tmp_dir:\n+      kernel = plgpu.kernel(\n+          body,\n+          out_shape=jax.ShapeDtypeStruct((128,), jnp.float32),\n+          scratch_shapes=[plgpu.SemaphoreType.REGULAR],\n+          grid=(2,),\n+          grid_names=(\"x\",),\n+          compiler_params=plgpu.CompilerParams(profile_space=32, profile_dir=tmp_dir),\n+      )\n+      text = jax.jit(kernel).lower().as_text()\n+      np.testing.assert_array_equal(kernel(), jnp.ones((128,), jnp.float32))\n+    self.assertIn(\n+        r\"(tensor<128xf32>, tensor<2xi32>) ->\"\n+        r\" (tensor<128xf32>, tensor<2xi32>, tensor<512xui32>)\",\n+        text,\n+    )\n+\n \n class ExamplesWGTest(\n     ExamplesTest, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas/Mosaic GPU] Add a Pallas:MGPU implementation of `ragged_dot`.\n\nPiperOrigin-RevId: 761875827",
        "output": [
            "```diff\nCommit: c670a2803897b06f86952a705ab93709bab1fb6f\nDate: 2025-05-22T09:37:46Z\nURL: https://github.com/jax-ml/jax/commit/c670a2803897b06f86952a705ab93709bab1fb6f\nFiles changed: 3\nAdditions: +478, Deletions: -0\ndiff --git a/jax/experimental/pallas/ops/gpu/ragged_dot_mgpu.py b/jax/experimental/pallas/ops/gpu/ragged_dot_mgpu.py\nnew file mode 100644\nindex 000000000000..6d295a36f435\n--- /dev/null\n+++ b/jax/experimental/pallas/ops/gpu/ragged_dot_mgpu.py\n@@ -0,0 +1,327 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Ragged dot Pallas-Mosaic-GPU implementation.\"\"\"\n+\n+import dataclasses\n+import functools\n+import itertools\n+import math\n+import jax\n+from jax import lax\n+from jax import numpy as jnp\n+from jax import random\n+from jax._src import test_util as jtu  # noqa: F401\n+from jax.experimental import pallas as pl\n+from jax.experimental.mosaic.gpu import profiler\n+from jax.experimental.pallas import mosaic_gpu as plgpu\n+import numpy as np\n+\n+\n+@dataclasses.dataclass(frozen=True)\n+class GroupInfo:\n+  \"\"\"Information regarding the group being processed in a block.\"\"\"\n+\n+  group_id: jax.Array\n+  block: jax.Array\n+  block_start: jax.Array\n+  actual_start: jax.Array\n+  actual_end: jax.Array\n+  start_within_block: jax.Array\n+  actual_size: jax.Array\n+\n+  @classmethod\n+  def create(cls, group_lengths, tile, tid):\n+    \"\"\"Get the group info for the current block.\"\"\"\n+\n+    tile = jnp.int32(tile)\n+    group_boundaries = [group_lengths[i] for i in range(group_lengths.shape[0])]\n+\n+    # We usually only have very few groups, so we unroll the loop processing\n+    # them. Normally we'd break out of the loop early, once we'd have found our\n+    # boundary, but we can't do that when unrolling, so we rely on many selects\n+    # to mask out the epilogue of the loop.\n+    group_end = group_start = block = group = end = jnp.array(\n+        0, dtype=jnp.int32\n+    )\n+\n+    for i, b in enumerate(group_boundaries):\n+      # Start/end are inclusive\n+      start = end\n+      end = start + b\n+      final = end - 1\n+      start_block = lax.div(start, tile)\n+      final_block = lax.div(final, tile)\n+      block_end = final_block + 1\n+      tid_begin = start_block + i\n+      tid_end = block_end + i\n+      # How many blocks after is our block?\n+      this_is_group = (tid_begin <= tid) & (tid < tid_end)\n+      block = lax.select(this_is_group, tid - tid_begin + start_block, block)\n+      group = lax.select(this_is_group, jnp.int32(i), group)\n+      group_start = lax.select(this_is_group, start, group_start)\n+      group_end = lax.select(this_is_group, end, group_end)\n+\n+    block_start = block * tile\n+    actual_start = jnp.maximum(group_start, block_start)\n+    actual_end = jnp.minimum(group_end, block_start + tile)\n+    start_within_block = actual_start - block_start\n+    actual_size = actual_end - actual_start\n+    return cls(\n+        group_id=group,\n+        block=block,\n+        block_start=block_start,\n+        actual_start=actual_start,\n+        actual_end=actual_end,\n+        start_within_block=start_within_block,\n+        actual_size=actual_size,\n+    )\n+\n+\n+def _find_swizzle(dim_size_bits: int, what: str):\n+  for swizzle_bytes in (128, 64, 32, 16):\n+    if dim_size_bits % (swizzle_bytes * 8) == 0:\n+      return swizzle_bytes\n+  raise ValueError(\n+      f\"No valid out swizzle for {what}: its minor dimension has\"\n+      f\" {dim_size_bits} bits, which is not a multiple of 128\"\n+  )\n+\n+\n+def ragged_dot(\n+    lhs,  # (M, K)\n+    rhs,  # (G, K, N)\n+    *,\n+    group_sizes,  # (G,)\n+    block_m: int,\n+    block_n: int,\n+    block_k: int,\n+    max_concurrent_steps: int,\n+    grid_block_n: int,\n+) -> jax.Array:\n+  if lhs.dtype != rhs.dtype:\n+    raise NotImplementedError(\n+        f\"lhs and rhs must have the same dtype, got {lhs.dtype} and {rhs.dtype}\"\n+    )\n+\n+  elem_bits = jnp.finfo(lhs.dtype).bits\n+  swizzle = _find_swizzle(elem_bits * block_k, \"lhs\")\n+  swizzle_elems = swizzle * 8 // elem_bits\n+\n+  m, k = lhs.shape\n+  g, k2, n = rhs.shape\n+\n+  if group_sizes.shape[0] != g:\n+    raise ValueError(\n+        f\"Expected group_sizes to have shape {g} but got {group_sizes.shape}\"\n+    )\n+\n+  if k != k2:\n+    raise ValueError(f\"lhs.shape={k} must match rhs.shape={k2}\")\n+\n+  if k % block_k != 0:\n+    raise ValueError(f\"k={k} must be a multiple of block_k={block_k}\")\n+\n+  def body(rows_per_expert_gmem, lhs_gmem, rhs_gmem, o_gmem):\n+    grid = (\n+        grid_block_n,\n+        pl.cdiv(m, block_m) + g - 1,\n+        pl.cdiv(n, grid_block_n * block_n),\n+    )\n+\n+    @functools.partial(\n+        plgpu.nd_loop, grid, init_val=None, collective_axes=\"sm\"\n+    )\n+    def mn_loop(idx, _):  # pylint: disable=unused-variable\n+      block_ni, mi, remainder_ni = idx\n+      ni = block_ni * pl.cdiv(n, block_n * grid_block_n) + remainder_ni\n+      group_info = GroupInfo.create(rows_per_expert_gmem, block_m, mi)\n+\n+      def acc_scope(acc_ref):\n+        transforms = (\n+            plgpu.TilingTransform((8, swizzle_elems)),\n+            plgpu.SwizzleTransform(swizzle),\n+        )\n+        plgpu.emit_pipeline(\n+            lambda _, lhs_smem, rhs_smem: plgpu.wgmma(acc_ref, lhs_smem, rhs_smem),\n+            grid=(k // block_k,),\n+            in_specs=[\n+                plgpu.BlockSpec(\n+                    (block_m, block_k),\n+                    lambda k: (group_info.block, k),\n+                    transforms=transforms,\n+                ),\n+                plgpu.BlockSpec(\n+                    (block_k, block_n), lambda k: (k, ni), transforms=transforms\n+                ),\n+            ],\n+            max_concurrent_steps=max_concurrent_steps,\n+            delay_release=1,\n+        )(lhs_gmem, rhs_gmem.at[group_info.group_id])\n+        return acc_ref[...]\n+\n+      acc = pl.run_scoped(acc_scope, plgpu.ACC((block_m, block_n)))\n+\n+      store_transforms = (\n+          plgpu.TilingTransform((1, swizzle_elems)),\n+          plgpu.SwizzleTransform(swizzle)\n+      )\n+      @functools.partial(\n+          pl.run_scoped,\n+          o_smem=plgpu.SMEM(\n+              (block_m, block_n),\n+              dtype=o_gmem.dtype,\n+              transforms=store_transforms,\n+          )\n+      )\n+      def store_scope(o_smem):  # pylint: disable=unused-variable\n+        o_smem[...] = acc.astype(o_smem.dtype)\n+        plgpu.commit_smem()\n+\n+        smem_start = group_info.start_within_block\n+        remaining_rows = min(block_m, m)\n+        # TMA descriptors need to be generated with static tile sizes along each\n+        # axis, but we do not know at compile time how many rows we will need to\n+        # store. We only know that the number of rows to store is bounded by\n+        # min(block_m, m).\n+        #\n+        # In order to work around that, we construct a logarithmic ladder of\n+        # TMA descriptors, where each descriptor can store 2**i rows for some\n+        # i between 0 and log2(min(block_m, m)). This allows storing any\n+        # number of rows we will need to store, so long as this number of rows\n+        # is between `1` and `min(block_m, m)`.\n+        #\n+        # E.g., imagine we have block_m = 8, m = 16. The loop below will be\n+        # unrolled into 4 iterations, where the first one will generate a TMA\n+        # descriptor that can store 8 rows, the second one will generate a TMA\n+        # descriptor that can store 4 rows, etc. all the way to 1 row.\n+        #\n+        # At run time, we finally know the actual number of rows we need to\n+        # store as we go through the unrolled loop iterations. Let's imagine\n+        # that we need to store 5 rows.\n+        #\n+        # The first unrolled iteration will check whether we can store 8 rows.\n+        # Since we only need to store 5 rows, we won't store anything then.\n+        #\n+        # The second unrolled iteration will check whether we can store 4 rows.\n+        # We're able to store 4 rows, and are left with a single remaining row.\n+        #\n+        # The fourth unrolled iteration will store the single remaining row, and\n+        # we end up with a storing scheme as follows for our 5 rows:\n+        #\n+        #     -----------------------------------------------------------\n+        #  0  |                                                         |\n+        #  1  |                                                         |\n+        #  2  |                       Store 4 rows                      |\n+        #  3  |                                                         |\n+        #     -----------------------------------------------------------\n+        #  4  |                       Store 1 row                       |\n+        #     -----------------------------------------------------------\n+        while remaining_rows > 0:\n+          const_rows_len = 1 << int(math.log2(remaining_rows))\n+          remaining_rows //= 2\n+\n+          @pl.when(group_info.actual_size & const_rows_len != 0)\n+          def _():\n+            o_smem_slice = o_smem.at[pl.ds(smem_start, const_rows_len)]\n+            o_gref_slice = o_gmem.at[\n+                pl.ds(group_info.block_start + smem_start, const_rows_len),\n+                pl.ds(ni * block_n, block_n),\n+            ]\n+            plgpu.copy_smem_to_gmem(o_smem_slice, o_gref_slice)\n+\n+          smem_start += group_info.actual_size & const_rows_len\n+        plgpu.wait_smem_to_gmem(0, wait_read_only=True)\n+\n+  # There are 132 SMs on a H100 SXM GPU.\n+  num_sms = 132\n+  kernel = plgpu.kernel(\n+      body,\n+      out_shape=jax.ShapeDtypeStruct((m, n), lhs.dtype),\n+      grid=(num_sms,),\n+      grid_names=(\"sm\",),\n+  )\n+  return kernel(group_sizes, lhs, rhs)\n+\n+\n+def main(unused_argv):\n+  m, k, n, num_groups = 16 * 1024, 2048, 16 * 1024, 16\n+  kx, ky, kz = random.split(random.key(1234), num=3)\n+\n+  lhs = jax.random.normal(kx, (m, k), jnp.float16)\n+  rhs = jax.random.normal(ky, (num_groups, k, n), jnp.float16)\n+  group_boundaries = jax.lax.sort(\n+      jax.random.randint(kz, (num_groups - 1,), 0, m, jnp.int32)\n+  )\n+  group_starts = lax.concatenate(\n+      [jnp.array([0], dtype=jnp.int32), group_boundaries], 0\n+  )\n+  group_ends = lax.concatenate(\n+      [group_boundaries, jnp.array([m], dtype=jnp.int32)], 0\n+  )\n+  group_sizes = group_ends - group_starts\n+  assert group_sizes.shape == (num_groups,)\n+\n+  block_m = block_n = (64, 128, 192)\n+  block_k = (64,)\n+  max_concurrent_steps = (2, 4, 5, 6)\n+  grid_block_n = (1, 2, 4, 8, 16)\n+  configs = itertools.product(\n+      block_m, block_n, block_k, max_concurrent_steps, grid_block_n\n+  )\n+  names = (\n+      \"block_m\", \"block_n\", \"block_k\", \"max_concurrent_steps\", \"grid_block_n\"\n+  )\n+  best_runtime = float(\"inf\")\n+  best_kwargs = {}\n+  for config in configs:\n+    kwargs = dict(zip(names, config))\n+    if n % (kwargs[\"grid_block_n\"] * kwargs[\"block_n\"]):\n+      continue\n+    try:\n+      f = functools.partial(ragged_dot, group_sizes=group_sizes, **kwargs)\n+      _, runtime = profiler.measure(f, mode=\"cupti\")(lhs, rhs)\n+    except ValueError as e:\n+      if \"Mosaic GPU kernel exceeds available shared memory\" not in str(e):\n+        raise\n+      runtime = float(\"inf\")\n+    # Enable this to get more detailed information.\n+    else:\n+      print(\" \".join(f\"{k}={v}\" for k, v in kwargs.items()), int(runtime * 1000))\n+    if runtime < best_runtime:  # pytype: disable=unsupported-operands\n+      best_runtime = runtime\n+      best_kwargs = kwargs\n+  if not best_kwargs:\n+    raise ValueError(\"No valid configuration found\")\n+\n+  ref, ref_runtime = profiler.measure(jax.lax.ragged_dot)(\n+      lhs, rhs, group_sizes=group_sizes\n+  )\n+  result = ragged_dot(lhs, rhs, group_sizes=group_sizes, **best_kwargs)\n+  np.testing.assert_allclose(result, ref, atol=1e-3, rtol=1e-3)\n+\n+  tflops = float(2 * k * m * n) / (best_runtime / 1e3) / 1e12\n+  ref_tflops = float(2 * k * m * n) / (ref_runtime / 1e3) / 1e12\n+  print(\n+      \"Best parameters: \", \" \".join(f\"{k}={v}\" for k, v in best_kwargs.items())\n+  )\n+  print(f\"Kernel:    {best_runtime * 1000:.1f} us = {tflops:.1f} TFLOPS\")\n+  print(f\"Reference: {ref_runtime * 1000:.1f} us = {ref_tflops:.1f} TFLOPS\")\n+\n+\n+if __name__ == \"__main__\":\n+  from absl import app\n+\n+  jax.config.config_with_absl()\n+  app.run(main)\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex d7df261a1ca9..c45e52b1fe88 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -805,6 +805,43 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"mgpu_ragged_dot_run\",\n+    srcs = [\"//jax/experimental/pallas/ops/gpu:ragged_dot_mgpu.py\"],\n+    enable_backends = [],\n+    enable_configs = [\n+        \"gpu_h100_x32\",\n+        \"gpu_h100\",\n+    ],\n+    env = {\"XLA_FLAGS\": \"--xla_gpu_autotune_level=0\"},\n+    tags = [\n+        \"manual\",\n+        \"notap\",\n+    ],\n+    deps = [\n+        \"//jax:pallas\",\n+        \"//jax:pallas_mosaic_gpu\",\n+    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+)\n+\n+jax_multiplatform_test(\n+    name = \"mgpu_ragged_dot_test\",\n+    srcs = [\"mgpu_ragged_dot_test.py\"],\n+    enable_backends = [],\n+    enable_configs = [\n+        \"gpu_h100\",\n+    ],\n+    shard_count = 12,\n+    deps = [\n+        \"//jax:pallas\",\n+        \"//jax:pallas_experimental_gpu_ops\",\n+        \"//jax:pallas_mosaic_gpu\",\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n+)\n+\n jax_multiplatform_test(\n     name = \"fuser_block_spec_test\",\n     srcs = [\ndiff --git a/tests/pallas/mgpu_ragged_dot_test.py b/tests/pallas/mgpu_ragged_dot_test.py\nnew file mode 100644\nindex 000000000000..e9137df1298a\n--- /dev/null\n+++ b/tests/pallas/mgpu_ragged_dot_test.py\n@@ -0,0 +1,114 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Test different parameterizations of our Mosaic GPU ragged dot kernel.\"\"\"\n+\n+import contextlib\n+import os\n+\n+from absl.testing import absltest, parameterized  # pylint: disable=g-multiple-import\n+from jax import random\n+from jax._src import config\n+from jax._src import test_util as jtu\n+from jax._src.pallas import pallas_call\n+import jax.numpy as jnp\n+import numpy as np\n+\n+# pylint: disable=g-import-not-at-top\n+try:\n+  # We only import this to see if Mosaic is available.\n+  import jax.experimental.mosaic.gpu  # noqa: F401\n+except ImportError:\n+  ragged_dot = None\n+else:\n+  from jax.experimental.pallas.ops.gpu import ragged_dot_mgpu\n+\n+\n+config.parse_flags_with_absl()\n+\n+\n+@jtu.with_config(jax_traceback_filtering=\"off\")\n+class RaggedDotTestCase(jtu.JaxTestCase):\n+\n+  def setUp(self):\n+    super().setUp()\n+    if ragged_dot_mgpu is None:\n+      self.skipTest(\"Mosaic GPU not available.\")\n+    if (not jtu.test_device_matches([\"cuda\"]) or\n+        not jtu.is_cuda_compute_capability_equal(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability sm90a\")\n+    context_stack = contextlib.ExitStack()\n+    context_stack.enter_context(pallas_call._PALLAS_USE_MOSAIC_GPU(True))\n+    self.addCleanup(context_stack.close)\n+\n+  @parameterized.product(\n+      block_m=(64, 128, 192),\n+      block_n=(64, 128, 192),\n+      block_k=(64, 128),\n+      grid_block_n=(2, 4),\n+      max_concurrent_steps=(2, 4),\n+      num_groups=(1, 3, 16),\n+  )\n+  def test_ragged_dot(\n+      self,\n+      block_m,\n+      block_n,\n+      block_k,\n+      grid_block_n,\n+      max_concurrent_steps,\n+      num_groups,\n+  ):\n+    dtype = jnp.float16\n+    lhs_smem_size = block_m * block_k * max_concurrent_steps * 2\n+    rhs_smem_size = block_k * block_n * max_concurrent_steps * 2\n+    # H100 SMEM limit is 228kB.\n+    if lhs_smem_size + rhs_smem_size > 228_000:\n+      self.skipTest(\"This configuration requires too much SMEM.\")\n+\n+    m, k, n = 16 * 1024, 2048, 16 * 1024\n+    kx, ky, kz = random.split(random.key(1234), num=3)\n+\n+    lhs = jax.random.normal(kx, (m, k), dtype)\n+    rhs = jax.random.normal(ky, (num_groups, k, n), dtype)\n+    group_boundaries = jax.lax.sort(\n+        jax.random.randint(kz, (num_groups - 1,), 0, m, jnp.int32)\n+    )\n+    group_starts = jax.lax.concatenate(\n+        [jnp.array([0], dtype=jnp.int32), group_boundaries], 0\n+    )\n+    group_ends = jax.lax.concatenate(\n+        [group_boundaries, jnp.array([m], dtype=jnp.int32)], 0\n+    )\n+    group_sizes = group_ends - group_starts\n+    assert group_sizes.shape == (num_groups,)\n+\n+    out = ragged_dot_mgpu.ragged_dot(\n+        lhs,\n+        rhs,\n+        group_sizes=group_sizes,\n+        block_m=block_m,\n+        block_n=block_n,\n+        block_k=block_k,\n+        max_concurrent_steps=max_concurrent_steps,\n+        grid_block_n=grid_block_n,\n+    )\n+    out_ref = jax.lax.ragged_dot(lhs, rhs, group_sizes=group_sizes)\n+    np.testing.assert_allclose(out, out_ref, atol=1e-3, rtol=1e-3)\n+\n+\n+if __name__ == \"__main__\":\n+  os.environ[\"XLA_FLAGS\"] = (\n+      os.environ.get(\"XLA_FLAGS\", \"\") + \" --xla_gpu_autotune_level=0\"\n+  )\n+  absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas] Simulate multiple cores per device when interpreting kernels on CPU.\n\nThe `TPUInterpreterParams` are extended with a field `num_cores_per_device` that specifies how many cores should be simulated per (TPU) device.\nPer-core devices are mapped along grid dimensions with _parallel_ dimension semantics.\n(I.e. the body of the grid loop is considered to execute on the next device-local core if the multi-dimensional loop index has changed along a _parallel_ grid dimension from one loop iteration to the next.)\n\nEach per-device core is identified by a `local_core_id`. Globally, each core is identified by a `global_core_id` (= `device_id` * `num_cores_per_device` + `local_core_id`).\nThere is one vector clock per core (previously, one per device), and each vector clock has as many entries as there are cores in total (= `num_device` * `num_cores_per_device`).\n\nPiperOrigin-RevId: 761872013",
        "output": [
            "```diff\nCommit: 012f9b2677cbe84053dd5a9a71ab1aba349a8973\nDate: 2025-05-22T09:25:26Z\nURL: https://github.com/jax-ml/jax/commit/012f9b2677cbe84053dd5a9a71ab1aba349a8973\nFiles changed: 2\nAdditions: +713, Deletions: -197\ndiff --git a/jax/_src/pallas/mosaic/interpret.py b/jax/_src/pallas/mosaic/interpret.py\nindex c0e52f54e6f3..4258e52e6541 100644\n--- a/jax/_src/pallas/mosaic/interpret.py\n+++ b/jax/_src/pallas/mosaic/interpret.py\n@@ -99,6 +99,8 @@ class TPUInterpretParams:\n       intended for inspecting the randomization of coordinates along grid\n       dimensions with 'parallel' semantics.\n       Default: None.\n+    num_cores_per_device: The number of cores per device.\n+      Default: 1.\n   \"\"\"\n   dma_execution_mode: Literal[\"eager\", \"on_wait\"] = \"on_wait\"\n   detect_races: bool = False\n@@ -106,6 +108,7 @@ class TPUInterpretParams:\n   uninitialized_memory: Literal[\"nan\", \"zero\"] = \"nan\"\n   random_seed: int | None = None\n   grid_point_recorder: Callable[[tuple[jnp.int32, ...]], None] | None = None\n+  num_cores_per_device: int = 1\n \n \n VectorClock = np.ndarray\n@@ -115,11 +118,12 @@ class TPUInterpretParams:\n # of DMAs.\n #\n # Instead, we use approximate vector clocks of fixed size.  We assign each DMA\n-# a virtual device ID in the range [num_devices + 1, NUM_VIRTUAL_DEVICES] --\n+# a virtual core ID in the range\n+#   [num_devices*num_cores_per_device + 1, NUM_VIRTUAL_CORES],\n # and each operation of a DMA increments the corresponding coordinate in its\n-# vector clock.  (So the \"virtual\" part of a vector clock is effectively\n-# counting, for each virtual device, the number of DMAs that happened-before\n-# the vector clock and were assigned to that virtual device.)\n+# vector clock. (So the \"virtual\" part of a vector clock is effectively\n+# counting, for each virtual core, the number of DMAs that happened-before\n+# the vector clock and were assigned to that virtual core.)\n #\n # If two approximate clocks are unordered, then their corresponding events are\n # not ordered by the happens-before relation.  So this approximation will not\n@@ -128,11 +132,11 @@ class TPUInterpretParams:\n # clocks are ordered, and we will treat the corresponding events as ordered\n # by the happens-before relation, but the corresponding events are not\n # actually ordered.\n-NUM_VIRTUAL_DEVICES = 32\n+NUM_VIRTUAL_CORES = 32\n \n-def make_vector_clock(num_devices: int) -> VectorClock:\n-  del num_devices\n-  return np.zeros(NUM_VIRTUAL_DEVICES, dtype=np.int32)\n+def make_vector_clock(_: int) -> VectorClock:\n+  del _\n+  return np.zeros(NUM_VIRTUAL_CORES, dtype=np.int32)\n \n def copy_vector_clock(x: VectorClock) -> VectorClock:\n   if x is None:\n@@ -140,7 +144,7 @@ def copy_vector_clock(x: VectorClock) -> VectorClock:\n   return x.copy()\n \n def update_vector_clock(x: VectorClock, y: VectorClock):\n-  x[:] = np.maximum(x, y)\n+  x[:] = np.maximum(x[:], y[:])\n \n def lt(x: VectorClock, y: VectorClock) -> bool:\n   return bool((x <= y).all() & (x < y).any())\n@@ -148,11 +152,17 @@ def lt(x: VectorClock, y: VectorClock) -> bool:\n def ordered(x: VectorClock, y: VectorClock) -> bool:\n   return lt(x, y) | lt(y, x)\n \n-def inc_vector_clock(x: VectorClock, device_id: int):\n-  if device_id >= len(x):\n-    raise ValueError(f'device_id={device_id} is out of range for x={x}')\n-  assert device_id < len(x)\n-  x[device_id] += 1\n+def inc_vector_clock(x: VectorClock, global_core_id: int):\n+  if global_core_id >= len(x):\n+    raise ValueError(f'device_id={global_core_id} is out of range for x={x}')\n+  assert global_core_id < len(x)\n+  x[global_core_id] += 1\n+\n+def _get_global_core_id(device_id, local_core_id):\n+  \"\"\"Computes the global core ID from the given device and local core ID.\"\"\"\n+  device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n+  return device_id * _get_shared_memory().num_cores_per_device + local_core_id\n \n \n class Semaphore:\n@@ -165,45 +175,45 @@ def __init__(self, semaphore_id=None):\n     # easier to do when we're using single integer device IDs.)\n     self.cv = threading.Condition()\n \n-    self.counts = np.zeros(shared_memory.num_devices, dtype=np.int32)\n+    self.counts = np.zeros(shared_memory.num_cores, dtype=np.int32)\n \n     self.interpret_params = shared_memory.interpret_params\n     if self.interpret_params.detect_races:\n       # We associate a vector clock with each count in self.counts.  Whenever\n       # self.counts[i] is signaled, self.clocks[i] is updated with the vector\n-      # clock of the signaling device.  Whenever device i successfully waits on\n-      # self.counts[i], the vector clock of device i is updated with\n+      # clock of the signaling core.  Whenever core i successfully waits on\n+      # self.counts[i], the vector clock of core i is updated with\n       # self.clocks[i].\n       #\n       # TODO(jburnim): Model happens-before more precisely for the case where\n       # semaphores are over-signaled.\n-      self.clocks = [None] * shared_memory.num_devices\n+      self.clocks = [None] * shared_memory.num_cores\n \n-  def signal(self, inc, device_id, clock):\n-    \"\"\"Signal the semaphore on `device_id` by `inc`.\n+  def signal(self, inc, global_core_id, clock):\n+    \"\"\"Signal the semaphore on `(device_id, core_id)` by `inc`.\n \n     Args:\n       inc: A positive integer.  The amount by which to increment the semaphore\n         on the target device.\n-      device_id: The ID of the target device.\n+      global_core_id: The ID of the target core.\n       clock: The vector clock of the signaling device at the time of the signal.\n     \"\"\"\n-    device_id = int(device_id)\n+    global_core_id = int(global_core_id)\n     with self.cv:\n-      self.counts[device_id] += inc\n+      self.counts[global_core_id] += inc\n       if self.interpret_params.detect_races:\n-        if self.clocks[device_id] is None:\n-          self.clocks[device_id] = copy_vector_clock(clock)\n+        if self.clocks[global_core_id] is None:\n+          self.clocks[global_core_id] = copy_vector_clock(clock)\n         else:\n-          update_vector_clock(self.clocks[device_id], clock)\n+          update_vector_clock(self.clocks[global_core_id], clock)\n       self.cv.notify_all()\n \n-  def read(self, device_id):\n+  def read(self, global_core_id):\n     with self.cv:\n-      return self.counts[device_id]\n+      return self.counts[global_core_id]\n \n-  def wait(self, value, device_id, *, is_dma=False):\n-    device_id = int(device_id)\n+  def wait(self, value, global_core_id, *, is_dma=False):\n+    global_core_id = int(global_core_id)\n     shared_memory = _get_shared_memory()\n \n     # TODO(jburnim):\n@@ -214,14 +224,14 @@ def wait(self, value, device_id, *, is_dma=False):\n     # Simple implementation for non-DMA semaphores.\n     if not is_dma or (self.interpret_params.dma_execution_mode == \"eager\"):\n       with self.cv:\n-        while self.counts[device_id] < value:\n+        while self.counts[global_core_id] < value:\n           self.cv.wait()\n-        self.counts[device_id] -= value\n+        self.counts[global_core_id] -= value\n         if self.interpret_params.detect_races:\n-          clock = copy_vector_clock(self.clocks[device_id])\n+          clock = copy_vector_clock(self.clocks[global_core_id])\n       if self.interpret_params.detect_races:\n         with shared_memory.lock:\n-          update_vector_clock(shared_memory.clocks[device_id], clock)\n+          update_vector_clock(shared_memory.clocks[global_core_id], clock)\n       return\n \n     # For DMA semaphores (when dma_execution_mode=='on_wait'), while our count\n@@ -235,15 +245,15 @@ def wait(self, value, device_id, *, is_dma=False):\n     while True:\n       clock = None\n       with self.cv:\n-        if self.counts[device_id] >= value:\n-          self.counts[device_id] -= value\n+        if self.counts[global_core_id] >= value:\n+          self.counts[global_core_id] -= value\n           if self.interpret_params.detect_races:\n-            clock = copy_vector_clock(self.clocks[device_id])\n+            clock = copy_vector_clock(self.clocks[global_core_id])\n           else:\n             return\n       if clock is not None:\n         with shared_memory.lock:\n-          update_vector_clock(shared_memory.clocks[device_id], clock)\n+          update_vector_clock(shared_memory.clocks[global_core_id], clock)\n         return\n \n       with shared_memory.lock:\n@@ -258,25 +268,32 @@ def wait(self, value, device_id, *, is_dma=False):\n       with dma.lock:\n         if dma.virtual_device_id is None:\n           dma.virtual_device_id = np.random.randint(\n-              shared_memory.num_devices, NUM_VIRTUAL_DEVICES)\n+              shared_memory.num_devices, NUM_VIRTUAL_CORES)\n \n         if dma.state == DmaState.STARTED:\n           # Do the read.\n           if self.interpret_params.detect_races:\n             inc_vector_clock(dma.clock, dma.virtual_device_id)\n           dma.data = get(dma.src_device_id,\n+                         dma.src_local_core_id,\n                          dma.src_memory_space,\n                          dma.src_buffer_id,\n                          dma.src_transforms,\n                          clock=copy_vector_clock(dma.clock),\n                          src_device_id=dma.id,\n+                         src_local_core_id=0,\n                          source_info=dma.source_info)\n           if self.interpret_params.detect_races:\n             inc_vector_clock(dma.clock, dma.virtual_device_id)\n           if dma.src_sem is not None:\n             data_size = dma.data.itemsize * dma.data.size\n             dma.src_sem.signal(\n-                data_size, device_id=dma.src_device_id, clock=dma.clock)\n+                data_size,\n+                global_core_id=_get_global_core_id(\n+                    dma.src_device_id, dma.src_local_core_id\n+                ),\n+                clock=dma.clock,\n+            )\n           dma.state = DmaState.READ\n \n         if dma.src_sem is self:\n@@ -290,18 +307,25 @@ def wait(self, value, device_id, *, is_dma=False):\n         if self.interpret_params.detect_races:\n           inc_vector_clock(dma.clock, dma.virtual_device_id)\n         store(dma.dst_device_id,\n+              dma.dst_local_core_id,\n               dma.dst_memory_space,\n               dma.dst_buffer_id,\n               dma.dst_transforms,\n               dma.data,\n               clock=copy_vector_clock(dma.clock),\n               src_device_id=dma.id,\n+              src_local_core_id=0,\n               source_info=dma.source_info)\n         if self.interpret_params.detect_races:\n           inc_vector_clock(dma.clock, dma.virtual_device_id)\n         data_size = dma.data.itemsize * dma.data.size\n         dma.dst_sem.signal(\n-            data_size, device_id=dma.dst_device_id, clock=dma.clock)\n+            data_size,\n+            global_core_id=_get_global_core_id(\n+                dma.dst_device_id, dma.dst_local_core_id\n+            ),\n+            clock=dma.clock,\n+        )\n \n         dma.data = None\n         dma.state = DmaState.COMPLETED\n@@ -317,10 +341,12 @@ class DMA:\n   id: int\n \n   src_device_id: int\n+  src_local_core_id: int\n   src_memory_space: int\n   src_buffer_id: int\n   src_transforms: tuple[Any, ...]\n   dst_device_id: int\n+  dst_local_core_id: int\n   dst_memory_space: int\n   dst_buffer_id: int\n   dst_transforms: tuple[Any, ...]\n@@ -339,13 +365,14 @@ class DMA:\n \n @dataclasses.dataclass\n class RaceDetectionState:\n-  num_devices: int\n+  num_cores: int\n+\n \n-  # (memory_space, buffer_id, device_id) -> [(device_id, VectorClock, range)]\n+  # (memory_space, buffer_id, device_id, local_core_id) -> [(device_id, local_core_id, VectorClock, range)]\n   reads: dict = dataclasses.field(\n       default_factory=lambda: collections.defaultdict(list))\n \n-  # (memory_space, buffer_id, device_id) -> [(device_id, VectorClock, range)]\n+  # (memory_space, buffer_id, device_id, local_core_id) -> [(device_id, local_core_id, VectorClock, range)]\n   writes: dict = dataclasses.field(\n       default_factory=lambda: collections.defaultdict(list))\n \n@@ -387,7 +414,10 @@ def ranges_overlap(range1: tuple[slice | int, ...],\n   return all(slices_overlap(r1, r2) for r1, r2\n              in itertools.zip_longest(range1, range2, fillvalue=slice(None)))\n \n-def check_read(device_id, clock, buffer_key, rnge, source_info=None):\n+\n+def check_read(\n+    device_id, local_core_id, clock, buffer_key, rnge, source_info=None\n+):\n   if source_info is not None:\n     user_frame = source_info_util.summarize(source_info)\n   else:\n@@ -396,24 +426,36 @@ def check_read(device_id, clock, buffer_key, rnge, source_info=None):\n   with races.lock:\n     writes = races.writes[buffer_key]\n     num_writes = len(writes)\n-    races.reads[buffer_key].append((device_id, clock, rnge, user_frame))\n+    races.reads[buffer_key].append(\n+        (device_id, local_core_id, clock, rnge, user_frame)\n+    )\n \n   for i in range(num_writes):\n-    write_device_id, write_clock, write_range, write_frame = writes[i]\n+    (\n+        write_device_id,\n+        write_local_core_id,\n+        write_clock,\n+        write_range,\n+        write_frame,\n+    ) = writes[i]\n     if ordered(write_clock, clock):\n       continue\n     if not ranges_overlap(rnge, write_range):\n       continue\n     # TODO(jburnim): When printing device IDs for reads/writes, distinguish\n     # between real device IDs vs. DMA IDs.\n-    print('RACE DETECTED\\n'\n-          f'  read of {buffer_key}[{rnge}] from {device_id}, {user_frame}\\n'\n-          f'  write of {buffer_key}[{write_range}] from {write_device_id}, {write_frame}')\n+    print(\n+        f'RACE DETECTED\\n  read of {buffer_key}[{rnge}] from {device_id},'\n+        f' {local_core_id}, {user_frame}\\n  write of'\n+        f' {buffer_key}[{write_range}] from {write_device_id},'\n+        f' {write_local_core_id} {write_frame}'\n+    )\n     with races.lock:\n       races.races_found = True\n     return\n \n-def check_write(device_id, clock, buffer_key, rnge, source_info=None):\n+\n+def check_write(device_id, local_core_id, clock, buffer_key, rnge, source_info=None):\n   if source_info is not None:\n     user_frame = source_info_util.summarize(source_info)\n   else:\n@@ -424,37 +466,50 @@ def check_write(device_id, clock, buffer_key, rnge, source_info=None):\n     reads = races.reads[buffer_key]\n     num_writes = len(writes)\n     num_reads = len(reads)\n-    races.writes[buffer_key].append((device_id, clock, rnge, user_frame))\n+    races.writes[buffer_key].append((device_id, local_core_id, clock, rnge, user_frame))\n \n   # TODO(jburnim): For performance, we should also probably remove any\n   # conflicting reads and writes that happened-before the current write.\n \n   for i in range(num_writes):\n-    write_device_id, write_clock, write_range, write_frame = writes[i]\n+    (\n+        write_device_id,\n+        write_local_core_id,\n+        write_clock,\n+        write_range,\n+        write_frame,\n+    ) = writes[i]\n     if ordered(write_clock, clock):\n       continue\n     if not ranges_overlap(rnge, write_range):\n       continue\n     # TODO(jburnim): When printing device IDs for reads/writes, distinguish\n     # between real device IDs vs. DMA IDs.\n-    print('RACE DETECTED\\n'\n-          f'  write of {buffer_key}[{rnge}] from {device_id}, {user_frame}\\n'\n-          f'  write of {buffer_key}[{write_range}] from {write_device_id}, {write_frame}')\n+    print(\n+        f'RACE DETECTED\\n  write of {buffer_key}[{rnge}] from {device_id},'\n+        f' {local_core_id}, {user_frame}\\n  write of'\n+        f' {buffer_key}[{write_range}] from {write_device_id},'\n+        f' {write_local_core_id}, {write_frame}'\n+    )\n     with races.lock:\n       races.races_found = True\n     break\n \n   for i in range(num_reads):\n-    read_device_id, read_clock, read_range, read_frame = reads[i]\n+    read_device_id, read_local_core_id, read_clock, read_range, read_frame = (\n+        reads[i]\n+    )\n     if ordered(read_clock, clock):\n       continue\n     if not ranges_overlap(rnge, read_range):\n       continue\n     # TODO(jburnim): When printing device IDs for reads/writes, distinguish\n     # between real device IDs vs. DMA IDs.\n-    print('RACE DETECTED\\n'\n-          f'  write of {buffer_key}[{rnge}] from {device_id}, {user_frame}\\n'\n-          f'  read of {buffer_key}[{read_range}] from {read_device_id}, {read_frame}')\n+    print(\n+        f'RACE DETECTED\\n  write of {buffer_key}[{rnge}] from {device_id},'\n+        f' {local_core_id}, {user_frame}\\n  read of {buffer_key}[{read_range}]'\n+        f' from {read_device_id}, {read_local_core_id}, {read_frame}'\n+    )\n     with races.lock:\n       races.races_found = True\n     return\n@@ -464,13 +519,14 @@ def check_write(device_id, clock, buffer_key, rnge, source_info=None):\n class SharedMemory:\n   interpret_params: TPUInterpretParams\n   num_devices: int\n+  num_cores_per_device: int\n   clocks: list[VectorClock]\n   barrier: threading.Barrier\n   clean_up_barrier: threading.Barrier\n \n-  # (memory_space, buffer_id, device_id) -> NumPy array\n+  # (memory_space, buffer_id, device_id, local_core_id) -> NumPy array\n   # TODO(jburnim): Handle Megacore.\n-  mem: dict[tuple[int, int, int], np.ndarray] = dataclasses.field(\n+  mem: dict[tuple[str, int, int, int], np.ndarray] = dataclasses.field(\n       default_factory=dict)\n \n   # semaphore_id -> Semaphore\n@@ -478,15 +534,18 @@ class SharedMemory:\n \n   # (semaphore_id, device_id)\n   #   -> list of DMAs that will signal the semaphore on the given device\n+  # TODO(jburnim): Fix uses of `dmas_by_sem` to align with the two lines of\n+  # documentation above, i.e. index `dmas_by_sem` with\n+  # `(semaphore_id, device_id)` (currently indexed with `semaphore_id only).\n   dmas_by_sem: dict[tuple[int, int], list[DMA]] = dataclasses.field(\n       default_factory=lambda: collections.defaultdict(list))\n \n   lock: threading.Lock = dataclasses.field(default_factory=threading.Lock)\n \n-  # device_id -> next buffer ID\n-  next_buffer_id: dict[int, int] = dataclasses.field(\n+  # (device_id, local_core_id) -> next buffer ID\n+  next_buffer_id: dict[tuple[int, int], int] = dataclasses.field(\n       default_factory=lambda: collections.defaultdict(lambda: 100))\n-  # device_id -> next semaphore ID\n+  # global_core_id -> next semaphore ID\n   next_semaphore_id: dict[int, int] = dataclasses.field(\n       default_factory=lambda: collections.defaultdict(lambda: 2000))\n \n@@ -494,6 +553,10 @@ class SharedMemory:\n \n   deallocated_bytes: int = 0\n \n+  @property\n+  def num_cores(self) -> int:\n+    return self.num_devices * self.num_cores_per_device\n+\n \n # TODO(jburnim): Do we want to support multiple instances of SharedMemory?\n # Maybe for running multiple distinct interpreted computations in parallel?\n@@ -510,34 +573,54 @@ def _clear_shared_memory():\n   with _shared_memory_init_lock:\n     _shared_memory = None\n \n-def _initialize_shared_memory(device_id, num_devices, *, interpret_params):\n+\n+def _initialize_shared_memory(\n+    device_id, num_devices, num_cores_per_device, *, interpret_params\n+):\n   global _shared_memory\n   del device_id\n   num_devices = int(num_devices)\n+  num_cores_per_device = int(num_cores_per_device)\n+  num_cores = num_devices * num_cores_per_device\n   with _shared_memory_init_lock:\n     if _shared_memory is None:\n       _shared_memory = SharedMemory(\n           interpret_params=interpret_params,\n           num_devices=num_devices,\n-          clocks=[make_vector_clock(num_devices) for _ in range(num_devices)],\n+          num_cores_per_device=num_cores_per_device,\n+          clocks=[make_vector_clock(num_cores) for _ in range(num_cores)],\n           barrier=threading.Barrier(\n               num_devices, action=_update_clocks_for_global_barrier),\n           clean_up_barrier=threading.Barrier(\n               num_devices, action=_clear_shared_memory))\n-  assert _shared_memory.num_devices == num_devices\n+  assert _shared_memory.num_cores == num_cores\n \n   global races\n-  races = RaceDetectionState(num_devices=num_devices)\n+  races = RaceDetectionState(num_cores=num_cores)\n \n-def _update_clocks_for_global_barrier():\n+def _update_clocks(low_global_core_id, high_global_core_id):\n+  \"\"\"Synchronizes the vector clocks for the cores with ids in the range between the two arguments.\"\"\"\n   shared_memory = _get_shared_memory()\n+  # Despite only updating the vector clocks for some cores, we still need to\n+  # hold the global lock to ensure that no other devices are concurrently\n+  # accessing the same vector clocks.\n   with shared_memory.lock:\n-    # Set the vector clock for device 0 to the max over all device clocks.\n-    for c in shared_memory.clocks[1:]:\n-      update_vector_clock(shared_memory.clocks[0], c)\n-    # Set all other device vector clocks to the max over all the clocks.\n-    for c in shared_memory.clocks[1:]:\n-      update_vector_clock(c, shared_memory.clocks[0])\n+    for c in shared_memory.clocks[low_global_core_id + 1 : high_global_core_id]:\n+      update_vector_clock(shared_memory.clocks[low_global_core_id], c)\n+    for c in shared_memory.clocks[low_global_core_id + 1 : high_global_core_id]:\n+      update_vector_clock(c, shared_memory.clocks[low_global_core_id])\n+\n+def _update_clocks_for_device_barrier(device_id):\n+  \"\"\"Synchronizes the vector clocks for the cores on the given device.\"\"\"\n+  shared_memory = _get_shared_memory()\n+  low_core_id = device_id * shared_memory.num_cores_per_device\n+  high_core_id = (device_id + 1) * shared_memory.num_cores_per_device\n+  _update_clocks(low_core_id, high_core_id)\n+\n+def _update_clocks_for_global_barrier():\n+  \"\"\"Synchronizes all vector clocks.\"\"\"\n+  shared_memory = _get_shared_memory()\n+  _update_clocks(0, shared_memory.num_cores)\n \n def _barrier(device_id):\n   device_id = int(device_id)\n@@ -564,30 +647,80 @@ def _validate(device_id):\n               f'Semaphore {sem.id} has non-zero count for {device_id} at '\n               f'kernel exit: {sem.counts[device_id]}')\n \n-def _allocate_buffer(device_id, memory_space, val):\n+def _allocate_buffer(\n+    device_id: Array,\n+    local_core_id: Array | None,\n+    memory_space: Array,\n+    val: Array,\n+):\n+  \"\"\"Allocates a memory buffer on the device with id `device_id` and core with id `local_core_id`.\n+\n+  Args:\n+    device_id: Singleton array holding the device id where the buffer will be\n+      allocated.\n+    local_core_id: None or singleton array holding the core id where the buffer\n+      will be allocated. If None, a buffer will be allocated on each cores on\n+      the device.\n+    memory_space: Singleton array indicating the memory space to allocate the\n+      buffer in. If the corresponding memory space is \"any\" (i.e. HBM), at most\n+      one buffer will be allocated and it will belong to (local) core id 0.\n+    val: Array of values to initialize the allocated buffer with.\n+\n+  Returns:\n+    Integer id for the allocated buffer.\n+  \"\"\"\n   device_id = int(device_id)\n-  memory_space = TPU_MEMORY_SPACE_NAMES[int(memory_space)]\n+  memory_space_str = TPU_MEMORY_SPACE_NAMES[int(memory_space)]\n+  del memory_space\n   val = np.array(val)\n \n   shared_memory = _get_shared_memory()\n+\n+  if local_core_id is None:\n+    local_core_id_int = 0\n+    local_core_ids = tuple(range(shared_memory.num_cores_per_device))\n+  else:\n+    local_core_id_int = int(local_core_id)\n+    local_core_ids = (local_core_id_int,)\n+  del local_core_id\n+\n+  local_core_id_to_buffer_id = {}\n   with shared_memory.lock:\n-    buffer_id = shared_memory.next_buffer_id[device_id]\n-    shared_memory.next_buffer_id[device_id] = buffer_id + 1\n-    # TODO(jburnim): Add options for initializing memory (e.g., with NaNs,\n-    # with zeros, or with the buffer ID).\n-    shared_memory.mem[(memory_space, buffer_id, device_id)] = val\n+    for lci in local_core_ids:\n+      buffer_id = shared_memory.next_buffer_id[(device_id, lci)]\n+      shared_memory.next_buffer_id[(device_id, lci)] = buffer_id + 1\n+      if lci == 0 or memory_space_str != 'any':\n+        # If allocating in HBM, only actually allocate a buffer for local core\n+        # id 0.\n+        # TODO(jburnim): Add options for initializing memory (e.g., with NaNs,\n+        # with zeros, or with the buffer ID).\n+        shared_memory.mem[(memory_space_str, buffer_id, device_id, lci)] = val\n+\n+      local_core_id_to_buffer_id[lci] = buffer_id\n+\n+  # The buffer ids should always be kept in sync across all cores.\n+  assert all(\n+      buffer_id == local_core_id_to_buffer_id[local_core_id_int]\n+      for buffer_id in local_core_id_to_buffer_id.values()\n+  )\n \n   # TODO(jburnim): Raise an error if buffer_id is too big for int16.\n-  return np.int16(buffer_id)\n+  return np.int16(local_core_id_to_buffer_id[local_core_id_int])\n \n-def _deallocate_buffer(device_id, memory_space, buffer_id):\n+def _deallocate_buffer(device_id, local_core_id, memory_space, buffer_id):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   memory_space = TPU_MEMORY_SPACE_NAMES[int(memory_space)]\n   buffer_id = int(buffer_id)\n \n+  if memory_space == 'any':\n+    local_core_id = 0\n+\n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n-    buff = shared_memory.mem.pop((memory_space, buffer_id, device_id))\n+    buff = shared_memory.mem.pop(\n+        (memory_space, buffer_id, device_id, local_core_id)\n+    )\n     shared_memory.deallocated_bytes += buff.size * buff.itemsize\n     del buff\n \n@@ -600,26 +733,80 @@ def _deallocate_buffer(device_id, memory_space, buffer_id):\n     # why arrays are not getting freed without this.\n     gc.collect()\n \n-def _allocate_semaphores(device_id, shape):\n+\n+def _allocate_semaphores(\n+    device_id: Array, local_core_id: Array | None, shape: Array\n+):\n+  \"\"\"Allocates semaphores on the device with id `device_id` and core with id `local_core_id`.\n+\n+  The number of sempahores allocated is given by the product of the entries in\n+  `shape`.\n+\n+  Since for each semaphore id there is really only one global `Semaphore`\n+  object, 'allocation' of semaphores per device and core here means that the\n+  internal counter of semaphore ids that is held by `SharedMemory` is\n+  incremented for each the device and core (or for all cores on the dive if\n+  argument `local_core_id` is None, see below).\n+\n+  Args:\n+    device_id: Singleton array holding the id for the device where the\n+      semaphores will be allocated.\n+    local_core_id: None or singleton array holding the id for the core where the\n+      semaphores will be allocated. If None, semaphores will be allocated on all\n+      cores on the device.\n+    shape: Shape of the semaphore array to allocate.\n+\n+  Returns:\n+    Array of semaphore ids.\n+  \"\"\"\n   device_id = int(device_id)\n   shape = tuple(map(int, shape))\n   num_semaphores = math.prod(shape)\n \n   shared_memory = _get_shared_memory()\n+\n+  if local_core_id is None:\n+    local_core_id_int = 0\n+    global_core_ids = tuple(\n+        _get_global_core_id(device_id, core_id)\n+        for core_id in range(shared_memory.num_cores_per_device)\n+    )\n+  else:\n+    local_core_id_int = int(local_core_id)\n+    global_core_ids = (_get_global_core_id(device_id, local_core_id_int),)\n+  del local_core_id\n+\n+  global_core_id_to_semaphore_id = {}\n   with shared_memory.lock:\n-    semaphore_id = shared_memory.next_semaphore_id[device_id]\n-    shared_memory.next_semaphore_id[device_id] = semaphore_id + num_semaphores\n-    for i in range(semaphore_id, semaphore_id + num_semaphores):\n-      if i not in shared_memory.sem:\n-        shared_memory.sem[i] = Semaphore(i)\n+    for gci in global_core_ids:\n+      semaphore_id = shared_memory.next_semaphore_id[gci]\n+      shared_memory.next_semaphore_id[gci] = (\n+          semaphore_id + num_semaphores\n+      )\n+\n+      # Ensure that only one global `Semaphore` object is allocated for each\n+      # `semaphore_id`.\n+      for i in range(semaphore_id, semaphore_id + num_semaphores):\n+        if i not in shared_memory.sem:\n+          shared_memory.sem[i] = Semaphore(i)\n+\n+      global_core_id_to_semaphore_id[gci] = semaphore_id\n+\n+  global_core_id = _get_global_core_id(device_id, local_core_id_int)\n+  # The semaphore ids should always be kept in sync across all cores.\n+  assert all(\n+      semaphore_id == global_core_id_to_semaphore_id[global_core_id]\n+      for semaphore_id in global_core_id_to_semaphore_id.values()\n+  )\n \n   # NOTE: For now, we use a relatively uncommon datatype (int16) for\n   # semaphore (and buffer) IDs, so these values are more easily identifiable\n   # in kernels.\n   #\n   # TODO(jburnim): Raise an error if any IDs are too big for int16.\n-  return np.int16(\n-      range(semaphore_id, semaphore_id + num_semaphores)\n+  semaphore_id = global_core_id_to_semaphore_id[global_core_id]\n+  return np.arange(\n+      semaphore_id, semaphore_id + num_semaphores, dtype=np.int16\n   ).reshape(shape)\n \n \n@@ -693,24 +880,48 @@ def _to_range(transforms) -> tuple[slice | int, ...]:\n         ret, tuple(_transform_slice_or_index(i) for i in transform.indices))\n   return ret\n \n-def get(device_id, memory_space, buffer_id, transforms, *,\n-        src_device_id=None, clock=None, source_info=None):\n+def _to_int(x : int | Array | None) -> int | None:\n+  \"\"\"Converts a value to an integer, or returns None if the value is None.\"\"\"\n+  if x is None:\n+    return None\n+  return int(x)\n+\n+def get(\n+    device_id,\n+    local_core_id,\n+    memory_space,\n+    buffer_id,\n+    transforms,\n+    *,\n+    src_device_id=None,\n+    src_local_core_id=None,\n+    clock=None,\n+    source_info=None,\n+):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   memory_space = TPU_MEMORY_SPACE_NAMES[int(memory_space)]\n   buffer_id = int(buffer_id)\n   try:\n     transforms = jax.tree.map(int, transforms)\n   except:\n     raise ValueError('Advanced indexers are not supported on TPU')\n+  src_device_id = _to_int(src_device_id)\n+  src_local_core_id = _to_int(src_local_core_id)\n+\n+  local_core_id_for_buffer = 0 if memory_space == 'any' else local_core_id\n+  global_core_id = _get_global_core_id(device_id, local_core_id)\n \n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n     read_range = _to_range(transforms)\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n+      inc_vector_clock(shared_memory.clocks[global_core_id], global_core_id)\n       if clock is None:\n-        clock = copy_vector_clock(shared_memory.clocks[device_id])\n-    buffer = shared_memory.mem[(memory_space, buffer_id, device_id)]\n+        clock = copy_vector_clock(shared_memory.clocks[global_core_id])\n+    buffer = shared_memory.mem[\n+        (memory_space, buffer_id, device_id, local_core_id_for_buffer)\n+    ]\n     ret = buffer[read_range].copy()\n     if transforms:\n       # TODO(jburnim): Instead of using NDIndexer, do the computation ourselves\n@@ -718,20 +929,43 @@ def get(device_id, memory_space, buffer_id, transforms, *,\n       expected_shape = transforms[-1].get_indexer_shape()\n       if expected_shape != ret.shape[:len(expected_shape)]:\n         raise ValueError(\n-            f'Out-of-bounds read of ({device_id} {memory_space} {buffer_id}): '\n-            f'reading [{read_range}] but bufer has shape {buffer.shape} .')\n+            'Out-of-bounds read of'\n+            f' ({device_id} {local_core_id} {memory_space} {buffer_id}):'\n+            f' reading [{read_range}] but bufer has shape {buffer.shape} .'\n+        )\n \n   if shared_memory.interpret_params.detect_races:\n     if src_device_id is None:\n       src_device_id = device_id\n-    check_read(src_device_id, clock, (memory_space, buffer_id, device_id),\n-               read_range, source_info=source_info)\n+    if src_local_core_id is None:\n+      src_local_core_id = local_core_id\n+    check_read(\n+        src_device_id,\n+        src_local_core_id,\n+        clock,\n+        (memory_space, buffer_id, device_id, local_core_id_for_buffer),\n+        read_range,\n+        source_info=source_info,\n+    )\n \n   return ret\n \n-def store(device_id, memory_space, buffer_id, transforms, val, *,\n-          src_device_id=None, clock=None, source_info=None):\n+\n+def store(\n+    device_id,\n+    local_core_id,\n+    memory_space,\n+    buffer_id,\n+    transforms,\n+    val,\n+    *,\n+    src_device_id=None,\n+    src_local_core_id=None,\n+    clock=None,\n+    source_info=None,\n+):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   memory_space = TPU_MEMORY_SPACE_NAMES[int(memory_space)]\n   buffer_id = int(buffer_id)\n   try:\n@@ -739,38 +973,67 @@ def store(device_id, memory_space, buffer_id, transforms, val, *,\n   except:\n     raise ValueError('Advanced indexers are not supported on TPU')\n   val = np.array(val)\n+  src_device_id = _to_int(src_device_id)\n+  src_local_core_id = _to_int(src_local_core_id)\n+\n+  local_core_id_for_buffer = 0 if memory_space == 'any' else local_core_id\n+  global_core_id = _get_global_core_id(device_id, local_core_id)\n \n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n+      inc_vector_clock(shared_memory.clocks[global_core_id], global_core_id)\n       if clock is None:\n-        clock = copy_vector_clock(shared_memory.clocks[device_id])\n+        clock = copy_vector_clock(shared_memory.clocks[global_core_id])\n \n-    buff = shared_memory.mem[(memory_space, buffer_id, device_id)]\n+    buff = shared_memory.mem[\n+        (memory_space, buffer_id, device_id, local_core_id_for_buffer)\n+    ]\n     assert buff.dtype == val.dtype  # TODO(jburnim): Catch this statically.\n     write_range = _to_range(transforms)\n     # TODO(jburnim): Better error message if this raises?\n     in_bounds_shape = buff[write_range].shape\n     if in_bounds_shape != val.shape:\n       raise ValueError(\n-          f'Out-of-bounds write of ({device_id} {memory_space} {buffer_id}): '\n-          f'writing [{write_range}] but buffer has shape {buff.shape} .')\n+          'Out-of-bounds write of'\n+          f' ({device_id} {local_core_id} {memory_space} {buffer_id}): writing'\n+          f' [{write_range}] but buffer has shape {buff.shape} .'\n+      )\n     buff[write_range] = val\n \n   if shared_memory.interpret_params.detect_races:\n     if src_device_id is None:\n       src_device_id = device_id\n-    check_write(src_device_id, clock, (memory_space, buffer_id, device_id),\n-                write_range, source_info=source_info)\n+    if src_local_core_id is None:\n+      src_local_core_id = local_core_id\n+    check_write(\n+        src_device_id,\n+        src_local_core_id,\n+        clock,\n+        (memory_space, buffer_id, device_id, local_core_id_for_buffer),\n+        write_range,\n+        source_info=source_info,\n+    )\n+\n \n-def swap(device_id, memory_space, buffer_id, transforms, val, mask, *,\n-         source_info=None):\n+def swap(\n+    device_id,\n+    local_core_id,\n+    memory_space,\n+    buffer_id,\n+    transforms,\n+    val,\n+    mask,\n+    *,\n+    source_info=None,\n+):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   memory_space = TPU_MEMORY_SPACE_NAMES[int(memory_space)]\n   buffer_id = int(buffer_id)\n   try:\n     transforms = jax.tree.map(int, transforms)\n+    # jax.debug.print(f'swap: {transforms}')\n   except:\n     raise ValueError('Advanced indexers are not supported on TPU')\n   val = np.array(val)\n@@ -778,12 +1041,17 @@ def swap(device_id, memory_space, buffer_id, transforms, val, mask, *,\n   if mask is not None:\n     assert mask.shape == val.shape\n \n+  local_core_id_for_buffer = 0 if memory_space == 'any' else local_core_id\n+  global_core_id = _get_global_core_id(device_id, local_core_id)\n+\n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n-      clock = copy_vector_clock(shared_memory.clocks[device_id])\n-    buff = shared_memory.mem[(memory_space, buffer_id, device_id)]\n+      inc_vector_clock(shared_memory.clocks[global_core_id], global_core_id)\n+      clock = copy_vector_clock(shared_memory.clocks[global_core_id])\n+    buff = shared_memory.mem[\n+        (memory_space, buffer_id, device_id, local_core_id_for_buffer)\n+    ]\n     assert buff.dtype == val.dtype  # TODO(jburnim): Catch this statically.\n     read_write_range = _to_range(transforms)\n     # TODO(jburnim): Better error message if this raises?\n@@ -792,8 +1060,11 @@ def swap(device_id, memory_space, buffer_id, transforms, val, mask, *,\n     if mask is None:\n       if in_bounds_shape != val.shape:\n         raise ValueError(\n-            f'Out-of-bounds swap of ({device_id} {memory_space} {buffer_id}): '\n-            f'swapping [{read_write_range}] but buffer has shape {buff.shape} .')\n+            'Out-of-bounds swap of'\n+            f' ({device_id} {local_core_id} {memory_space} {buffer_id}):'\n+            f' swapping [{read_write_range}] but buffer has shape'\n+            f' {buff.shape} .'\n+        )\n       buff[read_write_range] = val\n       return raw_result.copy()\n \n@@ -804,8 +1075,10 @@ def swap(device_id, memory_space, buffer_id, transforms, val, mask, *,\n       # TODO(jburnim): Include indices of out-of-bounds locations where mask\n       # is True.\n       raise ValueError(\n-          f'Out-of-bounds masked swap of ({device_id} {memory_space} {buffer_id}): '\n-          f'swapping [{read_write_range}] but buffer has shape {buff.shape} . ')\n+          'Out-of-bounds masked swap of'\n+          f' ({device_id} {local_core_id} {memory_space} {buffer_id}): swapping'\n+          f' [{read_write_range}] but buffer has shape {buff.shape} . '\n+      )\n \n     in_bounds_idx = tuple(slice(i) for i in in_bounds_shape)\n     result = val.copy()\n@@ -815,8 +1088,14 @@ def swap(device_id, memory_space, buffer_id, transforms, val, mask, *,\n         mask[in_bounds_idx], val[in_bounds_idx], raw_result)\n \n   if shared_memory.interpret_params.detect_races:\n-    check_write(device_id, clock, (memory_space, buffer_id, device_id),\n-                read_write_range, source_info=source_info)\n+    check_write(\n+        device_id,\n+        local_core_id,\n+        clock,\n+        (memory_space, buffer_id, device_id, local_core_id_for_buffer),\n+        read_write_range,\n+        source_info=source_info,\n+    )\n   return result\n \n def execute_dma(dma):\n@@ -828,17 +1107,19 @@ def execute_dma(dma):\n     if dma.virtual_device_id is None:\n       # See comment in Semaphore.wait .\n       dma.virtual_device_id = np.random.randint(\n-          shared_memory.num_devices, NUM_VIRTUAL_DEVICES)\n+          shared_memory.num_cores, NUM_VIRTUAL_CORES)\n \n     # Do the read.\n     if shared_memory.interpret_params.detect_races:\n       inc_vector_clock(dma.clock, dma.virtual_device_id)\n     dma.data = get(dma.src_device_id,\n+                   dma.src_local_core_id,\n                    dma.src_memory_space,\n                    dma.src_buffer_id,\n                    dma.src_transforms,\n                    clock=copy_vector_clock(dma.clock),\n                    src_device_id=dma.id,\n+                   src_local_core_id=0,\n                    source_info=dma.source_info)\n     data_size = dma.data.itemsize * dma.data.size\n \n@@ -847,19 +1128,26 @@ def execute_dma(dma):\n       inc_vector_clock(dma.clock, dma.virtual_device_id)\n     if dma.src_sem is not None:\n       dma.src_sem.signal(\n-          data_size, device_id=dma.src_device_id, clock=dma.clock)\n+          data_size,\n+          global_core_id=_get_global_core_id(\n+              dma.src_device_id, dma.src_local_core_id\n+          ),\n+          clock=dma.clock,\n+      )\n     dma.state = DmaState.READ\n \n     # Do the write.\n     if shared_memory.interpret_params.detect_races:\n       inc_vector_clock(dma.clock, dma.virtual_device_id)\n     store(dma.dst_device_id,\n+          dma.dst_local_core_id,\n           dma.dst_memory_space,\n           dma.dst_buffer_id,\n           dma.dst_transforms,\n           dma.data,\n           clock=copy_vector_clock(dma.clock),\n           src_device_id=dma.id,\n+          src_local_core_id=0,\n           source_info=dma.source_info)\n \n     # Signal the receive semaphore.\n@@ -867,7 +1155,12 @@ def execute_dma(dma):\n       inc_vector_clock(dma.clock, dma.virtual_device_id)\n     if dma.dst_sem is not None:\n       dma.dst_sem.signal(\n-          data_size, device_id=dma.dst_device_id, clock=dma.clock)\n+          data_size,\n+          global_core_id=_get_global_core_id(\n+              dma.dst_device_id, dma.dst_local_core_id\n+          ),\n+          clock=dma.clock,\n+      )\n \n     dma.data = None\n     dma.state = DmaState.COMPLETED\n@@ -879,11 +1172,24 @@ def print_memory(device_id):\n     with shared_memory.lock:\n       print(shared_memory.mem)\n \n-def dma_start(device_id, src_memory_space, src_id, src_transforms,\n-              dst_memory_space, dst_id, dst_transforms,\n-              dst_sem_id, src_sem_id, dst_device_id,\n-              source_info=None):\n+\n+def dma_start(\n+    device_id,\n+    src_local_core_id,\n+    src_memory_space,\n+    src_id,\n+    src_transforms,\n+    dst_memory_space,\n+    dst_id,\n+    dst_transforms,\n+    dst_sem_id,\n+    src_sem_id,\n+    dst_device_id,\n+    source_info=None,\n+):\n   device_id = int(device_id)\n+  src_local_core_id = int(src_local_core_id)\n+  src_global_core_id = _get_global_core_id(device_id, src_local_core_id)\n   src_memory_space, src_id = int(src_memory_space), int(src_id)\n   src_transforms = jax.tree.map(int, src_transforms)\n   dst_memory_space, dst_id = int(dst_memory_space), int(dst_id)\n@@ -902,15 +1208,25 @@ def dma_start(device_id, src_memory_space, src_id, src_transforms,\n \n     clock = None\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n-      clock = copy_vector_clock(shared_memory.clocks[device_id])\n+      inc_vector_clock(\n+          shared_memory.clocks[src_global_core_id], src_global_core_id\n+      )\n+      clock = copy_vector_clock(shared_memory.clocks[src_global_core_id])\n     dma_id = shared_memory.next_dma_id\n     shared_memory.next_dma_id += 1\n \n     dma = DMA(\n         dma_id,\n-        device_id, src_memory_space, src_id, src_transforms,\n-        dst_device_id, dst_memory_space, dst_id, dst_transforms,\n+        device_id,\n+        src_local_core_id,\n+        src_memory_space,\n+        src_id,\n+        src_transforms,\n+        dst_device_id,\n+        src_local_core_id,  # Same core on destination device as on source.\n+        dst_memory_space,\n+        dst_id,\n+        dst_transforms,\n         src_sem,\n         dst_sem,\n         clock=clock,\n@@ -926,52 +1242,61 @@ def dma_start(device_id, src_memory_space, src_id, src_transforms,\n   assert shared_memory.interpret_params.dma_execution_mode == 'eager'\n   execute_dma(dma)\n \n-def dma_wait(device_id, sem_id, size):\n+def dma_wait(device_id, local_core_id, sem_id, size):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   sem_id = int(sem_id)\n   size = int(size)\n+  global_core_id = _get_global_core_id(device_id, local_core_id)\n \n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n+      inc_vector_clock(shared_memory.clocks[global_core_id], global_core_id)\n     sem = shared_memory.sem[sem_id]\n-  sem.wait(size, device_id, is_dma=True)\n+  sem.wait(size, global_core_id, is_dma=True)\n \n-def semaphore_signal(device_id, sem_id, inc, target_device_id,\n-                     target_core_index):\n+def semaphore_signal(device_id, local_core_id, sem_id, inc, target_device_id,\n+                     target_local_core_id):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   sem_id = int(sem_id)\n   inc = int(inc)\n+  src_global_core_id = _get_global_core_id(device_id, local_core_id)\n   if target_device_id is None:\n     target_device_id = device_id\n   else:\n     target_device_id = int(target_device_id)\n \n-  if target_core_index is not None:\n-    if int(target_core_index) != 0:\n-      raise NotImplementedError('semaphore_signal with target_core_index != 0')\n+  if target_local_core_id is None:\n+    target_local_core_id = 0\n \n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n     clock = None\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n-      clock = copy_vector_clock(shared_memory.clocks[device_id])\n+      inc_vector_clock(\n+          shared_memory.clocks[src_global_core_id], src_global_core_id\n+      )\n+      clock = copy_vector_clock(shared_memory.clocks[src_global_core_id])\n     sem = shared_memory.sem[sem_id]\n-  sem.signal(inc, target_device_id, clock)\n+  sem.signal(\n+      inc, _get_global_core_id(target_device_id, target_local_core_id), clock\n+  )\n \n-def semaphore_wait(device_id, sem_id, value):\n+def semaphore_wait(device_id, local_core_id, sem_id, value):\n   device_id = int(device_id)\n+  local_core_id = int(local_core_id)\n   sem_id = int(sem_id)\n   value = int(value)\n+  global_core_id = _get_global_core_id(device_id, local_core_id)\n \n   shared_memory = _get_shared_memory()\n   with shared_memory.lock:\n     if shared_memory.interpret_params.detect_races:\n-      inc_vector_clock(shared_memory.clocks[device_id], device_id)\n+      inc_vector_clock(shared_memory.clocks[global_core_id], global_core_id)\n     sem = shared_memory.sem[sem_id]\n-  sem.wait(value, device_id)\n+  sem.wait(value, global_core_id)\n \n def _compute_transformed_shape_and_dtype(shape, dtype, transforms):\n   for transform in transforms:\n@@ -1022,7 +1347,10 @@ class Placeholder:\n   shape: tuple[int, ...]\n   dtype: jnp.dtype\n \n-def _interpret_jaxpr(jaxpr, *args, mesh, compiler_params, interpret_params):\n+\n+def _interpret_jaxpr(\n+    jaxpr, *args, mesh, local_core_id, compiler_params, interpret_params\n+):\n   env = {}\n \n   def read(var):\n@@ -1054,8 +1382,12 @@ def write(var, value):\n   #  - Handle other higher-order primitives?\n   #  - Megacore.\n   _interpret = functools.partial(\n-      _interpret_jaxpr, mesh=mesh, compiler_params=compiler_params,\n-      interpret_params=interpret_params)\n+      _interpret_jaxpr,\n+      mesh=mesh,\n+      local_core_id=local_core_id,\n+      compiler_params=compiler_params,\n+      interpret_params=interpret_params,\n+  )\n   for eqn in jaxpr.eqns:\n     with source_info_util.user_context(\n          eqn.source_info.traceback, name_stack=eqn.source_info.name_stack):\n@@ -1065,7 +1397,9 @@ def write(var, value):\n       # not need to do any reads if `interpret_params.skip_floating_point_ops`\n       # is True. If this is the case, we want to avoid materializing the read\n       # array into the jaxpr when this function is traced.\n-      deferred_invals = functools.partial(jax._src.util.safe_map, read, eqn.invars)\n+      deferred_invals = functools.partial(\n+          jax._src.util.safe_map, read, eqn.invars\n+      )\n \n       if prim is primitives.load_p:\n         (ref, transforms, mask, _) = jax.tree.unflatten(\n@@ -1076,6 +1410,7 @@ def write(var, value):\n             functools.partial(get, source_info=eqn.source_info),\n             eqn.outvars[0].aval,\n             device_id,\n+            local_core_id,\n             TPU_MEMORY_SPACE_IDXS[eqn.invars[0].aval.memory_space],\n             ref,\n             transforms,\n@@ -1088,6 +1423,7 @@ def write(var, value):\n             functools.partial(swap, source_info=eqn.source_info),\n             eqn.outvars[0].aval,\n             device_id,\n+            local_core_id,\n             TPU_MEMORY_SPACE_IDXS[eqn.invars[0].aval.memory_space],\n             ref,\n             transforms,\n@@ -1174,7 +1510,8 @@ def f(*args, jaxpr):\n               'run_scoped_p with collective axes is not supported'\n           )\n         # Allocate a buffer or semaphore for each element of\n-        # eqn.params['jaxpr'].invars .\n+        # eqn.params['jaxpr'].invars. It is assumed that each core\n+        # runs the same sequence of `run_scoped`s.\n         allocs = []\n         for v in eqn.params['jaxpr'].invars:\n           if v.aval.memory_space == mosaic_core.TPUMemorySpace.SEMAPHORE:\n@@ -1182,6 +1519,7 @@ def f(*args, jaxpr):\n                 _allocate_semaphores,\n                 jax.ShapeDtypeStruct(v.aval.shape, jnp.int16),\n                 device_id,\n+                local_core_id,\n                 v.aval.shape,\n                 ordered=True))\n           else:\n@@ -1189,6 +1527,7 @@ def f(*args, jaxpr):\n                 _allocate_buffer,\n                 jax.ShapeDtypeStruct((), jnp.int16),\n                 device_id,\n+                local_core_id,\n                 TPU_MEMORY_SPACE_IDXS[v.aval.memory_space],\n                 _uninitialized_value(\n                     v.aval.shape, v.aval.dtype, interpret_params),\n@@ -1211,6 +1550,7 @@ def f(*args, jaxpr):\n                 _deallocate_buffer,\n                 None,\n                 device_id,\n+                local_core_id,\n                 TPU_MEMORY_SPACE_IDXS[v.aval.memory_space],\n                 a,\n                 ordered=True)\n@@ -1221,6 +1561,7 @@ def f(*args, jaxpr):\n             functools.partial(get, source_info=eqn.source_info),\n             eqn.outvars[0].aval,\n             device_id,\n+            local_core_id,\n             TPU_MEMORY_SPACE_IDXS[eqn.invars[0].aval.memory_space],\n             invals[0],\n             jax.tree.unflatten(eqn.params['tree'], invals[1:]),\n@@ -1232,6 +1573,7 @@ def f(*args, jaxpr):\n             functools.partial(swap, source_info=eqn.source_info),\n             eqn.outvars[0].aval,\n             device_id,\n+            local_core_id,\n             TPU_MEMORY_SPACE_IDXS[eqn.invars[0].aval.memory_space],\n             invals[0],\n             jax.tree.unflatten(eqn.params['tree'], invals[2:]),\n@@ -1259,6 +1601,7 @@ def f(*args, jaxpr):\n             functools.partial(dma_start, source_info=eqn.source_info),\n             (),\n             device_id,\n+            local_core_id,\n             TPU_MEMORY_SPACE_IDXS[getattr(orig_src_ref.aval, 'memory_space', mosaic_core.TPUMemorySpace.ANY)],\n             src, src_transforms,\n             TPU_MEMORY_SPACE_IDXS[getattr(orig_dst_ref.aval, 'memory_space', mosaic_core.TPUMemorySpace.ANY)],\n@@ -1287,6 +1630,7 @@ def f(*args, jaxpr):\n             dma_wait,\n             (),\n             device_id,\n+            local_core_id,\n             state_discharge.transform_array(dst_sem, dst_sem_transforms),\n             math.prod(read_shape) * read_dtype.itemsize,\n             ordered=True)\n@@ -1309,6 +1653,7 @@ def f(*args, jaxpr):\n             semaphore_signal,\n             (),\n             device_id,\n+            local_core_id,\n             state_discharge.transform_array(sem, sem_transforms),\n             inc,\n             target_device_id,\n@@ -1323,6 +1668,7 @@ def f(*args, jaxpr):\n             semaphore_wait,\n             (),\n             device_id,\n+            local_core_id,\n             state_discharge.transform_array(sem, sem_transforms),\n             value,\n             ordered=True)\n@@ -1358,8 +1704,15 @@ def _compute_start_indices(\n     block_mapping, loop_idx, *args, mesh, compiler_params, interpret_params):\n   jaxpr = block_mapping.index_map_jaxpr\n   block_indices = _interpret_jaxpr(\n-      jaxpr.jaxpr, *jaxpr.consts, *loop_idx, *args, mesh=mesh,\n-      compiler_params=compiler_params, interpret_params=interpret_params)\n+      jaxpr.jaxpr,\n+      *jaxpr.consts,\n+      *loop_idx,\n+      *args,\n+      mesh=mesh,\n+      local_core_id=0,\n+      compiler_params=compiler_params,\n+      interpret_params=interpret_params,\n+  )\n   def _get_start_index(i, b):\n     match b:\n       case pallas_core.Squeezed():\n@@ -1397,12 +1750,12 @@ def _get_mosaic_params(compiler_params: dict[str, pallas_core.CompilerParams]) -\n \n \n def _get_parallel_dim_semantics(\n-    compiler_params: dict[str, Any], grid: tuple[int, ...]\n+    compiler_params: dict[str, Any], num_dimensions_in_grid: int,\n ) -> tuple[bool, ...]:\n-  \"\"\"Returns a tuple of booleans indicating whether the corresponding dimension in `grid` is parallel.\"\"\"\n+  \"\"\"Returns a tuple of booleans indicating whether the corresponding dimension in the grid is parallel.\"\"\"\n   mosaic_params = _get_mosaic_params(compiler_params)\n   if mosaic_params.dimension_semantics is None:\n-    return (False,) * len(grid)\n+    return (False,) * num_dimensions_in_grid\n   return tuple(ds == 'parallel' for ds in mosaic_params.dimension_semantics)\n \n _GridPointCoordinatesPerDim = tuple[Array, ...]\n@@ -1432,7 +1785,7 @@ def _get_randomized_grid_coordinates(\n       dimensions.\n   \"\"\"\n   parallel_semantics_per_dim = _get_parallel_dim_semantics(\n-      compiler_params, grid\n+      compiler_params, len(grid)\n   )\n \n   key = jax.random.key(random_seed or 0)\n@@ -1484,6 +1837,23 @@ def _get_grid_point(\n   return jnp.array(grid_point, dtype=np.int32)\n \n \n+def _get_next_local_core_id(\n+    local_core_id: int,\n+    parallel_semantics_per_dim: tuple[bool, ...],\n+    grid_point: Array,\n+    next_grid_point: Array,\n+    interpret_params: TPUInterpretParams,\n+) -> int:\n+  delta = next_grid_point - grid_point\n+  assert delta.shape == (len(parallel_semantics_per_dim),)\n+  parallel_semantics_per_dim = jnp.array(parallel_semantics_per_dim)\n+  deltas_along_parallel_dims = jnp.where(parallel_semantics_per_dim, delta, 0)\n+  return jax.lax.cond(\n+      jnp.any(deltas_along_parallel_dims),\n+      lambda: (local_core_id + 1) % interpret_params.num_cores_per_device,\n+      lambda: local_core_id,\n+  )\n+\n def _uninitialized_value(shape, dtype, interpret_params):\n   if interpret_params.uninitialized_memory == 'nan':\n     if jnp.issubdtype(dtype, jnp.floating):\n@@ -1562,6 +1932,7 @@ def interpret_pallas_call(\n       (),\n       device_id,\n       num_devices,\n+      interpret_params.num_cores_per_device,\n       ordered=True)\n \n   # Pad input arguments.\n@@ -1591,6 +1962,7 @@ def interpret_pallas_call(\n         _allocate_buffer,\n         jax.ShapeDtypeStruct((), jnp.int16),\n         device_id,\n+        None,  # local_core_id\n         TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n         input_args[i],\n         ordered=True))\n@@ -1613,14 +1985,19 @@ def interpret_pallas_call(\n                                      bm.array_shape_dtype.dtype,\n                                      interpret_params)\n       padded_val = _pad_to_block_dimension(\n-          out_val, output_block_shapes[i], interpret_params)\n-      output_buffer_ids.append(callback.io_callback(\n-          _allocate_buffer,\n-          jax.ShapeDtypeStruct((), jnp.int16),\n-          device_id,\n-          TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n-          padded_val,\n-          ordered=True))\n+          out_val, output_block_shapes[i], interpret_params\n+      )\n+      output_buffer_ids.append(\n+          callback.io_callback(\n+              _allocate_buffer,\n+              jax.ShapeDtypeStruct((), jnp.int16),\n+              device_id,\n+              None,  # local_core_id\n+              TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n+              padded_val,\n+              ordered=True,\n+          )\n+      )\n       output_buffer_shapes.append(padded_val.shape)\n       output_vals.append(out_val)\n \n@@ -1630,25 +2007,34 @@ def interpret_pallas_call(\n   for var, val in zip(jaxpr.invars[grid_mapping.slice_index_ops], scalars):\n     assert var.aval.shape == val.shape\n     assert var.aval.dtype == val.dtype\n-    scalar_buffer_ids.append(callback.io_callback(\n-        _allocate_buffer,\n-        jax.ShapeDtypeStruct((), jnp.int16),\n-        device_id,\n-        TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.SMEM],\n-        val,\n-        ordered=True))\n+    scalar_buffer_ids.append(\n+        callback.io_callback(\n+            _allocate_buffer,\n+            jax.ShapeDtypeStruct((), jnp.int16),\n+            device_id,\n+            None,  # local_core_id,\n+            TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.SMEM],\n+            val,\n+            ordered=True,\n+        )\n+    )\n+\n   kernel_buffer_ids = scalar_buffer_ids.copy()\n   for i, var in enumerate(jaxpr.invars[grid_mapping.num_index_operands:]):\n     output_idx = i - grid_mapping.num_inputs\n     is_input = i < grid_mapping.num_inputs\n     is_output = (output_idx >= 0) and (output_idx < grid_mapping.num_outputs)\n     if var.aval.memory_space == mosaic_core.TPUMemorySpace.SEMAPHORE:\n-      kernel_buffer_ids.append(callback.io_callback(\n-          _allocate_semaphores,\n-          jax.ShapeDtypeStruct(var.aval.shape, jnp.int16),\n-          device_id,\n-          var.aval.shape,\n-          ordered=True))\n+      kernel_buffer_ids.append(\n+          callback.io_callback(\n+              _allocate_semaphores,\n+              jax.ShapeDtypeStruct(var.aval.shape, jnp.int16),\n+              device_id,\n+              None,  # local_core_id\n+              var.aval.shape,\n+              ordered=True,\n+          )\n+      )\n     elif _is_any(var.aval.memory_space):\n       # Use the already-allocated HBM input or output buffer.\n       #\n@@ -1661,14 +2047,19 @@ def interpret_pallas_call(\n       if is_output:\n         kernel_buffer_ids.append(output_buffer_ids[output_idx])\n     else:\n-      kernel_buffer_ids.append(callback.io_callback(\n-          _allocate_buffer,\n-          jax.ShapeDtypeStruct((), jnp.int16),\n-          device_id,\n-          TPU_MEMORY_SPACE_IDXS[var.aval.memory_space],\n-          _uninitialized_value(\n-              var.aval.shape, var.aval.dtype, interpret_params),\n-          ordered=True))\n+      kernel_buffer_ids.append(\n+          callback.io_callback(\n+              _allocate_buffer,\n+              jax.ShapeDtypeStruct((), jnp.int16),\n+              device_id,\n+              None,  # local_core_id,\n+              TPU_MEMORY_SPACE_IDXS[var.aval.memory_space],\n+              _uninitialized_value(\n+                  var.aval.shape, var.aval.dtype, interpret_params\n+              ),\n+              ordered=True,\n+          )\n+      )\n \n   if _get_mosaic_params(compiler_params).collective_id is None:\n     # The kernel doesn't specify its own barrier semaphore, so we do a global\n@@ -1687,6 +2078,9 @@ def interpret_pallas_call(\n     # Base case is always one iteration when grid is ()\n     num_iterations = 1\n \n+  parallel_semantics_per_dim = _get_parallel_dim_semantics(\n+      compiler_params, len(grid)\n+  )\n   randomized_grid_coordinates = _get_randomized_grid_coordinates(\n       grid, compiler_params, interpret_params.random_seed  # type: ignore[arg-type]\n   )\n@@ -1703,18 +2097,38 @@ def _get_local_grid_env(loop_idx):\n \n   def body(\n       carry: tuple[\n-          jnp.int32, tuple[jnp.int32, ...], list[jnp.ndarray], list[jnp.ndarray]\n+          jnp.int32,\n+          tuple[jnp.int32, ...],\n+          jnp.ndarray,\n+          jnp.int32,\n+          jnp.int32,\n+          list[jnp.ndarray],\n+          list[jnp.ndarray],\n       ],\n-  ):\n+  ) -> tuple[\n+      jnp.int32,\n+      tuple[jnp.int32, ...],\n+      jnp.ndarray,\n+      jnp.int32,\n+      jnp.int32,\n+      list[jnp.ndarray],\n+      list[jnp.ndarray],\n+  ]:\n     \"\"\"Performs a single iteration of `jaxpr` in the device grid.\n \n     Execution of `jaxpr` is preceded by reading kernel input buffers and\n     followed by writing kernel output buffers.\n \n     Args:\n-      carry: (iteration_idx, loop_idx, prev_start_indices, cur_start_indices).\n+      carry: (iteration_idx, loop_idx, grid_point, prev_local_core_id,\n+              cur_local_core_id, prev_start_indices, cur_start_indices).\n         - iteration_idx is the interation index.\n         - loop_idx are the program ids for each grid axis.\n+        - grid_point is the grid point for the current loop iteration.\n+        - prev_local_core_id is the (device-local) core id from the previous\n+          loop iteration.\n+        - cur_local_core_id is the (device-local) core id for the current loop\n+          iteration.\n         - prev_start_indices is a rank-1 array that contains the start indices\n           for the slices of inputs and outputs processed in the previous loop\n           iteration.\n@@ -1729,9 +2143,16 @@ def body(\n     Returns:\n       The carry for the next iteration.\n     \"\"\"\n-    iteration_idx, loop_idx, prev_start_indices, cur_start_indices = carry\n+    (\n+        iteration_idx,\n+        loop_idx,\n+        grid_point,\n+        prev_local_core_id,\n+        cur_local_core_id,\n+        prev_start_indices,\n+        cur_start_indices,\n+    ) = carry\n     if interpret_params.grid_point_recorder is not None:\n-      grid_point = _get_grid_point(loop_idx, randomized_grid_coordinates)\n       callback.io_callback(interpret_params.grid_point_recorder, (), grid_point)\n \n     with pallas_core.grid_env(_get_local_grid_env(loop_idx)):\n@@ -1739,6 +2160,13 @@ def body(\n       next_grid_point = _get_grid_point(\n           next_loop_idx, randomized_grid_coordinates\n       )\n+      next_local_core_id = _get_next_local_core_id(\n+          cur_local_core_id,\n+          parallel_semantics_per_dim,\n+          grid_point,\n+          next_grid_point,\n+          interpret_params,\n+      )\n       next_start_indices = [\n           _compute_start_indices(\n               bm,\n@@ -1774,6 +2202,7 @@ def _store_slice_to_kernel_input(index, input_var):\n             get,\n             jax.ShapeDtypeStruct(input_var.aval.shape, input_var.aval.dtype),\n             device_id,\n+            cur_local_core_id,\n             TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n             input_buffer_ids[index],\n             (transform,),\n@@ -1785,6 +2214,7 @@ def _store_slice_to_kernel_input(index, input_var):\n             store,\n             (),\n             device_id,\n+            cur_local_core_id,\n             TPU_MEMORY_SPACE_IDXS[input_var.aval.memory_space],\n             input_ids[index],\n             (),\n@@ -1799,6 +2229,7 @@ def _store_slice_to_kernel_input(index, input_var):\n         assert len(prev_start_indices[j].shape) == 1\n         jax.lax.cond(\n             (iteration_idx == 0)\n+            | (cur_local_core_id != prev_local_core_id)\n             | jax.lax.reduce_or(\n                 cur_start_indices[j] != prev_start_indices[j], axes=(0,)\n             ),\n@@ -1807,9 +2238,14 @@ def _store_slice_to_kernel_input(index, input_var):\n         )\n \n       # Invoke the kernel.\n-      _interpret_jaxpr(jaxpr, *kernel_buffer_ids, mesh=mesh,\n-                       compiler_params=compiler_params,\n-                       interpret_params=interpret_params)\n+      _interpret_jaxpr(\n+          jaxpr,\n+          *kernel_buffer_ids,\n+          mesh=mesh,\n+          local_core_id=cur_local_core_id,\n+          compiler_params=compiler_params,\n+          interpret_params=interpret_params,\n+      )\n \n       # Copy from the kernel buffers to slices of the output in HBM.\n       def _store_to_output_buffer(index, output_var):\n@@ -1819,6 +2255,7 @@ def _store_to_output_buffer(index, output_var):\n             get,\n             output_var.aval,\n             device_id,\n+            cur_local_core_id,\n             TPU_MEMORY_SPACE_IDXS[output_var.aval.memory_space],\n             kernel_output_ids[j],\n             (),\n@@ -1842,6 +2279,7 @@ def _store_to_output_buffer(index, output_var):\n             store,\n             (),\n             device_id,\n+            cur_local_core_id,\n             TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n             output_buffer_ids[index],\n             (transform,),\n@@ -1856,6 +2294,7 @@ def _store_to_output_buffer(index, output_var):\n         assert len(next_start_indices[num_inputs + j].shape) == 1\n         jax.lax.cond(\n             (iteration_idx + 1 == num_iterations)\n+            | (cur_local_core_id != next_local_core_id)\n             | jax.lax.reduce_or(\n                 cur_start_indices[num_inputs + j]\n                 != next_start_indices[num_inputs + j],\n@@ -1865,7 +2304,15 @@ def _store_to_output_buffer(index, output_var):\n             lambda: None,\n         )\n \n-      return iteration_idx + 1, next_loop_idx, cur_start_indices, next_start_indices\n+      return (\n+          iteration_idx + 1,\n+          next_loop_idx,\n+          next_grid_point,\n+          cur_local_core_id,\n+          next_local_core_id,\n+          cur_start_indices,\n+          next_start_indices,\n+      )\n \n   initial_loop_idx = (jnp.int32(0),) * len(grid)\n   initial_grid_point = _get_grid_point(\n@@ -1884,16 +2331,25 @@ def _store_to_output_buffer(index, output_var):\n         for bm in grid_mapping.block_mappings\n     ]\n   # TODO(jburnim): Handle parallel grid dimensions + megacore.\n+  callback.io_callback(\n+      _update_clocks_for_device_barrier, (), device_id, ordered=True\n+  )\n   _ = lax.while_loop(\n       lambda carry: carry[0] < num_iterations,\n       body,\n       (\n           jnp.int32(0),\n           initial_loop_idx,\n+          initial_grid_point,\n+          jnp.int32(0),  # Previous core id is ignored on the first iteration.\n+          jnp.int32(0),  # Current core id is set to 0 for the first iteration.\n           initial_start_indices,  # Previous start indices are ignored on the first iteration.\n           initial_start_indices,\n       ),\n   )\n+  callback.io_callback(\n+      _update_clocks_for_device_barrier, (), device_id, ordered=True\n+  )\n \n   # Read the output from the allocated output buffers.\n   ret = [\n@@ -1903,6 +2359,7 @@ def _store_to_output_buffer(index, output_var):\n           get,\n           val,\n           device_id,\n+          0,  # local_core_id\n           TPU_MEMORY_SPACE_IDXS[mosaic_core.TPUMemorySpace.ANY],\n           output_buffer_id,\n           (indexing.NDIndexer.from_indices_shape(\ndiff --git a/tests/pallas/tpu_pallas_interpret_test.py b/tests/pallas/tpu_pallas_interpret_test.py\nindex 1af4b29d60ff..28c63dc3bd9b 100644\n--- a/tests/pallas/tpu_pallas_interpret_test.py\n+++ b/tests/pallas/tpu_pallas_interpret_test.py\n@@ -521,6 +521,65 @@ def alloc(x_vmem_ref, y_vmem_ref, sem):\n     y = f(x)\n     np.testing.assert_array_equal(y, x + 1)\n \n+  def test_two_cores_along_parallel_dimension_with_race(self):\n+    def kernel(x_ref, o_ref, vmem_ref):\n+      vmem_ref[...] = x_ref[...]\n+      o_ref[...] = x_ref[...] + vmem_ref[...]\n+\n+    x = jnp.ones((8, 128), jnp.float32)\n+    y = pl.pallas_call(\n+        kernel,\n+        grid=(2,),\n+        out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype),\n+        in_specs=[pl.BlockSpec(memory_space=pltpu.TPUMemorySpace.ANY)],\n+        scratch_shapes=[\n+            pltpu.VMEM(x.shape, x.dtype),\n+        ],\n+        interpret=mosaic_interpret.TPUInterpretParams(\n+            num_cores_per_device=2,\n+            detect_races=True,\n+        ),\n+        compiler_params=pltpu.TPUCompilerParams(\n+            dimension_semantics=('parallel',),\n+        ),\n+    )(x)\n+    self.assertTrue(mosaic_interpret.races.races_found)\n+    np.testing.assert_allclose(y, 2.0 * x)\n+\n+  def test_two_cores_along_parallel_dimension_no_race(self):\n+    def kernel(x_ref, o_ref, vmem_ref):\n+      vmem_ref[...] = x_ref[...]\n+      o_ref[...] = x_ref[...] + vmem_ref[...]\n+\n+    x = jnp.ones((16, 128), jnp.float32)\n+    y = pl.pallas_call(\n+        kernel,\n+        grid=(2,),\n+        out_shape=jax.ShapeDtypeStruct(x.shape, x.dtype),\n+        out_specs=pl.BlockSpec(\n+            (8, 128),\n+            lambda i: (i, 0),\n+        ),\n+        in_specs=[\n+            pl.BlockSpec(\n+                (8, 128),\n+                lambda i: (i, 0),\n+            ),\n+        ],\n+        scratch_shapes=[\n+            pltpu.VMEM((8, 128), x.dtype),\n+        ],\n+        interpret=mosaic_interpret.TPUInterpretParams(\n+            num_cores_per_device=2,\n+            detect_races=True,\n+        ),\n+        compiler_params=pltpu.TPUCompilerParams(\n+            dimension_semantics=('parallel',)\n+        ),\n+    )(x)\n+    self.assertFalse(mosaic_interpret.races.races_found)\n+    np.testing.assert_allclose(y, 2.0 * x)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix skip condition in Pallas Triton tests\n\nIt should not run in TPU configs at all.\n\nPiperOrigin-RevId: 761865943",
        "output": [
            "```diff\nCommit: 421aa54b269bb9ec0e003fd2736b0c0f2edb4759\nDate: 2025-05-22T09:05:19Z\nURL: https://github.com/jax-ml/jax/commit/421aa54b269bb9ec0e003fd2736b0c0f2edb4759\nFiles changed: 1\nAdditions: +8, Deletions: -5\ndiff --git a/tests/pallas/triton_pallas_test.py b/tests/pallas/triton_pallas_test.py\nindex 4e2b10e72eb1..fe13716705de 100644\n--- a/tests/pallas/triton_pallas_test.py\n+++ b/tests/pallas/triton_pallas_test.py\n@@ -33,11 +33,14 @@ class PallasBaseTest(jtu.JaxTestCase):\n   INTERPRET = False\n \n   def setUp(self):\n-    if jtu.test_device_matches([\"cpu\"]) and not self.INTERPRET:\n-      self.skipTest(\"On CPU the test works only in interpret mode\")\n-    if (jtu.test_device_matches([\"cuda\"]) and\n-        not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n-      self.skipTest(\"Only works on GPU with capability >= sm90\")\n+    if jtu.test_device_matches([\"cpu\"]):\n+      if not self.INTERPRET:\n+        self.skipTest(\"On CPU the test works only in interpret mode\")\n+    elif jtu.test_device_matches([\"gpu\"]):\n+      if not jtu.is_cuda_compute_capability_at_least(\"9.0\"):\n+        self.skipTest(\"Only works on GPU with capability >= sm90\")\n+    else:\n+      self.skipTest(\"Test only works on CPU and GPU\")\n \n     super().setUp()\n     _trace_kernel_to_jaxpr.cache_clear()\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28923 from mattjj:rahul-fix3\n\nPiperOrigin-RevId: 761806312",
        "output": [
            "```diff\nCommit: 2531d31d9645a12ecbc99f8b79d4251cc40a7d35\nDate: 2025-05-22T05:27:51Z\nURL: https://github.com/jax-ml/jax/commit/2531d31d9645a12ecbc99f8b79d4251cc40a7d35\nFiles changed: 3\nAdditions: +31, Deletions: -9\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex e49173c3df45..b20b85a43b6e 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -2745,10 +2745,8 @@ def __lt__(self, other):\n @dataclass(frozen=True)\n class NamedAxisEffect(effects.Effect):\n   \"\"\"A side-effect introducing a new named axis into the current scope.\"\"\"\n-\n   name: AxisName\n \n-\n effects.control_flow_allowed_effects.add_type(NamedAxisEffect)\n effects.custom_derivatives_allowed_effects.add_type(NamedAxisEffect)\n effects.lowerable_effects.add_type(NamedAxisEffect)\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex 9b28595e1835..d76d145fd0a6 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -1619,21 +1619,19 @@ def wrapped_fwd(*args, **kwargs) -> tuple[ReturnValue, Any]:\n     prim_tree, res_tree = out_trees()\n     num_res = res_tree.num_leaves\n \n-    if fwd_jaxpr.effects:\n+    disallowed_effects = effects.custom_derivatives_allowed_effects.filter_not_in(fwd_jaxpr.effects)\n+    if disallowed_effects:\n       raise NotImplementedError(\n           \"remat optimization for custom_vjp does not support forward \"\n-          f\"functions with side effects, but {fwd_name} has the following \"\n-          f\"effects: {fwd_jaxpr.effects}\")\n+          f\"functions with these side effects: {disallowed_effects}\")\n \n     @pe._memoize\n     def fun_jaxpr_thunk():\n       jaxpr, _, consts, () = pe.trace_to_jaxpr_dynamic(flat_fun, in_avals)\n       return jaxpr, consts\n \n-    out_flat = remat_opt_p.bind(*consts, *args_flat,\n-                                num_consts=len(consts),\n-                                num_res=num_res,\n-                                fwd_jaxpr=fwd_jaxpr,\n+    out_flat = remat_opt_p.bind(*consts, *args_flat, num_consts=len(consts),\n+                                num_res=num_res, fwd_jaxpr=fwd_jaxpr,\n                                 fun_jaxpr_thunk=fun_jaxpr_thunk)\n     res, out_flat = split_list(out_flat, [num_res])\n     out_tree = treedef_tuple((prim_tree, res_tree))\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 9b4ca76c3bc5..5fbace3c98e1 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -621,6 +621,32 @@ def f():\n     x = f()\n     self.assertAllClose(x, jnp.arange(4), check_dtypes=False)\n \n+  def test_optimize_remat(self):\n+    mesh = jtu.create_mesh((4,), 'x')\n+\n+    @jax.custom_vjp\n+    def f(x):\n+      return jnp.tan(x)\n+\n+    def f_fwd(x):\n+      return jax.lax.psum(x, 'x'), (x,)\n+\n+    def f_bwd(res, g):\n+      x, = res\n+      cos_x = jnp.cos(x)\n+      return (cos_x * g,)\n+\n+    f.defvjp(f_fwd, f_bwd, optimize_remat=True)\n+\n+    @jax.jit\n+    @jax.shard_map(mesh=mesh, in_specs=P(), out_specs=P())\n+    def temp(x):\n+      out = jax.remat(f)(x)\n+      out = out ** 2\n+      return out\n+\n+    jax.grad(lambda x: temp(x).sum())(jnp.arange(4.))\n+\n   def test_remat_basic(self):\n     # this tests remat-of-shmap\n     mesh = Mesh(np.array(jax.devices()[:4]), ('x',))\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "fix custom_vjp optimize_remat=True with collectives",
        "output": [
            "```diff\nCommit: bddb877c217e045f1210f0c831018ee0a54078a9\nDate: 2025-05-22T05:02:30Z\nURL: https://github.com/jax-ml/jax/commit/bddb877c217e045f1210f0c831018ee0a54078a9\nFiles changed: 3\nAdditions: +31, Deletions: -9\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex e49173c3df45..b20b85a43b6e 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -2745,10 +2745,8 @@ def __lt__(self, other):\n @dataclass(frozen=True)\n class NamedAxisEffect(effects.Effect):\n   \"\"\"A side-effect introducing a new named axis into the current scope.\"\"\"\n-\n   name: AxisName\n \n-\n effects.control_flow_allowed_effects.add_type(NamedAxisEffect)\n effects.custom_derivatives_allowed_effects.add_type(NamedAxisEffect)\n effects.lowerable_effects.add_type(NamedAxisEffect)\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex 9b28595e1835..d76d145fd0a6 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -1619,21 +1619,19 @@ def wrapped_fwd(*args, **kwargs) -> tuple[ReturnValue, Any]:\n     prim_tree, res_tree = out_trees()\n     num_res = res_tree.num_leaves\n \n-    if fwd_jaxpr.effects:\n+    disallowed_effects = effects.custom_derivatives_allowed_effects.filter_not_in(fwd_jaxpr.effects)\n+    if disallowed_effects:\n       raise NotImplementedError(\n           \"remat optimization for custom_vjp does not support forward \"\n-          f\"functions with side effects, but {fwd_name} has the following \"\n-          f\"effects: {fwd_jaxpr.effects}\")\n+          f\"functions with these side effects: {disallowed_effects}\")\n \n     @pe._memoize\n     def fun_jaxpr_thunk():\n       jaxpr, _, consts, () = pe.trace_to_jaxpr_dynamic(flat_fun, in_avals)\n       return jaxpr, consts\n \n-    out_flat = remat_opt_p.bind(*consts, *args_flat,\n-                                num_consts=len(consts),\n-                                num_res=num_res,\n-                                fwd_jaxpr=fwd_jaxpr,\n+    out_flat = remat_opt_p.bind(*consts, *args_flat, num_consts=len(consts),\n+                                num_res=num_res, fwd_jaxpr=fwd_jaxpr,\n                                 fun_jaxpr_thunk=fun_jaxpr_thunk)\n     res, out_flat = split_list(out_flat, [num_res])\n     out_tree = treedef_tuple((prim_tree, res_tree))\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 9b4ca76c3bc5..5fbace3c98e1 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -621,6 +621,32 @@ def f():\n     x = f()\n     self.assertAllClose(x, jnp.arange(4), check_dtypes=False)\n \n+  def test_optimize_remat(self):\n+    mesh = jtu.create_mesh((4,), 'x')\n+\n+    @jax.custom_vjp\n+    def f(x):\n+      return jnp.tan(x)\n+\n+    def f_fwd(x):\n+      return jax.lax.psum(x, 'x'), (x,)\n+\n+    def f_bwd(res, g):\n+      x, = res\n+      cos_x = jnp.cos(x)\n+      return (cos_x * g,)\n+\n+    f.defvjp(f_fwd, f_bwd, optimize_remat=True)\n+\n+    @jax.jit\n+    @jax.shard_map(mesh=mesh, in_specs=P(), out_specs=P())\n+    def temp(x):\n+      out = jax.remat(f)(x)\n+      out = out ** 2\n+      return out\n+\n+    jax.grad(lambda x: temp(x).sum())(jnp.arange(4.))\n+\n   def test_remat_basic(self):\n     # this tests remat-of-shmap\n     mesh = Mesh(np.array(jax.devices()[:4]), ('x',))\n\n```"
        ],
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "Merge pull request #28909 from levskaya:tree_broadcast\n\nPiperOrigin-RevId: 761758158",
        "output": [
            "```diff\nCommit: c9934912885bb7c4b72c5a9271598235a6789a81\nDate: 2025-05-22T02:09:32Z\nURL: https://github.com/jax-ml/jax/commit/c9934912885bb7c4b72c5a9271598235a6789a81\nFiles changed: 8\nAdditions: +108, Deletions: -5\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 1e866fae6af5..b34bf36997af 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -16,6 +16,9 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n \n ## Unreleased\n \n+* New features:\n+  * Added {func}`jax.tree.broadcast` which implements a pytree prefix broadcasting helper.\n+\n ## JAX 0.6.1 (May 21, 2025)\n \n * New features:\ndiff --git a/docs/jax.tree.rst b/docs/jax.tree.rst\nindex e65c77c757c1..1a0ddaec86d0 100644\n--- a/docs/jax.tree.rst\n+++ b/docs/jax.tree.rst\n@@ -12,6 +12,7 @@ List of Functions\n    :toctree: _autosummary\n \n    all\n+   broadcast\n    flatten\n    flatten_with_path\n    leaves\ndiff --git a/docs/jax.tree_util.rst b/docs/jax.tree_util.rst\nindex 73fd1f376e9f..c89b777ca548 100644\n--- a/docs/jax.tree_util.rst\n+++ b/docs/jax.tree_util.rst\n@@ -38,6 +38,7 @@ These APIs are now accessed via :mod:`jax.tree`.\n    :toctree: _autosummary\n \n    tree_all\n+   tree_broadcast\n    tree_flatten\n    tree_leaves\n    tree_map\ndiff --git a/jax/_src/tree.py b/jax/_src/tree.py\nindex 70d75a126804..9a3e001d902b 100644\n--- a/jax/_src/tree.py\n+++ b/jax/_src/tree.py\n@@ -378,3 +378,34 @@ def map_with_path(\n     - :func:`jax.tree_util.register_pytree_with_keys`\n   \"\"\"\n   return tree_util.tree_map_with_path(f, tree, *rest, is_leaf=is_leaf)\n+\n+\n+def broadcast(prefix_tree: Any, full_tree: Any,\n+              is_leaf: Callable[[Any], bool] | None = None\n+              ) -> list[Any]:\n+  \"\"\"Broadcasts a tree prefix into the full structure of a given tree.\n+\n+    Args:\n+      prefix_tree: a pytree that is a tree prefix of full_tree.\n+      full_tree: a pytree with the structure to broadcast the prefix leaves into.\n+      is_leaf: an optionally specified function that will be called at each\n+        flattening step. It should return a boolean, with true stopping the\n+        traversal and the whole subtree being treated as a leaf, and false\n+        indicating the flattening should traverse the current object.\n+\n+    Returns:\n+      A pytree matching the structure of full_tree where the leaves of prefix_tree have been\n+      broadcasted into the leaves of each corresponding subtree.\n+\n+    Examples:\n+      >>> import jax\n+      >>> prefix = (1, 2, 3)\n+      >>> full = (0, {'a': 0, 'b': 0}, (0, 0))\n+      >>> jax.tree.broadcast(prefix, full)\n+      (1, {'a': 2, 'b': 2}, (3, 3))\n+\n+    See Also:\n+      - :func:`jax.tree.leaves`\n+      - :func:`jax.tree.structure`\n+  \"\"\"\n+  return tree_util.tree_broadcast(prefix_tree, full_tree, is_leaf=is_leaf)\ndiff --git a/jax/_src/tree_util.py b/jax/_src/tree_util.py\nindex e2e97c90f120..6edbbfd62d12 100644\n--- a/jax/_src/tree_util.py\n+++ b/jax/_src/tree_util.py\n@@ -560,17 +560,42 @@ def __new__(klass, func, *args, **kw):\n )\n \n \n-# broadcast_prefix is not exported.\n+@export\n+def tree_broadcast(prefix_tree: Any, full_tree: Any,\n+                   is_leaf: Callable[[Any], bool] | None = None\n+                  ) -> list[Any]:\n+  \"\"\"Alias of :func:`jax.tree.broadcast`.\"\"\"\n+  broadcast_leaves = broadcast_prefix(prefix_tree, full_tree, is_leaf=is_leaf)\n+  return tree_structure(full_tree).unflatten(broadcast_leaves)\n+\n+\n+# broadcast_prefix is not exported\n def broadcast_prefix(prefix_tree: Any, full_tree: Any,\n                      is_leaf: Callable[[Any], bool] | None = None\n                      ) -> list[Any]:\n-  # If prefix_tree is not a tree prefix of full_tree, this code can raise a\n-  # ValueError; use prefix_errors to find disagreements and raise more precise\n-  # error messages.\n+  \"\"\"Broadcasts tree prefix leaves into the full set of leaves for a given full tree.\n+\n+    Args:\n+      prefix_tree: a pytree that is a tree prefix of full_tree.\n+      full_tree: a pytree with the structure to broadcast the prefix leaves into.\n+      is_leaf: an optionally specified function that will be called at each\n+        flattening step. It should return a boolean, with true stopping the\n+        traversal and the whole subtree being treated as a leaf, and false\n+        indicating the flattening should traverse the current object.\n+\n+    Returns:\n+      A list of leaves matching the expected count for the full tree,\n+      with the leaf of each prefix tree being duplicated to match the count of\n+      its corresponding subtree.\n+  \"\"\"\n   result = []\n   num_leaves = lambda t: tree_structure(t).num_leaves\n   add_leaves = lambda x, subtree: result.extend([x] * num_leaves(subtree))\n-  tree_map(add_leaves, prefix_tree, full_tree, is_leaf=is_leaf)\n+  try:\n+    tree_map(add_leaves, prefix_tree, full_tree, is_leaf=is_leaf)\n+  except ValueError:\n+      e, *_ = prefix_errors(prefix_tree, full_tree)\n+      raise e('broadcast_prefix prefix_tree') from None\n   return result\n \n \ndiff --git a/jax/tree.py b/jax/tree.py\nindex 270c34fe9647..03ca503f3a41 100644\n--- a/jax/tree.py\n+++ b/jax/tree.py\n@@ -19,6 +19,7 @@\n \n from jax._src.tree import (\n     all as all,\n+    broadcast as broadcast,\n     flatten_with_path as flatten_with_path,\n     flatten as flatten,\n     leaves_with_path as leaves_with_path,\ndiff --git a/jax/tree_util.py b/jax/tree_util.py\nindex 9f42284144ec..b35890dfc887 100644\n--- a/jax/tree_util.py\n+++ b/jax/tree_util.py\n@@ -58,6 +58,7 @@\n     register_pytree_with_keys as register_pytree_with_keys,\n     register_static as register_static,\n     tree_all as tree_all,\n+    tree_broadcast as tree_broadcast,\n     tree_flatten_with_path as tree_flatten_with_path,\n     tree_flatten as tree_flatten,\n     tree_leaves_with_path as tree_leaves_with_path,\ndiff --git a/tests/tree_util_test.py b/tests/tree_util_test.py\nindex 0df811d9da28..8d4cd5854e7d 100644\n--- a/tests/tree_util_test.py\n+++ b/tests/tree_util_test.py\n@@ -627,6 +627,39 @@ def testTransposeWithCustomObject(self):\n                                       FlatCache({\"a\": [3, 4], \"b\": [5, 6]}))\n     self.assertEqual(expected, actual)\n \n+  @parameterized.parameters(*TREES)\n+  def testBroadcast(self, tree):\n+    if isinstance(tree, FlatCache):\n+      # The tree_map construction below fails for FlatCache, because\n+      # the cached metadata becomes out of sync.\n+      self.skipTest(\"Test does not work properly for FlatCache.\")\n+    def make_inner(x):\n+      return [x, x, x]\n+    nested = tree_util.tree_map(make_inner, tree)\n+    actual = tree_util.tree_broadcast(tree, nested)\n+    self.assertEqual(actual, nested)\n+\n+  def testBroadcastSimple(self):\n+    prefix = (1, 2, 3)\n+    full = (0, {'a': 0, 'b': 0}, (0, 0))\n+    actual = tree_util.tree_broadcast(prefix, full)\n+    expected = (1, {'a': 2, 'b': 2}, (3, 3))\n+    self.assertEqual(actual, expected)\n+\n+  def testBroadcastError(self):\n+    prefix = (1, 2, 3)\n+    full = (0, {'a': 0, 'b': 0})\n+    with self.assertRaisesRegex(ValueError, \"pytree structure error\"):\n+      tree_util.tree_broadcast(prefix, full)\n+    prefix = (1, 2)\n+    full = (0, {'a': 0, 'b': 0}, (0, 0))\n+    with self.assertRaisesRegex(ValueError, \"pytree structure error\"):\n+      tree_util.tree_broadcast(prefix, full)\n+    prefix = (1, {'a': 0})\n+    full = (0, {'a': 0, 'b': 0})\n+    with self.assertRaisesRegex(ValueError, \"pytree structure error\"):\n+      tree_util.tree_broadcast(prefix, full)\n+\n   @parameterized.parameters([(*t, s) for t, s in zip(TREES, TREE_STRINGS)])\n   def testStringRepresentation(self, tree, correct_string):\n     \"\"\"Checks that the string representation of a tree works.\"\"\"\n@@ -1444,6 +1477,13 @@ def test_tree_transpose(self):\n       tree_util.tree_transpose(outer_treedef, inner_treedef, obj)\n     )\n \n+  def test_tree_broadcast(self):\n+    prefix = (1, 2, 3)\n+    full = (0, {'a': 0, 'b': 0}, (0, 0))\n+    actual = jax.tree.broadcast(prefix, full)\n+    expected = (1, {'a': 2, 'b': 2}, (3, 3))\n+    self.assertEqual(actual, expected)\n+\n   def test_tree_unflatten(self):\n     leaves, treedef = jax.tree.flatten([1, 2, (3, 4)])\n     self.assertEqual(\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add out_sharding to the function returned by `jax.nn.initializers.he_normal` and other APIs implementing the `Initializer` protocol. Currently it takes `key, shape, dtype` and now we added an optional out_sharding parameter to it.\n\nPiperOrigin-RevId: 761742909",
        "output": [
            "```diff\nCommit: 29e6647577ad98b2d1a35cccdfe9da8dfa0f1f24\nDate: 2025-05-22T01:06:08Z\nURL: https://github.com/jax-ml/jax/commit/29e6647577ad98b2d1a35cccdfe9da8dfa0f1f24\nFiles changed: 2\nAdditions: +58, Deletions: -15\ndiff --git a/jax/_src/nn/initializers.py b/jax/_src/nn/initializers.py\nindex 6f117eef749f..855729fa16ff 100644\n--- a/jax/_src/nn/initializers.py\n+++ b/jax/_src/nn/initializers.py\n@@ -30,6 +30,7 @@\n from jax import random\n from jax._src import core\n from jax._src import dtypes\n+from jax._src.sharding_impls import canonicalize_sharding\n from jax._src.typing import Array, ArrayLike\n from jax._src.util import set_module\n \n@@ -48,7 +49,8 @@ class Initializer(Protocol):\n   def __call__(self,\n                key: Array,\n                shape: core.Shape,\n-               dtype: DTypeLikeInexact = jnp.float_) -> Array:\n+               dtype: DTypeLikeInexact = jnp.float_,\n+               out_sharding=None) -> Array:\n     raise NotImplementedError\n \n @export\n@@ -100,9 +102,12 @@ def constant(value: ArrayLike,\n   \"\"\"\n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n     dtype = dtypes.canonicalize_dtype(dtype)\n-    return jnp.full(shape, value, dtype=dtype)\n+    out_sharding = canonicalize_sharding(\n+        out_sharding, 'nn.initializers.constant')\n+    return jnp.full(shape, value, dtype=dtype, device=out_sharding)\n   return init\n \n @export\n@@ -126,9 +131,11 @@ def uniform(scale: RealNumeric = 1e-2,\n   \"\"\"\n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n     dtype = dtypes.canonicalize_dtype(dtype)\n-    return random.uniform(key, shape, dtype) * jnp.array(scale, dtype)\n+    return random.uniform(key, shape, dtype,\n+                          out_sharding=out_sharding) * jnp.array(scale, dtype)\n   return init\n \n @export\n@@ -152,9 +159,11 @@ def normal(stddev: RealNumeric = 1e-2,\n   \"\"\"\n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n     dtype = dtypes.canonicalize_dtype(dtype)\n-    return random.normal(key, shape, dtype) * jnp.array(stddev, dtype)\n+    return random.normal(key, shape, dtype,\n+                         out_sharding=out_sharding) * jnp.array(stddev, dtype)\n   return init\n \n @export\n@@ -189,10 +198,12 @@ def truncated_normal(stddev: RealNumeric = 1e-2,\n \n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n     dtype = dtypes.canonicalize_dtype(dtype)\n     return random.truncated_normal(\n-        key, lower, upper, shape, dtype) * jnp.array(stddev, dtype)\n+        key, lower, upper, shape, dtype,\n+        out_sharding=out_sharding) * jnp.array(stddev, dtype)\n   return init\n \n @export\n@@ -315,7 +326,8 @@ def variance_scaling(\n \n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n     shape = core.canonicalize_shape(shape)\n     dtype = dtypes.canonicalize_dtype(dtype)\n     fan_in, fan_out = _compute_fans(shape, in_axis, out_axis, batch_axis)\n@@ -332,16 +344,19 @@ def init(key: Array,\n       if jnp.issubdtype(dtype, jnp.floating):\n         # constant is stddev of standard normal truncated to (-2, 2)\n         stddev = jnp.sqrt(variance) / jnp.array(.87962566103423978, dtype)\n-        return random.truncated_normal(key, -2, 2, shape, dtype) * stddev\n+        return random.truncated_normal(key, -2, 2, shape, dtype,\n+                                       out_sharding=out_sharding) * stddev\n       else:\n         # constant is stddev of complex standard normal truncated to 2\n         stddev = jnp.sqrt(variance) / jnp.array(.95311164380491208, dtype)\n         return _complex_truncated_normal(key, 2, shape, dtype) * stddev\n     elif distribution == \"normal\":\n-      return random.normal(key, shape, dtype) * jnp.sqrt(variance)\n+      return random.normal(key, shape, dtype,\n+                           out_sharding=out_sharding) * jnp.sqrt(variance)\n     elif distribution == \"uniform\":\n       if jnp.issubdtype(dtype, jnp.floating):\n-        return random.uniform(key, shape, dtype, -1) * jnp.sqrt(3 * variance)\n+        return random.uniform(key, shape, dtype, -1,\n+                              out_sharding=out_sharding) * jnp.sqrt(3 * variance)\n       else:\n         return _complex_uniform(key, shape, dtype) * jnp.sqrt(variance)\n     else:\n@@ -601,7 +616,10 @@ def orthogonal(scale: RealNumeric = 1.0,\n   \"\"\"\n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n+    if out_sharding is not None:\n+      raise NotImplementedError\n     dtype = dtypes.canonicalize_dtype(dtype)\n     if len(shape) < 2:\n       raise ValueError(\"orthogonal initializer requires at least a 2D shape\")\n@@ -651,7 +669,10 @@ def delta_orthogonal(\n   \"\"\"\n   def init(key: Array,\n            shape: core.Shape,\n-           dtype: DTypeLikeInexact = dtype) -> Array:\n+           dtype: DTypeLikeInexact = dtype,\n+           out_sharding=None) -> Array:\n+    if out_sharding is not None:\n+      raise NotImplementedError\n     dtype = dtypes.canonicalize_dtype(dtype)\n     if len(shape) not in [3, 4, 5]:\n       raise ValueError(\"Delta orthogonal initializer requires a 3D, 4D or 5D \"\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 5d616c43ce54..024901b746a8 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7868,6 +7868,28 @@ def f(x):\n     self.assertEqual(out.sharding,\n                      NamedSharding(mesh.abstract_mesh, P('x', 'y')))\n \n+  @jtu.with_explicit_mesh((2,), ('x',))\n+  def test_he_normal(self, mesh):\n+    init = jax.nn.initializers.he_normal(in_axis=0, out_axis=1)\n+    key = jax.random.key(0)\n+    out = init(key, (8, 2), jnp.float32, out_sharding=P('x'))\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P('x', None)))\n+\n+  @jtu.with_explicit_mesh((2,), ('x',))\n+  def test_nn_uniform(self, mesh):\n+    init = jax.nn.initializers.uniform()\n+    key = jax.random.key(0)\n+    out = init(key, (8, 2), jnp.float32, out_sharding=P('x'))\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P('x', None)))\n+\n+  @jtu.with_explicit_mesh((2,), ('x',))\n+  def test_nn_constant(self, mesh):\n+    init = jax.nn.initializers.constant(-7)\n+    key = jax.random.key(0)\n+    out = init(key, (8, 2), jnp.float32, out_sharding=P('x'))\n+    self.assertArraysEqual(out, jnp.full((8, 2), -7, dtype=jnp.float32))\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P('x', None)))\n+\n \n @jtu.pytest_mark_if_available('multiaccelerator')\n class PJitErrorTest(jtu.JaxTestCase):\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "remove jaxlib_extension_version, ifrt_version and jaxlib.__version_info__ guards after 0.6.1 release.\n\nPiperOrigin-RevId: 761737523",
        "output": [
            "```diff\nCommit: 0169f32fa2ea3166bcef7e113c9e3158195a9db3\nDate: 2025-05-22T00:45:02Z\nURL: https://github.com/jax-ml/jax/commit/0169f32fa2ea3166bcef7e113c9e3158195a9db3\nFiles changed: 34\nAdditions: +59, Deletions: -346\ndiff --git a/docs/autodidax.ipynb b/docs/autodidax.ipynb\nindex 07c7d7e84ff0..16d4da37b3f2 100644\n--- a/docs/autodidax.ipynb\n+++ b/docs/autodidax.ipynb\n@@ -1986,7 +1986,6 @@\n     \"from jax.extend.mlir import ir\\n\",\n     \"from jax.extend.mlir.dialects import func\\n\",\n     \"from jax.extend.mlir.dialects import stablehlo as hlo\\n\",\n-    \"import jax._src.lib\\n\",\n     \"from jax._src import xla_bridge as xb\\n\",\n     \"\\n\",\n     \"class MlirContext(NamedTuple):\\n\",\n@@ -2021,11 +2020,7 @@\n     \"  output = io.StringIO()\\n\",\n     \"  c.module.operation.print(file=output)\\n\",\n     \"  backend = xb.get_backend(None)\\n\",\n-    \"  if jax._src.lib.version >= (0, 6, 1):\\n\",\n-    \"    compiled = backend.compile(\\n\",\n-    \"      output.getvalue(), backend.devices()[:1])\\n\",\n-    \"  else:\\n\",\n-    \"    compiled = backend.compile(output.getvalue())\\n\",\n+    \"  compiled = backend.compile(output.getvalue(), backend.devices()[:1])\\n\",\n     \"  return partial(execute_compiled, compiled, [v.aval for v in jaxpr.outs])\\n\",\n     \"\\n\",\n     \"def _mlir_dtype(dtype: np.dtype) -> ir.Type:\\n\",\ndiff --git a/docs/autodidax.md b/docs/autodidax.md\nindex e78aeded41c0..870ee20f0f9a 100644\n--- a/docs/autodidax.md\n+++ b/docs/autodidax.md\n@@ -1556,7 +1556,6 @@ import io\n from jax.extend.mlir import ir\n from jax.extend.mlir.dialects import func\n from jax.extend.mlir.dialects import stablehlo as hlo\n-import jax._src.lib\n from jax._src import xla_bridge as xb\n \n class MlirContext(NamedTuple):\n@@ -1591,11 +1590,7 @@ def xla_callable(hashable_jaxpr: IDHashable,\n   output = io.StringIO()\n   c.module.operation.print(file=output)\n   backend = xb.get_backend(None)\n-  if jax._src.lib.version >= (0, 6, 1):\n-    compiled = backend.compile(\n-      output.getvalue(), backend.devices()[:1])\n-  else:\n-    compiled = backend.compile(output.getvalue())\n+  compiled = backend.compile(output.getvalue(), backend.devices()[:1])\n   return partial(execute_compiled, compiled, [v.aval for v in jaxpr.outs])\n \n def _mlir_dtype(dtype: np.dtype) -> ir.Type:\ndiff --git a/docs/autodidax.py b/docs/autodidax.py\nindex 9531ef7694c5..b0dbf9f73d9f 100644\n--- a/docs/autodidax.py\n+++ b/docs/autodidax.py\n@@ -1548,7 +1548,6 @@ def __eq__(self, other):\n from jax.extend.mlir import ir\n from jax.extend.mlir.dialects import func\n from jax.extend.mlir.dialects import stablehlo as hlo\n-import jax._src.lib\n from jax._src import xla_bridge as xb\n \n class MlirContext(NamedTuple):\n@@ -1583,11 +1582,7 @@ def main(*params):\n   output = io.StringIO()\n   c.module.operation.print(file=output)\n   backend = xb.get_backend(None)\n-  if jax._src.lib.version >= (0, 6, 1):\n-    compiled = backend.compile(\n-      output.getvalue(), backend.devices()[:1])\n-  else:\n-    compiled = backend.compile(output.getvalue())\n+  compiled = backend.compile(output.getvalue(), backend.devices()[:1])\n   return partial(execute_compiled, compiled, [v.aval for v in jaxpr.outs])\n \n def _mlir_dtype(dtype: np.dtype) -> ir.Type:\ndiff --git a/jax/_src/buffer_callback.py b/jax/_src/buffer_callback.py\nindex 739fdb4c408d..a1dfb5c2ff18 100644\n--- a/jax/_src/buffer_callback.py\n+++ b/jax/_src/buffer_callback.py\n@@ -27,16 +27,12 @@\n from jax._src.interpreters import ad\n from jax._src.interpreters import batching\n from jax._src.interpreters import mlir\n-from jax._src.lib import jaxlib_extension_version\n+from jax._src.lib import ffi as ffi_lib\n \n export = util.set_module(\"jax.experimental.buffer_callback\")\n-\n-if jaxlib_extension_version >= 334:\n-  from jax._src.lib import ffi as ffi_lib\n-\n-  Buffer = export(ffi_lib.Buffer)\n-  ExecutionStage = export(ffi_lib.ExecutionStage)\n-  ExecutionContext = export(ffi_lib.ExecutionContext)\n+Buffer = export(ffi_lib.Buffer)\n+ExecutionStage = export(ffi_lib.ExecutionStage)\n+ExecutionContext = export(ffi_lib.ExecutionContext)\n \n \n def buffer_callback(\ndiff --git a/jax/_src/compilation_cache.py b/jax/_src/compilation_cache.py\nindex aa1bd6ab65ba..058670642f41 100644\n--- a/jax/_src/compilation_cache.py\n+++ b/jax/_src/compilation_cache.py\n@@ -31,7 +31,6 @@\n from jax._src import config\n from jax._src import monitoring\n from jax._src.compilation_cache_interface import CacheInterface\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.lib import xla_client\n from jax._src.lib.mlir import ir\n from jax._src.lru_cache import LRUCache\n@@ -224,12 +223,8 @@ def get_executable_and_time(\n   executable_and_time = decompress_executable(executable_and_time)\n   serialized_executable, compile_time = extract_executable_and_time(\n       executable_and_time)\n-  if jaxlib_extension_version < 332:\n-    xla_executable_deserialized = backend.deserialize_executable(\n-        serialized_executable, compile_options)\n-  else:\n-    xla_executable_deserialized = backend.deserialize_executable(\n-        serialized_executable, executable_devices, compile_options)\n+  xla_executable_deserialized = backend.deserialize_executable(\n+      serialized_executable, executable_devices, compile_options)\n   return xla_executable_deserialized, compile_time\n \n \ndiff --git a/jax/_src/compiler.py b/jax/_src/compiler.py\nindex 04f993fed799..e1b8e7c35697 100644\n--- a/jax/_src/compiler.py\n+++ b/jax/_src/compiler.py\n@@ -35,7 +35,6 @@\n from jax._src import traceback_util\n from jax._src.interpreters import mlir\n from jax._src.lib import xla_client as xc\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.lib.mlir import ir\n import numpy as np\n \n@@ -314,12 +313,6 @@ def backend_compile(\n     )\n \n   try:\n-    if jaxlib_extension_version < 332:\n-      if host_callbacks:\n-        return backend.compile(\n-            built_c, compile_options=options, host_callbacks=host_callbacks)  # type: ignore\n-      return backend.compile(built_c, compile_options=options)  # type: ignore\n-\n     # we use a separate function call to ensure that XLA compilation appears\n     # separately in Python profiling results\n     if host_callbacks:\n@@ -692,12 +685,8 @@ def _compile_and_share_module(\n     serialized_executable = compilation_cache.decompress_executable(\n         serialized_executable\n     )\n-    if jaxlib_extension_version < 332:\n-      executable = backend.deserialize_executable(\n-          serialized_executable, compile_options)  # type: ignore\n-    else:\n-      executable = backend.deserialize_executable(\n-          serialized_executable, executable_devices, compile_options)  # type: ignore\n+    executable = backend.deserialize_executable(\n+        serialized_executable, executable_devices, compile_options)  # type: ignore\n \n   _compile_and_share_module.modules_cache[cache_key] = executable\n   return executable\ndiff --git a/jax/_src/lax/ann.py b/jax/_src/lax/ann.py\nindex bfcd45fba574..61d383ee29c2 100644\n--- a/jax/_src/lax/ann.py\n+++ b/jax/_src/lax/ann.py\n@@ -83,8 +83,6 @@ def pmap_mips(qy, db, db_offset, db_size, k, recall_target):\n from jax._src.interpreters import mlir\n from jax._src.lax import lax\n from jax._src.lib import _jax\n-from jax._src.lib import jaxlib_extension_version\n-from jax._src.lib import xla_client as xc\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import func\n from jax._src.lib.mlir.dialects import hlo\n@@ -233,14 +231,9 @@ def _approx_top_k_abstract_eval(operand, *, k, reduction_dimension,\n   if aggregate_to_topk:\n     dims[reduction_dimension] = k\n   elif core.is_constant_shape((reduction_input_size, k)):\n-    if jaxlib_extension_version >= 331:\n-      dims[reduction_dimension] = _jax.approx_top_k_reduction_output_size(\n-          reduction_input_size, len(dims), k, recall_target, aggregate_to_topk,\n-          reduction_input_size_override)[0]\n-    else:\n-      dims[reduction_dimension] = xc.ops.ApproxTopKReductionOutputSize(  # type: ignore  # pytype: disable=module-attr\n-          reduction_input_size, len(dims), k, recall_target, aggregate_to_topk,\n-          reduction_input_size_override)[0]\n+    dims[reduction_dimension] = _jax.approx_top_k_reduction_output_size(\n+        reduction_input_size, len(dims), k, recall_target, aggregate_to_topk,\n+        reduction_input_size_override)[0]\n   else:\n     raise NotImplementedError(\n          \"approx_top_k with aggregate_to_topk=False not yet implemented when \"\ndiff --git a/jax/_src/lax/linalg.py b/jax/_src/lax/linalg.py\nindex 857b115b06d8..2fda4a90369d 100644\n--- a/jax/_src/lax/linalg.py\n+++ b/jax/_src/lax/linalg.py\n@@ -48,7 +48,7 @@\n from jax._src.lib import gpu_solver\n from jax._src.lib import gpu_sparse\n from jax._src.lib import lapack\n-from jax._src.lib import version as jaxlib_version, jaxlib_extension_version\n+from jax._src.lib import version as jaxlib_version\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import chlo\n from jax._src.lib.mlir.dialects import hlo\n@@ -2530,30 +2530,6 @@ def _tridiagonal_solve_shape_rule(dl_shape, d_shape, du_shape, b_shape, **_):\n   return b_shape\n \n def _tridiagonal_solve_gpu_lowering(ctx, dl, d, du, b, *, target_name_prefix):\n-  if jaxlib_extension_version < 340:\n-    _, _, _, b_aval = ctx.avals_in\n-    *batch_dims, m, n = b_aval.shape\n-    batch_size = math.prod(batch_dims)\n-    mod = gpu_sparse._cusparse if target_name_prefix == \"cu\" else gpu_sparse._hipsparse\n-    assert mod is not None\n-    opaque = mod.build_gtsv2_descriptor(batch_size, m, n, m)\n-    if b_aval.dtype == np.float32:\n-      buffer_size = mod.gtsv2_f32_buffer_size(m, n, m)\n-      target_name = \"sparse_gtsv2_f32_ffi\"\n-    elif b_aval.dtype == np.float64:\n-      buffer_size = mod.gtsv2_f64_buffer_size(m, n, m)\n-      target_name = \"sparse_gtsv2_f64_ffi\"\n-    else:\n-      raise NotImplementedError(\n-          \"tridiagonal_solve is only implemented for float32 and float64 on GPU.\")\n-\n-    buffer_aval = core.ShapedArray(shape=(buffer_size,), dtype=np.int8)\n-    sub_ctx = ctx.replace(avals_out=[*ctx.avals_out, buffer_aval])\n-    rule = _linalg_ffi_lowering(\n-        f\"{target_name_prefix}{target_name}\", operand_output_aliases={3: 0},\n-        batch_partitionable=False)\n-    return rule(sub_ctx, dl, d, du, b, opaque=opaque)[:1]\n-\n   target_name = f\"{target_name_prefix}sparse_gtsv2_ffi\"\n   rule = _linalg_ffi_lowering(target_name, operand_output_aliases={3: 0})\n   return rule(ctx, dl, d, du, b)\ndiff --git a/jax/_src/lib/__init__.py b/jax/_src/lib/__init__.py\nindex 5cdcaf400c8a..8de05061ec99 100644\n--- a/jax/_src/lib/__init__.py\n+++ b/jax/_src/lib/__init__.py\n@@ -85,23 +85,13 @@ def _parse_version(v: str) -> tuple[int, ...]:\n \n import jaxlib.lapack as lapack  # noqa: F401\n import jaxlib.utils as utils  # noqa: F401\n-\n-if version >= (0, 6, 1):\n-  import jaxlib._jax as _jax  # noqa: F401\n-  from jaxlib._jax import guard_lib as guard_lib  # noqa: F401\n-  from jaxlib._jax import jax_jit as jax_jit  # noqa: F401\n-  from jaxlib._jax import pmap_lib as pmap_lib  # noqa: F401\n-  from jaxlib._jax import pytree as pytree  # noqa: F401\n-  from jaxlib._jax import Device as Device  # noqa: F401\n-  from jaxlib import _profiler as _profiler  # noqa: F401\n-else:\n-  import jaxlib.xla_extension as _jax  # type: ignore  # pytype: disable=import-error  # noqa: F401\n-  from jaxlib.xla_extension import guard_lib as guard_lib  # type: ignore  # pytype: disable=import-error  # noqa: F401\n-  from jaxlib.xla_extension import jax_jit as jax_jit  # type: ignore  # pytype: disable=import-error  # noqa: F401\n-  from jaxlib.xla_extension import pmap_lib as pmap_lib  # type: ignore  # pytype: disable=import-error  # noqa: F401\n-  from jaxlib.xla_extension import pytree as pytree  # type: ignore  # pytype: disable=import-error  # noqa: F401\n-  from jaxlib.xla_extension import Device as Device  # type: ignore  # pytype: disable=import-error  # noqa: F401\n-  from jaxlib.xla_extension import profiler as _profiler  # type: ignore  # pytype: disable=import-error  # noqa: F401\n+import jaxlib._jax as _jax  # noqa: F401\n+from jaxlib._jax import guard_lib as guard_lib  # noqa: F401\n+from jaxlib._jax import jax_jit as jax_jit  # noqa: F401\n+from jaxlib._jax import pmap_lib as pmap_lib  # noqa: F401\n+from jaxlib._jax import pytree as pytree  # noqa: F401\n+from jaxlib._jax import Device as Device  # noqa: F401\n+from jaxlib import _profiler as _profiler  # noqa: F401\n \n import jaxlib.xla_client as xla_client  # noqa: F401\n \n@@ -112,15 +102,9 @@ def _parse_version(v: str) -> tuple[int, ...]:\n jaxlib_extension_version: int = getattr(xla_client, '_version', 0)\n ifrt_version: int = getattr(xla_client, '_ifrt_version', 0)\n \n-if jaxlib_extension_version >= 334:\n-  from jaxlib._jax import ffi as ffi  # noqa: F401\n-\n-if jaxlib_extension_version >= 335:\n-  import jaxlib.cpu_sparse as cpu_sparse  # noqa: F401\n-\n-  has_cpu_sparse = True\n-else:\n-  has_cpu_sparse = False\n+from jaxlib._jax import ffi as ffi  # noqa: F401\n+import jaxlib.cpu_sparse as cpu_sparse  # noqa: F401\n+has_cpu_sparse = True\n \n import jaxlib.weakref_lru_cache as weakref_lru_cache  # noqa: F401\n \ndiff --git a/jax/_src/lib/mlir/dialects/__init__.py b/jax/_src/lib/mlir/dialects/__init__.py\nindex 5584afee2116..b49154e7936a 100644\n--- a/jax/_src/lib/mlir/dialects/__init__.py\n+++ b/jax/_src/lib/mlir/dialects/__init__.py\n@@ -57,7 +57,4 @@\n from jaxlib.mlir.dialects import stablehlo as hlo\n \n from jax._src import lib\n-if lib.version >= (0, 6, 1):\n-  from jaxlib.mlir.dialects import cf\n-else:\n-  cf = None  # type: ignore[no-redef]\n+from jaxlib.mlir.dialects import cf\ndiff --git a/jax/_src/named_sharding.py b/jax/_src/named_sharding.py\nindex ae99236a6cdc..faf0b2a9f2b2 100644\n--- a/jax/_src/named_sharding.py\n+++ b/jax/_src/named_sharding.py\n@@ -23,7 +23,6 @@\n from jax._src import config\n from jax._src.util import use_cpp_class, cache, use_cpp_method\n from jax._src.lib import xla_client as xc\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.lib.mlir.dialects import sdy\n from jax._src import mesh as mesh_lib\n from jax._src.mesh import AxisType\n@@ -317,17 +316,11 @@ def build(self) -> sdy.TensorShardingAttr:\n \n     replicated_axes = _get_axes(self.replicated_axes, self.mesh_shape)\n     unreduced_axes = _get_axes(self.unreduced_axes, self.mesh_shape)\n-    if jaxlib_extension_version >= 342:\n-      return sdy.TensorShardingAttr.get(\n-          mesh_attr,\n-          [dim_sharding.build() for dim_sharding in self.dim_shardings],\n-          replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes],\n-          unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in unreduced_axes])\n-    else:\n-      return sdy.TensorShardingAttr.get(\n-          mesh_attr,\n-          [dim_sharding.build() for dim_sharding in self.dim_shardings],\n-          replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes])\n+    return sdy.TensorShardingAttr.get(\n+        mesh_attr,\n+        [dim_sharding.build() for dim_sharding in self.dim_shardings],\n+        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes],\n+        unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in unreduced_axes])\n \n   def __repr__(self):\n     dim_sharding_repr = ', '.join(\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex b6960c479558..e212f9770a94 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -31,7 +31,6 @@\n from jax import lax\n from jax._src import checkify\n from jax._src import core as jax_core\n-from jax._src import lib as jaxlib\n from jax._src import linear_util as lu\n from jax._src import mesh as mesh_lib\n from jax._src import pjit\n@@ -1569,11 +1568,6 @@ def _broadcast_in_dim_lowering_rule_wg(\n         ir.VectorType.get(shape, mgpu_utils.dtype_to_ir_type(x_aval.dtype)),\n         x,\n     )\n-\n-  # TODO(dasenov): Remove this after the minimal jaxlib version is 0.6.1.\n-  if jaxlib.version < (0, 6, 1):\n-    raise NotImplementedError()\n-\n   mlir_type = mgpu_utils.dtype_to_ir_type(x_aval.dtype)\n   result_ty = ir.VectorType.get(shape, mlir_type)\n   return mgpu.dialect.broadcast_in_dim(result_ty, x, broadcast_dimensions)\ndiff --git a/jax/_src/profiler.py b/jax/_src/profiler.py\nindex 6b58b2ba6326..424e2b81035f 100644\n--- a/jax/_src/profiler.py\n+++ b/jax/_src/profiler.py\n@@ -215,7 +215,7 @@ def stop_trace():\n     if _profile_state.profile_session is None:\n       raise RuntimeError(\"No profile started\")\n     sess = _profile_state.profile_session\n-    sess.stop_and_export(str(_profile_state.log_dir))\n+    sess.stop_and_export(str(_profile_state.log_dir))  # type: ignore\n     if _profile_state.create_perfetto_trace:\n       abs_filename = _write_perfetto_trace_file(_profile_state.log_dir)\n       if _profile_state.create_perfetto_link:\ndiff --git a/jax/_src/test_util.py b/jax/_src/test_util.py\nindex bb1ef6595ec3..f6810b533b31 100644\n--- a/jax/_src/test_util.py\n+++ b/jax/_src/test_util.py\n@@ -54,7 +54,6 @@\n from jax._src import mesh as mesh_lib\n from jax._src.cloud_tpu_init import running_in_cloud_tpu_vm\n from jax._src.interpreters import mlir\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.lib.mlir.dialects import hlo\n from jax._src.numpy.util import promote_dtypes, promote_dtypes_inexact\n from jax._src.public_test_util import (  # noqa: F401\n@@ -357,18 +356,16 @@ def assert_num_jit_and_pmap_compilations(times):\n \n @contextmanager\n def count_internal_device_puts():\n-  if jaxlib_extension_version >= 341:\n-    before = jax._src.lib._jax.get_internal_device_put_info()\n+  before = jax._src.lib._jax.get_internal_device_put_info()\n   counts = {}\n   try:\n     yield lambda: counts\n   finally:\n-    if jaxlib_extension_version >= 341:\n-      after = jax._src.lib._jax.get_internal_device_put_info()\n-      for k, v in after.items():\n-        diff = v - before.get(k, 0)\n-        if diff != 0:\n-          counts[k] = diff\n+    after = jax._src.lib._jax.get_internal_device_put_info()\n+    for k, v in after.items():\n+      diff = v - before.get(k, 0)\n+      if diff != 0:\n+        counts[k] = diff\n \n def jaxlib_version() -> tuple[int, ...]:\n   return _jaxlib.version\ndiff --git a/jax/experimental/buffer_callback.py b/jax/experimental/buffer_callback.py\nindex 6c8514340af0..f919cfa10208 100644\n--- a/jax/experimental/buffer_callback.py\n+++ b/jax/experimental/buffer_callback.py\n@@ -12,16 +12,9 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from jax._src.lib import jaxlib_extension_version as _jaxlib_extension_version\n-\n-if _jaxlib_extension_version >= 334:\n-  from jax._src.buffer_callback import (\n-      Buffer as Buffer,\n-      ExecutionContext as ExecutionContext,\n-      ExecutionStage as ExecutionStage,\n-      buffer_callback as buffer_callback,\n-  )\n-\n-from jax._src.buffer_callback import buffer_callback as buffer_callback\n-\n-del _jaxlib_extension_version\n+from jax._src.buffer_callback import (\n+    Buffer as Buffer,\n+    ExecutionContext as ExecutionContext,\n+    ExecutionStage as ExecutionStage,\n+    buffer_callback as buffer_callback,\n+)\ndiff --git a/jax/experimental/jax2tf/tests/sharding_test.py b/jax/experimental/jax2tf/tests/sharding_test.py\nindex 5fc45df218cd..fa15522cbe90 100644\n--- a/jax/experimental/jax2tf/tests/sharding_test.py\n+++ b/jax/experimental/jax2tf/tests/sharding_test.py\n@@ -33,7 +33,6 @@\n from jax._src import config\n from jax._src import test_util as jtu\n from jax._src import xla_bridge\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.lib import xla_client as xc\n from jax import lax\n from jax.experimental import jax2tf\n@@ -111,12 +110,8 @@ def log_jax_hlo(self, f_jax, args: Sequence[Any], *,\n           device_assignment=device_assignment,\n           use_spmd_partitioning=use_spmd_partitioning,\n       )\n-      if jaxlib_extension_version < 332:\n-        executable = backend.compile(\n-            jax_hlo, compile_options=compile_options)  # type: ignore\n-      else:\n-        executable = backend.compile_and_load(\n-            jax_hlo, xc.DeviceList(tuple(self.devices.flat)), compile_options)  # type: ignore\n+      executable = backend.compile_and_load(\n+          jax_hlo, xc.DeviceList(tuple(self.devices.flat)), compile_options)  # type: ignore\n       jax_optimized_hlo = executable.hlo_modules()[0].to_string()\n       logging.info(\"[%s] got JAX optimized HLO for platform %s %s\",\n                    self._testMethodName, backend.platform, jax_optimized_hlo)\ndiff --git a/jax/experimental/serialize_executable.py b/jax/experimental/serialize_executable.py\nindex 6f5062d4ce99..7c112f56ef42 100644\n--- a/jax/experimental/serialize_executable.py\n+++ b/jax/experimental/serialize_executable.py\n@@ -19,7 +19,6 @@\n import io\n \n import jax\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.lib import xla_client as xc\n from typing import Sequence\n \n@@ -110,8 +109,6 @@ def __init__(self, file, backend, execution_devices=None):\n \n   def persistent_load(self, pid):\n     if pid[0] == 'exec':\n-      if jaxlib_extension_version < 332:\n-        return self.backend.deserialize_executable(pid[1])\n       return self.backend.deserialize_executable(\n           pid[1], executable_devices=self.execution_devices)\n     if pid[0] == 'device':\ndiff --git a/jaxlib/py_client.cc b/jaxlib/py_client.cc\nindex 0a99d94f81cc..842bdfecad3d 100644\n--- a/jaxlib/py_client.cc\n+++ b/jaxlib/py_client.cc\n@@ -373,14 +373,9 @@ std::unique_ptr<ifrt::CompileOptions> MakeIfrtCompileOptions(\n     ifrt_loaded_host_callbacks.push_back(tsl::FormRef(\n         static_cast<ifrt::LoadedHostCallback*>(host_callback.data())));\n   }\n-#if JAX_IFRT_VERSION_NUMBER >= 6\n   return std::make_unique<ifrt::XlaCompileOptions>(\n       std::move(options), std::move(executable_devices),\n       std::move(ifrt_loaded_host_callbacks));\n-#else\n-  return std::make_unique<ifrt::XlaCompileOptions>(\n-      std::move(options), std::move(ifrt_loaded_host_callbacks));\n-#endif\n }\n \n // Makes IFRT `DeserializeExecutableOptions` from XLA `CompileOptions` and\n@@ -398,14 +393,9 @@ MakeIfrtDeserializeExecutableOptions(std::optional<CompileOptions> options,\n     ifrt_loaded_host_callbacks.push_back(tsl::FormRef(\n         static_cast<ifrt::LoadedHostCallback*>(host_callback.data())));\n   }\n-#if JAX_IFRT_VERSION_NUMBER >= 6\n   return std::make_unique<ifrt::XlaDeserializeExecutableOptions>(\n       std::move(options), std::move(executable_devices),\n       std::move(ifrt_loaded_host_callbacks));\n-#else\n-  return std::make_unique<ifrt::XlaDeserializeExecutableOptions>(\n-      std::move(options), std::move(ifrt_loaded_host_callbacks));\n-#endif\n }\n \n }  // namespace\n@@ -504,14 +494,9 @@ PyClient::CompileAndLoad(nb_class_ptr<PyClient> client, std::string mlir_module,\n         client->ifrt_client(), std::move(host_callback));\n     ifrt_loaded_host_callbacks.push_back(callback);\n   }\n-#if JAX_IFRT_VERSION_NUMBER >= 6\n   auto compile_options = std::make_unique<ifrt::XlaCompileOptions>(\n       std::move(options), std::move(executable_devices),\n       std::move(ifrt_loaded_host_callbacks));\n-#else\n-  auto compile_options = std::make_unique<ifrt::XlaCompileOptions>(\n-      std::move(options), std::move(ifrt_loaded_host_callbacks));\n-#endif\n   return CompileAndLoadIfrtProgram(\n       client, std::make_unique<xla::ifrt::HloProgram>(module.get()),\n       std::move(compile_options));\ndiff --git a/jaxlib/py_compile_only_client.cc b/jaxlib/py_compile_only_client.cc\nindex 0fa2f4b48fd7..2de896d80bef 100644\n--- a/jaxlib/py_compile_only_client.cc\n+++ b/jaxlib/py_compile_only_client.cc\n@@ -91,12 +91,8 @@ class CompileOnlyPyClient : public PyClient {\n         llvm::dyn_cast_or_null<CompileOnlyIfRtClient>(this->ifrt_client());\n     CHECK(ifrt_client) << \"CompileOnlyPyClient requires ifrt_client be a \"\n                           \"CompileOnlyIfRtClient\";\n-#if JAX_IFRT_VERSION_NUMBER >= 6\n     auto xla_options = std::make_unique<ifrt::XlaCompileOptions>(\n         options, std::move(executable_devices));\n-#else\n-    auto xla_options = std::make_unique<ifrt::XlaCompileOptions>(options);\n-#endif\n     TF_ASSIGN_OR_RETURN(auto executable,\n                         PjRtCompile(std::move(options), module.get(),\n                                     *ifrt_client->topology().description()));\ndiff --git a/jaxlib/py_program.cc b/jaxlib/py_program.cc\nindex 40bfd3497ebd..ee2d3eef9973 100644\n--- a/jaxlib/py_program.cc\n+++ b/jaxlib/py_program.cc\n@@ -236,16 +236,11 @@ absl::StatusOr<std::unique_ptr<ifrt::CompileOptions>> MakeXlaCompileOptions(\n     ifrt_loaded_host_callbacks.push_back(tsl::FormRef(\n         static_cast<ifrt::LoadedHostCallback*>(host_callback.data())));\n   }\n-#if JAX_IFRT_VERSION_NUMBER >= 6\n   TF_ASSIGN_OR_RETURN(ifrt::DeviceListRef executable_devices,\n                       py_executable_devices.ifrt_device_list());\n   return std::make_unique<ifrt::XlaCompileOptions>(\n       std::move(options), std::move(executable_devices),\n       std::move(ifrt_loaded_host_callbacks));\n-#else\n-  return std::make_unique<ifrt::XlaCompileOptions>(\n-      std::move(options), std::move(ifrt_loaded_host_callbacks));\n-#endif\n }\n \n constexpr absl::string_view kColocatedPythonProgramType =\ndiff --git a/jaxlib/py_socket_transfer.cc b/jaxlib/py_socket_transfer.cc\nindex fde63df8da47..8086196b9df8 100644\n--- a/jaxlib/py_socket_transfer.cc\n+++ b/jaxlib/py_socket_transfer.cc\n@@ -110,58 +110,10 @@ absl::StatusOr<xla::PjRtMemorySpace*> MemorySpaceFromSharding(\n   }\n }\n \n-#if JAX_IFRT_VERSION_NUMBER < 8\n-class IfrtArrayEntry : public PullTable::Entry {\n- public:\n-  struct BufferRef {\n-    xla::ifrt::ArrayRef arr;\n-    xla::PjRtBuffer* buffer;\n-    size_t buf_size;\n-  };\n-  explicit IfrtArrayEntry(std::vector<BufferRef> arrs,\n-                          std::shared_ptr<PremappedCopierState> state,\n-                          size_t xfer_size)\n-      : arrs_(std::move(arrs)), state_(state), xfer_size_(xfer_size) {}\n-  bool Handle(tsl::RCReference<ConnectionState> state,\n-              const SocketTransferPullRequest& req,\n-              size_t base_req_id) override {\n-    for (uint64_t bid : req.buffer_ids()) {\n-      auto req_id = base_req_id;\n-      ++base_req_id;\n-      for (size_t i = 0; i * xfer_size_ < arrs_[bid].buf_size; ++i) {\n-        DmaCopyChunk blob = DmaCopyChunk::Make(\n-            std::move(arrs_[bid].arr), arrs_[bid].buffer, bid, i * xfer_size_,\n-            std::min(xfer_size_, arrs_[bid].buf_size - i * xfer_size_));\n-        bool is_largest = blob.size + blob.offset == arrs_[bid].buf_size;\n-        state_->ScheduleCopy(\n-            std::move(blob), [req_id, state, copier_state = state_, is_largest](\n-                                 PremappedCopierState* copier_state_ptr,\n-                                 void* buf, const DmaCopyChunk& chunk) {\n-              state->Send(\n-                  req_id, buf, chunk.offset, chunk.size, is_largest,\n-                  [copier_state, buf]() { copier_state->ReturnBuffer(buf); });\n-            });\n-      }\n-    }\n-\n-    num_consumed_bufs_ += req.buffer_ids().size();\n-    return num_consumed_bufs_ == arrs_.size();\n-  }\n-\n- private:\n-  absl::Mutex mu_;\n-  size_t num_consumed_bufs_ = 0;\n-  std::vector<BufferRef> arrs_;\n-  std::shared_ptr<PremappedCopierState> state_;\n-  size_t xfer_size_;\n-};\n-#endif\n-\n absl::StatusOr<tsl::RCReference<PullTable::Entry>> CreatePullEntry(\n     const std::vector<xla::ifrt::ArrayRef>& arrs,\n     std::shared_ptr<PremappedCopierState> state, size_t xfer_size,\n     bool use_raw_buffers) {\n-#if JAX_IFRT_VERSION_NUMBER >= 8\n   if (use_raw_buffers) {\n     std::vector<RawBufferEntry::BufferRef> refs;\n     for (auto& arr : arrs) {\n@@ -196,21 +148,6 @@ absl::StatusOr<tsl::RCReference<PullTable::Entry>> CreatePullEntry(\n     }\n   }\n   return tsl::MakeRef<PjRtBufferEntry>(std::move(refs), state, xfer_size);\n-#else\n-  std::vector<IfrtArrayEntry::BufferRef> refs;\n-  for (auto& arr : arrs) {\n-    auto* pjrt_arr = llvm::dyn_cast_or_null<xla::ifrt::PjRtArray>(arr.get());\n-    if (pjrt_arr == nullptr) {\n-      return absl::InvalidArgumentError(\n-          \"Cannot remote transfer non-pjrt arrays.\");\n-    }\n-    for (auto& pjrt_buf : pjrt_arr->pjrt_buffers()) {\n-      TF_ASSIGN_OR_RETURN(size_t buf_size, pjrt_buf->GetOnDeviceSizeInBytes());\n-      refs.push_back({arr, pjrt_buf.get(), buf_size});\n-    }\n-  }\n-  return tsl::MakeRef<IfrtArrayEntry>(std::move(refs), state, xfer_size);\n-#endif\n }\n \n class PyTransferServerConnection {\ndiff --git a/jaxlib/util.cc b/jaxlib/util.cc\nindex a014afa5bebe..a8d45749f4d1 100644\n--- a/jaxlib/util.cc\n+++ b/jaxlib/util.cc\n@@ -36,7 +36,6 @@ limitations under the License.\n namespace xla {\n \n void BlockUntilReadyWithCancel(xla::PjRtFuture<>& future) {\n-#if JAX_IFRT_VERSION_NUMBER >= 5\n   future.BlockUntilReady([](tsl::AsyncValue* value) {\n     auto state = std::make_shared<absl::Notification>();\n     value->AndThen([state]() { state->Notify(); });\n@@ -50,7 +49,6 @@ void BlockUntilReadyWithCancel(xla::PjRtFuture<>& future) {\n       }\n     }\n   });\n-#endif\n }\n \n absl::Status AwaitBuffersReady(absl::Span<ifrt::Array* const> ifrt_arrays) {\ndiff --git a/jaxlib/xla.cc b/jaxlib/xla.cc\nindex 3412766de6bd..d97c6868a04b 100644\n--- a/jaxlib/xla.cc\n+++ b/jaxlib/xla.cc\n@@ -490,27 +490,8 @@ NB_MODULE(_jax, m) {\n               &CompiledMemoryStats::host_temp_size_in_bytes)\n       .def_prop_ro(\"serialized_buffer_assignment_proto\",\n                    [](const CompiledMemoryStats& cms) -> nb::bytes {\n-#if JAX_IFRT_VERSION_NUMBER >= 9\n                      const std::string& s = cms.serialized_buffer_assignment;\n                      return nb::bytes(s.data(), s.size());\n-#elif JAX_IFRT_VERSION_NUMBER >= 7\n-                     if (cms.buffer_assignment.has_value()) {\n-                       std::string s =\n-                           cms.buffer_assignment->SerializeAsString();\n-                       return nb::bytes(s.data(), s.size());\n-                     } else {\n-                       return nb::bytes();\n-                     }\n-#else\n-                     xla::HloProto hlo;\n-                     if (!cms.serialized_hlo_proto.empty() &&\n-                         hlo.ParseFromString(cms.serialized_hlo_proto)) {\n-                       std::string s =\n-                           hlo.buffer_assignment().SerializeAsString();\n-                       return nb::bytes(s.data(), s.size());\n-                     }\n-                     return nb::bytes();\n-#endif\n                    })\n       .def(\"__str__\", &CompiledMemoryStats::DebugString);\n \ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 9963e2603588..f5b74e1e10d6 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -61,7 +61,6 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.compilation_cache import is_persistent_cache_enabled\n from jax._src.lib import _jax\n-from jax._src.lib import jaxlib_extension_version\n import jax._src.util as jax_util\n from jax.ad_checkpoint import checkpoint_name, checkpoint as new_checkpoint\n from jax.errors import (UnexpectedTracerError, TracerIntegerConversionError,\n@@ -1975,11 +1974,6 @@ def test_device_put_sharding_mismatched_tree_different_leaf_count(self):\n       jax.device_put((x, y, z), device=(s1, s2))\n \n   def test_internal_device_put_with_device(self):\n-    if jaxlib_extension_version < 341:\n-      raise unittest.SkipTest(\n-          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n-          \" DevicePut calls\")\n-\n     # Hitting the cache for a single-device jitted execution while using a numpy\n     # array calls internal `DevicePutWithDevice`.\n     f = jax.jit(lambda x: x + 1)\n@@ -1990,10 +1984,6 @@ def test_internal_device_put_with_device(self):\n     self.assertEqual(counts(), {\"device_put_with_device\": 1})\n \n   def test_internal_device_put_fully_replicated(self):\n-    if jaxlib_extension_version < 341:\n-      raise unittest.SkipTest(\n-          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n-          \" DevicePut calls\")\n     if jax.device_count() < 2:\n       raise unittest.SkipTest(\"Test requires >= 2 devices\")\n \n@@ -2011,10 +2001,6 @@ def test_internal_device_put_fully_replicated(self):\n     )\n \n   def test_internal_device_put_batched(self):\n-    if jaxlib_extension_version < 341:\n-      raise unittest.SkipTest(\n-          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n-          \" DevicePut calls\")\n     if jax.device_count() < 2:\n       raise unittest.SkipTest(\"Test requires >= 2 devices\")\n \n@@ -2031,10 +2017,6 @@ def test_internal_device_put_batched(self):\n     )\n \n   def test_internal_device_put_assembled(self):\n-    if jaxlib_extension_version < 341:\n-      raise unittest.SkipTest(\n-          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n-          \" DevicePut calls\")\n     if jax.device_count() < 2:\n       raise unittest.SkipTest(\"Test requires >= 2 devices\")\n \ndiff --git a/tests/buffer_callback_test.py b/tests/buffer_callback_test.py\nindex e77ee4af687f..8bef4135f5d5 100644\n--- a/tests/buffer_callback_test.py\n+++ b/tests/buffer_callback_test.py\n@@ -19,7 +19,6 @@\n import jax\n import jax.numpy as jnp\n from jax._src import test_util as jtu\n-from jax._src.lib import jaxlib_extension_version\n from jax.experimental import buffer_callback\n \n jax.config.parse_flags_with_absl()\n@@ -29,10 +28,6 @@ class BufferCallbackTest(jtu.JaxTestCase):\n \n   def setUp(self):\n     super().setUp()\n-    if jaxlib_extension_version < 334:\n-      self.skipTest(\n-          \"Requires a version of jaxlib with buffer callback support.\"\n-      )\n     if jtu.test_device_matches([\"tpu\"]):\n       self.skipTest(\"Not supported on TPU.\")\n \n@@ -102,7 +97,7 @@ def callback(ctx, out, arg):\n   )\n   @jtu.run_on_devices(\"cuda\")\n   def test_cuda_array_interface(self, dtype, command_buffer_compatible):\n-    if command_buffer_compatible and jaxlib_extension_version < 337:\n+    if command_buffer_compatible:\n       self.skipTest(\"Requires jaxlib extension version of at least 337.\")\n \n     def callback(ctx, out, arg):\ndiff --git a/tests/compilation_cache_test.py b/tests/compilation_cache_test.py\nindex 5a76d732bd76..3f1bb7fab4b1 100644\n--- a/tests/compilation_cache_test.py\n+++ b/tests/compilation_cache_test.py\n@@ -146,14 +146,10 @@ def test_diff_executables(self):\n     )\n     backend = xla_bridge.get_backend()\n     executable_devices = xc.DeviceList(tuple(backend.local_devices()))\n-    if jax._src.lib.jaxlib_extension_version < 331:\n-      executable1 = backend.compile(computation1, compile_options)\n-      executable2 = backend.compile(computation2, compile_options)\n-    else:\n-      executable1 = backend.compile_and_load(\n-          computation1, executable_devices, compile_options)\n-      executable2 = backend.compile_and_load(\n-          computation2, executable_devices, compile_options)\n+    executable1 = backend.compile_and_load(\n+        computation1, executable_devices, compile_options)\n+    executable2 = backend.compile_and_load(\n+        computation2, executable_devices, compile_options)\n     cc.put_executable_and_time(\n         \"key1\", \"computation1\", executable1, backend, FAKE_COMPILE_TIME)\n     cc.put_executable_and_time(\n@@ -177,11 +173,8 @@ def test_put_executable(self):\n     )\n     backend = xla_bridge.get_backend()\n     executable_devices = xc.DeviceList(tuple(devices.flat))\n-    if jax._src.lib.jaxlib_extension_version < 331:\n-      executable = backend.compile(str(computation), compile_options)\n-    else:\n-      executable = backend.compile_and_load(\n-          str(computation), executable_devices, compile_options)\n+    executable = backend.compile_and_load(\n+        str(computation), executable_devices, compile_options)\n     key = cc.get_cache_key(computation, devices, compile_options, backend)\n     cc.put_executable_and_time(\n         key, \"alambda\", executable, backend, FAKE_COMPILE_TIME)\n@@ -577,13 +570,9 @@ def test_backend_serialization_deserialization(self):\n         .runtime_executable()\n     )\n     serialized_executable = backend.serialize_executable(executable)\n-    if jax._src.lib.jaxlib_extension_version < 331:\n-      deserialized_executable = backend.deserialize_executable(  # type: ignore\n-          serialized_executable, None)\n-    else:\n-      deserialized_executable = backend.deserialize_executable(  # type: ignore\n-          serialized_executable,\n-          xc.DeviceList(tuple(jax.local_devices(backend=backend))), None)\n+    deserialized_executable = backend.deserialize_executable(  # type: ignore\n+        serialized_executable,\n+        xc.DeviceList(tuple(jax.local_devices(backend=backend))), None)\n     self.assertEqual(\n         executable.fingerprint, deserialized_executable.fingerprint)\n \ndiff --git a/tests/fused_attention_stablehlo_test.py b/tests/fused_attention_stablehlo_test.py\nindex 925fc2ed4825..64e0f4377462 100644\n--- a/tests/fused_attention_stablehlo_test.py\n+++ b/tests/fused_attention_stablehlo_test.py\n@@ -503,9 +503,6 @@ def test_sdpa_broadcast_bias_and_dbias(self):\n   )\n   @jtu.run_on_devices(\"cuda\")\n   def test_sdpa_dbias(self, batch_size: int):\n-    # TODO: Delete once 0.6.0 is no longer supported.\n-    if jtu.jaxlib_version() == (0, 6, 0):\n-      self.skipTest(\"jaxlib 0.6.0 has a bug\")\n     if jax.device_count() < 4:\n       self.skipTest(\"Requires more than 4 devices.\")\n     # cuDNN only supports dbias when batch size is 1. If the batch size is\ndiff --git a/tests/linalg_sharding_test.py b/tests/linalg_sharding_test.py\nindex e68e94e16494..5d7b3b8a637b 100644\n--- a/tests/linalg_sharding_test.py\n+++ b/tests/linalg_sharding_test.py\n@@ -22,7 +22,6 @@\n from jax import lax\n from jax._src import config\n from jax._src import test_util as jtu\n-from jax._src.lib import jaxlib_extension_version\n from jax.sharding import PartitionSpec as P\n \n config.parse_flags_with_absl()\n@@ -70,8 +69,7 @@ def get_fun_and_shapes(self, fun_and_shapes, grad=False):\n         self.skipTest(\n             f\"Partitioning {fun_and_shapes[0].__name__} only supported on GPU \"\n             \"when shardy is enabled.\")\n-      if (fun_and_shapes[0] == lax.linalg.tridiagonal_solve and\n-          jaxlib_extension_version < 340):\n+      if fun_and_shapes[0] == lax.linalg.tridiagonal_solve:\n         self.skipTest(\n             f\"Partitioning {fun_and_shapes[0].__name__} on GPU, requires a \"\n             \"more recent jaxlib version.\")\ndiff --git a/tests/linalg_test.py b/tests/linalg_test.py\nindex cba3dbb7189d..99cb66c92857 100644\n--- a/tests/linalg_test.py\n+++ b/tests/linalg_test.py\n@@ -33,7 +33,6 @@\n from jax._src.lax import linalg as lax_linalg\n from jax._src import test_util as jtu\n from jax._src import xla_bridge\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.numpy.util import promote_dtypes_inexact\n \n config.parse_flags_with_absl()\n@@ -2205,7 +2204,7 @@ def testSelect(self, dtype):\n   @jtu.sample_product(shape=[(3,), (3, 4), (3, 4, 5)],\n                       dtype=float_types + complex_types)\n   def test_tridiagonal_solve(self, shape, dtype):\n-    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]) and jaxlib_extension_version < 340:\n+    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]):\n       self.skipTest(\"Data type not supported on GPU\")\n     rng = self.rng()\n     d = 1.0 + jtu.rand_positive(rng)(shape, dtype)\n@@ -2244,7 +2243,7 @@ def test_tridiagonal_solve_endpoints(self):\n \n   @jtu.sample_product(shape=[(3,), (3, 4)], dtype=float_types + complex_types)\n   def test_tridiagonal_solve_grad(self, shape, dtype):\n-    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]) and jaxlib_extension_version < 340:\n+    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]):\n       self.skipTest(\"Data type not supported on GPU\")\n     rng = self.rng()\n     d = 1.0 + jtu.rand_positive(rng)(shape, dtype)\ndiff --git a/tests/mosaic/gpu_layout_inference_test.py b/tests/mosaic/gpu_layout_inference_test.py\nindex 038766542f3b..cdc840b0a6f1 100644\n--- a/tests/mosaic/gpu_layout_inference_test.py\n+++ b/tests/mosaic/gpu_layout_inference_test.py\n@@ -19,7 +19,6 @@\n from absl.testing import parameterized\n import jax\n from jax._src import config\n-from jax._src import lib as jaxlib\n from jax._src import test_util as jtu\n from jax._src.interpreters import mlir as mlir_interpreter\n from jax._src.lib.mlir import ir\n@@ -245,12 +244,7 @@ def body(x):\n   def test_infer_broadcast_in_dim_layout(\n       self, broadcast_dim, in_cast, out_cast, in_layout, out_layout\n   ):\n-    # TODO(dasenov): Remove this after the minimal jaxlib version is 0.6.1.\n-    if jaxlib.version < (0, 6, 1):\n-      self.skipTest(\"Test requires jaxlib version >= 0.6.1\")\n-\n     bcast = None\n-\n     in_shape = (64,)\n     out_shape = (64, 64)\n \ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 4e0544d1758e..e7fc9723347b 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -27,7 +27,6 @@\n from absl.testing import absltest, parameterized\n import jax\n from jax._src import config\n-from jax._src import lib as jaxlib\n from jax._src import test_util as jtu\n from jax._src.interpreters import mlir\n from jax._src.lib.mlir import ir\n@@ -3116,10 +3115,6 @@ def add(\n       ((64,), (128, 64), [1]),\n   )\n   def test_broadcast_in_dim(self, input_shape, output_shape, bcast_dims):\n-    # TODO(dasenov): Remove this after the minimal jaxlib version is 0.6.1.\n-    if jaxlib.version < (0, 6, 1):\n-      self.skipTest(\"Test requires jaxlib version >= 0.6.1\")\n-\n     element_value = 42.0\n     def body(ctx, result_gmem_ref, smem):\n       del ctx\ndiff --git a/tests/pallas/gpu_pallas_distributed_test.py b/tests/pallas/gpu_pallas_distributed_test.py\nindex 81433b8c5067..d862e6b9b819 100644\n--- a/tests/pallas/gpu_pallas_distributed_test.py\n+++ b/tests/pallas/gpu_pallas_distributed_test.py\n@@ -34,8 +34,6 @@\n class PallasCallRemoteDMATest(jt_multiprocess.MultiProcessTest):\n \n   def setUp(self):\n-    if jtu.jaxlib_version() < (0, 6, 1):\n-      self.skipTest(\"Test requires jaxlib >= 0.6.1\")\n     if (not jtu.test_device_matches([\"cuda\"]) or\n         not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n       self.skipTest(\"Only works on GPU with capability >= sm90\")\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex aa2e0af2a57c..5d616c43ce54 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -63,7 +63,6 @@\n from jax._src import xla_bridge\n from jax._src.lib import xla_client as xc\n from jax._src.lib import _jax\n-from jax._src.lib import jaxlib_extension_version\n from jax._src.util import curry, unzip2\n \n config.parse_flags_with_absl()\n@@ -7762,8 +7761,6 @@ def f(x):\n   @config.use_shardy_partitioner(True)\n   @jtu.with_explicit_mesh((2, 2), ('x', 'y'))\n   def test_unreduced_basic(self, mesh):\n-    if jaxlib_extension_version < 342:\n-      self.skipTest(\"Test requires a newer jaxlib\")\n     np_inp = np.arange(16).reshape(8, 2)\n     x = jax.device_put(np_inp, P('x', 'y'))\n     y = jax.device_put(np_inp.T, P('y', None))\ndiff --git a/tests/python_callback_test.py b/tests/python_callback_test.py\nindex 26664faa6faf..eef45b3b412b 100644\n--- a/tests/python_callback_test.py\n+++ b/tests/python_callback_test.py\n@@ -31,7 +31,6 @@\n from jax.experimental import io_callback\n from jax.experimental import pjit\n from jax._src.shard_map import shard_map\n-from jax._src.lib import jaxlib_extension_version\n import jax.numpy as jnp\n from jax.sharding import Mesh\n import numpy as np\n@@ -588,8 +587,6 @@ def fun(x):\n \n   @parameterized.parameters(\"int2\", \"int4\", \"uint2\", \"uint4\", \"float4_e2m1fn\")\n   def test_subbyte_operands(self, dtype: str):\n-    if jaxlib_extension_version < 336:\n-      self.skipTest(\"Requires jaxlib_extension_version >= 336.\")\n     if \"2\" in dtype and jtu.test_device_matches([\"tpu\"]):\n       self.skipTest(\n           \"TODO(dsuo): TPU callbacks send SIGABRT for int2, uint2, and\"\n@@ -609,8 +606,6 @@ def f(x):\n \n   @parameterized.parameters(\"int2\", \"int4\", \"uint2\", \"uint4\", \"float4_e2m1fn\")\n   def test_subbyte_results(self, dtype: str):\n-    if jaxlib_extension_version < 336:\n-      self.skipTest(\"Requires jaxlib_extension_version >= 336.\")\n     if \"2\" in dtype and jtu.test_device_matches([\"tpu\"]):\n       self.skipTest(\n           \"TODO(dsuo): TPU callbacks send SIGABRT for int2, uint2, and\"\n@@ -630,8 +625,6 @@ def f():\n \n   @parameterized.parameters(\"int2\", \"int4\", \"uint2\", \"uint4\", \"float4_e2m1fn\")\n   def test_non_default_stride_subbyte_results(self, dtype: str):\n-    if jaxlib_extension_version < 336:\n-      self.skipTest(\"Requires jaxlib_extension_version >= 336.\")\n     if \"2\" in dtype and jtu.test_device_matches([\"tpu\"]):\n       self.skipTest(\n           \"TODO(dsuo): TPU callbacks send SIGABRT for int2, uint2, and\"\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add initial support aligned jnp.swapaxes on major/minor dims\n\nNext steps:\n  - non-tile aligned\n  - Clean up fn and utilize it for general changeTiling\n\nPiperOrigin-RevId: 761731600",
        "output": [
            "```diff\nCommit: 62c46ff976db981912d2182760d4f2abffb972fa\nDate: 2025-05-22T00:24:34Z\nURL: https://github.com/jax-ml/jax/commit/62c46ff976db981912d2182760d4f2abffb972fa\nFiles changed: 4\nAdditions: +375, Deletions: -13\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 919c548adc9b..dd0b9ba4b4a7 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -2383,7 +2383,9 @@ def _gather_lowering_rule(\n \n @register_lowering_rule(lax.transpose_p)\n def _transpose_lowering_rule(ctx: LoweringRuleContext, x, *, permutation):\n-  if permutation != (1, 0):\n+  minormost_transpose = (1, 0)\n+  untiled_tiled_swap = (1, 0, 2)\n+  if permutation not in (minormost_transpose, untiled_tiled_swap):\n     raise NotImplementedError\n   out_type = aval_to_ir_type(\n       ctx.lowering_context.dynamic_shape_replacement_fn, ctx.avals_out[0]\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 99134b46315f..6502a9c6682e 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -5013,11 +5013,315 @@ LogicalResult vector_transpose_rule(RewriteContext &ctx, Operation &op,\n                   ctx.target_shape));\n   ArrayRef<int64_t> permutation = transpose_op.getPermutation();\n   const auto tile_perm = permutation.take_back(2);\n+\n+  // Major minor pemute\n   if (tile_perm != ArrayRef<int64_t>{rank - 2, rank - 1} &&\n       tile_perm != ArrayRef<int64_t>{rank - 1, rank - 2}) {\n-    return transpose_op->emitOpError(\n-        \"Not implemented: Unsupported permutation\");\n+    // This is a 3 stage algorithm that uses combinations and shuffles\n+    // to do a transposition of an 8x8 block of sublanes.\n+    // In the following algorithm description, A, B, ..., H represent 8\n+    // distinct input vregs that form an 8x8 block of data\n+    // to be transposed. In our notation, B2 identifies the third\n+    // sublane (2) of the second vreg (B)\".\n+    //\n+    //\n+    // If we think of each starting input vreg as a row in an 8x8 block of\n+    // elements:\n+    // A: A0 A1 A2 A3 A4 A5 A6 A7\n+    // B: B0 B1 B2 B3 B4 B5 B6 B7\n+    // ...\n+    // H: H0 H1 H2 H3 H4 H5 H6 H7\n+    //\n+    // The goal is to transpose this block, so the output vregs are:\n+    // out0: A0 B0 C0 D0 E0 F0 G0 H0\n+    // out1: A1 B1 C1 D1 E1 F1 G1 H1\n+    // ...\n+    // out7: A7 B7 C7 D7 E7 F7 G7 H7\n+    //\n+    // Stage 1: Operates on pairs of input vregs (e.g., A and B).\n+    //\n+    // Input to Stage 1 (example pair A, B):\n+    // A: A0 A1 A2 A3 A4 A5 A6 A7\n+    // B: B0 B1 B2 B3 B4 B5 B6 B7\n+    //\n+    // Step 1.1: Combine low/high halves.\n+    //   combine_low(A, B)  -> CL_AB: [A0 A1 A2 A3 | B0 B1 B2 B3] (8 elements)\n+    //   combine_high(A, B) -> CH_AB: [A4 A5 A6 A7 | B4 B5 B6 B7] (8 elements)\n+    //   (Notation: '|' separates the 4 elements from A and 4 from B)\n+    //\n+    // Step 1.2: Shuffle.\n+    //   The shuffle pattern for the low part (applied to CL_AB using\n+    //   `shuffle(CL_AB, CH_AB, pattern)`) is {0, 4, 1, 5, 2, 6, 3, 7}.\n+    //   The shuffle pattern for the high part (applied to CH_AB using\n+    //   `shuffle(CL_AB, CH_AB, pattern)`) is {8, 12, 9, 13, 10, 14, 11, 15}.\n+    //   (Indices 0-7 in shuffle refer to CL_AB, 8-15 to CH_AB).\n+    // This results in:\n+    //   s1_AB_0: A0 B0 A1 B1 A2 B2 A3 B3 (from shuffling CL_AB elements)\n+    //   s1_AB_1: A4 B4 A5 B5 A6 B6 A7 B7 (from shuffling CH_AB elements)\n+    //\n+    // Output of Stage 1 / Input to Stage 2 (example for A,B,C,D processing):\n+    //   s1_vregs[0] (from A,B): A0 B0 A1 B1 A2 B2 A3 B3\n+    //   s1_vregs[1] (from A,B): A4 B4 A5 B5 A6 B6 A7 B7\n+    //   s1_vregs[2] (from C,D): C0 D0 C1 D1 C2 D2 C3 D3\n+    //   s1_vregs[3] (from C,D): C4 D4 C5 D5 C6 D6 C7 D7\n+    //   ... (and so on for E,F,G,H into s1_vregs[4-7])\n+\n+    // Stage 2: Operates on groups of 4 vregs from Stage 1 output.\n+    //          (e.g., s1_vregs[0], s1_vregs[1], s1_vregs[2], s1_vregs[3])\n+    //\n+    // Input to Stage 2 (example processing s1_vregs[0] and s1_vregs[2]):\n+    //   X = s1_vregs[0] = [A0 B0 A1 B1 | A2 B2 A3 B3]\n+    //   Y = s1_vregs[2] = [C0 D0 C1 D1 | C2 D2 C3 D3]\n+    //\n+    // Step 2.1: Combine low/high halves.\n+    //   combine_low(X, Y)  -> CL_XY: [A0 B0 A1 B1 | C0 D0 C1 D1]\n+    //   combine_high(X, Y) -> CH_XY: [A2 B2 A3 B3 | C2 D2 C3 D3]\n+    //\n+    //   (Similarly for s1_vregs[1] and s1_vregs[3], let them be X' and Y')\n+    //   combine_low(X', Y')  -> CL_X'Y': [A4 B4 A5 B5 | C4 D4 C5 D5]\n+    //   combine_high(X', Y') -> CH_X'Y': [A6 B6 A7 B7 | C6 D6 C7 D7]\n+    //\n+    // Step 2.2: Shuffle.\n+    //   The shuffle pattern for the low part (e.g., applied to CL_XY) is {0, 1,\n+    //   4, 5, 2, 3, 6, 7}. The shuffle pattern for the high part (e.g., applied\n+    //   to CH_XY, effectively) is {8, 9, 12, 13, 10, 11, 14, 15}.\n+    //\n+    // This results in (for the first group of 4 input vregs A,B,C,D):\n+    //   s2_vregs[0]: A0 B0 C0 D0 A1 B1 C1 D1 (from shuffling CL_XY elements)\n+    //   s2_vregs[1]: A2 B2 C2 D2 A3 B3 C3 D3 (from shuffling CH_XY elements)\n+    //   s2_vregs[2]: A4 B4 C4 D4 A5 B5 C5 D5 (from shuffling CL_X'Y' elements)\n+    //   s2_vregs[3]: A6 B6 C6 D6 A7 B7 C7 D7 (from shuffling CH_X'Y' elements)\n+    //\n+    // Output of Stage 2 / Input to Stage 3:\n+    //   s2_vregs[0]: A0 B0 C0 D0 A1 B1 C1 D1\n+    //   s2_vregs[1]: A2 B2 C2 D2 A3 B3 C3 D3\n+    //   s2_vregs[2]: A4 B4 C4 D4 A5 B5 C5 D5\n+    //   s2_vregs[3]: A6 B6 C6 D6 A7 B7 C7 D7\n+    //   s2_vregs[4]: E0 F0 G0 H0 E1 F1 G1 H1 (from E,F,G,H processing)\n+    //   s2_vregs[5]: E2 F2 G2 H2 E3 F3 G3 H3\n+    //   s2_vregs[6]: E4 F4 G4 H4 E5 F5 G5 H5\n+    //   s2_vregs[7]: E6 F6 G6 H6 E7 F7 G7 H7\n+\n+    // Stage 3: Combine results from Stage 2. No shuffle needed after combine.\n+    // Input to Stage 3 (example for the first two rows of the final transpose):\n+    //   L = s2_vregs[0] = [A0 B0 C0 D0 | A1 B1 C1 D1]\n+    //   R = s2_vregs[4] = [E0 F0 G0 H0 | E1 F1 G1 H1]\n+    //\n+    // Step 3.1: Combine low/high halves.\n+    //   combine_low(L, R)  -> [A0 B0 C0 D0 | E0 F0 G0 H0] ->\n+    //     Final out0: A0 B0 C0 D0 E0 F0 G0 H0\n+    //   combine_high(L, R) -> [A1 B1 C1 D1 | E1 F1 G1 H1] ->\n+    //     Final out1: A1 B1 C1 D1 E1 F1 G1 H1\n+    //   ... and so on for other pairs from Stage 2 output\n+    // (e.g. L=s2_vregs[1], R=s2_vregs[5]).\n+    //\n+    // This results in the correctly transposed 8x8 block.\n+\n+    constexpr int64_t kMajorDimOriginalIdx = 0;\n+    constexpr int64_t kSecondMinorDimOriginalIdx = 1;\n+    constexpr int64_t kMinorMostDimOriginalIdx = 2;\n+\n+    auto vec_shape = src_ty.getShape();\n+    auto major_dim_size = vec_shape[kMajorDimOriginalIdx];\n+    auto second_minor_dim_size = vec_shape[kSecondMinorDimOriginalIdx];\n+\n+    if (layout_in.offsets() != LayoutOffsets{0, 0}) {\n+      return transpose_op.emitOpError(\"Not implemented: Layout with offset.\");\n+    }\n+    if (layout_in.implicit_dim() != VectorLayout::ImplicitDim::kNone) {\n+      return transpose_op.emitOpError(\n+          \"Not implemented: Layout with implicit dimension.\");\n+    }\n+\n+    auto sublane_count = ctx.target_shape[0];\n+    if (second_minor_dim_size % sublane_count != 0 ||\n+        major_dim_size % sublane_count != 0) {\n+      return transpose_op.emitOpError(\n+          \"Not implemented: Swapping major and second minor dimensions must \"\n+          \"result in dimension sizes that are multiples of sublane_count.\");\n+    }\n+\n+    if (!layout_in.hasNativeTiling(ctx.target_shape)) {\n+      return transpose_op.emitOpError(\n+          \"Not implemented: Expected native input tiling.\");\n+    }\n+    if (layout_in != layout_out) {\n+      return transpose_op.emitOpError(\n+          \"Not implemented: Expected same input and output layouts.\");\n+    }\n+    xla::Array<Value> dst_vregs(\n+        layout_out.tileArrayShape(dst_ty.getShape(), ctx.target_shape));\n+\n+    if (layout_in.bitwidth() != 32) {\n+      return transpose_op.emitOpError(\n+          \"Not implemented: Major-second-minor transpose only supported for \"\n+          \"32-bit vectors. Also, input must be a vector type.\");\n+    }\n+    if (ctx.target_shape[0] != 8) {\n+      return transpose_op.emitOpError(\n+          \"Not implemented: Major-second-minor transpose expects 8 sublanes.\");\n+    }\n+\n+    auto vreg_dimensions = src_vregs.dimensions();\n+    // Note(mvoz): Slice is a weird word here, This is used for constructing\n+    // the output vregs - the reason we divide here is because we multiply it\n+    // back later on to get the correct index into src_vregs, but the reason\n+    // we cannot just resolve that in our outer loop is because of the nature\n+    // of a transpose - this dim value goes unmultiplied into the output vregs.\n+    // effectively, our indexing:\n+    // {major_dim_slice_idx * sublane_count, second_minor_dim_slice_idx,\n+    // minor_most_dim_slice_idx} becomes {second_minor_dim_slice_idx *\n+    // sublane_count, major_dim_slice_idx, minor_most_dim_slice_idx}\n+    auto num_slices_in_major_dim =\n+        vreg_dimensions[kMajorDimOriginalIdx] / sublane_count;\n+    auto num_slices_in_second_minor_dim =\n+        vreg_dimensions[kSecondMinorDimOriginalIdx];\n+    auto num_slices_in_minor_most_dim =\n+        vreg_dimensions[kMinorMostDimOriginalIdx];\n+\n+    auto shuffle = [&](Value lhs_vreg, Value rhs_vreg, ArrayRef<int> pattern) {\n+      auto lhs_vreg_type = lhs_vreg.getType();\n+      auto pattern_attr = builder.getDenseI32ArrayAttr(pattern);\n+      return builder\n+          .create<tpu::SublaneShuffleOp>(transpose_op.getLoc(), lhs_vreg_type,\n+                                         lhs_vreg, rhs_vreg, pattern_attr)\n+          .getResult();\n+    };\n+\n+    static constexpr std::array<int, 8> combine_low_pattern = {0, 1, 2,  3,\n+                                                               8, 9, 10, 11};\n+    static constexpr std::array<int, 8> combine_high_pattern = {4,  5,  6,  7,\n+                                                                12, 13, 14, 15};\n+\n+    auto combine_low = [&](Value lhs_vreg, Value rhs_vreg) {\n+      return shuffle(lhs_vreg, rhs_vreg, combine_low_pattern);\n+    };\n+    auto combine_high = [&](Value lhs_vreg, Value rhs_vreg) {\n+      return shuffle(lhs_vreg, rhs_vreg, combine_high_pattern);\n+    };\n+\n+    // Shuffle patterns for Stage 1\n+    // Input to shuffle: (combine_low_val, combine_high_val)\n+    // combine_low_val has A0-A3, B0-B3. Indices 0-7 for shuffle.\n+    // combine_high_val has A4-A7, B4-B7. Indices 8-15 for shuffle.\n+    static constexpr std::array<int, 8> permute_pattern_stage1_low_arr = {\n+        0, 4, 1, 5,\n+        2, 6, 3, 7};  // Selects from combine_low_val to make A0B0A1B1A2B2A3B3\n+    static constexpr std::array<int, 8> permute_pattern_stage1_high_arr = {\n+        8,  12, 9, 13, 10,\n+        14, 11, 15};  // Selects from combine_high_val to make A4B4A5B5A6B6A7B7\n+\n+    // Shuffle patterns for Stage 2\n+    // Input to shuffle: (CL_XY, CH_XY) from Step 2.1 in comments.\n+    // CL_XY has A0B0A1B1C0D0C1D1. Indices 0-7 for shuffle.\n+    // CH_XY has A2B2A3B3C2D2C3D3. Indices 8-15 for shuffle.\n+    static constexpr std::array<int, 8> permute_pattern_stage2_low_arr = {\n+        0, 1, 4, 5, 2, 3, 6, 7};  // Selects from CL_XY to make A0B0C0D0A1B1C1D1\n+    static constexpr std::array<int, 8> permute_pattern_stage2_high_arr = {\n+        8,  9,  12, 13,\n+        10, 11, 14, 15};  // Selects from CH_XY to make A2B2C2D2A3B3C3D3\n+\n+    for (int major_dim_slice_idx = 0;\n+         major_dim_slice_idx < num_slices_in_major_dim; ++major_dim_slice_idx) {\n+      for (int second_minor_dim_slice_idx = 0;\n+           second_minor_dim_slice_idx < num_slices_in_second_minor_dim;\n+           ++second_minor_dim_slice_idx) {\n+        for (int minor_most_dim_slice_idx = 0;\n+             minor_most_dim_slice_idx < num_slices_in_minor_most_dim;\n+             ++minor_most_dim_slice_idx) {\n+          // STAGE 1!\n+          std::array<Value, 8>\n+              stage1_output_vregs;  // Stores s1_vregs from comments\n+          constexpr int num_pairs_stage1 =\n+              4;  // Processes 4 pairs of vregs (A,B), (C,D), (E,F), (G,H)\n+\n+          for (int i = 0; i < num_pairs_stage1; ++i) {\n+            Value first_vreg = src_vregs(\n+                {(2 * i) + (sublane_count * major_dim_slice_idx),\n+                 second_minor_dim_slice_idx, minor_most_dim_slice_idx});\n+            Value second_vreg = src_vregs(\n+                {(2 * i) + (sublane_count * major_dim_slice_idx) + 1,\n+                 second_minor_dim_slice_idx, minor_most_dim_slice_idx});\n+\n+            auto combined_low_val = combine_low(first_vreg, second_vreg);\n+            auto combined_high_val = combine_high(first_vreg, second_vreg);\n+\n+            stage1_output_vregs[2 * i] =\n+                shuffle(combined_low_val, combined_high_val,\n+                        permute_pattern_stage1_low_arr);\n+            stage1_output_vregs[2 * i + 1] =\n+                shuffle(combined_low_val, combined_high_val,\n+                        permute_pattern_stage1_high_arr);\n+          }\n+\n+          // STAGE 2!\n+          std::array<Value, 8>\n+              stage2_output_vregs;  // Stores s2_vregs from comments\n+          constexpr int num_pairs_stage2 =\n+              4;  // Processes 4 pairs of vregs from stage1_output_vregs\n+\n+          for (int i = 0; i < num_pairs_stage2; ++i) {\n+            // Determine the indices for the input pair from\n+            // stage1_output_vregs. The 4 pairs processed in this stage are:\n+            // i=0: (s1_vregs[0], s1_vregs[2])\n+            // i=1: (s1_vregs[1], s1_vregs[3])\n+            // i=2: (s1_vregs[4], s1_vregs[6])\n+            // i=3: (s1_vregs[5], s1_vregs[7])\n+            int s1_lhs_idx = (i / 2) * 4 + (i % 2);\n+            int s1_rhs_idx = s1_lhs_idx + 2;\n+\n+            Value s1_lhs_vreg = stage1_output_vregs[s1_lhs_idx];\n+            Value s1_rhs_vreg = stage1_output_vregs[s1_rhs_idx];\n+\n+            auto combined_low_val = combine_low(s1_lhs_vreg, s1_rhs_vreg);\n+            auto combined_high_val = combine_high(s1_lhs_vreg, s1_rhs_vreg);\n+\n+            // Determine the output indices for stage2_output_vregs.\n+            // Each pair from Stage 1 produces a pair of vregs for Stage 2.\n+            // Results are stored pair-wise:\n+            // i=0 -> s2_vregs[0], s2_vregs[1]\n+            // i=1 -> s2_vregs[2], s2_vregs[3]\n+            // i=2 -> s2_vregs[4], s2_vregs[5]\n+            // i=3 -> s2_vregs[6], s2_vregs[7]\n+            int s2_out_idx_base = 2 * i;\n+\n+            stage2_output_vregs[s2_out_idx_base] =\n+                shuffle(combined_low_val, combined_high_val,\n+                        permute_pattern_stage2_low_arr);\n+            stage2_output_vregs[s2_out_idx_base + 1] =\n+                shuffle(combined_low_val, combined_high_val,\n+                        permute_pattern_stage2_high_arr);\n+          }\n+\n+          // STAGE 3! Combine results from stage 2.\n+          std::array<int64_t, 3> output_idx_parts{\n+              second_minor_dim_slice_idx * sublane_count, major_dim_slice_idx,\n+              minor_most_dim_slice_idx};\n+\n+          constexpr int num_final_combines =\n+              4;  // Corresponds to s2_vregs[0]..s2_vregs[3] pairing with\n+                  // s2_vregs[4]..s2_vregs[7]\n+          for (int i = 0; i < num_final_combines; ++i) {\n+            Value lhs = stage2_output_vregs[i];      // e.g., s2_ABCD_0\n+            Value rhs = stage2_output_vregs[i + 4];  // e.g., s2_EFGH_0\n+            auto final_combined_low = combine_low(lhs, rhs);\n+            auto final_combined_high = combine_high(lhs, rhs);\n+\n+            dst_vregs(output_idx_parts) = final_combined_low;\n+            output_idx_parts[0] += 1;\n+            dst_vregs(output_idx_parts) = final_combined_high;\n+            output_idx_parts[0] += 1;\n+          }\n+        }\n+      }\n+    }\n+    auto assembled =\n+        assemble(builder, dst_ty, layout_out, dst_vregs, ctx.target_shape);\n+    transpose_op.getOperation()->replaceAllUsesWith(assembled);\n+    transpose_op.erase();\n+    return success();\n   }\n+\n   {\n     SmallVector<int64_t> p(permutation);\n     p[rank - 2] = rank - 2;\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex f01d0b4c5888..976e31cb55f4 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -1680,17 +1680,27 @@ class VectorLayoutInferer {\n     auto src_ty = op.getSourceVectorType();\n     TPU_CHECK_OP(permutation.size() == src_ty.getRank(),\n                  \"Transpose permutation has incorrect rank\");\n-    for (auto dim : permutation.drop_back(2)) {\n-      TPU_CHECK_OP(dim < src_ty.getRank() - 2,\n-                   \"Unsupported transpose permutation - minor dims into major\");\n-    }\n-    for (auto dim : permutation.take_back(2)) {\n-      TPU_CHECK_OP(dim >= src_ty.getRank() - 2,\n-                   \"Unsupported transpose permutation - major dims into minor\");\n+    bool untiled_tiled_swap = false;\n+    // TODO(mvoz): Expand to more general cases. b/419268277\n+    if (permutation.size() == 3 && permutation[0] == 1 && permutation[1] == 0) {\n+      untiled_tiled_swap = true;\n+    } else {\n+      for (auto dim : permutation.drop_back(2)) {\n+        TPU_CHECK_OP(dim < src_ty.getRank() - 2,\n+                     \"Unsupported transpose permutation - minor dims into \"\n+                     \"major > 3 dimensions\");\n+      }\n+      for (auto dim : permutation.take_back(2)) {\n+        TPU_CHECK_OP(dim >= src_ty.getRank() - 2,\n+                     \"Unsupported transpose permutation - major dims into \"\n+                     \"minor > 3 dimensions\");\n+      }\n     }\n     Layout required_layout = some_layout;\n-    // Require native tiling if we're going to use the XLU.\n-    if (permutation[permutation.size() - 1] == permutation.size() - 2) {\n+    // Require native tiling if we're going to use the XLU, or doing a\n+    // major/minor permute.\n+    if (untiled_tiled_swap ||\n+        permutation[permutation.size() - 1] == permutation.size() - 2) {\n       auto native_tiling = nativeTiling(layout.bitwidth());\n       required_layout = VectorLayout(layout.bitwidth(), LayoutOffsets{0, 0},\n                                      native_tiling, ImplicitDim::kNone);\ndiff --git a/tests/pallas/ops_test.py b/tests/pallas/ops_test.py\nindex bcda0ca9f71e..3e777ac7ea2c 100644\n--- a/tests/pallas/ops_test.py\n+++ b/tests/pallas/ops_test.py\n@@ -300,7 +300,7 @@ def pallas_call(cls, *args, **kwargs):\n     return pl.pallas_call(*args, interpret=cls.INTERPRET, **kwargs)\n \n   def skip_if_mosaic_gpu(self):\n-    if jtu.test_device_matches([\"cuda\"]) and use_mosaic_gpu:\n+    if jtu.test_device_matches([\"gpu\"]) and use_mosaic_gpu:\n       self.skipTest(\"TODO: Mosaic GPU does not support this yet\")\n \n \n@@ -2569,6 +2569,52 @@ def kernel(x_ref, out_ref):\n     )(x)\n     np.testing.assert_array_equal(out, np.diagonal(x))\n \n+  @parameterized.product(\n+      # Skip some steps to just run less cases\n+      # TODO(mvoz): Hypothesis?\n+      x_dim_size=tuple(8 * i for i in range(1, 5)),\n+      y_dim_size=tuple(8 * i for i in range(1, 5)),\n+      z_dim_size=tuple(128 * i for i in range(1, 3)),\n+      dtype=(jnp.float32,),\n+  )\n+  def test_jnp_swapaxes_major_minor(\n+      self, x_dim_size, y_dim_size, z_dim_size, dtype\n+  ):\n+    if jtu.test_device_matches([\"gpu\"]):\n+      if any(\n+          not is_power_of_two(x) for x in [x_dim_size, y_dim_size, z_dim_size]\n+      ):\n+        self.skipTest(\n+            \"the Pallas Triton lowering currently requires that all operations\"\n+            \" have array arguments and results whose size is a power of 2.\"\n+            f\" Encountered an array of shape ({x_dim_size}, {y_dim_size},\"\n+            f\" {z_dim_size})\"\n+        )\n+      if x_dim_size * y_dim_size * z_dim_size * 4 > 32768:\n+        self.skipTest(\n+            \"Mosaic GPU kernel exceeds available shared memory\"\n+            f\" smem_bytes={x_dim_size * y_dim_size * z_dim_size * 4} > 32768\"\n+        )\n+    self.skip_if_mosaic_gpu()\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 22):\n+      self.skipTest(\"Requires libtpu built after 2025-5-22\")\n+\n+    x = jnp.arange(x_dim_size * y_dim_size * z_dim_size, dtype=dtype).reshape(\n+        (x_dim_size, y_dim_size, z_dim_size)\n+    )\n+\n+    def kernel(x_ref, out_ref):\n+      out_ref[...] = jnp.swapaxes(x_ref[...], 0, 1)\n+\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct(\n+            (y_dim_size, x_dim_size, z_dim_size), dtype\n+        ),\n+    )(x)\n+    expected = jnp.swapaxes(x, 0, 1)\n+    np.testing.assert_array_equal(out, expected)\n+\n \n class OpsInterpretTest(OpsTest):\n   INTERPRET = True\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move _src/interpreters/ad.py to its own BUILD rule.\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nThis required moving some internal utilities out of dispatch.py, which is part of the main JAX build rule. I chose api_util.py because they seem to fit there.\n\nPiperOrigin-RevId: 761722054",
        "output": [
            "```diff\nCommit: 8da86ea0a3128d2cff517251f40a5b3e285f4d7b\nDate: 2025-05-21T23:54:46Z\nURL: https://github.com/jax-ml/jax/commit/8da86ea0a3128d2cff517251f40a5b3e285f4d7b\nFiles changed: 9\nAdditions: +81, Deletions: -61\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 4218cd3f0a77..e431a3c5056c 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -316,7 +316,6 @@ py_library_providing_imports_info(\n         \"_src/ffi.py\",\n         \"_src/flatten_util.py\",\n         \"_src/interpreters/__init__.py\",\n-        \"_src/interpreters/ad.py\",\n         \"_src/interpreters/batching.py\",\n         \"_src/interpreters/pxla.py\",\n         \"_src/pjit.py\",\n@@ -381,6 +380,7 @@ py_library_providing_imports_info(\n     visibility = [\"//visibility:public\"],\n     deps = [\n         \":abstract_arrays\",\n+        \":ad\",\n         \":ad_util\",\n         \":api_util\",\n         \":basearray\",\n@@ -671,6 +671,23 @@ pytype_strict_library(\n     ] + py_deps(\"numpy\"),\n )\n \n+pytype_strict_library(\n+    name = \"ad\",\n+    srcs = [\"_src/interpreters/ad.py\"],\n+    deps = [\n+        \":ad_util\",\n+        \":api_util\",\n+        \":config\",\n+        \":core\",\n+        \":dtypes\",\n+        \":mesh\",\n+        \":partial_eval\",\n+        \":source_info_util\",\n+        \":tree_util\",\n+        \":util\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"mlir\",\n     srcs = [\"_src/interpreters/mlir.py\"],\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 059db1c92c98..3ff103997dc7 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -37,6 +37,7 @@\n import numpy as np\n from contextlib import contextmanager\n \n+from jax._src import api_util\n from jax._src import deprecations\n from jax._src import linear_util as lu\n from jax._src import stages\n@@ -113,14 +114,14 @@ def _nan_check_posthook(fun, args, kwargs, output):\n \n   try:\n     dispatch.check_special(pjit.pjit_p.name, buffers)\n-  except dispatch.InternalFloatingPointError as e:\n+  except api_util.InternalFloatingPointError as e:\n     assert config.debug_nans.value or config.debug_infs.value\n     if hasattr(fun, '_fun'):\n       f = fun._fun\n       if getattr(f, '_apply_primitive', False):\n         raise FloatingPointError(f\"invalid value ({e.ty}) encountered in {f.__qualname__}\") from None\n       # compiled_fun can only raise in this case\n-      dispatch.maybe_recursive_nan_check(e, f, args, kwargs)\n+      api_util.maybe_recursive_nan_check(e, f, args, kwargs)\n       raise AssertionError(\"Unreachable\") from e\n     else:\n       # TODO(emilyaf): Shouldn't need this fallback.\n@@ -1707,7 +1708,7 @@ def cache_miss(*args, **kwargs):\n           out = execute(*p.flat_args)\n         else:\n           out = pxla.xla_pmap_p.bind_with_trace(trace, (p.flat_fun, *p.flat_args), params)\n-      except dispatch.InternalFloatingPointError as e:\n+      except api_util.InternalFloatingPointError as e:\n         raise FloatingPointError(f'Invalid value ({e.ty}) encountered in parallel computation.')\n \n     out_tree, out_flat = p.out_tree, out\ndiff --git a/jax/_src/api_util.py b/jax/_src/api_util.py\nindex 163bade2065c..2e7ba551c624 100644\n--- a/jax/_src/api_util.py\n+++ b/jax/_src/api_util.py\n@@ -767,3 +767,41 @@ def _check_no_aliased_closed_over_refs(dbg: core.DebugInfo, consts, args) -> Non\n           f\"array reference of type {a.str_short()} was both closed over and \"\n           f\"passed as the argument \"\n           f\"{dbg.safe_arg_names(len(args))[i]}\" if dbg else \"at flat index {i}\")\n+\n+class InternalFloatingPointError(Exception):\n+  name: str\n+  ty: str\n+\n+  def __init__(self, name: str, ty: str):\n+    self.name = name\n+    self.ty = ty\n+\n+def maybe_recursive_nan_check(e: Exception, fun: Callable, args, kwargs,\n+) -> None:  # always raises an exception\n+  print(\"Invalid nan value encountered in the output of a jax.jit \"\n+        \"function. Calling the de-optimized version.\")\n+  try:\n+    _ = fun(*args, **kwargs)\n+  except (FloatingPointError, ZeroDivisionError) as e2:\n+    raise e2 from None\n+  else:\n+    _raise_no_nan_in_deoptimized(e)\n+\n+\n+def _raise_no_nan_in_deoptimized(e) -> None:\n+  msg = (f\"{str(e)}. Because \"\n+        \"jax_config.debug_nans.value and/or config.jax_debug_infs is set, the \"\n+        \"de-optimized function (i.e., the function as if the `jit` \"\n+        \"decorator were removed) was called in an attempt to get a more \"\n+        \"precise error message. However, the de-optimized function did not \"\n+        \"produce invalid values during its execution. This behavior can \"\n+        \"result from `jit` optimizations causing the invalid value to be \"\n+        \"produced. It may also arise from having nan/inf literals as \"\n+        \"inputs or outputs, like `jax.jit(lambda ...: jax.numpy.nan)(...)`. \"\n+        \"\\n\\n\"\n+        \"It may be possible to avoid the invalid value by removing the \"\n+        \"`jit` decorator, at the cost of losing optimizations. \"\n+        \"\\n\\n\"\n+        \"If you see this error, consider opening a bug report at \"\n+        \"https://github.com/jax-ml/jax.\")\n+  raise FloatingPointError(msg) from None\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex 9a11ffa104a8..d1ea7439cb0c 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -24,7 +24,7 @@\n import logging\n import threading\n import time\n-from typing import Any, Callable\n+from typing import Any\n \n import jax\n from jax._src import api\n@@ -42,6 +42,7 @@\n from jax._src.interpreters import mlir\n from jax._src.interpreters import pxla\n from jax._src.interpreters import xla\n+from jax._src.api_util import InternalFloatingPointError\n from jax._src.layout import DeviceLocalLayout, Layout\n from jax._src.lib import xla_client as xc\n from jax._src.mesh import AbstractMesh, Mesh\n@@ -341,43 +342,6 @@ class CopySemantics(enum.Enum):\n   COPY = enum.auto()\n   DONATE = enum.auto()\n \n-class InternalFloatingPointError(Exception):\n-  name: str\n-  ty: str\n-\n-  def __init__(self, name: str, ty: str):\n-    self.name = name\n-    self.ty = ty\n-\n-def maybe_recursive_nan_check(e: Exception, fun: Callable, args, kwargs,\n-) -> None:  # always raises an exception\n-  print(\"Invalid nan value encountered in the output of a jax.jit \"\n-        \"function. Calling the de-optimized version.\")\n-  try:\n-    _ = fun(*args, **kwargs)\n-  except (FloatingPointError, ZeroDivisionError) as e2:\n-    raise e2 from None\n-  else:\n-    _raise_no_nan_in_deoptimized(e)\n-\n-def _raise_no_nan_in_deoptimized(e) -> None:\n-  msg = (f\"{str(e)}. Because \"\n-        \"jax_config.debug_nans.value and/or config.jax_debug_infs is set, the \"\n-        \"de-optimized function (i.e., the function as if the `jit` \"\n-        \"decorator were removed) was called in an attempt to get a more \"\n-        \"precise error message. However, the de-optimized function did not \"\n-        \"produce invalid values during its execution. This behavior can \"\n-        \"result from `jit` optimizations causing the invalid value to be \"\n-        \"produced. It may also arise from having nan/inf literals as \"\n-        \"inputs or outputs, like `jax.jit(lambda ...: jax.numpy.nan)(...)`. \"\n-        \"\\n\\n\"\n-        \"It may be possible to avoid the invalid value by removing the \"\n-        \"`jit` decorator, at the cost of losing optimizations. \"\n-        \"\\n\\n\"\n-        \"If you see this error, consider opening a bug report at \"\n-        \"https://github.com/jax-ml/jax.\")\n-  raise FloatingPointError(msg) from None\n-\n def _identity_fn(x):\n   return x\n \ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 29af03416a76..9366b91f8022 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -21,12 +21,12 @@\n from functools import partial\n from typing import Any\n \n+from jax._src import api_util\n from jax._src import config\n-from jax._src import dispatch\n from jax._src import linear_util as lu\n from jax._src.interpreters import partial_eval as pe\n-from jax.tree_util import (tree_flatten, tree_unflatten,\n-                           register_pytree_node, Partial, PyTreeDef)\n+from jax._src.tree_util import (tree_flatten, tree_unflatten,\n+                                register_pytree_node, Partial, PyTreeDef)\n from jax._src import mesh as mesh_lib\n from jax._src import core\n from jax._src import source_info_util\n@@ -1125,7 +1125,7 @@ def out_axes_thunk():\n \n   try:\n     out_flat = primitive.bind(fun, *all_args, **new_params)\n-  except dispatch.InternalFloatingPointError as e:\n+  except api_util.InternalFloatingPointError as e:\n     print(\"Invalid nan value encountered in the backward pass of a jax.jit \"\n           \"function. Calling the de-optimized backward pass.\")\n     try:\n@@ -1135,7 +1135,7 @@ def out_axes_thunk():\n     else:\n       # If control reaches this line, we got a NaN on the output of `compiled`\n       # but not `fun.call_wrapped` on the same arguments. Let's tell the user.\n-      dispatch._raise_no_nan_in_deoptimized(e)\n+      api_util._raise_no_nan_in_deoptimized(e)\n   arg_cts = tree_unflatten(out_tree(), out_flat)\n \n   # The freevars are being fanned out (not mapped). During transpose the\n@@ -1266,11 +1266,3 @@ def __init__(self):\n \n # TODO(mattjj): remove this vestigial dict\n reducing_transposes: dict[core.Primitive, Callable] = {}\n-\n-########################### pvary ##################################\n-\n-def _pvary_transpose_rule(cts, *_, axes, axis_index_groups):\n-  from jax._src.lax import parallel as lax_parallel\n-  return lax_parallel.psum_invariant_p.bind(\n-      *cts, axes=axes, axis_index_groups=axis_index_groups)\n-deflinear2(core.pvary_p, _pvary_transpose_rule)\ndiff --git a/jax/_src/lax/parallel.py b/jax/_src/lax/parallel.py\nindex a9abf8f12939..bf27261a2c8e 100644\n--- a/jax/_src/lax/parallel.py\n+++ b/jax/_src/lax/parallel.py\n@@ -2059,3 +2059,10 @@ def _psum_invariant_transpose_rule(cts, *args, axes, axis_index_groups):\n   del args\n   return core.pvary_p.bind(*cts, axes=axes, axis_index_groups=axis_index_groups)\n ad.deflinear2(psum_invariant_p, _psum_invariant_transpose_rule)\n+\n+########################### pvary ##################################\n+\n+def _pvary_transpose_rule(cts, *_, axes, axis_index_groups):\n+  return psum_invariant_p.bind(\n+      *cts, axes=axes, axis_index_groups=axis_index_groups)\n+ad.deflinear2(core.pvary_p, _pvary_transpose_rule)\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 6340d96a55ee..0624dad88a2b 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -176,10 +176,10 @@ def _python_pjit_helper(fun: Callable, jit_info: PjitInfo, *args, **kwargs):\n               f\"Argument '{name}' of shape {aval.str_short()} of type\"\n               f' {type(arg)} is not a valid JAX type.') from e\n       raise AssertionError(\"Unreachable\") from e\n-  except dispatch.InternalFloatingPointError as e:\n+  except api_util.InternalFloatingPointError as e:\n     if getattr(fun, '_apply_primitive', False):\n       raise FloatingPointError(f\"invalid value ({e.ty}) encountered in {fun.__qualname__}\") from None\n-    dispatch.maybe_recursive_nan_check(e, fun, args, kwargs)\n+    api_util.maybe_recursive_nan_check(e, fun, args, kwargs)\n \n   if p.box_data:\n     box_treedef, out_tree = p.out_tree.children()\n@@ -2562,7 +2562,7 @@ def prune_type(ty, xs, maybe_zeros):\n         keep_unused=keep_unused,\n         inline=inline,\n         compiler_options_kvs=compiler_options_kvs)\n-  except dispatch.InternalFloatingPointError as e:\n+  except api_util.InternalFloatingPointError as e:\n     print(\"Invalid nan value encountered in the backward pass of a jax.jit \"\n           \"function. Calling the de-optimized backward pass.\")\n     try:\n@@ -2572,7 +2572,7 @@ def prune_type(ty, xs, maybe_zeros):\n     else:\n       # If control reaches this line, we got a NaN on the output of `compiled`\n       # but not `fun.call_wrapped` on the same arguments. Let's tell the user.\n-      dispatch._raise_no_nan_in_deoptimized(e)\n+      api_util._raise_no_nan_in_deoptimized(e)\n \n   if attrs_tracked:\n     final_states, nz_cts_out = split_list(nz_cts_out, [num_attr_outs])\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 72e1420b0b2b..66df2505100c 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -1054,7 +1054,7 @@ def _maybe_check_special(outs):\n           for s in getattr(leaf, 'addressable_shards', [])]\n   try:\n     dispatch.check_special('shard_map', bufs)\n-  except dispatch.InternalFloatingPointError as e:\n+  except api_util.InternalFloatingPointError as e:\n     raise FloatingPointError(f'Invalid value ({e.ty}) encountered in sharded computation.') from None\n \n class ShardMapTrace(core.Trace):\n@@ -1562,7 +1562,7 @@ def new_out_specs_thunk():\n     except (FloatingPointError, ZeroDivisionError) as e2:\n       raise e2 from None\n     else:\n-      dispatch._raise_no_nan_in_deoptimized(e)\n+      api_util._raise_no_nan_in_deoptimized(e)\n   return tree_unflatten(out_tree(), out_flat)\n ad.primitive_transposes[shard_map_p] = _shard_map_transpose\n \ndiff --git a/jax/extend/BUILD b/jax/extend/BUILD\nindex 59958c1da389..06fb8e671120 100644\n--- a/jax/extend/BUILD\n+++ b/jax/extend/BUILD\n@@ -43,6 +43,7 @@ py_library_providing_imports_info(\n     deps = [\n         \"//jax\",\n         \"//jax:abstract_arrays\",\n+        \"//jax:ad\",\n         \"//jax:ad_util\",\n         \"//jax:core\",\n     ],\n\n```"
        ],
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Expose tree prefix broadcasting as a public API in tree utils.",
        "output": [
            "```diff\nCommit: f9a1475e1a286a8d21ec8685aea9f92b2c40d941\nDate: 2025-05-21T23:18:40Z\nURL: https://github.com/jax-ml/jax/commit/f9a1475e1a286a8d21ec8685aea9f92b2c40d941\nFiles changed: 8\nAdditions: +108, Deletions: -5\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 1e866fae6af5..b34bf36997af 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -16,6 +16,9 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n \n ## Unreleased\n \n+* New features:\n+  * Added {func}`jax.tree.broadcast` which implements a pytree prefix broadcasting helper.\n+\n ## JAX 0.6.1 (May 21, 2025)\n \n * New features:\ndiff --git a/docs/jax.tree.rst b/docs/jax.tree.rst\nindex e65c77c757c1..1a0ddaec86d0 100644\n--- a/docs/jax.tree.rst\n+++ b/docs/jax.tree.rst\n@@ -12,6 +12,7 @@ List of Functions\n    :toctree: _autosummary\n \n    all\n+   broadcast\n    flatten\n    flatten_with_path\n    leaves\ndiff --git a/docs/jax.tree_util.rst b/docs/jax.tree_util.rst\nindex 73fd1f376e9f..c89b777ca548 100644\n--- a/docs/jax.tree_util.rst\n+++ b/docs/jax.tree_util.rst\n@@ -38,6 +38,7 @@ These APIs are now accessed via :mod:`jax.tree`.\n    :toctree: _autosummary\n \n    tree_all\n+   tree_broadcast\n    tree_flatten\n    tree_leaves\n    tree_map\ndiff --git a/jax/_src/tree.py b/jax/_src/tree.py\nindex 70d75a126804..9a3e001d902b 100644\n--- a/jax/_src/tree.py\n+++ b/jax/_src/tree.py\n@@ -378,3 +378,34 @@ def map_with_path(\n     - :func:`jax.tree_util.register_pytree_with_keys`\n   \"\"\"\n   return tree_util.tree_map_with_path(f, tree, *rest, is_leaf=is_leaf)\n+\n+\n+def broadcast(prefix_tree: Any, full_tree: Any,\n+              is_leaf: Callable[[Any], bool] | None = None\n+              ) -> list[Any]:\n+  \"\"\"Broadcasts a tree prefix into the full structure of a given tree.\n+\n+    Args:\n+      prefix_tree: a pytree that is a tree prefix of full_tree.\n+      full_tree: a pytree with the structure to broadcast the prefix leaves into.\n+      is_leaf: an optionally specified function that will be called at each\n+        flattening step. It should return a boolean, with true stopping the\n+        traversal and the whole subtree being treated as a leaf, and false\n+        indicating the flattening should traverse the current object.\n+\n+    Returns:\n+      A pytree matching the structure of full_tree where the leaves of prefix_tree have been\n+      broadcasted into the leaves of each corresponding subtree.\n+\n+    Examples:\n+      >>> import jax\n+      >>> prefix = (1, 2, 3)\n+      >>> full = (0, {'a': 0, 'b': 0}, (0, 0))\n+      >>> jax.tree.broadcast(prefix, full)\n+      (1, {'a': 2, 'b': 2}, (3, 3))\n+\n+    See Also:\n+      - :func:`jax.tree.leaves`\n+      - :func:`jax.tree.structure`\n+  \"\"\"\n+  return tree_util.tree_broadcast(prefix_tree, full_tree, is_leaf=is_leaf)\ndiff --git a/jax/_src/tree_util.py b/jax/_src/tree_util.py\nindex e2e97c90f120..6edbbfd62d12 100644\n--- a/jax/_src/tree_util.py\n+++ b/jax/_src/tree_util.py\n@@ -560,17 +560,42 @@ def __new__(klass, func, *args, **kw):\n )\n \n \n-# broadcast_prefix is not exported.\n+@export\n+def tree_broadcast(prefix_tree: Any, full_tree: Any,\n+                   is_leaf: Callable[[Any], bool] | None = None\n+                  ) -> list[Any]:\n+  \"\"\"Alias of :func:`jax.tree.broadcast`.\"\"\"\n+  broadcast_leaves = broadcast_prefix(prefix_tree, full_tree, is_leaf=is_leaf)\n+  return tree_structure(full_tree).unflatten(broadcast_leaves)\n+\n+\n+# broadcast_prefix is not exported\n def broadcast_prefix(prefix_tree: Any, full_tree: Any,\n                      is_leaf: Callable[[Any], bool] | None = None\n                      ) -> list[Any]:\n-  # If prefix_tree is not a tree prefix of full_tree, this code can raise a\n-  # ValueError; use prefix_errors to find disagreements and raise more precise\n-  # error messages.\n+  \"\"\"Broadcasts tree prefix leaves into the full set of leaves for a given full tree.\n+\n+    Args:\n+      prefix_tree: a pytree that is a tree prefix of full_tree.\n+      full_tree: a pytree with the structure to broadcast the prefix leaves into.\n+      is_leaf: an optionally specified function that will be called at each\n+        flattening step. It should return a boolean, with true stopping the\n+        traversal and the whole subtree being treated as a leaf, and false\n+        indicating the flattening should traverse the current object.\n+\n+    Returns:\n+      A list of leaves matching the expected count for the full tree,\n+      with the leaf of each prefix tree being duplicated to match the count of\n+      its corresponding subtree.\n+  \"\"\"\n   result = []\n   num_leaves = lambda t: tree_structure(t).num_leaves\n   add_leaves = lambda x, subtree: result.extend([x] * num_leaves(subtree))\n-  tree_map(add_leaves, prefix_tree, full_tree, is_leaf=is_leaf)\n+  try:\n+    tree_map(add_leaves, prefix_tree, full_tree, is_leaf=is_leaf)\n+  except ValueError:\n+      e, *_ = prefix_errors(prefix_tree, full_tree)\n+      raise e('broadcast_prefix prefix_tree') from None\n   return result\n \n \ndiff --git a/jax/tree.py b/jax/tree.py\nindex 270c34fe9647..03ca503f3a41 100644\n--- a/jax/tree.py\n+++ b/jax/tree.py\n@@ -19,6 +19,7 @@\n \n from jax._src.tree import (\n     all as all,\n+    broadcast as broadcast,\n     flatten_with_path as flatten_with_path,\n     flatten as flatten,\n     leaves_with_path as leaves_with_path,\ndiff --git a/jax/tree_util.py b/jax/tree_util.py\nindex 9f42284144ec..b35890dfc887 100644\n--- a/jax/tree_util.py\n+++ b/jax/tree_util.py\n@@ -58,6 +58,7 @@\n     register_pytree_with_keys as register_pytree_with_keys,\n     register_static as register_static,\n     tree_all as tree_all,\n+    tree_broadcast as tree_broadcast,\n     tree_flatten_with_path as tree_flatten_with_path,\n     tree_flatten as tree_flatten,\n     tree_leaves_with_path as tree_leaves_with_path,\ndiff --git a/tests/tree_util_test.py b/tests/tree_util_test.py\nindex 0df811d9da28..8d4cd5854e7d 100644\n--- a/tests/tree_util_test.py\n+++ b/tests/tree_util_test.py\n@@ -627,6 +627,39 @@ def testTransposeWithCustomObject(self):\n                                       FlatCache({\"a\": [3, 4], \"b\": [5, 6]}))\n     self.assertEqual(expected, actual)\n \n+  @parameterized.parameters(*TREES)\n+  def testBroadcast(self, tree):\n+    if isinstance(tree, FlatCache):\n+      # The tree_map construction below fails for FlatCache, because\n+      # the cached metadata becomes out of sync.\n+      self.skipTest(\"Test does not work properly for FlatCache.\")\n+    def make_inner(x):\n+      return [x, x, x]\n+    nested = tree_util.tree_map(make_inner, tree)\n+    actual = tree_util.tree_broadcast(tree, nested)\n+    self.assertEqual(actual, nested)\n+\n+  def testBroadcastSimple(self):\n+    prefix = (1, 2, 3)\n+    full = (0, {'a': 0, 'b': 0}, (0, 0))\n+    actual = tree_util.tree_broadcast(prefix, full)\n+    expected = (1, {'a': 2, 'b': 2}, (3, 3))\n+    self.assertEqual(actual, expected)\n+\n+  def testBroadcastError(self):\n+    prefix = (1, 2, 3)\n+    full = (0, {'a': 0, 'b': 0})\n+    with self.assertRaisesRegex(ValueError, \"pytree structure error\"):\n+      tree_util.tree_broadcast(prefix, full)\n+    prefix = (1, 2)\n+    full = (0, {'a': 0, 'b': 0}, (0, 0))\n+    with self.assertRaisesRegex(ValueError, \"pytree structure error\"):\n+      tree_util.tree_broadcast(prefix, full)\n+    prefix = (1, {'a': 0})\n+    full = (0, {'a': 0, 'b': 0})\n+    with self.assertRaisesRegex(ValueError, \"pytree structure error\"):\n+      tree_util.tree_broadcast(prefix, full)\n+\n   @parameterized.parameters([(*t, s) for t, s in zip(TREES, TREE_STRINGS)])\n   def testStringRepresentation(self, tree, correct_string):\n     \"\"\"Checks that the string representation of a tree works.\"\"\"\n@@ -1444,6 +1477,13 @@ def test_tree_transpose(self):\n       tree_util.tree_transpose(outer_treedef, inner_treedef, obj)\n     )\n \n+  def test_tree_broadcast(self):\n+    prefix = (1, 2, 3)\n+    full = (0, {'a': 0, 'b': 0}, (0, 0))\n+    actual = jax.tree.broadcast(prefix, full)\n+    expected = (1, {'a': 2, 'b': 2}, (3, 3))\n+    self.assertEqual(actual, expected)\n+\n   def test_tree_unflatten(self):\n     leaves, treedef = jax.tree.flatten([1, 2, (3, 4)])\n     self.assertEqual(\n\n```"
        ],
        "from_id": [
            "levskaya"
        ]
    },
    {
        "text_input": "Allow eval_shape to propagate shardings if the aval has shardings in full explicit mode\n\nPiperOrigin-RevId: 761708753",
        "output": [
            "```diff\nCommit: 61a9bd2b3d58640452c9f3c8514736cff9ac4cfe\nDate: 2025-05-21T23:15:06Z\nURL: https://github.com/jax-ml/jax/commit/61a9bd2b3d58640452c9f3c8514736cff9ac4cfe\nFiles changed: 2\nAdditions: +25, Deletions: -5\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 0503e58b2e45..6340d96a55ee 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -350,11 +350,17 @@ def jit_lower(jit_func, *args, **kwargs):\n @api_boundary\n def jit_eval_shape(jit_func, *args, **kwargs):\n   p, _ = _infer_params(jit_func._fun, jit_func._jit_info, args, kwargs)\n-  out_s = [None if isinstance(s, UnspecifiedValue) else s for s in p.params['out_shardings']]\n-  # TODO(yashkatariya): Add `Layout` to SDS.\n-  out = [api.ShapeDtypeStruct(x.shape, x.dtype, sharding=s,\n-                              weak_type=x.weak_type)\n-         for x, s in zip(p.params['jaxpr'].out_avals, out_s)]\n+  out_shardings = [None if isinstance(s, UnspecifiedValue) else s\n+                   for s in p.params['out_shardings']]\n+  out = []\n+  for a, out_s in zip(p.params['jaxpr'].out_avals, out_shardings):\n+    if out_s is None:\n+      s = a.sharding if a.sharding.mesh._are_all_axes_explicit else out_s\n+    else:\n+      s = out_s\n+    # TODO(yashkatariya): Add `Layout` to SDS.\n+    out.append(api.ShapeDtypeStruct(a.shape, a.dtype, sharding=s,\n+                                    weak_type=a.weak_type))\n   return tree_unflatten(p.out_tree, out)\n \n def jit_evict_fn(self):\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 523601691a97..aa2e0af2a57c 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7857,6 +7857,20 @@ def g(x, y):\n         core.ShardingTypeError, \"lhs is unreduced while rhs is not\"):\n       g.trace(x, y)\n \n+  @jtu.with_explicit_mesh((2, 2), ('x', 'y'))\n+  def test_eval_shape(self, mesh):\n+    np_inp = np.arange(16).reshape(8, 2)\n+    arr = jax.device_put(np_inp, P('x', 'y'))\n+\n+    @jax.jit\n+    def f(x):\n+      return x * 2\n+\n+    out = jax.eval_shape(f, arr)\n+    self.assertIsInstance(out, jax.ShapeDtypeStruct)\n+    self.assertEqual(out.sharding,\n+                     NamedSharding(mesh.abstract_mesh, P('x', 'y')))\n+\n \n @jtu.pytest_mark_if_available('multiaccelerator')\n class PJitErrorTest(jtu.JaxTestCase):\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic:TPU][Relayout] Remove minor implicit dimension for 32-bit native tiling\n\nPiperOrigin-RevId: 761692972",
        "output": [
            "```diff\nCommit: efc70a06e29c6f4a6f18d86e261a2fe1546572a5\nDate: 2025-05-21T22:32:06Z\nURL: https://github.com/jax-ml/jax/commit/efc70a06e29c6f4a6f18d86e261a2fe1546572a5\nFiles changed: 1\nAdditions: +13, Deletions: -1\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 4d1323d68057..99134b46315f 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -6887,7 +6887,7 @@ FailureOr<std::pair<VectorLayout, xla::Array<Value>>> changeTiling(\n \n FailureOr<std::pair<VectorLayout, xla::Array<Value>>> changeImplicitDim(\n     RewriteContext &ctx, OpBuilder &builder, const Location loc, VectorType vty,\n-    const VectorLayout src, xla::Array<Value> vregs,\n+    VectorLayout src, xla::Array<Value> vregs,\n     const VectorLayout::ImplicitDim dst_implicit_dim,\n     const LayoutOffsets dst_offset_hints) {\n   const auto &target_shape = ctx.target_shape;\n@@ -7032,6 +7032,18 @@ FailureOr<std::pair<VectorLayout, xla::Array<Value>>> changeImplicitDim(\n                      src.tiling(), VectorLayout::ImplicitDim::kSecondMinor);\n     return std::make_pair(dst, std::move(dst_vregs));\n   }\n+  if (src.implicit_dim() == VectorLayout::ImplicitDim::kMinor &&\n+      dst_implicit_dim == VectorLayout::ImplicitDim::kNone &&\n+      src.bitwidth() == 32 && src.hasNativeTiling(ctx.target_shape)) {\n+    FAILUREOR_ASSIGN_OR_RETURN(\n+        std::tie(src, vregs),\n+        changeImplicitDim(ctx, builder, loc, vty, src, std::move(vregs),\n+                          VectorLayout::ImplicitDim::kSecondMinor,\n+                          dst_offset_hints));\n+    return changeImplicitDim(ctx, builder, loc, vty, src, std::move(vregs),\n+                             VectorLayout::ImplicitDim::kNone,\n+                             dst_offset_hints);\n+  }\n   return emitError(loc,\n                    \"Not implemented: Unsupported implicit dim change: from \")\n          << src << \" to \" << dst_implicit_dim;\n\n```"
        ],
        "from_id": [
            "tlongeri",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28904 from hawkinsp:postrelease\n\nPiperOrigin-RevId: 761690584",
        "output": [
            "```diff\nCommit: ba8120d28a76ee2e5d687eb500aeab57a420fcdc\nDate: 2025-05-21T22:25:01Z\nURL: https://github.com/jax-ml/jax/commit/ba8120d28a76ee2e5d687eb500aeab57a420fcdc\nFiles changed: 4\nAdditions: +9, Deletions: -5\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 9fd4e50304d0..1e866fae6af5 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -16,6 +16,8 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n \n ## Unreleased\n \n+## JAX 0.6.1 (May 21, 2025)\n+\n * New features:\n   * Added {func}`jax.lax.axis_size` which returns the size of the mapped axis\n     given its name.\ndiff --git a/jax/experimental/jax2tf/tests/jax2tf_test.py b/jax/experimental/jax2tf/tests/jax2tf_test.py\nindex db608adc3dde..ece88841fdc5 100644\n--- a/jax/experimental/jax2tf/tests/jax2tf_test.py\n+++ b/jax/experimental/jax2tf/tests/jax2tf_test.py\n@@ -48,6 +48,7 @@\n config.parse_flags_with_absl()\n \n \n+@unittest.skip(\"Failing after jax 0.6.1 release\")\n class Jax2TfTest(tf_test_util.JaxToTfTestCase):\n \n   def setUp(self):\n@@ -1782,6 +1783,7 @@ def func():\n     jax_result = func()\n     self.assertEqual(tf_result, jax_result)\n \n+@unittest.skip(\"Failing after jax 0.6.1 release\")\n class Jax2TfVersioningTest(tf_test_util.JaxToTfTestCase):\n   # Use a separate test case with the default jax_serialization_version\n   def setUp(self):\ndiff --git a/jax/version.py b/jax/version.py\nindex 9301848b0cfb..e15af7ab50fc 100644\n--- a/jax/version.py\n+++ b/jax/version.py\n@@ -21,7 +21,7 @@\n import pathlib\n import subprocess\n \n-_version = \"0.6.1\"\n+_version = \"0.6.2\"\n # The following line is overwritten by build scripts in distributions &\n # releases. Do not modify this manually, or jax/jaxlib build will fail.\n _release_version: str | None = None\n@@ -152,7 +152,7 @@ def make_release_tree(self, base_dir, files):\n \n \n __version__ = _get_version_string()\n-_minimum_jaxlib_version = \"0.6.0\"\n+_minimum_jaxlib_version = \"0.6.1\"\n \n def _version_as_tuple(version_str):\n   return tuple(int(i) for i in version_str.split(\".\") if i.isdigit())\ndiff --git a/setup.py b/setup.py\nindex 823354adb70d..4c5c86f588c3 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -19,11 +19,11 @@\n \n project_name = 'jax'\n \n-_current_jaxlib_version = '0.6.0'\n+_current_jaxlib_version = '0.6.1'\n # The following should be updated after each new jaxlib release.\n-_latest_jaxlib_version_on_pypi = '0.6.0'\n+_latest_jaxlib_version_on_pypi = '0.6.1'\n \n-_libtpu_version = '0.0.13.*'\n+_libtpu_version = '0.0.15.*'\n \n def load_version_module(pkg_path):\n   spec = importlib.util.spec_from_file_location(\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move _src/stages.py to its own build target\n\nCreating smaller build rules enforces better organized dependency graphs in the JAX project, helps pytype propagate annotations correctly, and leads to improved build and iteration times.\n\nThis refactor required moving the definitions of a few private utilities from pjit and pxla, because these files are part of the larger jax build target.\n\nPiperOrigin-RevId: 761689391",
        "output": [
            "```diff\nCommit: a1d28dc2df6c8545f63b93466ef54442e56bd00b\nDate: 2025-05-21T22:22:03Z\nURL: https://github.com/jax-ml/jax/commit/a1d28dc2df6c8545f63b93466ef54442e56bd00b\nFiles changed: 5\nAdditions: +150, Deletions: -132\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 18e670e0269c..4218cd3f0a77 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -326,7 +326,6 @@ py_library_providing_imports_info(\n         \"_src/shard_alike.py\",\n         \"_src/shard_map.py\",\n         \"_src/sourcemap.py\",\n-        \"_src/stages.py\",\n         \"_src/tree.py\",\n     ] + glob(\n         [\n@@ -415,6 +414,7 @@ py_library_providing_imports_info(\n         \":sharding_impls\",\n         \":sharding_specs\",\n         \":source_info_util\",\n+        \":stages\",\n         \":traceback_util\",\n         \":tree_util\",\n         \":typing\",\n@@ -1001,6 +1001,25 @@ pytype_strict_library(\n     ],\n )\n \n+pytype_strict_library(\n+    name = \"stages\",\n+    srcs = [\"_src/stages.py\"],\n+    deps = [\n+        \":config\",\n+        \":core\",\n+        \":layout\",\n+        \":mlir\",\n+        \":sharding\",\n+        \":sharding_impls\",\n+        \":source_info_util\",\n+        \":traceback_util\",\n+        \":tree_util\",\n+        \":typing\",\n+        \":util\",\n+        \"//jax/_src/lib\",\n+    ],\n+)\n+\n pytype_strict_library(\n     name = \"compute_on\",\n     srcs = [\"_src/compute_on.py\"],\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex 8f553ea884d7..9a11ffa104a8 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -24,7 +24,7 @@\n import logging\n import threading\n import time\n-from typing import Any, Callable, NamedTuple\n+from typing import Any, Callable\n \n import jax\n from jax._src import api\n@@ -34,7 +34,6 @@\n from jax._src import core\n from jax._src import dtypes\n from jax._src import lib\n-from jax._src import source_info_util\n from jax._src import traceback_util\n from jax._src import util\n from jax._src.abstract_arrays import array_types\n@@ -52,6 +51,7 @@\n from jax._src.sharding_impls import (\n     NamedSharding, SingleDeviceSharding, TransferToMemoryKind, GSPMDSharding,\n     is_single_device_sharding)\n+from jax._src.stages import SourceInfo\n import numpy as np\n \n \n@@ -240,11 +240,6 @@ def jaxpr_has_prim_requiring_devices(jaxpr: core.Jaxpr) -> bool:\n   return False\n \n \n-class SourceInfo(NamedTuple):\n-  source_info: source_info_util.SourceInfo\n-  eqn_name: str\n-\n-\n @util.weakref_lru_cache\n def get_intermediate_shardings(\n     jaxpr: core.Jaxpr) -> Sequence[tuple[Sharding, SourceInfo]]:\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex 4a21fae59e52..d0a22cd784b4 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -15,7 +15,6 @@\n \n from __future__ import annotations\n \n-import enum\n import collections\n from collections import namedtuple\n from collections.abc import Callable, Sequence, Iterable\n@@ -1660,67 +1659,10 @@ def check_if_any_auto(\n       return True\n   return False\n \n-class MismatchType(enum.Enum):\n-  ARG_SHARDING = 0\n-  OUT_SHARDING = 1\n-  SHARDING_INSIDE_COMPUTATION = 2\n-  CONTEXT_DEVICES = 3\n-  IN_SHARDING = 4\n-\n-  def __str__(self):\n-    if self.name == 'IN_SHARDING':\n-      return 'explicit input sharding'\n-    elif self.name == 'OUT_SHARDING':\n-      return 'explicit output sharding'\n-    elif self.name == 'CONTEXT_DEVICES':\n-      return 'context mesh'\n-    return f'{self.name}'\n-\n-\n-@dataclasses.dataclass\n-class DeviceAssignmentMismatch:\n-  da: Sequence[xc.Device]\n-  m_type: MismatchType\n-  source_info: dispatch.SourceInfo | None\n-\n-  @property\n-  def device_ids(self) -> Sequence[int]:\n-    return [d.id for d in self.da]\n-\n-  @property\n-  def platform(self) -> str:\n-    return self.da[0].platform.upper()\n-\n-  def _maybe_api_name(self, api_name) -> str:\n-    return f\" {api_name}'s\" if self.m_type == MismatchType.CONTEXT_DEVICES else \"\"\n-\n-  @property\n-  def source_info_str(self):\n-    return (\n-        \"\" if self.source_info is None\n-        else f\" at {source_info_util.summarize(self.source_info.source_info)}\"\n-    )\n-\n-  @property\n-  def _dev_ids_plat_str(self):\n-    return f\"device ids {self.device_ids} on platform {self.platform}\"\n-\n-  def m_type_str(self, api_name):\n-    return (f'{self.source_info and self.source_info.eqn_name} inside {api_name}'\n-            if self.m_type == MismatchType.SHARDING_INSIDE_COMPUTATION else self.m_type)\n-\n-  def _str(self, api_name):\n-    return (f\"{self._maybe_api_name(api_name)} {self.m_type_str(api_name)} with \"\n-            f\"{self._dev_ids_plat_str}{self.source_info_str}\")\n-\n-\n-class DeviceAssignmentMismatchError(Exception):\n-  pass\n-\n \n ShardingInfo = tuple[\n     Union[JSharding, UnspecifiedValue, AUTO],\n-    MismatchType,\n+    stages.MismatchType,\n     Union[Any, None],  # Any is dispatch.SourceInfo to avoid circular imports\n ]\n \n@@ -1752,14 +1694,14 @@ def _get_and_check_device_assignment(\n                              else sh._device_assignment)\n     if not devices:\n       if first_sharding_info[0] != arr_device_assignment:\n-        raise DeviceAssignmentMismatchError([\n-            DeviceAssignmentMismatch(*first_sharding_info),\n-            DeviceAssignmentMismatch(arr_device_assignment, s_type, source_info)])\n+        raise stages.DeviceAssignmentMismatchError([\n+            stages.DeviceAssignmentMismatch(*first_sharding_info),\n+            stages.DeviceAssignmentMismatch(arr_device_assignment, s_type, source_info)])\n     else:\n       if devices != arr_device_assignment:\n-        raise DeviceAssignmentMismatchError([\n-            DeviceAssignmentMismatch(devices, MismatchType.CONTEXT_DEVICES, None),\n-            DeviceAssignmentMismatch(arr_device_assignment, s_type, source_info)])\n+        raise stages.DeviceAssignmentMismatchError([\n+            stages.DeviceAssignmentMismatch(devices, stages.MismatchType.CONTEXT_DEVICES, None),\n+            stages.DeviceAssignmentMismatch(arr_device_assignment, s_type, source_info)])\n   if first_sharding_info is None and devices:\n     final_device_assignment = devices\n   elif first_sharding_info is None:\n@@ -2283,9 +2225,9 @@ def lower_sharding_computation(\n   unique_out_shardings = util.stable_unique(out_shardings)\n   backend, device_assignment = _get_and_check_device_assignment(\n       it.chain(\n-          ((i, MismatchType.ARG_SHARDING, None) for i in unique_in_shardings),\n-          ((o, MismatchType.OUT_SHARDING, None) for o in unique_out_shardings),\n-          ((js, MismatchType.SHARDING_INSIDE_COMPUTATION, source_info)\n+          ((i, stages.MismatchType.ARG_SHARDING, None) for i in unique_in_shardings),\n+          ((o, stages.MismatchType.OUT_SHARDING, None) for o in unique_out_shardings),\n+          ((js, stages.MismatchType.SHARDING_INSIDE_COMPUTATION, source_info)\n            for js, source_info in unique_intermediate_shardings)),\n       devices_from_context)\n   unique_intermediate_shardings = [js for js, _ in unique_intermediate_shardings]\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex de01f4c05983..0503e58b2e45 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -96,48 +96,6 @@\n logger = logging.getLogger(__name__)\n \n \n-def _find_arg_mismatch(arg_list, fails, fun_name):\n-  mismatched_args_msg = []\n-  def mismatch(err):\n-    for name, inp_da, aval in arg_list:\n-      if err.m_type == pxla.MismatchType.ARG_SHARDING and err.da == inp_da:\n-        mismatched_args_msg.append(\n-            f\"argument {name} of {fun_name} with shape {aval.str_short()} and \"\n-            f\"{err._dev_ids_plat_str}\")\n-        break\n-  first_err, second_err = fails\n-  mismatch(first_err)\n-  mismatch(second_err)\n-  return mismatched_args_msg\n-\n-\n-def _device_assignment_mismatch_error(fun_name, fails, args_flat, api_name,\n-                                      arg_names):\n-  arg_list = []\n-  if arg_names is None:\n-    arg_names = [''] * len(args_flat)\n-  for a, n in zip(args_flat, arg_names):\n-    da = (a.sharding._device_assignment\n-          if getattr(a, 'sharding', None) is not None else None)\n-    arg_list.append((n, da, core.shaped_abstractify(a)))\n-\n-  mismatched_args_msg = _find_arg_mismatch(arg_list, fails, fun_name)\n-\n-  if len(mismatched_args_msg) == 2:\n-    first, second = mismatched_args_msg  # pytype: disable=bad-unpacking\n-    extra_msg = f\" Got {first} and {second}\"\n-  elif len(mismatched_args_msg) == 1:\n-    first, second  = fails\n-    # Choose the failure left which is not already covered by ARG_SHARDING.\n-    left = second if first.m_type == pxla.MismatchType.ARG_SHARDING else first\n-    extra_msg = f\" Got {mismatched_args_msg[0]} and{left._str(api_name)}\"\n-  else:\n-    first, second = fails\n-    extra_msg = f\" Got{first._str(api_name)} and{second._str(api_name)}\"\n-  msg = (f\"Received incompatible devices for {api_name}ted computation.{extra_msg}\")\n-  return msg\n-\n-\n class PjitInfo(NamedTuple):\n   \"\"\"Things that we know about a jit instance before it is called.\n \n@@ -197,10 +155,10 @@ def _python_pjit_helper(fun: Callable, jit_info: PjitInfo, *args, **kwargs):\n       out_flat = pjit_p.bind(*args_flat, **p.params)\n       compiled = None\n       profiler = None\n-  except pxla.DeviceAssignmentMismatchError as e:\n+  except stages.DeviceAssignmentMismatchError as e:\n     fails, = e.args\n     fun_name = getattr(fun, '__qualname__', getattr(fun, '__name__', str(fun)))\n-    msg = _device_assignment_mismatch_error(\n+    msg = stages._device_assignment_mismatch_error(\n         fun_name, fails, args_flat, 'jit', p.arg_names)\n     raise ValueError(msg) from None\n   except xla.InvalidInputException as e:\n@@ -1740,7 +1698,7 @@ def _resolve_in_shardings(args, pjit_in_shardings: Sequence[PjitSharding]\n     if isinstance(arg_s, PmapSharding):\n       continue\n     if getattr(a, '_committed', True):\n-      committed_arg_shardings.append((arg_s, pxla.MismatchType.ARG_SHARDING, None))\n+      committed_arg_shardings.append((arg_s, stages.MismatchType.ARG_SHARDING, None))\n \n   resolved_in_shardings: list[PjitSharding] = []\n   for arg, pjit_in_s in zip(args, pjit_in_shardings):\ndiff --git a/jax/_src/stages.py b/jax/_src/stages.py\nindex 3c5d710f3bdc..d92d1ccb2aa3 100644\n--- a/jax/_src/stages.py\n+++ b/jax/_src/stages.py\n@@ -30,18 +30,20 @@\n \"\"\"\n from __future__ import annotations\n \n+import dataclasses\n+import enum\n import functools\n from collections.abc import Sequence\n from dataclasses import dataclass\n from typing import Any, NamedTuple, Protocol, Union, runtime_checkable\n \n-import jax\n-\n from jax._src import core\n from jax._src import config\n+from jax._src import sharding as sharding_lib\n from jax._src import source_info_util\n from jax._src import traceback_util\n from jax._src import tree_util\n+from jax._src import typing\n from jax._src import util\n from jax._src.sharding_impls import UnspecifiedValue, AUTO\n from jax._src.layout import Layout\n@@ -79,7 +81,7 @@ def create_cpp_call(self, no_kwargs, in_tree, out_tree) -> Any:\n     \"\"\"Optionally constructs a fast c++ dispatcher.\"\"\"\n     return None\n \n-  def input_shardings(self) -> Sequence[jax.sharding.Sharding]:\n+  def input_shardings(self) -> Sequence[sharding_lib.Sharding]:\n     \"\"\"Flat sequence of input shardings.\n \n     May raise ``NotImplementedError`` if unavailable, e.g. based on backend,\n@@ -88,7 +90,7 @@ def input_shardings(self) -> Sequence[jax.sharding.Sharding]:\n     raise NotImplementedError(\n         \"compiled executable carries no input sharding information\")\n \n-  def output_shardings(self) -> Sequence[jax.sharding.Sharding]:\n+  def output_shardings(self) -> Sequence[sharding_lib.Sharding]:\n     \"\"\"Flat sequence of output shardings.\n \n     May raise ``NotImplementedError`` if unavailable, e.g. based on backend,\n@@ -310,8 +312,8 @@ def dtype(self):\n @dataclass(frozen=True)\n class OutInfo:\n   shape: tuple[int, ...]\n-  dtype: jax.typing.DTypeLike\n-  sharding: jax.sharding.Sharding | None = None\n+  dtype: typing.DTypeLike\n+  sharding: sharding_lib.Sharding | None = None\n \n \n class Stage:\n@@ -689,9 +691,6 @@ def out_info(self):\n   def lower(self, *, lowering_platforms: tuple[str, ...] | None = None,\n             _private_parameters: mlir.LoweringParameters | None = None):\n     \"\"\"Lower to compiler input, returning a ``Lowered`` instance.\"\"\"\n-    from jax._src.interpreters import pxla\n-    from jax._src import pjit\n-\n     if _private_parameters is None:\n       _private_parameters = mlir.LoweringParameters()\n     new_callable = functools.partial(\n@@ -699,9 +698,9 @@ def lower(self, *, lowering_platforms: tuple[str, ...] | None = None,\n         lowering_parameters=_private_parameters)\n     try:\n       lowering = new_callable()\n-    except pxla.DeviceAssignmentMismatchError as e:\n+    except DeviceAssignmentMismatchError as e:\n       fails, = e.args\n-      msg = pjit._device_assignment_mismatch_error(\n+      msg = _device_assignment_mismatch_error(\n           self.fun_name, fails, self._args_flat, 'jit', self._arg_names)\n       raise ValueError(msg) from None\n     return Lowered(lowering, self.args_info, self._out_tree)\n@@ -745,3 +744,108 @@ def lower(self, *args, **kwargs) -> Lowered:\n       A ``Lowered`` instance representing the lowering.\n     \"\"\"\n     raise NotImplementedError\n+\n+\n+class MismatchType(enum.Enum):\n+  ARG_SHARDING = 0\n+  OUT_SHARDING = 1\n+  SHARDING_INSIDE_COMPUTATION = 2\n+  CONTEXT_DEVICES = 3\n+  IN_SHARDING = 4\n+\n+  def __str__(self):\n+    if self.name == 'IN_SHARDING':\n+      return 'explicit input sharding'\n+    elif self.name == 'OUT_SHARDING':\n+      return 'explicit output sharding'\n+    elif self.name == 'CONTEXT_DEVICES':\n+      return 'context mesh'\n+    return f'{self.name}'\n+\n+\n+class SourceInfo(NamedTuple):\n+  source_info: source_info_util.SourceInfo\n+  eqn_name: str\n+\n+\n+@dataclasses.dataclass\n+class DeviceAssignmentMismatch:\n+  da: Sequence[xc.Device]\n+  m_type: MismatchType\n+  source_info: SourceInfo | None\n+\n+  @property\n+  def device_ids(self) -> Sequence[int]:\n+    return [d.id for d in self.da]\n+\n+  @property\n+  def platform(self) -> str:\n+    return self.da[0].platform.upper()\n+\n+  def _maybe_api_name(self, api_name) -> str:\n+    return f\" {api_name}'s\" if self.m_type == MismatchType.CONTEXT_DEVICES else \"\"\n+\n+  @property\n+  def source_info_str(self):\n+    return (\n+        \"\" if self.source_info is None\n+        else f\" at {source_info_util.summarize(self.source_info.source_info)}\"\n+    )\n+\n+  @property\n+  def _dev_ids_plat_str(self):\n+    return f\"device ids {self.device_ids} on platform {self.platform}\"\n+\n+  def m_type_str(self, api_name):\n+    return (f'{self.source_info and self.source_info.eqn_name} inside {api_name}'\n+            if self.m_type == MismatchType.SHARDING_INSIDE_COMPUTATION else self.m_type)\n+\n+  def _str(self, api_name):\n+    return (f\"{self._maybe_api_name(api_name)} {self.m_type_str(api_name)} with \"\n+            f\"{self._dev_ids_plat_str}{self.source_info_str}\")\n+\n+\n+class DeviceAssignmentMismatchError(Exception):\n+  pass\n+\n+\n+def _find_arg_mismatch(arg_list, fails, fun_name):\n+  mismatched_args_msg = []\n+  def mismatch(err):\n+    for name, inp_da, aval in arg_list:\n+      if err.m_type == MismatchType.ARG_SHARDING and err.da == inp_da:\n+        mismatched_args_msg.append(\n+            f\"argument {name} of {fun_name} with shape {aval.str_short()} and \"\n+            f\"{err._dev_ids_plat_str}\")\n+        break\n+  first_err, second_err = fails\n+  mismatch(first_err)\n+  mismatch(second_err)\n+  return mismatched_args_msg\n+\n+\n+def _device_assignment_mismatch_error(fun_name, fails, args_flat, api_name,\n+                                      arg_names):\n+  arg_list = []\n+  if arg_names is None:\n+    arg_names = [''] * len(args_flat)\n+  for a, n in zip(args_flat, arg_names):\n+    da = (a.sharding._device_assignment\n+          if getattr(a, 'sharding', None) is not None else None)\n+    arg_list.append((n, da, core.shaped_abstractify(a)))\n+\n+  mismatched_args_msg = _find_arg_mismatch(arg_list, fails, fun_name)\n+\n+  if len(mismatched_args_msg) == 2:\n+    first, second = mismatched_args_msg  # pytype: disable=bad-unpacking\n+    extra_msg = f\" Got {first} and {second}\"\n+  elif len(mismatched_args_msg) == 1:\n+    first, second  = fails\n+    # Choose the failure left which is not already covered by ARG_SHARDING.\n+    left = second if first.m_type == MismatchType.ARG_SHARDING else first\n+    extra_msg = f\" Got {mismatched_args_msg[0]} and{left._str(api_name)}\"\n+  else:\n+    first, second = fails\n+    extra_msg = f\" Got{first._str(api_name)} and{second._str(api_name)}\"\n+  msg = (f\"Received incompatible devices for {api_name}ted computation.{extra_msg}\")\n+  return msg\n\n```"
        ],
        "from_id": [
            "vanderplas@google.com",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update version numbers after 0.6.1 release.",
        "output": [
            "```diff\nCommit: 7e9c7e69427628bce16a39701710e3d781b18468\nDate: 2025-05-21T21:59:50Z\nURL: https://github.com/jax-ml/jax/commit/7e9c7e69427628bce16a39701710e3d781b18468\nFiles changed: 4\nAdditions: +7, Deletions: -3\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 939177e01311..1e866fae6af5 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -14,7 +14,9 @@ Remember to align the itemized text with the first line of an item within a list\n When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.md.\n -->\n \n-## JAX 0.6.1\n+## Unreleased\n+\n+## JAX 0.6.1 (May 21, 2025)\n \n * New features:\n   * Added {func}`jax.lax.axis_size` which returns the size of the mapped axis\ndiff --git a/jax/experimental/jax2tf/tests/jax2tf_test.py b/jax/experimental/jax2tf/tests/jax2tf_test.py\nindex db608adc3dde..ece88841fdc5 100644\n--- a/jax/experimental/jax2tf/tests/jax2tf_test.py\n+++ b/jax/experimental/jax2tf/tests/jax2tf_test.py\n@@ -48,6 +48,7 @@\n config.parse_flags_with_absl()\n \n \n+@unittest.skip(\"Failing after jax 0.6.1 release\")\n class Jax2TfTest(tf_test_util.JaxToTfTestCase):\n \n   def setUp(self):\n@@ -1782,6 +1783,7 @@ def func():\n     jax_result = func()\n     self.assertEqual(tf_result, jax_result)\n \n+@unittest.skip(\"Failing after jax 0.6.1 release\")\n class Jax2TfVersioningTest(tf_test_util.JaxToTfTestCase):\n   # Use a separate test case with the default jax_serialization_version\n   def setUp(self):\ndiff --git a/jax/version.py b/jax/version.py\nindex acbfb7577e49..e15af7ab50fc 100644\n--- a/jax/version.py\n+++ b/jax/version.py\n@@ -21,7 +21,7 @@\n import pathlib\n import subprocess\n \n-_version = \"0.6.1\"\n+_version = \"0.6.2\"\n # The following line is overwritten by build scripts in distributions &\n # releases. Do not modify this manually, or jax/jaxlib build will fail.\n _release_version: str | None = None\ndiff --git a/setup.py b/setup.py\nindex 85ba1fdb4e96..4c5c86f588c3 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -21,7 +21,7 @@\n \n _current_jaxlib_version = '0.6.1'\n # The following should be updated after each new jaxlib release.\n-_latest_jaxlib_version_on_pypi = '0.6.0'\n+_latest_jaxlib_version_on_pypi = '0.6.1'\n \n _libtpu_version = '0.0.15.*'\n \n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "[Mosaic:TPU][Relayout] Support minor to 2nd minor implicit dimension for unpacked types and native tiling on TPUv5\n\nPiperOrigin-RevId: 761676578",
        "output": [
            "```diff\nCommit: 2e070cade219ddb0033ec3ca2438bd8f74853218\nDate: 2025-05-21T21:45:27Z\nURL: https://github.com/jax-ml/jax/commit/2e070cade219ddb0033ec3ca2438bd8f74853218\nFiles changed: 1\nAdditions: +128, Deletions: -0\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 4200551ff450..4d1323d68057 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -441,6 +441,121 @@ FailureOr<Value> maskOOB(RewriteContext &ctx, ImplicitLocOpBuilder &builder,\n       .getResult();\n }\n \n+// Transpose the 2nd minor dimension of the implicit shape.\n+//\n+// Shape of (..., N, 1) becomes (..., 1, N)\n+FailureOr<xla::Array<Value>> transposeSingletonMinorDimension(\n+    RewriteContext &ctx, OpBuilder &builder, const Location loc,\n+    xla::Array<Value> vregs, const ArrayRef<int64_t> ishape,\n+    VectorLayout layout, const int64_t new_minor_offset) {\n+  if (layout.bitwidth() != 32 || !layout.hasNativeTiling(ctx.target_shape)) {\n+    // Note: For non-native tilings it is probably better to retile first, to\n+    //       to make the most out of each lane rotate (they are expensive).\n+    return emitError(loc, \"Not implemented: Unsupported bitwidth or tiling\");\n+  }\n+  auto create_index_const = [&](const int64_t idx) {\n+    return builder.create<arith::ConstantIndexOp>(loc, idx);\n+  };\n+  auto create_i32_vreg_const = [&](const int64_t val) {\n+    return I32Const(val, ctx.target_shape, builder, loc);\n+  };\n+  if (layout.offsets()[1].has_value()) {\n+    // Replicate minor dimension\n+    // TODO(tlongeri): Move into its own function (it will be needed for\n+    // relayout) and make this a precondition of this function, so that we have\n+    // \"building block\" functions with minimal overlap\n+    vregs.Each([&](const absl::Span<const int64_t> idxs, Value *vreg) {\n+      *vreg = builder.create<tpu::DynamicGatherOp>(\n+          loc, vreg->getType(), *vreg,\n+          create_i32_vreg_const(*layout.offsets()[1]), 1);\n+    });\n+    layout =\n+        VectorLayout(layout.bitwidth(), {layout.offsets()[0], std::nullopt},\n+                     layout.tiling(), VectorLayout::ImplicitDim::kNone);\n+  }\n+  if (!layout.offsets()[0].has_value()) {\n+    return vregs;\n+  }\n+  const int64_t old_2nd_minor_offset = *layout.offsets()[0];\n+  SmallVector<int64_t> new_ishape(ishape);\n+  CHECK_EQ(new_ishape.back(), 1);\n+  std::iter_swap(new_ishape.end() - 2, new_ishape.end() - 1);\n+  // new_layout is only to get the new vreg array shape, the implicit dim is\n+  // irrelevant (since we already have the implicit shape):\n+  const VectorLayout new_layout(\n+      layout.bitwidth(), {std::nullopt, new_minor_offset}, layout.tiling(),\n+      VectorLayout::ImplicitDim::kNone);\n+  xla::Array<Value> new_vregs(new_layout.tileArrayShape(\n+      /*src_is_implicit=*/true, /*res_is_implicit=*/true, new_ishape,\n+      ctx.target_shape));\n+  VectorType iota_vreg_ty =\n+      getNativeVregType(builder.getI32Type(), ctx.target_shape);\n+  // Preallocate an indices vector to avoid repeated allocations:\n+  SmallVector<int64_t> old_idxs;\n+  new_vregs.Each([&](const absl::Span<const int64_t> new_idxs,\n+                     Value *new_vreg) {\n+    const int64_t uncorrected_shape_start =\n+        ctx.target_shape[1] * new_idxs.back() - new_minor_offset;\n+    // The start and end of the data contained by new_vreg in the implicit shape\n+    const int64_t shape_start = std::max<int64_t>(uncorrected_shape_start, 0);\n+    const int64_t shape_end = std::min(\n+        uncorrected_shape_start + ctx.target_shape[1], new_ishape.back());\n+    old_idxs.assign(new_idxs.begin(), new_idxs.end());\n+    CHECK_EQ(*(old_idxs.end() - 2), 0);\n+    old_idxs.back() = 0;\n+    *new_vreg = nullptr;\n+    VectorType vmask_ty =\n+        getNativeVregOrVmaskType(builder.getI1Type(), 32, ctx.target_shape);\n+    int64_t shape_offset = shape_start;\n+    // The data in the new vreg is composed of data from multiple of the old\n+    // vregs, so iterate over them until the new vreg is full\n+    while (shape_offset < shape_end) {\n+      // Find the vreg that contains the data at shape_offset\n+      *(old_idxs.end() - 2) =\n+          (shape_offset + old_2nd_minor_offset) / ctx.target_shape[0];\n+      const int64_t old_sublane_offset =\n+          (shape_offset + old_2nd_minor_offset) % ctx.target_shape[0];\n+      const int64_t new_lane_offset =\n+          (shape_offset + new_minor_offset) % ctx.target_shape[1];\n+      // We will blend in all the relevant data contained by the old vreg\n+      const int64_t data_size =\n+          std::min(ctx.target_shape[0] - old_sublane_offset,\n+                   ctx.target_shape[1] - new_lane_offset);\n+      // [ a a a a a a a a ]    [ . . a b c . . . ]\n+      // [ b b b b b b b b ] => [ . . a b c . . . ]\n+      // [ c c c c c c c c ]    [ . . a b c . . . ]\n+      // [ . . . . . . . . ]    [ . . a b c . . . ]\n+      // Every lane has all the data, so at each sublane we can just pick out\n+      // the element that we want using a sublane shuffle.\n+      Value vreg = vregs(old_idxs);\n+      Value iota_vreg = builder.create<tpu::IotaOp>(\n+          loc, iota_vreg_ty,\n+          /*dimension =*/builder.getI32IntegerAttr(1));\n+      iota_vreg = builder.create<arith::AddIOp>(\n+          loc, iota_vreg,\n+          create_i32_vreg_const(old_sublane_offset - new_lane_offset));\n+      vreg = builder.create<tpu::DynamicGatherOp>(loc, vreg.getType(), vreg,\n+                                                  iota_vreg, 0);\n+      // Now, blend the transposed data into new_vreg\n+      if (*new_vreg == nullptr) {\n+        *new_vreg = vreg;\n+      } else {\n+        Value mask = builder.create<tpu::CreateMaskOp>(\n+            loc, vmask_ty,\n+            ArrayRef<Value>{create_index_const(0),\n+                            create_index_const(new_lane_offset)},\n+            ArrayRef<Value>{create_index_const(ctx.target_shape[0]),\n+                            create_index_const(new_lane_offset + data_size)});\n+        *new_vreg = builder.create<arith::SelectOp>(loc, mask, vreg, *new_vreg);\n+      }\n+      shape_offset += data_size;\n+      ++*(old_idxs.end() - 2);\n+    }\n+    CHECK(*new_vreg != nullptr);\n+  });\n+  return new_vregs;\n+}\n+\n // Insert a minor dimension to the implicit shape. The original minor dimension\n // becomes the new second minor dimension, laid out across sublanes.\n //\n@@ -6904,6 +7019,19 @@ FailureOr<std::pair<VectorLayout, xla::Array<Value>>> changeImplicitDim(\n                                      dst.offsets()));\n     return std::make_pair(dst, std::move(dst_vregs));\n   }\n+  if (src.implicit_dim() == VectorLayout::ImplicitDim::kMinor &&\n+      dst_implicit_dim == VectorLayout::ImplicitDim::kSecondMinor &&\n+      src.bitwidth() == 32 && src.hasNativeTiling(ctx.target_shape)) {\n+    const int64_t dst_minor_offset = dst_offset_hints[1].value_or(0);\n+    FAILUREOR_ASSIGN_OR_RETURN(\n+        xla::Array<Value> dst_vregs,\n+        transposeSingletonMinorDimension(ctx, builder, loc, vregs,\n+                                         src.implicitShape(vty.getShape()), src,\n+                                         dst_minor_offset));\n+    VectorLayout dst(src.bitwidth(), {std::nullopt, dst_minor_offset},\n+                     src.tiling(), VectorLayout::ImplicitDim::kSecondMinor);\n+    return std::make_pair(dst, std::move(dst_vregs));\n+  }\n   return emitError(loc,\n                    \"Not implemented: Unsupported implicit dim change: from \")\n          << src << \" to \" << dst_implicit_dim;\n\n```"
        ],
        "from_id": [
            "tlongeri",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge branch 'release/0.6.1'",
        "output": [
            "```diff\nCommit: 880d31d6b105e1deb470b60690bbf5c8a86c7d06\nDate: 2025-05-21T21:22:49Z\nURL: https://github.com/jax-ml/jax/commit/880d31d6b105e1deb470b60690bbf5c8a86c7d06\nFiles changed: 3\nAdditions: +4, Deletions: -4\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 9fd4e50304d0..939177e01311 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -14,7 +14,7 @@ Remember to align the itemized text with the first line of an item within a list\n When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.md.\n -->\n \n-## Unreleased\n+## JAX 0.6.1\n \n * New features:\n   * Added {func}`jax.lax.axis_size` which returns the size of the mapped axis\ndiff --git a/jax/version.py b/jax/version.py\nindex 9301848b0cfb..acbfb7577e49 100644\n--- a/jax/version.py\n+++ b/jax/version.py\n@@ -152,7 +152,7 @@ def make_release_tree(self, base_dir, files):\n \n \n __version__ = _get_version_string()\n-_minimum_jaxlib_version = \"0.6.0\"\n+_minimum_jaxlib_version = \"0.6.1\"\n \n def _version_as_tuple(version_str):\n   return tuple(int(i) for i in version_str.split(\".\") if i.isdigit())\ndiff --git a/setup.py b/setup.py\nindex 823354adb70d..85ba1fdb4e96 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -19,11 +19,11 @@\n \n project_name = 'jax'\n \n-_current_jaxlib_version = '0.6.0'\n+_current_jaxlib_version = '0.6.1'\n # The following should be updated after each new jaxlib release.\n _latest_jaxlib_version_on_pypi = '0.6.0'\n \n-_libtpu_version = '0.0.13.*'\n+_libtpu_version = '0.0.15.*'\n \n def load_version_module(pkg_path):\n   spec = importlib.util.spec_from_file_location(\n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "[ragged-paged-attn] Use select for initialization in flash attention.\n\nPiperOrigin-RevId: 761612390",
        "output": [
            "```diff\nCommit: 08c2a36e280b3380ed715301a0d6e396bccd9310\nDate: 2025-05-21T18:55:14Z\nURL: https://github.com/jax-ml/jax/commit/08c2a36e280b3380ed715301a0d6e396bccd9310\nFiles changed: 1\nAdditions: +8, Deletions: -26\ndiff --git a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\nindex df47674a59a9..d9d952d5a378 100644\n--- a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n+++ b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n@@ -454,6 +454,11 @@ def masked_store(ref, val, start, end, group=1):\n         mask = jnp.logical_and(iota >= start, iota < end)\n         pl.store(ref, idx=tuple(slice(None) for _ in ref.shape), val=val, mask=mask)\n \n+      def load_with_init(ref, init_val):\n+        return jnp.where(\n+            kv_blk_idx == 0, jnp.full_like(ref, init_val), ref[...]\n+        )\n+\n       # kv lens will be contracting dim, we should mask out the NaNs.\n       kv_mask = (\n           lax.broadcasted_iota(jnp.int32, k.shape, 0) < kv_len - kv_len_start\n@@ -468,29 +473,6 @@ def masked_store(ref, val, start, end, group=1):\n       store_start = jnp.maximum(q_start - q_len_start, 0)\n       store_end = jnp.minimum(q_end - q_len_start, num_q_per_blk)\n \n-      @pl.when(kv_blk_idx == 0)\n-      def init_scratch_ref():\n-        masked_store(\n-            head_m_ref,\n-            jnp.full_like(head_m_ref, -jnp.inf),\n-            store_start,\n-            store_end,\n-            num_q_heads_per_kv_head,\n-        )\n-        masked_store(\n-            head_l_ref,\n-            jnp.zeros_like(head_l_ref),\n-            store_start,\n-            store_end,\n-            num_q_heads_per_kv_head,\n-        )\n-        masked_store(\n-            head_acc_ref,\n-            jnp.zeros_like(head_acc_ref),\n-            store_start,\n-            store_end,\n-        )\n-\n       row_ids = (\n           (kv_len - q_len)\n           + q_len_start\n@@ -522,8 +504,8 @@ def init_scratch_ref():\n       l_curr = jnp.broadcast_to(\n           s_curr.sum(axis=1, keepdims=True), lm_store_shape\n       )\n-      m_prev = head_m_ref[...]\n-      l_prev = head_l_ref[...]\n+      m_prev = load_with_init(head_m_ref, -jnp.inf)\n+      l_prev = load_with_init(head_l_ref, 0.0)\n       m_next = jnp.maximum(m_prev, m_curr)\n       masked_store(\n           head_m_ref, m_next, store_start, store_end, num_q_heads_per_kv_head\n@@ -552,7 +534,7 @@ def broadcast_to_shape(arr, shape):\n             [arr for _ in range(shape[1] // arr.shape[1])], axis=1\n         )\n \n-      o_curr = head_acc_ref[...].reshape(-1, head_dim)\n+      o_curr = load_with_init(head_acc_ref, 0.0).reshape(-1, head_dim)\n       l_alpha = broadcast_to_shape(l_alpha, qkv.shape)\n       beta = broadcast_to_shape(beta, qkv.shape)\n       l_next_safe = broadcast_to_shape(l_next_safe, qkv.shape)\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Show exactly what the copy paths/patterns are when copying wheels.\n\nThis will make it easier to track down unexpected path mismatches in the future.\n\nPiperOrigin-RevId: 761584888",
        "output": [
            "```diff\nCommit: 06c323ccc9a73f25b869555cf7b4822bbf35971e\nDate: 2025-05-21T17:48:55Z\nURL: https://github.com/jax-ml/jax/commit/06c323ccc9a73f25b869555cf7b4822bbf35971e\nFiles changed: 1\nAdditions: +5, Deletions: -2\ndiff --git a/build/tools/utils.py b/build/tools/utils.py\nindex 4bf871067501..7ed7f74d07a5 100644\n--- a/build/tools/utils.py\n+++ b/build/tools/utils.py\n@@ -293,9 +293,12 @@ def copy_dir_recursively(src, dst):\n   logging.info(\"Editable wheel path: %s\" % dst)\n \n \n-def copy_individual_files(src, dst, regex):\n+def copy_individual_files(src: str, dst: str, glob_pattern: str):\n   os.makedirs(dst, exist_ok=True)\n-  for f in glob.glob(os.path.join(src, regex)):\n+  logging.debug(\n+    f\"Copying files matching pattern {glob_pattern!r} from {src!r} to {dst!r}\"\n+  )\n+  for f in glob.glob(os.path.join(src, glob_pattern)):\n     dst_file = os.path.join(dst, os.path.basename(f))\n     if os.path.exists(dst_file):\n       os.remove(dst_file)\n\n```"
        ],
        "from_id": [
            "belitskiy",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Add faster implementation for s8->bf16 and s4->bf16 on TPUv6+.\n\nPiperOrigin-RevId: 761578503",
        "output": [
            "```diff\nCommit: 3179e5d84a624f8ac456a2588191f92813ddf2f6\nDate: 2025-05-21T17:35:07Z\nURL: https://github.com/jax-ml/jax/commit/3179e5d84a624f8ac456a2588191f92813ddf2f6\nFiles changed: 4\nAdditions: +61, Deletions: -0\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu.td b/jaxlib/mosaic/dialect/tpu/tpu.td\nindex b6ae1e52e822..505478b9ad72 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu.td\n+++ b/jaxlib/mosaic/dialect/tpu/tpu.td\n@@ -510,6 +510,14 @@ def TPU_FPToSIOp : TPU_Op<\"fptosi\", [Pure, ElementwiseMappable]> {\n   let hasCanonicalizeMethod = 1;\n }\n \n+// Internal operation. All arith.sitofp operations that change the bitwidth\n+// must be canonicalized to this operation.\n+def TPU_SIToFPOp : TPU_Op<\"sitofp\", [Pure, ElementwiseMappable]> {\n+  let arguments = (ins AnyVectorOfAnyRank:$in, TPU_RoundingModeEnum:$rounding_mode);\n+  let results = (outs AnyVectorOfAnyRank:$output);\n+  let assemblyFormat = [{ $in attr-dict `:` type($in) `->` type($output) }];\n+}\n+\n def TPU_DotDimensionNumbersAttr : TPU_Attr<\"DotDimensionNumbers\", \"dot_dimension_numbers\"> {\n   let parameters = (ins\n     ArrayRefParameter<\"int64_t\", \"\">:$lhs_contracting_dims,\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex fa14c8ef9238..4200551ff450 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -1027,6 +1027,40 @@ LogicalResult tpu_fptosi_rule(RewriteContext &ctx, Operation &op,\n   return op.emitOpError(\"Unsupported FPToSI conversion\");\n }\n \n+LogicalResult tpu_sitofp_rule(RewriteContext &ctx, Operation &op,\n+                              const ArrayRef<Layout> layouts_in,\n+                              const ArrayRef<Layout> layouts_out) {\n+  TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n+  TPU_ASSERT_OP(layouts_in.front().has_value());\n+  TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n+  TPU_ASSERT_OP(layouts_out.front().has_value());\n+  auto &layout_in = *layouts_in.front();\n+  auto &layout_out = *layouts_out.front();\n+  if (layout_in.bitwidth() == layout_out.bitwidth()) {\n+    return elementwise_op_rule(ctx, op, layouts_in, layouts_out);\n+  } else if (layout_in.bitwidth() < layout_out.bitwidth()) {\n+    auto sitofp_op = cast<tpu::SIToFPOp>(op);\n+    switch (sitofp_op.getRoundingMode()) {\n+      case tpu::RoundingMode::kToNearestEven: {\n+        ImplicitLocOpBuilder builder(op.getLoc(), &op);\n+        FAILUREOR_ASSIGN_OR_RETURN(\n+            xla::Array<Value> vregs,\n+            ext_op_rule_impl(ctx, builder, sitofp_op, layout_in, layout_out));\n+        sitofp_op.replaceAllUsesWith(assemble(builder, sitofp_op.getType(),\n+                                              layout_out, std::move(vregs),\n+                                              ctx.target_shape)\n+                                         .getResult());\n+        sitofp_op.erase();\n+        return success();\n+      }\n+      case tpu::RoundingMode::kTowardsZero:\n+        return op.emitOpError(\n+            \"Not implemented: SIToFP with rounding mode kTowardsZero\");\n+    }\n+  }\n+  return op.emitOpError(\"Unsupported SIToFP conversion\");\n+}\n+\n LogicalResult func_return_rule(RewriteContext &ctx, Operation &op,\n                                const ArrayRef<Layout> layouts_in,\n                                const ArrayRef<Layout> layouts_out) {\n@@ -7164,6 +7198,7 @@ const llvm::StringMap<rule_type> &rules() {\n         {tpu::PRNGRandomBitsOp::getOperationName(), tpu_prng_random_bits_rule},\n         {tpu::RelayoutOp::getOperationName(), tpu_relayout_rule},\n         {tpu::FPToSIOp::getOperationName(), tpu_fptosi_rule},\n+        {tpu::SIToFPOp::getOperationName(), tpu_sitofp_rule},\n         {vector::BroadcastOp::getOperationName(), vector_broadcast_rule},\n         {vector::ExtractOp::getOperationName(), vector_extract_rule},\n         {vector::LoadOp::getOperationName(), vector_load_rule},\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\nindex 368bfc596732..1d8ea1299f04 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n@@ -685,6 +685,18 @@ LogicalResult canonicalize_sitofp(const CanonicalizeContext &ctx,\n   FAILUREOR_ASSIGN_OR_RETURN(const unsigned dst_bitwidth,\n                              getElementTypeBitwidth(op.getType()));\n \n+  // We have low-level optimized code for s8->bf16 and s4->bf16 casts on v6.\n+  if (ctx.hardware_generation >= 6 && is_vector &&\n+      (src_vty.getElementType().isSignlessInteger(8) ||\n+       src_vty.getElementType().isSignlessInteger(4)) &&\n+      dst_vty.getElementType().isBF16()) {\n+    auto new_op = builder.create<tpu::SIToFPOp>(\n+        op.getType(), op.getIn(), tpu::RoundingMode::kToNearestEven);\n+    op.replaceAllUsesWith(new_op.getResult());\n+    op.erase();\n+    return success();\n+  }\n+\n   if ((src_bitwidth < 32 || dst_bitwidth < 32) && !ctx.compatibility_mode) {\n     return op.emitOpError(\n         \"On this target integer-to-float conversions can only happen on \"\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex 9c4a7b4c397d..f01d0b4c5888 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -154,6 +154,12 @@ class VectorLayoutInferer {\n         if (inferExt(&any_op).failed()) {\n           return failure();\n         }\n+      } else if (auto op = dyn_cast<tpu::SIToFPOp>(any_op);\n+                 op && op.getIn().getType().getElementTypeBitWidth() <\n+                           op.getType().getElementTypeBitWidth()) {\n+        if (inferExt(&any_op).failed()) {\n+          return failure();\n+        }\n       } else if (isa<arith::TruncFOp, arith::TruncIOp>(any_op)) {\n         if (inferTrunc(&any_op).failed()) {\n           return failure();\n\n```"
        ],
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Adjust triton dialect lowering rounding mode to allow upcasting fp8 types\n\nFix: https://github.com/jax-ml/jax/issues/28416\nPiperOrigin-RevId: 761577943",
        "output": [
            "```diff\nCommit: f227b13613fe8d1a9c263006b40d31484b24f4c7\nDate: 2025-05-21T17:33:13Z\nURL: https://github.com/jax-ml/jax/commit/f227b13613fe8d1a9c263006b40d31484b24f4c7\nFiles changed: 3\nAdditions: +98, Deletions: -4\ndiff --git a/jax/_src/pallas/triton/lowering.py b/jax/_src/pallas/triton/lowering.py\nindex 2cddb623b33f..bd70dc8d470c 100644\n--- a/jax/_src/pallas/triton/lowering.py\n+++ b/jax/_src/pallas/triton/lowering.py\n@@ -1572,11 +1572,10 @@ def _float_float_cast(src: ir.Value, dst_type: ir.Type) -> ir.Value:\n   src_element_type = ir.FloatType(_element_type(src.type))\n   dst_element_type = ir.FloatType(_element_type(dst_type))\n   if src_element_type.width == 8 or dst_element_type.width == 8:\n-    return tt_dialect.fp_to_fp(\n-        dst_type,\n-        src,\n-        rounding=tt_dialect.RoundingMode.RTNE,\n+    rounding = (\n+        tt_dialect.RoundingMode.RTNE if src_element_type.width > 8 else None\n     )\n+    return tt_dialect.fp_to_fp(dst_type, src, rounding=rounding)\n   if src_element_type.width > dst_element_type.width:\n     return arith_dialect.truncf(dst_type, src)\n   elif src_element_type.width < dst_element_type.width:\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 6690fb2dac62..d7df261a1ca9 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -748,6 +748,24 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"triton_pallas_test\",\n+    srcs = [\n+        \"triton_pallas_test.py\",\n+    ],\n+    enable_backends = [\"cpu\"],\n+    enable_configs = [\n+        \"gpu_h100_x32\",\n+    ],\n+    shard_count = 1,\n+    deps = [\n+        \"//jax:pallas\",\n+        \"//jax:pallas_gpu\",\n+    ] + py_deps([\n+        \"absl/testing\",\n+    ]),\n+)\n+\n jax_multiplatform_test(\n     name = \"mgpu_attention_run\",\n     srcs = [\"//jax/experimental/pallas/ops/gpu:attention_mgpu.py\"],\ndiff --git a/tests/pallas/triton_pallas_test.py b/tests/pallas/triton_pallas_test.py\nnew file mode 100644\nindex 000000000000..4e2b10e72eb1\n--- /dev/null\n+++ b/tests/pallas/triton_pallas_test.py\n@@ -0,0 +1,77 @@\n+# Copyright 2025 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+\"\"\"Test the Triton dialect lowering for a variety of atomic operations.\"\"\"\n+\n+from absl.testing import absltest\n+from absl.testing import parameterized\n+import jax\n+from jax._src import config\n+from jax._src import dtypes\n+from jax._src import test_util as jtu\n+from jax._src.pallas.pallas_call import _trace_kernel_to_jaxpr\n+from jax.experimental import pallas as pl\n+import jax.numpy as jnp\n+\n+config.parse_flags_with_absl()\n+\n+\n+@jtu.with_config(jax_traceback_filtering=\"off\")\n+class PallasBaseTest(jtu.JaxTestCase):\n+  INTERPRET = False\n+\n+  def setUp(self):\n+    if jtu.test_device_matches([\"cpu\"]) and not self.INTERPRET:\n+      self.skipTest(\"On CPU the test works only in interpret mode\")\n+    if (jtu.test_device_matches([\"cuda\"]) and\n+        not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n+      self.skipTest(\"Only works on GPU with capability >= sm90\")\n+\n+    super().setUp()\n+    _trace_kernel_to_jaxpr.cache_clear()\n+\n+  def pallas_call(self, *args, **kwargs):\n+    return pl.pallas_call(*args, **kwargs, interpret=self.INTERPRET)\n+\n+\n+DTYPE_LIST = [jnp.float32, jnp.float16, jnp.bfloat16,\n+              jnp.float8_e4m3fn, jnp.float8_e5m2]\n+\n+\n+class TritonPallasTest(PallasBaseTest):\n+  INTERPRET = False\n+\n+  @parameterized.product(src_dtype=DTYPE_LIST, dst_dtype=DTYPE_LIST)\n+  def test_fp_dtype_cast(self, src_dtype, dst_dtype):\n+    if src_dtype == dst_dtype:\n+      self.skipTest(\"No need to test the same dtype\")\n+    if dtypes.bit_width(src_dtype) == 8 and dtypes.bit_width(dst_dtype) == 8:\n+      self.skipTest(\"Not casting between 8-bit types\")\n+\n+    def body(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].astype(dst_dtype)\n+\n+    x = 10 * jax.random.normal(jax.random.key(0), (64, 64), dtype=src_dtype)\n+    y = self.pallas_call(body,\n+        in_specs=[pl.BlockSpec((64, 64), lambda i: (0, 0))],\n+        out_specs=pl.BlockSpec((64, 64), lambda i: (0, 0)),\n+        out_shape=jax.ShapeDtypeStruct((64, 64), dst_dtype),\n+        grid=(1,),\n+    )(x)\n+    self.assertEqual(y.dtype, dst_dtype)\n+    self.assertArraysEqual(y, x.astype(dst_dtype))\n+\n+if __name__ == \"__main__\":\n+  absltest.main()\n\n```"
        ],
        "from_id": [
            "rdyro",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Introduce the flag `add_pypi_cuda_wheel_deps` that controls if the tests depend on NVIDIA CUDA wheels hermetically.\n\nThe flag is enabled by default.\n\nTo disable the dependency, pass `add_pypi_cuda_wheel_deps=False` in the Bazel options.\n\nPiperOrigin-RevId: 761568590",
        "output": [
            "```diff\nCommit: 8e4f3b5dab9f88a8218b7ec79369210bb95305c6\nDate: 2025-05-21T17:09:37Z\nURL: https://github.com/jax-ml/jax/commit/8e4f3b5dab9f88a8218b7ec79369210bb95305c6\nFiles changed: 2\nAdditions: +27, Deletions: -5\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex eceb38e35aab..678d92bc434a 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -637,3 +637,10 @@ def wheel_sources(\n         \":{}_data\".format(name),\n         \":{}_hdrs\".format(name),\n     ] + static_srcs)\n+\n+def if_pypi_cuda_wheel_deps(if_true, if_false = []):\n+    \"\"\" select() on whether we're adding pypi CUDA wheel deps. \"\"\"\n+    return select({\n+        \"//jaxlib/tools:pypi_cuda_wheel_deps\": if_true,\n+        \"//conditions:default\": if_false,\n+    })\ndiff --git a/jaxlib/tools/BUILD.bazel b/jaxlib/tools/BUILD.bazel\nindex d6a5f94dfd4b..433747e2bb8d 100644\n--- a/jaxlib/tools/BUILD.bazel\n+++ b/jaxlib/tools/BUILD.bazel\n@@ -15,7 +15,7 @@\n # JAX is Autograd and XLA\n \n load(\"@bazel_skylib//lib:selects.bzl\", \"selects\")\n-load(\"@bazel_skylib//rules:common_settings.bzl\", \"string_flag\")\n+load(\"@bazel_skylib//rules:common_settings.bzl\", \"bool_flag\", \"string_flag\")\n load(\"@local_config_cuda//cuda:build_defs.bzl\", \"if_cuda\")\n load(\"@local_config_rocm//rocm:build_defs.bzl\", \"if_rocm\")\n load(\n@@ -29,6 +29,7 @@ load(\n load(\n     \"//jaxlib:jax.bzl\",\n     \"PLATFORM_TAGS_DICT\",\n+    \"if_pypi_cuda_wheel_deps\",\n     \"jax_py_test\",\n     \"jax_wheel\",\n     \"pytype_strict_library\",\n@@ -470,6 +471,20 @@ filegroup(\n     ],\n )\n \n+# The flag configures whether to add the pypi NVIDIA CUDA deps to py_import.\n+bool_flag(\n+    name = \"add_pypi_cuda_wheel_deps\",\n+    build_setting_default = True,\n+)\n+\n+config_setting(\n+    name = \"pypi_cuda_wheel_deps\",\n+    flag_values = {\n+        \":add_pypi_cuda_wheel_deps\": \"True\",\n+        \"@local_config_cuda//:enable_cuda\": \"True\",\n+    },\n+)\n+\n py_import(\n     name = \"jaxlib_py_import\",\n     wheel = \":jaxlib_wheel\",\n@@ -478,26 +493,26 @@ py_import(\n py_import(\n     name = \"jax_cuda_plugin_py_import\",\n     wheel = \":jax_cuda_plugin_wheel\",\n-    wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n+    wheel_deps = if_pypi_cuda_wheel_deps([\":nvidia_wheel_deps\"]),\n )\n \n py_import(\n     name = \"jax_cuda_pjrt_py_import\",\n     wheel = \":jax_cuda_pjrt_wheel\",\n-    wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n+    wheel_deps = if_pypi_cuda_wheel_deps([\":nvidia_wheel_deps\"]),\n )\n \n # The targets below are used for GPU tests with `--//jax:build_jaxlib=false`.\n py_import(\n     name = \"pypi_jax_cuda_plugin_with_cuda_deps\",\n     wheel = \"@pypi_jax_cuda12_plugin//:whl\",\n-    wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n+    wheel_deps = if_pypi_cuda_wheel_deps([\":nvidia_wheel_deps\"]),\n )\n \n py_import(\n     name = \"pypi_jax_cuda_pjrt_with_cuda_deps\",\n     wheel = \"@pypi_jax_cuda12_pjrt//:whl\",\n-    wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n+    wheel_deps = if_pypi_cuda_wheel_deps([\":nvidia_wheel_deps\"]),\n )\n \n # Wheel tests.\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28886 from dfm:np-doc-links\n\nPiperOrigin-RevId: 761552874",
        "output": [
            "```diff\nCommit: fb4279a584b4c50f83625cd95aea2c8b85dc5176\nDate: 2025-05-21T16:28:27Z\nURL: https://github.com/jax-ml/jax/commit/fb4279a584b4c50f83625cd95aea2c8b85dc5176\nFiles changed: 1\nAdditions: +6, Deletions: -1\ndiff --git a/docs/conf.py b/docs/conf.py\nindex a44177407344..dd7533aecf83 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -29,6 +29,7 @@\n import inspect\n import operator\n import os\n+from pathlib import Path\n import sys\n \n sys.path.insert(0, os.path.abspath('..'))\n@@ -354,7 +355,11 @@ def linkcode_resolve(domain, info):\n     source, linenum = inspect.getsourcelines(obj)\n   except:\n     return None\n-  filename = os.path.relpath(filename, start=os.path.dirname(jax.__file__))\n+  try:\n+    filename = Path(filename).relative_to(Path(jax.__file__).parent)\n+  except ValueError:\n+    # Source file is not a relative to jax; this must be a re-exported function.\n+    return None\n   lines = f\"#L{linenum}-L{linenum + len(source)}\" if linenum else \"\"\n   return f\"https://github.com/jax-ml/jax/blob/main/jax/{filename}{lines}\"\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Skip generating source links for re-exported numpy functions in docs.",
        "output": [
            "```diff\nCommit: 6d2dc34bb05e4f4fca3488b2f7ad587762ead497\nDate: 2025-05-21T16:17:03Z\nURL: https://github.com/jax-ml/jax/commit/6d2dc34bb05e4f4fca3488b2f7ad587762ead497\nFiles changed: 1\nAdditions: +6, Deletions: -1\ndiff --git a/docs/conf.py b/docs/conf.py\nindex a44177407344..dd7533aecf83 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -29,6 +29,7 @@\n import inspect\n import operator\n import os\n+from pathlib import Path\n import sys\n \n sys.path.insert(0, os.path.abspath('..'))\n@@ -354,7 +355,11 @@ def linkcode_resolve(domain, info):\n     source, linenum = inspect.getsourcelines(obj)\n   except:\n     return None\n-  filename = os.path.relpath(filename, start=os.path.dirname(jax.__file__))\n+  try:\n+    filename = Path(filename).relative_to(Path(jax.__file__).parent)\n+  except ValueError:\n+    # Source file is not a relative to jax; this must be a re-exported function.\n+    return None\n   lines = f\"#L{linenum}-L{linenum + len(source)}\" if linenum else \"\"\n   return f\"https://github.com/jax-ml/jax/blob/main/jax/{filename}{lines}\"\n \n\n```"
        ],
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Merge pull request #28887 from dfm:uv-on-rtds\n\nPiperOrigin-RevId: 761545482",
        "output": [
            "```diff\nCommit: 609fb7f6085b52861f65c7aa3b339c40dfd207fa\nDate: 2025-05-21T16:05:16Z\nURL: https://github.com/jax-ml/jax/commit/609fb7f6085b52861f65c7aa3b339c40dfd207fa\nFiles changed: 2\nAdditions: +12, Deletions: -9\ndiff --git a/.readthedocs.yml b/.readthedocs.yml\nindex 3b7ba275a0d6..0ac20301cee2 100644\n--- a/.readthedocs.yml\n+++ b/.readthedocs.yml\n@@ -6,15 +6,23 @@\n version: 2\n \n build:\n-  os: \"ubuntu-22.04\"\n+  os: \"ubuntu-24.04\"\n   tools:\n-    python: \"3.10\"\n+    python: \"3.12\"\n   jobs:\n     post_checkout:\n       # Skip building PRs unless tagged with the \"documentation\" label.\n       - |\n         [ \"${READTHEDOCS_VERSION_TYPE}\" != \"external\" ] && echo \"Building latest\" && exit 0\n         (curl -sL https://api.github.com/repos/jax-ml/jax/issues/${READTHEDOCS_VERSION}/labels | grep -q \"https://api.github.com/repos/jax-ml/jax/labels/documentation\") && echo \"Building PR with label\" || exit 183\n+    create_environment:\n+      - asdf plugin add uv\n+      - asdf install uv latest\n+      - asdf global uv latest\n+      - uv venv $READTHEDOCS_VIRTUALENV_PATH\n+      - UV_PROJECT_ENVIRONMENT=$READTHEDOCS_VIRTUALENV_PATH uv pip install -r docs/requirements.txt\n+    install:\n+      - \"true\"  # skip\n \n # Build documentation in the docs/ directory with Sphinx\n sphinx:\n@@ -24,8 +32,3 @@ sphinx:\n # Optionally build your docs in additional formats such as PDF and ePub\n formats:\n   - htmlzip\n-\n-# Optionally set the version of Python and requirements required to build your docs\n-python:\n-  install:\n-    - requirements: docs/requirements.txt\ndiff --git a/docs/conf.py b/docs/conf.py\nindex a7a52c9db38c..a44177407344 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -38,11 +38,11 @@\n from typing import ForwardRef\n \n def _do_not_evaluate_in_jax(\n-    self, globalns, *args, _evaluate=ForwardRef._evaluate,\n+    self, globalns, *args, _evaluate=ForwardRef._evaluate, **kwargs,\n ):\n   if globalns.get('__name__', '').startswith('jax'):\n     return self\n-  return _evaluate(self, globalns, *args)\n+  return _evaluate(self, globalns, *args, **kwargs)\n \n ForwardRef._evaluate = _do_not_evaluate_in_jax\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Try using uv for installing packages on Read the Docs.",
        "output": [
            "```diff\nCommit: 99a0e678c8b6747efa3ed10e1ed4d28fecde525a\nDate: 2025-05-21T15:15:15Z\nURL: https://github.com/jax-ml/jax/commit/99a0e678c8b6747efa3ed10e1ed4d28fecde525a\nFiles changed: 2\nAdditions: +12, Deletions: -9\ndiff --git a/.readthedocs.yml b/.readthedocs.yml\nindex 3b7ba275a0d6..0ac20301cee2 100644\n--- a/.readthedocs.yml\n+++ b/.readthedocs.yml\n@@ -6,15 +6,23 @@\n version: 2\n \n build:\n-  os: \"ubuntu-22.04\"\n+  os: \"ubuntu-24.04\"\n   tools:\n-    python: \"3.10\"\n+    python: \"3.12\"\n   jobs:\n     post_checkout:\n       # Skip building PRs unless tagged with the \"documentation\" label.\n       - |\n         [ \"${READTHEDOCS_VERSION_TYPE}\" != \"external\" ] && echo \"Building latest\" && exit 0\n         (curl -sL https://api.github.com/repos/jax-ml/jax/issues/${READTHEDOCS_VERSION}/labels | grep -q \"https://api.github.com/repos/jax-ml/jax/labels/documentation\") && echo \"Building PR with label\" || exit 183\n+    create_environment:\n+      - asdf plugin add uv\n+      - asdf install uv latest\n+      - asdf global uv latest\n+      - uv venv $READTHEDOCS_VIRTUALENV_PATH\n+      - UV_PROJECT_ENVIRONMENT=$READTHEDOCS_VIRTUALENV_PATH uv pip install -r docs/requirements.txt\n+    install:\n+      - \"true\"  # skip\n \n # Build documentation in the docs/ directory with Sphinx\n sphinx:\n@@ -24,8 +32,3 @@ sphinx:\n # Optionally build your docs in additional formats such as PDF and ePub\n formats:\n   - htmlzip\n-\n-# Optionally set the version of Python and requirements required to build your docs\n-python:\n-  install:\n-    - requirements: docs/requirements.txt\ndiff --git a/docs/conf.py b/docs/conf.py\nindex a7a52c9db38c..a44177407344 100644\n--- a/docs/conf.py\n+++ b/docs/conf.py\n@@ -38,11 +38,11 @@\n from typing import ForwardRef\n \n def _do_not_evaluate_in_jax(\n-    self, globalns, *args, _evaluate=ForwardRef._evaluate,\n+    self, globalns, *args, _evaluate=ForwardRef._evaluate, **kwargs,\n ):\n   if globalns.get('__name__', '').startswith('jax'):\n     return self\n-  return _evaluate(self, globalns, *args)\n+  return _evaluate(self, globalns, *args, **kwargs)\n \n ForwardRef._evaluate = _do_not_evaluate_in_jax\n \n\n```"
        ],
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Prepare for jax release v0.6.1",
        "output": [
            "```diff\nCommit: 382506f1705db9c9ac348b9783497e310feef6a5\nDate: 2025-05-21T15:01:15Z\nURL: https://github.com/jax-ml/jax/commit/382506f1705db9c9ac348b9783497e310feef6a5\nFiles changed: 3\nAdditions: +4, Deletions: -4\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 9fd4e50304d0..939177e01311 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -14,7 +14,7 @@ Remember to align the itemized text with the first line of an item within a list\n When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.md.\n -->\n \n-## Unreleased\n+## JAX 0.6.1\n \n * New features:\n   * Added {func}`jax.lax.axis_size` which returns the size of the mapped axis\ndiff --git a/jax/version.py b/jax/version.py\nindex 9301848b0cfb..acbfb7577e49 100644\n--- a/jax/version.py\n+++ b/jax/version.py\n@@ -152,7 +152,7 @@ def make_release_tree(self, base_dir, files):\n \n \n __version__ = _get_version_string()\n-_minimum_jaxlib_version = \"0.6.0\"\n+_minimum_jaxlib_version = \"0.6.1\"\n \n def _version_as_tuple(version_str):\n   return tuple(int(i) for i in version_str.split(\".\") if i.isdigit())\ndiff --git a/setup.py b/setup.py\nindex 823354adb70d..85ba1fdb4e96 100644\n--- a/setup.py\n+++ b/setup.py\n@@ -19,11 +19,11 @@\n \n project_name = 'jax'\n \n-_current_jaxlib_version = '0.6.0'\n+_current_jaxlib_version = '0.6.1'\n # The following should be updated after each new jaxlib release.\n _latest_jaxlib_version_on_pypi = '0.6.0'\n \n-_libtpu_version = '0.0.13.*'\n+_libtpu_version = '0.0.15.*'\n \n def load_version_module(pkg_path):\n   spec = importlib.util.spec_from_file_location(\n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "[Pallas/Mosaic GPU] Loosen tiling requirements for `get` and `swap`.\n\nWe now allow arbitrary 2D tilings where the minor dimension fits the associated\nswizzle.\n\nPiperOrigin-RevId: 761490845",
        "output": [
            "```diff\nCommit: 5f764b55d82595141837a7a141a650625b4f7679\nDate: 2025-05-21T12:58:01Z\nURL: https://github.com/jax-ml/jax/commit/5f764b55d82595141837a7a141a650625b4f7679\nFiles changed: 2\nAdditions: +47, Deletions: -7\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex eb5fc136082e..b6960c479558 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -1298,11 +1298,14 @@ def _get_lowering_rule(ctx: LoweringRuleContext, x_ref, *leaves, tree):\n \n   match transforms:\n     case (gpu_core.UnswizzleRef(swizzle), gpu_core.UntileRef(tiling)):\n-      if tiling != (\n-          8,\n-          (swizzle * 8) // pallas_utils.dtype_bitwidth(dtype),\n-      ):\n-        raise NotImplementedError(\"Tiling does not fit swizzle\")\n+      if len(tiling) != 2:\n+        raise NotImplementedError(f\"Only 2D tiling is supported, got: {tiling}\")\n+      expected_minor_tiling = swizzle * 8 // pallas_utils.dtype_bitwidth(dtype)\n+      if tiling[-1] != expected_minor_tiling:\n+        raise NotImplementedError(\n+            \"Minor tiling dimension does not fit swizzle: \"\n+            f\" expected {expected_minor_tiling}, got {tiling[-1]}\"\n+        )\n       return mgpu.FragmentedArray.load_tiled(\n           x_smem, is_signed=mgpu_utils.is_signed(dtype), swizzle=swizzle\n       )\n@@ -1383,8 +1386,15 @@ def _swap_lowering_rule(\n         gpu_core.UntileRef(tiling),\n         *maybe_transpose,\n     ):\n-      if tiling != (8, swizzle // v_aval.dtype.itemsize):\n-        raise NotImplementedError(\"Tiling does not fit swizzle\")\n+      if len(tiling) != 2:\n+        raise NotImplementedError(f\"Only 2D tiling is supported, got: {tiling}\")\n+      bw = pallas_utils.dtype_bitwidth(v_aval.dtype)\n+      expected_minor_tiling = swizzle * 8 // bw\n+      if tiling[-1] != expected_minor_tiling:\n+        raise NotImplementedError(\n+            \"Minor tiling dimension does not fit swizzle: \"\n+            f\" expected {expected_minor_tiling}, got {tiling[-1]}\"\n+        )\n \n       if transposed_value != bool(maybe_transpose):\n         raise ValueError(\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex b10bc0f390b0..aedd79e23194 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -998,6 +998,36 @@ def kernel(x_ref, o_ref):\n \n     self.assertIn(\"x: [1, 0, 43, 23]: 6871\\n\", output())\n \n+  @parameterized.parameters(\n+          (plgpu.TilingTransform((1, 32)), plgpu.SwizzleTransform(128)),\n+          (plgpu.TilingTransform((8, 32)), plgpu.SwizzleTransform(128)),\n+          (),\n+  )\n+  def test_get_swap_with_transforms(self, *transforms):\n+    self.skip_if_wg_semantics()\n+\n+    shape = (128, 128)\n+\n+    @functools.partial(\n+        self.pallas_call,\n+        in_specs=[plgpu.BlockSpec(memory_space=plgpu.GMEM)],\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.GMEM),\n+        out_shape=jax.ShapeDtypeStruct(shape, jnp.int32),\n+        scratch_shapes=[\n+            plgpu.SMEM(shape, jnp.int32, transforms=tuple(transforms)),\n+            plgpu.Barrier(num_arrivals=1),\n+        ]\n+    )\n+    def kernel(x_ref, o_ref, scratch_ref, barrier_ref):\n+      plgpu.copy_gmem_to_smem(x_ref, scratch_ref, barrier_ref)\n+      plgpu.barrier_wait(barrier_ref)\n+      scratch_ref[...] = scratch_ref[...] * 2\n+      plgpu.copy_smem_to_gmem(scratch_ref, o_ref)\n+      plgpu.wait_smem_to_gmem(0)\n+\n+    x = jnp.arange(math.prod(shape), dtype=jnp.int32).reshape(shape)\n+    np.testing.assert_array_equal(kernel(x), x * 2)\n+\n   def test_check(self):\n     self.skip_if_wg_semantics()\n \n\n```"
        ],
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Support `s8xs8->s32` WGMMA.\n\nPiperOrigin-RevId: 761489756",
        "output": [
            "```diff\nCommit: 8653a78b80eb7bd6dce2a9c78bce6af949dd53d2\nDate: 2025-05-21T12:52:57Z\nURL: https://github.com/jax-ml/jax/commit/8653a78b80eb7bd6dce2a9c78bce6af949dd53d2\nFiles changed: 3\nAdditions: +136, Deletions: -17\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex 62b43b4de737..f69d3f33fe7c 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -2551,7 +2551,7 @@ def _repack(regs_it, reg_ty):\n   for array in arrays:\n     reg_ty = array.registers.flat[0].type\n     dtype = array.mlir_dtype\n-    if ir.F32Type.isinstance(dtype):\n+    if ir.F32Type.isinstance(dtype) or dtype == i32:\n       if ir.VectorType.isinstance(reg_ty):\n         [vec_len] = ir.VectorType(reg_ty).shape\n         array_regs = [  # pylint: disable=g-complex-comprehension\n@@ -2561,7 +2561,7 @@ def _repack(regs_it, reg_ty):\n         ]\n       else:\n         array_regs = list(array.registers.flat)\n-      reg_constraint = \"f\"\n+      reg_constraint = \"r\" if dtype == i32 else \"f\"\n     elif ir.BF16Type.isinstance(dtype) or ir.F16Type.isinstance(dtype):\n       if not ir.VectorType.isinstance(reg_ty):\n         raise NotImplementedError(array.mlir_dtype)\ndiff --git a/jax/experimental/mosaic/gpu/wgmma.py b/jax/experimental/mosaic/gpu/wgmma.py\nindex 3637778c371b..abbd517fb37d 100644\n--- a/jax/experimental/mosaic/gpu/wgmma.py\n+++ b/jax/experimental/mosaic/gpu/wgmma.py\n@@ -63,7 +63,10 @@ def zero(cls, m, n, dtype=None, *, is_signed: bool | None = None):\n     f32 = ir.F32Type.get()\n     if dtype is None:\n       dtype = f32\n-    zero = arith.constant(dtype, ir.FloatAttr.get(dtype, 0.0))\n+    if ir.IntegerType.isinstance(dtype):\n+      zero = arith.constant(dtype, ir.IntegerAttr.get(dtype, 0))\n+    else:\n+      zero = arith.constant(dtype, ir.FloatAttr.get(dtype, 0.0))\n     return cls(\n         _value=fa.FragmentedArray.splat(\n             zero, (m, n), fa.WGMMA_LAYOUT, is_signed=is_signed\n@@ -90,6 +93,8 @@ def _supported_wgmma_types(dtype, abtype) -> bool:\n     return any(input_types_are(ty) for ty in (ir.FloatTF32Type, ir.BF16Type, *f16_acc_types))\n   elif ir.F16Type.isinstance(dtype):\n     return any(input_types_are(ty) for ty in f16_acc_types)\n+  elif ir.IntegerType.get_signless(32).isinstance(dtype):\n+    return input_types_are(ir.IntegerType.get_signless(8))\n   else:\n     return False\n \n@@ -135,7 +140,7 @@ def wgmma_m64(\n     if a_transpose is None:\n       raise ValueError\n \n-  if ir.F32Type.isinstance(out_ty):\n+  if ir.F32Type.isinstance(out_ty) or out_ty == i32:\n     num_acc_regs = n // 2\n     out_ty_field = out_ty\n     acc_regs = [  # pylint: disable=g-complex-comprehension\n@@ -143,8 +148,9 @@ def wgmma_m64(\n         for reg in acc.flat\n         for pos in range(2)\n     ]\n-    to_acc_vec_regs = functools.partial(_as_fragmented_reg_ndarray, dtype=out_ty, shape=acc.shape)\n-    acc_constraint = \"f\"\n+    to_acc_vec_regs = functools.partial(\n+        _as_fragmented_reg_ndarray, dtype=out_ty, shape=acc.shape)\n+    acc_constraint = \"r\" if ir.IntegerType.isinstance(out_ty) else \"f\"\n   elif ir.F16Type.isinstance(out_ty):\n     num_acc_regs = n // 4\n     out_ty_field = i32\n@@ -153,9 +159,15 @@ def wgmma_m64(\n     to_acc_vec_regs = lambda regs : np.array([_unpack_i32(vec_ty, reg) for reg in regs]).reshape(acc.shape)\n     acc_constraint = \"r\"\n   else:\n-    raise ValueError(f\"WGMMA instruciton only supports f32 and f16 out (got {out_ty})\")\n+    raise ValueError(\n+        f\"WGMMA instruction only supports f32, f16 and s32 out (got {out_ty})\")\n \n-  num_imm_regs = 4 if supports_transpose else 2\n+  if supports_transpose:\n+    num_imm_regs = 4\n+  elif out_ty == i32:\n+    num_imm_regs = 0\n+  else:\n+    num_imm_regs = 2\n \n   if a_in_regs:\n     a_reg_constraints = [\"r\"] * 4  # 4x f16x2 registers\n@@ -172,7 +184,6 @@ def wgmma_m64(\n       + [\"n\"] * (1 + num_imm_regs)  # literal constants\n   )\n   reg_constraints = \",\".join(reg_constraints_list)\n-\n   reg_count = itertools.count()\n \n   def take_regs(n):\n@@ -186,7 +197,8 @@ def take_regs(n):\n   else:\n     a_regs, = take_regs(1)\n   b_desc_reg, use_out_reg = take_regs(2)\n-  imm_regs = \", \".join(take_regs(num_imm_regs))  # Immediate regs (scale, ...).\n+  # Immediate regs (scale, ...).\n+  imm_regs = \"\".join(f\", {r}\" for r in take_regs(num_imm_regs))\n   assert next(reg_count) == len(reg_constraints_list)\n   k_instr = 32 // bytewidth(element_type)\n   el_ty = str(element_type)\n@@ -194,9 +206,19 @@ def take_regs(n):\n     el_ty = \"e5m2\"\n   elif ir.Float8E4M3FNType.isinstance(element_type):\n     el_ty = \"e4m3\"\n+  elif ir.IntegerType.get_signless(8).isinstance(element_type):\n+    # TODO(bchetioui): add u8 support in the future. Currently we always assume\n+    # that 8-bit integers are s8, and we would need to change the signature of\n+    # `wgmma` to indicate whether the input should be treated as signed or not.\n+    el_ty = \"s8\"\n+\n+  out_ty_str = str(out_ty)\n+  if out_ty == i32:\n+    out_ty_str = \"s32\"\n+\n   wgmma_instr = (\n-      f\"wgmma.mma_async.sync.aligned.m64n{n}k{k_instr}.{out_ty}.{el_ty}.{el_ty} \"\n-      f\"{acc_reg_vector}, {a_regs}, {b_desc_reg}, p, {imm_regs};\"\n+      f\"wgmma.mma_async.sync.aligned.m64n{n}k{k_instr}.{out_ty_str}.{el_ty}.{el_ty} \"\n+      f\"{acc_reg_vector}, {a_regs}, {b_desc_reg}, p{imm_regs};\"\n   )\n   ptx = f\"{{ .reg .pred p; setp.ne.b32 p, {use_out_reg}, 0; {wgmma_instr} }}\\n\"\n \n@@ -297,6 +319,8 @@ def wgmma(\n     )\n   f32 = ir.F32Type.get()\n   f16 = ir.F16Type.get()\n+  i32 = ir.IntegerType.get_signless(32)\n+  i8 = ir.IntegerType.get_signless(8)\n   if element_type == f32 or element_type == ir.BF16Type.get():\n     if acc.value.mlir_dtype != f32:\n       raise ValueError(\n@@ -312,6 +336,14 @@ def wgmma(\n           f\"WGMMA with element type {element_type} only supports accumulators \"\n           f\"of type f32 or f16, but got: {acc.value.mlir_dtype}\"\n       )\n+  elif element_type == i8:\n+    if a_in_regs and not a.is_signed:\n+      raise NotImplementedError(\"WGMMA with lhs of type u8\")\n+    if acc.value.mlir_dtype != i32 or not acc.value.is_signed:\n+      raise ValueError(\n+          f\"WGMMA with element type {element_type} only supports accumulators \"\n+          f\"of type s32, but got: {acc.value.mlir_dtype}\"\n+      )\n   else:\n     raise NotImplementedError(f\"Unsupported element type: {element_type}\")\n \ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 80e67b20e1ef..4e0544d1758e 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -645,6 +645,19 @@ def kernel(ctx, in_, out, smem):\n     np.testing.assert_array_equal(iota, expected)\n \n \n+class I8Type:\n+  \"\"\"A type that represents a 8-bit signed integer.\n+\n+  This is a workaround to bypass the fact that we don't have a proper 8-bit\n+  integer type class available in MLIR, and can't instantiate types without a\n+  MLIR context.\n+  \"\"\"\n+\n+  @staticmethod\n+  def get():  # pylint: disable=no-method-argument\n+    return ir.IntegerType.get_signless(8)\n+\n+\n class WGMMATest(TestCase):\n \n   def setUp(self):\n@@ -670,7 +683,67 @@ def setUp(self):\n       rhs_tiling_kind=(\"large\", \"small\", \"small+no_transpose\"),\n       lhs_tiling_kind=(\"large\", \"small\", \"small+no_transpose\"),\n   )\n-  def test_wgmma_basic(\n+  def test_wgmma_basic_float(\n+      self,\n+      lhs_transpose,\n+      rhs_transpose,\n+      in_mlir_dtype_cls,\n+      m,\n+      n,\n+      k_steps,\n+      swizzle,\n+      jax_out_dtype,\n+      rhs_tiling_kind,\n+      lhs_tiling_kind,\n+  ):\n+    self._test_wgmma_basic(\n+        m,\n+        n,\n+        k_steps,\n+        in_mlir_dtype_cls,\n+        lhs_transpose,\n+        rhs_transpose,\n+        swizzle,\n+        jax_out_dtype,\n+        rhs_tiling_kind,\n+        lhs_tiling_kind,\n+    )\n+\n+  @parameterized.product(\n+      in_mlir_dtype_cls=(I8Type,),\n+      m=(64, 128, 192),\n+      n=(64, 128, 192),\n+      k_steps=(1, 2),\n+      swizzle=(32, 64, 128),\n+      jax_out_dtype=(jnp.int32,),\n+      rhs_tiling_kind=(\"large\", \"small\", \"small+no_transpose\"),\n+      lhs_tiling_kind=(\"large\", \"small\"),\n+  )\n+  def test_wgmma_basic_int(\n+      self,\n+      in_mlir_dtype_cls,\n+      m,\n+      n,\n+      k_steps,\n+      swizzle,\n+      jax_out_dtype,\n+      rhs_tiling_kind,\n+      lhs_tiling_kind,\n+  ):\n+    self._test_wgmma_basic(\n+        m,\n+        n,\n+        k_steps,\n+        in_mlir_dtype_cls,\n+        lhs_transpose=False,\n+        rhs_transpose=True,\n+        swizzle=swizzle,\n+        jax_out_dtype=jax_out_dtype,\n+        rhs_tiling_kind=rhs_tiling_kind,\n+        lhs_tiling_kind=lhs_tiling_kind,\n+    )\n+\n+  def _test_wgmma_basic(\n       self,\n       m,\n       n,\n@@ -683,6 +756,10 @@ def test_wgmma_basic(\n       rhs_tiling_kind,\n       lhs_tiling_kind,\n   ):\n+    if jax_out_dtype == jnp.int32 and in_mlir_dtype_cls != I8Type:\n+      self.skipTest(\"s32 accumulator only supported with s8 inputs\")\n+    if jax_out_dtype != jnp.int32 and in_mlir_dtype_cls == I8Type:\n+      self.skipTest(\"s8 inputs only supported with s32 accumulator\")\n     if jax_out_dtype == jnp.float16 and in_mlir_dtype_cls in {ir.F32Type, ir.BF16Type}:\n       self.skipTest(f\"{in_mlir_dtype_cls.get()} does not support f16 output.\")\n     if swizzle != 128 and lhs_transpose and lhs_tiling_kind == \"large\":\n@@ -716,6 +793,9 @@ def test_wgmma_basic(\n     elif in_mlir_dtype_cls == ir.Float8E4M3FNType:\n       in_jax_dtype = jnp.float8_e4m3fn\n       exponent_bits, mantissa_bits = 4, 3\n+    elif in_mlir_dtype_cls == I8Type:\n+      in_jax_dtype = jnp.int8\n+      exponent_bits = mantissa_bits = None\n     else:\n       raise NotImplementedError(in_mlir_dtype)\n     nk_tile = swizzle // bytewidth(in_mlir_dtype)\n@@ -755,7 +835,8 @@ def kernel(ctx, lhs, rhs, out, scratch):\n       )\n       for i in range(2):\n         barriers[i].wait()\n-      init_acc = mgpu.WGMMAAccumulator.zero(m=m, n=n, dtype=out_mlir_dtype)\n+      is_signed = True if ir.IntegerType.isinstance(in_mlir_dtype) else None\n+      init_acc = mgpu.WGMMAAccumulator.zero(m=m, n=n, dtype=out_mlir_dtype, is_signed=is_signed)\n       if lhs_transpose:\n         perm = (0, 1, 3, 2) if transpose_lhs_tiles else (1, 0, 3, 2)\n         lhs_smem = memref_transpose(lhs_smem, perm)\n@@ -772,9 +853,13 @@ def quantize(x):\n       return jax.lax.reduce_precision(x, exponent_bits, mantissa_bits)\n \n     x_shape = (k, m) if lhs_transpose else (m, k)\n-    x = quantize(self.prng.uniform(-1, 1, x_shape)).astype(in_jax_dtype)\n     y_shape = (n, k) if rhs_transpose else (k, n)\n-    y = quantize(self.prng.uniform(-1, 1, y_shape)).astype(in_jax_dtype)\n+    if in_mlir_dtype_cls == I8Type:\n+      x = self.prng.integers(-128, 127, x_shape).astype(in_jax_dtype)\n+      y = self.prng.integers(-128, 127, y_shape).astype(in_jax_dtype)\n+    else:\n+      x = quantize(self.prng.uniform(-1, 1, x_shape)).astype(in_jax_dtype)\n+      y = quantize(self.prng.uniform(-1, 1, y_shape)).astype(in_jax_dtype)\n     out_shape = jax.ShapeDtypeStruct((m, n), jax_out_dtype)\n     if transpose_rhs_tiles:\n       rhs_tiling_t = rhs_tiling[::-1] if rhs_transpose else rhs_tiling\n@@ -797,7 +882,9 @@ def quantize(x):\n     x32, y32 = x.astype(np.float32), y.astype(np.float32)\n     ref = (x32.T if lhs_transpose else x32) @ (y32.T if rhs_transpose else y32)\n     atol = 2e-2 if jax_out_dtype == jnp.float16 else 5e-6\n-    if utils.bitwidth(in_mlir_dtype) == 8:\n+    if ir.IntegerType.isinstance(in_mlir_dtype) and ir.IntegerType.isinstance(out_mlir_dtype):\n+      atol = 0\n+    elif utils.bitwidth(in_mlir_dtype) == 8:\n       atol = 3e-2\n     np.testing.assert_allclose(z, ref, atol=atol)\n \n\n```"
        ],
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Removed `GPU*` aliases\n\nPiperOrigin-RevId: 761482778",
        "output": [
            "```diff\nCommit: 2f0ca797f0a90801714cd5093f32983275db3296\nDate: 2025-05-21T12:26:06Z\nURL: https://github.com/jax-ml/jax/commit/2f0ca797f0a90801714cd5093f32983275db3296\nFiles changed: 1\nAdditions: +0, Deletions: -6\ndiff --git a/jax/experimental/pallas/mosaic_gpu.py b/jax/experimental/pallas/mosaic_gpu.py\nindex cc0e185e296a..8c7870412403 100644\n--- a/jax/experimental/pallas/mosaic_gpu.py\n+++ b/jax/experimental/pallas/mosaic_gpu.py\n@@ -69,9 +69,3 @@\n SMEM = MemorySpace.SMEM\n #: Alias of :data:`jax.experimental.pallas.mosaic_gpu.MemorySpace.TMEM`.\n TMEM = MemorySpace.TMEM\n-\n-# TODO(slebedev): Deprecate and remove these aliases.\n-GPUBlockSpec = BlockSpec\n-GPUCompilerParams = CompilerParams\n-GPUMemorySpace = MemorySpace\n-GPUMesh = Mesh\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Disable the newly added tfrt targets that never worked\n\nPiperOrigin-RevId: 761442340",
        "output": [
            "```diff\nCommit: 4d6e39e4dd38e60dcd4b999284c54030a4076cea\nDate: 2025-05-21T09:53:32Z\nURL: https://github.com/jax-ml/jax/commit/4d6e39e4dd38e60dcd4b999284c54030a4076cea\nFiles changed: 1\nAdditions: +6, Deletions: -0\ndiff --git a/tests/BUILD b/tests/BUILD\nindex aa777080fd92..2418c8224869 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -460,6 +460,9 @@ jax_multiplatform_test(\n     backend_tags = {\n         \"gpu\": [\"noasan\"],  # Memory leaks in NCCL, see https://github.com/NVIDIA/nccl/pull/1143\n     },\n+    disable_configs = [\n+        \"gpu_h100x2_tfrt\",  # TODO(b/419192167): Doesn't work\n+    ],\n     enable_backends = [\"gpu\"],\n     tags = [\n         \"config-cuda-only\",\n@@ -1844,6 +1847,9 @@ jax_multiplatform_test(\n jax_multiplatform_test(\n     name = \"shard_map_test\",\n     srcs = [\"shard_map_test.py\"],\n+    disable_configs = [\n+        \"gpu_h100x2_tfrt\",  # TODO(b/419192167): Doesn't work\n+    ],\n     enable_configs = [\n         \"gpu_p100x2_shardy\",\n         \"tpu_v3_x4_shardy\",\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add missing test skips (for too old jaxlib) + bump minimum libtpu version\n\nPiperOrigin-RevId: 761429155",
        "output": [
            "```diff\nCommit: d8804aae654bf902fb1cc7c92e22ad65b1c59e6b\nDate: 2025-05-21T09:08:19Z\nURL: https://github.com/jax-ml/jax/commit/d8804aae654bf902fb1cc7c92e22ad65b1c59e6b\nFiles changed: 4\nAdditions: +7, Deletions: -2\ndiff --git a/.github/workflows/cloud-tpu-ci-nightly.yml b/.github/workflows/cloud-tpu-ci-nightly.yml\nindex 5cc2aebe3cd0..061b399132e2 100644\n--- a/.github/workflows/cloud-tpu-ci-nightly.yml\n+++ b/.github/workflows/cloud-tpu-ci-nightly.yml\n@@ -44,7 +44,7 @@ jobs:\n             jaxlib-version: \"pypi_latest\"\n     name: \"TPU test (jaxlib=${{ matrix.jaxlib-version }}, ${{ matrix.tpu.type }})\"\n     env:\n-      LIBTPU_OLDEST_VERSION_DATE: 20241205\n+      LIBTPU_OLDEST_VERSION_DATE: 20250226\n       PYTHON: python${{ matrix.python-version }}\n     runs-on: ${{ matrix.tpu.runner }}\n     container: \"us-central1-docker.pkg.dev/tensorflow-sigs/tensorflow/ml-build:latest\"\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 55a0b4cc1a5f..8ecccbe274e9 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -82,7 +82,7 @@ jobs:\n     # End Presubmit Naming Check github-tpu-presubmits\n \n     env:\n-      LIBTPU_OLDEST_VERSION_DATE: 20241205\n+      LIBTPU_OLDEST_VERSION_DATE: 20250226\n       JAXCI_HERMETIC_PYTHON_VERSION: \"${{ inputs.python }}\"\n       JAXCI_PYTHON: \"python${{ inputs.python }}\"\n       JAXCI_RUN_FULL_TPU_TEST_SUITE: \"${{ inputs.run-full-tpu-test-suite }}\"\ndiff --git a/tests/pallas/gpu_pallas_distributed_test.py b/tests/pallas/gpu_pallas_distributed_test.py\nindex d862e6b9b819..81433b8c5067 100644\n--- a/tests/pallas/gpu_pallas_distributed_test.py\n+++ b/tests/pallas/gpu_pallas_distributed_test.py\n@@ -34,6 +34,8 @@\n class PallasCallRemoteDMATest(jt_multiprocess.MultiProcessTest):\n \n   def setUp(self):\n+    if jtu.jaxlib_version() < (0, 6, 1):\n+      self.skipTest(\"Test requires jaxlib >= 0.6.1\")\n     if (not jtu.test_device_matches([\"cuda\"]) or\n         not jtu.is_cuda_compute_capability_at_least(\"9.0\")):\n       self.skipTest(\"Only works on GPU with capability >= sm90\")\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 88ec58ec37cd..523601691a97 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -63,6 +63,7 @@\n from jax._src import xla_bridge\n from jax._src.lib import xla_client as xc\n from jax._src.lib import _jax\n+from jax._src.lib import jaxlib_extension_version\n from jax._src.util import curry, unzip2\n \n config.parse_flags_with_absl()\n@@ -7761,6 +7762,8 @@ def f(x):\n   @config.use_shardy_partitioner(True)\n   @jtu.with_explicit_mesh((2, 2), ('x', 'y'))\n   def test_unreduced_basic(self, mesh):\n+    if jaxlib_extension_version < 342:\n+      self.skipTest(\"Test requires a newer jaxlib\")\n     np_inp = np.arange(16).reshape(8, 2)\n     x = jax.device_put(np_inp, P('x', 'y'))\n     y = jax.device_put(np_inp.T, P('y', None))\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Support passing PartitionSpecs to ShapeDtypeStruct when there is a mesh in context.\n\nPiperOrigin-RevId: 761322712",
        "output": [
            "```diff\nCommit: eef1f6cf9af4fabd47a70b71f78bf1339c9a36cd\nDate: 2025-05-21T02:36:59Z\nURL: https://github.com/jax-ml/jax/commit/eef1f6cf9af4fabd47a70b71f78bf1339c9a36cd\nFiles changed: 3\nAdditions: +35, Deletions: -10\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 33802e494304..059db1c92c98 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -2826,9 +2826,10 @@ def __init__(self, shape, dtype, *, sharding=None, weak_type=False):\n     if dtype is None:\n       raise ValueError(\"ShapeDtypeStruct: dtype must be specified.\")\n     self.dtype = dtype if dtypes.issubdtype(dtype, dtypes.extended) else np.dtype(dtype)\n-    if sharding is not None and not isinstance(sharding, (Sharding, Layout)):\n+    if sharding is not None and not isinstance(sharding, (Sharding, Layout, P)):\n       raise ValueError(\n-          \"sharding should be an instance of `jax.sharding.Sharding` or\"\n+          \"sharding should be an instance of `jax.sharding.Sharding`, \"\n+          \"`jax.sharding.PartitionSpec` or\"\n           f\" `jax.experimental.layout.Layout`. Got {sharding} of type\"\n           f\" {type(sharding)}.\")\n     if (isinstance(sharding, Layout) and\n@@ -2836,7 +2837,19 @@ def __init__(self, shape, dtype, *, sharding=None, weak_type=False):\n       raise TypeError(\n           \"`DeviceLocalLayout.AUTO` cannot be used in place of a device-local\"\n           f\" layout in a `ShapeDtypeStruct`. Got {sharding}\")\n-    self.sharding = sharding.sharding if isinstance(sharding, Layout) else sharding\n+    if isinstance(sharding, Layout):\n+      self.sharding = sharding.sharding\n+    elif isinstance(sharding, P):\n+      # TODO(yashkatariya): Should this be abstract mesh?\n+      cur_mesh = get_concrete_mesh()\n+      if cur_mesh is None:\n+        raise TypeError(\n+            \"When specifying PartitionSpec to `ShapeDtypeStruct`, the context\"\n+            \" mesh cannot be empty. Please use `jax.sharding.use_mesh` to set\"\n+            \" the mesh context.\")\n+      self.sharding = NamedSharding(cur_mesh, sharding)\n+    else:\n+      self.sharding = sharding\n     self._dll = sharding.device_local_layout if isinstance(sharding, Layout) else None\n     self.weak_type = weak_type\n \ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 3fe3d6fa7514..9963e2603588 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -4525,13 +4525,6 @@ def foo(x):\n     with self.assertRaisesRegex(TypeError, \"applied to foo\"):\n       f_vjp(1.0, 1.0)\n \n-  def test_shapedtypestruct_sharding_error(self):\n-    with self.assertRaisesRegex(\n-        ValueError,\n-        \"sharding should be an instance of `jax.sharding.Sharding`.\"):\n-      jax.ShapeDtypeStruct((8, 2), np.float32,\n-                           sharding=jax.sharding.PartitionSpec('x'))\n-\n   def test_make_jaxpr_weakref(self):\n     class Foo(NamedTuple):\n       x: int\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 7d87705e20bd..88ec58ec37cd 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -5006,6 +5006,25 @@ def test_sds_update(self):\n     with self.assertRaisesRegex(ValueError, \"updating ShapeDtypeStruct\"):\n       s4.update(sharding=NamedSharding(mesh, P('x')))\n \n+  @jtu.with_explicit_mesh((2, 1), ('x', 'y'), axis_types=(AxisType.Auto,) * 2)\n+  def test_sds_pspec_input(self, mesh):\n+    inp = jax.ShapeDtypeStruct((2, 2), np.float32, sharding=P('x'))\n+    lowered = jax.jit(lambda x: x * 2).lower(inp)\n+    self.assertIn('num_partitions = 2', lowered.as_text())\n+\n+    np_inp = np.arange(4, dtype=np.float32).reshape(2, 2)\n+    arr = jax.device_put(np_inp, P('x'))\n+    out = lowered.compile()(arr)\n+    self.assertArraysEqual(out, np_inp * 2)\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P('x')))\n+\n+  def test_sds_pspec_no_mesh_ctx_error(self):\n+    with self.assertRaisesRegex(\n+        TypeError,\n+        'When specifying PartitionSpec to `ShapeDtypeStruct`, the context mesh'\n+        ' cannot be empty'):\n+      jax.ShapeDtypeStruct((2, 2), np.float32, sharding=P('x'))\n+\n \n def spec_regex(s):\n   return str(s).replace(r\"(\", r\"\\(\").replace(r\")\", r\"\\)\")\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28871 from hawkinsp:ci\n\nPiperOrigin-RevId: 761306262",
        "output": [
            "```diff\nCommit: 56d4cb0fff1bd24f14c026efffda64252598fd43\nDate: 2025-05-21T01:22:34Z\nURL: https://github.com/jax-ml/jax/commit/56d4cb0fff1bd24f14c026efffda64252598fd43\nFiles changed: 1\nAdditions: +6, Deletions: -0\ndiff --git a/.bazelrc b/.bazelrc\nindex 53676637c839..8906234c9061 100644\n--- a/.bazelrc\n+++ b/.bazelrc\n@@ -244,6 +244,9 @@ build:ci_linux_aarch64_base --config=clang --verbose_failures=true\n build:ci_linux_aarch64_base --action_env=TF_SYSROOT=\"/dt10\"\n build:ci_linux_aarch64_base --color=yes\n \n+# This appears to help avoid a timeout in CI for linalg_test.\n+build:ci_linux_aarch64_base --test_env=OMP_NUM_THREADS=8\n+\n build:ci_linux_aarch64 --config=ci_linux_aarch64_base\n build:ci_linux_aarch64 --host_crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n build:ci_linux_aarch64 --crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n@@ -379,6 +382,9 @@ build:rbe_cross_compile_base --remote_instance_name=projects/tensorflow-testing/\n build:rbe_cross_compile_linux_aarch64 --config=cross_compile_linux_aarch64\n build:rbe_cross_compile_linux_aarch64 --config=rbe_cross_compile_base\n \n+# Avoids a timeout in linalg_test on ARM.\n+build:rbe_cross_compile_linux_aarch64 --test_env=OMP_NUM_THREADS=8\n+\n # Mac x86\n build:cross_compile_darwin_x86_64 --config=cross_compile_base\n build:cross_compile_darwin_x86_64 --config=nonccl\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reland the limit on the number of OpenBLAS threads.\n\nThis was previously removed in https://github.com/jax-ml/jax/commit/18ff6caa4f767701dd7cca3a1333d9b99465e045, and that promptly broke our CI again. I am guessing the problem is actually too few threads, not a NumPy deadlock as I originally guessed.",
        "output": [
            "```diff\nCommit: 862791a91e2a8c87fe4c2c46ae9fd6a9e01adbec\nDate: 2025-05-21T01:15:10Z\nURL: https://github.com/jax-ml/jax/commit/862791a91e2a8c87fe4c2c46ae9fd6a9e01adbec\nFiles changed: 1\nAdditions: +6, Deletions: -0\ndiff --git a/.bazelrc b/.bazelrc\nindex 53676637c839..8906234c9061 100644\n--- a/.bazelrc\n+++ b/.bazelrc\n@@ -244,6 +244,9 @@ build:ci_linux_aarch64_base --config=clang --verbose_failures=true\n build:ci_linux_aarch64_base --action_env=TF_SYSROOT=\"/dt10\"\n build:ci_linux_aarch64_base --color=yes\n \n+# This appears to help avoid a timeout in CI for linalg_test.\n+build:ci_linux_aarch64_base --test_env=OMP_NUM_THREADS=8\n+\n build:ci_linux_aarch64 --config=ci_linux_aarch64_base\n build:ci_linux_aarch64 --host_crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n build:ci_linux_aarch64 --crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n@@ -379,6 +382,9 @@ build:rbe_cross_compile_base --remote_instance_name=projects/tensorflow-testing/\n build:rbe_cross_compile_linux_aarch64 --config=cross_compile_linux_aarch64\n build:rbe_cross_compile_linux_aarch64 --config=rbe_cross_compile_base\n \n+# Avoids a timeout in linalg_test on ARM.\n+build:rbe_cross_compile_linux_aarch64 --test_env=OMP_NUM_THREADS=8\n+\n # Mac x86\n build:cross_compile_darwin_x86_64 --config=cross_compile_base\n build:cross_compile_darwin_x86_64 --config=nonccl\n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Merge pull request #28863 from hawkinsp:tsan2\n\nPiperOrigin-RevId: 761283866",
        "output": [
            "```diff\nCommit: 7de62de0b613b23fd9519ce69b71263539397b3e\nDate: 2025-05-21T00:01:51Z\nURL: https://github.com/jax-ml/jax/commit/7de62de0b613b23fd9519ce69b71263539397b3e\nFiles changed: 3\nAdditions: +0, Deletions: -7\ndiff --git a/.github/workflows/tsan-suppressions_3.13.txt b/.github/workflows/tsan-suppressions_3.13.txt\nindex a929a8c44728..483e3f0b3c2a 100644\n--- a/.github/workflows/tsan-suppressions_3.13.txt\n+++ b/.github/workflows/tsan-suppressions_3.13.txt\n@@ -23,9 +23,6 @@ race_top:PyMember_GetOne\n race_top:new_reference\n race:_Py_IsOwnedByCurrentThread\n \n-# https://github.com/python/cpython/issues/129748\n-race:mi_block_set_nextx\n-\n # https://github.com/python/cpython/issues/128130\n race_top:run_eval_code_obj\n \ndiff --git a/.github/workflows/tsan-suppressions_3.14.txt b/.github/workflows/tsan-suppressions_3.14.txt\nindex 384560128cfc..008b61933a0b 100644\n--- a/.github/workflows/tsan-suppressions_3.14.txt\n+++ b/.github/workflows/tsan-suppressions_3.14.txt\n@@ -8,9 +8,6 @@ race:dnnl_sgemm\n # https://github.com/python/cpython/issues/128050\n race:partial_vectorcall_fallback\n \n-# https://github.com/python/cpython/issues/129748\n-race:mi_block_set_nextx\n-\n # Races because the LAPACK and BLAS in our scipy isn't TSAN instrumented.\n race:heevd_ffi\n race:gesdd_ffi\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex 882e140b91ad..ce4130c31a30 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -14,7 +14,6 @@ on:\n     paths:\n       - '**/workflows/tsan.yaml'\n       - '**/workflows/tsan-suppressions*.txt'\n-      - '**/workflows/requirements_lock_3_13_ft.patch'\n \n jobs:\n   tsan:\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[JAX] Fix float typo in a code example in the sharded-computation doc.\n\nPiperOrigin-RevId: 761268857",
        "output": [
            "```diff\nCommit: b96a63904faf82b9404c8b551cff9c1c6bbb962f\nDate: 2025-05-20T23:15:28Z\nURL: https://github.com/jax-ml/jax/commit/b96a63904faf82b9404c8b551cff9c1c6bbb962f\nFiles changed: 2\nAdditions: +4, Deletions: -4\ndiff --git a/docs/sharded-computation.ipynb b/docs/sharded-computation.ipynb\nindex 1bae4014b5a8..72cc2d193bfd 100644\n--- a/docs/sharded-computation.ipynb\n+++ b/docs/sharded-computation.ipynb\n@@ -7,7 +7,7 @@\n     \"(sharded-computation)=\\n\",\n     \"# Introduction to parallel programming\\n\",\n     \"\\n\",\n-    \"<!--* freshness: { reviewed: '2024-05-10' } *-->\\n\",\n+    \"<!--* freshness: { reviewed: '2025-05-19' } *-->\\n\",\n     \"\\n\",\n     \"This tutorial serves as an introduction to device parallelism for Single-Program Multi-Data (SPMD) code in JAX. SPMD is a parallelism technique where the same computation, such as the forward pass of a neural network, can be run on different input data (for example, different inputs in a batch) in parallel on different devices, such as several GPUs or Google TPUs.\\n\",\n     \"\\n\",\n@@ -495,7 +495,7 @@\n    \"id\": \"c09acf7d\",\n    \"metadata\": {},\n    \"source\": [\n-    \"We should read the type `f32[4@X, 2]` as \\\"a 4-by-2 array of 32-bit floats whose first dimension\\n\",\n+    \"We should read the type `int32[4@X, 2]` as \\\"a 4-by-2 array of 32-bit ints whose first dimension\\n\",\n     \"is sharded along mesh axis 'X'. The array is replicated along all other mesh\\n\",\n     \"axes\\\"\\n\",\n     \"\\n\",\ndiff --git a/docs/sharded-computation.md b/docs/sharded-computation.md\nindex 16a5dc8cfa08..89ffbc07da38 100644\n--- a/docs/sharded-computation.md\n+++ b/docs/sharded-computation.md\n@@ -14,7 +14,7 @@ kernelspec:\n (sharded-computation)=\n # Introduction to parallel programming\n \n-<!--* freshness: { reviewed: '2024-05-10' } *-->\n+<!--* freshness: { reviewed: '2025-05-19' } *-->\n \n This tutorial serves as an introduction to device parallelism for Single-Program Multi-Data (SPMD) code in JAX. SPMD is a parallelism technique where the same computation, such as the forward pass of a neural network, can be run on different input data (for example, different inputs in a batch) in parallel on different devices, such as several GPUs or Google TPUs.\n \n@@ -193,7 +193,7 @@ print(f\"replicated_array type: {jax.typeof(replicated_array)}\")\n print(f\"sharded_array type: {jax.typeof(sharded_array)}\")\n ```\n \n-We should read the type `f32[4@X, 2]` as \"a 4-by-2 array of 32-bit floats whose first dimension\n+We should read the type `int32[4@X, 2]` as \"a 4-by-2 array of 32-bit ints whose first dimension\n is sharded along mesh axis 'X'. The array is replicated along all other mesh\n axes\"\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reverts 06448864abd6e8187e5b4d9b1ff08ab14fe3b8e0\n\nPiperOrigin-RevId: 761237485",
        "output": [
            "```diff\nCommit: 12e07c963286c9fa7eb7d8651ce968537be1ab8a\nDate: 2025-05-20T21:44:40Z\nURL: https://github.com/jax-ml/jax/commit/12e07c963286c9fa7eb7d8651ce968537be1ab8a\nFiles changed: 4\nAdditions: +11, Deletions: -54\ndiff --git a/jax/_src/compiler.py b/jax/_src/compiler.py\nindex e8ef647a1312..04f993fed799 100644\n--- a/jax/_src/compiler.py\n+++ b/jax/_src/compiler.py\n@@ -292,19 +292,6 @@ def backend_compile(\n     executable_devices: xc.DeviceList,\n     options: xc.CompileOptions,\n     host_callbacks: Sequence[Any],\n-) -> xc.LoadedExecutable:\n-  return backend_compile_and_load(\n-      backend, module, executable_devices, options, host_callbacks\n-  )\n-\n-\n-@profiler.annotate_function\n-def backend_compile_and_load(\n-    backend: xc.Client,\n-    module: ir.Module,\n-    executable_devices: xc.DeviceList,\n-    options: xc.CompileOptions,\n-    host_callbacks: Sequence[Any],\n ) -> xc.LoadedExecutable:\n   sym_name = module.operation.attributes['sym_name']\n   module_name = ir.StringAttr(sym_name).value\n@@ -335,35 +322,18 @@ def backend_compile_and_load(\n \n     # we use a separate function call to ensure that XLA compilation appears\n     # separately in Python profiling results\n-    elif jaxlib_extension_version < 342 or isinstance(backend, xc.CompileOnlyPyClient):\n-      if host_callbacks:\n-        return backend.compile(\n-            built_c,\n-            executable_devices=executable_devices,  # type: ignore\n-            compile_options=options,\n-            host_callbacks=host_callbacks,\n-        )\n-      # Some backends don't have `host_callbacks` option yet\n-      # TODO(sharadmv): remove this fallback when all backends allow `compile`\n-      # to take in `host_callbacks`\n+    if host_callbacks:\n       return backend.compile(\n-          built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n-    else:\n-      if host_callbacks:\n-        return backend.compile_and_load(\n-            built_c,\n-            executable_devices=executable_devices,\n-            compile_options=options,\n-            host_callbacks=host_callbacks,\n-        )\n-      # Some backends don't have `host_callbacks` option yet\n-      # TODO(sharadmv): remove this fallback when all backends allow `compile`\n-      # to take in `host_callbacks`\n-      return backend.compile_and_load(\n           built_c,\n-          executable_devices=executable_devices,\n+          executable_devices=executable_devices,  # type: ignore\n           compile_options=options,\n+          host_callbacks=host_callbacks,\n       )\n+    # Some backends don't have `host_callbacks` option yet\n+    # TODO(sharadmv): remove this fallback when all backends allow `compile`\n+    # to take in `host_callbacks`\n+    return backend.compile(\n+        built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n   except xc.XlaRuntimeError as e:\n     for error_handler in _XLA_RUNTIME_ERROR_HANDLERS:\n       handler_result = error_handler(e)\n@@ -428,7 +398,7 @@ def compile_or_get_cached(\n   )\n \n   if cache_key is None:\n-    return backend_compile_and_load(\n+    return backend_compile(\n         backend, computation, executable_devices, compile_options,\n         host_callbacks)\n \n@@ -456,7 +426,7 @@ def compile_or_get_cached(\n       config.share_binary_between_hosts.value\n       and is_multi_process\n       and distributed.global_state.client is not None\n-      # Host callbacks are currently baked into the HLO module so we can't share\n+      # Host callbacks are currently baked into the HLO module so we cant share\n       # them.\n       and len(host_callbacks) == 0\n   ):\n@@ -746,7 +716,7 @@ def _compile_and_write_cache(\n     cache_key: str,\n ) -> xc.LoadedExecutable:\n   start_time = time.monotonic()\n-  executable = backend_compile_and_load(\n+  executable = backend_compile(\n       backend, computation, executable_devices, compile_options, host_callbacks\n   )\n   compile_time = time.monotonic() - start_time\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 000c05acacad..1d7f3042e8a3 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -551,17 +551,6 @@ class Client:\n   ) -> PjRtLayout: ...\n   def __getattr__(self, name: str) -> Any: ...\n \n-\n-class CompileOnlyPyClient(Client):\n-  def compile(\n-      self,\n-      computation: str | bytes,\n-      executable_devices: DeviceList | Sequence[Device],\n-      compile_options: CompileOptions = ...,\n-      host_callbacks: Sequence[Any] = ...,\n-  ) -> LoadedExecutable: ...\n-\n-\n class CpuCollectives: ...\n \n def make_gloo_tcp_collectives(\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex b1bbc464610e..8f8c829ee6c7 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -304,7 +304,6 @@ def computation_count():\n \n XlaComputation = _xla.XlaComputation\n Client = _xla.Client\n-CompileOnlyPyClient = _xla.CompileOnlyPyClient\n Memory = _xla.Memory\n Array = _xla.Array\n ArrayImpl = _xla.ArrayImpl\ndiff --git a/jaxlib/xla_client.pyi b/jaxlib/xla_client.pyi\nindex fce114f45474..80599e86676b 100644\n--- a/jaxlib/xla_client.pyi\n+++ b/jaxlib/xla_client.pyi\n@@ -24,7 +24,6 @@ from jaxlib._jax import ArrayCopySemantics as ArrayCopySemantics\n from jaxlib._jax import ArrayImpl as ArrayImpl\n from jaxlib._jax import AutotuneCacheMode as AutotuneCacheMode\n from jaxlib._jax import Client as Client\n-from jaxlib._jax import CompileOnlyPyClient as CompileOnlyPyClient\n from jaxlib._jax import CompileOptions as CompileOptions\n from jaxlib._jax import Device as Device\n from jaxlib._jax import DeviceAssignment as DeviceAssignment\n\n```"
        ],
        "from_id": [
            "hawkinsp",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Make sure tests with `--build_jaxlib=false` depend on NVIDIA CUDA wheels hermetically.\n\nPiperOrigin-RevId: 761231648",
        "output": [
            "```diff\nCommit: 5d3134e9fa3d40f0a24ce08a6225d507d65b634c\nDate: 2025-05-20T21:28:35Z\nURL: https://github.com/jax-ml/jax/commit/5d3134e9fa3d40f0a24ce08a6225d507d65b634c\nFiles changed: 2\nAdditions: +15, Deletions: -2\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex a8dc67eb3804..eceb38e35aab 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -190,8 +190,8 @@ def _gpu_test_deps():\n             \"@pypi//nvidia_nvshmem_cu12\",\n         ],\n         \"//jax:config_build_jaxlib_false\": [\n-            \"@pypi//jax_cuda12_plugin\",\n-            \"@pypi//jax_cuda12_pjrt\",\n+            \"//jaxlib/tools:pypi_jax_cuda_plugin_with_cuda_deps\",\n+            \"//jaxlib/tools:pypi_jax_cuda_pjrt_with_cuda_deps\",\n             \"@pypi//nvidia_nvshmem_cu12\",\n         ],\n         \"//jax:config_build_jaxlib_wheel\": [\ndiff --git a/jaxlib/tools/BUILD.bazel b/jaxlib/tools/BUILD.bazel\nindex 22bae26a4420..d6a5f94dfd4b 100644\n--- a/jaxlib/tools/BUILD.bazel\n+++ b/jaxlib/tools/BUILD.bazel\n@@ -487,6 +487,19 @@ py_import(\n     wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n )\n \n+# The targets below are used for GPU tests with `--//jax:build_jaxlib=false`.\n+py_import(\n+    name = \"pypi_jax_cuda_plugin_with_cuda_deps\",\n+    wheel = \"@pypi_jax_cuda12_plugin//:whl\",\n+    wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n+)\n+\n+py_import(\n+    name = \"pypi_jax_cuda_pjrt_with_cuda_deps\",\n+    wheel = \"@pypi_jax_cuda12_pjrt//:whl\",\n+    wheel_deps = if_cuda([\":nvidia_wheel_deps\"]),\n+)\n+\n # Wheel tests.\n \n AARCH64_MANYLINUX_TAG = \"_\".join(PLATFORM_TAGS_DICT[(\"Linux\", \"aarch64\")])\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic:TPU] Enforce that tpu.dynamic_gather operands and result have the same shape\n\nPiperOrigin-RevId: 761230458",
        "output": [
            "```diff\nCommit: 4ff6eb25f21e7a8296bcb6f8adbc5ffec4e8ae6b\nDate: 2025-05-20T21:25:26Z\nURL: https://github.com/jax-ml/jax/commit/4ff6eb25f21e7a8296bcb6f8adbc5ffec4e8ae6b\nFiles changed: 3\nAdditions: +23, Deletions: -10\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex a9fbf8dcd982..eb5e6df7b381 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -2369,7 +2369,7 @@ def _gather_lowering_rule(\n         operand_batching_dims=(1,),\n         start_indices_batching_dims=(1,),\n     ):\n-      return tpu.dynamic_gather(out_type, x, recovered_indices, 0)\n+      return tpu.dynamic_gather(x, recovered_indices, 0)\n     if dimension_numbers == lax.GatherDimensionNumbers(\n         offset_dims=(),\n         collapsed_slice_dims=(1,),\n@@ -2377,7 +2377,7 @@ def _gather_lowering_rule(\n         operand_batching_dims=(0,),\n         start_indices_batching_dims=(0,),\n     ):\n-      return tpu.dynamic_gather(out_type, x, recovered_indices, 1)\n+      return tpu.dynamic_gather(x, recovered_indices, 1)\n   raise NotImplementedError(\"Unsupported gather\")\n \n \ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu.td b/jaxlib/mosaic/dialect/tpu/tpu.td\nindex 29ce9c84de07..b6ae1e52e822 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu.td\n+++ b/jaxlib/mosaic/dialect/tpu/tpu.td\n@@ -466,10 +466,19 @@ def TPU_GatherOp : TPU_Op<\"gather\", [Pure]> {\n   }];\n }\n \n-def TPU_DynamicGatherOp : TPU_Op<\"dynamic_gather\", [Pure]> {\n+def TPU_DynamicGatherOp : TPU_Op<\"dynamic_gather\", [Pure, SameOperandsAndResultShape, AllTypesMatch<[\"source\", \"output\"]>]> {\n+  let description = [{\n+    Gathers elements from `source` using `indices`.\n+\n+    Given a shape `N0 x N1 x ...`, `output[i0, i1, ...]` is given by\n+    `input[j0, j1, ...]` where `jn = indices[i0, i1, ...] mod Ni` for\n+    `n = dimension` and `jn = in` otherwise.\n+\n+    Similar to `np.take_along_axis`, except that OOB indices wrap.\n+  }];\n   let arguments = (ins\n     AnyVectorOfNonZeroRank:$source,\n-    AnyVectorOfNonZeroRank:$indices,\n+    VectorOfNonZeroRankOf<[AnyInteger]>:$indices,\n     I32Attr:$dimension\n   );\n   let results = (outs AnyVectorOfNonZeroRank:$output);\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 5ddff9d9ee53..fa14c8ef9238 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -3537,12 +3537,13 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n                  std::array{false, true}) {  // Lane broadcast\n         TPU_ASSERT_EQ_OP(*(src_tiles.dimensions().end() - 1), 1);\n         TPU_ASSERT_OP(offsets_in[1].has_value());\n+        VectorType i32_vreg_ty =\n+            getNativeVregType(builder.getI32Type(), ctx.target_shape);\n         const int64_t offset = *offsets_in[1];\n         const int64_t lane_offset = offset % ctx.target_shape[1];\n         const int64_t tile_offset = offset / ctx.target_shape[1];\n         Value lane_offset_cst = getFullVector(\n-            builder, getNativeVregType(builder.getI32Type(), ctx.target_shape),\n-            builder.getI32IntegerAttr(lane_offset));\n+            builder, i32_vreg_ty, builder.getI32IntegerAttr(lane_offset));\n         DenseI32ArrayAttr sublane_pattern;\n         if (num_tiles != 1) {\n           SmallVector<int32_t> pattern;\n@@ -3555,7 +3556,7 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n           sublane_pattern = builder.getDenseI32ArrayAttr(pattern);\n         }\n         src_tiles.Each([&](const absl::Span<const int64_t> src_idx,\n-                           Value *const src_tile) {\n+                           Value *const src_vreg) {\n           SmallVector<int64_t> dst_starts(dst_tiles_implicit_shape.size());\n           SmallVector<int64_t> dst_limits(dst_tiles_implicit_shape.size());\n           for (int64_t i = 0; i < dst_tiles.num_dimensions(); ++i) {\n@@ -3567,10 +3568,13 @@ LogicalResult vector_broadcast_rule(RewriteContext &ctx, Operation &op,\n               dst_limits[i] = dst_starts[i] + 1;\n             }\n           }\n-          Value res_vreg = builder.create<tpu::DynamicGatherOp>(\n-              broadcast_op.getLoc(), src_tile->getType(), *src_tile,\n-              lane_offset_cst,\n+          Value src_vreg_i32 =\n+              builder.create<tpu::BitcastVregOp>(i32_vreg_ty, *src_vreg);\n+          Value res_vreg_i32 = builder.create<tpu::DynamicGatherOp>(\n+              broadcast_op.getLoc(), i32_vreg_ty, src_vreg_i32, lane_offset_cst,\n               /*dimension=*/1);\n+          Value res_vreg = builder.create<tpu::BitcastVregOp>(\n+              src_vreg->getType(), res_vreg_i32);\n           if (num_tiles != 1) {\n             res_vreg = builder.create<tpu::GatherOp>(\n                 broadcast_op.getLoc(), res_vreg.getType(), res_vreg,\n\n```"
        ],
        "from_id": [
            "tlongeri",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[ragged-paged-attn] Apply kv mask to filter out NaNs\n\nExpected small regression as we insert kv masking logic which needs to unpack/pack.\n\nPiperOrigin-RevId: 761217303",
        "output": [
            "```diff\nCommit: 11cf85deb0776f800144e829d4ba8e38eb9d76fc\nDate: 2025-05-20T20:53:11Z\nURL: https://github.com/jax-ml/jax/commit/11cf85deb0776f800144e829d4ba8e38eb9d76fc\nFiles changed: 2\nAdditions: +76, Deletions: -40\ndiff --git a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\nindex cd5de96ccca7..df47674a59a9 100644\n--- a/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n+++ b/jax/experimental/pallas/ops/tpu/ragged_paged_attention/kernel.py\n@@ -39,19 +39,16 @@ def __init__(\n       vmem_buf,  # [num_kv_pages_per_blk, page_size, num_combined_kv_heads_per_blk, head_dim]\n       sem,\n       page_indices_ref,  # i32[max_num_seqs, pages_per_seq]\n-      offset,  # [seq_idx, kv_pages_start]\n+      metadata,  # [seq_idx, start_page_idx, end_page_idx]\n   ):\n     self._vmem_buf = vmem_buf\n-    seq_id, kv_pages_start = offset\n-    pages_per_seq = page_indices_ref.shape[1]\n+    seq_id, start_page_idx, end_page_idx = metadata\n     self._async_copies = []\n     # TODO(jevinjiang): Only fetch dynamic shape in need! This will insert\n     # a bunch of if-ops. Check the performance when we have benchmarking setup.\n     for i in range(vmem_buf.shape[0]):\n-      page_idx = kv_pages_start + i\n-      page_idx = jax.lax.select(\n-          page_idx < pages_per_seq, page_idx, pages_per_seq - 1\n-      )\n+      page_idx = start_page_idx + i\n+      page_idx = jax.lax.select(page_idx < end_page_idx, page_idx, 0)\n       self._async_copies.append(\n           pltpu.make_async_copy(\n               pages_hbm_ref.at[page_indices_ref[seq_id, page_idx]],\n@@ -298,6 +295,7 @@ def ragged_paged_attention_kernel(\n   if mask_value is None:\n     mask_value = DEFAULT_MASK_VALUE\n   num_q_per_blk, num_q_heads_per_blk, head_dim = q_ref.shape\n+  pages_per_seq = page_indices_ref.shape[-1]\n   num_seqs = num_seqs_ref[0]\n   _, num_kv_pages_per_blk, page_size, num_combined_kv_heads_per_blk, _ = (\n       kv_bufs.shape\n@@ -318,7 +316,11 @@ def ragged_paged_attention_kernel(\n   def create_kv_async_copy_descriptors(\n       heads_blk_idx, seq_idx, kv_blk_idx, buf_idx\n   ):\n-    offset = (seq_idx, kv_blk_idx * num_kv_pages_per_blk)\n+    start_kv_page_idx = kv_blk_idx * num_kv_pages_per_blk\n+    end_kv_page_idx = jnp.minimum(\n+        pages_per_seq, cdiv(kv_lens_ref[seq_idx], page_size)\n+    )\n+    metadata = (seq_idx, start_kv_page_idx, end_kv_page_idx)\n     heads_start = heads_blk_idx * num_combined_kv_heads_per_blk\n     async_copy_kv = MultiPageAsyncCopyDescriptor(\n         kv_pages_hbm_ref.at[\n@@ -327,7 +329,7 @@ def create_kv_async_copy_descriptors(\n         kv_bufs.at[buf_idx],\n         sems.at[buf_idx],\n         page_indices_ref,\n-        offset,\n+        metadata,\n     )\n     return async_copy_kv\n \n@@ -423,18 +425,22 @@ def flash_attention(\n           num_q_per_blk * num_q_heads_per_kv_head,\n           head_dim,\n       )\n-      assert k.shape == (\n-          num_kv_per_blk,\n-          head_dim,\n-      ), f\"{k.shape=}, {(num_kv_per_blk, head_dim)=} {k.dtype=}\"\n-      assert v.shape == (num_kv_per_blk, head_dim)\n-      assert head_m_ref.shape == (\n-          num_q_per_blk * num_q_heads_per_kv_head,\n-          128,\n+      assert (\n+          k.shape\n+          == v.shape\n+          == (\n+              num_kv_per_blk,\n+              head_dim,\n+          )\n       )\n-      assert head_l_ref.shape == (\n-          num_q_per_blk * num_q_heads_per_kv_head,\n-          128,\n+      assert k.dtype == v.dtype\n+      assert (\n+          head_m_ref.shape\n+          == head_l_ref.shape\n+          == (\n+              num_q_per_blk * num_q_heads_per_kv_head,\n+              128,\n+          )\n       )\n       assert head_acc_ref.shape == (\n           num_q_per_blk,\n@@ -448,6 +454,13 @@ def masked_store(ref, val, start, end, group=1):\n         mask = jnp.logical_and(iota >= start, iota < end)\n         pl.store(ref, idx=tuple(slice(None) for _ in ref.shape), val=val, mask=mask)\n \n+      # kv lens will be contracting dim, we should mask out the NaNs.\n+      kv_mask = (\n+          lax.broadcasted_iota(jnp.int32, k.shape, 0) < kv_len - kv_len_start\n+      )\n+      k = jnp.where(kv_mask, k.astype(jnp.float32), 0).astype(k.dtype)\n+      v = jnp.where(kv_mask, v.astype(jnp.float32), 0).astype(v.dtype)\n+\n       qk = (\n           jnp.einsum(\"nd,md->nm\", q, k, preferred_element_type=jnp.float32)\n           * sm_scale\n@@ -709,7 +722,7 @@ def ragged_paged_attention(\n \n   Args:\n     q: concatenated all sequences' queries.\n-    kv_pages: paged K cache. Normally in HBM.\n+    kv_pages: paged KV cache. Normally in HBM.\n     kv_lens: padded kv lengths. Only the first num_seqs values are valid.\n     page_indices: the first index indicates which page to use in the kv cache\n       for each sequence. Only the first num_seqs values are valid.\ndiff --git a/tests/pallas/tpu_ragged_paged_attention_test.py b/tests/pallas/tpu_ragged_paged_attention_test.py\nindex f86d54575519..4265445c69c7 100644\n--- a/tests/pallas/tpu_ragged_paged_attention_test.py\n+++ b/tests/pallas/tpu_ragged_paged_attention_test.py\n@@ -19,6 +19,7 @@\n import jax\n from jax._src import test_util as jtu\n from jax.experimental.pallas.ops.tpu.ragged_paged_attention import (\n+    cdiv,\n     dynamic_validate_inputs,\n     ragged_paged_attention,\n     ref_ragged_paged_attention,\n@@ -29,13 +30,8 @@\n jax.config.parse_flags_with_absl()\n \n \n-def ceil_div(x, a):\n-  assert a != 0\n-  return (x + a - 1) // a\n-\n-\n @jtu.with_config(jax_numpy_dtype_promotion=\"standard\")\n-class PagedAttentionKernelTest(jtu.JaxTestCase):\n+class RaggedPagedAttentionKernelTest(jtu.JaxTestCase):\n \n   def _test_ragged_paged_attention(\n       self,\n@@ -66,29 +62,56 @@ def _test_ragged_paged_attention(\n     max_num_batched_tokens = max(cu_q_lens[-1], max_num_batched_tokens)\n     max_num_seq = max(len(seq_lens), max_num_seq)\n     max_kv_len = max(kv_lens)\n-    pages_per_seq = ceil_div(max_kv_len, page_size)\n+    pages_per_seq = cdiv(max_kv_len, page_size)\n     num_q_heads, num_kv_heads = num_heads\n \n-    cu_q_lens = jnp.array(cu_q_lens, dtype=jnp.int32)\n-    kv_lens = jnp.array(kv_lens, dtype=jnp.int32)\n-    cu_q_lens = jnp.pad(cu_q_lens, (0, max_num_seq + 1 - cu_q_lens.shape[0]))\n-    kv_lens = jnp.pad(kv_lens, (0, max_num_seq - kv_lens.shape[0]))\n     prng_key = jax.random.key(1234)\n-    k0, k1, k2 = jax.random.split(prng_key, 3)\n+    k0, k1 = jax.random.split(prng_key, 2)\n     q = jax.random.normal(\n         k0,\n         (max_num_batched_tokens, num_q_heads, head_dim),\n         dtype=dtype,\n     )\n-    kv_pages = jax.random.normal(\n-        k1,\n-        (num_pages, page_size, num_kv_heads * 2, head_dim),\n-        dtype=dtype,\n+    page_cnt = 0\n+    page_indices_list = []\n+    kv_pages_list = []\n+    for kv_len in kv_lens:\n+      kv = jax.random.normal(\n+          k1,\n+          (kv_len, num_kv_heads * 2, head_dim),\n+          dtype=dtype,\n+      )\n+      kv = jnp.pad(\n+          kv,\n+          ((0, cdiv(kv_len, page_size) * page_size - kv_len), (0, 0), (0, 0)),\n+          constant_values=jnp.nan,\n+      ).reshape(-1, page_size, num_kv_heads * 2, head_dim)\n+      indices = page_cnt + jnp.arange(kv.shape[0], dtype=jnp.int32)\n+      indices = jnp.pad(\n+          indices,\n+          ((0, pages_per_seq - indices.shape[0]),),\n+          constant_values=jnp.nan,\n+      )\n+      page_indices_list.append(indices)\n+      page_cnt += kv.shape[0]\n+      kv_pages_list.append(kv)\n+\n+    kv_pages = jnp.concatenate(kv_pages_list, axis=0)\n+    kv_pages = jnp.pad(\n+        kv_pages,\n+        ((0, num_pages - kv_pages.shape[0]), (0, 0), (0, 0), (0, 0)),\n+        constant_values=jnp.nan,\n     )\n-    page_indices = jax.random.randint(\n-        k2, (max_num_seq, pages_per_seq), 0, num_pages, dtype=jnp.int32\n+    page_indices = jnp.stack(page_indices_list, axis=0)\n+    page_indices = jnp.pad(\n+        page_indices,\n+        ((0, max_num_seq - page_indices.shape[0]), (0, 0)),\n+        constant_values=jnp.nan,\n     )\n-\n+    cu_q_lens = jnp.array(cu_q_lens, dtype=jnp.int32)\n+    cu_q_lens = jnp.pad(cu_q_lens, (0, max_num_seq + 1 - cu_q_lens.shape[0]))\n+    kv_lens = jnp.array(kv_lens, dtype=jnp.int32)\n+    kv_lens = jnp.pad(kv_lens, (0, max_num_seq - kv_lens.shape[0]))\n     num_seqs = jnp.array([len(seq_lens)], dtype=jnp.int32)\n \n     dynamic_validate_inputs(\n\n```"
        ],
        "from_id": [
            "bythew3i",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[JAX] Enable tfrt gpu in jax multi platform test\n\nPiperOrigin-RevId: 761204561",
        "output": [
            "```diff\nCommit: 048db94ed914fd656818f21efd70d7356326a893\nDate: 2025-05-20T20:20:38Z\nURL: https://github.com/jax-ml/jax/commit/048db94ed914fd656818f21efd70d7356326a893\nFiles changed: 1\nAdditions: +11, Deletions: -1\ndiff --git a/tests/BUILD b/tests/BUILD\nindex 1b33f292d503..aa777080fd92 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -121,6 +121,10 @@ jax_py_test(\n jax_multiplatform_test(\n     name = \"array_interoperability_test\",\n     srcs = [\"array_interoperability_test.py\"],\n+    disable_configs = [\n+        \"gpu_h100_tfrt\",  # TODO(b/411472145): Re-enable once fixed.\n+        \"gpu_h100x2_tfrt\",\n+    ],\n     enable_backends = [\n         \"cpu\",\n         \"gpu\",\n@@ -128,7 +132,9 @@ jax_multiplatform_test(\n     enable_configs = [\n         \"gpu_h100x2\",\n     ],\n-    tags = [\"multiaccelerator\"],\n+    tags = [\n+        \"multiaccelerator\",\n+    ],\n     deps = py_deps([\n         \"absl/testing\",\n         \"numpy\",\n@@ -1134,6 +1140,10 @@ jax_multiplatform_test(\n jax_multiplatform_test(\n     name = \"pytorch_interoperability_test\",\n     srcs = [\"pytorch_interoperability_test.py\"],\n+    disable_configs = [\n+        \"gpu_h100_tfrt\",  # TODO(b/411472145): Re-enable once fixed.\n+        \"gpu_h100x2_tfrt\",\n+    ],\n     enable_backends = [\n         \"cpu\",\n         \"gpu\",\n\n```"
        ],
        "from_id": [
            "sizhit2",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Remove pspec -> names conversion during `shard_map_p.bind` and instead preserve partition specs everywhere internally.\n\n**This is because spec -> names canonicalization gets rid of unreduced axes present on PartitionSpecs and we want to preserve that**. We can thread 2 new parameters called `in_unreduced` and `out_unreduced` and keep `in_names`, `out_names` but that doesn't buy us anything except for more lines added and complexity :)\n\nIt's better to just use pspecs everywhere. It's a net reduction in lines of code too!\n\nPiperOrigin-RevId: 761196531",
        "output": [
            "```diff\nCommit: e896282219481c3c6edbd1334c186c7bfbdbdae6\nDate: 2025-05-20T19:59:40Z\nURL: https://github.com/jax-ml/jax/commit/e896282219481c3c6edbd1334c186c7bfbdbdae6\nFiles changed: 6\nAdditions: +258, Deletions: -278\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex f26b4222b23b..0061c9c63f7b 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -53,6 +53,7 @@\n from jax._src.tree_util import tree_map\n from jax._src.tree_util import tree_unflatten\n from jax._src.typing import Array\n+from jax._src.partition_spec import PartitionSpec as P\n from jax._src.util import (as_hashable_function, split_list, safe_map, safe_zip,\n                            unzip3, weakref_lru_cache, HashableWrapper, foreach)\n \n@@ -958,7 +959,7 @@ def remat_error_check(error, enabled_errors, *vals_in, jaxpr, **params):\n \n def shard_map_error_check(\n     error: Error, enabled_errors, *vals_in,\n-    jaxpr: core.Jaxpr, in_names, out_names, **kwargs\n+    jaxpr: core.Jaxpr, in_specs, out_specs, **kwargs\n ):\n   if (mesh := kwargs.get('mesh')) is None:\n     raise ValueError('Mesh must be provided for shard_map with checkify.')\n@@ -966,7 +967,7 @@ def shard_map_error_check(\n   err_vals, err_tree = jtu.tree_flatten(error)\n   num_error_vals = len(err_vals)\n   # Replicated sharding for in errors.\n-  new_in_names = (*([{}] * num_error_vals), *in_names)\n+  new_in_specs = (*([P()] * num_error_vals), *in_specs)\n   new_vals_in = [*err_vals, *vals_in]\n   in_avals = list(map(core.get_aval, new_vals_in))\n   manual_axes = kwargs.get('manual_axes')\n@@ -974,7 +975,7 @@ def shard_map_error_check(\n   for i, v in enumerate(in_avals):\n     if not (sharder := core.shard_aval_handlers.get(type(v))):\n       raise ValueError(f'Unsupported aval type: {type(v)}')\n-    in_avals[i] = sharder(mesh, manual_axes, check_vma, new_in_names[i], v)\n+    in_avals[i] = sharder(mesh, manual_axes, check_vma, new_in_specs[i], v)\n \n   with (jshmap._extend_axis_env(mesh, manual_axes),\n         mesh_lib.use_abstract_mesh(jshmap._as_manual_mesh(mesh, manual_axes)),  # type: ignore[arg-type]\n@@ -983,7 +984,7 @@ def shard_map_error_check(\n     checked_jaxpr, out_tree, _ = jaxpr_to_checkify_jaxpr(\n         pe.close_jaxpr(jaxpr), enabled_errors, err_tree, *in_avals\n     )\n-  num_out_error_vals = out_tree.num_leaves - len(out_names)\n+  num_out_error_vals = out_tree.num_leaves - len(out_specs)\n \n   def expand_errors_leading_dim(*xs):\n     outs = core.eval_jaxpr(checked_jaxpr.jaxpr, checked_jaxpr.consts, *xs)\n@@ -1001,15 +1002,15 @@ def expand_errors_leading_dim(*xs):\n \n   # Update shard_map params to account for extra error values.\n   # Use fully sharded partitioning for out errors.\n-  new_out_names = (*([{0: mesh.axis_names}] * num_out_error_vals), *out_names)\n+  new_out_specs = (*([P(mesh.axis_names)] * num_out_error_vals), *out_specs)\n   subfun = lu.hashable_partial(\n       lu.wrap_init(core.eval_jaxpr, debug_info=checked_jaxpr.jaxpr.debug_info),\n       checked_jaxpr.jaxpr, checked_jaxpr.consts\n   )\n   new_params = dict(\n       jaxpr=checked_jaxpr.jaxpr,\n-      in_names=new_in_names,\n-      out_names=new_out_names,\n+      in_specs=new_in_specs,\n+      out_specs=new_out_specs,\n       **kwargs,\n   )\n   _, new_params = jshmap.shard_map_p.get_bind_params(new_params)\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex 1ab560fb58c5..8f553ea884d7 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -264,14 +264,12 @@ def get_intermediate_shardings(\n       out.extend((i, source_info) for i in eqn.params['in_shardings'])\n       out.extend((o, source_info) for o in eqn.params['out_shardings'])\n     elif eqn.primitive is shard_map.shard_map_p:\n-      if isinstance(eqn.params['mesh'], AbstractMesh):\n+      mesh = eqn.params['mesh']\n+      if isinstance(mesh, AbstractMesh):\n         continue\n       source_info = SourceInfo(eqn.source_info, eqn.primitive.name)\n-      def _names_to_pspec(names):\n-        ndmin = max(names) + 1 if names else 0\n-        return PartitionSpec(*(names.get(i) for i in range(ndmin)))\n-      out.extend((NamedSharding(eqn.params['mesh'], _names_to_pspec(names)), source_info)\n-                 for names in [*eqn.params['in_names'], *eqn.params['out_names']])\n+      out.extend((NamedSharding(mesh, spec), source_info)\n+                 for spec in [*eqn.params['in_specs'], *eqn.params['out_specs']])\n     elif eqn.primitive is device_put_p:\n       source_info = SourceInfo(eqn.source_info, eqn.primitive.name)\n       out.extend((s, source_info) for s in eqn.params['devices']\ndiff --git a/jax/_src/interpreters/pxla.py b/jax/_src/interpreters/pxla.py\nindex a7782063491c..4a21fae59e52 100644\n--- a/jax/_src/interpreters/pxla.py\n+++ b/jax/_src/interpreters/pxla.py\n@@ -72,7 +72,8 @@\n     PartitionSpec as P)\n from jax._src.util import (safe_map, safe_zip, partition_list, wrap_name,\n                            tuple_update, tuple_delete, distributed_debug_log,\n-                           unzip2, HashableFunction, weakref_lru_cache)\n+                           unzip2, HashableFunction, weakref_lru_cache,\n+                           tuple_insert)\n from jax._src.state.types import AbstractRef, RefEffect\n \n \n@@ -3339,6 +3340,12 @@ def check_array_xla_sharding_layout_match(\n           \"compiled with. \"\n           f\"Here are {num_mismatch_str}:\\n{str_errors}\")\n \n+def batch_spec(spec, dim, val):\n+  too_short = dim - len(spec)\n+  if too_short > 0:\n+    spec += (None,) * too_short\n+  new_partitions = tuple_insert(spec, dim, val)  # type: ignore\n+  return PartitionSpec(*new_partitions)\n \n def get_array_mapping(pspec: PartitionSpec) -> ArrayMappingOrAutoOrUnspecified:\n   pspec = sharding_impls.prepare_axis_resources(pspec, \"pspec to array_mapping\")\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 10e7e697e706..de01f4c05983 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -77,7 +77,7 @@\n     treedef_children, broadcast_prefix, all_leaves, prefix_errors, keystr,\n     PyTreeDef, none_leaf_registry as none_lr, tree_map, tree_flatten_with_path)\n from jax._src.util import (\n-    HashableFunction, safe_map, safe_zip, wraps, tuple_insert,\n+    HashableFunction, safe_map, safe_zip, wraps,\n     distributed_debug_log, split_list, split_list_checked, weakref_lru_cache,\n     merge_lists, subs_list, fun_name, fun_qual_name)\n from jax._src.attrs import (Box, List, dne_sentinel, jax_setattr, jax_getattr,\n@@ -2190,12 +2190,6 @@ def _pjit_batcher(axis_data, vals_in,\n batching.fancy_primitive_batchers[pjit_p] = _pjit_batcher\n batching.ragged_prop_rules[pjit_p] = batching.ragged_mask_no_op_rule\n \n-def _insert_axis_partitions(spec, dim, val):\n-  too_short = dim - len(spec)\n-  if too_short > 0:\n-    spec += (None,) * too_short\n-  new_partitions = tuple_insert(spec, dim, val)  # type: ignore\n-  return PartitionSpec(*new_partitions)\n \n def _pjit_batcher_for_sharding(\n     s: Sharding | UnspecifiedValue,\n@@ -2209,7 +2203,7 @@ def _pjit_batcher_for_sharding(\n       return s\n     if isinstance(s, NamedSharding) and isinstance(s.mesh, AbstractMesh):\n       return NamedSharding(\n-          s.mesh, _insert_axis_partitions(s.spec, dim, PartitionSpec.UNCONSTRAINED))\n+          s.mesh, pxla.batch_spec(s.spec, dim, PartitionSpec.UNCONSTRAINED))\n     new_op = hlo_s.to_proto().clone()\n     tad = list(new_op.tile_assignment_dimensions)\n     tad.insert(dim, 1)  # type: ignore\n@@ -2221,7 +2215,7 @@ def _pjit_batcher_for_sharding(\n   else:\n     if isinstance(s, NamedSharding) and isinstance(s.mesh, AbstractMesh):\n       return NamedSharding(\n-          s.mesh, _insert_axis_partitions(s.spec, dim, spmd_axis_name))\n+          s.mesh, pxla.batch_spec(s.spec, dim, spmd_axis_name))\n     if isinstance(s, NamedSharding):\n       mesh = s.mesh\n     if mesh is None or mesh.empty:\n@@ -2234,7 +2228,7 @@ def _pjit_batcher_for_sharding(\n           f' manager scope{s!r}')\n     spec = parse_flatten_op_sharding(hlo_s, mesh)[0]\n     return NamedSharding(\n-        mesh, _insert_axis_partitions(spec, dim, spmd_axis_name))\n+        mesh, pxla.batch_spec(spec, dim, spmd_axis_name))\n \n \n def _pjit_jvp(primals_in, tangents_in,\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 010773f74d0c..72e1420b0b2b 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -47,7 +47,8 @@\n from jax._src.lib.mlir.dialects import hlo, sdy\n from jax._src.util import (HashableFunction, HashablePartial, unzip2,\n                            as_hashable_function, memoize, partition_list,\n-                           merge_lists, split_list, subs_list2)\n+                           merge_lists, split_list, subs_list2,\n+                           fun_name as util_fun_name)\n from jax._src.interpreters import batching\n from jax._src.interpreters import mlir\n from jax._src.interpreters import partial_eval as pe\n@@ -209,11 +210,11 @@ def wrapped(*args):\n     dyn_argnums, in_specs_flat = unzip2((i, s) for i, s in enumerate(in_specs_flat)\n                                         if s is not None)\n     fun, args_flat = api_util.argnums_partial(fun, dyn_argnums, args_flat, False)\n-    _check_specs_vs_args(f, mesh, in_tree, in_specs, dyn_argnums, in_specs_flat, args_flat)\n-    in_names_flat = tuple(map(_canonicalize_spec, in_specs_flat))\n+    _check_specs_vs_args(f, mesh, in_tree, in_specs, dyn_argnums, in_specs_flat,\n+                         args_flat)\n \n     @memoize\n-    def out_names_thunk():\n+    def out_specs_thunk():\n       if callable(out_specs):\n         out_specs_ = out_specs()\n         _check_specs(SpecErrorType.out, out_specs_, axis_names)\n@@ -225,15 +226,15 @@ def out_names_thunk():\n       except ValueError:\n         e, *_ = prefix_errors(out_specs_, dummy)\n         raise e('shard_map out_specs') from None\n-      return tuple(map(_canonicalize_spec, out_specs_flat))\n+      return tuple(out_specs_flat)\n \n     if check_vma:\n-      fun = _implicit_pvary_on_output(fun, out_names_thunk)\n+      fun = _implicit_pvary_on_output(fun, out_specs_thunk)\n \n     try:\n       out_flat = shard_map_p.bind(\n-          fun, *args_flat, mesh=mesh, in_names=in_names_flat,\n-          out_names_thunk=out_names_thunk, check_vma=check_vma,\n+          fun, *args_flat, mesh=mesh, in_specs=in_specs_flat,\n+          out_specs_thunk=out_specs_thunk, check_vma=check_vma,\n           manual_axes=axis_names)\n     except _SpecError as e:\n       fails, = e.args\n@@ -305,16 +306,6 @@ def _shmap_checks(mesh, axis_names, in_specs, out_specs, _skip_mesh_check,\n     _check_specs(SpecErrorType.out, out_specs, axis_names)\n   return mesh, axis_names\n \n-\n-# Internally use AxisNames = dict[int, tuple[AxisName, ...]], not PartitionSpecs\n-AxisNames = dict[int, tuple[AxisName, ...]]  # TODO(mattjj): make it hashable\n-def _canonicalize_spec(spec: PartitionSpec) -> AxisNames:\n-  if isinstance(spec, PartitionSpec):\n-    return {i: names if isinstance(names, tuple) else (names,)\n-            for i, names in enumerate(spec) if names is not None}\n-  else:\n-    return spec\n-\n def _manual_spec(manual_axes, spec: P) -> P:\n   out = []  # type: ignore\n   for s in spec:\n@@ -391,7 +382,7 @@ def _check_specs_vs_args(\n     fail = _expand_fail(in_tree, dyn_argnums, fail)\n     msg = _spec_rank_error(SpecErrorType.input, f, in_tree, in_specs, fail)\n     raise ValueError(msg)\n-  in_names_flat = tuple(map(_canonicalize_spec, in_specs_flat))\n+  in_names_flat = tuple(map(_spec_to_names, in_specs_flat))\n   fail = [a if any(a.shape[d] % prod(mesh.shape[n] for n in ns)\n                    for d, ns in names.items()) else no_fail\n           for a, names in zip(in_avals, in_names_flat)]\n@@ -411,7 +402,7 @@ def _expand_fail(in_tree: PyTreeDef, dyn_argnums: Sequence[int],\n def _spec_rank_error(\n     error_type: SpecErrorType, f: Callable, tree: PyTreeDef, specs: Specs,\n     fails: list[core.ShapedArray | NoFail]) -> str:\n-  fun_name = getattr(f, '__name__', str(f))\n+  fun_name = util_fun_name(f)\n   if error_type == SpecErrorType.input:\n     prefix, base = 'in', 'args'\n     ba = _try_infer_args(f, tree)\n@@ -472,7 +463,7 @@ def _spec_divisibility_error(\n         extra = (f\", where args{arg_key} is the index \"\n                  f\"{arg_key.idx - len(ba.signature.parameters) + 1} component \"\n                  f\"of {fun_name}'s varargs parameter '{param.name}',\")\n-    names = _canonicalize_spec(spec)\n+    names = _spec_to_names(spec)\n     for d, ns in names.items():\n       if aval.shape[d] % prod(mesh.shape[n] for n in ns):\n         axis = f\"axes {ns}\" if len(ns) > 1 else f\"axis '{ns[0]}'\"\n@@ -504,8 +495,7 @@ def _inout_vma_error(f: Callable, mesh: Mesh | AbstractMesh, tree: PyTreeDef,\n   fun_name = getattr(f, '__name__', str(f))\n   msgs = []\n   for (spec_key, spec), (fail_key, vma) in _iter_paths(tree, specs, fails):\n-    dst = _canonicalize_spec(spec)\n-    unmentioned = _unmentioned(mesh, dst)\n+    unmentioned = _unmentioned(mesh, spec)\n     if len(unmentioned) > 1:\n       need_vma = ','.join(map(str, order_wrt_mesh(mesh, _spec_to_vma(spec))))\n       got_vma = ','.join(map(str, order_wrt_mesh(mesh, vma)))\n@@ -536,9 +526,9 @@ def _inout_vma_error(f: Callable, mesh: Mesh | AbstractMesh, tree: PyTreeDef,\n          \"check_vma=False argument to `jax.shard_map`.\")\n   return msg\n \n-def _unmentioned(mesh: Mesh | AbstractMesh, names: AxisNames) -> list[AxisName]:\n-  name_set = {n for ns in names.values() for n in ns}\n-  return [n for n in mesh.axis_names if n not in name_set]\n+def _unmentioned(mesh: Mesh | AbstractMesh, spec) -> list[AxisName]:\n+  vma_set = _spec_to_vma(spec)\n+  return [n for n in mesh.axis_names if n not in vma_set]\n \n \n def _try_infer_args(f, tree):\n@@ -563,10 +553,10 @@ def _iter_paths(tree: PyTreeDef, specs: Specs, fails: list[T | NoFail]\n # Primitive\n \n @lu.transformation2\n-def _implicit_pvary_on_output(f, out_names_thunk, *args, **kwargs):\n+def _implicit_pvary_on_output(f, out_specs_thunk, *args, **kwargs):\n   out_flat = f(*args, **kwargs)\n-  return [pvary(o, tuple(_names_to_vma(n) - typeof(o).vma))\n-          for o, n in zip(out_flat, out_names_thunk())]\n+  return [pvary(o, tuple(_spec_to_vma(sp) - typeof(o).vma))\n+          for o, sp in zip(out_flat, out_specs_thunk())]\n \n JaxType = Any\n MaybeTracer = Union[JaxType, Tracer]\n@@ -588,8 +578,8 @@ def get_bind_params(self, params):\n     subfun = lu.hashable_partial(lu.wrap_init(core.eval_jaxpr,\n                                               debug_info=jaxpr.debug_info),\n                                  jaxpr, ())\n-    axes = new_params.pop('out_names')\n-    new_params['out_names_thunk'] = HashableFunction(lambda: axes, closure=axes)\n+    axes = new_params.pop('out_specs')\n+    new_params['out_specs_thunk'] = HashableFunction(lambda: axes, closure=axes)\n     return [subfun], new_params\n \n shard_map_p = ShardMapPrimitive('shard_map')\n@@ -631,38 +621,35 @@ def _extend_axis_env(mesh, manual_axes):\n def _shard_map_staging(\n     trace: pe.DynamicJaxprTrace, prim: core.Primitive, f: lu.WrappedFun,\n     in_tracers: Sequence[Any], *, mesh: Mesh,\n-    in_names: tuple[AxisNames, ...],\n-    out_names_thunk: Callable[[], tuple[AxisNames, ...]],\n-    check_vma: bool,\n-    manual_axes: frozenset,\n+    in_specs, out_specs_thunk, check_vma: bool, manual_axes: frozenset,\n   ) -> Sequence[pe.DynamicJaxprTracer]:\n   source_info = source_info_util.current()\n   to_jaxpr_tracer = partial(trace.to_jaxpr_tracer, source_info=source_info)\n   in_tracers = map(to_jaxpr_tracer, in_tracers)\n   inner_mesh = _as_manual_mesh(mesh, manual_axes | set(mesh.manual_axes))\n   in_avals = [t.aval for t in in_tracers]\n-  in_avals_ = map(partial(_shard_aval, mesh, manual_axes, check_vma), in_names,\n+  in_avals_ = map(partial(_shard_aval, mesh, manual_axes, check_vma), in_specs,\n                   in_avals)\n   with (_extend_axis_env(mesh, manual_axes), use_abstract_mesh(inner_mesh),\n         config._check_vma(check_vma)):\n     jaxpr, out_avals_, consts, () = pe.trace_to_jaxpr_dynamic(f, in_avals_)\n-  _check_names(out_names_thunk(), out_avals_)\n+  _check_names(out_specs_thunk(), out_avals_)\n   if check_vma:\n     out_vma = [v.aval.vma for v in jaxpr.outvars]\n-    _check_vmas(mesh, out_names_thunk(), out_vma)\n+    _check_vmas(mesh, out_specs_thunk(), out_vma)\n   out_avals = map(_check_shapedarray, out_avals_)\n-  out_avals = [_check_shapedarray(_unshard_aval(mesh, check_vma, names, aval))\n-               for names, aval in zip(out_names_thunk(), out_avals)]\n+  out_avals = [_check_shapedarray(_unshard_aval(mesh, check_vma, spec, aval))\n+               for spec, aval in zip(out_specs_thunk(), out_avals)]\n   out_tracers = [pe.DynamicJaxprTracer(trace, a, source_info) for a in out_avals]\n   invars = map(trace.getvar, in_tracers)\n   constvars = map(trace.getvar, map(to_jaxpr_tracer, consts))\n   outvars = map(trace.makevar, out_tracers)\n-  in_names_staged = ({},) * len(consts) + tuple(in_names)  # type: ignore\n+  in_specs_staged = (P(),) * len(consts) + tuple(in_specs)  # type: ignore\n   with (_extend_axis_env(mesh, manual_axes), use_abstract_mesh(inner_mesh),\n         config._check_vma(check_vma)):\n     jaxpr = pe.convert_constvars_jaxpr(jaxpr)\n-  params = dict(mesh=mesh, in_names=in_names_staged,\n-                out_names=tuple(out_names_thunk()), jaxpr=jaxpr,\n+  params = dict(mesh=mesh, in_specs=in_specs_staged,\n+                out_specs=tuple(out_specs_thunk()), jaxpr=jaxpr,\n                 check_vma=check_vma, manual_axes=manual_axes)\n   effs = core.filter_named_axis_effects(jaxpr.effects, mesh.axis_names)\n   eqn = pe.new_jaxpr_eqn([*constvars, *invars], outvars, prim, params,\n@@ -673,44 +660,48 @@ def _shard_map_staging(\n \n # TODO add underscore version, for direct-linearize to consume\n \n+def _spec_to_names(spec: PartitionSpec):\n+  return {i: names if isinstance(names, tuple) else (names,)\n+          for i, names in enumerate(spec) if names is not None}\n+\n def _check_shapedarray(aval: core.AbstractValue) -> core.ShapedArray:\n   assert isinstance(aval, core.ShapedArray)\n   return aval\n \n-def _shard_aval(mesh: Mesh, manual_axes, check_vma, names: AxisNames,\n+def _shard_aval(mesh: Mesh, manual_axes, check_vma, spec,\n                 aval: core.AbstractValue) -> core.AbstractValue:\n   if type(aval) in core.shard_aval_handlers:\n     return core.shard_aval_handlers[type(aval)](mesh, manual_axes, check_vma,\n-                                                names, aval)\n+                                                spec, aval)\n   raise NotImplementedError(f\"Unsupported aval type: {type(aval)}\")\n \n-def _unshard_aval(mesh: Mesh, check_vma, names: AxisNames,\n+def _unshard_aval(mesh: Mesh, check_vma, spec,\n                   aval: core.AbstractValue) -> core.AbstractValue:\n   if type(aval) in core.unshard_aval_handlers:\n-    return core.unshard_aval_handlers[type(aval)](mesh, check_vma, names, aval)\n+    return core.unshard_aval_handlers[type(aval)](mesh, check_vma, spec, aval)\n   else:\n     raise NotImplementedError(f\"Unsupported aval type: {type(aval)}\")\n \n def _shard_shaped_array(mesh: Mesh, manual_axes: frozenset, check_vma,\n-                        names: AxisNames, aval: core.AbstractValue\n-                        ) -> core.AbstractValue:\n+                        spec, aval: core.AbstractValue) -> core.AbstractValue:\n   assert isinstance(aval, core.ShapedArray)\n+  names = _spec_to_names(spec)\n   new_shape = tuple(sz // prod(mesh.shape[n] for n in names.get(i, ()))\n                     for i, sz in enumerate(aval.shape))\n   manual_mesh = _as_manual_mesh(mesh, manual_axes | set(mesh.manual_axes))\n   new_sharding = NamedSharding(manual_mesh, aval.sharding.spec)\n-  vma = (frozenset({n for ns in names.values() for n in ns})\n-         if check_vma else frozenset())\n+  vma = _spec_to_vma(spec) if check_vma else frozenset()\n   vma = vma | aval.vma\n   return aval.update(shape=new_shape, sharding=new_sharding, vma=vma)\n core.shard_aval_handlers[core.ShapedArray] = _shard_shaped_array\n \n-def _unshard_shaped_array(mesh: Mesh, check_vma, names: AxisNames,\n-                          aval: core.AbstractValue,) -> core.AbstractValue:\n+def _unshard_shaped_array(mesh: Mesh, check_vma, spec, aval: core.AbstractValue\n+                          ) -> core.AbstractValue:\n   assert isinstance(aval, core.ShapedArray)\n+  names = _spec_to_names(spec)\n   new_shape = tuple(sz * prod(mesh.shape[n] for n in names.get(i, ()))\n                     for i, sz in enumerate(aval.shape))\n-  names_spec = _names_to_pspec(names)._normalized_spec_for_aval(aval.ndim)\n+  names_spec = spec._normalized_spec_for_aval(aval.ndim)\n   if aval.ndim == 0:\n     out_spec = P()\n   else:\n@@ -739,32 +730,32 @@ def _unshard_shaped_array(mesh: Mesh, check_vma, names: AxisNames,\n \n # Type-checking\n \n-def _shard_map_typecheck(_, *in_atoms, jaxpr, mesh, in_names, out_names,\n+def _shard_map_typecheck(_, *in_atoms, jaxpr, mesh, in_specs, out_specs,\n                          check_vma, manual_axes):\n   # TODO(mattjj,parkers): check auto\n-  for v, x, in_name in zip(jaxpr.invars, in_atoms, in_names):\n+  for v, x, in_spec in zip(jaxpr.invars, in_atoms, in_specs):\n     if not core.typecompat(v.aval, _shard_aval(\n-        mesh, manual_axes, check_vma, in_name, x.aval)):\n+        mesh, manual_axes, check_vma, in_spec, x.aval)):\n       raise core.JaxprTypeError(\"shard_map argument avals not compatible with \"\n-                                \"jaxpr binder avals and in_names\")\n+                                \"jaxpr binder avals and in_specs\")\n   with _extend_axis_env(mesh, manual_axes), config._check_vma(check_vma):\n     core.check_jaxpr(jaxpr)\n   if check_vma:\n     out_vma = [v.aval.vma for v in jaxpr.outvars]\n-    for vma, dst in zip(out_vma, out_names):\n-      if not _valid_repeats(mesh, vma, dst):\n+    for vma, out_spec in zip(out_vma, out_specs):\n+      if not _valid_repeats(mesh, vma, out_spec):\n         raise core.JaxprTypeError(\n             \"shard_map can't prove output is sufficiently replicated\")\n   out_avals_sharded = [x.aval for x in jaxpr.outvars]\n-  out_avals = map(partial(_unshard_aval, mesh, check_vma), out_names,\n+  out_avals = map(partial(_unshard_aval, mesh, check_vma), out_specs,\n                   out_avals_sharded)\n   effs = core.filter_named_axis_effects(jaxpr.effects, mesh.axis_names)\n   return out_avals, effs\n core.custom_typechecks[shard_map_p] = _shard_map_typecheck\n \n \n-def _valid_repeats(mesh: Mesh, vma: Set[AxisName], names: AxisNames) -> bool:\n-  um = set(_unmentioned(mesh, names)) - set(mesh.manual_axes)\n+def _valid_repeats(mesh: Mesh, vma: Set[AxisName], spec) -> bool:\n+  um = set(_unmentioned(mesh, spec)) - set(mesh.manual_axes)\n   if any(u in vma for u in um):\n     return False\n   return True\n@@ -772,10 +763,9 @@ def _valid_repeats(mesh: Mesh, vma: Set[AxisName], names: AxisNames) -> bool:\n # Lowering\n \n def _shardy_shard_map_sharding(\n-    ctx: mlir.LoweringRuleContext, mesh, manual_axes, names, aval_in\n+    ctx: mlir.LoweringRuleContext, mesh, manual_axes, spec, aval_in\n ) -> sharding_impls.SdyArray:\n-  axes = {name: i for i, ns in names.items() for name in ns}\n-  ns = _make_scoped_manual_sharding(ctx, mesh, axes)\n+  ns = _make_scoped_manual_sharding(ctx, mesh, spec)\n   if dtypes.issubdtype(aval_in.dtype, dtypes.extended):\n     ns = sharding_impls.physical_sharding(aval_in, ns)\n     aval_in = core.physical_aval(aval_in)\n@@ -789,12 +779,12 @@ def _shardy_shard_map_sharding(\n def _shardy_shard_map_token_sharding(\n     ctx: mlir.LoweringRuleContext, mesh\n   ) -> ir.Attribute:\n-  ns = _make_scoped_manual_sharding(ctx, mesh, {})\n+  ns = _make_scoped_manual_sharding(ctx, mesh, P())\n   return ns._to_sdy_sharding(0)\n \n \n def _shard_map_lowering_shardy(\n-    ctx, in_nodes, jaxpr, mesh, in_names, out_names, manual_axes, check_vma):\n+    ctx, in_nodes, jaxpr, mesh, in_specs, out_specs, manual_axes, check_vma):\n   axis_ctx = ctx.module_context.axis_context\n   in_avals_ = [v.aval for v in jaxpr.invars]\n   if isinstance(axis_ctx, sharding_impls.SPMDAxisContext):\n@@ -820,17 +810,17 @@ def _shard_map_lowering_shardy(\n       ctx.set_tokens_out(tokens_out)\n     return out_nodes\n \n-  in_shardings = list(map(\n-      partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n-      in_names, ctx.avals_in))\n+  in_shardings = list(\n+      map(partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n+          in_specs, ctx.avals_in))\n   num_dim_vars = len(ctx.dim_var_values)\n   in_shardings = ([_shardy_shard_map_token_sharding(ctx, mesh)]\n                   * (num_tokens + num_dim_vars) + in_shardings)\n   in_shardings = sharding_impls.SdyArrayList(in_shardings).build()\n \n-  out_shardings = list(map(\n-      partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n-      out_names, ctx.avals_out))\n+  out_shardings = list(\n+      map(partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n+          out_specs, ctx.avals_out))\n   out_shardings = [\n       _shardy_shard_map_token_sharding(ctx, mesh)] * num_tokens + out_shardings\n   out_shardings = sharding_impls.SdyArrayList(out_shardings).build()\n@@ -868,15 +858,15 @@ def _shard_map_lowering_shardy(\n   return manual_computation_op.results[num_tokens:]\n \n \n-def _shard_map_lowering(ctx, *in_nodes, jaxpr, mesh, in_names, out_names,\n+def _shard_map_lowering(ctx, *in_nodes, jaxpr, mesh, in_specs, out_specs,\n                         check_vma, manual_axes):\n   if config.use_shardy_partitioner.value:\n     return _shard_map_lowering_shardy(\n-        ctx, in_nodes, jaxpr, mesh, in_names, out_names, manual_axes, check_vma)\n+        ctx, in_nodes, jaxpr, mesh, in_specs, out_specs, manual_axes, check_vma)\n \n   in_avals_ = [v.aval for v in jaxpr.invars]\n   out_avals_ = [x.aval for x in jaxpr.outvars]\n-  in_nodes_ = map(partial(_xla_shard, ctx, mesh, manual_axes), in_names,\n+  in_nodes_ = map(partial(_xla_shard, ctx, mesh, manual_axes), in_specs,\n                   ctx.avals_in, in_avals_, in_nodes)\n   new_axis_context = sharding_impls.SPMDAxisContext(mesh, manual_axes)\n   sub_ctx = ctx.module_context.replace(axis_context=new_axis_context)\n@@ -885,28 +875,26 @@ def _shard_map_lowering(ctx, *in_nodes, jaxpr, mesh, in_names, out_names,\n         \"shmap_body\", ctx.name_stack, jaxpr, None, sub_ctx, in_avals_,\n         out_avals_, ctx.tokens_in, *in_nodes_,\n         dim_var_values=ctx.dim_var_values,\n-        arg_names=map(_pspec_mhlo_attrs, in_names, in_avals_),\n-        result_names=map(_pspec_mhlo_attrs, out_names, out_avals_))\n+        arg_names=map(_pspec_mhlo_attrs, in_specs, in_avals_),\n+        result_names=map(_pspec_mhlo_attrs, out_specs, out_avals_))\n   ctx.set_tokens_out(tokens_out)\n-  return map(partial(_xla_unshard, ctx, mesh, manual_axes), out_names,\n+  return map(partial(_xla_unshard, ctx, mesh, manual_axes), out_specs,\n              out_avals_, ctx.avals_out, out_nodes_)\n mlir.register_lowering(shard_map_p, _shard_map_lowering)\n \n-def _make_scoped_manual_sharding(ctx, mesh, axes):\n+def _make_scoped_manual_sharding(ctx, mesh, spec):\n   axis_ctx = ctx.module_context.axis_context\n   mesh = mesh.abstract_mesh\n   if isinstance(axis_ctx, sharding_impls.SPMDAxisContext):\n     mesh = mesh.update_axis_types(\n         {a: AxisType.Manual for a in axis_ctx.manual_axes})\n-  return NamedSharding(\n-      mesh, sharding_impls.array_mapping_to_axis_resources(axes))  # type: ignore\n+  return NamedSharding(mesh, spec)\n \n-def _xla_shard(ctx: mlir.LoweringRuleContext, mesh, manual_axes, names,\n+def _xla_shard(ctx: mlir.LoweringRuleContext, mesh, manual_axes, spec,\n                aval_in, aval_out, x):\n   if prod([size for n, size in mesh.shape.items() if n in manual_axes]) == 1:\n     return x\n-  axes = {name: i for i, ns in names.items() for name in ns}\n-  ns = _make_scoped_manual_sharding(ctx, mesh, axes)\n+  ns = _make_scoped_manual_sharding(ctx, mesh, spec)\n   if dtypes.issubdtype(aval_in.dtype, dtypes.extended):\n     ns = sharding_impls.physical_sharding(aval_in, ns)\n     aval_in = core.physical_aval(aval_in)\n@@ -920,12 +908,11 @@ def _xla_shard(ctx: mlir.LoweringRuleContext, mesh, manual_axes, names,\n   return mlir.wrap_with_full_to_shard_op(ctx, sx, aval_out, manual_proto,\n                                          unspecified)\n \n-def _xla_unshard(ctx: mlir.LoweringRuleContext, mesh, manual_axes, names,\n+def _xla_unshard(ctx: mlir.LoweringRuleContext, mesh, manual_axes, spec,\n                  aval_in, aval_out, x):\n   if prod([size for n, size in mesh.shape.items() if n in manual_axes]) == 1:\n     return x\n-  axes = {name: i for i, ns in names.items() for name in ns}\n-  ns = _make_scoped_manual_sharding(ctx, mesh, axes)\n+  ns = _make_scoped_manual_sharding(ctx, mesh, spec)\n   if dtypes.issubdtype(aval_out.dtype, dtypes.extended):\n     ns = sharding_impls.physical_sharding(aval_out, ns)\n     aval_out = core.physical_aval(aval_out)\n@@ -941,8 +928,9 @@ def _xla_unshard(ctx: mlir.LoweringRuleContext, mesh, manual_axes, names,\n   return mlir.wrap_with_shard_to_full_op(ctx, sx, aval_out, shard_proto,\n                                          unspecified)\n \n-def _pspec_mhlo_attrs(names: AxisNames, aval: core.AbstractValue) -> str:\n+def _pspec_mhlo_attrs(spec, aval: core.AbstractValue) -> str:\n   if isinstance(aval, core.ShapedArray):\n+    names = _spec_to_names(spec)\n     return str(map(names.get, range(aval.ndim)))\n   return ''\n \n@@ -969,15 +957,13 @@ def _vma_to_spec(mesh, vma):\n   return P(order_wrt_mesh(mesh, vma))\n \n def _spec_to_vma(spec):\n-  return _names_to_vma(_canonicalize_spec(spec))\n-\n-def _names_to_vma(names):\n-  return {n for ns in names.values() for n in ns}\n+  return frozenset(p for s in spec if s is not None\n+                   for p in (s if isinstance(s, tuple) else (s,)))\n \n def order_wrt_mesh(mesh, x):\n   return tuple(a for a in mesh.axis_names if a in x)\n \n-def _shard_map_impl(trace, prim, fun, args, *, mesh, in_names, out_names_thunk,\n+def _shard_map_impl(trace, prim, fun, args, *, mesh, in_specs, out_specs_thunk,\n                     check_vma, manual_axes):\n   if len(manual_axes) < len(mesh.axis_names):\n     raise NotImplementedError\n@@ -988,18 +974,18 @@ def _shard_map_impl(trace, prim, fun, args, *, mesh, in_names, out_names_thunk,\n     mesh = get_mesh_from_args(args, mesh)\n   cur_mesh = get_abstract_mesh()\n   args = map(partial(_unmatch_spec, mesh, check_vma, context_mesh=cur_mesh),\n-             in_names, args)\n-  in_vma = map(_names_to_vma, in_names)\n+             in_specs, args)\n+  in_vma = map(_spec_to_vma, in_specs)\n   outs, out_vma = _run_shmap(fun, mesh, manual_axes, args, in_vma, check_vma,\n                              cur_mesh)\n   out_avals = [core.mapped_aval(x.shape[0], 0, core.get_aval(x)) for x in outs]\n-  _check_names(out_names_thunk(), out_avals)  # pytype: disable=wrong-arg-types\n+  _check_names(out_specs_thunk(), out_avals)  # pytype: disable=wrong-arg-types\n   if check_vma:\n-    _check_vmas(mesh, out_names_thunk(), out_vma)\n+    _check_vmas(mesh, out_specs_thunk(), out_vma)\n     src_pspecs = tuple(_vma_to_spec(mesh, r) for r in out_vma)\n   else:\n     src_pspecs = tuple(P(mesh.axis_names) for _ in out_vma)\n-  dst_pspecs = map(_names_to_pspec, out_names_thunk())\n+  dst_pspecs = out_specs_thunk()\n   return map(partial(_match_spec, mesh, check_vma), src_pspecs, dst_pspecs,\n              outs)\n core.EvalTrace.process_shard_map = _shard_map_impl\n@@ -1014,42 +1000,35 @@ def _run_shmap(f, mesh, manual_axes, args, vmas, check_vma, context_mesh):\n     outs, out_vma = unzip2(map(trace.to_val_vma_pair, ans))\n   return outs, out_vma\n \n-def _names_to_pspec(names: AxisNames) -> PartitionSpec:\n-  ndmin = max(names) + 1 if names else 0\n-  unpack = lambda t: t[0] if t is not None and len(t) == 1 else t\n-  return PartitionSpec(*(unpack(names.get(i)) for i in range(ndmin)))\n \n-def _unmatch_spec(mesh: Mesh, check_vma, src: AxisNames, x: JaxType,\n-                  context_mesh) -> JaxType:\n+def _unmatch_spec(mesh: Mesh, check_vma, in_spec, x: JaxType, context_mesh\n+                  ) -> JaxType:\n   with (core.eval_context(), jax.disable_jit(False),\n         use_abstract_mesh(context_mesh)):\n-    return jax.jit(HashablePartial(_unmatch, mesh, check_vma,\n-                                   tuple(src.items())))(x)\n+    return jax.jit(HashablePartial(_unmatch, mesh, check_vma, in_spec))(x)\n \n-def _unmatch(mesh, check_vma, src_tup, x):\n-  src = _names_to_pspec(dict(src_tup))\n+def _unmatch(mesh, check_vma, in_spec, x):\n   if check_vma:\n-    used_axes = {i for _, ns in src_tup for i in ns}\n+    used_axes = _spec_to_vma(in_spec)\n     dst = P(order_wrt_mesh(mesh, used_axes))\n   else:\n     dst = P(mesh.axis_names)\n     check_vma = False\n-  return shard_map(_add_singleton, mesh=mesh, in_specs=(src,), out_specs=dst,\n-                   check_vma=check_vma)(x)\n+  return shard_map(_add_singleton, mesh=mesh, in_specs=(in_spec,),\n+                   out_specs=dst, check_vma=check_vma)(x)\n \n-def _check_names(names: Sequence[AxisNames], avals: Sequence[core.ShapedArray]\n-                 ) -> None:\n-  fail = [a if n and not max(n) < a.ndim else no_fail\n-          for n, a in zip(names, avals)]\n+def _check_names(specs, avals: Sequence[core.ShapedArray]) -> None:\n+  fail = [a if sp and len(sp) > a.ndim else no_fail\n+          for sp, a in zip(specs, avals)]\n   if any(f is not no_fail for f in fail):\n     raise _SpecError(fail)\n \n class _SpecError(Exception):\n   pass\n \n-def _check_vmas(mesh, names, vmas):\n-  fail = [vma if not _valid_repeats(mesh, vma, n) else no_fail\n-          for n, vma in zip(names, vmas)]\n+def _check_vmas(mesh, specs, vmas):\n+  fail = [vma if not _valid_repeats(mesh, vma, sp) else no_fail\n+          for sp, vma in zip(specs, vmas)]\n   if any(f is not no_fail for f in fail):\n     raise _RepError(fail)\n \n@@ -1099,7 +1078,7 @@ def to_val_vma_pair(self, val):\n     elif isinstance(val, Tracer):\n       raise Exception(f\"Shouldn't have any non-shard_map tracers: {val}\")\n     else:\n-      val_ = _unmatch_spec(self.mesh, self.check, {}, val, self.context_mesh)\n+      val_ = _unmatch_spec(self.mesh, self.check, P(), val, self.context_mesh)\n       return val_, frozenset()\n \n   def process_primitive(self, prim, tracers, params):\n@@ -1205,6 +1184,7 @@ def __str__(self) -> str:\n     return '\\n'.join(\n         f\"On {device} at mesh coordinates {axis_names} = {idx}:\\n{block}\\n\"\n         for (idx, device), block in zip(np.ndenumerate(mesh.devices), blocks))\n+\n   __repr__ = __str__  # for debuggers, like `p x`\n \n def _prim_applier(prim, check_vma, params_tup, mesh, in_specs, out_specs, *args):\n@@ -1251,34 +1231,33 @@ def _device_put_eager_rule(mesh, *xs, srcs, devices, copy_semantics):\n def _shard_map_batch(\n     trace: batching.BatchTrace, prim: core.Primitive, fun: lu.WrappedFun,\n     in_tracers: Sequence[batching.BatchTracer], mesh: Mesh,\n-    in_names: tuple[AxisNames, ...],\n-    out_names_thunk: Callable[[], tuple[AxisNames, ...]],\n-    check_vma: bool,\n-    manual_axes: frozenset) -> Sequence[batching.BatchTracer]:\n+    in_specs, out_specs_thunk, check_vma: bool, manual_axes: frozenset\n+    ) -> Sequence[batching.BatchTracer]:\n   in_vals, in_dims = unzip2(map(trace.to_batch_info, in_tracers))\n   if any(isinstance(d, batching.RaggedAxis) for d in in_dims):\n     raise NotImplementedError\n-  new_in_names = [{ax + (d is not batching.not_mapped and d <= ax): names[ax]\n-                   for ax in names} for names, d in zip(in_names, in_dims)]\n   spmd_axis_name = trace.axis_data.spmd_name\n   if spmd_axis_name is not None:\n-    used = {n for names in in_names for ns in names.values() for n in ns}\n+    used = {n for spec in in_specs for n in _spec_to_vma(spec)}\n     if not config.disable_vmap_shmap_error.value and set(spmd_axis_name) & used:\n       raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map in_specs\")\n-    new_in_names = [{**ns, d:spmd_axis_name} if d is not batching.not_mapped\n-                    else ns for ns, d in zip(new_in_names, in_dims)]\n+    new_in_specs = [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, spmd_axis_name)\n+                    for sp, d in zip(in_specs, in_dims)]\n     new_size = trace.axis_data.size // prod(mesh.shape[n] for n in spmd_axis_name)\n     new_axis_data = batching.AxisData(trace.axis_data.name, new_size,\n                                       trace.axis_data.spmd_name, None)\n   else:\n+    new_in_specs = [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, None)\n+                    for sp, d in zip(in_specs, in_dims)]\n     new_axis_data = trace.axis_data\n   fun, out_dims = batching.batch_subtrace(fun, trace.tag, new_axis_data, tuple(in_dims))\n-  @as_hashable_function(closure=out_names_thunk)\n-  def new_out_names_thunk():\n-    return _batch_out_names(spmd_axis_name, out_dims(), out_names_thunk())\n \n-  new_params = dict(mesh=mesh, in_names=new_in_names,\n-                    out_names_thunk=new_out_names_thunk, check_vma=check_vma,\n+  @as_hashable_function(closure=out_specs_thunk)\n+  def new_out_specs_thunk():\n+    return _batch_out_specs(spmd_axis_name, out_dims(), out_specs_thunk())\n+\n+  new_params = dict(mesh=mesh, in_specs=new_in_specs,\n+                    out_specs_thunk=new_out_specs_thunk, check_vma=check_vma,\n                     manual_axes=manual_axes)\n   with core.set_current_trace(trace.parent_trace):\n     out_vals = prim.bind(fun, *in_vals, **new_params)\n@@ -1287,36 +1266,36 @@ def new_out_names_thunk():\n   return map(make_tracer, out_vals, out_dims())\n batching.BatchTrace.process_shard_map = _shard_map_batch\n \n-def _batch_out_names(spmd_axis_name, dims, out_names):\n-  out_names_ = [{ax + (d is not batching.not_mapped and d <= ax): names[ax]\n-                  for ax in names} for names, d in zip(out_names, dims)]\n-  if spmd_axis_name is not None:\n-    used = {n for names in out_names for ns in names.values() for n in ns}\n-    if not config.disable_vmap_shmap_error.value and set(spmd_axis_name) & used:\n+def _batch_out_specs(spmd_name, dims, out_specs):\n+  if spmd_name is not None:\n+    used = {n for spec in out_specs for n in _spec_to_vma(spec)}\n+    if not config.disable_vmap_shmap_error.value and set(spmd_name) & used:\n       raise ValueError(\"vmap spmd_axis_name cannot appear in shard_map out_specs\")\n-    out_names_ = [{**ns, d:spmd_axis_name} if d is not batching.not_mapped\n-                  else ns for ns, d in zip(out_names_, dims)]\n-  return out_names_\n+    return [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, spmd_name)\n+            for sp, d in zip(out_specs, dims)]\n+  else:\n+    return [sp if d is batching.not_mapped else pxla.batch_spec(sp, d, None)\n+            for sp, d in zip(out_specs, dims)]\n \n \n # Autodiff\n \n-def _shard_map_jvp(trace, shard_map_p, f, tracers, mesh, in_names,\n-                   out_names_thunk, check_vma, manual_axes):\n+def _shard_map_jvp(trace, shard_map_p, f, tracers, mesh, in_specs,\n+                   out_specs_thunk, check_vma, manual_axes):\n   primals, tangents = unzip2(map(trace.to_primal_tangent_pair, tracers))\n   which_nz = [     type(t) is not ad.Zero           for t in tangents]\n   tangents = [t if type(t) is not ad.Zero else None for t in tangents]\n   args, in_tree = tree_flatten((primals, tangents))\n   f_jvp = ad.jvp_subtrace(f, trace.tag)\n   f_jvp, which_nz_out = ad.nonzero_tangent_outputs(f_jvp)\n-  tangent_in_names = [ax for ax, nz in zip(in_names, which_nz) if nz]\n+  tangent_in_specs = [sp for sp, nz in zip(in_specs, which_nz) if nz]\n \n-  @as_hashable_function(closure=out_names_thunk)\n-  def new_out_names_thunk():\n-    out_ax = out_names_thunk()\n+  @as_hashable_function(closure=out_specs_thunk)\n+  def new_out_specs_thunk():\n+    out_ax = out_specs_thunk()\n     return (*out_ax, *(ax for ax, nz in zip(out_ax, which_nz_out()) if nz))\n-  params = dict(mesh=mesh, in_names=(*in_names, *tangent_in_names),\n-                out_names_thunk=new_out_names_thunk, check_vma=check_vma,\n+  params = dict(mesh=mesh, in_specs=(*in_specs, *tangent_in_specs),\n+                out_specs_thunk=new_out_specs_thunk, check_vma=check_vma,\n                 manual_axes=manual_axes)\n   f_jvp, out_tree = ad.traceable(f_jvp, in_tree)\n   result = shard_map_p.bind_with_trace(trace.parent_trace, (f_jvp,) + tuple(args), params)\n@@ -1327,32 +1306,32 @@ def new_out_names_thunk():\n ad.JVPTrace.process_shard_map = _shard_map_jvp\n \n def _shard_map_partial_eval(trace: pe.JaxprTrace, shard_map_p,\n-                            f: lu.WrappedFun, tracers, mesh, in_names,\n-                            out_names_thunk, check_vma, manual_axes):\n+                            f: lu.WrappedFun, tracers, mesh, in_specs,\n+                            out_specs_thunk, check_vma, manual_axes):\n   tracers = map(trace.to_jaxpr_tracer, tracers)\n   in_pvals = [t.pval for t in tracers]\n   in_knowns, in_avals, in_consts = pe.partition_pvals(in_pvals)\n-  unk_in_names, known_in_names = pe.partition_list(in_knowns, in_names)\n+  unk_in_specs, known_in_specs = pe.partition_list(in_knowns, in_specs)\n   in_avals_sharded = map(partial(_shard_aval, mesh, manual_axes, check_vma),\n-                         unk_in_names, in_avals)\n+                         unk_in_specs, in_avals)\n   f = pe.trace_to_subjaxpr_nounits_fwd2(f, trace.tag, f.debug_info, False)\n   f = _promote_scalar_residuals(f)\n   f_known, aux = pe.partial_eval_wrapper_nounits2(\n       f, (*in_knowns,), (*in_avals_sharded,))\n   all_names = _all_newly_manual_mesh_names(mesh, manual_axes)\n \n-  @as_hashable_function(closure=out_names_thunk)\n-  def known_out_names():\n+  @as_hashable_function(closure=out_specs_thunk)\n+  def known_out_specs():\n     _, _, out_knowns, res_avals, _, _ = aux()\n-    _, out_known_names = pe.partition_list(out_knowns, out_names_thunk())\n+    _, out_known_specs = pe.partition_list(out_knowns, out_specs_thunk())\n     if check_vma:\n-      res_names = [{0: order_wrt_mesh(mesh, a.vma)} for a in res_avals]\n+      res_specs = [P(order_wrt_mesh(mesh, a.vma)) for a in res_avals]\n     else:\n-      res_names = [{0: all_names}] * len(res_avals)\n-    return (*out_known_names, *res_names)\n+      res_specs = [P(all_names)] * len(res_avals)\n+    return (*out_known_specs, *res_specs)\n \n-  known_params = dict(mesh=mesh, in_names=(*known_in_names,),\n-                      out_names_thunk=known_out_names, check_vma=check_vma,\n+  known_params = dict(mesh=mesh, in_specs=(*known_in_specs,),\n+                      out_specs_thunk=known_out_specs, check_vma=check_vma,\n                       manual_axes=manual_axes)\n   out = shard_map_p.bind_with_trace(trace.parent_trace, (f_known, *in_consts),\n                                     known_params)\n@@ -1360,32 +1339,32 @@ def known_out_names():\n   num_res = sum(f1 is None and f2 is None for f1, f2 in zip(in_fwd, out_fwd))\n   out_consts, non_fwd_res = split_list(out, [len(out) - num_res])\n   assert not jaxpr.constvars\n-  unk_out_names, _ = pe.partition_list(out_knowns, out_names_thunk())\n-  known_out_names_ = known_out_names()\n+  unk_out_specs, _ = pe.partition_list(out_knowns, out_specs_thunk())\n+  known_out_specs_ = known_out_specs()\n   res = subs_list2(in_fwd, out_fwd, in_consts, out_consts, non_fwd_res)\n   # TODO make res_avals be the full set, not just the non-fwd ones\n   res_avals_iter = iter(res_avals)\n-  res_names = []\n+  res_specs = []\n   for f1, f2 in zip(in_fwd, out_fwd):\n     if f1 is not None:\n-      res_names.append(known_in_names[f1])\n+      res_specs.append(known_in_specs[f1])\n     elif f2 is not None:\n-      res_names.append(known_out_names_[f2])\n+      res_specs.append(known_out_specs_[f2])\n     else:\n       if check_vma:\n         res_vma = next(res_avals_iter).vma\n-        res_names.append({0: order_wrt_mesh(mesh, res_vma)})\n+        res_specs.append(P(order_wrt_mesh(mesh, res_vma)))\n       else:\n-        res_names.append({0: all_names})\n-  unk_in_names = (*res_names,) + ({},) * len(env) + (*unk_in_names,)  # type: ignore[assignment]\n+        res_specs.append(P(all_names))\n+  unk_in_specs = (*res_specs,) + (P(),) * len(env) + (*unk_in_specs,)  # type: ignore[assignment]\n   const_tracers = map(trace.new_instantiated_const, res)\n   env_tracers = map(trace.to_jaxpr_tracer, env)\n   unk_arg_tracers = [t for t in tracers if not t.is_known()]\n   out_avals_sharded = [v.aval for v in jaxpr.outvars]\n-  unk_params = dict(mesh=mesh, in_names=unk_in_names,\n-                    out_names=unk_out_names, jaxpr=jaxpr,\n+  unk_params = dict(mesh=mesh, in_specs=unk_in_specs,\n+                    out_specs=unk_out_specs, jaxpr=jaxpr,\n                     check_vma=check_vma, manual_axes=manual_axes)\n-  out_avals = map(partial(_unshard_aval, mesh, check_vma), unk_out_names,\n+  out_avals = map(partial(_unshard_aval, mesh, check_vma), unk_out_specs,\n                   out_avals_sharded)\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(a), None)\n                  for a in out_avals]\n@@ -1398,8 +1377,8 @@ def known_out_names():\n pe.JaxprTrace.process_shard_map = _shard_map_partial_eval\n \n def _shard_map_linearize(trace, shard_map_p, f: lu.WrappedFun,\n-                         tracers, mesh, in_names,\n-                         out_names_thunk, check_vma, manual_axes):\n+                         tracers, mesh, in_specs, out_specs_thunk, check_vma,\n+                         manual_axes):\n   primals, tangents = unzip2(map(trace.to_primal_tangent_pair, tracers))\n   nzs_in = tuple(type(t) is not ad.Zero for t in tangents)\n   f_primal, linearize_outs_thunk = ad.linearize_subtrace(f, trace.tag, nzs_in, f.debug_info)\n@@ -1407,19 +1386,19 @@ def _shard_map_linearize(trace, shard_map_p, f: lu.WrappedFun,\n   all_names = _all_newly_manual_mesh_names(mesh, manual_axes)\n \n   @as_hashable_function(closure=linearize_outs_thunk)\n-  def fwd_out_names_thunk():\n+  def fwd_out_specs_thunk():\n     res_avals, _, _, _, in_fwd, out_fwd = linearize_outs_thunk()\n     res_avals = [r for r, f1, f2 in zip(res_avals, in_fwd, out_fwd)\n                  if f1 is None and f2 is None]\n-    out_names = out_names_thunk()\n+    out_specs = out_specs_thunk()\n     if check_vma:\n-      res_names = [{0: order_wrt_mesh(mesh, a.vma)} for a in res_avals]\n+      res_specs = [P(order_wrt_mesh(mesh, a.vma)) for a in res_avals]\n     else:\n-      res_names = [{0: all_names}] * len(res_avals)\n-    return (*res_names, *out_names)\n+      res_specs = [P(all_names)] * len(res_avals)\n+    return (*res_specs, *out_specs)\n   fwd_params = dict(\n-      mesh=mesh, in_names=in_names,\n-      out_names_thunk=fwd_out_names_thunk, check_vma=check_vma,\n+      mesh=mesh, in_specs=in_specs,\n+      out_specs_thunk=fwd_out_specs_thunk, check_vma=check_vma,\n       manual_axes=manual_axes)\n   all_fwd_results = shard_map_p.bind_with_trace(\n       trace.parent_trace, (f_primal, *primals), fwd_params)\n@@ -1434,30 +1413,31 @@ def fwd_out_names_thunk():\n         use_abstract_mesh(_as_manual_mesh(mesh, manual_axes | set(mesh.manual_axes))),\n         config._check_vma(check_vma)):\n     lin_jaxpr = _promote_scalar_residuals_jaxpr(lin_jaxpr, args_to_promote)\n-  out_names = out_names_thunk()\n+  out_specs = out_specs_thunk()\n   res_avals2 = [r for r, f1, f2 in zip(res_avals, in_fwd, out_fwd)\n                 if f1 is None and f2 is None]\n   res_avals_iter = iter(res_avals2)\n-  res_names = []\n+  res_specs = []\n   for f1, f2 in zip(in_fwd, out_fwd):\n     if f1 is not None:\n-      res_names.append(in_names[f1])\n+      res_specs.append(in_specs[f1])\n     elif f2 is not None:\n-      res_names.append(out_names[f2])\n+      res_specs.append(out_specs[f2])\n     else:\n       if check_vma:\n         res_vma = next(res_avals_iter).vma\n-        res_names.append({0: order_wrt_mesh(mesh, res_vma)})\n+        res_specs.append(P(order_wrt_mesh(mesh, res_vma)))\n       else:\n-        res_names.append({0: all_names})\n-  new_in_names = (*res_names, *({} for _ in range(len(env))),\n-                  *(ax for ax, nz in zip(in_names, nzs_in) if nz))\n-  tangent_out_names = tuple(ax for ax, nz in zip(out_names_thunk(), nzs_out) if nz)\n-  @as_hashable_function(closure=tangent_out_names)\n-  def tangent_out_names_thunk():\n-    return tangent_out_names\n+        res_specs.append(P(all_names))\n+  new_in_specs = (*res_specs, *(P(),) * len(env),\n+                  *(ax for ax, nz in zip(in_specs, nzs_in) if nz))\n+  tangent_out_specs = tuple(ax for ax, nz in zip(out_specs_thunk(), nzs_out)\n+                            if nz)\n+  @as_hashable_function(closure=tangent_out_specs)\n+  def tangent_out_specs_thunk():\n+    return tangent_out_specs\n   tangent_params = dict(\n-      mesh=mesh, in_names=new_in_names, out_names_thunk=tangent_out_names_thunk,\n+      mesh=mesh, in_specs=new_in_specs, out_specs_thunk=tangent_out_specs_thunk,\n       check_vma=check_vma, manual_axes=manual_axes)\n \n   # TODO(mattjj): avoid round-tripping the jaxpr through eval_jaxpr here\n@@ -1509,29 +1489,29 @@ def fun(*res_and_args):\n   return jaxpr\n \n \n-def _unmentioned2(mesh: Mesh, names: AxisNames,\n-                  manual_axes: frozenset[AxisName]) -> list[AxisName]:\n+def _unmentioned2(mesh: Mesh, spec, manual_axes: frozenset[AxisName]\n+                  ) -> list[AxisName]:\n   # We use a filtered-down version of unmentioned to avoid defensive-psum over\n   # more chips than required in the transpose-no-check-vma case.\n-  name_set = {n for ns in names.values() for n in ns}\n+  name_set = _spec_to_vma(spec)\n   return [n for n in _all_mesh_names_except_spmd(mesh, manual_axes)\n           if n not in name_set]\n \n \n def _shard_map_transpose(out_cts, *args,\n-                         jaxpr: core.Jaxpr, mesh, in_names, out_names,\n+                         jaxpr: core.Jaxpr, mesh, in_specs, out_specs,\n                          check_vma, manual_axes):\n   mb_div = lambda x, y: x / y if y != 1 else x\n   out_cts = [\n-      ad.Zero(_shard_aval(mesh, manual_axes, check_vma, ns, x.aval))\n+      ad.Zero(_shard_aval(mesh, manual_axes, check_vma, sp, x.aval))\n       if type(x) is ad.Zero else x if check_vma or dtypes.dtype(x) == dtypes.float0\n-      else mb_div(x, prod(map(mesh.shape.get, _unmentioned2(mesh, ns, manual_axes))))\n-      for ns, x in zip(out_names, out_cts)\n+      else mb_div(x, prod(map(mesh.shape.get, _unmentioned2(mesh, sp, manual_axes))))\n+      for sp, x in zip(out_specs, out_cts)\n   ]\n   args = tuple(x if type(x) is not ad.UndefinedPrimal else\n                ad.UndefinedPrimal(\n-                   _shard_aval(mesh, manual_axes, check_vma, ns, x.aval))\n-               for ns, x in zip(in_names, args))\n+                   _shard_aval(mesh, manual_axes, check_vma, sp, x.aval))\n+               for sp, x in zip(in_specs, args))\n   all_args, in_tree = tree_flatten((out_cts, args))\n \n   def fun_trans_callable(out_cts, args):\n@@ -1544,11 +1524,11 @@ def fun_trans_callable(out_cts, args):\n     in_cts = ad.backward_pass(\n         jaxpr_unknown.jaxpr, False, (), (*res_reshaped, *undefs), out_cts\n     )[len(res_reshaped):]\n-    _, in_ct_names = partition_list(in_undef, in_names)\n-    in_cts = [ad.Zero(_unshard_aval(mesh, check_vma, ns, x.aval))\n+    _, in_ct_specs = partition_list(in_undef, in_specs)\n+    in_cts = [ad.Zero(_unshard_aval(mesh, check_vma, sp, x.aval))\n               if type(x) is ad.Zero else x if check_vma\n-              else jax.lax.psum(x, tuple(_unmentioned2(mesh, ns, manual_axes)))\n-              for ns, x in zip(in_ct_names, in_cts)]\n+              else jax.lax.psum(x, tuple(_unmentioned2(mesh, sp, manual_axes)))\n+              for sp, x in zip(in_ct_specs, in_cts)]\n     res_zeros = [ad_util.zero_from_primal(r) for r in res]\n     return merge_lists(in_undef, res_zeros, in_cts)\n \n@@ -1556,17 +1536,17 @@ def fun_trans_callable(out_cts, args):\n   fun_trans, nz_arg_cts = ad.nonzero_outputs(fun_trans)\n   fun_trans_flat, out_tree = api_util.flatten_fun_nokwargs(fun_trans, in_tree)\n \n-  new_in_names = \\\n-      [n for n, x in zip(out_names, out_cts) if type(x) is not ad.Zero] + \\\n-      [n for n, x in zip(in_names, args) if type(x) is not ad.UndefinedPrimal]\n+  new_in_specs = (\n+      [n for n, x in zip(out_specs, out_cts) if type(x) is not ad.Zero] +\n+      [n for n, x in zip(in_specs, args) if type(x) is not ad.UndefinedPrimal])\n \n-  def new_out_names_thunk():\n-    return tuple(names for names, nz in zip(in_names, nz_arg_cts()) if nz)\n+  def new_out_specs_thunk():\n+    return tuple(sp for sp, nz in zip(in_specs, nz_arg_cts()) if nz)\n \n   try:\n     out_flat = shard_map_p.bind(\n-        fun_trans_flat, *all_args, mesh=mesh, in_names=tuple(new_in_names),\n-        out_names_thunk=new_out_names_thunk, check_vma=check_vma,\n+        fun_trans_flat, *all_args, mesh=mesh, in_specs=tuple(new_in_specs),\n+        out_specs_thunk=new_out_specs_thunk, check_vma=check_vma,\n         manual_axes=manual_axes)\n   except (FloatingPointError, ZeroDivisionError) as e:\n     print(\"Invalid nan value encountered in the backward pass of a shard_map \"\n@@ -1576,8 +1556,8 @@ def new_out_names_thunk():\n       # in eager mode so that output of shmap are not manual.\n       with jax.disable_jit(True):\n         _ = shard_map_p.bind(\n-            fun_trans_flat, *all_args, mesh=mesh, in_names=tuple(new_in_names),\n-            out_names_thunk=new_out_names_thunk, check_vma=check_vma,\n+            fun_trans_flat, *all_args, mesh=mesh, in_specs=tuple(new_in_specs),\n+            out_specs_thunk=new_out_specs_thunk, check_vma=check_vma,\n             manual_axes=manual_axes)\n     except (FloatingPointError, ZeroDivisionError) as e2:\n       raise e2 from None\n@@ -1618,22 +1598,22 @@ def _partial_eval_jaxpr_custom_rule(\n   _, ins_staged = partition_list(inst_in, eqn.invars)\n   _, out_binders_staged = partition_list(inst_out, eqn.outvars)\n   newvar = core.gensym()\n-  residuals, staged_in_res_names = [], []\n+  residuals, staged_in_res_specs = [], []\n   for var, w in zip(jaxpr_staged.invars[:num_res], which):\n     if w:\n-      rn = ({0: order_wrt_mesh(mesh, var.aval.vma)}  # type: ignore\n-            if check_vma else {0: _all_newly_manual_mesh_names(mesh, manual_axes)})\n+      rn = (P(order_wrt_mesh(mesh, var.aval.vma))  # type: ignore\n+            if check_vma else P(_all_newly_manual_mesh_names(mesh, manual_axes)))\n       residuals.append(newvar(_unshard_aval(mesh, check_vma, rn, var.aval)))\n-      staged_in_res_names.append(rn)\n+      staged_in_res_specs.append(rn)\n   if check_vma:\n-    out_res_names_known = [{0: order_wrt_mesh(mesh, var.aval.vma)}  # type: ignore\n+    out_res_specs_known = [P(order_wrt_mesh(mesh, var.aval.vma))  # type: ignore\n                            for var, o in zip(res_vars, out_fwd) if o is None]\n   else:\n-    out_res_names_known = [\n-        {0: _all_newly_manual_mesh_names(mesh, manual_axes)}] * sum(which)\n+    out_res_specs_known = [\n+        P(_all_newly_manual_mesh_names(mesh, manual_axes))] * sum(which)\n   params_known, params_staged = _pe_custom_params(\n       unks_in, inst_in, map(op.not_, unks_out), inst_out, in_fwd, out_fwd,\n-      out_res_names_known, staged_in_res_names,\n+      out_res_specs_known, staged_in_res_specs,\n       dict(eqn.params, jaxpr=jaxpr_known), dict(eqn.params, jaxpr=jaxpr_staged))\n   eqn_known = pe.new_jaxpr_eqn(ins_known, [*out_binders_known, *residuals],\n                                eqn.primitive, params_known, jaxpr_known.effects,\n@@ -1681,27 +1661,27 @@ def staged(*args):\n   return jaxpr_known, jaxpr_staged\n \n def _pe_custom_params(unks_in, inst_in, kept_outs_known, kept_outs_staged,\n-                      in_fwd, out_fwd, out_res_names_known, staged_in_res_names,\n+                      in_fwd, out_fwd, out_res_specs_known, staged_in_res_specs,\n                       params_known, params_staged):\n   # prune inputs to jaxpr_known according to unks_in\n-  in_names_known, _ = partition_list(unks_in, params_known['in_names'])\n-  _, out_names_known = partition_list(kept_outs_known, params_known['out_names'])\n-  out_names_known = out_names_known + out_res_names_known\n-  assert len(out_names_known) == len(params_known['jaxpr'].outvars)\n-  new_params_known = dict(params_known, in_names=tuple(in_names_known),\n-                          out_names=tuple(out_names_known))\n+  in_specs_known, _ = partition_list(unks_in, params_known['in_specs'])\n+  _, out_specs_known = partition_list(kept_outs_known, params_known['out_specs'])\n+  out_specs_known = out_specs_known + out_res_specs_known\n+  assert len(out_specs_known) == len(params_known['jaxpr'].outvars)\n+  new_params_known = dict(params_known, in_specs=tuple(in_specs_known),\n+                          out_specs=tuple(out_specs_known))\n \n   # added num_res new inputs to jaxpr_staged, pruning according to inst_in\n-  _, in_names_staged = partition_list(inst_in, params_staged['in_names'])\n-  iter_staged = iter(staged_in_res_names)\n-  res_names = [in_names_known[f1] if f1 is not None else\n-               out_names_known[f2] if f2 is not None else\n+  _, in_specs_staged = partition_list(inst_in, params_staged['in_specs'])\n+  iter_staged = iter(staged_in_res_specs)\n+  res_specs = [in_specs_known[f1] if f1 is not None else\n+               out_specs_known[f2] if f2 is not None else\n                next(iter_staged) for f1, f2 in zip(in_fwd, out_fwd)]\n \n-  in_names_staged = res_names + in_names_staged\n-  _, out_names_staged = partition_list(kept_outs_staged, params_staged['out_names'])\n-  new_params_staged = dict(params_staged, in_names=tuple(in_names_staged),\n-                           out_names=tuple(out_names_staged))\n+  in_specs_staged = res_specs + in_specs_staged\n+  _, out_specs_staged = partition_list(kept_outs_staged, params_staged['out_specs'])\n+  new_params_staged = dict(params_staged, in_specs=tuple(in_specs_staged),\n+                           out_specs=tuple(out_specs_staged))\n   return new_params_known, new_params_staged\n \n # TODO(mattjj): remove this mechanism when we revise mesh scopes\n@@ -1742,10 +1722,10 @@ def _shard_map_dce(used_outputs: list[bool], eqn: core.JaxprEqn\n   if not any(used_inputs) and not any(used_outputs) and not jaxpr.effects:\n     return used_inputs, None\n   else:\n-    _, in_names = partition_list(used_inputs, eqn.params['in_names'])\n-    _, out_names = partition_list(used_outputs, eqn.params['out_names'])\n-    new_params = dict(eqn.params, jaxpr=jaxpr, in_names=tuple(in_names),\n-                      out_names=tuple(out_names))\n+    _, in_specs = partition_list(used_inputs, eqn.params['in_specs'])\n+    _, out_specs = partition_list(used_outputs, eqn.params['out_specs'])\n+    new_params = dict(eqn.params, jaxpr=jaxpr, in_specs=tuple(in_specs),\n+                      out_specs=tuple(out_specs))\n     effs = core.filter_named_axis_effects(jaxpr.effects, mesh.axis_names)\n     new_eqn = pe.new_jaxpr_eqn(\n         [v for v, used in zip(eqn.invars, used_inputs) if used],\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 1bebba095896..9b4ca76c3bc5 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -736,10 +736,10 @@ def f(x):\n     x = jnp.arange(4 * 4).reshape(4, 4)\n     jaxpr = jax.make_jaxpr(jax.vmap(f, spmd_axis_name='y'))(x).jaxpr\n     e, = jaxpr.eqns\n-    self.assertIn('in_names', e.params)\n-    self.assertEqual(e.params['in_names'], ({0: ('y',), 1: ('x',)},))\n-    self.assertIn('out_names', e.params)\n-    self.assertEqual(e.params['out_names'], ({0: ('y',), 1: ('x',)},))\n+    self.assertIn('in_specs', e.params)\n+    self.assertEqual(e.params['in_specs'], (P('y', 'x'),))\n+    self.assertIn('out_specs', e.params)\n+    self.assertEqual(e.params['out_specs'], (P('y', 'x'),))\n \n   def test_vmap_of_grad_spmd_axis_name(self):\n     mesh = jtu.create_mesh((2, 2), ('x', 'y'))\n@@ -771,10 +771,10 @@ def f(x):\n     x = jnp.arange(4 * 4).reshape(4, 4)\n     jaxpr = jax.make_jaxpr(jax.vmap(f, spmd_axis_name=('x', 'y')))(x).jaxpr\n     e, = jaxpr.eqns\n-    self.assertIn('in_names', e.params)\n-    self.assertEqual(e.params['in_names'], ({0: ('x', 'y',)},))\n-    self.assertIn('out_names', e.params)\n-    self.assertEqual(e.params['out_names'], ({0: ('x', 'y',)},))\n+    self.assertIn('in_specs', e.params)\n+    self.assertEqual(e.params['in_specs'][0], P(('x', 'y')))\n+    self.assertIn('out_specs', e.params)\n+    self.assertEqual(e.params['out_specs'][0], P(('x', 'y')))\n \n   def test_nested_vmap_with_capture_spmd_axis_name(self):\n     self.skipTest('https://github.com/jax-ml/jax/issues/23476')\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Remove tsan suppression from python/cpython#129748\n\nThis bug is marked as fixed upstream.",
        "output": [
            "```diff\nCommit: 0144ec1edf538a1e19dc7736cdf403861975087b\nDate: 2025-05-20T19:55:08Z\nURL: https://github.com/jax-ml/jax/commit/0144ec1edf538a1e19dc7736cdf403861975087b\nFiles changed: 3\nAdditions: +0, Deletions: -7\ndiff --git a/.github/workflows/tsan-suppressions_3.13.txt b/.github/workflows/tsan-suppressions_3.13.txt\nindex a929a8c44728..483e3f0b3c2a 100644\n--- a/.github/workflows/tsan-suppressions_3.13.txt\n+++ b/.github/workflows/tsan-suppressions_3.13.txt\n@@ -23,9 +23,6 @@ race_top:PyMember_GetOne\n race_top:new_reference\n race:_Py_IsOwnedByCurrentThread\n \n-# https://github.com/python/cpython/issues/129748\n-race:mi_block_set_nextx\n-\n # https://github.com/python/cpython/issues/128130\n race_top:run_eval_code_obj\n \ndiff --git a/.github/workflows/tsan-suppressions_3.14.txt b/.github/workflows/tsan-suppressions_3.14.txt\nindex 384560128cfc..008b61933a0b 100644\n--- a/.github/workflows/tsan-suppressions_3.14.txt\n+++ b/.github/workflows/tsan-suppressions_3.14.txt\n@@ -8,9 +8,6 @@ race:dnnl_sgemm\n # https://github.com/python/cpython/issues/128050\n race:partial_vectorcall_fallback\n \n-# https://github.com/python/cpython/issues/129748\n-race:mi_block_set_nextx\n-\n # Races because the LAPACK and BLAS in our scipy isn't TSAN instrumented.\n race:heevd_ffi\n race:gesdd_ffi\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex 882e140b91ad..ce4130c31a30 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -14,7 +14,6 @@ on:\n     paths:\n       - '**/workflows/tsan.yaml'\n       - '**/workflows/tsan-suppressions*.txt'\n-      - '**/workflows/requirements_lock_3_13_ft.patch'\n \n jobs:\n   tsan:\n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "[Mosaic] Tweak `tpu.log` verification on SC.\n\nPiperOrigin-RevId: 761182341",
        "output": [
            "```diff\nCommit: 683a215a154eebc94a3a980e2b80453fb2392553\nDate: 2025-05-20T19:21:39Z\nURL: https://github.com/jax-ml/jax/commit/683a215a154eebc94a3a980e2b80453fb2392553\nFiles changed: 1\nAdditions: +20, Deletions: -10\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\nindex b5e68bf08370..3733bf5d4465 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n+++ b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n@@ -24,6 +24,7 @@ limitations under the License.\n #include \"absl/strings/str_format.h\"\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/ADT/SmallVector.h\"\n+#include \"llvm/Support/Casting.h\"\n #include \"llvm/Support/FormatVariadic.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n@@ -52,15 +53,15 @@ LogicalResult UnrollVectorsOp::canonicalize(UnrollVectorsOp op,\n   RollVectorsOp roll_op =\n       dyn_cast_or_null<RollVectorsOp>(op.getOperand().getDefiningOp());\n   if (!roll_op) {\n-     return failure();\n+    return failure();\n   }\n   if (roll_op.getNumOperands() != op.getNumResults()) {\n-     return failure();\n+    return failure();\n   }\n   for (auto [v1, v2] :\n        llvm::zip(roll_op.getOperandTypes(), op.getResultTypes())) {\n     if (v1 != v2) {\n-       return failure();\n+      return failure();\n     }\n   }\n   rewriter.replaceOp(op, roll_op.getOperands());\n@@ -499,8 +500,7 @@ LogicalResult MemRefReshapeOp::canonicalize(MemRefReshapeOp op,\n   }\n   auto layout_ref = erase_layout_op.getOperand();\n   auto layout_ty = layout_ref.getType();\n-  auto layout =\n-      dyn_cast<tpu::TiledLayoutAttr>(layout_ty.getLayout());\n+  auto layout = dyn_cast<tpu::TiledLayoutAttr>(layout_ty.getLayout());\n   CHECK(!layout.getTiles().empty());\n   auto tile = layout.getTiles().front().dimensions();\n   auto new_tile_strides = ComputeTileStrides(dst_ty, tile);\n@@ -594,8 +594,8 @@ LogicalResult MemRefBitcastOp::canonicalize(MemRefBitcastOp op,\n   if (tile[0] * src_bitwidth % tgt_bitwidth != 0) {\n     return failure();\n   }\n-  SmallVector<xla::Tile, 2> new_tiles =\n-      {xla::Tile({tile[0] * src_bitwidth / tgt_bitwidth, 128})};\n+  SmallVector<xla::Tile, 2> new_tiles = {\n+      xla::Tile({tile[0] * src_bitwidth / tgt_bitwidth, 128})};\n   if (tgt_bitwidth < 32) {\n     new_tiles.push_back(xla::Tile({32 / tgt_bitwidth, 1}));\n   }\n@@ -1325,11 +1325,21 @@ LogicalResult LogOp::verify() {\n     return failure();\n   }\n   CoreType logging_core_type = logging_core_type_maybe->value_or(CoreType::kTc);\n-  if ((logging_core_type == CoreType::kScScalarSubcore ||\n-       logging_core_type == CoreType::kScVectorSubcore) &&\n-      getFormattedAttr() != nullptr && getFormattedAttr().getValue()) {\n+  bool is_sc_core = logging_core_type == CoreType::kScScalarSubcore ||\n+                    logging_core_type == CoreType::kScVectorSubcore;\n+  if (is_sc_core && getFormattedAttr() != nullptr &&\n+      getFormattedAttr().getValue()) {\n     return emitOpError(\"Formatted logging is not supported on SC\");\n   }\n+  if (is_sc_core && getInputs().size() > 1) {\n+    return emitOpError(\"SC logging only supports 0 or 1 inputs\");\n+  }\n+  if (is_sc_core && getInputs().size() == 1) {\n+    Type input_type = getInputs().front().getType();\n+    if (!llvm::isa<MemRefType, IntegerType, FloatType, IndexType>(input_type)) {\n+      return emitOpError(\"SC logging only supports memrefs or scalars\");\n+    }\n+  }\n   switch (logging_core_type) {\n     case CoreType::kTc:\n     case CoreType::kScScalarSubcore:\n\n```"
        ],
        "from_id": [
            "sashabu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Migrate users of backend.compile to backend.compile_and_load.\n\nPiperOrigin-RevId: 761150621",
        "output": [
            "```diff\nCommit: 8cfabd7d545e601fdd111d36acc2142e048d11b5\nDate: 2025-05-20T18:01:48Z\nURL: https://github.com/jax-ml/jax/commit/8cfabd7d545e601fdd111d36acc2142e048d11b5\nFiles changed: 3\nAdditions: +6, Deletions: -6\ndiff --git a/jax/experimental/jax2tf/tests/sharding_test.py b/jax/experimental/jax2tf/tests/sharding_test.py\nindex 55ccb1328c87..5fc45df218cd 100644\n--- a/jax/experimental/jax2tf/tests/sharding_test.py\n+++ b/jax/experimental/jax2tf/tests/sharding_test.py\n@@ -115,7 +115,7 @@ def log_jax_hlo(self, f_jax, args: Sequence[Any], *,\n         executable = backend.compile(\n             jax_hlo, compile_options=compile_options)  # type: ignore\n       else:\n-        executable = backend.compile(\n+        executable = backend.compile_and_load(\n             jax_hlo, xc.DeviceList(tuple(self.devices.flat)), compile_options)  # type: ignore\n       jax_optimized_hlo = executable.hlo_modules()[0].to_string()\n       logging.info(\"[%s] got JAX optimized HLO for platform %s %s\",\ndiff --git a/jax/experimental/jax2tf/tests/tf_test_util.py b/jax/experimental/jax2tf/tests/tf_test_util.py\nindex e87a8af5d15e..faecf9f0f09e 100644\n--- a/jax/experimental/jax2tf/tests/tf_test_util.py\n+++ b/jax/experimental/jax2tf/tests/tf_test_util.py\n@@ -346,7 +346,7 @@ def log_message(extra):\n \n         backend = xla_bridge.get_backend()\n         device_list = xc.DeviceList(tuple(backend.local_devices()))\n-        modules = backend.compile(\n+        modules = backend.compile_and_load(\n             str(jax_lowered.compiler_ir()), device_list).hlo_modules()\n         jax_opt_hlo = modules[0].to_string()\n         logging.info(\"[%s] JAX OPT HLO\\n%s\", self._testMethodName,\ndiff --git a/tests/compilation_cache_test.py b/tests/compilation_cache_test.py\nindex 1ba6b1221a88..5a76d732bd76 100644\n--- a/tests/compilation_cache_test.py\n+++ b/tests/compilation_cache_test.py\n@@ -150,9 +150,9 @@ def test_diff_executables(self):\n       executable1 = backend.compile(computation1, compile_options)\n       executable2 = backend.compile(computation2, compile_options)\n     else:\n-      executable1 = backend.compile(\n+      executable1 = backend.compile_and_load(\n           computation1, executable_devices, compile_options)\n-      executable2 = backend.compile(\n+      executable2 = backend.compile_and_load(\n           computation2, executable_devices, compile_options)\n     cc.put_executable_and_time(\n         \"key1\", \"computation1\", executable1, backend, FAKE_COMPILE_TIME)\n@@ -180,7 +180,7 @@ def test_put_executable(self):\n     if jax._src.lib.jaxlib_extension_version < 331:\n       executable = backend.compile(str(computation), compile_options)\n     else:\n-      executable = backend.compile(\n+      executable = backend.compile_and_load(\n           str(computation), executable_devices, compile_options)\n     key = cc.get_cache_key(computation, devices, compile_options, backend)\n     cc.put_executable_and_time(\n@@ -251,7 +251,7 @@ def test_enable_compilation_cache(self):\n           g = jit(lambda x: x * 3)\n           g(2)\n           cache = cc._get_cache(backend)\n-          self.assertIsNotNone(cache) # Cache should be initalized\n+          self.assertIsNotNone(cache) # Cache should be initialized\n \n   def test_xla_autofdo_profile_version(self):\n     original_profile_version = config.jax_xla_profile_version.value\n\n```"
        ],
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Include Pallas TPU random ops in JAX wheel.\n\nSince the `pallas/tpu/ops/random` directory was missing an `__init__.py` file, it was inadvertently excluded from the released JAX distribution. I don't see any reason why this submodule shouldn't be included so let's fix that!\n\nTo deal with the fact that they weren't included in the distribution, we were also monkey patching these files into the wheel when testing, but that's no longer needed.\n\nPiperOrigin-RevId: 761138525",
        "output": [
            "```diff\nCommit: 86680a9b1282d00398f3d3d3a56336c0452a76b8\nDate: 2025-05-20T17:33:45Z\nURL: https://github.com/jax-ml/jax/commit/86680a9b1282d00398f3d3d3a56336c0452a76b8\nFiles changed: 2\nAdditions: +14, Deletions: -3\ndiff --git a/BUILD.bazel b/BUILD.bazel\nindex 59fd949b7ad8..887f28d4583e 100644\n--- a/BUILD.bazel\n+++ b/BUILD.bazel\n@@ -108,9 +108,6 @@ genrule(\n         \"//jax:internal_test_harnesses\",\n         \"//jax:internal_test_util\",\n         \"//jax:internal_export_back_compat_test_data\",\n-        \"//jax:experimental/pallas/ops/tpu/random/philox.py\",\n-        \"//jax:experimental/pallas/ops/tpu/random/prng_utils.py\",\n-        \"//jax:experimental/pallas/ops/tpu/random/threefry.py\",\n         \"//jax/experimental/mosaic/gpu/examples:flash_attention.py\",\n         \"//jax/experimental/mosaic/gpu/examples:matmul.py\",\n         \"//jax:test_multiprocess\",\ndiff --git a/jax/experimental/pallas/ops/tpu/random/__init__.py b/jax/experimental/pallas/ops/tpu/random/__init__.py\nnew file mode 100644\nindex 000000000000..3da0dd1fa3ca\n--- /dev/null\n+++ b/jax/experimental/pallas/ops/tpu/random/__init__.py\n@@ -0,0 +1,14 @@\n+# Copyright 2025 The JAX Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n\n```"
        ],
        "from_id": [
            "dfm",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Use `MemorySpace` aliases\n\nThis just makes the corresponding conditions a bit easier to read.\n\nPiperOrigin-RevId: 761137840",
        "output": [
            "```diff\nCommit: 4a3ce2b2dc75bdb02b52687c24dfb7d182278be5\nDate: 2025-05-20T17:31:35Z\nURL: https://github.com/jax-ml/jax/commit/4a3ce2b2dc75bdb02b52687c24dfb7d182278be5\nFiles changed: 2\nAdditions: +5, Deletions: -6\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex db959b1dbc24..eb5fc136082e 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -649,7 +649,7 @@ def ref_for_aval(aval: jax_core.AbstractValue):\n     aval = v.aval\n     if (isinstance(aval, pallas_core.AbstractMemoryRef) and\n         jnp.issubdtype(aval.dtype, pallas_core.semaphore_dtype)):\n-      if aval.memory_space != gpu_core.MemorySpace.GMEM:\n+      if aval.memory_space != gpu_core.GMEM:\n         raise ValueError(\n             \"Only GMEM memory space is supported for semaphores in Mosaic GPU.\"\n         )\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex af9d4138cfbb..53c890932e38 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -1200,19 +1200,18 @@ def _tcgen05_mma_abstract_eval(acc, a, b, barrier, accumulate,\n                                collective_axis):\n   del (accumulate, transforms_leaves, a_transforms_tree, b_transforms_tree)\n \n-  if acc.memory_space != gpu_core.MemorySpace.TMEM:\n+  if acc.memory_space != gpu_core.TMEM:\n     raise ValueError(\"Accumulator must be a TMEM Ref.\")\n-  if a.memory_space not in (gpu_core.MemorySpace.SMEM,\n-                            gpu_core.MemorySpace.TMEM):\n+  if a.memory_space not in (gpu_core.SMEM, gpu_core.TMEM):\n     raise ValueError(\"LHS must be a TMEM/SMEM Ref.\")\n-  if b.memory_space != gpu_core.MemorySpace.SMEM:\n+  if b.memory_space != gpu_core.SMEM:\n     raise ValueError(\"RHS must be an SMEM Ref.\")\n \n   if collective_axis is not None:\n     if not acc.collective:\n       raise ValueError(\n           \"Accumulator Ref must be collective if collective_axis is set.\")\n-    if a.memory_space == gpu_core.MemorySpace.TMEM and not a.collective:\n+    if a.memory_space == gpu_core.TMEM and not a.collective:\n       raise ValueError(\n           \"LHS TMEM Ref must be collective if collective_axis is set.\")\n \n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Replace `core_map` + `run_state` with `plgpu.kernel` for simpler code. Slightly more efficient because we don’t initialize the outputs now.\n\nPiperOrigin-RevId: 761113870",
        "output": [
            "```diff\nCommit: 496cbd07cea5134e9ab83a72fc33941acc6149b8\nDate: 2025-05-20T16:27:22Z\nURL: https://github.com/jax-ml/jax/commit/496cbd07cea5134e9ab83a72fc33941acc6149b8\nFiles changed: 1\nAdditions: +22, Deletions: -38\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex 6da468f2cc3e..c7e9f95e3f99 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -658,10 +658,9 @@ def attention_with_pipeline_emitter(q, k, v, config: TuningConfig, save_residual\n   if rem:\n     raise NotImplementedError(f\"{q_seq_len=} must be a multiple of {block_q * 2=}\")\n \n-  def fa3_kernel(q_ref, k_ref, v_ref, out_ref, lse_ref, scoped):\n+  def fa3_kernel(q_ref, k_ref, v_ref, out_ref, lse_ref, smem_buffers, q_barriers, schedule_barrier):\n     batch = lax.axis_index(\"batch\")\n     wg_idx = lax.axis_index(\"wg\")\n-    smem_buffers, q_barriers, schedule_barrier = scoped\n     qo_smem2, lse_smem2 = smem_buffers\n     q_seq_base = lax.axis_index(\"q_seq\") * (2 * block_q) + wg_idx * block_q\n     q_head = lax.axis_index(\"heads\")\n@@ -758,46 +757,31 @@ def compute_pv(acc_ref):\n     k_ref = k_ref.at[batch, :, kv_head, :]\n     v_ref = v_ref.at[batch, :, kv_head, :]\n     pipeline(k_ref, v_ref)\n-  mesh = plgpu.Mesh(\n+\n+  out_shape = [q, None]\n+  if save_residuals:\n+    out_shape[1] = jax.ShapeDtypeStruct((batch_size, num_q_heads, q_seq_len), jnp.float32)\n+\n+  qo_scratch = plgpu.SMEM((compute_wgs, block_q, head_dim), jnp.float16)\n+  smem_scratch = [qo_scratch, None]\n+  if save_residuals:\n+    smem_scratch[1] = plgpu.SMEM((compute_wgs, block_q), jnp.float32)\n+\n+  out, lse = plgpu.kernel(\n+      fa3_kernel,\n       grid=(batch_size, num_q_tiles, num_q_heads),\n       grid_names=(\"batch\", \"q_seq\", \"heads\"),\n       num_threads=3,\n       thread_name=\"wg\",\n-  )\n-  def run(refs):\n-    q_ref, k_ref, v_ref, out_ref, lse_ref = refs\n-\n-    @pl.core_map(\n-        mesh,\n-        compiler_params=plgpu.CompilerParams(\n-            approx_math=True, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n-        ),\n-    )\n-    def _kernel_entry():\n-      qo_scratch = plgpu.SMEM(\n-          (compute_wgs, block_q, head_dim), jnp.float16,\n-      )\n-      scratch = [qo_scratch, None]\n-      if save_residuals:\n-        scratch[1] = plgpu.SMEM((compute_wgs, block_q), jnp.float32)\n-      pl.run_scoped(\n-          lambda *args: fa3_kernel(q_ref, k_ref, v_ref, out_ref, lse_ref, args),\n-          scratch,\n-          plgpu.Barrier(1, num_barriers=compute_wgs),\n-          plgpu.Barrier(num_arrivals=compute_wgs),\n-          collective_axes=\"wg\",\n-      )\n-  @jax.jit\n-  def run_function(q, k, v, o, lse):\n-    *_, out, lse = pl.run_state(run)((q, k, v, o, lse))\n-    return out, lse\n-\n-  lse = (\n-      jnp.full((batch_size, num_q_heads, q_seq_len), -jnp.inf, dtype=jnp.float32)\n-      if save_residuals\n-      else None\n-  )\n-  out, lse = run_function(q, k, v, jnp.full_like(q, jnp.inf), lse)\n+            out_shape=out_shape,\n+      scratch_shapes=(\n+          tuple(smem_scratch),  # type: ignore\n+          plgpu.Barrier(1, num_barriers=compute_wgs),  # type: ignore\n+          plgpu.Barrier(num_arrivals=compute_wgs),),  # type: ignore\n+      compiler_params=plgpu.CompilerParams(\n+          approx_math=True, lowering_semantics=plgpu.LoweringSemantics.Warpgroup,\n+      ),\n+  )(q, k, v)\n \n   if save_residuals:\n     assert lse is not None\n\n```"
        ],
        "from_id": [
            "Rifur13",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28848 from dfm:fix-welch-ci\n\nPiperOrigin-RevId: 761101581",
        "output": [
            "```diff\nCommit: 69a182599ae953807a56ff99ff11bdf3b46c904b\nDate: 2025-05-20T15:52:51Z\nURL: https://github.com/jax-ml/jax/commit/69a182599ae953807a56ff99ff11bdf3b46c904b\nFiles changed: 2\nAdditions: +3, Deletions: -4\ndiff --git a/jax/_src/third_party/scipy/signal_helper.py b/jax/_src/third_party/scipy/signal_helper.py\nindex 4a021675804d..ad7bdfbef62a 100644\n--- a/jax/_src/third_party/scipy/signal_helper.py\n+++ b/jax/_src/third_party/scipy/signal_helper.py\n@@ -57,7 +57,7 @@ def _triage_segments(window: ArrayLike | str | tuple[Any, ...], nperseg: int | N\n       win = get_window(window, nperseg_int)\n     win = jnp.array(win, dtype=dtype)\n   else:\n-    win = jnp.asarray(window)\n+    win = jnp.asarray(window, dtype=dtype)\n     nperseg_int = win.size if nperseg is None else int(nperseg)\n     if win.ndim != 1:\n       raise ValueError('window must be 1-D')\ndiff --git a/tests/scipy_signal_test.py b/tests/scipy_signal_test.py\nindex 7ff3c87435c7..b1c5d9c98fed 100644\n--- a/tests/scipy_signal_test.py\n+++ b/tests/scipy_signal_test.py\n@@ -357,12 +357,11 @@ def testWelchWithDefaultStepArgsAgainstNumpy(\n     if use_nperseg:\n       kwargs['nperseg'] = nperseg\n     if use_window:\n-      kwargs['window'] = jnp.array(osp_signal.get_window('hann', nperseg),\n-                                   dtype=dtypes.to_complex_dtype(dtype))\n+      kwargs['window'] = jnp.array(osp_signal.get_window('hann', nperseg))\n     if use_noverlap:\n       kwargs['noverlap'] = noverlap\n \n-    @jtu.ignore_warning(message=\"nperseg = 256 is greater than\")\n+    @jtu.ignore_warning(message=\"nperseg\")\n     def osp_fun(x):\n       freqs, Pxx = osp_signal.welch(x, **kwargs)\n       return freqs.astype(_real_dtype(dtype)), Pxx.astype(_real_dtype(dtype))\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Rename backend.compile to backend.compile_and_load.\n\nPart of a larger refactor. Today, `compile` returns a loaded executable i.e., fuses the compile and load functions. Eventually, `compile` should return an unloaded executable and `load` should return a loaded exectuable; the default jit path will still return a loaded executable.\n\nPiperOrigin-RevId: 761098001",
        "output": [
            "```diff\nCommit: 06448864abd6e8187e5b4d9b1ff08ab14fe3b8e0\nDate: 2025-05-20T15:41:36Z\nURL: https://github.com/jax-ml/jax/commit/06448864abd6e8187e5b4d9b1ff08ab14fe3b8e0\nFiles changed: 4\nAdditions: +54, Deletions: -11\ndiff --git a/jax/_src/compiler.py b/jax/_src/compiler.py\nindex 04f993fed799..e8ef647a1312 100644\n--- a/jax/_src/compiler.py\n+++ b/jax/_src/compiler.py\n@@ -292,6 +292,19 @@ def backend_compile(\n     executable_devices: xc.DeviceList,\n     options: xc.CompileOptions,\n     host_callbacks: Sequence[Any],\n+) -> xc.LoadedExecutable:\n+  return backend_compile_and_load(\n+      backend, module, executable_devices, options, host_callbacks\n+  )\n+\n+\n+@profiler.annotate_function\n+def backend_compile_and_load(\n+    backend: xc.Client,\n+    module: ir.Module,\n+    executable_devices: xc.DeviceList,\n+    options: xc.CompileOptions,\n+    host_callbacks: Sequence[Any],\n ) -> xc.LoadedExecutable:\n   sym_name = module.operation.attributes['sym_name']\n   module_name = ir.StringAttr(sym_name).value\n@@ -322,18 +335,35 @@ def backend_compile(\n \n     # we use a separate function call to ensure that XLA compilation appears\n     # separately in Python profiling results\n-    if host_callbacks:\n+    elif jaxlib_extension_version < 342 or isinstance(backend, xc.CompileOnlyPyClient):\n+      if host_callbacks:\n+        return backend.compile(\n+            built_c,\n+            executable_devices=executable_devices,  # type: ignore\n+            compile_options=options,\n+            host_callbacks=host_callbacks,\n+        )\n+      # Some backends don't have `host_callbacks` option yet\n+      # TODO(sharadmv): remove this fallback when all backends allow `compile`\n+      # to take in `host_callbacks`\n       return backend.compile(\n+          built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n+    else:\n+      if host_callbacks:\n+        return backend.compile_and_load(\n+            built_c,\n+            executable_devices=executable_devices,\n+            compile_options=options,\n+            host_callbacks=host_callbacks,\n+        )\n+      # Some backends don't have `host_callbacks` option yet\n+      # TODO(sharadmv): remove this fallback when all backends allow `compile`\n+      # to take in `host_callbacks`\n+      return backend.compile_and_load(\n           built_c,\n-          executable_devices=executable_devices,  # type: ignore\n+          executable_devices=executable_devices,\n           compile_options=options,\n-          host_callbacks=host_callbacks,\n       )\n-    # Some backends don't have `host_callbacks` option yet\n-    # TODO(sharadmv): remove this fallback when all backends allow `compile`\n-    # to take in `host_callbacks`\n-    return backend.compile(\n-        built_c, executable_devices=executable_devices, compile_options=options)  # type: ignore\n   except xc.XlaRuntimeError as e:\n     for error_handler in _XLA_RUNTIME_ERROR_HANDLERS:\n       handler_result = error_handler(e)\n@@ -398,7 +428,7 @@ def compile_or_get_cached(\n   )\n \n   if cache_key is None:\n-    return backend_compile(\n+    return backend_compile_and_load(\n         backend, computation, executable_devices, compile_options,\n         host_callbacks)\n \n@@ -426,7 +456,7 @@ def compile_or_get_cached(\n       config.share_binary_between_hosts.value\n       and is_multi_process\n       and distributed.global_state.client is not None\n-      # Host callbacks are currently baked into the HLO module so we cant share\n+      # Host callbacks are currently baked into the HLO module so we can't share\n       # them.\n       and len(host_callbacks) == 0\n   ):\n@@ -716,7 +746,7 @@ def _compile_and_write_cache(\n     cache_key: str,\n ) -> xc.LoadedExecutable:\n   start_time = time.monotonic()\n-  executable = backend_compile(\n+  executable = backend_compile_and_load(\n       backend, computation, executable_devices, compile_options, host_callbacks\n   )\n   compile_time = time.monotonic() - start_time\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 1d7f3042e8a3..000c05acacad 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -551,6 +551,17 @@ class Client:\n   ) -> PjRtLayout: ...\n   def __getattr__(self, name: str) -> Any: ...\n \n+\n+class CompileOnlyPyClient(Client):\n+  def compile(\n+      self,\n+      computation: str | bytes,\n+      executable_devices: DeviceList | Sequence[Device],\n+      compile_options: CompileOptions = ...,\n+      host_callbacks: Sequence[Any] = ...,\n+  ) -> LoadedExecutable: ...\n+\n+\n class CpuCollectives: ...\n \n def make_gloo_tcp_collectives(\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 8f8c829ee6c7..b1bbc464610e 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -304,6 +304,7 @@ def computation_count():\n \n XlaComputation = _xla.XlaComputation\n Client = _xla.Client\n+CompileOnlyPyClient = _xla.CompileOnlyPyClient\n Memory = _xla.Memory\n Array = _xla.Array\n ArrayImpl = _xla.ArrayImpl\ndiff --git a/jaxlib/xla_client.pyi b/jaxlib/xla_client.pyi\nindex 80599e86676b..fce114f45474 100644\n--- a/jaxlib/xla_client.pyi\n+++ b/jaxlib/xla_client.pyi\n@@ -24,6 +24,7 @@ from jaxlib._jax import ArrayCopySemantics as ArrayCopySemantics\n from jaxlib._jax import ArrayImpl as ArrayImpl\n from jaxlib._jax import AutotuneCacheMode as AutotuneCacheMode\n from jaxlib._jax import Client as Client\n+from jaxlib._jax import CompileOnlyPyClient as CompileOnlyPyClient\n from jaxlib._jax import CompileOptions as CompileOptions\n from jaxlib._jax import Device as Device\n from jaxlib._jax import DeviceAssignment as DeviceAssignment\n\n```"
        ],
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Removed the `GPU*` prefix from Mosaic GPU-specific types\n\nThese APIs are always used qualified, e.g. `plgpu.GPUCompilerParams`, so the\nprefix is redundant.\n\nPiperOrigin-RevId: 761088896",
        "output": [
            "```diff\nCommit: d1511c1d781cf0b3992ee9e62ae0773deb9cb833\nDate: 2025-05-20T15:13:24Z\nURL: https://github.com/jax-ml/jax/commit/d1511c1d781cf0b3992ee9e62ae0773deb9cb833\nFiles changed: 11\nAdditions: +157, Deletions: -135\ndiff --git a/docs/jax.experimental.pallas.mosaic_gpu.rst b/docs/jax.experimental.pallas.mosaic_gpu.rst\nindex 2d3452609c75..4191dde74df7 100644\n--- a/docs/jax.experimental.pallas.mosaic_gpu.rst\n+++ b/docs/jax.experimental.pallas.mosaic_gpu.rst\n@@ -10,9 +10,9 @@ Classes\n    :toctree: _autosummary\n \n    Barrier\n-   GPUBlockSpec\n-   GPUCompilerParams\n-   GPUMemorySpace\n+   BlockSpec\n+   CompilerParams\n+   MemorySpace\n    Layout\n    SwizzleTransform\n    TilingTransform\ndiff --git a/docs/pallas/gpu/reference.md b/docs/pallas/gpu/reference.md\nindex 0db31e11b459..7b4a1e6e9c7d 100644\n--- a/docs/pallas/gpu/reference.md\n+++ b/docs/pallas/gpu/reference.md\n@@ -225,17 +225,20 @@ def body(..., scratch_ref):\n There are two ways in which references are allocated and each has a way to select\n the desired transforms:\n \n-**1. Using `GPUBlockSpec`**\n+**1. Using `plgpu.BlockSpec`**\n \n ```python\n transforms = (plgpu.TileTransform((8, 64)), plgpu.SwizzleTransform(128))\n f = pl.pallas_call(\n-  in_specs=plgpu.GPUBlockSpec(in_block_shape, in_index_map, transforms=transforms),\n-  out_specs=plgpu.GPUBlockSpec(out_block_shape, out_index_map, transforms=transforms),\n+  in_specs=plgpu.BlockSpec(in_block_shape, in_index_map, transforms=transforms),\n+  out_specs=plgpu.BlockSpec(out_block_shape, out_index_map, transforms=transforms),\n   ...\n )\n ```\n \n+Note that unlike `plgpu.BlockSpec`, `pl.BlockSpec` does *not* allow specifying\n+transforms.\n+\n **2. Specifying the `transforms` argument on the allocated `SMEM`**\n \n ```python\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex d13977ac4fbf..08e47cec4b2e 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -71,7 +71,7 @@ def _slices(d):\n \n \n @dataclasses.dataclass(frozen=True, kw_only=True)\n-class GPUCompilerParams(pallas_core.CompilerParams):\n+class CompilerParams(pallas_core.CompilerParams):\n   \"\"\"Mosaic GPU compiler parameters.\n \n   Attributes:\n@@ -108,7 +108,7 @@ def __post_init__(self):\n       )\n \n \n-class GPUMemorySpace(enum.Enum):\n+class MemorySpace(enum.Enum):\n   #: Global memory.\n   GMEM = \"gmem\"\n   #: Shared memory.\n@@ -145,7 +145,7 @@ def __call__(self, shape: tuple[int, ...]):\n       dtype = pallas_core.BarrierSemaphore()\n     else:\n       dtype = pallas_core.Semaphore()\n-    return pallas_core.MemoryRef(shape, dtype, GPUMemorySpace.GMEM)\n+    return pallas_core.MemoryRef(shape, dtype, MemorySpace.GMEM)\n \n   def get_array_aval(self) -> jax_core.ShapedArray:\n     return self(()).get_array_aval()\n@@ -183,7 +183,7 @@ def kernel(\n   def wrapper(*operands):\n     def stateful(operand_and_out_refs):\n       operand_refs, out_refs = operand_and_out_refs\n-      mesh = GPUMesh(**mesh_kwargs)\n+      mesh = Mesh(**mesh_kwargs)\n       thread_name = mesh.thread_name if mesh.thread_name is not None else ()\n       def cmap_body():\n         pallas_primitives.run_scoped(\n@@ -234,7 +234,7 @@ class GPUMemoryRef(pallas_core.MemoryRef):\n   collective: bool | None = dataclasses.field(default=None, kw_only=True)\n \n   def __post_init__(self):\n-    if self.memory_space != GPUMemorySpace.TMEM:\n+    if self.memory_space != MemorySpace.TMEM:\n       if self.packed is not None:\n         raise ValueError(\"Packed option is only supported for TMEM.\")\n       if self.collective is not None:\n@@ -244,7 +244,7 @@ def get_ref_aval(self) -> _Ref:\n     aval = jax_core.ShapedArray(self.shape, self.dtype)\n     for t in self.transforms:\n       aval = t(aval)\n-    if self.memory_space == GPUMemorySpace.TMEM:\n+    if self.memory_space == MemorySpace.TMEM:\n       ref = pallas_core.TransformedRef(\n           AbstractTMEMRef(aval,\n                           memory_space=self.memory_space,\n@@ -785,7 +785,7 @@ def pretty_print(self, context: jax_core.JaxprPpContext) -> pp.Doc:\n \n \n @dataclasses.dataclass\n-class GPUBlockSpec(pallas_core.BlockSpec):\n+class BlockSpec(pallas_core.BlockSpec):\n   transforms: Sequence[MemoryRefTransform] = ()\n \n   def to_block_mapping(\n@@ -817,10 +817,10 @@ def to_block_mapping(\n     )\n \n \n-GMEM = GPUMemorySpace.GMEM\n-SMEM = GPUMemorySpace.SMEM\n-TMEM = GPUMemorySpace.TMEM\n-REGS = GPUMemorySpace.REGS\n+GMEM = MemorySpace.GMEM\n+SMEM = MemorySpace.SMEM\n+TMEM = MemorySpace.TMEM\n+REGS = MemorySpace.REGS\n \n \n class barrier_dtype(dtypes.extended):\n@@ -903,7 +903,7 @@ def get_ref_aval(self) -> AbstractMemoryRef:\n           \"Preinitialized WGMMAAccumulatorRef only supported in pl.run_state.\"\n       )\n     return WGMMAAbstractAccumulatorRef(\n-        jax_core.ShapedArray(shape=self.shape, dtype=self.dtype), GPUMemorySpace.REGS\n+        jax_core.ShapedArray(shape=self.shape, dtype=self.dtype), MemorySpace.REGS\n     )\n \n   @staticmethod\n@@ -913,7 +913,7 @@ def init(array):\n \n def _wgmma_ref_type_mapping(ref: WGMMAAccumulatorRef):\n   aval = WGMMAAbstractAccumulatorRef(\n-      jax_core.ShapedArray(shape=ref.shape, dtype=ref.dtype), GPUMemorySpace.REGS\n+      jax_core.ShapedArray(shape=ref.shape, dtype=ref.dtype), MemorySpace.REGS\n   )\n   return aval, ref._init\n state_types._ref_type_aval_mappings[WGMMAAccumulatorRef] = _wgmma_ref_type_mapping\n@@ -962,7 +962,7 @@ def __repr__(self) -> str:\n _WARPGROUP_AXIS_NAME = object()\n \n @dataclasses.dataclass(frozen=True, kw_only=True)\n-class GPUMesh:\n+class Mesh:\n   grid: Sequence[int] = ()\n   grid_names: Sequence[str] = ()\n   cluster: Sequence[int] = ()\n@@ -1049,15 +1049,15 @@ def _gpu_mesh_discharge_rule(\n     cost_estimate,\n     name,\n ):\n-  if not isinstance(mesh, GPUMesh):\n-    raise TypeError(f\"Mesh must be a GPUMesh, got {type(mesh)}\")\n-  if compiler_params and not isinstance(compiler_params, GPUCompilerParams):\n+  if not isinstance(mesh, Mesh):\n+    raise TypeError(f\"Mesh must be a `plgpu.Mesh`, got {type(mesh)}\")\n+  if compiler_params and not isinstance(compiler_params, CompilerParams):\n     raise TypeError(\n-        \"Compiler params must be a GPUCompilerParams, got\"\n+        \"Compiler params must be a `plgpu.CompilerParams`, got\"\n         f\" {type(compiler_params)}\"\n     )\n   if not compiler_params:\n-    compiler_params = GPUCompilerParams()\n+    compiler_params = CompilerParams()\n   return pallas_core.default_mesh_discharge_rule(\n       in_avals,\n       out_avals,\n@@ -1073,7 +1073,7 @@ def _gpu_mesh_discharge_rule(\n   )\n \n \n-pallas_core._core_map_mesh_rules[GPUMesh] = _gpu_mesh_discharge_rule\n+pallas_core._core_map_mesh_rules[Mesh] = _gpu_mesh_discharge_rule\n \n \n class MemoryEffect(jax_core.Effect):\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 751a2bae2ed0..db959b1dbc24 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -568,7 +568,7 @@ def index_map(*indices):\n     )\n     return eval_index_map(*new_indices)\n \n-  return gpu_core.GPUBlockSpec(\n+  return gpu_core.BlockSpec(\n       bm.block_shape,\n       index_map,\n       memory_space=bm.transformed_block_aval.memory_space,\n@@ -581,7 +581,7 @@ def lower_pipelined_jaxpr_to_module(\n     gpu_mesh: pallas_core.Mesh | None,\n     jax_mesh: mesh_lib.Mesh | None,\n     jaxpr: jax_core.Jaxpr,\n-    params: gpu_core.GPUCompilerParams,\n+    params: gpu_core.CompilerParams,\n     cost_estimate: pallas_core.CostEstimate | None,\n ) -> LoweringResult:\n   del cost_estimate  # Unused.\n@@ -604,7 +604,7 @@ def lower_pipelined_jaxpr_to_module(\n   )\n \n   if gpu_mesh:\n-    assert isinstance(gpu_mesh, gpu_core.GPUMesh)\n+    assert isinstance(gpu_mesh, gpu_core.Mesh)\n     block = (128 * (gpu_mesh.num_threads or 1), 1, 1)\n     grid = gpu_mesh.grid\n     thread_axis = (\n@@ -649,7 +649,7 @@ def ref_for_aval(aval: jax_core.AbstractValue):\n     aval = v.aval\n     if (isinstance(aval, pallas_core.AbstractMemoryRef) and\n         jnp.issubdtype(aval.dtype, pallas_core.semaphore_dtype)):\n-      if aval.memory_space != gpu_core.GPUMemorySpace.GMEM:\n+      if aval.memory_space != gpu_core.MemorySpace.GMEM:\n         raise ValueError(\n             \"Only GMEM memory space is supported for semaphores in Mosaic GPU.\"\n         )\n@@ -747,7 +747,7 @@ def lower_jaxpr_to_module(\n     out_shapes: Sequence[jax.ShapeDtypeStruct],\n     gmem_scratch_shapes: Sequence[jax.ShapeDtypeStruct],\n     jaxpr: jax_core.Jaxpr,\n-    params: gpu_core.GPUCompilerParams,\n+    params: gpu_core.CompilerParams,\n     consts=(),\n ) -> LoweringResult:\n   debug_info = jaxpr.debug_info\n@@ -2048,7 +2048,7 @@ def _resolve_cluster_axis(axis_names: _AxisNames | None, axis_name: str):\n   if not axis_names:\n     raise LookupError(\n         \"No axis names are available. Make sure you are using `pl.core_map`\"\n-        \" with a `plgpu.GPUMesh`.\"\n+        \" with a `plgpu.Mesh`.\"\n     )\n   if not axis_names or axis_name not in axis_names.cluster:\n     raise LookupError(\n@@ -2066,7 +2066,7 @@ def _axis_index_rule(ctx: LoweringRuleContext, *, axis_name: Hashable):\n   if gpu_axis_names is None and not jax_axis_names:\n     raise LookupError(\n         \"No axis names are available. Make sure you are using `pl.core_map`\"\n-        \" with a `plgpu.GPUMesh` or an appropriate JAX device mesh.\"\n+        \" with a `plgpu.Mesh` or an appropriate JAX device mesh.\"\n     )\n   if axis_name not in itertools.chain((gpu_axis_names or ()), jax_axis_names):\n     raise LookupError(\ndiff --git a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\nindex 72e6f96c125a..ef1ba37f0f5c 100644\n--- a/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n+++ b/jax/_src/pallas/mosaic_gpu/pallas_call_registration.py\n@@ -63,9 +63,9 @@ def pallas_call_lowering(\n   mgpu.dialect.register_dialect(ctx.module_context.context)  # pytype: disable=attribute-error\n \n   if \"mosaic_gpu\" in compiler_params:\n-    params = cast(gpu_core.GPUCompilerParams, compiler_params[\"mosaic_gpu\"])\n+    params = cast(gpu_core.CompilerParams, compiler_params[\"mosaic_gpu\"])\n   else:\n-    params = gpu_core.GPUCompilerParams()\n+    params = gpu_core.CompilerParams()\n \n   jax_mesh = None\n   axis_context = ctx.module_context.axis_context\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 0e9319972949..af9d4138cfbb 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -56,7 +56,7 @@\n \n \n def _check_ref(\n-    aval: object, name: str, memory_space: gpu_core.GPUMemorySpace\n+    aval: object, name: str, memory_space: gpu_core.MemorySpace\n ) -> None:\n   if not isinstance(aval, state_types.AbstractRef):\n     raise TypeError(f\"{name} must be a reference, got {aval}\")\n@@ -1200,19 +1200,19 @@ def _tcgen05_mma_abstract_eval(acc, a, b, barrier, accumulate,\n                                collective_axis):\n   del (accumulate, transforms_leaves, a_transforms_tree, b_transforms_tree)\n \n-  if acc.memory_space != gpu_core.GPUMemorySpace.TMEM:\n+  if acc.memory_space != gpu_core.MemorySpace.TMEM:\n     raise ValueError(\"Accumulator must be a TMEM Ref.\")\n-  if a.memory_space not in (gpu_core.GPUMemorySpace.SMEM,\n-                            gpu_core.GPUMemorySpace.TMEM):\n+  if a.memory_space not in (gpu_core.MemorySpace.SMEM,\n+                            gpu_core.MemorySpace.TMEM):\n     raise ValueError(\"LHS must be a TMEM/SMEM Ref.\")\n-  if b.memory_space != gpu_core.GPUMemorySpace.SMEM:\n+  if b.memory_space != gpu_core.MemorySpace.SMEM:\n     raise ValueError(\"RHS must be an SMEM Ref.\")\n \n   if collective_axis is not None:\n     if not acc.collective:\n       raise ValueError(\n           \"Accumulator Ref must be collective if collective_axis is set.\")\n-    if a.memory_space == gpu_core.GPUMemorySpace.TMEM and not a.collective:\n+    if a.memory_space == gpu_core.MemorySpace.TMEM and not a.collective:\n       raise ValueError(\n           \"LHS TMEM Ref must be collective if collective_axis is set.\")\n \ndiff --git a/jax/_src/pallas/pallas_call.py b/jax/_src/pallas/pallas_call.py\nindex 2d27bd3cc485..964709b4c915 100644\n--- a/jax/_src/pallas/pallas_call.py\n+++ b/jax/_src/pallas/pallas_call.py\n@@ -1551,7 +1551,7 @@ def pallas_call(\n       backend-specific dataclass\n       (:class:`jax.experimental.pallas.tpu.TPUCompilerParams`,\n       :class:`jax.experimental.pallas.triton.TritonCompilerParams`,\n-      :class:`jax.experimental.pallas.mosaic_gpu.GPUCompilerParams`) or a dict\n+      :class:`jax.experimental.pallas.mosaic_gpu.CompilerParams`) or a dict\n       mapping backend name to the corresponding platform-specific dataclass.\n     backend: Optional string literal one of  ``\"mosaic_tpu\"``, ``\"triton\"`` or\n       ``\"mosaic_gpu\"`` determining the backend to be used. None means let Pallas\ndiff --git a/jax/experimental/pallas/mosaic_gpu.py b/jax/experimental/pallas/mosaic_gpu.py\nindex a7d8c3e34223..cc0e185e296a 100644\n--- a/jax/experimental/pallas/mosaic_gpu.py\n+++ b/jax/experimental/pallas/mosaic_gpu.py\n@@ -19,10 +19,10 @@\n \n from jax._src.pallas.mosaic_gpu.core import Barrier as Barrier\n from jax._src.pallas.mosaic_gpu.core import ClusterBarrier as ClusterBarrier\n-from jax._src.pallas.mosaic_gpu.core import GPUBlockSpec as GPUBlockSpec\n-from jax._src.pallas.mosaic_gpu.core import GPUCompilerParams as GPUCompilerParams\n-from jax._src.pallas.mosaic_gpu.core import GPUMesh as GPUMesh\n-from jax._src.pallas.mosaic_gpu.core import GPUMemorySpace as GPUMemorySpace\n+from jax._src.pallas.mosaic_gpu.core import BlockSpec as BlockSpec\n+from jax._src.pallas.mosaic_gpu.core import CompilerParams as CompilerParams\n+from jax._src.pallas.mosaic_gpu.core import Mesh as Mesh\n+from jax._src.pallas.mosaic_gpu.core import MemorySpace as MemorySpace\n from jax._src.pallas.mosaic_gpu.core import kernel as kernel\n from jax._src.pallas.mosaic_gpu.core import PeerMemRef as PeerMemRef\n from jax._src.pallas.mosaic_gpu.core import RefUnion as RefUnion\n@@ -63,9 +63,15 @@\n from jax.experimental.mosaic.gpu.core import LoweringSemantics as LoweringSemantics\n \n \n-#: Alias of :data:`jax.experimental.pallas.mosaic_gpu.GPUMemorySpace.GMEM`.\n-GMEM = GPUMemorySpace.GMEM\n-#: Alias of :data:`jax.experimental.pallas.mosaic_gpu.GPUMemorySpace.SMEM`.\n-SMEM = GPUMemorySpace.SMEM\n-#: Alias of :data:`jax.experimental.pallas.mosaic_gpu.GPUMemorySpace.TMEM`.\n-TMEM = GPUMemorySpace.TMEM\n+#: Alias of :data:`jax.experimental.pallas.mosaic_gpu.MemorySpace.GMEM`.\n+GMEM = MemorySpace.GMEM\n+#: Alias of :data:`jax.experimental.pallas.mosaic_gpu.MemorySpace.SMEM`.\n+SMEM = MemorySpace.SMEM\n+#: Alias of :data:`jax.experimental.pallas.mosaic_gpu.MemorySpace.TMEM`.\n+TMEM = MemorySpace.TMEM\n+\n+# TODO(slebedev): Deprecate and remove these aliases.\n+GPUBlockSpec = BlockSpec\n+GPUCompilerParams = CompilerParams\n+GPUMemorySpace = MemorySpace\n+GPUMesh = Mesh\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex a100aa96faba..6da468f2cc3e 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -306,7 +306,7 @@ def entry(q_ref, k_ref, v_ref, out_ref, lse_ref):\n       grid_names=(\"batch\", \"q_seq\", \"heads\"),\n       num_threads=3,\n       thread_name=\"wg\",\n-      compiler_params=plgpu.GPUCompilerParams(approx_math=True),\n+      compiler_params=plgpu.CompilerParams(approx_math=True),\n   )(q, k, v)\n \n   if save_residuals:\n@@ -451,11 +451,11 @@ def compute_dq(acc_ref):\n         manual_consumed_barriers=True,\n         compute_context=_compute_thread,\n         in_specs=[\n-            plgpu.GPUBlockSpec(  # k\n+            plgpu.BlockSpec(  # k\n                 block_shape=(block_kv, head_dim),\n                 index_map=lambda i: (i, 0),\n                 transforms=[tiling, swizzle]),\n-            plgpu.GPUBlockSpec(  # v\n+            plgpu.BlockSpec(  # v\n                 block_shape=(block_kv, head_dim),\n                 index_map=lambda i: (i, 0),\n                 transforms=[tiling, swizzle]),\n@@ -558,16 +558,16 @@ def compute_dk(acc_ref):\n       manual_consumed_barriers=True,\n       compute_context=_compute_thread,\n       in_specs=[\n-          plgpu.GPUBlockSpec(  # q\n+          plgpu.BlockSpec(  # q\n               block_shape=(block_q, head_dim),\n               index_map=lambda i: (i, 0),\n               transforms=[tiling, swizzle]),\n-          plgpu.GPUBlockSpec(  # do\n+          plgpu.BlockSpec(  # do\n               block_shape=(block_q, head_dim),\n               index_map=lambda i: (i, 0),\n               transforms=[tiling, swizzle]),\n-          plgpu.GPUBlockSpec(block_shape=(block_q,), index_map=lambda i: (i,)),\n-          plgpu.GPUBlockSpec(block_shape=(block_q,), index_map=lambda i: (i,))\n+          plgpu.BlockSpec(block_shape=(block_q,), index_map=lambda i: (i,)),\n+          plgpu.BlockSpec(block_shape=(block_q,), index_map=lambda i: (i,))\n       ])\n     q_ref = q_ref.at[batch, :, q_head, :]\n     do_ref = do_ref.at[batch, :, q_head, :]\n@@ -589,7 +589,7 @@ def compute_dk(acc_ref):\n           (q_scratch, do_scratch, lse_scratch, delta_scratch),  # type: ignore\n           (plgpu.Barrier(1, num_barriers=compute_wgs),) * 4  # type: ignore\n       ],\n-      compiler_params=plgpu.GPUCompilerParams(approx_math=True),\n+      compiler_params=plgpu.CompilerParams(approx_math=True),\n       grid=(batch_size, num_q_tiles, num_q_heads),\n       grid_names=(\"batch\", \"q_seq\", \"heads\"),\n       num_threads=compute_wgs + 1,\n@@ -610,7 +610,7 @@ def compute_dk(acc_ref):\n         (k_scratch, v_scratch),  # type: ignore\n         (plgpu.Barrier(1, num_barriers=compute_wgs),) * 2  # type: ignore\n   ],\n-    compiler_params=plgpu.GPUCompilerParams(approx_math=True),\n+    compiler_params=plgpu.CompilerParams(approx_math=True),\n     grid=(batch_size, num_kv_tiles, num_q_heads),\n     grid_names=(\"batch\", \"kv_seq\", \"heads\"),\n     num_threads=compute_wgs + 1,\n@@ -746,10 +746,10 @@ def compute_pv(acc_ref):\n         manual_consumed_barriers=True,\n         compute_context=_compute_thread,\n         in_specs=[\n-            plgpu.GPUBlockSpec(  # k\n+            plgpu.BlockSpec(  # k\n                 block_shape=(block_kv, head_dim),\n                 index_map=lambda i: (i, 0)),\n-            plgpu.GPUBlockSpec(  # v\n+            plgpu.BlockSpec(  # v\n                 block_shape=(block_kv, head_dim),\n                 index_map=lambda i: (i, 0)),\n         ],\n@@ -758,7 +758,7 @@ def compute_pv(acc_ref):\n     k_ref = k_ref.at[batch, :, kv_head, :]\n     v_ref = v_ref.at[batch, :, kv_head, :]\n     pipeline(k_ref, v_ref)\n-  mesh = plgpu.GPUMesh(\n+  mesh = plgpu.Mesh(\n       grid=(batch_size, num_q_tiles, num_q_heads),\n       grid_names=(\"batch\", \"q_seq\", \"heads\"),\n       num_threads=3,\n@@ -769,7 +769,7 @@ def run(refs):\n \n     @pl.core_map(\n         mesh,\n-        compiler_params=plgpu.GPUCompilerParams(\n+        compiler_params=plgpu.CompilerParams(\n             approx_math=True, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n         ),\n     )\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 68aeea5a03e8..b10bc0f390b0 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -94,14 +94,14 @@ def skip_if_wg_semantics(self):\n \n   def kernel(self, *args, **kwargs):\n     compiler_params = dataclasses.replace(\n-        kwargs.pop(\"compiler_params\", plgpu.GPUCompilerParams()),\n+        kwargs.pop(\"compiler_params\", plgpu.CompilerParams()),\n         lowering_semantics=self.LOWERING_SEMANTICS,\n     )\n     return plgpu.kernel(*args, compiler_params=compiler_params, **kwargs)\n \n   def pallas_call(self, *args, **kwargs):\n     compiler_params = dataclasses.replace(\n-        kwargs.pop(\"compiler_params\", plgpu.GPUCompilerParams()),\n+        kwargs.pop(\"compiler_params\", plgpu.CompilerParams()),\n         lowering_semantics=self.LOWERING_SEMANTICS,\n     )\n     return pl.pallas_call(*args, compiler_params=compiler_params, **kwargs)\n@@ -153,7 +153,7 @@ def test_unary_op(self, op, approx_math):\n     @functools.partial(\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct([256], dtype),\n-        compiler_params=plgpu.GPUCompilerParams(approx_math=approx_math),\n+        compiler_params=plgpu.CompilerParams(approx_math=approx_math),\n     )\n     def kernel(x_ref, o_ref):\n       o_ref[...] = op(x_ref[...])\n@@ -296,12 +296,13 @@ def kernel(x_ref, o_ref, scratch_ref):\n \n   @parameterized.product(max_concurrent_steps=[1, 2, 3, 4, 16])\n   def test_add_one_grid_pipelined(self, max_concurrent_steps):\n+\n     @functools.partial(\n         self.pallas_call,\n         in_specs=[pl.BlockSpec((128, 16), lambda i, j: (i, j))],\n         out_specs=pl.BlockSpec((128, 16), lambda i, j: (i, j)),\n         out_shape=jax.ShapeDtypeStruct([128 * 2, 64], jnp.float32),\n-        compiler_params=plgpu.GPUCompilerParams(\n+        compiler_params=plgpu.CompilerParams(\n             dimension_semantics=[\"parallel\", \"sequential\"],\n             max_concurrent_steps=max_concurrent_steps,\n         ),\n@@ -314,11 +315,12 @@ def kernel(x_ref, o_ref):\n     np.testing.assert_array_equal(kernel(x), x + 1.0)\n \n   def test_add_one_grid_pipelined_program_id(self):\n+\n     @functools.partial(\n         self.pallas_call,\n         out_specs=pl.BlockSpec((16, 16), lambda i, j: (i, j)),\n         out_shape=jax.ShapeDtypeStruct([16, 64], jnp.int32),\n-        compiler_params=plgpu.GPUCompilerParams(\n+        compiler_params=plgpu.CompilerParams(\n             dimension_semantics=[\"parallel\", \"sequential\"],\n             max_concurrent_steps=2,\n         ),\n@@ -339,7 +341,7 @@ def test_add_one_grid_pipelined_sequential_invariant_output(self):\n         in_specs=[pl.BlockSpec((32, 16), lambda i, j: (i, j))],\n         out_specs=pl.BlockSpec((32, 16), lambda i, j: (i, 0)),\n         out_shape=jax.ShapeDtypeStruct([32 * 2, 64], jnp.float32),\n-        compiler_params=plgpu.GPUCompilerParams(\n+        compiler_params=plgpu.CompilerParams(\n             dimension_semantics=[\"parallel\", \"sequential\"],\n             max_concurrent_steps=2,\n         ),\n@@ -634,7 +636,7 @@ def test_gmem_to_smem_with_multiple_smem_indexers_and_transforms(self):\n         grid=(4, 4),\n         out_shape=jax.ShapeDtypeStruct((256, 128), jnp.int32),\n         in_specs=(\n-            plgpu.GPUBlockSpec(\n+            plgpu.BlockSpec(\n                 block_shape=(128, 128),\n                 index_map=lambda i, j: (i, j),\n                 memory_space=plgpu.SMEM,\n@@ -645,7 +647,7 @@ def test_gmem_to_smem_with_multiple_smem_indexers_and_transforms(self):\n             ),\n         ),\n         out_specs=(\n-            plgpu.GPUBlockSpec(\n+            plgpu.BlockSpec(\n                 block_shape=(64, 32),\n                 index_map=lambda i, j: (i, j),\n                 memory_space=plgpu.SMEM,\n@@ -696,7 +698,7 @@ def kernel(x_ref, o_ref, barrier_ref):\n         plgpu.wait_smem_to_gmem(0)\n \n     in_spec = pl.BlockSpec(memory_space=plgpu.GMEM)\n-    out_spec = plgpu.GPUBlockSpec(\n+    out_spec = plgpu.BlockSpec(\n         transforms=(\n             plgpu.TilingTransform((8, 32)),\n             plgpu.SwizzleTransform(128),\n@@ -727,7 +729,7 @@ def body(tmp_ref):\n       pl.run_scoped(body, plgpu.SMEM((128, 128), jnp.float32, transforms=ts))\n \n     in_spec = pl.BlockSpec(memory_space=plgpu.GMEM)\n-    out_spec = plgpu.GPUBlockSpec(transforms=ts, memory_space=plgpu.SMEM)\n+    out_spec = plgpu.BlockSpec(transforms=ts, memory_space=plgpu.SMEM)\n     f = self.pallas_call(\n         kernel,\n         out_shape=jax.ShapeDtypeStruct([128, 128], jnp.float32),\n@@ -767,7 +769,7 @@ def kernel(x_ref, o_ref, barrier_ref):\n         plgpu.barrier_wait(barrier_ref)\n \n     in_spec = pl.BlockSpec(memory_space=plgpu.GMEM)\n-    out_spec = plgpu.GPUBlockSpec(\n+    out_spec = plgpu.BlockSpec(\n         transforms=(\n             plgpu.TilingTransform((8, 32)),\n             plgpu.TransposeTransform((0, 2, 1, 3, 4)),\n@@ -797,7 +799,7 @@ def test_load_to_strided_layout_with_indexing(self, src_memory_space, layout):\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct([2, 128], jnp.float32),\n         in_specs=[pl.BlockSpec(memory_space=src_memory_space)],\n-        out_specs=plgpu.GPUBlockSpec(memory_space=plgpu.SMEM),\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.SMEM),\n     )\n     def kernel(x_ref, o_ref):\n       for i in range(2):\n@@ -818,7 +820,7 @@ def kernel(x_ref, o_ref, barrier_ref):\n         plgpu.barrier_wait(barrier_ref)\n \n     in_spec = pl.BlockSpec(memory_space=plgpu.GMEM)\n-    out_spec = plgpu.GPUBlockSpec(memory_space=plgpu.SMEM)\n+    out_spec = plgpu.BlockSpec(memory_space=plgpu.SMEM)\n     f = self.pallas_call(\n         kernel,\n         out_shape=jax.ShapeDtypeStruct([2, 64, 2, 128], jnp.float32),\n@@ -920,7 +922,7 @@ def test_print_wgmma_tiled_layout(self):\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct(shape, jnp.float32),\n         in_specs=[\n-            plgpu.GPUBlockSpec(\n+            plgpu.BlockSpec(\n                 transforms=(\n                     plgpu.TilingTransform((8, 32)),\n                     plgpu.SwizzleTransform(128),\n@@ -1013,10 +1015,11 @@ def kernel(x_ref, o_ref):\n     np.testing.assert_array_equal(kernel(x), x)\n \n   def test_load_scalar(self):\n+\n     @functools.partial(\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct((128,), jnp.int32),\n-        in_specs=[plgpu.GPUBlockSpec(memory_space=plgpu.GMEM)],\n+        in_specs=[plgpu.BlockSpec(memory_space=plgpu.GMEM)],\n     )\n     def kernel(x_ref, o_ref):\n       o_ref[...] = jnp.broadcast_to(x_ref[10], (128,))\n@@ -1135,7 +1138,7 @@ def kernel(o_ref):\n   def test_swizzled_blockspec_shapes(self):\n     self.skip_if_wg_semantics()\n \n-    spec = plgpu.GPUBlockSpec(\n+    spec = plgpu.BlockSpec(\n         (128, 64),\n         lambda *i: i,\n         transforms=(\n@@ -1344,7 +1347,7 @@ def test_tile_slicing(self):\n     self.skip_if_wg_semantics()\n \n     shape = (256, 128)\n-    block_spec = plgpu.GPUBlockSpec(\n+    block_spec = plgpu.BlockSpec(\n         transforms=(plgpu.TilingTransform((8, 64)), plgpu.SwizzleTransform(128))\n     )\n     @functools.partial(\n@@ -1375,8 +1378,8 @@ def kernel(a_ref, b_ref):\n     a = np.zeros((64, 64), dtype=jnp.float32)\n     b = self.pallas_call(\n         kernel,\n-        in_specs=[plgpu.GPUBlockSpec(memory_space=plgpu.GMEM)],\n-        out_specs=plgpu.GPUBlockSpec(memory_space=plgpu.GMEM),\n+        in_specs=[plgpu.BlockSpec(memory_space=plgpu.GMEM)],\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.GMEM),\n         input_output_aliases={0: 0},\n         out_shape=a,\n     )(a)\n@@ -1395,7 +1398,7 @@ def rotate(src, dst):\n       dst[lower, left] = src[lower, right]\n \n     x = jnp.arange(128 * 128).astype(jnp.float16).reshape(128, 128)\n-    spec = plgpu.GPUBlockSpec(\n+    spec = plgpu.BlockSpec(\n         transforms=(plgpu.TilingTransform((8, 64)), plgpu.SwizzleTransform(128))\n     )\n     f = self.pallas_call(rotate, out_shape=x, in_specs=[spec], out_specs=spec)\n@@ -1472,7 +1475,7 @@ def kernel(x_ref, o_ref):\n       y = self.pallas_call(\n           kernel,\n           out_shape=jax.ShapeDtypeStruct([256], jnp.float32),\n-          compiler_params=plgpu.GPUCompilerParams(\n+          compiler_params=plgpu.CompilerParams(\n               profile_space=16, profile_dir=tmpdir\n           ),\n       )(x)\n@@ -1836,11 +1839,12 @@ def test_fori_loop_accumulator(self, force_while):\n       transforms = (plgpu.TilingTransform((8, 64)), plgpu.SwizzleTransform(128))\n     else:\n       transforms = ()\n+\n     @functools.partial(\n         self.pallas_call,\n-        in_specs=[plgpu.GPUBlockSpec((64, 64), transforms=transforms)],\n+        in_specs=[plgpu.BlockSpec((64, 64), transforms=transforms)],\n         out_shape=jax.ShapeDtypeStruct((64, 64), jnp.float16),\n-        out_specs=plgpu.GPUBlockSpec((64, 64)),\n+        out_specs=plgpu.BlockSpec((64, 64)),\n     )\n     def kernel(i_ref, o_ref):\n       def scope(acc_ref):\n@@ -1907,7 +1911,7 @@ def _epilogue():\n     )\n \n     if self.LOWERING_SEMANTICS == plgpu.LoweringSemantics.Lane:\n-      lhs_spec = plgpu.GPUBlockSpec(\n+      lhs_spec = plgpu.BlockSpec(\n           lhs_spec.block_shape,\n           lhs_spec.index_map,\n           transforms=(\n@@ -1915,7 +1919,7 @@ def _epilogue():\n               plgpu.SwizzleTransform(128),\n           ),\n       )\n-      rhs_spec = plgpu.GPUBlockSpec(\n+      rhs_spec = plgpu.BlockSpec(\n           rhs_spec.block_shape,\n           rhs_spec.index_map,\n           transforms=(\n@@ -1923,7 +1927,7 @@ def _epilogue():\n               plgpu.SwizzleTransform(128),\n           ),\n       )\n-      out_spec = plgpu.GPUBlockSpec(\n+      out_spec = plgpu.BlockSpec(\n           out_spec.block_shape,\n           out_spec.index_map,\n           transforms=(\n@@ -1939,7 +1943,7 @@ def _epilogue():\n         out_shape=jax.ShapeDtypeStruct((m, n), jnp.float16),\n         scratch_shapes=[plgpu.ACC((tile_m, tile_n), jnp.float32)],\n         grid=(grid_m, grid_n, grid_k),\n-        compiler_params=plgpu.GPUCompilerParams(\n+        compiler_params=plgpu.CompilerParams(\n             dimension_semantics=[\"parallel\", \"parallel\", \"sequential\"],\n             max_concurrent_steps=2,\n             delay_release=1,\n@@ -1980,7 +1984,7 @@ def scope(acc_ref):\n     res = self.pallas_call(\n         kernel,\n         in_specs=[\n-            plgpu.GPUBlockSpec(\n+            plgpu.BlockSpec(\n                 (64, 128),\n                 lambda i, j: (i, j),\n                 transforms=(\n@@ -1988,13 +1992,13 @@ def scope(acc_ref):\n                     plgpu.SwizzleTransform(128),\n                 ),\n             ),\n-            plgpu.GPUBlockSpec(\n+            plgpu.BlockSpec(\n                 b_shape,\n                 lambda *i: i,\n                 transforms=(*rhs_transforms, plgpu.SwizzleTransform(128)),\n             ),\n         ],\n-        out_specs=plgpu.GPUBlockSpec((64, 192), lambda *i: i),\n+        out_specs=plgpu.BlockSpec((64, 192), lambda *i: i),\n         out_shape=jax.ShapeDtypeStruct((64, 192), jnp.float32),\n         grid=(1, 1),\n     )(a, b)\n@@ -2019,8 +2023,8 @@ def scope(acc_ref):\n     res = self.pallas_call(\n         kernel,\n         in_specs=[\n-            plgpu.GPUBlockSpec(transforms=transforms),\n-            plgpu.GPUBlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n         ],\n         out_shape=jax.ShapeDtypeStruct((64, 192), jnp.float32),\n     )(a, b)\n@@ -2044,9 +2048,9 @@ def scope(acc_ref):\n     res = self.pallas_call(\n         kernel,\n         in_specs=[\n-            plgpu.GPUBlockSpec(transforms=transforms),\n-            plgpu.GPUBlockSpec(transforms=transforms),\n-            plgpu.GPUBlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n         ],\n         out_shape=jax.ShapeDtypeStruct((64, 192), jnp.float16),\n     )(a, b, i)\n@@ -2073,8 +2077,8 @@ def scope(acc_ref):\n     res = self.pallas_call(\n         kernel,\n         in_specs=[\n-            plgpu.GPUBlockSpec(transforms=transforms),\n-            plgpu.GPUBlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n         ],\n         out_shape=jax.ShapeDtypeStruct((64, 192), jnp.float32),\n     )(a, b)\n@@ -2104,14 +2108,10 @@ def scope(acc_ref):\n     res = self.pallas_call(\n         kernel,\n         in_specs=[\n-            plgpu.GPUBlockSpec(\n-                (64, 128), lambda *ij: ij, transforms=transforms\n-            ),\n-            plgpu.GPUBlockSpec(\n-                (128, 128), lambda *ij: ij, transforms=transforms\n-            ),\n+            plgpu.BlockSpec((64, 128), lambda *ij: ij, transforms=transforms),\n+            plgpu.BlockSpec((128, 128), lambda *ij: ij, transforms=transforms),\n         ],\n-        out_specs=plgpu.GPUBlockSpec((64, 128), lambda *ij: ij),\n+        out_specs=plgpu.BlockSpec((64, 128), lambda *ij: ij),\n         out_shape=jax.ShapeDtypeStruct((64, 128), jnp.float32),\n         grid=(1, 1),\n     )(a, b)\n@@ -2129,7 +2129,7 @@ def test_load_to_wgmma_row_col_layout_with_indexing(self, src_memory_space, layo\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct([2, m], jnp.float32),\n         in_specs=[pl.BlockSpec(memory_space=src_memory_space)],\n-        out_specs=plgpu.GPUBlockSpec(memory_space=plgpu.SMEM),\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.SMEM),\n     )\n     def kernel(x_ref, o_ref):\n       for i in range(2):\n@@ -2175,14 +2175,14 @@ def compute(acc_ref):\n         out_shape=jax.ShapeDtypeStruct([m, n], jnp.float32),\n         in_specs=(\n             pl.BlockSpec(memory_space=src_memory_space),\n-            plgpu.GPUBlockSpec(\n+            plgpu.BlockSpec(\n                 transforms=(\n                     plgpu.TilingTransform((8, 64)),\n                     plgpu.SwizzleTransform(128),\n                 ),\n             ),\n         ),\n-        out_specs=plgpu.GPUBlockSpec(memory_space=plgpu.SMEM),\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.SMEM),\n     )\n \n     out_ref = (\n@@ -2294,12 +2294,10 @@ def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref,\n     f = self.pallas_call(\n         kernel,\n         in_specs=(\n-            plgpu.GPUBlockSpec(transforms=transforms,\n-                               memory_space=plgpu.SMEM),\n-            plgpu.GPUBlockSpec(transforms=transforms,\n-                               memory_space=plgpu.SMEM),\n+            plgpu.BlockSpec(transforms=transforms, memory_space=plgpu.SMEM),\n+            plgpu.BlockSpec(transforms=transforms, memory_space=plgpu.SMEM),\n         ),\n-        out_specs=plgpu.GPUBlockSpec(memory_space=plgpu.GMEM),\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.GMEM),\n         out_shape=jax.ShapeDtypeStruct(shape, dtype),\n         scratch_shapes=scratch_shapes,\n     )\n@@ -2511,12 +2509,12 @@ def kernel(x_gmem, o_gmem):\n         plgpu.emit_pipeline(\n             kernel_body,\n             in_specs=[\n-                plgpu.GPUBlockSpec(\n+                plgpu.BlockSpec(\n                     (64, 64), lambda i: (0, i), transforms=transforms\n                 )\n             ],\n             out_specs=[\n-                plgpu.GPUBlockSpec(\n+                plgpu.BlockSpec(\n                     (64, 64), lambda i: (0, i), transforms=transforms\n                 )\n             ],\n@@ -2705,10 +2703,10 @@ def kernel_body(_, a_smem, b_smem):\n       plgpu.emit_pipeline(\n           kernel_body,\n           in_specs=[\n-              plgpu.GPUBlockSpec(\n+              plgpu.BlockSpec(\n                   (tile_m, tile_k), lambda k: (pid_m, k), transforms=transforms\n               ),\n-              plgpu.GPUBlockSpec(\n+              plgpu.BlockSpec(\n                   (tile_k, tile_n), lambda k: (k, pid_n), transforms=transforms\n               ),\n           ],\n@@ -2729,7 +2727,7 @@ def kernel_body(_, a_smem, b_smem):\n             pl.BlockSpec(memory_space=plgpu.GMEM),\n             pl.BlockSpec(memory_space=plgpu.GMEM),\n         ],\n-        out_specs=plgpu.GPUBlockSpec(\n+        out_specs=plgpu.BlockSpec(\n             (tile_m, tile_n), lambda m, n: (m, n), transforms=transforms\n         ),\n         out_shape=jax.ShapeDtypeStruct((m, n), jnp.float16),\n@@ -2794,7 +2792,7 @@ def body(*gmem_refs):\n             jax.ShapeDtypeStruct((m, n), jnp.float16),\n             jax.ShapeDtypeStruct((blk_m, blk_n), jnp.float16),\n         ),\n-        compiler_params=plgpu.GPUCompilerParams(approx_math=True),\n+        compiler_params=plgpu.CompilerParams(approx_math=True),\n         grid=(1,),\n         grid_names=(\"_\",),\n         num_threads=3,\n@@ -2839,7 +2837,7 @@ def pipeline(*gmem_refs):\n     kernel = self.kernel(\n         pipeline,\n         out_shape=jax.ShapeDtypeStruct((m, n), jnp.float32),\n-        compiler_params=plgpu.GPUCompilerParams(approx_math=True),\n+        compiler_params=plgpu.CompilerParams(approx_math=True),\n         grid=(1,),\n         grid_names=(\"_\",),\n         num_threads=num_compute_wgs + 1,\n@@ -2858,7 +2856,7 @@ def test_carry_accumulate(self, m=256, n=256, num_compute_wgs=2):\n         scratch_shapes=[\n             plgpu.SMEM((blk_m, blk_n), jnp.float32),\n         ],\n-        compiler_params=plgpu.GPUCompilerParams(approx_math=True),\n+        compiler_params=plgpu.CompilerParams(approx_math=True),\n         grid=(1,),\n         grid_names=(\"_\",),\n         num_threads=num_compute_wgs + 1,\n@@ -3176,11 +3174,12 @@ class CoreMapWGTest(\n class PrettyPrintingTest(PallasTest):\n \n   def test_load(self):\n+\n     @functools.partial(\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct([2, 128], jnp.float32),\n         in_specs=[pl.BlockSpec(memory_space=plgpu.GMEM)],\n-        out_specs=plgpu.GPUBlockSpec(memory_space=plgpu.SMEM),\n+        out_specs=plgpu.BlockSpec(memory_space=plgpu.SMEM),\n     )\n     def kernel(x_ref, o_ref):\n       for i in range(2):\n@@ -3228,8 +3227,8 @@ def test_wgmma(self):\n         self.pallas_call,\n         out_shape=jax.ShapeDtypeStruct((64, 192), jnp.float32),\n         in_specs=[\n-            plgpu.GPUBlockSpec(transforms=transforms),\n-            plgpu.GPUBlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n+            plgpu.BlockSpec(transforms=transforms),\n         ],\n     )\n     def kernel(a_ref, b_ref, o_ref):\n@@ -3348,9 +3347,13 @@ def kernel(l_ref, r_ref, o_ref):\n       def compute(_, l_smem, r_smem, o_smem):\n         o_smem[...] = l_smem[...] + r_smem[...]\n       r = lax.axis_index(\"rows\")\n-      block = plgpu.GPUBlockSpec(\n-          (row_block, col_block), lambda c: (r, c),\n-          transforms=(plgpu.TilingTransform((8, 32)), plgpu.SwizzleTransform(64)),\n+      block = plgpu.BlockSpec(\n+          (row_block, col_block),\n+          lambda c: (r, c),\n+          transforms=(\n+              plgpu.TilingTransform((8, 32)),\n+              plgpu.SwizzleTransform(64),\n+          ),\n       )\n       plgpu.emit_pipeline(\n           compute,\n@@ -3420,9 +3423,19 @@ def do_wgmma(acc_ref):\n       plgpu.emit_pipeline(\n           compute,\n           grid=(l_ref.shape[1] // k_block,),\n-          in_specs=[plgpu.GPUBlockSpec((m_block, k_block), lambda k: (m, k), transforms=lo_transforms),\n-                    plgpu.GPUBlockSpec((k_block, n_block), lambda k: (k, n), transforms=r_transforms)],\n-          out_specs=[plgpu.GPUBlockSpec((m_block, n_block), lambda k: (m, n), transforms=lo_transforms)],\n+          in_specs=[\n+              plgpu.BlockSpec(\n+                  (m_block, k_block), lambda k: (m, k), transforms=lo_transforms\n+              ),\n+              plgpu.BlockSpec(\n+                  (k_block, n_block), lambda k: (k, n), transforms=r_transforms\n+              ),\n+          ],\n+          out_specs=[\n+              plgpu.BlockSpec(\n+                  (m_block, n_block), lambda k: (m, n), transforms=lo_transforms\n+              )\n+          ],\n       )(l_ref, r_ref, o_ref)\n \n     np.testing.assert_allclose(kernel(x, x), x @ x)\ndiff --git a/tests/pallas/ops_test.py b/tests/pallas/ops_test.py\nindex 3baa26e5efd7..bcda0ca9f71e 100644\n--- a/tests/pallas/ops_test.py\n+++ b/tests/pallas/ops_test.py\n@@ -292,7 +292,7 @@ def setUp(self):\n   def pallas_call(cls, *args, **kwargs):\n     if jtu.test_device_matches([\"cuda\"]) and use_mosaic_gpu:\n       assert plgpu_mgpu is not None\n-      compiler_params = plgpu_mgpu.GPUCompilerParams(\n+      compiler_params = plgpu_mgpu.CompilerParams(\n           lowering_semantics=plgpu_mgpu.LoweringSemantics.Warpgroup\n       )\n       kwargs[\"compiler_params\"] = compiler_params\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[mosaic_gpu] Removed `uniform=` from `async_copy`\n\nIt is redundant in the presence of `predicate=`.\n\nPiperOrigin-RevId: 761085680",
        "output": [
            "```diff\nCommit: 3c2cb8026e4ba3df1ad9e943c90da0cf4cfed3f8\nDate: 2025-05-20T15:03:49Z\nURL: https://github.com/jax-ml/jax/commit/3c2cb8026e4ba3df1ad9e943c90da0cf4cfed3f8\nFiles changed: 5\nAdditions: +65, Deletions: -75\ndiff --git a/jax/experimental/mosaic/gpu/dialect_lowering.py b/jax/experimental/mosaic/gpu/dialect_lowering.py\nindex 320ae32607e9..20138bbe6fd4 100644\n--- a/jax/experimental/mosaic/gpu/dialect_lowering.py\n+++ b/jax/experimental/mosaic/gpu/dialect_lowering.py\n@@ -713,7 +713,6 @@ def _mgpu_async_load_op_lowering_rule(\n       gmem_slice=tuple(gmem_slice),\n       barrier=barrier.barrier_ref,\n       arrive=False,\n-      uniform=True,\n       swizzle=swizzle,\n       gmem_transform=transforms,\n       predicate=ctx.single_thread_per_warpgroup_predicate,\n@@ -755,7 +754,6 @@ def _mgpu_async_store_op_lowering_rule(\n       gmem_slice=tuple(gmem_slice),\n       swizzle=swizzle,\n       gmem_transform=transforms,\n-      uniform=True,\n       predicate=ctx.single_thread_per_warpgroup_predicate,\n       arrive=store_op.commit_group,\n   )\ndiff --git a/jax/experimental/mosaic/gpu/examples/flash_attention.py b/jax/experimental/mosaic/gpu/examples/flash_attention.py\nindex 071a4dec81fd..78ef1faddc59 100644\n--- a/jax/experimental/mosaic/gpu/examples/flash_attention.py\n+++ b/jax/experimental/mosaic/gpu/examples/flash_attention.py\n@@ -309,7 +309,7 @@ def start_kv_copy(slot, kv_seq_base, smem, gmem, barrier, transform):\n                 gmem_slice=(kv_head_idx, ds(kv_seq_base, blocks.kv)),\n                 gmem_transform=transform,\n                 barrier=barrier,\n-                uniform=False,\n+                predicate=None,\n                 swizzle=128,\n             )\n         def start_k_copy(slot, kv_seq_base):\n@@ -403,7 +403,7 @@ def kv_copy_init(slot, kv_seq_base):\n               gmem_transform=t,\n               barrier=barriers[slot],\n               arrive=False,\n-              uniform=False,\n+              predicate=None,\n               swizzle=128,\n           )\n \ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul.py b/jax/experimental/mosaic/gpu/examples/matmul.py\nindex a5dd29e0dc4d..5c8363fa8b27 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul.py\n@@ -206,7 +206,7 @@ def fetch(slot, ki):\n       rhs_tma_tile_bytes = int(np.prod(block_tiling.kn) * rhs_elem_bytes)\n       txcount = lhs_tma_tile_bytes + rhs_tma_tile_bytes\n       common_copy_args = dict(\n-          swizzle=swizzle, barrier=barrier, arrive=False, uniform=False,\n+          swizzle=swizzle, barrier=barrier, arrive=False, predicate=None,\n       )\n       with single_thread():\n         barrier.arrive_expect_tx(txcount)\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex 03363c1e365f..f771c8bc1ef1 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -114,7 +114,7 @@ def _tma_body(ki, _):\n             swizzle=swizzle,\n             barrier=full_barrier,\n             arrive=False,\n-            uniform=False,\n+            predicate=None,\n             collective=gpu.Dimension.x,\n             partitioned=0,  # Non-contracting dim is always 0.\n         )\ndiff --git a/jax/experimental/mosaic/gpu/launch_context.py b/jax/experimental/mosaic/gpu/launch_context.py\nindex 175dc8b0ac74..aaae007a67f0 100644\n--- a/jax/experimental/mosaic/gpu/launch_context.py\n+++ b/jax/experimental/mosaic/gpu/launch_context.py\n@@ -321,6 +321,10 @@ def finalize_size(self):\n         init_callback(self._alloc_op.result)\n \n \n+class _DefaultPredicate:\n+  pass\n+\n+\n @dataclasses.dataclass()\n class LaunchContext:\n   module: ir.Module\n@@ -506,12 +510,10 @@ def async_copy(\n       barrier: utils.BarrierRef | None = None,\n       swizzle: int | None = None,\n       arrive: bool | None = None,\n-      uniform: bool = True,\n       collective: Sequence[gpu.Dimension] | gpu.Dimension | None = None,\n       partitioned: int | None = None,\n-      predicate: (\n-          ir.Value | None\n-      ) = None,  # Should select 0 or 1 threads from the WG.\n+      # Should select 0 or 1 threads from the WG.\n+      predicate: ir.Value | None | _DefaultPredicate = _DefaultPredicate(),\n       reduction_op: ReductionOp | None = None,\n   ):\n     \"\"\"Initiates an async copy between GMEM and SMEM.\n@@ -553,8 +555,8 @@ def async_copy(\n           f\"Expected same element type, got {element_type} and\"\n           f\" {dst_ref_ty.element_type}\"\n       )\n-    if predicate is not None and not uniform:\n-      raise ValueError(\"Predicate can only be defined when uniform is True\")\n+    if isinstance(predicate, _DefaultPredicate):\n+      predicate = utils.single_thread_predicate(utils.ThreadSubset.WARPGROUP)\n     if not isinstance(gmem_transform, tuple):\n       gmem_transform = (gmem_transform,)\n \n@@ -756,13 +758,6 @@ def partition_dim(dim: int, idx: ir.Value, num_chunks: int):\n         arith.index_cast(i32, idx) for idx in reversed(dyn_base_indices)\n     ]\n \n-    uniform_ctx = (\n-        functools.partial(\n-            utils.single_thread, scope=utils.ThreadSubset.WARPGROUP)\n-        if uniform and predicate is None\n-        else contextlib.nullcontext\n-    )\n-\n     if max(slice_shape) > 256:\n       raise ValueError(\n           \"Async copies only support copying <=256 elements along each\"\n@@ -792,68 +787,65 @@ def partition_dim(dim: int, idx: ir.Value, num_chunks: int):\n           np.prod(slice_shape) * element_bitwidth * collective_size // 8, i32\n       )\n       barrier_ptr = barrier.get_ptr()\n-      with uniform_ctx():\n-        assert reduction_op is None\n-        if collective_size > 1 and partitioned is not None:\n-          if predicate is None:\n-            predicate = c(1, ir.IntegerType.get_signless(1))\n-          if arrive:\n-            first_block = arith.cmpi(\n-                arith.CmpIPredicate.eq, self.cluster_idx(collective), c(0, index),\n-            )\n-            arrive_predicate = arith.andi(predicate, first_block)\n-            nvvm.mbarrier_arrive_expect_tx_shared(\n-                barrier_ptr, transfer_bytes, predicate=arrive_predicate\n-            )\n-          rank = len(slice_shape)\n-          idx_operands = \",\".join(f\"${i}\" for i in range(4, 4 + rank))\n-          llvm.inline_asm(\n-              ir.Type.parse(\"!llvm.void\"),\n-              [predicate, smem_ptr, tma_desc, barrier_ptr, *rev_dyn_base_indices],\n-              f\"\"\"\n-              {{\n-              .reg .b32 mapped_addr;\n-              @$0 mapa.shared::cluster.u32 mapped_addr, $3, 0;\n-              @$0 cp.async.bulk.tensor.{rank}d.shared::cta.global.tile.mbarrier::complete_tx::bytes.cta_group::2\n-                                   [$1], [$2, {{{idx_operands}}}], [mapped_addr];\n-              }}\n-              \"\"\",\n-              \"b,r,l,r\" + \",r\" * rank,\n-              has_side_effects=True,\n+      assert reduction_op is None\n+      if collective_size > 1 and partitioned is not None:\n+        if predicate is None:\n+          predicate = c(1, ir.IntegerType.get_signless(1))\n+        if arrive:\n+          first_block = arith.cmpi(\n+              arith.CmpIPredicate.eq, self.cluster_idx(collective), c(0, index),\n           )\n-        else:\n-          if arrive:\n-            nvvm.mbarrier_arrive_expect_tx_shared(\n-                barrier_ptr, transfer_bytes, predicate=predicate\n-            )\n-          nvvm.cp_async_bulk_tensor_shared_cluster_global(\n-              smem_ptr, tma_desc, rev_dyn_base_indices, barrier_ptr, [],\n-              multicast_mask=multicast_mask, predicate=predicate\n+          arrive_predicate = arith.andi(predicate, first_block)\n+          nvvm.mbarrier_arrive_expect_tx_shared(\n+              barrier_ptr, transfer_bytes, predicate=arrive_predicate\n           )\n-    else:\n-      assert multicast_mask is None\n-      if reduction_op is not None:\n-        with uniform_ctx():\n-          if predicate is None:\n-            predicate = c(1, ir.IntegerType.get_signless(1))\n-          rank = len(slice_shape)\n-          idx_operands = \",\".join(f\"${i}\" for i in range(3, 3 + rank))\n-          llvm.inline_asm(\n+        rank = len(slice_shape)\n+        idx_operands = \",\".join(f\"${i}\" for i in range(4, 4 + rank))\n+        llvm.inline_asm(\n             ir.Type.parse(\"!llvm.void\"),\n-            [predicate,smem_ptr,tma_desc,*rev_dyn_base_indices],\n-            f\"@$0 cp.reduce.async.bulk.tensor.{rank}d.global.shared::cta.{reduction_op}.tile.bulk_group [$2,{{{idx_operands}}}], [$1];\",\n-            \"b,r,l\" + \",r\" * rank,\n+            [predicate, smem_ptr, tma_desc, barrier_ptr, *rev_dyn_base_indices],\n+            f\"\"\"\n+            {{\n+            .reg .b32 mapped_addr;\n+            @$0 mapa.shared::cluster.u32 mapped_addr, $3, 0;\n+            @$0 cp.async.bulk.tensor.{rank}d.shared::cta.global.tile.mbarrier::complete_tx::bytes.cta_group::2\n+                                  [$1], [$2, {{{idx_operands}}}], [mapped_addr];\n+            }}\n+            \"\"\",\n+            \"b,r,l,r\" + \",r\" * rank,\n             has_side_effects=True,\n-          )\n-          if arrive:\n-            nvvm.cp_async_bulk_commit_group()\n+        )\n       else:\n-        with uniform_ctx():\n-          nvvm.cp_async_bulk_tensor_global_shared_cta(\n-              tma_desc, smem_ptr, rev_dyn_base_indices, predicate=predicate\n+        if arrive:\n+          nvvm.mbarrier_arrive_expect_tx_shared(\n+              barrier_ptr, transfer_bytes, predicate=predicate\n           )\n-          if arrive:\n-            nvvm.cp_async_bulk_commit_group()\n+        nvvm.cp_async_bulk_tensor_shared_cluster_global(\n+            smem_ptr, tma_desc, rev_dyn_base_indices, barrier_ptr, [],\n+            multicast_mask=multicast_mask, predicate=predicate\n+        )\n+    else:\n+      assert multicast_mask is None\n+      if reduction_op is not None:\n+        if predicate is None:\n+          predicate = c(1, ir.IntegerType.get_signless(1))\n+        rank = len(slice_shape)\n+        idx_operands = \",\".join(f\"${i}\" for i in range(3, 3 + rank))\n+        llvm.inline_asm(\n+          ir.Type.parse(\"!llvm.void\"),\n+          [predicate,smem_ptr,tma_desc,*rev_dyn_base_indices],\n+          f\"@$0 cp.reduce.async.bulk.tensor.{rank}d.global.shared::cta.{reduction_op}.tile.bulk_group [$2,{{{idx_operands}}}], [$1];\",\n+          \"b,r,l\" + \",r\" * rank,\n+          has_side_effects=True,\n+        )\n+        if arrive:\n+          nvvm.cp_async_bulk_commit_group()\n+      else:\n+        nvvm.cp_async_bulk_tensor_global_shared_cta(\n+            tma_desc, smem_ptr, rev_dyn_base_indices, predicate=predicate\n+        )\n+        if arrive:\n+          nvvm.cp_async_bulk_commit_group()\n \n   def await_async_copy(\n       self, allow_groups: int, await_read_only: bool = False\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Only run relevant GPU tests in the optional GPU CI\n\nThis aims to prepare those tests to be used as a presubmit.\nWe can later add a more complete nightly configuration for testing.\n\nPiperOrigin-RevId: 761044847",
        "output": [
            "```diff\nCommit: 414db00e807a4faeeba953fd4a61b4f5fe3d864c\nDate: 2025-05-20T12:51:14Z\nURL: https://github.com/jax-ml/jax/commit/414db00e807a4faeeba953fd4a61b4f5fe3d864c\nFiles changed: 1\nAdditions: +22, Deletions: -11\ndiff --git a/.github/workflows/bazel_optional_h100_b200.yml b/.github/workflows/bazel_optional_h100_b200.yml\nindex bde033361609..0c73b238505e 100644\n--- a/.github/workflows/bazel_optional_h100_b200.yml\n+++ b/.github/workflows/bazel_optional_h100_b200.yml\n@@ -49,19 +49,23 @@ jobs:\n             --run_under \"$(pwd)/build/parallel_accelerator_execute.sh\" \\\n             --test_output=errors \\\n             --test_env=JAX_ACCELERATOR_COUNT=1 \\\n-            --test_env=JAX_TESTS_PER_ACCELERATOR=32 \\\n+            --test_env=JAX_TESTS_PER_ACCELERATOR=8 \\\n             --strategy=TestRunner=local \\\n-            --local_test_jobs=32 \\\n-            --test_env=JAX_EXCLUDE_TEST_TARGETS=PmapTest.testSizeOverflow \\\n-            --test_tag_filters=-multiaccelerator \\\n+            --local_test_jobs=8 \\\n+            --test_env=JAX_EXCLUDE_TEST_TARGETS='PmapTest.testSizeOverflow|.*InterpretTest.*' \\\n             --test_env=TF_CPP_MIN_LOG_LEVEL=0 \\\n             --test_env=JAX_SKIP_SLOW_TESTS=true \\\n             --action_env=JAX_ENABLE_X64=\"1\" \\\n             --action_env=NCCL_DEBUG=WARN \\\n+            --flaky_test_attempts=1 \\\n+            --test_timeout=420 \\\n             --color=yes \\\n-            //tests:gpu_tests //tests:backend_independent_tests \\\n-            //tests/pallas:gpu_tests //tests/pallas:backend_independent_tests \\\n-            //tests/mosaic:gpu_tests //tests/mosaic:backend_independent_tests\n+            //tests:cudnn_fusion_test_gpu \\\n+            //tests:scaled_matmul_stablehlo_test_gpu \\\n+            //tests:fused_attention_stablehlo_test_gpu \\\n+            //tests:nn_test_gpu \\\n+            //tests/pallas:gpu_tests \\\n+            //tests/mosaic:gpu_tests\n   run_multiaccelerator_tests:\n     if: ${{ github.event.repository.fork == false && (github.event_name == 'schedule' || github.event_name == 'workflow_dispatch' || contains(github.event.pull_request.labels.*.name, 'CI Optional GPU Presubmit')) }}\n     runs-on: linux-x86-a3-8g-h100-8gpu\n@@ -86,13 +90,20 @@ jobs:\n             --test_output=errors \\\n             --strategy=TestRunner=local \\\n             --local_test_jobs=8 \\\n-            --test_env=JAX_EXCLUDE_TEST_TARGETS=PmapTest.testSizeOverflow \\\n+            --test_env=JAX_EXCLUDE_TEST_TARGETS='PmapTest.testSizeOverflow|.*InterpretTest.*' \\\n             --test_tag_filters=multiaccelerator \\\n             --test_env=TF_CPP_MIN_LOG_LEVEL=0 \\\n             --test_env=JAX_SKIP_SLOW_TESTS=true \\\n             --action_env=JAX_ENABLE_X64=\"1\" \\\n             --action_env=NCCL_DEBUG=WARN \\\n+            --flaky_test_attempts=1 \\\n             --color=yes \\\n-            //tests:gpu_tests //tests:backend_independent_tests \\\n-            //tests/pallas:gpu_tests //tests/pallas:backend_independent_tests \\\n-            //tests/mosaic:gpu_tests //tests/mosaic:backend_independent_tests\n\\ No newline at end of file\n+            //tests/mosaic:gpu_tests \\\n+            //tests/pallas:gpu_tests \\\n+            //tests:array_interoperability_test_gpu \\\n+            //tests:cudnn_fusion_test_gpu \\\n+            //tests:fused_attention_stablehlo_test_gpu\n+            //tests:fused_attention_stablehlo_test_gpu \\\n+            //tests:gpu_tests \\\n+            //tests:python_callback_test_gpu \\\n+            //tests:ragged_collective_test_gpu\n\\ No newline at end of file\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add Python 3.13 support to traceback code under PLATFORM_GOOGLE.\n\nThis build options uses CPython internals, which changed under Python 3.13.\n\nPiperOrigin-RevId: 761042927",
        "output": [
            "```diff\nCommit: b8383df255fb9b056146911807a9a786cc4110e6\nDate: 2025-05-20T12:42:46Z\nURL: https://github.com/jax-ml/jax/commit/b8383df255fb9b056146911807a9a786cc4110e6\nFiles changed: 1\nAdditions: +16, Deletions: -5\ndiff --git a/jaxlib/traceback.cc b/jaxlib/traceback.cc\nindex 3eba5288335c..48edc584c94f 100644\n--- a/jaxlib/traceback.cc\n+++ b/jaxlib/traceback.cc\n@@ -68,11 +68,12 @@ Traceback::Traceback() {\n #else  // PY_VERSION_HEX < 0x030b0000\n \n #ifdef PLATFORM_GOOGLE\n-  // This code is equivalent to the version using public APIs, but it saves us\n-  // an allocation of one object per stack frame. However, this is definitely\n-  // violating the API contract of CPython, so we only use this where we can be\n-  // confident we know exactly which CPython we are using (internal to Google).\n-  // Feel free to turn this on if you like, but it might break at any time!\n+// This code is equivalent to the version using public APIs, but it saves us\n+// an allocation of one object per stack frame. However, this is definitely\n+// violating the API contract of CPython, so we only use this where we can be\n+// confident we know exactly which CPython we are using (internal to Google).\n+// Feel free to turn this on if you like, but it might break at any time!\n+#if PY_VERSION_HEX < 0x030d0000\n   for (_PyInterpreterFrame* f = thread_state->cframe->current_frame;\n        f != nullptr; f = f->previous) {\n     if (_PyFrame_IsIncomplete(f)) continue;\n@@ -80,6 +81,16 @@ Traceback::Traceback() {\n     frames_.emplace_back(f->f_code,\n                          _PyInterpreterFrame_LASTI(f) * sizeof(_Py_CODEUNIT));\n   }\n+#else   // PY_VERSION_HEX < 0x030d0000\n+  for (_PyInterpreterFrame* f = thread_state->current_frame; f != nullptr;\n+       f = f->previous) {\n+    if (_PyFrame_IsIncomplete(f)) continue;\n+    Py_INCREF(f->f_executable);\n+    frames_.emplace_back(reinterpret_cast<PyCodeObject*>(f->f_executable),\n+                         _PyInterpreterFrame_LASTI(f) * sizeof(_Py_CODEUNIT));\n+  }\n+#endif  // PY_VERSION_HEX < 0x030d0000\n+\n #else   // PLATFORM_GOOGLE\n   PyFrameObject* next;\n   for (PyFrameObject* py_frame = PyThreadState_GetFrame(thread_state);\n\n```"
        ],
        "from_id": [
            "hawkinsp",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Implement reshapes from and to refs with empty shapes\n\nPiperOrigin-RevId: 761038562",
        "output": [
            "```diff\nCommit: e77df81f14f44bfe5ed1bf47b989673a13f3ceee\nDate: 2025-05-20T12:24:57Z\nURL: https://github.com/jax-ml/jax/commit/e77df81f14f44bfe5ed1bf47b989673a13f3ceee\nFiles changed: 2\nAdditions: +35, Deletions: -3\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex bf0b06ccb9c9..9eedc3402579 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -511,12 +511,42 @@ def memref_reshape(ref: ir.Value, shape: tuple[int, ...]) -> ir.Value:\n         f\" allowed) {shape}\"\n     )\n \n-  return _reshape(ref, list(ref_ty.shape), list(shape))\n+  src_shape = list(ref_ty.shape)\n+  dst_shape = list(shape)\n+  if src_shape == dst_shape:\n+    return ref\n+  if not src_shape:\n+    _, offset = ref_ty.get_strides_and_offset()\n+    identity = ir.AffineMapAttr.get(ir.AffineMap.get_identity(0))\n+    if ref_ty.layout == identity:\n+      new_layout = ir.AffineMapAttr.get(ir.AffineMap.get_identity(len(dst_shape)))\n+    else:\n+      new_layout = ir.StridedLayoutAttr.get(offset, [1] * len(dst_shape))\n+    result_ty = ir.MemRefType.get(dst_shape, ref_ty.element_type, new_layout, ref_ty.memory_space)\n+    return memref.expand_shape(result_ty, ref, [], [], dst_shape)\n+  if not dst_shape:\n+    _, offset = ref_ty.get_strides_and_offset()\n+    identity = ir.AffineMapAttr.get(ir.AffineMap.get_identity(ref_ty.rank))\n+    contig_strided_1d = ir.Attribute.parse(\"strided<[1]>\")\n+    if ref_ty.layout == identity or ref_ty.layout == contig_strided_1d:\n+      new_layout = ir.AffineMapAttr.get(ir.AffineMap.get_identity(0))\n+    else:\n+      new_layout = ir.StridedLayoutAttr.get(offset, [])\n+    result_ty = ir.MemRefType.get((), ref_ty.element_type, new_layout, ref_ty.memory_space)\n+    return memref.collapse_shape(result_ty, ref, [])\n+  return _reshape(ref, src_shape, dst_shape)\n \n \n def memref_fold(ref: ir.Value, dim, fold_rank) -> ir.Value:\n   ref_ty = ir.MemRefType(ref.type)\n   new_shape = list(ref_ty.shape)\n+  if dim < 0:\n+    raise ValueError(f\"Dimension {dim} is negative\")\n+  if dim + fold_rank > len(new_shape):\n+    raise ValueError(\n+        f\"Folding {fold_rank} dimensions starting from {dim} is out of bounds\"\n+        f\" for shape {new_shape}\"\n+    )\n   new_shape[dim : dim + fold_rank] = [np.prod(new_shape[dim : dim + fold_rank])]\n   identity = ir.AffineMapAttr.get(ir.AffineMap.get_identity(ref_ty.rank))\n   contig_strided_1d = ir.Attribute.parse(\"strided<[1]>\")\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 39bd8aa77331..80e67b20e1ef 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -389,17 +389,19 @@ def kernel(ctx, inp, out, _):\n       (\"add_1s\", (5, 1, 2), (1, 1, 5, 1, 1, 2, 1, 1)),\n       (\"fold\", (1, 5, 2, 1,), (1, 10, 1)),\n       (\"un\", (1, 10, 1), (1, 5, 2, 1,)),\n+      (\"to_scalar\", (1, 1, 1), ()),\n+      (\"from_scalar\", (), (1, 1, 1)),\n   )\n   def test_reshape(self, inp_shape, out_shape):\n     def kernel(ctx, inp, out, _):\n       copy(memref_reshape(inp, out_shape), out)\n \n-    x = np.arange(math.prod(inp_shape), dtype=jnp.float32).reshape(*inp_shape)\n+    x = np.arange(math.prod(inp_shape), dtype=jnp.float32).reshape(inp_shape)\n     out_ty = jax.ShapeDtypeStruct(out_shape, jnp.float32)\n     y = mgpu.as_gpu_kernel(\n         kernel, (1, 1, 1), (128, 1, 1), x, out_ty, ()\n     )(x)\n-    np.testing.assert_array_equal(y, x.reshape(*out_shape))\n+    np.testing.assert_array_equal(y, x.reshape(out_shape))\n \n   @parameterized.named_parameters([\n       (\"packed\", (4, 4, 4), (16, 4, 1), 1, 2, False),\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update scipy.signal.welch tests to be compatible with upstream dev version.",
        "output": [
            "```diff\nCommit: e9cdbaca8dec2ed39bbe592372f203fce428027a\nDate: 2025-05-20T10:39:03Z\nURL: https://github.com/jax-ml/jax/commit/e9cdbaca8dec2ed39bbe592372f203fce428027a\nFiles changed: 2\nAdditions: +3, Deletions: -4\ndiff --git a/jax/_src/third_party/scipy/signal_helper.py b/jax/_src/third_party/scipy/signal_helper.py\nindex 4a021675804d..ad7bdfbef62a 100644\n--- a/jax/_src/third_party/scipy/signal_helper.py\n+++ b/jax/_src/third_party/scipy/signal_helper.py\n@@ -57,7 +57,7 @@ def _triage_segments(window: ArrayLike | str | tuple[Any, ...], nperseg: int | N\n       win = get_window(window, nperseg_int)\n     win = jnp.array(win, dtype=dtype)\n   else:\n-    win = jnp.asarray(window)\n+    win = jnp.asarray(window, dtype=dtype)\n     nperseg_int = win.size if nperseg is None else int(nperseg)\n     if win.ndim != 1:\n       raise ValueError('window must be 1-D')\ndiff --git a/tests/scipy_signal_test.py b/tests/scipy_signal_test.py\nindex 7ff3c87435c7..b1c5d9c98fed 100644\n--- a/tests/scipy_signal_test.py\n+++ b/tests/scipy_signal_test.py\n@@ -357,12 +357,11 @@ def testWelchWithDefaultStepArgsAgainstNumpy(\n     if use_nperseg:\n       kwargs['nperseg'] = nperseg\n     if use_window:\n-      kwargs['window'] = jnp.array(osp_signal.get_window('hann', nperseg),\n-                                   dtype=dtypes.to_complex_dtype(dtype))\n+      kwargs['window'] = jnp.array(osp_signal.get_window('hann', nperseg))\n     if use_noverlap:\n       kwargs['noverlap'] = noverlap\n \n-    @jtu.ignore_warning(message=\"nperseg = 256 is greater than\")\n+    @jtu.ignore_warning(message=\"nperseg\")\n     def osp_fun(x):\n       freqs, Pxx = osp_signal.welch(x, **kwargs)\n       return freqs.astype(_real_dtype(dtype)), Pxx.astype(_real_dtype(dtype))\n\n```"
        ],
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Added `plgpu.nd_loop`\n\nThis is a generalization of `lax.fori_loop` which partitions the flat iteration\nspace across the given axes, and is useful for writing persistent kernels.\n\nPiperOrigin-RevId: 761007326",
        "output": [
            "```diff\nCommit: e5e9be55950d1358a93cd3eed10a91e6f5ae0168\nDate: 2025-05-20T10:36:06Z\nURL: https://github.com/jax-ml/jax/commit/e5e9be55950d1358a93cd3eed10a91e6f5ae0168\nFiles changed: 5\nAdditions: +123, Deletions: -0\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 4018bff873bd..61b5a99dfe31 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -877,6 +877,7 @@ pytype_strict_library(\n     deps = [\n         \":mosaic_gpu\",\n         \"//jax/_src/pallas/mosaic_gpu:core\",\n+        \"//jax/_src/pallas/mosaic_gpu:helpers\",\n         \"//jax/_src/pallas/mosaic_gpu:pallas_call_registration\",  # build_cleaner: keep\n         \"//jax/_src/pallas/mosaic_gpu:pipeline\",\n         \"//jax/_src/pallas/mosaic_gpu:primitives\",\ndiff --git a/jax/_src/pallas/mosaic_gpu/BUILD b/jax/_src/pallas/mosaic_gpu/BUILD\nindex 2652be7a7c9a..74b44fb8f991 100644\n--- a/jax/_src/pallas/mosaic_gpu/BUILD\n+++ b/jax/_src/pallas/mosaic_gpu/BUILD\n@@ -123,3 +123,9 @@ pytype_strict_library(\n         \"//jax/_src/pallas\",\n     ],\n )\n+\n+pytype_strict_library(\n+    name = \"helpers\",\n+    srcs = [\"helpers.py\"],\n+    deps = [\"//jax\"],\n+)\ndiff --git a/jax/_src/pallas/mosaic_gpu/helpers.py b/jax/_src/pallas/mosaic_gpu/helpers.py\nnew file mode 100644\nindex 000000000000..54c4910059d5\n--- /dev/null\n+++ b/jax/_src/pallas/mosaic_gpu/helpers.py\n@@ -0,0 +1,86 @@\n+# Copyright 2025 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"Helpers for Pallas Mosaic GPU kernels.\"\"\"\n+\n+from collections.abc import Callable, Hashable, Sequence\n+import math\n+from typing import TypeVar\n+\n+import jax\n+from jax import lax\n+\n+_T = TypeVar(\"_T\")\n+\n+\n+def nd_loop(\n+    grid: Sequence[int],\n+    body: Callable[[Sequence[jax.Array], _T], _T],\n+    init_val: _T,\n+    *,\n+    collective_axes: Sequence[Hashable] | Hashable,\n+) -> _T:\n+  \"\"\"A loop over a multi-dimensional grid partitioned along the given axes.\n+\n+  For example, if ``collective_axes`` is ``\"x\"`` with :func:`lax.axis_size`\n+  equal to 4 and the grid is (2, 3), the implementation would produce the\n+  following iteration order\n+\n+      loop step    index    axis index\n+\n+          0        (0, 0)       0\n+          1        (0, 1)       1\n+          2        (0, 2)       2\n+          3        (1, 0)       3\n+          4        (1, 1)       0\n+          5        (1, 2)       1\n+\n+  which comes from partitioning the flat iteration space into chunks in an\n+  interleaved fashion wrt the ``\"x\"`` axis index.\n+\n+  Note that in the example the total number of loop steps is not divisible\n+  by the axis size of ``\"x\"``, and thus for some ``\"x\"`` axis indices the\n+  loop will do one iteration less.\n+\n+      axis index       indices\n+\n+          0         (0, 0), (1, 1)\n+          1         (0, 1), (1, 2)\n+          2         (0, 2)\n+          3         (1, 0)\n+\n+  See also:\n+    - :func:`jax.lax.fori_loop`: A single-dimensional indexed loop.\n+  \"\"\"\n+  axis_index = lax.axis_index(collective_axes)\n+  axis_size = lax.axis_size(collective_axes)\n+  grid_size = math.prod(grid)\n+\n+  def wrapper(step, carry):\n+    step = step * axis_size + axis_index\n+    # The loop below is conceptually ``jnp.unravel_index``, but it uses\n+    # ``lax`` APIs instead of ``jax.numpy`` to minimize the number of\n+    # primitives used.\n+    index = []\n+    for grid_dim in reversed(grid):\n+      grid_dim = lax.convert_element_type(grid_dim, step.dtype)\n+      index.append(lax.rem(step, grid_dim))\n+      step = lax.div(step, grid_dim)\n+    index.reverse()\n+    return body(tuple(index), carry)\n+\n+  upper = lax.div(grid_size, axis_size) + lax.convert_element_type(\n+      axis_index < grid_size % axis_size, axis_index.dtype\n+  )\n+  return lax.fori_loop(0, upper, wrapper, init_val)\ndiff --git a/jax/experimental/pallas/mosaic_gpu.py b/jax/experimental/pallas/mosaic_gpu.py\nindex 7b300b8cfbfa..a7d8c3e34223 100644\n--- a/jax/experimental/pallas/mosaic_gpu.py\n+++ b/jax/experimental/pallas/mosaic_gpu.py\n@@ -38,6 +38,7 @@\n from jax._src.pallas.mosaic_gpu.core import WarpMesh as WarpMesh\n from jax._src.pallas.mosaic_gpu.core import WGMMAAccumulatorRef as ACC  # noqa: F401\n from jax._src.pallas.mosaic_gpu.core import WGMMAAccumulatorRef as WGMMAAccumulatorRef\n+from jax._src.pallas.mosaic_gpu.helpers import nd_loop as nd_loop\n from jax._src.pallas.mosaic_gpu.pipeline import emit_pipeline as emit_pipeline\n from jax._src.pallas.mosaic_gpu.pipeline import emit_pipeline_warp_specialized as emit_pipeline_warp_specialized\n from jax._src.pallas.mosaic_gpu.primitives import barrier_arrive as barrier_arrive\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex ba7f2d74bbb1..68aeea5a03e8 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1762,6 +1762,35 @@ def kernel(x_ref, o_ref128, aliased_ref):\n     with self.assertRaisesRegex(ValueError, \"can't be assigned to\"):\n       kernel(jnp.arange(128).astype(jnp.float32))\n \n+  @parameterized.parameters(1, 2, 3)\n+  def test_nd_loop(self, sm_steps):\n+    @functools.partial(\n+        self.kernel,\n+        out_shape=jax.ShapeDtypeStruct((sm_steps, 132, 128), jnp.int32),\n+        grid=(132,),\n+        grid_names=(\"sm\",),\n+    )\n+    def kernel(o_ref):\n+      def body(idx, _):\n+        assert len(idx) == 3\n+        # We need to use `mode=\"clip\"`, because the indices are not static.\n+        flat_idx = jnp.ravel_multi_index(idx, (sm_steps, 4, 33), mode=\"clip\")\n+        sm_step = lax.div(\n+            flat_idx, lax.convert_element_type(lax.axis_size(\"sm\"), jnp.int32)\n+        )\n+        o_ref[sm_step, lax.axis_index(\"sm\")] = lax.broadcast(\n+            flat_idx, o_ref.shape[-1:]\n+        )\n+\n+      plgpu.nd_loop((sm_steps, 4, 33), body, None, collective_axes=\"sm\")\n+\n+    result = kernel()\n+    for sm_step in range(sm_steps):\n+      np.testing.assert_array_equal(\n+          result[sm_step],\n+          jnp.tile((132 * sm_step + jnp.arange(132))[:, None], 128),\n+      )\n+\n \n class PallasCallWGTest(\n     PallasCallTest, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] Pulled `runtime_assert_enabled` from `pltpu` to `pl`\n\nPiperOrigin-RevId: 760983479",
        "output": [
            "```diff\nCommit: 7025c2310b576030875327275c761cfb64c3720e\nDate: 2025-05-20T09:10:13Z\nURL: https://github.com/jax-ml/jax/commit/7025c2310b576030875327275c761cfb64c3720e\nFiles changed: 7\nAdditions: +93, Deletions: -38\ndiff --git a/jax/_src/pallas/core.py b/jax/_src/pallas/core.py\nindex 709bb4640241..fe755d61a310 100644\n--- a/jax/_src/pallas/core.py\n+++ b/jax/_src/pallas/core.py\n@@ -44,6 +44,23 @@\n from jax._src.state.types import TransformedRef\n import jax.numpy as jnp\n \n+# TODO(slebedev): Rename to --jax_pallas_debug_assertions.\n+_ENABLE_RUNTIME_ASSERT = config.bool_state(\n+    \"jax_pallas_enable_runtime_assert\",\n+    default=False,\n+    help=(\n+        \"If set, enables runtime assertions in the kernel via checkify.check.\"\n+        \" Otherwise, runtime asserts will be ignored unless functionalized\"\n+        \" using checkify.checkify.\"\n+    ),\n+)\n+\n+\n+def runtime_assert_enabled() -> bool:\n+  \"\"\"Returns whether runtime asserts are enabled.\"\"\"\n+  return _ENABLE_RUNTIME_ASSERT.value\n+\n+\n class DynamicGridDim:\n   def __repr__(self):\n     return \"DynamicGridDim\"\ndiff --git a/jax/_src/pallas/mosaic/core.py b/jax/_src/pallas/mosaic/core.py\nindex c04fc6f155b9..49ff632f5c14 100644\n--- a/jax/_src/pallas/mosaic/core.py\n+++ b/jax/_src/pallas/mosaic/core.py\n@@ -23,7 +23,6 @@\n from typing import Any, ClassVar, Literal\n \n import jax\n-from jax._src import config\n from jax._src import core as jax_core\n from jax._src import util\n from jax._src.pallas import core as pallas_core\n@@ -48,16 +47,6 @@\n _out_shape_to_aval_mapping = pallas_core._out_shape_to_aval_mapping\n split_list = util.split_list\n \n-_ENABLE_RUNTIME_ASSERT = config.bool_state(\n-    \"jax_pallas_enable_runtime_assert\",\n-    default=False,\n-    help=(\n-        \"If set, enables runtime assertions in the kernel via checkify.check.\"\n-        \" Otherwise, runtime asserts will be ignored unless functionalized\"\n-        \" using checkify.checkify.\"\n-    ),\n-)\n-\n \n class KernelType(enum.Enum):\n   TC = 0\n@@ -221,11 +210,6 @@ def create_tensorcore_mesh(\n   )\n \n \n-def runtime_assert_enabled() -> bool:\n-  \"\"\"Returns whether runtime asserts are enabled.\"\"\"\n-  return _ENABLE_RUNTIME_ASSERT.value\n-\n-\n def _tensorcore_mesh_discharge_rule(\n     in_avals,\n     out_avals,\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex a67a71dd40f7..a9fbf8dcd982 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -235,7 +235,7 @@ def _memory_space_to_mosaic_attribute(memory_space: MemorySpace | None\n   tpu_memory_space = _memory_space_to_tpu_memory_space(memory_space)\n   return ir.Attribute.parse(f\"#tpu.memory_space<{tpu_memory_space}>\")\n \n-def _dtype_to_ir_type(dtype: jnp.dtype,\n+def _dtype_to_ir_type(dtype: jax.typing.DTypeLike,\n                       is_kernel_boundary: bool = False) -> ir.Type:\n   if jnp.issubdtype(dtype, pallas_core.semaphore_dtype):\n     if jnp.issubdtype(dtype, tpu_core.dma_semaphore):\n@@ -246,11 +246,11 @@ def _dtype_to_ir_type(dtype: jnp.dtype,\n       return ir.Type.parse(\"!tpu.semaphore\")\n     else:\n       raise NotImplementedError\n-  if is_kernel_boundary and jnp.issubdtype(dtype, jnp.dtype('bool')):\n+  if is_kernel_boundary and jnp.issubdtype(dtype, jnp.bool):\n     dtype = BOOL_MEMREF_TYPE\n   # TODO(justinfu): Remove after mosaic supports unsigned types.\n   # This conversion makes mosaic interpret all unsigned types as signed types.\n-  type =  mlir.dtype_to_ir_type(dtype)\n+  type =  mlir.dtype_to_ir_type(jnp.dtype(dtype))\n   if isinstance(type, ir.IntegerType):\n     return ir.IntegerType.get_signless(type.width)\n   else:\n@@ -3766,14 +3766,15 @@ def _join_key_lowering_rule(ctx: LoweringRuleContext, *scalars, impl):\n @register_lowering_rule(checkify.check_p)\n def _checkify_lowering_rule(\n     ctx: LoweringRuleContext, *err_args, err_tree, debug):\n-  if not tpu_core.runtime_assert_enabled():\n+  if not pallas_core.runtime_assert_enabled():\n     if debug:\n       return []\n     else:\n-      raise LoweringException(\"Non-debug check must be functionalized. \"\n-                              \"Enable runtime asserts with \"\n-                              \"--jax_pallas_enable_runtime_assert \"\n-                              \"or functionalize with checkify.check.\")\n+      raise LoweringException(\n+          \"Non-debug check must be functionalized. Enable runtime asserts via\"\n+          \" ``pl.enable_runtime_assert`` or --jax_pallas_enable_runtime_assert\"\n+          \" or, alternatively, functionalize with ``checkify.check``.\"\n+      )\n \n   if cf is None:\n     # TODO(slebedev): Remove once the minimal jaxlib version is 0.6.1.\n@@ -3782,20 +3783,16 @@ def _checkify_lowering_rule(\n     )\n \n   error = jax.tree.unflatten(err_tree, err_args)\n-  assert len(error._pred) == 1\n-  assert len(error._metadata) == 1\n-  assert len(error._payload) == 1\n-  pred = list(error._pred.items())[0][1]\n-  metadata = list(error._metadata.items())[0]\n-  payload = list(error._payload.items())[0][1]\n-  exception_tree = metadata[1]\n+  [pred] = error._pred.values()\n+  [exception_tree] = error._metadata.values()\n+  [payload] = error._payload.values()\n   exception = jax.tree.unflatten(exception_tree, payload)\n   assert isinstance(exception, checkify.FailedCheckError)\n+  assert isinstance(exception, checkify.FailedCheckError)\n \n-  # check_p has an inverted predicate compared to assert,\n-  # so we need to compute not(pred) here.\n-  out_scalar_type = _dtype_to_ir_type(jnp.dtype('bool'))\n-  minus_one = ir_constant(-1, out_scalar_type)\n+  # check_p has an inverted predicate compared to assert, so we need to compute\n+  # ``not pred`` here.\n+  minus_one = ir_constant(-1, _dtype_to_ir_type(jnp.bool))\n   not_pred = arith.xori(pred, minus_one)\n   cf.assert_(not_pred, exception.fmt_string)\n   return []\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex bd304e8b6745..751a2bae2ed0 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -29,6 +29,7 @@\n import jax\n from jax import api_util\n from jax import lax\n+from jax._src import checkify\n from jax._src import core as jax_core\n from jax._src import lib as jaxlib\n from jax._src import linear_util as lu\n@@ -41,6 +42,7 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import arith as arith_dialect\n+from jax._src.lib.mlir.dialects import cf as cf_dialect\n from jax._src.lib.mlir.dialects import gpu as gpu_dialect\n from jax._src.lib.mlir.dialects import llvm as llvm_dialect\n from jax._src.lib.mlir.dialects import math as math_dialect\n@@ -3051,3 +3053,38 @@ def _semaphore_wait_lowering_rule(ctx: LoweringRuleContext, *args, args_tree):\n       scf_dialect.yield_(after_block.arguments)\n   mgpu_utils.warpgroup_barrier()\n   return ()\n+\n+\n+@register_lowering_rule(checkify.check_p, mgpu.LoweringSemantics.Lane)\n+def _checkify_lowering_rule(\n+    ctx: LoweringRuleContext, *err_args, err_tree, debug\n+):\n+  if not pallas_core.runtime_assert_enabled():\n+    if debug:\n+      return []\n+    else:\n+      raise LoweringError(\n+          \"Non-debug check must be functionalized. Enable runtime asserts via\"\n+          \" ``pl.enable_runtime_assert`` or --jax_pallas_enable_runtime_assert\"\n+          \" or, alternatively, functionalize with ``checkify.check``.\"\n+      )\n+\n+  if cf_dialect is None:\n+    # TODO(slebedev): Remove once the minimal jaxlib version is 0.6.1.\n+    raise ValueError(\n+        \"cf dialect is not available. Make sure you have jaxlib 0.6.1 or later.\"\n+    )\n+\n+  error = jax.tree.unflatten(err_tree, err_args)\n+  [pred] = error._pred.values()\n+  [exception_tree] = error._metadata.values()\n+  [payload] = error._payload.values()\n+  exception = jax.tree.unflatten(exception_tree, payload)\n+  assert isinstance(exception, checkify.FailedCheckError)\n+\n+  # check_p has an inverted predicate compared to assert, so we need to compute\n+  # ``not pred`` here.\n+  minus_one = _ir_constant(-1, mgpu_utils.dtype_to_ir_type(jnp.bool))\n+  not_pred = arith_dialect.xori(pred.registers.item(), minus_one)\n+  cf_dialect.assert_(not_pred, exception.fmt_string)\n+  return []\ndiff --git a/jax/experimental/pallas/__init__.py b/jax/experimental/pallas/__init__.py\nindex 1e631ad407fd..406d6e965322 100644\n--- a/jax/experimental/pallas/__init__.py\n+++ b/jax/experimental/pallas/__init__.py\n@@ -18,6 +18,7 @@\n https://docs.jax.dev/en/latest/pallas.html.\n \"\"\"\n \n+from jax._src.pallas.core import _ENABLE_RUNTIME_ASSERT as enable_runtime_assert  # noqa: F401\n from jax._src.pallas.core import BlockDim as BlockDim\n from jax._src.pallas.core import Blocked as Blocked\n from jax._src.pallas.core import BlockSpec as BlockSpec\n@@ -32,6 +33,7 @@\n from jax._src.pallas.core import MemoryRef as MemoryRef\n from jax._src.pallas.core import MemorySpace as MemorySpace\n from jax._src.pallas.core import no_block_spec as no_block_spec\n+from jax._src.pallas.core import runtime_assert_enabled as runtime_assert_enabled\n from jax._src.pallas.core import semaphore as semaphore\n from jax._src.pallas.core import Squeezed as Squeezed\n from jax._src.pallas.core import squeezed as squeezed\ndiff --git a/jax/experimental/pallas/tpu.py b/jax/experimental/pallas/tpu.py\nindex 5ed6968c673e..c8e2ba131a9b 100644\n--- a/jax/experimental/pallas/tpu.py\n+++ b/jax/experimental/pallas/tpu.py\n@@ -25,8 +25,6 @@\n from jax._src.pallas.mosaic.core import SemaphoreType as SemaphoreType\n from jax._src.pallas.mosaic.core import TPUMemorySpace as TPUMemorySpace\n from jax._src.pallas.mosaic.core import TPUCompilerParams as TPUCompilerParams\n-from jax._src.pallas.mosaic.core import runtime_assert_enabled as runtime_assert_enabled\n-from jax._src.pallas.mosaic.core import _ENABLE_RUNTIME_ASSERT as enable_runtime_assert  # noqa: F401\n from jax._src.pallas.mosaic.helpers import sync_copy as sync_copy\n from jax._src.pallas.mosaic.helpers import core_barrier as core_barrier\n from jax._src.pallas.mosaic.helpers import run_on_first_core as run_on_first_core\n@@ -53,6 +51,8 @@\n # Those primitives got moved to Pallas core. Keeping the updated imports\n # here for backward compatibility.\n from jax._src.pallas.core import semaphore as semaphore\n+from jax._src.pallas.core import runtime_assert_enabled as runtime_assert_enabled\n+from jax._src.pallas.core import _ENABLE_RUNTIME_ASSERT as enable_runtime_assert  # noqa: F401\n from jax._src.pallas.primitives import DeviceIdType as DeviceIdType\n from jax._src.pallas.primitives import semaphore_read as semaphore_read\n from jax._src.pallas.primitives import semaphore_signal as semaphore_signal\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex d71593dc9078..ba7f2d74bbb1 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -28,14 +28,15 @@\n import jax\n from jax import export\n from jax import lax\n+from jax._src import checkify\n from jax._src import test_util as jtu\n from jax._src.pallas import core as pallas_core\n from jax._src.pallas import pallas_call\n+from jax._src.pallas import primitives as pallas_primitives\n from jax._src.pallas.mosaic_gpu import core as gpu_core\n from jax._src.pallas.mosaic_gpu import lowering as mgpu_lowering\n from jax._src.pallas.mosaic_gpu import pipeline as mgpu_pipeline\n from jax._src.pallas.mosaic_gpu import primitives as mgpu_primitives\n-from jax._src.pallas import primitives as pallas_primitives\n from jax._src.state import types as state_types\n from jax.experimental import pallas as pl\n import jax.experimental.mosaic.gpu as mgpu\n@@ -995,6 +996,22 @@ def kernel(x_ref, o_ref):\n \n     self.assertIn(\"x: [1, 0, 43, 23]: 6871\\n\", output())\n \n+  def test_check(self):\n+    self.skip_if_wg_semantics()\n+\n+    self.enter_context(pallas_core._ENABLE_RUNTIME_ASSERT(True))\n+\n+    @functools.partial(\n+        self.pallas_call,\n+        out_shape=jax.ShapeDtypeStruct([256], jnp.int32),\n+    )\n+    def kernel(x_ref, o_ref):\n+      checkify.check(_sum_same_dtype(x_ref[...]) > 0, \"x.sum() is negative\")\n+      o_ref[...] = x_ref[...]\n+\n+    x = jnp.arange(256, dtype=jnp.int32)\n+    np.testing.assert_array_equal(kernel(x), x)\n+\n   def test_load_scalar(self):\n     @functools.partial(\n         self.pallas_call,\n@@ -1776,6 +1793,7 @@ def test_missing_primitive_lowerings_are_tracked(self):\n         pallas_primitives.semaphore_signal_p,\n         pallas_primitives.semaphore_wait_p,\n         pallas_primitives.semaphore_read_p,\n+        checkify.check_p,\n     }\n \n     self.assertSetEqual(actual_missing_primitives, expected_missing_primitives)\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Use native bf16 ops for tanh, exp and log on TPUv6+.\n\nReplace `needs_cast` condition during canonicalization with `need_elementwise_canonicalization`.\n\nPiperOrigin-RevId: 760934277",
        "output": [
            "```diff\nCommit: fc786d7422812d48637d03a47baf2b5b3bf15738\nDate: 2025-05-20T06:28:03Z\nURL: https://github.com/jax-ml/jax/commit/fc786d7422812d48637d03a47baf2b5b3bf15738\nFiles changed: 1\nAdditions: +13, Deletions: -18\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\nindex 645e6d615722..368bfc596732 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n@@ -359,12 +359,7 @@ LogicalResult canonicalize_elementwise(const CanonicalizeContext &ctx,\n       auto element_type = ty.getElementType();\n       // There's an annoying hodgepodge of elementwise ops that need to be\n       // rewritten to f32 on later hardware.\n-      // TODO(mvoz): Look into (1) what it would take to support these ops\n-      // natively on later hardware, and (2) how to better organize this list.\n-      bool needs_cast = ctx.hardware_generation <= 5 || isa<math::PowFOp>(op) ||\n-                        isa<math::TanhOp>(op) || isa<math::ExpOp>(op) ||\n-                        isa<math::LogOp>(op);\n-      if (needs_cast && element_type.isBF16()) {\n+      if (element_type.isBF16()) {\n         if (ctx.compatibility_mode) {\n           auto target_f32 =\n               builder.create<arith::ExtFOp>(op.getLoc(), target_f32_ty, operand)\n@@ -918,21 +913,22 @@ const llvm::StringMap<canonicalize_rule_type> &rules() {\n   return *rules;\n }\n \n-const llvm::StringMap<int> &bf16_upcast_min_supported_versions() {\n+const llvm::StringMap<int> &bf16_ops_min_supported_versions() {\n   constexpr int kAlwaysUpcast = std::numeric_limits<int>::max();\n   static const auto m = new llvm::StringMap<int>{\n       {arith::DivFOp::getOperationName(), 4},\n       {arith::SelectOp::getOperationName(), 5},\n       {arith::CmpFOp::getOperationName(), 5},\n-      {arith::MulFOp::getOperationName(), kAlwaysUpcast},\n-      {arith::AddFOp::getOperationName(), kAlwaysUpcast},\n-      {arith::SubFOp::getOperationName(), kAlwaysUpcast},\n-      {arith::MaximumFOp::getOperationName(), kAlwaysUpcast},\n-      {arith::MinimumFOp::getOperationName(), kAlwaysUpcast},\n+      {arith::MulFOp::getOperationName(), 6},\n+      {arith::AddFOp::getOperationName(), 6},\n+      {arith::SubFOp::getOperationName(), 6},\n+      {arith::MaximumFOp::getOperationName(), 6},\n+      {arith::MinimumFOp::getOperationName(), 6},\n       {math::PowFOp::getOperationName(), kAlwaysUpcast},\n-      {math::TanhOp::getOperationName(), kAlwaysUpcast},\n-      {math::ExpOp::getOperationName(), kAlwaysUpcast},\n-      {math::LogOp::getOperationName(), kAlwaysUpcast},\n+      {math::TanhOp::getOperationName(), 6},\n+      {math::ExpOp::getOperationName(), 6},\n+      {math::Exp2Op::getOperationName(), 6},\n+      {math::LogOp::getOperationName(), 6},\n   };\n   return *m;\n }\n@@ -941,9 +937,8 @@ bool need_elementwise_canonicalization(const CanonicalizeContext &ctx,\n                                        Operation &op) {\n   // Only rewrite when the hardware generation is below the minimum supported\n   // version.\n-  auto it =\n-      bf16_upcast_min_supported_versions().find(op.getName().getStringRef());\n-  if (it == bf16_upcast_min_supported_versions().end() ||\n+  auto it = bf16_ops_min_supported_versions().find(op.getName().getStringRef());\n+  if (it == bf16_ops_min_supported_versions().end() ||\n       ctx.hardware_generation >= it->second) {\n     return false;\n   }\n\n```"
        ],
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Add support for currently unsupported reshapes for 32-bit datatypes with native tiling and adds tests for those cases. The cases supported are (k % 128 == 0 in the below):\n- (q, m, n, k) -> (q, m, n * k)\n- (p, q, m, n, k) -> (p, q * m * n * k)\n- (q, m, n, k) -> (q, m, 1, n * k) (in 2 steps, first to n*k then add unit dim)\n- (q, m, n, k) -> (q * m, n * k)\n- (q * m, n, k) -> (q, m, n * k)\n- (q * m, n * k) -> (q, m, n, k)\n- (q, m, n * k) -> (q * m, n, k)\n\nPiperOrigin-RevId: 760904758",
        "output": [
            "```diff\nCommit: 30339b08e1f88d7e9808589b67fdd40550324c77\nDate: 2025-05-20T04:38:26Z\nURL: https://github.com/jax-ml/jax/commit/30339b08e1f88d7e9808589b67fdd40550324c77\nFiles changed: 5\nAdditions: +433, Deletions: -44\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex ba1dfc95c66c..5ddff9d9ee53 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -4301,6 +4301,43 @@ LogicalResult vector_multi_reduction_rule(RewriteContext &ctx, Operation &op,\n   return success();\n }\n \n+// Copy one sublane from a vreg to another vreg.\n+//\n+// Arguments:\n+//  src_vreg: The source vreg to copy a sublane from.\n+//  src_sl_idx: The sublane index in src_vreg to copy from.\n+//  dst_vreg: The base vreg to copy the sublane into. May be null.\n+//  dst_sl_idx: The sublane index in the result.\n+//\n+// Returns:\n+//  A new dst_vreg with the copied sublane.\n+Value copyOneSublane(OpBuilder &builder, Value src_vreg, int src_sl_idx,\n+                     Value dst_vreg, int dst_sl_idx,\n+                     const std::array<int64_t, 2> target_shape) {\n+  src_vreg = builder.create<tpu::RotateOp>(\n+      src_vreg.getLoc(), src_vreg,\n+      /*amount=*/(dst_sl_idx - src_sl_idx + target_shape[0]) % target_shape[0],\n+      /*dimension=*/0, /*stride=*/nullptr, /*stride_dimension=*/nullptr);\n+  if (dst_vreg) {\n+    auto boundIdxConst =\n+        std::bind(IdxConst, std::placeholders::_1, builder, src_vreg.getLoc());\n+    const int bitwidth =\n+        cast<VectorType>(src_vreg.getType()).getElementTypeBitWidth();\n+    CHECK_EQ(bitwidth,\n+             cast<VectorType>(dst_vreg.getType()).getElementTypeBitWidth());\n+    const VectorType vmask_ty =\n+        getNativeVregOrVmaskType(builder.getI1Type(), bitwidth, target_shape);\n+    auto sublanes_mask = builder.create<tpu::CreateMaskOp>(\n+        src_vreg.getLoc(), vmask_ty,\n+        ValueRange{boundIdxConst(dst_sl_idx), boundIdxConst(0)},\n+        ValueRange{boundIdxConst(dst_sl_idx + 1),\n+                   boundIdxConst(target_shape[1])});\n+    src_vreg = builder.create<arith::SelectOp>(src_vreg.getLoc(), sublanes_mask,\n+                                               src_vreg, dst_vreg);\n+  }\n+  return src_vreg;\n+}\n+\n LogicalResult vector_shape_cast_rule(RewriteContext &ctx, Operation &op,\n                                      const ArrayRef<Layout> layouts_in,\n                                      const ArrayRef<Layout> layouts_out) {\n@@ -4397,6 +4434,132 @@ LogicalResult vector_shape_cast_rule(RewriteContext &ctx, Operation &op,\n       dst_vregs_local.Reshape(\n           layout_out.tileArrayImplicitShape(dst_shape, ctx.target_shape));\n       return dst_vregs_local;\n+    } else if (\n+        // Lower shape_casts for 32-bit types where the minor dimension both\n+        // before and after the shape cast is a multiple of 128. We allow\n+        // folding or unfolding multiple number of minor dimensions and folding\n+        // or unfolding some number of leading dimensions. For example (given\n+        // k % 128 == 0 in the following):\n+        // (q, m, n, k) -> (q, m, n * k)\n+        // (p, q, m, n, k) -> (p, q * m * n * k)\n+        // (q, m, n, k) -> (q, m, 1, n * k) (in 2 steps, first to fold n, k then\n+        //    to add the unit dimension)\n+        // (q, m, n, k) -> (q * m, n * k)\n+        // (q * m, n, k) -> (q, m, n * k)\n+        // (q * m, n * k) -> (q, m, n, k)\n+        // (q, m, n * k) -> (q * m, n, k)\n+        dst_shape.size() > 1 && src_shape.size() > 1 &&\n+        (mlir::tpu::canFoldMinorDimsToSize(src_shape, dst_shape.back()) ||\n+         mlir::tpu::canFoldMinorDimsToSize(dst_shape, src_shape.back())) &&\n+        dst_shape.back() % ctx.target_shape[1] == 0 &&\n+        src_shape.back() % ctx.target_shape[1] == 0 &&\n+        layout_in.offsets() == LayoutOffsets{0, 0} &&\n+        layout_in.hasNativeTiling(ctx.target_shape) &&\n+        layout_in.bitwidth() == 32 &&\n+        layout_in.implicit_dim() == VectorLayout::ImplicitDim::kNone &&\n+        layout_out == layout_in) {\n+      auto target_sublanes = ctx.target_shape[0];\n+      auto target_lanes = ctx.target_shape[1];\n+      xla::Array<Value> dst_vregs(\n+          layout_out.tileArrayShape(false, false, dst_shape, ctx.target_shape));\n+\n+      auto to_linear_index = [&](absl::Span<const int64_t> indices,\n+                                 absl::Span<const int64_t> bounds) {\n+        CHECK_EQ(indices.size(), bounds.size());\n+        int linear_index = 0;\n+        int multiplier = 1;\n+        for (int i = indices.size() - 1; i >= 0; --i) {\n+          linear_index += multiplier * indices[i];\n+          multiplier *= bounds[i];\n+        }\n+        return linear_index;\n+      };\n+      auto from_linear_index = [&](int linear_index,\n+                                   absl::Span<const int64_t> bounds) {\n+        SmallVector<int64_t> indices(bounds.size(), 0);\n+        int64_t divisor = std::accumulate(bounds.begin(), bounds.end(), 1,\n+                                          std::multiplies<int64_t>());\n+        CHECK_GT(divisor, 0);\n+        int64_t remainder = linear_index % divisor;\n+        for (int i = 0; i < bounds.size(); ++i) {\n+          int64_t radix = bounds[i];\n+          CHECK_GT(radix, 0);\n+          divisor /= radix;\n+          CHECK_GT(divisor, 0);\n+          indices[i] = remainder / divisor;\n+          remainder = remainder % divisor;\n+        }\n+        return indices;\n+      };\n+      // Gather sublanes from src_vregs via rotating and selecting each relevant\n+      // sublane from the source, into the destination vreg.\n+      // Args:\n+      // * src_sublane_indices: the mixed-radix indices of the sublanes to\n+      // gather in the order they should be gathered.\n+      // * src_vregs: the vregs to gather from.\n+      // Returns:\n+      // * a vreg with the gathered sublanes.\n+      auto gather_sublanes = [target_sublanes](\n+                                 RewriteContext &ctx, Operation &op,\n+                                 SmallVector<SmallVector<int64_t>>\n+                                     src_sublane_indices,\n+                                 const xla::Array<Value> &src_vregs) {\n+        ImplicitLocOpBuilder builder(op.getLoc(), &op);\n+        Value dst_vreg = getZerosVector(\n+            builder, cast<VectorType>(src_vregs.begin()->getType()));\n+        for (int sublane_number = 0;\n+             sublane_number < src_sublane_indices.size(); ++sublane_number) {\n+          SmallVector<int64_t> src_vreg_index =\n+              src_sublane_indices[sublane_number];\n+          src_vreg_index[src_vreg_index.size() - 2] /= target_sublanes;\n+          Value src_vreg = src_vregs(src_vreg_index);\n+          int sublane_within_src_vreg =\n+              src_sublane_indices[sublane_number]\n+                                 [src_sublane_indices[sublane_number].size() -\n+                                  2] %\n+              target_sublanes;\n+          dst_vreg = copyOneSublane(builder, src_vreg, sublane_within_src_vreg,\n+                                    dst_vreg, sublane_number, ctx.target_shape);\n+        }\n+        return dst_vreg;\n+      };\n+      SmallVector<int64_t> dst_shape_in_sublanes(dst_shape);\n+      dst_shape_in_sublanes[dst_shape.size() - 1] =\n+          dst_shape[dst_shape.size() - 1] / target_lanes;\n+      SmallVector<int64_t> src_shape_in_sublanes(src_shape);\n+      src_shape_in_sublanes[src_shape.size() - 1] =\n+          src_shape[src_shape.size() - 1] / target_lanes;\n+      // The algorithm operates on 1 destination vreg at a time:\n+      // 1. For each destination vreg, compute the linear index of each sublane\n+      // within it\n+      // 2. Map the destination sublane linear index to a source sublane linear\n+      // index\n+      // 3. convert that to a mixed-radix index into the source shape\n+      // 4. Gather from those source sublane indices.\n+      SmallVector<int64_t> indices;\n+      dst_vregs.Each([&](absl::Span<const int64_t> dst_vreg_indices,\n+                         Value *dst_vreg) {\n+        indices.assign(dst_vreg_indices.begin(), dst_vreg_indices.end());\n+        indices[indices.size() - 2] *= target_sublanes;\n+        int sublane_offset = to_linear_index(indices, dst_shape_in_sublanes);\n+\n+        // Only move non-padding sublanes to the destination vreg.\n+        int num_non_padding_sublanes = std::min(\n+            dst_shape_in_sublanes[dst_shape_in_sublanes.size() - 2] -\n+                dst_vreg_indices[dst_vreg_indices.size() - 2] * target_sublanes,\n+            target_sublanes);\n+        CHECK_EQ(dst_shape.back() % target_lanes, 0);\n+        int stride_in_sublanes = dst_shape.back() / target_lanes;\n+        SmallVector<SmallVector<int64_t>> gathered_sublanes(\n+            num_non_padding_sublanes);\n+        for (int i = 0; i < gathered_sublanes.size(); ++i) {\n+          gathered_sublanes[i] =\n+              from_linear_index(sublane_offset, src_shape_in_sublanes);\n+          sublane_offset += stride_in_sublanes;\n+        }\n+        *dst_vreg = gather_sublanes(ctx, op, gathered_sublanes, src_vregs);\n+      });\n+      return dst_vregs;\n     } else {\n       return shape_cast_op.emitOpError(\n                  \"Not implemented: Unsupported vector.shape_cast: \")\n@@ -5262,45 +5425,6 @@ xla::Array<Value> retileToReducedSublanes(\n   return dst_vreg_array;\n }\n \n-\n-// Copy one sublane from a vreg to another vreg.\n-//\n-// Arguments:\n-//  src_vreg: The source vreg to copy a sublane from.\n-//  src_sl_idx: The sublane index in src_vreg to copy from.\n-//  dst_vreg: The base vreg to copy the sublane into. May be null.\n-//  dst_sl_idx: The sublane index in the result.\n-//\n-// Returns:\n-//  A new dst_vreg with the copied sublane.\n-Value copy_one_sublane(OpBuilder &builder, Value src_vreg, int src_sl_idx,\n-                       Value dst_vreg, int dst_sl_idx,\n-                       const std::array<int64_t, 2> target_shape) {\n-  src_vreg = builder.create<tpu::RotateOp>(\n-      src_vreg.getLoc(), src_vreg,\n-      /*amount=*/(dst_sl_idx - src_sl_idx + target_shape[0]) % target_shape[0],\n-      /*dimension=*/0, /*stride=*/nullptr, /*stride_dimension=*/nullptr);\n-  if (dst_vreg) {\n-    auto boundIdxConst =\n-        std::bind(IdxConst, std::placeholders::_1, builder, src_vreg.getLoc());\n-    const int bitwidth =\n-        cast<VectorType>(src_vreg.getType()).getElementTypeBitWidth();\n-    CHECK_EQ(bitwidth,\n-             cast<VectorType>(dst_vreg.getType()).getElementTypeBitWidth());\n-    const VectorType vmask_ty =\n-        getNativeVregOrVmaskType(builder.getI1Type(), bitwidth, target_shape);\n-    auto sublanes_mask = builder.create<tpu::CreateMaskOp>(\n-        src_vreg.getLoc(), vmask_ty,\n-        ValueRange{boundIdxConst(dst_sl_idx), boundIdxConst(0)},\n-        ValueRange{boundIdxConst(dst_sl_idx + 1),\n-                   boundIdxConst(target_shape[1])});\n-    src_vreg = builder.create<arith::SelectOp>(src_vreg.getLoc(), sublanes_mask,\n-                                               src_vreg, dst_vreg);\n-  }\n-  return src_vreg;\n-}\n-\n-\n void rotateVregs(OpBuilder &builder, xla::Array<Value> &vregs,\n                  const int64_t amount, const int dimension) {\n   if (amount != 0) {\n@@ -6714,9 +6838,9 @@ FailureOr<std::pair<VectorLayout, xla::Array<Value>>> changeImplicitDim(\n         for (int tile_idx = 0; tile_idx < tiles_per_vreg; ++tile_idx) {\n           int tile_off = tile_idx * sublanes_per_tile;\n           *tile =\n-              copy_one_sublane(builder, vregs(src_idx),\n-                               tile_off + src.offsets()[0].value_or(dst_sl_idx),\n-                               *tile, tile_off + dst_sl_idx, target_shape);\n+              copyOneSublane(builder, vregs(src_idx),\n+                             tile_off + src.offsets()[0].value_or(dst_sl_idx),\n+                             *tile, tile_off + dst_sl_idx, target_shape);\n         }\n       }\n     });\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex f42cfb139a37..9c4a7b4c397d 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -60,7 +60,6 @@ using ImplicitDim = VectorLayout::ImplicitDim;\n \n static constexpr int kLayoutLog = 10;\n \n-\n bool is_fully_replicated(const Layout &layout) {\n   static LayoutOffsets replicated_offsets = {std::nullopt, std::nullopt};\n   return layout.has_value() && layout->offsets() == replicated_offsets;\n@@ -1520,7 +1519,30 @@ class VectorLayoutInferer {\n                              native_tiling, ImplicitDim::kNone));\n       return success();\n     }\n-    op.emitOpError(\"unsupported shape cast\");\n+\n+    // Shape cast (..., m, n, k * target_shape_[1]) -> (..., m, n * k *\n+    // target_shape_[1]) for 32-bit types. We allow multiple major or minor\n+    // dimensions to be folded or unfolded.\n+    if (kNativeBitwidth == bitwidth && res_shape.size() >= 2 &&\n+        src_shape.size() >= 2 && src_shape.back() % native_tiling[1] == 0 &&\n+        res_shape.back() % native_tiling[1] == 0 &&\n+        (mlir::tpu::canFoldMinorDimsToSize(src_shape, res_shape.back()) ||\n+         mlir::tpu::canFoldMinorDimsToSize(res_shape, src_shape.back()))) {\n+      // TODO(jsreeram): Add support for picking space-efficient tilings for\n+      // small 2nd minor dim shapes.\n+      // Example 1: (4, 2, 1024) -> (4, 2048) If we infer src and tgt layout to\n+      // be (1, 128), it is no-op because essentially we just shufflle the VREGs\n+      // in VREG array.\n+      // Example 2: (4, 256) -> (1, 1024) is actually sublane\n+      // shuffle inside each vreg from [0, 1, 2, 3, 4,..7] to [0, 4, 1, 5, ...]\n+      setLayout(op,\n+                VectorLayout(layout.bitwidth(), {0, 0}, native_tiling,\n+                             ImplicitDim::kNone),\n+                VectorLayout(layout.bitwidth(), {0, 0}, native_tiling,\n+                             ImplicitDim::kNone));\n+      return success();\n+    }\n+    op.emitOpError(\"infer-vector-layout: unsupported shape cast\");\n     return failure();\n   }\n \ndiff --git a/jaxlib/mosaic/dialect/tpu/util.cc b/jaxlib/mosaic/dialect/tpu/util.cc\nindex b562f81ad534..02598bd16f9a 100644\n--- a/jaxlib/mosaic/dialect/tpu/util.cc\n+++ b/jaxlib/mosaic/dialect/tpu/util.cc\n@@ -301,4 +301,16 @@ std::optional<int64_t> getIntConst(Value v) {\n   return std::nullopt;\n }\n \n+bool canFoldMinorDimsToSize(ArrayRef<int64_t> shape, int64_t target_size) {\n+  CHECK_GE(shape.size(), 2);\n+  int64_t product = shape.back();\n+  for (int i = shape.size() - 2; i >= 1; --i) {\n+    product *= shape[i];\n+    if (product >= target_size) {\n+      break;\n+    }\n+  }\n+  return product == target_size;\n+}\n+\n }  // namespace mlir::tpu\ndiff --git a/jaxlib/mosaic/dialect/tpu/util.h b/jaxlib/mosaic/dialect/tpu/util.h\nindex ac83d95b715e..2a7325ee7b24 100644\n--- a/jaxlib/mosaic/dialect/tpu/util.h\n+++ b/jaxlib/mosaic/dialect/tpu/util.h\n@@ -284,6 +284,12 @@ inline arith::ConstantOp I32Const(int32_t value, ArrayRef<int64_t> shape,\n }\n \n std::optional<int64_t> getIntConst(Value v);\n+\n+// Returns true if the product of up to `shape.size() - 1` minor-most dimensions\n+// in `shape` equals `target_size`. The major-most dimension is not considered.\n+// Precondition: `shape` has at least 2 dimensions.\n+bool canFoldMinorDimsToSize(ArrayRef<int64_t> shape, int64_t target_size);\n+\n }  // namespace mlir::tpu\n \n #endif  // THIRD_PARTY_PY_JAX_JAXLIB_MOSAIC_DIALECT_TPU_UTIL_H_\ndiff --git a/tests/pallas/tpu_pallas_test.py b/tests/pallas/tpu_pallas_test.py\nindex 83f21bca7fc1..aac249251e2b 100644\n--- a/tests/pallas/tpu_pallas_test.py\n+++ b/tests/pallas/tpu_pallas_test.py\n@@ -3022,6 +3022,231 @@ def kernel(x_ref, out_ref):\n         out, np.zeros((8, 8, 2, 128), dtype=jnp.float32)\n     )\n \n+  # (q, m, n) -> (q, m * n) where n % 128 == 0\n+  @parameterized.parameters(\n+      (32, 16, 512, jnp.float32),\n+      (24, 1, 512, jnp.uint32),\n+      (3, 3, 256, jnp.uint32),\n+      (9, 15, 256, jnp.float32),\n+      (3, 2, 256, jnp.float32),\n+  )\n+  def test_reshape_two_minor_dims_to_R2(self, q, m, n, dtype):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(\n+          x_ref.shape[0], x_ref.shape[1] * x_ref.shape[2]\n+      )\n+\n+    x = np.arange(q * m * n, dtype=dtype).reshape(q, m, n)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m * n), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m * n]))\n+\n+  # (q, m, n, k) -> (q, m, n * k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (3, 8, 17, 512, jnp.float32),\n+      (1, 8, 9, 256, jnp.float32),\n+      (1, 8, 3, 256, jnp.uint32),\n+      (10, 1, 4, 256, jnp.uint32),\n+      (1, 2, 2, 256, jnp.float32),\n+  )\n+  def test_reshape_two_minor_dims_to_R3(self, q, m, n, k, dtype):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(\n+          x_ref.shape[0], x_ref.shape[1], x_ref.shape[2] * x_ref.shape[3]\n+      )\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q, m, n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m, n * k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m, n * k]))\n+\n+  # (p, q, m, n, k) -> (p, q * m * n * k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (5, 3, 8, 17, 512, jnp.float32),\n+      (6, 1, 8, 9, 256, jnp.float32),\n+      (16, 1, 8, 3, 256, jnp.uint32),\n+      (3, 2, 1, 4, 256, jnp.uint32),\n+      (1, 7, 2, 2, 256, jnp.float32),\n+  )\n+  def test_reshape_four_minor_dims_to_R2(self, p, q, m, n, k, dtype):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(\n+          x_ref.shape[0],\n+          x_ref.shape[1] * x_ref.shape[2] * x_ref.shape[3] * x_ref.shape[4],\n+      )\n+\n+    x = np.arange(p * q * m * n * k, dtype=dtype).reshape(p, q, m, n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((p, q * m * n * k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([p, q * m * n * k]))\n+\n+  # (q, m, n, k) -> (q, m, 1, n * k) where k % 128 == 0\n+  def test_reshape_two_minor_dims_preserve_rank(self):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = (\n+          x_ref[...]\n+          .reshape(\n+              x_ref.shape[0], x_ref.shape[1], x_ref.shape[2] * x_ref.shape[3]\n+          )\n+          .reshape(\n+              x_ref.shape[0], 1, x_ref.shape[1], x_ref.shape[2] * x_ref.shape[3]\n+          )\n+      )\n+\n+    q, m, n, k = 10, 1, 4, 256\n+    x = np.arange(q * m * n * k, dtype=jnp.float32).reshape(q, m, n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m, 1, n * k), jnp.float32),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m, 1, n * k]))\n+\n+  # (q, m, n, k) -> (q * m, n * k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (3, 8, 17, 512, jnp.float32),\n+      (1, 8, 9, 256, jnp.float32),\n+      (1, 8, 3, 256, jnp.uint32),\n+      (10, 1, 4, 256, jnp.uint32),\n+      (1, 2, 2, 256, jnp.float32),\n+  )\n+  def test_reshape_fold_two_leading_dims_and_two_minor_dims_R4_to_R2(\n+      self, q, m, n, k, dtype\n+  ):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(\n+          x_ref.shape[0] * x_ref.shape[1], x_ref.shape[2] * x_ref.shape[3]\n+      )\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q, m, n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q * m, n * k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q * m, n * k]))\n+\n+  # (q * m, n, k) -> (q, m, n * k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (2, 2, 17, 512, jnp.float32),\n+      (3, 2, 3, 256, jnp.float32),\n+      (1, 5, 4, 384, jnp.uint32),\n+  )\n+  def test_reshape_unfold_leading_dim_and_fold_two_minor_dims_R3_to_R3(\n+      self, q, m, n, k, dtype\n+  ):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(\n+          q,\n+          m,\n+          x_ref.shape[1] * x_ref.shape[2],\n+      )\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q * m, n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m, n * k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m, n * k]))\n+\n+  # (q * m, n * k) -> (q, m, n, k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (2, 2, 17, 512, jnp.float32),\n+      (3, 2, 3, 256, jnp.float32),\n+      (1, 5, 4, 384, jnp.uint32),\n+  )\n+  def test_reshape_unfold_leading_and_minor_dims_R2_to_R4(\n+      self, q, m, n, k, dtype\n+  ):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(q, m, n, k)\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q * m, n * k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m, n, k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m, n, k]))\n+\n+  # (q, m, n * k) -> (q * m, n, k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (2, 2, 17, 512, jnp.float32),\n+      (3, 2, 8, 256, jnp.float32),\n+      (1, 5, 4, 384, jnp.uint32),\n+  )\n+  def test_reshape_fold_leading_dims_and_unfold_minor_dim(\n+      self, q, m, n, k, dtype\n+  ):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(q * m, n, k)\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q, m, n * k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q * m, n, k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q * m, n, k]))\n+\n+  # (q, m, n, k) -> (q, m * n, k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (2, 2, 17, 512, jnp.float32),\n+      (3, 2, 8, 256, jnp.float32),\n+      (1, 5, 4, 384, jnp.uint32),\n+  )\n+  def test_reshape_fold_middle_dims(self, q, m, n, k, dtype):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(q, m * n, k)\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q, m, n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m * n, k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m * n, k]))\n+\n+  # (q, m * n, k) -> (q, m, n, k) where k % 128 == 0\n+  @parameterized.parameters(\n+      (2, 2, 17, 512, jnp.float32),\n+      (3, 2, 8, 256, jnp.float32),\n+      (1, 5, 4, 384, jnp.uint32),\n+  )\n+  def test_reshape_unfold_middle_dims(self, q, m, n, k, dtype):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 23):\n+      self.skipTest('Needs a newer libTPU')\n+\n+    def kernel(x_ref, y_ref):\n+      y_ref[...] = x_ref[...].reshape(q, m, n, k)\n+\n+    x = np.arange(q * m * n * k, dtype=dtype).reshape(q, m * n, k)\n+    out = self.pallas_call(\n+        kernel,\n+        out_shape=jax.ShapeDtypeStruct((q, m, n, k), dtype),\n+    )(x)\n+    np.testing.assert_array_equal(out, x.reshape([q, m, n, k]))\n+\n \n class MiscellaneousInterpretTest(MiscellaneousTest):\n   INTERPRET: bool = True\n\n```"
        ],
        "from_id": [
            "jaswanth-",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Propagate use_shardy_partitioner to XlaCallModule op.\n\nPiperOrigin-RevId: 760876735",
        "output": [
            "```diff\nCommit: a17810d1afd77add4fa7bc827d897346ca67a240\nDate: 2025-05-20T02:53:00Z\nURL: https://github.com/jax-ml/jax/commit/a17810d1afd77add4fa7bc827d897346ca67a240\nFiles changed: 2\nAdditions: +7, Deletions: -0\ndiff --git a/jax/experimental/jax2tf/README.md b/jax/experimental/jax2tf/README.md\nindex cb1c97bc7b7c..06cc5c86a109 100644\n--- a/jax/experimental/jax2tf/README.md\n+++ b/jax/experimental/jax2tf/README.md\n@@ -1007,6 +1007,8 @@ We list here a history of the serialization version numbers:\n     available in JAX since October 20th, 2023 (JAX 0.4.20),\n     and the default since February 1st, 2024 (JAX 0.4.24).\n     This is the only supported version as of 27th of March, 2024.\n+  * Version 10 propagate the `jax.config.use_shardy_partitioner` value to\n+    XlaCallModule.\n \n ## Known issues\n \ndiff --git a/jax/experimental/jax2tf/jax2tf.py b/jax/experimental/jax2tf/jax2tf.py\nindex 536bf1f201f0..3c34a26af982 100644\n--- a/jax/experimental/jax2tf/jax2tf.py\n+++ b/jax/experimental/jax2tf/jax2tf.py\n@@ -944,6 +944,11 @@ def _convert_value(val, aval):\n       if DisabledSafetyCheck.platform() in exported.disabled_safety_checks:\n         call_module_attrs[\"platforms\"] = ()  # No platform checking\n \n+  if version >= 10:\n+    call_module_attrs[\"use_shardy_partitioner\"] = (\n+        config.use_shardy_partitioner.value\n+    )\n+\n   if logging.vlog_is_on(3):\n     # We already logged the MLIR module when we exported it.\n     logging.vlog(3, \"XlaCallModule %s\", str(call_module_attrs))\n\n```"
        ],
        "from_id": [
            "bixia1",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas] Fix 1D Iota\n\nPiperOrigin-RevId: 760705572",
        "output": [
            "```diff\nCommit: 62d59ace3087877347f606405bbcbc7196fe5201\nDate: 2025-05-19T18:33:43Z\nURL: https://github.com/jax-ml/jax/commit/62d59ace3087877347f606405bbcbc7196fe5201\nFiles changed: 2\nAdditions: +7, Deletions: -5\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 9af3cf1e3c0a..a67a71dd40f7 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -2302,11 +2302,13 @@ def _iota_lowering_rule(ctx: LoweringRuleContext, dtype, shape, dimension,\n   if len(shape) == 1:\n     if dimension != 0:\n       raise ValueError(\"Dimension must be 0 for 1D iota.\")\n-    def _1d_iota_helper(dtype, shape, dimension, sharding):\n-      iota_2d = lax.iota_p.bind(dtype, (1,) + shape, dimension, sharding)\n+    def _1d_iota_helper():\n+      iota_2d = lax.iota_p.bind(dtype=dtype,\n+                                shape=(1,) + shape,\n+                                dimension=1,\n+                                sharding=sharding)\n       return iota_2d[0]\n-    return lower_fun(_1d_iota_helper, multiple_results=False)(\n-        ctx, dtype, shape, dimension, sharding)\n+    return lower_fun(_1d_iota_helper, multiple_results=False)(ctx)\n   out_type = aval_to_ir_type(\n       ctx.lowering_context.dynamic_shape_replacement_fn, ctx.avals_out[0]\n   )\ndiff --git a/tests/pallas/ops_test.py b/tests/pallas/ops_test.py\nindex 9bb6d31d15e1..3baa26e5efd7 100644\n--- a/tests/pallas/ops_test.py\n+++ b/tests/pallas/ops_test.py\n@@ -1554,7 +1554,7 @@ def kernel(x_ref, y_ref, o_ref):\n   def test_iota(self, shape, dtype, dimension):\n     self.skip_if_mosaic_gpu()\n \n-    if jtu.test_device_matches([\"tpu\"]):\n+    if jtu.test_device_matches([\"tpu\"]) and dtype != jnp.int32:\n       self.skipTest(\"Only 32-bit integer iota supported\")\n \n     f = lambda: jax.lax.broadcasted_iota(dtype, shape, dimension)\n\n```"
        ],
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28812 from yliu120:yl/debug_pbroadcast\n\nPiperOrigin-RevId: 760640954",
        "output": [
            "```diff\nCommit: bb86bd64794146ef792820e1e2aff19c644c1aef\nDate: 2025-05-19T15:54:18Z\nURL: https://github.com/jax-ml/jax/commit/bb86bd64794146ef792820e1e2aff19c644c1aef\nFiles changed: 2\nAdditions: +49, Deletions: -3\ndiff --git a/jax/_src/lax/parallel.py b/jax/_src/lax/parallel.py\nindex 6df8690f1123..a9abf8f12939 100644\n--- a/jax/_src/lax/parallel.py\n+++ b/jax/_src/lax/parallel.py\n@@ -1122,14 +1122,27 @@ def _pbroadcast_lowering(ctx, x, *, axis_name, source):\n   def source_to_front(group):\n     return [group[source]] + list(group[:source]) + list(group[source + 1:])\n   replica_groups = [source_to_front(group) for group in replica_groups]\n-  channel = ctx.module_context.new_channel()\n+  is_spmd = isinstance(\n+      ctx.module_context.axis_context,\n+      (SPMDAxisContext, ShardingContext),\n+  )\n+  if is_spmd:\n+    # We want to emit the collective-broadcast with global device IDs and a unique\n+    # channel ID, as otherwise it interprets the devices as replicas instead\n+    # of partitions - and XLA is configured with only a single replica.\n+    channel = ctx.module_context.new_channel()\n+    channel_handle = hlo.ChannelHandle.get(channel, mlir.DEVICE_TO_DEVICE_TYPE)\n+    other_args = dict(channel_handle=channel_handle)\n+  else:\n+    other_args = {}\n   return hlo.CollectiveBroadcastOp(\n-      x, replica_groups=_replica_groups_hlo(replica_groups)).results\n+      x, replica_groups=_replica_groups_hlo(replica_groups), **other_args\n+  ).results\n \n pbroadcast_p = core.Primitive('pbroadcast')\n pbroadcast_p.def_abstract_eval(_raise_to_shaped_abstract_eval)\n ad.deflinear2(pbroadcast_p, _pbroadcast_transpose_rule)\n-mlir.register_lowering(pbroadcast_p, _pbroadcast_lowering)\n+mlir.register_lowering(pbroadcast_p, _pbroadcast_lowering, platform='gpu')\n batching.fancy_primitive_batchers[pbroadcast_p] = _pbroadcast_batcher\n batching.skippable_batchers[pbroadcast_p] = partial(_names_in_param, 'axis_name')\n \ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 2fdc846a356b..1bebba095896 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -290,6 +290,39 @@ def fwd(a):\n     c = fwd(a)\n     assert (c == jnp.reshape(a.T, (1, 64))).all()\n \n+  @parameterized.named_parameters(\n+      dict(\n+          testcase_name='_partial_replicated', replicate_on_axes='x',\n+      ),\n+      dict(\n+          testcase_name='_fully_replicated',\n+          replicate_on_axes=('x', 'y'),\n+      ),\n+  )\n+  @jtu.run_on_devices(\"gpu\")\n+  def test_pbroadcast(self, replicate_on_axes):\n+    mesh = jtu.create_mesh((4, 2), ('x', 'y'))\n+    sharded_axes = set(mesh.axis_names) - set(replicate_on_axes)\n+    sharded_axes = None if not sharded_axes else list(sharded_axes)\n+    in_out_sharding = jax.sharding.NamedSharding(mesh, P(sharded_axes, None))\n+    a = jax.device_put(jnp.arange(16).reshape((4, 4)), in_out_sharding)\n+\n+    @jax.jit\n+    @partial(\n+        shard_map,\n+        mesh=mesh,\n+        in_specs=(in_out_sharding.spec,),\n+        out_specs=in_out_sharding.spec,\n+        check_vma=False,\n+    )\n+    def fwd(x):\n+      axis_index = lax.axis_index(replicate_on_axes)\n+      x = jnp.where(axis_index == 0, x + 1, x)\n+      return lax.pbroadcast(x, replicate_on_axes, source=0)\n+\n+    c = fwd(a)  # Don't crash\n+    self.assertAllClose(c, a + 1)\n+\n   def test_all_to_all_with_axis_index_groups(self):\n     mesh = jtu.create_mesh((4,), ('x',))\n     a = jax.device_put(\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28829 from hawkinsp:req\n\nPiperOrigin-RevId: 760636732",
        "output": [
            "```diff\nCommit: 74b595fe0d8b78e3d775adac5995243ff7b28e05\nDate: 2025-05-19T15:41:15Z\nURL: https://github.com/jax-ml/jax/commit/74b595fe0d8b78e3d775adac5995243ff7b28e05\nFiles changed: 10\nAdditions: +72, Deletions: -80\ndiff --git a/.bazelrc b/.bazelrc\nindex 79df03863b02..53676637c839 100644\n--- a/.bazelrc\n+++ b/.bazelrc\n@@ -244,10 +244,6 @@ build:ci_linux_aarch64_base --config=clang --verbose_failures=true\n build:ci_linux_aarch64_base --action_env=TF_SYSROOT=\"/dt10\"\n build:ci_linux_aarch64_base --color=yes\n \n-# Workaround for https://github.com/numpy/numpy/issues/28843\n-# TODO(phawkins): remove this after upgrading to NumPy 2.2.6.\n-build:ci_linux_aarch64_base --test_env=OMP_NUM_THREADS=8\n-\n build:ci_linux_aarch64 --config=ci_linux_aarch64_base\n build:ci_linux_aarch64 --host_crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n build:ci_linux_aarch64 --crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n@@ -383,10 +379,6 @@ build:rbe_cross_compile_base --remote_instance_name=projects/tensorflow-testing/\n build:rbe_cross_compile_linux_aarch64 --config=cross_compile_linux_aarch64\n build:rbe_cross_compile_linux_aarch64 --config=rbe_cross_compile_base\n \n-# Workaround for https://github.com/numpy/numpy/issues/28843\n-# TODO(phawkins): remove this after upgrading to NumPy 2.2.6.\n-build:rbe_cross_compile_linux_aarch64 --test_env=OMP_NUM_THREADS=8\n-\n # Mac x86\n build:cross_compile_darwin_x86_64 --config=cross_compile_base\n build:cross_compile_darwin_x86_64 --config=nonccl\ndiff --git a/build/freethreading-requirements.txt b/build/freethreading-requirements.txt\nindex cc302cffdd0c..467578870ee9 100644\n--- a/build/freethreading-requirements.txt\n+++ b/build/freethreading-requirements.txt\n@@ -1,3 +1,3 @@\n # Under free-threading, we need an up-to-date numpy at least for the moment.\n-numpy~=2.2.5; python_version==\"3.13\"\n-numpy>=2.2.5; python_version>=\"3.14\"\n+numpy~=2.2.6; python_version==\"3.13\"\n+numpy>=2.2.6; python_version>=\"3.14\"\ndiff --git a/build/nonfreethreading-requirements.txt b/build/nonfreethreading-requirements.txt\nindex f8171559a142..8bd139bf99ac 100644\n--- a/build/nonfreethreading-requirements.txt\n+++ b/build/nonfreethreading-requirements.txt\n@@ -1,6 +1,6 @@\n numpy~=2.0.0; python_version<=\"3.12\"\n numpy~=2.1.0; python_version==\"3.13\"\n-numpy>=2.2.5; python_version>=\"3.14\"\n+numpy>=2.2.6; python_version>=\"3.14\"\n \n # These packages have not released free-threaded wheels.\n zstandard\ndiff --git a/build/requirements.in b/build/requirements.in\nindex 8b8af9d6b591..c5ce2ea279bd 100644\n--- a/build/requirements.in\n+++ b/build/requirements.in\n@@ -19,8 +19,8 @@ wheel\n jaxlib\n \n # The with-cuda extra also includes NVIDIA's pip packages.\n-jax-cuda12-plugin[with-cuda]\n-jax-cuda12-pjrt\n+jax-cuda12-plugin[with-cuda] ; sys_platform == \"linux\"\n+jax-cuda12-pjrt ; sys_platform == \"linux\"\n \n # TPU dependencies\n libtpu ; sys_platform == \"linux\" and platform_machine == \"x86_64\"\ndiff --git a/build/requirements_lock_3_10.txt b/build/requirements_lock_3_10.txt\nindex c4ca6088e4bf..a4c6b1bf2b77 100644\n--- a/build/requirements_lock_3_10.txt\n+++ b/build/requirements_lock_3_10.txt\n@@ -160,13 +160,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_11.txt b/build/requirements_lock_3_11.txt\nindex 1f667115af04..0633e733414b 100644\n--- a/build/requirements_lock_3_11.txt\n+++ b/build/requirements_lock_3_11.txt\n@@ -154,13 +154,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_12.txt b/build/requirements_lock_3_12.txt\nindex 20ca67a3e921..1ab77a6ec36e 100644\n--- a/build/requirements_lock_3_12.txt\n+++ b/build/requirements_lock_3_12.txt\n@@ -154,13 +154,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_13.txt b/build/requirements_lock_3_13.txt\nindex 804373b03899..c20068b732e6 100644\n--- a/build/requirements_lock_3_13.txt\n+++ b/build/requirements_lock_3_13.txt\n@@ -181,13 +181,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_13_ft.txt b/build/requirements_lock_3_13_ft.txt\nindex c7a1c882fc73..3795343df0cb 100644\n--- a/build/requirements_lock_3_13_ft.txt\n+++ b/build/requirements_lock_3_13_ft.txt\n@@ -172,13 +172,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n@@ -371,62 +371,62 @@ mpmath==1.3.0 \\\n     --hash=sha256:7a28eb2a9774d00c7bc92411c19a89209d5da7c4c9a9e227be8330a23a25b91f \\\n     --hash=sha256:a0b2b9fe80bbcd81a6647ff13108738cfb482d481d826cc0e02f5b35e5c88d2c\n     # via -r build/test-requirements.txt\n-numpy==2.2.5 ; python_version == \"3.13\" \\\n-    --hash=sha256:0255732338c4fdd00996c0421884ea8a3651eea555c3a56b84892b66f696eb70 \\\n-    --hash=sha256:02f226baeefa68f7d579e213d0f3493496397d8f1cff5e2b222af274c86a552a \\\n-    --hash=sha256:059b51b658f4414fff78c6d7b1b4e18283ab5fa56d270ff212d5ba0c561846f4 \\\n-    --hash=sha256:0bcb1d057b7571334139129b7f941588f69ce7c4ed15a9d6162b2ea54ded700c \\\n-    --hash=sha256:0cd48122a6b7eab8f06404805b1bd5856200e3ed6f8a1b9a194f9d9054631beb \\\n-    --hash=sha256:19f4718c9012e3baea91a7dba661dcab2451cda2550678dc30d53acb91a7290f \\\n-    --hash=sha256:1a161c2c79ab30fe4501d5a2bbfe8b162490757cf90b7f05be8b80bc02f7bb8e \\\n-    --hash=sha256:1f4a922da1729f4c40932b2af4fe84909c7a6e167e6e99f71838ce3a29f3fe26 \\\n-    --hash=sha256:261a1ef047751bb02f29dfe337230b5882b54521ca121fc7f62668133cb119c9 \\\n-    --hash=sha256:262d23f383170f99cd9191a7c85b9a50970fe9069b2f8ab5d786eca8a675d60b \\\n-    --hash=sha256:2ba321813a00e508d5421104464510cc962a6f791aa2fca1c97b1e65027da80d \\\n-    --hash=sha256:2c1a1c6ccce4022383583a6ded7bbcda22fc635eb4eb1e0a053336425ed36dfa \\\n-    --hash=sha256:352d330048c055ea6db701130abc48a21bec690a8d38f8284e00fab256dc1376 \\\n-    --hash=sha256:369e0d4647c17c9363244f3468f2227d557a74b6781cb62ce57cf3ef5cc7c610 \\\n-    --hash=sha256:36ab5b23915887543441efd0417e6a3baa08634308894316f446027611b53bf1 \\\n-    --hash=sha256:37e32e985f03c06206582a7323ef926b4e78bdaa6915095ef08070471865b906 \\\n-    --hash=sha256:3a801fef99668f309b88640e28d261991bfad9617c27beda4a3aec4f217ea073 \\\n-    --hash=sha256:3d14b17b9be5f9c9301f43d2e2a4886a33b53f4e6fdf9ca2f4cc60aeeee76372 \\\n-    --hash=sha256:422cc684f17bc963da5f59a31530b3936f57c95a29743056ef7a7903a5dbdf88 \\\n-    --hash=sha256:4520caa3807c1ceb005d125a75e715567806fed67e315cea619d5ec6e75a4191 \\\n-    --hash=sha256:47834cde750d3c9f4e52c6ca28a7361859fcaf52695c7dc3cc1a720b8922683e \\\n-    --hash=sha256:47f9ed103af0bc63182609044b0490747e03bd20a67e391192dde119bf43d52f \\\n-    --hash=sha256:498815b96f67dc347e03b719ef49c772589fb74b8ee9ea2c37feae915ad6ebda \\\n-    --hash=sha256:54088a5a147ab71a8e7fdfd8c3601972751ded0739c6b696ad9cb0343e21ab73 \\\n-    --hash=sha256:55f09e00d4dccd76b179c0f18a44f041e5332fd0e022886ba1c0bbf3ea4a18d0 \\\n-    --hash=sha256:5a0ac90e46fdb5649ab6369d1ab6104bfe5854ab19b645bf5cda0127a13034ae \\\n-    --hash=sha256:6411f744f7f20081b1b4e7112e0f4c9c5b08f94b9f086e6f0adf3645f85d3a4d \\\n-    --hash=sha256:6413d48a9be53e183eb06495d8e3b006ef8f87c324af68241bbe7a39e8ff54c3 \\\n-    --hash=sha256:7451f92eddf8503c9b8aa4fe6aa7e87fd51a29c2cfc5f7dbd72efde6c65acf57 \\\n-    --hash=sha256:8b4c0773b6ada798f51f0f8e30c054d32304ccc6e9c5d93d46cb26f3d385ab19 \\\n-    --hash=sha256:8dfa94b6a4374e7851bbb6f35e6ded2120b752b063e6acdd3157e4d2bb922eba \\\n-    --hash=sha256:97c8425d4e26437e65e1d189d22dff4a079b747ff9c2788057bfb8114ce1e133 \\\n-    --hash=sha256:9d75f338f5f79ee23548b03d801d28a505198297534f62416391857ea0479571 \\\n-    --hash=sha256:9de6832228f617c9ef45d948ec1cd8949c482238d68b2477e6f642c33a7b0a54 \\\n-    --hash=sha256:a4cbdef3ddf777423060c6f81b5694bad2dc9675f110c4b2a60dc0181543fac7 \\\n-    --hash=sha256:a9c0d994680cd991b1cb772e8b297340085466a6fe964bc9d4e80f5e2f43c291 \\\n-    --hash=sha256:aa70fdbdc3b169d69e8c59e65c07a1c9351ceb438e627f0fdcd471015cd956be \\\n-    --hash=sha256:abe38cd8381245a7f49967a6010e77dbf3680bd3627c0fe4362dd693b404c7f8 \\\n-    --hash=sha256:b13f04968b46ad705f7c8a80122a42ae8f620536ea38cf4bdd374302926424dd \\\n-    --hash=sha256:b4ea7e1cff6784e58fe281ce7e7f05036b3e1c89c6f922a6bfbc0a7e8768adbe \\\n-    --hash=sha256:b6f91524d31b34f4a5fee24f5bc16dcd1491b668798b6d85585d836c1e633a6a \\\n-    --hash=sha256:c26843fd58f65da9491165072da2cccc372530681de481ef670dcc8e27cfb066 \\\n-    --hash=sha256:c42365005c7a6c42436a54d28c43fe0e01ca11eb2ac3cefe796c25a5f98e5e9b \\\n-    --hash=sha256:c8b82a55ef86a2d8e81b63da85e55f5537d2157165be1cb2ce7cfa57b6aef38b \\\n-    --hash=sha256:ced69262a8278547e63409b2653b372bf4baff0870c57efa76c5703fd6543282 \\\n-    --hash=sha256:d2e3bdadaba0e040d1e7ab39db73e0afe2c74ae277f5614dad53eadbecbbb169 \\\n-    --hash=sha256:d403c84991b5ad291d3809bace5e85f4bbf44a04bdc9a88ed2bb1807b3360bb8 \\\n-    --hash=sha256:d7543263084a85fbc09c704b515395398d31d6395518446237eac219eab9e55e \\\n-    --hash=sha256:d8882a829fd779f0f43998e931c466802a77ca1ee0fe25a3abe50278616b1471 \\\n-    --hash=sha256:e4f0b035d9d0ed519c813ee23e0a733db81ec37d2e9503afbb6e54ccfdee0fa7 \\\n-    --hash=sha256:e8b025c351b9f0e8b5436cf28a07fa4ac0204d67b38f01433ac7f9b870fa38c6 \\\n-    --hash=sha256:eb7fd5b184e5d277afa9ec0ad5e4eb562ecff541e7f60e69ee69c8d59e9aeaba \\\n-    --hash=sha256:ec31367fd6a255dc8de4772bd1658c3e926d8e860a0b6e922b615e532d320ddc \\\n-    --hash=sha256:ee461a4eaab4f165b68780a6a1af95fb23a29932be7569b9fab666c407969051 \\\n-    --hash=sha256:f5045039100ed58fa817a6227a356240ea1b9a1bc141018864c306c1a16d4175\n+numpy==2.2.6 ; python_version == \"3.13\" \\\n+    --hash=sha256:038613e9fb8c72b0a41f025a7e4c3f0b7a1b5d768ece4796b674c8f3fe13efff \\\n+    --hash=sha256:0678000bb9ac1475cd454c6b8c799206af8107e310843532b04d49649c717a47 \\\n+    --hash=sha256:0811bb762109d9708cca4d0b13c4f67146e3c3b7cf8d34018c722adb2d957c84 \\\n+    --hash=sha256:0b605b275d7bd0c640cad4e5d30fa701a8d59302e127e5f79138ad62762c3e3d \\\n+    --hash=sha256:0bca768cd85ae743b2affdc762d617eddf3bcf8724435498a1e80132d04879e6 \\\n+    --hash=sha256:1bc23a79bfabc5d056d106f9befb8d50c31ced2fbc70eedb8155aec74a45798f \\\n+    --hash=sha256:287cc3162b6f01463ccd86be154f284d0893d2b3ed7292439ea97eafa8170e0b \\\n+    --hash=sha256:37c0ca431f82cd5fa716eca9506aefcabc247fb27ba69c5062a6d3ade8cf8f49 \\\n+    --hash=sha256:37e990a01ae6ec7fe7fa1c26c55ecb672dd98b19c3d0e1d1f326fa13cb38d163 \\\n+    --hash=sha256:389d771b1623ec92636b0786bc4ae56abafad4a4c513d36a55dce14bd9ce8571 \\\n+    --hash=sha256:3d70692235e759f260c3d837193090014aebdf026dfd167834bcba43e30c2a42 \\\n+    --hash=sha256:41c5a21f4a04fa86436124d388f6ed60a9343a6f767fced1a8a71c3fbca038ff \\\n+    --hash=sha256:481b49095335f8eed42e39e8041327c05b0f6f4780488f61286ed3c01368d491 \\\n+    --hash=sha256:4eeaae00d789f66c7a25ac5f34b71a7035bb474e679f410e5e1a94deb24cf2d4 \\\n+    --hash=sha256:55a4d33fa519660d69614a9fad433be87e5252f4b03850642f88993f7b2ca566 \\\n+    --hash=sha256:5a6429d4be8ca66d889b7cf70f536a397dc45ba6faeb5f8c5427935d9592e9cf \\\n+    --hash=sha256:5bd4fc3ac8926b3819797a7c0e2631eb889b4118a9898c84f585a54d475b7e40 \\\n+    --hash=sha256:5beb72339d9d4fa36522fc63802f469b13cdbe4fdab4a288f0c441b74272ebfd \\\n+    --hash=sha256:6031dd6dfecc0cf9f668681a37648373bddd6421fff6c66ec1624eed0180ee06 \\\n+    --hash=sha256:71594f7c51a18e728451bb50cc60a3ce4e6538822731b2933209a1f3614e9282 \\\n+    --hash=sha256:74d4531beb257d2c3f4b261bfb0fc09e0f9ebb8842d82a7b4209415896adc680 \\\n+    --hash=sha256:7befc596a7dc9da8a337f79802ee8adb30a552a94f792b9c9d18c840055907db \\\n+    --hash=sha256:894b3a42502226a1cac872f840030665f33326fc3dac8e57c607905773cdcde3 \\\n+    --hash=sha256:8e41fd67c52b86603a91c1a505ebaef50b3314de0213461c7a6e99c9a3beff90 \\\n+    --hash=sha256:8e9ace4a37db23421249ed236fdcdd457d671e25146786dfc96835cd951aa7c1 \\\n+    --hash=sha256:8fc377d995680230e83241d8a96def29f204b5782f371c532579b4f20607a289 \\\n+    --hash=sha256:9551a499bf125c1d4f9e250377c1ee2eddd02e01eac6644c080162c0c51778ab \\\n+    --hash=sha256:b0544343a702fa80c95ad5d3d608ea3599dd54d4632df855e4c8d24eb6ecfa1c \\\n+    --hash=sha256:b093dd74e50a8cba3e873868d9e93a85b78e0daf2e98c6797566ad8044e8363d \\\n+    --hash=sha256:b412caa66f72040e6d268491a59f2c43bf03eb6c96dd8f0307829feb7fa2b6fb \\\n+    --hash=sha256:b4f13750ce79751586ae2eb824ba7e1e8dba64784086c98cdbbcc6a42112ce0d \\\n+    --hash=sha256:b64d8d4d17135e00c8e346e0a738deb17e754230d7e0810ac5012750bbd85a5a \\\n+    --hash=sha256:ba10f8411898fc418a521833e014a77d3ca01c15b0c6cdcce6a0d2897e6dbbdf \\\n+    --hash=sha256:bd48227a919f1bafbdda0583705e547892342c26fb127219d60a5c36882609d1 \\\n+    --hash=sha256:c1f9540be57940698ed329904db803cf7a402f3fc200bfe599334c9bd84a40b2 \\\n+    --hash=sha256:c820a93b0255bc360f53eca31a0e676fd1101f673dda8da93454a12e23fc5f7a \\\n+    --hash=sha256:ce47521a4754c8f4593837384bd3424880629f718d87c5d44f8ed763edd63543 \\\n+    --hash=sha256:d042d24c90c41b54fd506da306759e06e568864df8ec17ccc17e9e884634fd00 \\\n+    --hash=sha256:de749064336d37e340f640b05f24e9e3dd678c57318c7289d222a8a2f543e90c \\\n+    --hash=sha256:e1dda9c7e08dc141e0247a5b8f49cf05984955246a327d4c48bda16821947b2f \\\n+    --hash=sha256:e29554e2bef54a90aa5cc07da6ce955accb83f21ab5de01a62c8478897b264fd \\\n+    --hash=sha256:e3143e4451880bed956e706a3220b4e5cf6172ef05fcc397f6f36a550b1dd868 \\\n+    --hash=sha256:e8213002e427c69c45a52bbd94163084025f533a55a59d6f9c5b820774ef3303 \\\n+    --hash=sha256:efd28d4e9cd7d7a8d39074a4d44c63eda73401580c5c76acda2ce969e0a38e83 \\\n+    --hash=sha256:f0fd6321b839904e15c46e0d257fdd101dd7f530fe03fd6359c1ea63738703f3 \\\n+    --hash=sha256:f1372f041402e37e5e633e586f62aa53de2eac8d98cbfb822806ce4bbefcb74d \\\n+    --hash=sha256:f2618db89be1b4e05f7a1a847a9c1c0abd63e63a1607d892dd54668dd92faf87 \\\n+    --hash=sha256:f447e6acb680fd307f40d3da4852208af94afdfab89cf850986c3ca00562f4fa \\\n+    --hash=sha256:f92729c95468a2f4f15e9bb94c432a9229d0d50de67304399627a943201baa2f \\\n+    --hash=sha256:f9f1adb22318e121c5c69a09142811a201ef17ab257a1e66ca3025065b7f53ae \\\n+    --hash=sha256:fc0c5673685c508a142ca65209b4e79ed6740a4ed6b2267dbba90f34b0b3cfda \\\n+    --hash=sha256:fc7b73d02efb0e18c000e9ad8b83480dfcd5dfd11065997ed4c6747470ae8915 \\\n+    --hash=sha256:fd83c01228a688733f1ded5201c678f0c53ecc1006ffbc404db9f7a899ac6249 \\\n+    --hash=sha256:fe27749d33bb772c80dcd84ae7e8df2adc920ae8297400dabec45f0dedb3f6de \\\n+    --hash=sha256:fee4236c876c4e8369388054d02d0e9bb84821feb1a64dd59e137e6511a551f8\n     # via\n     #   -r build/freethreading-requirements.txt\n     #   contourpy\ndiff --git a/build/requirements_lock_3_14.txt b/build/requirements_lock_3_14.txt\nindex 6edcd30ebe16..157dca5adbab 100644\n--- a/build/requirements_lock_3_14.txt\n+++ b/build/requirements_lock_3_14.txt\n@@ -48,7 +48,7 @@ ml-dtypes==0.5.1\n     #   tensorstore\n mpmath==1.4.0a4\n     # via -r build/test-requirements.txt\n-numpy==2.2.5\n+numpy==2.2.6\n     # via\n     #   -r build/nonfreethreading-requirements.txt\n     #   contourpy\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28828 from hawkinsp:vercheck\n\nPiperOrigin-RevId: 760633986",
        "output": [
            "```diff\nCommit: 2765178b10b8b42d14c322a5275c402d2fcde0c1\nDate: 2025-05-19T15:34:03Z\nURL: https://github.com/jax-ml/jax/commit/2765178b10b8b42d14c322a5275c402d2fcde0c1\nFiles changed: 1\nAdditions: +8, Deletions: -5\ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 9df7fc69ff1a..02bcbcf16dbc 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -180,11 +180,14 @@ def _version_check(name: str,\n                  cuda_versions.cufft_build_version,\n                  # Ignore patch versions.\n                  scale_for_comparison=100)\n-  _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n-                 cuda_versions.cusolver_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=11400)\n+  # TODO(phawkins): for some reason this check fails with a cusolver internal\n+  # error when fetching the version. This may be a path error from our stubs.\n+  # Figure out what's happening here and reenable.\n+  # _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n+  #                cuda_versions.cusolver_build_version,\n+  #                # Ignore patch versions.\n+  #                scale_for_comparison=100,\n+  #                min_supported_version=11400)\n   _version_check(\"cuPTI\", cuda_versions.cupti_get_version,\n                  cuda_versions.cupti_build_version,\n                  min_supported_version=18)\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "shard map pbroadcast",
        "output": [
            "```diff\nCommit: 3143214ff69c62523ed1a6baf47fd89dc40bfef9\nDate: 2025-05-19T15:25:27Z\nURL: https://github.com/jax-ml/jax/commit/3143214ff69c62523ed1a6baf47fd89dc40bfef9\nFiles changed: 2\nAdditions: +49, Deletions: -3\ndiff --git a/jax/_src/lax/parallel.py b/jax/_src/lax/parallel.py\nindex 6df8690f1123..a9abf8f12939 100644\n--- a/jax/_src/lax/parallel.py\n+++ b/jax/_src/lax/parallel.py\n@@ -1122,14 +1122,27 @@ def _pbroadcast_lowering(ctx, x, *, axis_name, source):\n   def source_to_front(group):\n     return [group[source]] + list(group[:source]) + list(group[source + 1:])\n   replica_groups = [source_to_front(group) for group in replica_groups]\n-  channel = ctx.module_context.new_channel()\n+  is_spmd = isinstance(\n+      ctx.module_context.axis_context,\n+      (SPMDAxisContext, ShardingContext),\n+  )\n+  if is_spmd:\n+    # We want to emit the collective-broadcast with global device IDs and a unique\n+    # channel ID, as otherwise it interprets the devices as replicas instead\n+    # of partitions - and XLA is configured with only a single replica.\n+    channel = ctx.module_context.new_channel()\n+    channel_handle = hlo.ChannelHandle.get(channel, mlir.DEVICE_TO_DEVICE_TYPE)\n+    other_args = dict(channel_handle=channel_handle)\n+  else:\n+    other_args = {}\n   return hlo.CollectiveBroadcastOp(\n-      x, replica_groups=_replica_groups_hlo(replica_groups)).results\n+      x, replica_groups=_replica_groups_hlo(replica_groups), **other_args\n+  ).results\n \n pbroadcast_p = core.Primitive('pbroadcast')\n pbroadcast_p.def_abstract_eval(_raise_to_shaped_abstract_eval)\n ad.deflinear2(pbroadcast_p, _pbroadcast_transpose_rule)\n-mlir.register_lowering(pbroadcast_p, _pbroadcast_lowering)\n+mlir.register_lowering(pbroadcast_p, _pbroadcast_lowering, platform='gpu')\n batching.fancy_primitive_batchers[pbroadcast_p] = _pbroadcast_batcher\n batching.skippable_batchers[pbroadcast_p] = partial(_names_in_param, 'axis_name')\n \ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 2fdc846a356b..1bebba095896 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -290,6 +290,39 @@ def fwd(a):\n     c = fwd(a)\n     assert (c == jnp.reshape(a.T, (1, 64))).all()\n \n+  @parameterized.named_parameters(\n+      dict(\n+          testcase_name='_partial_replicated', replicate_on_axes='x',\n+      ),\n+      dict(\n+          testcase_name='_fully_replicated',\n+          replicate_on_axes=('x', 'y'),\n+      ),\n+  )\n+  @jtu.run_on_devices(\"gpu\")\n+  def test_pbroadcast(self, replicate_on_axes):\n+    mesh = jtu.create_mesh((4, 2), ('x', 'y'))\n+    sharded_axes = set(mesh.axis_names) - set(replicate_on_axes)\n+    sharded_axes = None if not sharded_axes else list(sharded_axes)\n+    in_out_sharding = jax.sharding.NamedSharding(mesh, P(sharded_axes, None))\n+    a = jax.device_put(jnp.arange(16).reshape((4, 4)), in_out_sharding)\n+\n+    @jax.jit\n+    @partial(\n+        shard_map,\n+        mesh=mesh,\n+        in_specs=(in_out_sharding.spec,),\n+        out_specs=in_out_sharding.spec,\n+        check_vma=False,\n+    )\n+    def fwd(x):\n+      axis_index = lax.axis_index(replicate_on_axes)\n+      x = jnp.where(axis_index == 0, x + 1, x)\n+      return lax.pbroadcast(x, replicate_on_axes, source=0)\n+\n+    c = fwd(a)  # Don't crash\n+    self.assertAllClose(c, a + 1)\n+\n   def test_all_to_all_with_axis_index_groups(self):\n     mesh = jtu.create_mesh((4,), ('x',))\n     a = jax.device_put(\n\n```"
        ],
        "from_id": [
            "yliu120"
        ]
    },
    {
        "text_input": "Fix the CAS loop in semaphore_wait lowering\n\nThe loop incorrectly assumed that we're waiting for the semaphore\nvalue to be equal to the wait value, so that we can reset it to 0.\nThis, however, is not how semaphores work. The CAS loop should wait\nuntil the value is _at least_ the wait value, and should update its\nexpectation at every step in case the swap failed.\n\nThe attached test deadlocks with the original implementation, but\nworks fine with the new one.\n\nPiperOrigin-RevId: 760619094",
        "output": [
            "```diff\nCommit: ea5a47d44a69f782d153fbecd5cf6999af77caec\nDate: 2025-05-19T14:41:17Z\nURL: https://github.com/jax-ml/jax/commit/ea5a47d44a69f782d153fbecd5cf6999af77caec\nFiles changed: 2\nAdditions: +41, Deletions: -8\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 73623de43e39..bd304e8b6745 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -2999,6 +2999,11 @@ def _semaphore_signal_lowering_rule(\n     )\n   # TODO(apaszke): Narrow the scope from .sys to .gpu when the semaphore is local.\n   val = _ir_constant(value, i32)\n+  # We only signal the semaphore from a single lane, which does not guarantee\n+  # anything about the state of the other three warps in the warpgroup (they\n+  # might still be e.g. reading memory that someone will overwrite once they\n+  # receive a signal).\n+  mgpu.utils.warpgroup_barrier()\n   pred = ctx.module_ctx.single_wg_lane_predicate\n   llvm_dialect.inline_asm(\n     i32,\n@@ -3022,23 +3027,25 @@ def _semaphore_wait_lowering_rule(ctx: LoweringRuleContext, *args, args_tree):\n   sem_ptr = mgpu.utils.memref_ptr(sem)\n   i32_ty = ir.IntegerType.get_signless(32)\n   ne_pred = arith_dialect.CmpIPredicate.ne\n-  zero_const = mgpu.utils.c(0, i32_ty)\n   val = _ir_constant(value, i32_ty)\n \n   with mgpu.single_thread(scope=mgpu.ThreadSubset.WARPGROUP):\n     # Create the while loop for busy waiting\n-    while_op = scf_dialect.WhileOp([i32_ty], [zero_const])\n+    while_op = scf_dialect.WhileOp([i32_ty], [val])\n     before_block = while_op.before.blocks.append(i32_ty)\n     with ir.InsertionPoint.at_block_begin(before_block):\n-      old_val = llvm_dialect.inline_asm(\n+      [expected_in_memory] = before_block.arguments\n+      new_val = arith_dialect.subi(expected_in_memory, val)\n+      in_memory = llvm_dialect.inline_asm(\n         i32_ty,\n-        [sem_ptr, val, zero_const],\n+        [sem_ptr, expected_in_memory, new_val],\n         \"atom.acquire.sys.global.cas.b32 $0, [$1], $2, $3;\",\n         \"=r,l,r,r\",\n         has_side_effects=True,\n       )\n-      comparison = arith_dialect.cmpi(ne_pred, old_val, val)\n-      scf_dialect.condition(comparison, before_block.arguments)\n+      comparison = arith_dialect.cmpi(ne_pred, in_memory, expected_in_memory)\n+      new_expected_in_memory = arith_dialect.maxui(in_memory, val)\n+      scf_dialect.condition(comparison, [new_expected_in_memory])\n     after_block = while_op.after.blocks.append(i32_ty)\n     with ir.InsertionPoint.at_block_begin(after_block):\n       scf_dialect.yield_(after_block.arguments)\ndiff --git a/tests/pallas/gpu_pallas_distributed_test.py b/tests/pallas/gpu_pallas_distributed_test.py\nindex 3da39ba925c7..d862e6b9b819 100644\n--- a/tests/pallas/gpu_pallas_distributed_test.py\n+++ b/tests/pallas/gpu_pallas_distributed_test.py\n@@ -44,8 +44,6 @@ def setUp(self):\n     super().setUp()\n \n   def test_basic_remote_dma(self):\n-    if jax.process_count() < 2:\n-      self.skipTest(\"Test requires multiple processes.\")\n     if jax.process_index() > 2:\n       return  # Only 2 processes needed.\n     def kernel(x_ref, y_ref, ready_sem, recv_sem):\n@@ -86,6 +84,34 @@ def body(x):\n     expected = x[8:] if jax.process_index() == 0 else x[:8]\n     np.testing.assert_allclose(y.addressable_shards[0].data, expected)\n \n+  def test_wait_twice(self):\n+    if jax.process_index() > 2:\n+      return  # Only 2 processes needed.\n+\n+    def kernel(y_ref, sem):\n+      other_dev_id = 1 - lax.axis_index('x')\n+      pl.semaphore_signal(sem, 2, device_id=other_dev_id,\n+                          device_id_type=pl.DeviceIdType.LOGICAL)\n+      pl.semaphore_wait(sem)\n+      pl.semaphore_wait(sem)\n+      y_ref[...] = jnp.ones_like(y_ref)\n+\n+    kernel_call = pl.pallas_call(\n+        kernel,\n+        out_specs=pl.BlockSpec(memory_space=plgpu.GMEM),\n+        out_shape=jax.ShapeDtypeStruct((8, 128), jnp.float32),\n+        scratch_shapes=[plgpu.SemaphoreType.REGULAR],\n+    )\n+\n+    devices = jax.devices()[:2]\n+    mesh = jax.sharding.Mesh(devices, ['x'])\n+    y = jax.jit(\n+        shard_map.shard_map(\n+            kernel_call, mesh, in_specs=(), out_specs=P(None), check_rep=False,\n+        )\n+    )()\n+    np.testing.assert_allclose(y, jnp.ones_like(y))\n+\n \n if __name__ == '__main__':\n   jt_multiprocess.main()\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Adding an optional causal masking to the manual pipelining example.\nTesting shows the runtime is about half the flops with causal masking vs without.\n\nTests pass if we revert to cuda 12.0 with `--//third_party/gpus/cuda:by_exception_only_cuda_version_override=12_0`\n\nPiperOrigin-RevId: 760616647",
        "output": [
            "```diff\nCommit: ec9e71e1e71e015bc6cdccf825a554da41f5368d\nDate: 2025-05-19T14:29:19Z\nURL: https://github.com/jax-ml/jax/commit/ec9e71e1e71e015bc6cdccf825a554da41f5368d\nFiles changed: 3\nAdditions: +95, Deletions: -23\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex 3256953cd332..a100aa96faba 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -33,6 +33,7 @@ class TuningConfig:\n   block_kv: int\n   max_concurrent_steps: int\n   use_schedule_barrier: bool = True\n+  causal: bool = False\n   compute_wgs_bwd: int = 1\n \n   block_q_dkv: int | None = None\n@@ -84,6 +85,8 @@ def _attention_forward(q, k, v, config: TuningConfig, save_residuals: bool = Fal\n       config.max_concurrent_steps, kv_seq_len // config.block_kv\n   )\n   block_q, block_kv = config.block_q, config.block_kv\n+  if kv_seq_len % block_kv:\n+    raise ValueError(f\"{kv_seq_len=} must be a multiple of {block_kv=}\")\n \n   def kernel(q_ref, k_ref, v_ref, out_ref, lse_ref, scoped):\n     batch = lax.axis_index(\"batch\")\n@@ -97,6 +100,12 @@ def perform_schedule_barrier():\n       plgpu.barrier_arrive(schedule_barrier)\n       plgpu.barrier_wait(schedule_barrier)\n \n+    if config.causal:\n+      block_q_end = (lax.axis_index(\"q_seq\") + 1) * (2 * block_q)\n+      block_max_kv_steps = pl.cdiv(block_q_end, jnp.array(block_kv, jnp.int32))\n+    else:\n+      block_max_kv_steps = kv_seq_len // block_kv\n+\n     @pl.when(wg_idx < 2)\n     def _compute_wg():\n       plgpu.set_max_registers(232, action=\"increase\")\n@@ -104,6 +113,11 @@ def _compute_wg():\n       lse_smem = lse_smem2.at[wg_idx] if lse_smem2 is not None else None\n       q_seq_base = lax.axis_index(\"q_seq\") * (2 * block_q) + wg_idx * block_q\n \n+      if config.causal:\n+        kv_steps = pl.cdiv(q_seq_base + block_q, jnp.array(block_kv, jnp.int32))\n+      else:\n+        kv_steps = block_max_kv_steps\n+\n       plgpu.copy_gmem_to_smem(\n           q_ref.at[batch, pl.ds(q_seq_base, block_q), q_head],\n           qo_smem,\n@@ -121,12 +135,14 @@ def _compute_wg():\n           jnp.full((block_q, head_dim), 0, dtype=jnp.float32), plgpu.Layout.WGMMA,\n       )\n \n-      plgpu.barrier_wait(k_barriers.at[0])\n+      @pl.when(kv_steps > 0)\n+      def _():\n+        plgpu.barrier_wait(k_barriers.at[0])\n \n       pl.when(wg_idx == 1)(perform_schedule_barrier)\n-      def kv_loop(kv_step, carry):\n+      def kv_loop(kv_step, carry, causal: bool = False):\n         acc, m_i, l_i = carry\n-        slot = lax.rem(kv_step, max_concurrent_steps)\n+        slot = lax.rem(kv_step, jnp.array(max_concurrent_steps, kv_step.dtype))\n \n         # QK\n         def compute_qk(acc_ref):\n@@ -136,6 +152,12 @@ def compute_qk(acc_ref):\n         qk = pl.run_scoped(compute_qk, plgpu.ACC((block_q, block_kv), jnp.float32))\n         plgpu.barrier_arrive(k_consumed_barriers.at[slot])\n \n+        if causal:\n+          q_ids = plgpu.broadcasted_iota(jnp.int32, (block_q, block_kv), 0, layout=plgpu.Layout.WGMMA)\n+          kv_ids = plgpu.broadcasted_iota(jnp.int32, (block_q, block_kv), 1, layout=plgpu.Layout.WGMMA)\n+          mask = (q_ids + q_seq_base) >= (kv_ids + kv_step * block_kv)\n+          qk = jnp.where(mask, qk, -jnp.inf)\n+\n         # Softmax\n         # We keep m scaled by log2e to use FMA instructions when computing p.\n         log2e = math.log2(math.e)\n@@ -166,18 +188,35 @@ def compute_pv(acc_ref):\n           plgpu.wgmma(acc_ref, p16, v_smem.at[slot])\n \n           wait_step = kv_step + 1\n-          wait_slot = lax.rem(wait_step, max_concurrent_steps)\n-          @pl.when(wait_step < kv_seq_len // block_kv)\n+          wait_slot = lax.rem(wait_step, jnp.array(max_concurrent_steps, kv_step.dtype))\n+          @pl.when(wait_step < kv_steps)\n           def _wait():\n             plgpu.barrier_wait(k_barriers.at[wait_slot])\n         acc = pl.run_state(compute_pv)(plgpu.ACC.init(acc))\n         plgpu.barrier_arrive(v_consumed_barriers.at[slot])\n         return acc, m_i, l_i\n-      if kv_seq_len % block_kv:\n-        raise ValueError(f\"{kv_seq_len=} must be a multiple of {block_kv=}\")\n-      acc, m_i, l_i = lax.fori_loop(\n-          0, kv_seq_len // block_kv, kv_loop, (acc, m_i, l_i)\n-      )\n+\n+      if not config.causal:\n+        acc, m_i, l_i = lax.fori_loop(0, block_max_kv_steps, kv_loop, (acc, m_i, l_i))\n+      else:\n+        def epilogue_kv_loop(kv_step, _):\n+          # This loop makes sure that all the pipelined KV data is processed, even\n+          # if one compute wg finishes early like with causal masking.\n+          slot = lax.rem(kv_step, jnp.array(max_concurrent_steps, kv_step.dtype))\n+          plgpu.barrier_arrive(k_consumed_barriers.at[slot])\n+          plgpu.barrier_arrive(v_consumed_barriers.at[slot])\n+          perform_schedule_barrier()\n+          perform_schedule_barrier()\n+\n+        causal_kv_loop = functools.partial(kv_loop, causal=True)\n+        full_kv_steps = lax.div(q_seq_base, jnp.array(block_kv, jnp.int32))\n+        # With causal masking, the KV loop unrolling is split in 3 sections:\n+        # 1. A fast path where no causal mask is needed.\n+        acc, m_i, l_i = lax.fori_loop(0, full_kv_steps, kv_loop, (acc, m_i, l_i))\n+        # 2. Causal masking.\n+        acc, m_i, l_i = lax.fori_loop(full_kv_steps, kv_steps, causal_kv_loop, (acc, m_i, l_i))\n+        # 3. Epilogue to flush the data pipeline.\n+        lax.fori_loop(kv_steps, block_max_kv_steps, epilogue_kv_loop, None)\n       pl.when(wg_idx == 0)(perform_schedule_barrier)\n \n       # TODO(apaszke): Invert and multiply to avoid expensive divisions.\n@@ -208,13 +247,13 @@ def _memory_wg():\n \n       def kv_loop(kv_step, _):\n         tma_step = kv_step + max_concurrent_steps\n-        tma_slot = lax.rem(kv_step, max_concurrent_steps)\n+        tma_slot = lax.rem(kv_step, jnp.array(max_concurrent_steps, kv_step.dtype))\n         s = (batch, pl.ds(tma_step * block_kv, block_kv), kv_head)\n         plgpu.barrier_wait(k_consumed_barriers.at[tma_slot])\n         plgpu.copy_gmem_to_smem(k_ref.at[s], k_smem.at[tma_slot], k_barriers.at[tma_slot])\n         plgpu.barrier_wait(v_consumed_barriers.at[tma_slot])\n         plgpu.copy_gmem_to_smem(v_ref.at[s], v_smem.at[tma_slot], v_barriers.at[tma_slot])\n-      lax.fori_loop(0, kv_seq_len // block_kv - max_concurrent_steps, kv_loop, None)\n+      lax.fori_loop(0, block_max_kv_steps - max_concurrent_steps, kv_loop, None)\n \n   def entry(q_ref, k_ref, v_ref, out_ref, lse_ref):\n     compute_wgs = 2\n@@ -291,6 +330,9 @@ def _attention_bwd(config: TuningConfig, save_residuals: bool, res, do):\n   del save_residuals\n   q, k, v, out, lse = res\n \n+  if config.causal:\n+    raise NotImplementedError(\"Causal attention not supported in the backwards pass yet.\")\n+\n   if not config.has_backward_blocks:\n     raise ValueError(\"Need to specify backward blocks.\")\n \n@@ -586,6 +628,8 @@ def compute_dk(acc_ref):\n \n @functools.partial(jax.jit, static_argnames=[\"config\", \"save_residuals\"])\n def attention_with_pipeline_emitter(q, k, v, config: TuningConfig, save_residuals=False):\n+  if config.causal:\n+    raise NotImplementedError(\"Causal attention is not supported with the pipeline emitter yet.\")\n   if q.ndim != 4 or k.ndim != 4 or v.ndim != 4:\n     raise ValueError(f\"q, k, and v should all be 4D, got: {q.ndim=}, {k.ndim=}, {v.ndim=}\")\n   batch_size, q_seq_len, num_q_heads, head_dim = q.shape\n@@ -762,15 +806,21 @@ def run_function(q, k, v, o, lse):\n   return out\n \n \n-@functools.partial(jax.jit, static_argnames=[\"save_residuals\"])\n-def attention_reference(q, k, v, save_residuals=False):\n+@functools.partial(jax.jit, static_argnames=[\"causal\", \"save_residuals\"])\n+def attention_reference(q, k, v, causal=False, save_residuals=False):\n   batch_size, q_seq_len, num_q_heads, head_dim = q.shape\n-  num_kv_heads = k.shape[2]\n+  kv_seq_len, num_kv_heads = k.shape[1], k.shape[2]\n   q, k, v = map(lambda x: x.astype(jnp.float32), (q, k, v))\n   q_reshaped = q.reshape(\n       batch_size, q_seq_len, num_kv_heads, num_q_heads // num_kv_heads, head_dim\n   )\n   logits = jnp.einsum(\"bqHhc,bkHc->bqHhk\", q_reshaped, k)\n+\n+  if causal:\n+    mask = jnp.arange(q_seq_len)[:, None] >= jnp.arange(kv_seq_len)[None, :]\n+    mask = jnp.broadcast_to(mask[:, None, None, :], logits.shape)\n+    logits = jnp.where(mask, logits, -jnp.inf)\n+\n   m = logits.max(axis=-1, keepdims=True)\n   unnormalized = jnp.exp(logits - m)\n   l = unnormalized.sum(axis=-1, keepdims=True)\n@@ -798,11 +848,13 @@ def main(unused_argv):\n     schedule_barrier_opts = (True,)\n \n   problem_it = itertools.product(\n-      (1,), (4096, 32768,), (64, 128, 256,), schedule_barrier_opts)\n-  for batch_size, seq_len, head_dim, use_schedule_barrier in problem_it:\n+      (1,), (4096, 32768,), (64, 128, 256,), schedule_barrier_opts, (False, True))\n+  for batch_size, seq_len, head_dim, use_schedule_barrier, causal in problem_it:\n+    if causal and use_pipeline_emitter:\n+      continue\n     q_seq_len = kv_seq_len = seq_len\n     print(f\"==== {batch_size=:<6} {kv_seq_len=:<6} {q_seq_len=:<6}\"\n-          f\"{num_q_heads=:<4} {head_dim=:<6} {use_schedule_barrier=:} ====\")\n+          f\"{num_q_heads=:<4} {head_dim=:<6} {use_schedule_barrier=:} {causal=:} ====\")\n     k1, k2, k3 = jax.random.split(jax.random.key(42), 3)\n     q = jax.random.normal(k1, (batch_size, q_seq_len, num_q_heads, head_dim), jnp.float16)\n     k = jax.random.normal(k2, (batch_size, kv_seq_len, num_kv_heads, head_dim), jnp.float16)\n@@ -810,11 +862,11 @@ def main(unused_argv):\n     block_q = 64\n     best = None\n     for block_kv in (256, 128, 64):\n-      config = TuningConfig(block_q=block_q, block_kv=block_kv, max_concurrent_steps=2, use_schedule_barrier=use_schedule_barrier)\n+      config = TuningConfig(block_q=block_q, block_kv=block_kv, max_concurrent_steps=2, use_schedule_barrier=use_schedule_barrier, causal=causal)\n       try:\n         out, runtime_ms = profiler.measure(functools.partial(attention_impl, config=config))(q, k, v)\n         if seq_len < 32768:\n-          out_ref = attention_reference(q, k, v)\n+          out_ref = attention_reference(q, k, v, causal=causal)\n           np.testing.assert_allclose(out, out_ref, atol=2e-3, rtol=1e-3)\n       except ValueError as e:\n         if \"exceeds available shared memory\" in e.args[0]:\n@@ -824,6 +876,8 @@ def main(unused_argv):\n       matmul_flops = (\n           4 * q_seq_len * kv_seq_len * head_dim * num_q_heads * batch_size\n       )\n+      if causal:\n+        matmul_flops //= 2\n       peak_flops = 1e15  # f16 TensorCore peak = 1000TFLOPS\n       optimal_time = matmul_flops / peak_flops * 1e6  # us\n       achieved_tc_util = optimal_time / runtime_us * 100\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 86c3fe187b79..6690fb2dac62 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -776,7 +776,7 @@ jax_multiplatform_test(\n         \"gpu_h100\",\n     ],\n     env = {\"XLA_FLAGS\": \"--xla_gpu_autotune_level=0\"},\n-    shard_count = 4,\n+    shard_count = 8,\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_experimental_gpu_ops\",\ndiff --git a/tests/pallas/mgpu_attention_test.py b/tests/pallas/mgpu_attention_test.py\nindex 50f9a455c9a2..f86793174c16 100644\n--- a/tests/pallas/mgpu_attention_test.py\n+++ b/tests/pallas/mgpu_attention_test.py\n@@ -21,6 +21,7 @@\n from absl.testing import absltest, parameterized\n from jax._src import config\n from jax._src import test_util as jtu\n+from jax._src.lib import cuda_versions\n from jax._src.pallas import pallas_call\n import jax.numpy as jnp\n \n@@ -63,11 +64,13 @@ def setUp(self):\n           (4, 4),\n       ),  # MHA\n       head_dim=(64, 128, 256),\n+      blocks=((64, 64), (64, 128), (128, 64)),\n       attention_impl=(\n           attention_mgpu.attention,\n           attention_mgpu.attention_with_pipeline_emitter,\n       ),\n       save_residuals=(True,),\n+      causal=(True, False,),\n   )\n   def test_flash_attention(\n       self,\n@@ -76,10 +79,24 @@ def test_flash_attention(\n       kv_seq_len,\n       num_q_and_kv_heads,\n       head_dim,\n+      blocks,\n       attention_impl,\n       save_residuals,\n+      causal,\n   ):\n+    cuda_runtime_version = cuda_versions.cuda_runtime_get_version()\n+    # TODO(pobudzey): Undo when we upgrade to cuda 12.9.1.\n+    if causal and (cuda_runtime_version >= 12080 and cuda_runtime_version < 12091):\n+      self.skipTest(\"Skipping because of ptxas miscompilation.\")\n+\n+    if causal and attention_impl == attention_mgpu.attention_with_pipeline_emitter:\n+      self.skipTest(\"Pipeline emitter does not support causal attention.\")\n+\n+    if head_dim >= 256 and max(blocks) >= 128:\n+      self.skipTest(\"Head dim too large for block sizes.\")\n+\n     num_q_heads, num_kv_heads = num_q_and_kv_heads\n+    block_q, block_kv = blocks\n     k1, k2, k3 = jax.random.split(jax.random.key(42), 3)\n     q = jax.random.normal(k1, (batch_size, q_seq_len, num_q_heads, head_dim), jnp.float16)\n     k = jax.random.normal(k2, (batch_size, kv_seq_len, num_kv_heads, head_dim), jnp.float16)\n@@ -89,11 +106,12 @@ def test_flash_attention(\n         k,\n         v,\n         attention_mgpu.TuningConfig(\n-            block_q=64, block_kv=64, max_concurrent_steps=2\n+            block_q=block_q, block_kv=block_kv, max_concurrent_steps=2, causal=causal\n         ),\n         save_residuals=save_residuals,\n     )\n-    out_ref, *res_ref = attention_mgpu.attention_reference(q, k, v, save_residuals=save_residuals)\n+    out_ref, *res_ref = attention_mgpu.attention_reference(\n+        q, k, v, causal=causal, save_residuals=save_residuals)\n     np.testing.assert_allclose(out, out_ref, atol=2e-3, rtol=1e-3)\n     if save_residuals:\n       (lse,) = res[0]\n\n```"
        ],
        "from_id": [
            "Rifur13",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Bump NumPy version to 2.2.6 for 3.13+ Python versions.\n\nRemove workaround for NumPy 2.2 bug on aarch64.\n\nAlso add a linux system constraint to GPU wheels.",
        "output": [
            "```diff\nCommit: 18ff6caa4f767701dd7cca3a1333d9b99465e045\nDate: 2025-05-19T14:08:25Z\nURL: https://github.com/jax-ml/jax/commit/18ff6caa4f767701dd7cca3a1333d9b99465e045\nFiles changed: 10\nAdditions: +72, Deletions: -80\ndiff --git a/.bazelrc b/.bazelrc\nindex 79df03863b02..53676637c839 100644\n--- a/.bazelrc\n+++ b/.bazelrc\n@@ -244,10 +244,6 @@ build:ci_linux_aarch64_base --config=clang --verbose_failures=true\n build:ci_linux_aarch64_base --action_env=TF_SYSROOT=\"/dt10\"\n build:ci_linux_aarch64_base --color=yes\n \n-# Workaround for https://github.com/numpy/numpy/issues/28843\n-# TODO(phawkins): remove this after upgrading to NumPy 2.2.6.\n-build:ci_linux_aarch64_base --test_env=OMP_NUM_THREADS=8\n-\n build:ci_linux_aarch64 --config=ci_linux_aarch64_base\n build:ci_linux_aarch64 --host_crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n build:ci_linux_aarch64 --crosstool_top=\"@ml2014_clang_aarch64_config_aarch64//crosstool:toolchain\"\n@@ -383,10 +379,6 @@ build:rbe_cross_compile_base --remote_instance_name=projects/tensorflow-testing/\n build:rbe_cross_compile_linux_aarch64 --config=cross_compile_linux_aarch64\n build:rbe_cross_compile_linux_aarch64 --config=rbe_cross_compile_base\n \n-# Workaround for https://github.com/numpy/numpy/issues/28843\n-# TODO(phawkins): remove this after upgrading to NumPy 2.2.6.\n-build:rbe_cross_compile_linux_aarch64 --test_env=OMP_NUM_THREADS=8\n-\n # Mac x86\n build:cross_compile_darwin_x86_64 --config=cross_compile_base\n build:cross_compile_darwin_x86_64 --config=nonccl\ndiff --git a/build/freethreading-requirements.txt b/build/freethreading-requirements.txt\nindex cc302cffdd0c..467578870ee9 100644\n--- a/build/freethreading-requirements.txt\n+++ b/build/freethreading-requirements.txt\n@@ -1,3 +1,3 @@\n # Under free-threading, we need an up-to-date numpy at least for the moment.\n-numpy~=2.2.5; python_version==\"3.13\"\n-numpy>=2.2.5; python_version>=\"3.14\"\n+numpy~=2.2.6; python_version==\"3.13\"\n+numpy>=2.2.6; python_version>=\"3.14\"\ndiff --git a/build/nonfreethreading-requirements.txt b/build/nonfreethreading-requirements.txt\nindex f8171559a142..8bd139bf99ac 100644\n--- a/build/nonfreethreading-requirements.txt\n+++ b/build/nonfreethreading-requirements.txt\n@@ -1,6 +1,6 @@\n numpy~=2.0.0; python_version<=\"3.12\"\n numpy~=2.1.0; python_version==\"3.13\"\n-numpy>=2.2.5; python_version>=\"3.14\"\n+numpy>=2.2.6; python_version>=\"3.14\"\n \n # These packages have not released free-threaded wheels.\n zstandard\ndiff --git a/build/requirements.in b/build/requirements.in\nindex 8b8af9d6b591..c5ce2ea279bd 100644\n--- a/build/requirements.in\n+++ b/build/requirements.in\n@@ -19,8 +19,8 @@ wheel\n jaxlib\n \n # The with-cuda extra also includes NVIDIA's pip packages.\n-jax-cuda12-plugin[with-cuda]\n-jax-cuda12-pjrt\n+jax-cuda12-plugin[with-cuda] ; sys_platform == \"linux\"\n+jax-cuda12-pjrt ; sys_platform == \"linux\"\n \n # TPU dependencies\n libtpu ; sys_platform == \"linux\" and platform_machine == \"x86_64\"\ndiff --git a/build/requirements_lock_3_10.txt b/build/requirements_lock_3_10.txt\nindex c4ca6088e4bf..a4c6b1bf2b77 100644\n--- a/build/requirements_lock_3_10.txt\n+++ b/build/requirements_lock_3_10.txt\n@@ -160,13 +160,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_11.txt b/build/requirements_lock_3_11.txt\nindex 1f667115af04..0633e733414b 100644\n--- a/build/requirements_lock_3_11.txt\n+++ b/build/requirements_lock_3_11.txt\n@@ -154,13 +154,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_12.txt b/build/requirements_lock_3_12.txt\nindex 20ca67a3e921..1ab77a6ec36e 100644\n--- a/build/requirements_lock_3_12.txt\n+++ b/build/requirements_lock_3_12.txt\n@@ -154,13 +154,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_13.txt b/build/requirements_lock_3_13.txt\nindex 804373b03899..c20068b732e6 100644\n--- a/build/requirements_lock_3_13.txt\n+++ b/build/requirements_lock_3_13.txt\n@@ -181,13 +181,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\ndiff --git a/build/requirements_lock_3_13_ft.txt b/build/requirements_lock_3_13_ft.txt\nindex c7a1c882fc73..3795343df0cb 100644\n--- a/build/requirements_lock_3_13_ft.txt\n+++ b/build/requirements_lock_3_13_ft.txt\n@@ -172,13 +172,13 @@ iniconfig==2.0.0 \\\n     --hash=sha256:2d91e135bf72d31a410b17c16da610a82cb55f6b0477d1a902134b24a455b8b3 \\\n     --hash=sha256:b6a85871a79d2e3b22d2d1b94ac2824226a63c6b741c88f7ae975f18b6778374\n     # via pytest\n-jax-cuda12-pjrt==0.6.0 \\\n+jax-cuda12-pjrt==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:68371bd9c135244b89663039be208255698a75bec9854d419ea3c3f957ca4646 \\\n     --hash=sha256:9bfebb06a39614cb6899f7730ea8561f11156ac81cbb3ec6884a62afb3b15ff3\n     # via\n     #   -r build/requirements.in\n     #   jax-cuda12-plugin\n-jax-cuda12-plugin[with-cuda]==0.6.0 \\\n+jax-cuda12-plugin[with-cuda]==0.6.0 ; sys_platform == \"linux\" \\\n     --hash=sha256:0d9ecede66c40258702a42261e868cdb56a103551a7c3c884b35f531c9acd48e \\\n     --hash=sha256:28ae6cb1a09b1824d4baeb68386bc615976e89f7a65d403a93822b76dcd1e508 \\\n     --hash=sha256:530ad851ca462991ce82db26ad47f02b08cebe483c9c8d0c0037e9e27a7b529f \\\n@@ -371,62 +371,62 @@ mpmath==1.3.0 \\\n     --hash=sha256:7a28eb2a9774d00c7bc92411c19a89209d5da7c4c9a9e227be8330a23a25b91f \\\n     --hash=sha256:a0b2b9fe80bbcd81a6647ff13108738cfb482d481d826cc0e02f5b35e5c88d2c\n     # via -r build/test-requirements.txt\n-numpy==2.2.5 ; python_version == \"3.13\" \\\n-    --hash=sha256:0255732338c4fdd00996c0421884ea8a3651eea555c3a56b84892b66f696eb70 \\\n-    --hash=sha256:02f226baeefa68f7d579e213d0f3493496397d8f1cff5e2b222af274c86a552a \\\n-    --hash=sha256:059b51b658f4414fff78c6d7b1b4e18283ab5fa56d270ff212d5ba0c561846f4 \\\n-    --hash=sha256:0bcb1d057b7571334139129b7f941588f69ce7c4ed15a9d6162b2ea54ded700c \\\n-    --hash=sha256:0cd48122a6b7eab8f06404805b1bd5856200e3ed6f8a1b9a194f9d9054631beb \\\n-    --hash=sha256:19f4718c9012e3baea91a7dba661dcab2451cda2550678dc30d53acb91a7290f \\\n-    --hash=sha256:1a161c2c79ab30fe4501d5a2bbfe8b162490757cf90b7f05be8b80bc02f7bb8e \\\n-    --hash=sha256:1f4a922da1729f4c40932b2af4fe84909c7a6e167e6e99f71838ce3a29f3fe26 \\\n-    --hash=sha256:261a1ef047751bb02f29dfe337230b5882b54521ca121fc7f62668133cb119c9 \\\n-    --hash=sha256:262d23f383170f99cd9191a7c85b9a50970fe9069b2f8ab5d786eca8a675d60b \\\n-    --hash=sha256:2ba321813a00e508d5421104464510cc962a6f791aa2fca1c97b1e65027da80d \\\n-    --hash=sha256:2c1a1c6ccce4022383583a6ded7bbcda22fc635eb4eb1e0a053336425ed36dfa \\\n-    --hash=sha256:352d330048c055ea6db701130abc48a21bec690a8d38f8284e00fab256dc1376 \\\n-    --hash=sha256:369e0d4647c17c9363244f3468f2227d557a74b6781cb62ce57cf3ef5cc7c610 \\\n-    --hash=sha256:36ab5b23915887543441efd0417e6a3baa08634308894316f446027611b53bf1 \\\n-    --hash=sha256:37e32e985f03c06206582a7323ef926b4e78bdaa6915095ef08070471865b906 \\\n-    --hash=sha256:3a801fef99668f309b88640e28d261991bfad9617c27beda4a3aec4f217ea073 \\\n-    --hash=sha256:3d14b17b9be5f9c9301f43d2e2a4886a33b53f4e6fdf9ca2f4cc60aeeee76372 \\\n-    --hash=sha256:422cc684f17bc963da5f59a31530b3936f57c95a29743056ef7a7903a5dbdf88 \\\n-    --hash=sha256:4520caa3807c1ceb005d125a75e715567806fed67e315cea619d5ec6e75a4191 \\\n-    --hash=sha256:47834cde750d3c9f4e52c6ca28a7361859fcaf52695c7dc3cc1a720b8922683e \\\n-    --hash=sha256:47f9ed103af0bc63182609044b0490747e03bd20a67e391192dde119bf43d52f \\\n-    --hash=sha256:498815b96f67dc347e03b719ef49c772589fb74b8ee9ea2c37feae915ad6ebda \\\n-    --hash=sha256:54088a5a147ab71a8e7fdfd8c3601972751ded0739c6b696ad9cb0343e21ab73 \\\n-    --hash=sha256:55f09e00d4dccd76b179c0f18a44f041e5332fd0e022886ba1c0bbf3ea4a18d0 \\\n-    --hash=sha256:5a0ac90e46fdb5649ab6369d1ab6104bfe5854ab19b645bf5cda0127a13034ae \\\n-    --hash=sha256:6411f744f7f20081b1b4e7112e0f4c9c5b08f94b9f086e6f0adf3645f85d3a4d \\\n-    --hash=sha256:6413d48a9be53e183eb06495d8e3b006ef8f87c324af68241bbe7a39e8ff54c3 \\\n-    --hash=sha256:7451f92eddf8503c9b8aa4fe6aa7e87fd51a29c2cfc5f7dbd72efde6c65acf57 \\\n-    --hash=sha256:8b4c0773b6ada798f51f0f8e30c054d32304ccc6e9c5d93d46cb26f3d385ab19 \\\n-    --hash=sha256:8dfa94b6a4374e7851bbb6f35e6ded2120b752b063e6acdd3157e4d2bb922eba \\\n-    --hash=sha256:97c8425d4e26437e65e1d189d22dff4a079b747ff9c2788057bfb8114ce1e133 \\\n-    --hash=sha256:9d75f338f5f79ee23548b03d801d28a505198297534f62416391857ea0479571 \\\n-    --hash=sha256:9de6832228f617c9ef45d948ec1cd8949c482238d68b2477e6f642c33a7b0a54 \\\n-    --hash=sha256:a4cbdef3ddf777423060c6f81b5694bad2dc9675f110c4b2a60dc0181543fac7 \\\n-    --hash=sha256:a9c0d994680cd991b1cb772e8b297340085466a6fe964bc9d4e80f5e2f43c291 \\\n-    --hash=sha256:aa70fdbdc3b169d69e8c59e65c07a1c9351ceb438e627f0fdcd471015cd956be \\\n-    --hash=sha256:abe38cd8381245a7f49967a6010e77dbf3680bd3627c0fe4362dd693b404c7f8 \\\n-    --hash=sha256:b13f04968b46ad705f7c8a80122a42ae8f620536ea38cf4bdd374302926424dd \\\n-    --hash=sha256:b4ea7e1cff6784e58fe281ce7e7f05036b3e1c89c6f922a6bfbc0a7e8768adbe \\\n-    --hash=sha256:b6f91524d31b34f4a5fee24f5bc16dcd1491b668798b6d85585d836c1e633a6a \\\n-    --hash=sha256:c26843fd58f65da9491165072da2cccc372530681de481ef670dcc8e27cfb066 \\\n-    --hash=sha256:c42365005c7a6c42436a54d28c43fe0e01ca11eb2ac3cefe796c25a5f98e5e9b \\\n-    --hash=sha256:c8b82a55ef86a2d8e81b63da85e55f5537d2157165be1cb2ce7cfa57b6aef38b \\\n-    --hash=sha256:ced69262a8278547e63409b2653b372bf4baff0870c57efa76c5703fd6543282 \\\n-    --hash=sha256:d2e3bdadaba0e040d1e7ab39db73e0afe2c74ae277f5614dad53eadbecbbb169 \\\n-    --hash=sha256:d403c84991b5ad291d3809bace5e85f4bbf44a04bdc9a88ed2bb1807b3360bb8 \\\n-    --hash=sha256:d7543263084a85fbc09c704b515395398d31d6395518446237eac219eab9e55e \\\n-    --hash=sha256:d8882a829fd779f0f43998e931c466802a77ca1ee0fe25a3abe50278616b1471 \\\n-    --hash=sha256:e4f0b035d9d0ed519c813ee23e0a733db81ec37d2e9503afbb6e54ccfdee0fa7 \\\n-    --hash=sha256:e8b025c351b9f0e8b5436cf28a07fa4ac0204d67b38f01433ac7f9b870fa38c6 \\\n-    --hash=sha256:eb7fd5b184e5d277afa9ec0ad5e4eb562ecff541e7f60e69ee69c8d59e9aeaba \\\n-    --hash=sha256:ec31367fd6a255dc8de4772bd1658c3e926d8e860a0b6e922b615e532d320ddc \\\n-    --hash=sha256:ee461a4eaab4f165b68780a6a1af95fb23a29932be7569b9fab666c407969051 \\\n-    --hash=sha256:f5045039100ed58fa817a6227a356240ea1b9a1bc141018864c306c1a16d4175\n+numpy==2.2.6 ; python_version == \"3.13\" \\\n+    --hash=sha256:038613e9fb8c72b0a41f025a7e4c3f0b7a1b5d768ece4796b674c8f3fe13efff \\\n+    --hash=sha256:0678000bb9ac1475cd454c6b8c799206af8107e310843532b04d49649c717a47 \\\n+    --hash=sha256:0811bb762109d9708cca4d0b13c4f67146e3c3b7cf8d34018c722adb2d957c84 \\\n+    --hash=sha256:0b605b275d7bd0c640cad4e5d30fa701a8d59302e127e5f79138ad62762c3e3d \\\n+    --hash=sha256:0bca768cd85ae743b2affdc762d617eddf3bcf8724435498a1e80132d04879e6 \\\n+    --hash=sha256:1bc23a79bfabc5d056d106f9befb8d50c31ced2fbc70eedb8155aec74a45798f \\\n+    --hash=sha256:287cc3162b6f01463ccd86be154f284d0893d2b3ed7292439ea97eafa8170e0b \\\n+    --hash=sha256:37c0ca431f82cd5fa716eca9506aefcabc247fb27ba69c5062a6d3ade8cf8f49 \\\n+    --hash=sha256:37e990a01ae6ec7fe7fa1c26c55ecb672dd98b19c3d0e1d1f326fa13cb38d163 \\\n+    --hash=sha256:389d771b1623ec92636b0786bc4ae56abafad4a4c513d36a55dce14bd9ce8571 \\\n+    --hash=sha256:3d70692235e759f260c3d837193090014aebdf026dfd167834bcba43e30c2a42 \\\n+    --hash=sha256:41c5a21f4a04fa86436124d388f6ed60a9343a6f767fced1a8a71c3fbca038ff \\\n+    --hash=sha256:481b49095335f8eed42e39e8041327c05b0f6f4780488f61286ed3c01368d491 \\\n+    --hash=sha256:4eeaae00d789f66c7a25ac5f34b71a7035bb474e679f410e5e1a94deb24cf2d4 \\\n+    --hash=sha256:55a4d33fa519660d69614a9fad433be87e5252f4b03850642f88993f7b2ca566 \\\n+    --hash=sha256:5a6429d4be8ca66d889b7cf70f536a397dc45ba6faeb5f8c5427935d9592e9cf \\\n+    --hash=sha256:5bd4fc3ac8926b3819797a7c0e2631eb889b4118a9898c84f585a54d475b7e40 \\\n+    --hash=sha256:5beb72339d9d4fa36522fc63802f469b13cdbe4fdab4a288f0c441b74272ebfd \\\n+    --hash=sha256:6031dd6dfecc0cf9f668681a37648373bddd6421fff6c66ec1624eed0180ee06 \\\n+    --hash=sha256:71594f7c51a18e728451bb50cc60a3ce4e6538822731b2933209a1f3614e9282 \\\n+    --hash=sha256:74d4531beb257d2c3f4b261bfb0fc09e0f9ebb8842d82a7b4209415896adc680 \\\n+    --hash=sha256:7befc596a7dc9da8a337f79802ee8adb30a552a94f792b9c9d18c840055907db \\\n+    --hash=sha256:894b3a42502226a1cac872f840030665f33326fc3dac8e57c607905773cdcde3 \\\n+    --hash=sha256:8e41fd67c52b86603a91c1a505ebaef50b3314de0213461c7a6e99c9a3beff90 \\\n+    --hash=sha256:8e9ace4a37db23421249ed236fdcdd457d671e25146786dfc96835cd951aa7c1 \\\n+    --hash=sha256:8fc377d995680230e83241d8a96def29f204b5782f371c532579b4f20607a289 \\\n+    --hash=sha256:9551a499bf125c1d4f9e250377c1ee2eddd02e01eac6644c080162c0c51778ab \\\n+    --hash=sha256:b0544343a702fa80c95ad5d3d608ea3599dd54d4632df855e4c8d24eb6ecfa1c \\\n+    --hash=sha256:b093dd74e50a8cba3e873868d9e93a85b78e0daf2e98c6797566ad8044e8363d \\\n+    --hash=sha256:b412caa66f72040e6d268491a59f2c43bf03eb6c96dd8f0307829feb7fa2b6fb \\\n+    --hash=sha256:b4f13750ce79751586ae2eb824ba7e1e8dba64784086c98cdbbcc6a42112ce0d \\\n+    --hash=sha256:b64d8d4d17135e00c8e346e0a738deb17e754230d7e0810ac5012750bbd85a5a \\\n+    --hash=sha256:ba10f8411898fc418a521833e014a77d3ca01c15b0c6cdcce6a0d2897e6dbbdf \\\n+    --hash=sha256:bd48227a919f1bafbdda0583705e547892342c26fb127219d60a5c36882609d1 \\\n+    --hash=sha256:c1f9540be57940698ed329904db803cf7a402f3fc200bfe599334c9bd84a40b2 \\\n+    --hash=sha256:c820a93b0255bc360f53eca31a0e676fd1101f673dda8da93454a12e23fc5f7a \\\n+    --hash=sha256:ce47521a4754c8f4593837384bd3424880629f718d87c5d44f8ed763edd63543 \\\n+    --hash=sha256:d042d24c90c41b54fd506da306759e06e568864df8ec17ccc17e9e884634fd00 \\\n+    --hash=sha256:de749064336d37e340f640b05f24e9e3dd678c57318c7289d222a8a2f543e90c \\\n+    --hash=sha256:e1dda9c7e08dc141e0247a5b8f49cf05984955246a327d4c48bda16821947b2f \\\n+    --hash=sha256:e29554e2bef54a90aa5cc07da6ce955accb83f21ab5de01a62c8478897b264fd \\\n+    --hash=sha256:e3143e4451880bed956e706a3220b4e5cf6172ef05fcc397f6f36a550b1dd868 \\\n+    --hash=sha256:e8213002e427c69c45a52bbd94163084025f533a55a59d6f9c5b820774ef3303 \\\n+    --hash=sha256:efd28d4e9cd7d7a8d39074a4d44c63eda73401580c5c76acda2ce969e0a38e83 \\\n+    --hash=sha256:f0fd6321b839904e15c46e0d257fdd101dd7f530fe03fd6359c1ea63738703f3 \\\n+    --hash=sha256:f1372f041402e37e5e633e586f62aa53de2eac8d98cbfb822806ce4bbefcb74d \\\n+    --hash=sha256:f2618db89be1b4e05f7a1a847a9c1c0abd63e63a1607d892dd54668dd92faf87 \\\n+    --hash=sha256:f447e6acb680fd307f40d3da4852208af94afdfab89cf850986c3ca00562f4fa \\\n+    --hash=sha256:f92729c95468a2f4f15e9bb94c432a9229d0d50de67304399627a943201baa2f \\\n+    --hash=sha256:f9f1adb22318e121c5c69a09142811a201ef17ab257a1e66ca3025065b7f53ae \\\n+    --hash=sha256:fc0c5673685c508a142ca65209b4e79ed6740a4ed6b2267dbba90f34b0b3cfda \\\n+    --hash=sha256:fc7b73d02efb0e18c000e9ad8b83480dfcd5dfd11065997ed4c6747470ae8915 \\\n+    --hash=sha256:fd83c01228a688733f1ded5201c678f0c53ecc1006ffbc404db9f7a899ac6249 \\\n+    --hash=sha256:fe27749d33bb772c80dcd84ae7e8df2adc920ae8297400dabec45f0dedb3f6de \\\n+    --hash=sha256:fee4236c876c4e8369388054d02d0e9bb84821feb1a64dd59e137e6511a551f8\n     # via\n     #   -r build/freethreading-requirements.txt\n     #   contourpy\ndiff --git a/build/requirements_lock_3_14.txt b/build/requirements_lock_3_14.txt\nindex 6edcd30ebe16..157dca5adbab 100644\n--- a/build/requirements_lock_3_14.txt\n+++ b/build/requirements_lock_3_14.txt\n@@ -48,7 +48,7 @@ ml-dtypes==0.5.1\n     #   tensorstore\n mpmath==1.4.0a4\n     # via -r build/test-requirements.txt\n-numpy==2.2.5\n+numpy==2.2.6\n     # via\n     #   -r build/nonfreethreading-requirements.txt\n     #   contourpy\n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Disable cusolver version check.\n\nThis appears to be failing for some reason, and it's safest just to disable it for now.",
        "output": [
            "```diff\nCommit: d62c10f1b5075b3aa5b3327c71ed43d133fc975c\nDate: 2025-05-19T13:44:11Z\nURL: https://github.com/jax-ml/jax/commit/d62c10f1b5075b3aa5b3327c71ed43d133fc975c\nFiles changed: 1\nAdditions: +8, Deletions: -5\ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 9df7fc69ff1a..02bcbcf16dbc 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -180,11 +180,14 @@ def _version_check(name: str,\n                  cuda_versions.cufft_build_version,\n                  # Ignore patch versions.\n                  scale_for_comparison=100)\n-  _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n-                 cuda_versions.cusolver_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=11400)\n+  # TODO(phawkins): for some reason this check fails with a cusolver internal\n+  # error when fetching the version. This may be a path error from our stubs.\n+  # Figure out what's happening here and reenable.\n+  # _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n+  #                cuda_versions.cusolver_build_version,\n+  #                # Ignore patch versions.\n+  #                scale_for_comparison=100,\n+  #                min_supported_version=11400)\n   _version_check(\"cuPTI\", cuda_versions.cupti_get_version,\n                  cuda_versions.cupti_build_version,\n                  min_supported_version=18)\n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Add version guards on unreduced lowering to shardy\n\nPiperOrigin-RevId: 760603883",
        "output": [
            "```diff\nCommit: 169ad4ae521872a85d5c5a93102f3c31663dd497\nDate: 2025-05-19T13:41:14Z\nURL: https://github.com/jax-ml/jax/commit/169ad4ae521872a85d5c5a93102f3c31663dd497\nFiles changed: 2\nAdditions: +13, Deletions: -6\ndiff --git a/jax/_src/named_sharding.py b/jax/_src/named_sharding.py\nindex faf0b2a9f2b2..ae99236a6cdc 100644\n--- a/jax/_src/named_sharding.py\n+++ b/jax/_src/named_sharding.py\n@@ -23,6 +23,7 @@\n from jax._src import config\n from jax._src.util import use_cpp_class, cache, use_cpp_method\n from jax._src.lib import xla_client as xc\n+from jax._src.lib import jaxlib_extension_version\n from jax._src.lib.mlir.dialects import sdy\n from jax._src import mesh as mesh_lib\n from jax._src.mesh import AxisType\n@@ -316,11 +317,17 @@ def build(self) -> sdy.TensorShardingAttr:\n \n     replicated_axes = _get_axes(self.replicated_axes, self.mesh_shape)\n     unreduced_axes = _get_axes(self.unreduced_axes, self.mesh_shape)\n-    return sdy.TensorShardingAttr.get(\n-        mesh_attr,\n-        [dim_sharding.build() for dim_sharding in self.dim_shardings],\n-        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes],\n-        unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in unreduced_axes])\n+    if jaxlib_extension_version >= 342:\n+      return sdy.TensorShardingAttr.get(\n+          mesh_attr,\n+          [dim_sharding.build() for dim_sharding in self.dim_shardings],\n+          replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes],\n+          unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in unreduced_axes])\n+    else:\n+      return sdy.TensorShardingAttr.get(\n+          mesh_attr,\n+          [dim_sharding.build() for dim_sharding in self.dim_shardings],\n+          replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes])\n \n   def __repr__(self):\n     dim_sharding_repr = ', '.join(\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 69e168de9c2d..8f8c829ee6c7 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 341\n+_version = 342\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] A handful of minor bug fixes\n\nPiperOrigin-RevId: 760601235",
        "output": [
            "```diff\nCommit: b2418681d7b55dc62bbc7a1646ea3d7b1d1da2ee\nDate: 2025-05-19T13:32:26Z\nURL: https://github.com/jax-ml/jax/commit/b2418681d7b55dc62bbc7a1646ea3d7b1d1da2ee\nFiles changed: 3\nAdditions: +14, Deletions: -4\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 3d43e2c0ab61..d13977ac4fbf 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -627,7 +627,7 @@ def remote_ref(\n ) -> pallas_core.TransformedRef:\n   \"\"\"Translate memref to a symmetric memref on a peer device.\"\"\"\n   if not isinstance(ref, pallas_core.TransformedRef):\n-    if not isinstance(jax_core.get_aval(ref), pallas_core.AbstractMemoryRef):\n+    if not isinstance(jax_core.get_aval(ref), state_types.AbstractRef):\n       raise TypeError(\"ref must be a reference\")\n     ref = pallas_core.TransformedRef(ref, transforms=())\n   return pallas_core.TransformedRef(\n@@ -640,7 +640,7 @@ def transform_ref(\n     transform: state_types.Transform\n ) -> pallas_core.TransformedRef:\n   if not isinstance(ref, pallas_core.TransformedRef):\n-    if not isinstance(jax_core.get_aval(ref), pallas_core.AbstractMemoryRef):\n+    if not isinstance(jax_core.get_aval(ref), state_types.AbstractRef):\n       raise TypeError(\"ref must be a reference\")\n     ref = pallas_core.TransformedRef(ref, transforms=())\n   return pallas_core.TransformedRef(\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex acbcf7c2cda1..62b43b4de737 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -1993,7 +1993,7 @@ def _store_untiled_wg_strided(self, ref: ir.Value):\n       idxs = ([i] for i in self.layout.linear_thread_idxs())\n     except NotImplementedError:\n       ref_ = ref\n-      idxs = self.layout.thread_idxs()\n+      idxs = self.layout.thread_idxs(self.shape)\n     ref_shape = tuple(ref_ty.shape)\n     if ref_shape != self.shape:\n       raise ValueError((ref_shape, self.shape))\ndiff --git a/jax/experimental/mosaic/gpu/launch_context.py b/jax/experimental/mosaic/gpu/launch_context.py\nindex eccb363f7537..175dc8b0ac74 100644\n--- a/jax/experimental/mosaic/gpu/launch_context.py\n+++ b/jax/experimental/mosaic/gpu/launch_context.py\n@@ -878,8 +878,18 @@ def _ensure_nvshmem_decls(self):\n   def to_remote(self, ref: ir.Value, peer: ir.Value):\n     self._ensure_nvshmem_decls()\n     if ir.MemRefType.isinstance(ref.type):\n+      # We replace the offset in the ref type by 0, because memref_ptr always\n+      # folds the offset into the pointer.\n+      ref_ty = ir.MemRefType(ref.type)\n+      strides, _ = ref_ty.get_strides_and_offset()\n+      result_type = ir.MemRefType.get(\n+          ref_ty.shape,\n+          ref_ty.element_type,\n+          ir.StridedLayoutAttr.get(0, strides),\n+          ref_ty.memory_space,\n+      )\n       return utils.ptr_as_memref(\n-          self.to_remote(utils.memref_ptr(ref), peer), ref.type\n+          self.to_remote(utils.memref_ptr(ref), peer), result_type\n       )\n     if ref.type != ir.Type.parse(\"!llvm.ptr\"):\n       raise ValueError(f\"Unsupported type for to_remote: {ref.type}\")\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add missing test skips and improve compatibility with older jaxlib versions\n\nPiperOrigin-RevId: 760589385",
        "output": [
            "```diff\nCommit: 168f771c93cc8773fdc53e924defe8bae2b07c6d\nDate: 2025-05-19T12:53:20Z\nURL: https://github.com/jax-ml/jax/commit/168f771c93cc8773fdc53e924defe8bae2b07c6d\nFiles changed: 3\nAdditions: +10, Deletions: -3\ndiff --git a/jax/_src/distributed.py b/jax/_src/distributed.py\nindex dad445b8e539..ae1baf8052c0 100644\n--- a/jax/_src/distributed.py\n+++ b/jax/_src/distributed.py\n@@ -159,7 +159,9 @@ def shutdown(self):\n     if self.preemption_sync_manager:\n       # It's important to shut down the preemption sync manager before the\n       # client because the preemption sync manager depends on the client.\n-      self.preemption_sync_manager.shutdown()\n+      # TODO: Delete hasattr check once 0.6.1 is the minimum jaxlib version\n+      if hasattr(self.preemption_sync_manager, \"shutdown\"):\n+        self.preemption_sync_manager.shutdown()\n       self.preemption_sync_manager = None\n     if self.client:\n       self.client.shutdown()\ndiff --git a/tests/fused_attention_stablehlo_test.py b/tests/fused_attention_stablehlo_test.py\nindex 64e0f4377462..925fc2ed4825 100644\n--- a/tests/fused_attention_stablehlo_test.py\n+++ b/tests/fused_attention_stablehlo_test.py\n@@ -503,6 +503,9 @@ def test_sdpa_broadcast_bias_and_dbias(self):\n   )\n   @jtu.run_on_devices(\"cuda\")\n   def test_sdpa_dbias(self, batch_size: int):\n+    # TODO: Delete once 0.6.0 is no longer supported.\n+    if jtu.jaxlib_version() == (0, 6, 0):\n+      self.skipTest(\"jaxlib 0.6.0 has a bug\")\n     if jax.device_count() < 4:\n       self.skipTest(\"Requires more than 4 devices.\")\n     # cuDNN only supports dbias when batch size is 1. If the batch size is\ndiff --git a/tests/multiprocess_gpu_test.py b/tests/multiprocess_gpu_test.py\nindex 20a2b9ba972b..c2ec44916745 100644\n--- a/tests/multiprocess_gpu_test.py\n+++ b/tests/multiprocess_gpu_test.py\n@@ -82,9 +82,11 @@ def test_gpu_distributed_initialize(self):\n \n       try:\n         for proc in subprocesses:\n-          out, _ = proc.communicate()\n+          out, err = proc.communicate()\n           self.assertEqual(proc.returncode, 0)\n-          self.assertEqual(out, f'{num_gpus_per_task},{num_gpus}')\n+          self.assertEqual(\n+              out, f\"{num_gpus_per_task},{num_gpus}\", msg=f\"Process failed:\\n\\n{err}\",\n+          )\n       finally:\n         for proc in subprocesses:\n           proc.kill()\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "#sdy fix `shard_map` lowering if there is only one device.\n\nThis was wrong when the shmap contains callbacks, which cause tokens to be created. I was calling `jaxpr_subcomp` incorrectly.\n\nPiperOrigin-RevId: 760588882",
        "output": [
            "```diff\nCommit: 0f5e952975a15435a4e71395c45f4f3998ab79d9\nDate: 2025-05-19T12:51:20Z\nURL: https://github.com/jax-ml/jax/commit/0f5e952975a15435a4e71395c45f4f3998ab79d9\nFiles changed: 2\nAdditions: +14, Deletions: -9\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex ac529a667d04..010773f74d0c 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -812,17 +812,13 @@ def _shard_map_lowering_shardy(\n   if np.prod([mesh.shape[a] for a in manual_axes]) == 1:\n     # No need for a `ManualComputationOp` if all manual axes are size 1.\n     with _extend_axis_env(mesh, manual_axes), config._check_vma(check_vma):\n-      args = (*ctx.dim_var_values, *tokens, *in_nodes)\n       out_nodes, tokens_out = mlir.jaxpr_subcomp(\n           sub_ctx, jaxpr, ctx.name_stack,\n-          mlir.TokenSet(zip(ctx.tokens_in.effects(), in_nodes[:num_tokens])),\n-        (), *args[num_tokens:],\n+          mlir.TokenSet(zip(ctx.tokens_in.effects(), tokens)),\n+          (), *in_nodes,\n           dim_var_values=ctx.dim_var_values)\n-      num_tokens = len(tokens_out.effects())\n-      tokens_out = tokens_out.update_tokens(mlir.TokenSet(zip(\n-          ctx.tokens_in.effects(), out_nodes[:num_tokens])))\n       ctx.set_tokens_out(tokens_out)\n-    return out_nodes[num_tokens:]\n+    return out_nodes\n \n   in_shardings = list(map(\n       partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\ndiff --git a/tests/python_callback_test.py b/tests/python_callback_test.py\nindex 9a3b26530044..26664faa6faf 100644\n--- a/tests/python_callback_test.py\n+++ b/tests/python_callback_test.py\n@@ -1363,9 +1363,18 @@ def f_base(i, x):\n     jax.effects_barrier()\n     self.assertEqual(_collected, expected)\n \n-  def test_can_shard_io_callback_manually(self):\n+  @parameterized.named_parameters(\n+    dict(testcase_name='multi_device',\n+         single_device=False),\n+    dict(testcase_name='single_device',\n+         single_device=True)\n+  )\n+  def test_can_shard_io_callback_manually(self, single_device: bool):\n \n-    mesh = Mesh(np.array(jax.devices()), axis_names=('x',))\n+    devices = jax.devices()\n+    if single_device:\n+      devices = devices[:1]\n+    mesh = Mesh(np.array(devices), axis_names=('x',))\n \n     spec = jax.sharding.PartitionSpec('x')\n     sharding = jax.sharding.NamedSharding(mesh, spec)\n\n```"
        ],
        "from_id": [
            "bartchr808",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Ignore singleton tiling dims when constructing nested shape\n\nThey don't contribute anything, but they can trigger some NotImplemented errors\ndown the line.\n\nPiperOrigin-RevId: 760579238",
        "output": [
            "```diff\nCommit: 8f7f3e10be5755b2b3296830ba024dbb381462d0\nDate: 2025-05-19T12:15:10Z\nURL: https://github.com/jax-ml/jax/commit/8f7f3e10be5755b2b3296830ba024dbb381462d0\nFiles changed: 2\nAdditions: +21, Deletions: -0\ndiff --git a/jax/experimental/mosaic/gpu/fragmented_array.py b/jax/experimental/mosaic/gpu/fragmented_array.py\nindex 62c5903f475f..acbcf7c2cda1 100644\n--- a/jax/experimental/mosaic/gpu/fragmented_array.py\n+++ b/jax/experimental/mosaic/gpu/fragmented_array.py\n@@ -2166,10 +2166,12 @@ def transfer_tiled2(\n       raise ValueError()\n     nested_ref_shape = tuple(\n         (ref_ty.shape[i], ref_ty.shape[i + ref_logical_rank])\n+        if ref_ty.shape[i + ref_logical_rank] != 1 else (ref_ty.shape[i],)\n         for i in range(ref_logical_rank)\n     )\n     nested_ref_strides = tuple(\n         (ref_strides[i], ref_strides[i + ref_logical_rank])\n+        if ref_ty.shape[i + ref_logical_rank] != 1 else (ref_strides[i],)\n         for i in range(ref_logical_rank)\n     )\n     tiled_nested_shape, tiled_nested_strides = tiling.tile_nested_shape_strides(\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 08b424731f19..39bd8aa77331 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -1469,6 +1469,25 @@ def kernel(ctx, src, dst, smem):\n     y = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), x, x, smem)(x)\n     np.testing.assert_array_equal(y, x)\n \n+  def test_tma_with_1d_tiling(self):\n+    swizzle = 128\n+    dtype = jnp.float16\n+    shape = (64, 128)\n+    tiling = (1, swizzle // jnp.dtype(dtype).itemsize)\n+    def kernel(ctx, dst, smem):\n+      iota_tensor(*shape, dtype=dtype).store_tiled(smem, swizzle=swizzle)\n+      ctx.async_copy(\n+          src_ref=smem,\n+          dst_ref=dst,\n+          swizzle=swizzle,\n+          gmem_transform=mgpu.TileTransform(tiling),\n+      )\n+      ctx.await_async_copy(0)\n+    x = np.arange(np.prod(shape), dtype=dtype).reshape(shape)\n+    smem = jax.ShapeDtypeStruct(utils.tile_shape(shape, tiling), dtype)\n+    y = mgpu.as_gpu_kernel(kernel, (1, 1, 1), (128, 1, 1), (), x, smem)()\n+    np.testing.assert_array_equal(y, x)\n+\n   @parameterized.named_parameters(\n       (\n           f\"_{''.join(map(str, collective_dims))}={collective_size}{'_' + ''.join(map(str, noncollective_dims)) if noncollective_dims else ''}\",\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add support for fp8 types in WGMMA\n\nPiperOrigin-RevId: 760551961",
        "output": [
            "```diff\nCommit: 097e755b22400f1d6ea633610b70e03d311d2e09\nDate: 2025-05-19T10:38:52Z\nURL: https://github.com/jax-ml/jax/commit/097e755b22400f1d6ea633610b70e03d311d2e09\nFiles changed: 3\nAdditions: +40, Deletions: -13\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex 89a58e4788f5..c4f670527a0c 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -195,7 +195,7 @@ def mma(\n           f\" type f32 or f16, but got: {d.dtype}\"\n       )\n   else:\n-    raise NotImplementedError(f\"Unsupported element type: {element_type}\", type(element_type))\n+    raise NotImplementedError(f\"Unsupported element type: {element_type}\")\n \n   # Step 2. Decide on the instruction shapes we'll use. Note that with swizzles,\n   # instructions must be issued in groups of the same width as the swizzle.\ndiff --git a/jax/experimental/mosaic/gpu/wgmma.py b/jax/experimental/mosaic/gpu/wgmma.py\nindex 8baa16d8a7e9..3637778c371b 100644\n--- a/jax/experimental/mosaic/gpu/wgmma.py\n+++ b/jax/experimental/mosaic/gpu/wgmma.py\n@@ -85,10 +85,11 @@ def tree_unflatten(cls, aux, value):\n \n def _supported_wgmma_types(dtype, abtype) -> bool:\n   input_types_are = lambda ty: ty.isinstance(abtype)\n+  f16_acc_types = (ir.F16Type, ir.Float8E5M2Type, ir.Float8E4M3FNType)\n   if ir.F32Type.isinstance(dtype):\n-    return any(input_types_are(ty) for ty in (ir.FloatTF32Type, ir.BF16Type, ir.F16Type))\n+    return any(input_types_are(ty) for ty in (ir.FloatTF32Type, ir.BF16Type, *f16_acc_types))\n   elif ir.F16Type.isinstance(dtype):\n-    return input_types_are(ir.F16Type)\n+    return any(input_types_are(ty) for ty in f16_acc_types)\n   else:\n     return False\n \n@@ -187,8 +188,12 @@ def take_regs(n):\n   b_desc_reg, use_out_reg = take_regs(2)\n   imm_regs = \", \".join(take_regs(num_imm_regs))  # Immediate regs (scale, ...).\n   assert next(reg_count) == len(reg_constraints_list)\n-  el_ty = element_type\n   k_instr = 32 // bytewidth(element_type)\n+  el_ty = str(element_type)\n+  if ir.Float8E5M2Type.isinstance(element_type):\n+    el_ty = \"e5m2\"\n+  elif ir.Float8E4M3FNType.isinstance(element_type):\n+    el_ty = \"e4m3\"\n   wgmma_instr = (\n       f\"wgmma.mma_async.sync.aligned.m64n{n}k{k_instr}.{out_ty}.{el_ty}.{el_ty} \"\n       f\"{acc_reg_vector}, {a_regs}, {b_desc_reg}, p, {imm_regs};\"\n@@ -291,18 +296,24 @@ def wgmma(\n         f\"Accumulator shape mismatch: expected {(m, n)}, got {acc.value.shape}\"\n     )\n   f32 = ir.F32Type.get()\n+  f16 = ir.F16Type.get()\n   if element_type == f32 or element_type == ir.BF16Type.get():\n     if acc.value.mlir_dtype != f32:\n       raise ValueError(\n           f\"WGMMA with element type {element_type} only supports accumulators\"\n           f\" of type f32, but got: {acc.value.mlir_dtype}\"\n       )\n-  elif element_type == ir.F16Type.get():\n-    if acc.value.mlir_dtype != element_type and acc.value.mlir_dtype != f32:\n+  elif any(\n+      t.isinstance(element_type)\n+      for t in {ir.F16Type, ir.Float8E5M2Type, ir.Float8E4M3FNType}\n+  ):\n+    if acc.value.mlir_dtype != f16 and acc.value.mlir_dtype != f32:\n       raise ValueError(\n-          \"WGMMA with element type f16 only supports accumulators of type f32\"\n-          f\" or f16, but got: {acc.value.mlir_dtype}\"\n+          f\"WGMMA with element type {element_type} only supports accumulators \"\n+          f\"of type f32 or f16, but got: {acc.value.mlir_dtype}\"\n       )\n+  else:\n+    raise NotImplementedError(f\"Unsupported element type: {element_type}\")\n \n   # Step 2. Decide on the instruction shapes we'll use. Note that with swizzles,\n   # instructions must be issued in groups of the same width as the swizzle.\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 62f377f031a0..08b424731f19 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -653,7 +653,13 @@ def setUp(self):\n   @parameterized.product(\n       lhs_transpose=(False, True),\n       rhs_transpose=(False, True),\n-      in_mlir_dtype_cls=(ir.F16Type, ir.BF16Type, ir.F32Type),\n+      in_mlir_dtype_cls=(\n+          ir.F16Type,\n+          ir.BF16Type,\n+          ir.F32Type,\n+          ir.Float8E5M2Type,\n+          ir.Float8E4M3FNType,\n+      ),\n       m=(64, 128, 192),\n       n=(64, 128, 192),\n       k_steps=(1, 2),\n@@ -675,8 +681,8 @@ def test_wgmma_basic(\n       rhs_tiling_kind,\n       lhs_tiling_kind,\n   ):\n-    if jax_out_dtype == jnp.float16 and in_mlir_dtype_cls is not ir.F16Type:\n-      self.skipTest(\"Only f16 input is supported for f16 output.\")\n+    if jax_out_dtype == jnp.float16 and in_mlir_dtype_cls in {ir.F32Type, ir.BF16Type}:\n+      self.skipTest(f\"{in_mlir_dtype_cls.get()} does not support f16 output.\")\n     if swizzle != 128 and lhs_transpose and lhs_tiling_kind == \"large\":\n       self.skipTest(\"Transpose only supported in 128B swizzled WGMMA\")\n     if rhs_tiling_kind == \"small+no_transpose\" and not rhs_transpose:\n@@ -686,10 +692,10 @@ def test_wgmma_basic(\n \n     in_mlir_dtype = in_mlir_dtype_cls.get()\n     out_mlir_dtype = utils.dtype_to_ir_type(jax_out_dtype)\n+    if (lhs_transpose or not rhs_transpose) and bytewidth(in_mlir_dtype) != 2:\n+      self.skipTest(\"Transpose only supported in 16-bit WGMMA\")\n     if ir.F32Type.isinstance(in_mlir_dtype):  # We actually use tf32 instead\n       in_jax_dtype = jnp.float32\n-      if lhs_transpose or not rhs_transpose:\n-        self.skipTest(\"Transpose only supported in 16-bit WGMMA\")\n       exponent_bits, mantissa_bits = 8, 10  # Use tf32\n     elif bytewidth(in_mlir_dtype) == 2:\n       if n % 64 != 0:\n@@ -702,10 +708,18 @@ def test_wgmma_basic(\n         exponent_bits, mantissa_bits = 8, 7\n       else:\n         raise NotImplementedError(in_mlir_dtype)\n+    elif in_mlir_dtype_cls == ir.Float8E5M2Type:\n+      in_jax_dtype = jnp.float8_e5m2\n+      exponent_bits, mantissa_bits = 5, 2\n+    elif in_mlir_dtype_cls == ir.Float8E4M3FNType:\n+      in_jax_dtype = jnp.float8_e4m3fn\n+      exponent_bits, mantissa_bits = 4, 3\n     else:\n       raise NotImplementedError(in_mlir_dtype)\n     nk_tile = swizzle // bytewidth(in_mlir_dtype)\n     k = nk_tile * k_steps\n+    if n % nk_tile:\n+      self.skipTest(\"tiling does not divide N\")\n     assert m % 64 == 0 and n % nk_tile == 0\n \n     small_rhs_tile = rhs_tiling_kind != \"large\"\n@@ -781,6 +795,8 @@ def quantize(x):\n     x32, y32 = x.astype(np.float32), y.astype(np.float32)\n     ref = (x32.T if lhs_transpose else x32) @ (y32.T if rhs_transpose else y32)\n     atol = 2e-2 if jax_out_dtype == jnp.float16 else 5e-6\n+    if utils.bitwidth(in_mlir_dtype) == 8:\n+      atol = 3e-2\n     np.testing.assert_allclose(z, ref, atol=atol)\n \n   # TODO(apaszke): Add support for f32\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Added support for unrolling to `lax.fori_loop` lowering\n\nWe currently require that `unroll` divides `length` for simplicity. This\nrestriction can be lifted later if/when necessary.\n\nPiperOrigin-RevId: 760540276",
        "output": [
            "```diff\nCommit: 88105e90e03dc52055a57f2d84628bb563a053e9\nDate: 2025-05-19T09:56:21Z\nURL: https://github.com/jax-ml/jax/commit/88105e90e03dc52055a57f2d84628bb563a053e9\nFiles changed: 2\nAdditions: +55, Deletions: -23\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex b611ea4c17f9..73623de43e39 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -2370,11 +2370,11 @@ def _lower_jaxpr_to_for_loop(\n     ctx: LoweringRuleContext,\n     jaxpr: jax_core.Jaxpr,\n     start: ir.Value,\n-    length: ir.Value | int,\n+    length: int | ir.Value,\n     consts,\n     *args,\n     has_loop_index: bool,\n-    unroll: bool = False,\n+    unroll: int | None = None,\n ):\n   _consts_avals, arg_avals = util.split_list(ctx.avals_in, [len(consts)])\n   arg_avals = arg_avals[has_loop_index:]\n@@ -2395,22 +2395,42 @@ def as_values(vals, avals):\n     return [v if a else _ensure(v, av) for a, v, av in zip(is_acc, vals, avals)]\n \n   def loop(loop_index, body_args):\n-    if has_loop_index:\n-      loop_index = arith_dialect.addi(loop_index, start)\n-      jaxpr_args = [*consts, loop_index, *body_args]\n-    else:\n-      jaxpr_args = [*consts, *body_args]\n-    outs = lower_jaxpr_to_mosaic_gpu(\n-        ctx.module_ctx, ctx.launch_ctx, jaxpr, jaxpr_args\n-    )\n+    outs = body_args\n+    if unroll is not None:\n+      loop_index = arith_dialect.muli(\n+          loop_index, _ir_constant(unroll, start.type)\n+      )\n+    loop_index = arith_dialect.addi(loop_index, start)\n+    for step in range(unroll or 1):\n+      if has_loop_index:\n+        loop_index = arith_dialect.addi(\n+            loop_index, _ir_constant(step, start.type)\n+        )\n+        jaxpr_args = [*consts, loop_index, *outs]\n+      else:\n+        jaxpr_args = [*consts, *outs]\n+      outs = lower_jaxpr_to_mosaic_gpu(\n+          ctx.module_ctx, ctx.launch_ctx, jaxpr, jaxpr_args\n+      )\n     return as_values(outs, out_avals)\n \n-  if unroll:\n-    assert isinstance(length, int)\n-    outs = as_values(args, arg_avals)\n-    for i in range(length):\n-      outs = loop(_ir_constant(i, start.type), outs)\n-    return outs\n+  if unroll is not None:\n+    if not isinstance(length, int):\n+      raise NotImplementedError(\n+          \"``length`` must be an integer when ``unroll` is specified, got\"\n+          f\" {length}\"\n+      )\n+    if length % unroll:\n+      # TODO(slebedev): Emit an epilogue taking care of the remaining steps.\n+      raise NotImplementedError(\n+          f\"``unroll`` must divide ``length``, got {unroll=} and {length=}\"\n+      )\n+    if unroll == length:\n+      # Special-case: the loop is fully unrolled.\n+      return loop(_ir_constant(0, start.type), as_values(args, arg_avals))\n+    return mgpu.fori(\n+        _ir_constant(length // unroll, start.type), as_values(args, arg_avals)\n+    )(loop).results\n   else:\n     if not isinstance(length, ir.Value):\n       length = _ir_constant(length, start.type)\n@@ -2432,11 +2452,7 @@ def _scan_lowering_rule(\n     _split_transpose: bool,\n ):\n   # Can only handle fori_loop-like scans.\n-  if (\n-      (num_extensive := len(args) - num_consts - num_carry)\n-      or reverse\n-      or not (unroll == 1 or unroll == length)\n-  ):\n+  if (num_extensive := len(args) - num_consts - num_carry) or reverse:\n     raise NotImplementedError\n   del linear, num_extensive, reverse\n \n@@ -2465,7 +2481,7 @@ def _scan_lowering_rule(\n       consts,\n       *args,\n       has_loop_index=has_loop_index,\n-      unroll=unroll == length,\n+      unroll=unroll,\n   )\n   if has_loop_index:\n     # Need to return the final loop index value if the outer scan expects\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex dc35f03843f7..d71593dc9078 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1147,11 +1147,27 @@ def test_fori_loop_array(self, force_while):\n     )\n     def kernel(x_ref, o_ref):\n       # Equivalent to x_ref[...] + 2 + 3.\n-      o_ref[...] = _fori_loop(force_while, 2, 4, lambda i, x: x + i, x_ref[...])\n+      o_ref[...] = _fori_loop(\n+          force_while, 2, 4, lambda i, x: x + i, x_ref[...]\n+      )\n \n     x = jnp.arange(256, dtype=jnp.int32)\n     np.testing.assert_array_equal(kernel(x), x + 2 + 3)\n \n+  @parameterized.product(unroll=[1, 2])\n+  def test_fori_loop_array_unrolled(self, unroll):\n+    @functools.partial(\n+        self.pallas_call, out_shape=jax.ShapeDtypeStruct([256], jnp.int32)\n+    )\n+    def kernel(x_ref, o_ref):\n+      # Equivalent to x_ref[...] + 2 + 3 + 4 + 5.\n+      o_ref[...] = lax.fori_loop(\n+          2, 6, lambda i, x: x + i, x_ref[...], unroll=unroll\n+      )\n+\n+    x = jnp.arange(256, dtype=jnp.int32)\n+    np.testing.assert_array_equal(kernel(x), x + 2 + 3 + 4 + 5)\n+\n   @parameterized.product(force_while=[False, True])\n   def test_fori_loop_scalar(self, force_while):\n     @functools.partial(\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] Generalized `empty_like` to accept a pytree of shapes/dtypes\n\nPiperOrigin-RevId: 760529300",
        "output": [
            "```diff\nCommit: 0a3e8dc9b7dc636b4deba44adc8ca8174d2f562b\nDate: 2025-05-19T09:17:02Z\nURL: https://github.com/jax-ml/jax/commit/0a3e8dc9b7dc636b4deba44adc8ca8174d2f562b\nFiles changed: 2\nAdditions: +30, Deletions: -42\ndiff --git a/jax/_src/pallas/helpers.py b/jax/_src/pallas/helpers.py\nindex 684101e47e9e..6b274c0b6cce 100644\n--- a/jax/_src/pallas/helpers.py\n+++ b/jax/_src/pallas/helpers.py\n@@ -13,10 +13,7 @@\n # limitations under the License.\n \"\"\"Pallas helper functions.\"\"\"\n \n-from typing import Any, Protocol\n-\n import jax\n-import jax.numpy as jnp\n from jax._src.pallas import pallas_call\n from jax._src.pallas import core as pl_core\n \n@@ -24,39 +21,42 @@\n @jax.named_call\n def empty(\n     shape: tuple[int, ...],\n-    dtype: jnp.dtype,\n+    dtype: jax.typing.DTypeLike,\n     *,\n-    memory_space: Any = None,\n-    interpret: Any = False,\n+    memory_space: object | None = None,\n+    interpret: bool = False,\n+    backend: pl_core.Backend | None = None,\n ):\n-  def _empty_kernel(_):\n-    # No-op to leave the out_ref uninitialized\n-    pass\n+  return empty_like(\n+      jax.ShapeDtypeStruct(shape, dtype),\n+      memory_space=memory_space,\n+      interpret=interpret,\n+      backend=backend,\n+  )\n+\n \n+@jax.named_call\n+def empty_like(\n+    x: object,\n+    *,\n+    memory_space: object | None = None,\n+    interpret: bool = False,\n+    backend: pl_core.Backend | None = None,\n+):\n   if memory_space is None:\n-    kernel_memory_space = pl_core.MemorySpace.ANY\n-    memory_space = jax.ShapeDtypeStruct\n-  else:\n-    kernel_memory_space = memory_space\n+    memory_space = pl_core.MemorySpace.ANY\n   return pallas_call.pallas_call(\n-      _empty_kernel,\n-      in_specs=[],\n-      out_specs=pl_core.BlockSpec(memory_space=kernel_memory_space),\n-      out_shape=memory_space(shape, dtype),\n+      # No-op to leave the out_ref uninitialized\n+      lambda *_: None,\n+      out_specs=jax.tree.map(\n+          lambda _: pl_core.BlockSpec(memory_space=memory_space), x\n+      ),\n+      out_shape=x,\n       interpret=interpret,\n+      backend=backend,\n   )()\n \n \n-class ArrayLike(Protocol):\n-  shape: tuple[int, ...]\n-  dtype: jnp.dtype\n-\n-\n-def empty_like(\n-    x: ArrayLike, *, memory_space: Any = None, interpret: Any = False):\n-  return empty(x.shape, x.dtype, memory_space=memory_space, interpret=interpret)\n-\n-\n def when(condition):\n   def _wrapped(f):\n     if isinstance(condition, bool):\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 4a5cfbf3517f..3d43e2c0ab61 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -33,7 +33,7 @@\n from jax._src import tree_util\n from jax._src.lib.mlir.dialects import arith as arith_dialect\n from jax._src.pallas import core as pallas_core\n-from jax._src.pallas import pallas_call\n+from jax._src.pallas import helpers as pallas_helpers\n from jax._src.pallas import primitives as pallas_primitives\n import jax._src.pallas.utils as pallas_utils\n from jax._src.state import discharge as state_discharge\n@@ -175,7 +175,7 @@ def kernel(\n     out_shape: object,\n     *,\n     scratch_shapes: pallas_core.ScratchShapeTree = (),\n-    compiler_params: object | None = None,\n+    compiler_params: pallas_core.CompilerParams | None = None,\n     **mesh_kwargs: object,\n ):\n   if unwrap_out := not isinstance(out_shape, (tuple, list)):\n@@ -195,24 +195,12 @@ def cmap_body():\n           mesh, compiler_params=compiler_params\n       )(cmap_body)\n     _, outs = state_discharge.run_state(stateful)(\n-        (operands, empty_like(out_shape))\n+        (operands, pallas_helpers.empty_like(out_shape, backend=\"mosaic_gpu\"))\n     )\n     return outs[0] if unwrap_out else outs\n   return wrapper\n \n \n-def empty_like(shape):\n-  return pallas_call.pallas_call(\n-      lambda *_: None,\n-      out_shape=shape,\n-      out_specs=jax.tree.map(\n-          lambda _: pallas_core.BlockSpec(memory_space=GPUMemorySpace.GMEM),\n-          shape,\n-      ),\n-      backend=\"mosaic_gpu\",\n-  )()\n-\n-\n def _is_known_divisible(value, divisor, fuel=10) -> bool:\n   \"\"\"Returns True if the value is statically known to be divisible by the divisor.\"\"\"\n   if divisor == 1:\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28804 from mattjj:hijax\n\nPiperOrigin-RevId: 759960246",
        "output": [
            "```diff\nCommit: 6cb11d87657b630d9a87d3ae7c2f1ff4644eaefe\nDate: 2025-05-17T11:25:00Z\nURL: https://github.com/jax-ml/jax/commit/6cb11d87657b630d9a87d3ae7c2f1ff4644eaefe\nFiles changed: 6\nAdditions: +511, Deletions: -314\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex ab97c7ff1c2c..e49173c3df45 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,7 +88,7 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info', '_is_high', '_mut_types']\n+               '_effects', '_debug_info', '_is_high', '_final_typechange_env']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -97,7 +97,7 @@ class Jaxpr:\n   _effects: Effects\n   _debug_info: DebugInfo\n   _is_high: bool\n-  _mut_types: dict[Var, Any]\n+  _final_typechange_env: dict[Var, Any]\n \n   @property\n   def constvars(self) -> list[Var]:\n@@ -128,8 +128,8 @@ def is_high(self) -> bool:\n     return self._is_high\n \n   @property\n-  def mut_types(self) -> dict[Var, Any]:\n-    return self._mut_types\n+  def final_typechange_env(self) -> dict[Var, Any]:\n+    return self._final_typechange_env\n \n   def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                outvars: Sequence[Atom], eqns: Sequence[JaxprEqn],\n@@ -139,7 +139,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n                is_high: bool = False,\n-               mut_types: dict | None = None,\n+               final_typechange_env: dict | None = None,\n                ):\n     \"\"\"\n     Args:\n@@ -165,7 +165,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n     self._is_high = is_high\n-    self._mut_types = mut_types or {}\n+    self._final_typechange_env = final_typechange_env or {}\n \n   def __str__(self):\n     return str(self.pretty_print())\n@@ -193,7 +193,8 @@ def replace(self, **kwargs):\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n         is_high=kwargs.pop(\"is_high\", self.is_high),\n-        mut_types=kwargs.pop(\"mut_types\", self.mut_types),\n+        final_typechange_env=kwargs.pop(\"final_typechange_env\",\n+                                        self.final_typechange_env),\n     )\n     if kwargs:\n       raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 9e875f43d831..f77db5443a86 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -1183,13 +1183,13 @@ def has_effects(effects) -> bool:\n   known_effects = make_jaxpr_effects(jaxpr.constvars, ins_known_and_ref_res,\n                                      known_outvars, known_eqns)\n   known_mut, staged_mut, ins_known_ = {}, {}, set(ins_known)  # type: ignore\n-  for v, t in jaxpr.mut_types.items():\n+  for v, t in jaxpr.final_typechange_env.items():\n     [staged_mut, known_mut][v in ins_known_][v] = t\n \n   # TODO(mattjj,necula): debug info should be updated here\n   jaxpr_known = jaxpr.replace(\n       invars=ins_known_and_ref_res, outvars=known_outvars,\n-      eqns=known_eqns, effects=known_effects, mut_types=known_mut)\n+      eqns=known_eqns, effects=known_effects, final_typechange_env=known_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_known)\n \n   _, ins_staged = partition_list(in_inst, jaxpr.invars)\n@@ -1200,7 +1200,7 @@ def has_effects(effects) -> bool:\n   # TODO(mattjj,necula): debug info should be updated here\n   jaxpr_staged = jaxpr.replace(\n       invars=staged_invars, outvars=outs_staged, eqns=staged_eqns,\n-      effects=staged_effects, mut_types=staged_mut)\n+      effects=staged_effects, final_typechange_env=staged_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_staged)\n \n   return (jaxpr_known, jaxpr_staged, out_unknowns, out_inst, len(residuals),\n@@ -1713,6 +1713,7 @@ class JaxprStackFrame:\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n   is_high: bool\n+  final_typechange_env: dict\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1728,6 +1729,7 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_vars = []\n     self.debug_info = debug_info\n     self.is_high = False\n+    self.final_typechange_env = {}\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1753,9 +1755,8 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n-    mut_types = {v: v.aval for v in invars if v.aval.mutable} if self.is_high else {}\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info, self.is_high, mut_types)\n+                  debug_info, self.is_high, self.final_typechange_env)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1872,6 +1873,8 @@ def new_arg(self, aval, source_info: SourceInfo):\n     self.frame.tracers.append(tracer)\n     self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n     self.frame.invars.append(var)\n+    if aval.mutable:\n+      self.frame.final_typechange_env[var] = aval\n     return tracer\n \n   def new_const(self, c, source_info: SourceInfo):\n@@ -2692,7 +2695,7 @@ def lower_traceable(jaxpr, *lo_args):\n   assert (problem := next(lo_args_, None)) is None\n   hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.mut_types.items()\n+  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.final_typechange_env.items()\n               for lo_val in ty.get(hi_args[in_idx[v]])]\n   lo_outs = [lo_val for t, hi_val in zip(jaxpr.out_avals, hi_outs)\n              for lo_val in t.lower_val(hi_val)]\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 5edd74fe74ef..10e7e697e706 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -1597,21 +1597,18 @@ def _is_high(jaxpr, **_) -> bool:\n   return jaxpr.jaxpr.is_high\n pjit_p.is_high = _is_high  # type: ignore\n \n-def _to_lojax(*hi_args, jaxpr, out_shardings, out_layouts, **params):\n-  num_mut = [len(ty.lo_ty()) for ty in jaxpr.jaxpr.mut_types.values()]\n-  out_shardings = (UNSPECIFIED,) * sum(num_mut) + out_shardings\n-  out_layouts = (None,) * sum(num_mut) + out_layouts\n+def _to_lojax( *hi_args, jaxpr, **params):\n+  params, num_mutants = _lojax_expand_params(jaxpr, **params)\n \n   lo_args = [lo_val for t, hi_val in zip(jaxpr.in_avals, hi_args)\n              for lo_val in t.lower_val(hi_val)]\n   lo_jaxpr = pe.lower_jaxpr(jaxpr)\n-  all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, out_shardings=out_shardings,\n-                         out_layouts=out_layouts, **params)\n-  out_mut, lo_outs = split_list(all_outs, [sum(num_mut)])\n+  all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, **params)\n+  out_mut, lo_outs = split_list(all_outs, [num_mutants])\n \n   out_mut_ = iter(out_mut)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  for var, ty in jaxpr.jaxpr.mut_types.items():\n+  for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n     ty.set(hi_args[in_idx[var]], *it.islice(out_mut_, len(ty.lo_ty())))\n   assert next(out_mut_, None) is None\n \n@@ -1623,6 +1620,31 @@ def _to_lojax(*hi_args, jaxpr, out_shardings, out_layouts, **params):\n   return hi_outs\n pjit_p.to_lojax = _to_lojax\n \n+def _lojax_expand_params(\n+    hi_jaxpr, *, donated_invars, in_shardings, in_layouts, out_shardings,\n+    out_layouts, **params):\n+  # some pjit params match the length of hi_jaxpr.invars/outvars, so when\n+  # lowering we must expand them to match their number of lojax types\n+  def expand(hi_tys, xs):\n+    return tuple(y for hi, x in zip(hi_tys, xs) for y in (x,) * len(hi.lo_ty()))\n+  donated_invars = expand(hi_jaxpr.in_avals , donated_invars)\n+  in_shardings   = expand(hi_jaxpr.in_avals , in_shardings  )\n+  in_layouts     = expand(hi_jaxpr.in_avals , in_layouts    )\n+  out_shardings  = expand(hi_jaxpr.out_avals, out_shardings )\n+  out_layouts    = expand(hi_jaxpr.out_avals, out_layouts   )\n+\n+  # also, the lo_jaxpr has pure outputs corresponding to mutable hi_jaxpr types\n+  num_mutants = sum(len(hi_ty.lo_ty()) for hi_ty in\n+                    hi_jaxpr.jaxpr.final_typechange_env.values())\n+  out_shardings = (UNSPECIFIED,) * num_mutants + out_shardings\n+  out_layouts = (None,) * num_mutants + out_layouts\n+\n+  new_params = dict(params, donated_invars=donated_invars,\n+                    in_shardings=in_shardings, in_layouts=in_layouts,\n+                    out_shardings=out_shardings, out_layouts=out_layouts)\n+  return new_params, num_mutants\n+\n+\n def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n   # If device or backend is set, return the default layout. This is because you\n   # can pass arrays on cpu (with untiled layouts) to jit with backend='tpu'\ndiff --git a/tests/BUILD b/tests/BUILD\nindex c51d40715d15..c417a63404a9 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -1875,6 +1875,17 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"hijax_test\",\n+    srcs = [\"hijax_test.py\"],\n+    deps = [\n+        \"//jax:experimental\",\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n+)\n+\n jax_multiplatform_test(\n     name = \"colocated_python_test\",\n     srcs = [\"colocated_python_test.py\"],\ndiff --git a/tests/attrs_test.py b/tests/attrs_test.py\nindex b6cef7fec4dc..60a3753a7ba5 100644\n--- a/tests/attrs_test.py\n+++ b/tests/attrs_test.py\n@@ -15,9 +15,7 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n-from functools import partial\n import itertools as it\n-import unittest\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -27,10 +25,6 @@\n import jax.numpy as jnp\n \n from jax._src import config\n-from jax._src import core\n-from jax._src import dtypes\n-from jax._src.interpreters import ad\n-from jax._src.interpreters import partial_eval as pe\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip, safe_map\n \n@@ -1332,292 +1326,5 @@ def f(lst1, lst2):\n       f(b, b)\n \n \n-class HiPrimitive(core.Primitive):\n-  def __init__(self, name):\n-    self.name = name\n-    ad.primitive_jvps[self] = self.jvp\n-    ad.primitive_transposes[self] = self.transpose\n-    pe.custom_staging_rules[self] = self.staging\n-\n-  def staging(self, trace, *args, **kwargs):\n-    trace.frame.is_high = True\n-    return trace.default_process_primitive(self, args, kwargs)\n-\n-  def is_high(self, **params):\n-    return True\n-\n-  def abstract_eval(self, *arg_avals, **params):\n-    assert False, \"must override\"\n-\n-  def to_lojax(self, *lotypes_wrapped_in_hitypes, **params):\n-    assert False, \"must override\"\n-\n-  def jvp(self, primals, tangents, **params):\n-    assert False, \"must override\"\n-\n-  def transpose(self, *args, **params):\n-    assert False  # TODO\n-\n-\n-class HijaxTest(jtu.JaxTestCase):\n-\n-  def test_custom_types_and_primitive(self):\n-    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n-\n-    @dataclass(frozen=True)\n-    class MyArray:\n-      arr: jax.Array  # always f32\n-\n-    @dataclass(frozen=True)\n-    class MyTy(core.AbstractValue):\n-      mutable = False\n-\n-      def to_tangent_aval(self):\n-        return MyTy()\n-      def str_short(self, short_dtypes=False):\n-        return 'MyTy'\n-      def lo_ty(self):\n-        return [core.ShapedArray((), jnp.dtype('float32'))]\n-      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n-        return [hi_val.arr]\n-      def raise_val(self, val) -> MyArray:\n-        return MyArray(val)\n-\n-      def __eq__(self, other): return isinstance(other, MyTy)\n-\n-      def vspace_zero(self):\n-        return MyArray(jnp.zeros((), 'float32'))\n-      def vspace_add(self, x, y):\n-        return add(x, y)\n-\n-      def strip_weak_type(self): return self\n-      def normalize(self): return self\n-    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n-\n-    class ToMy(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, lo_aval):\n-        return MyTy(), set()\n-\n-      def to_lojax(_, lo):\n-        return MyArray(lo)\n-\n-      def jvp(_, primals, tangents):\n-        x, x_dot = *primals, *tangents\n-        return to(x), to(x_dot)\n-\n-      def transpose(self, out_bar, _):\n-        return from_(out_bar),\n-\n-    class FromMy(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, hi_aval):\n-        return hi_aval.lo_ty()[0], set()\n-\n-      def to_lojax(_, hi):\n-        return hi.arr\n-\n-      def jvp(_, primals, tangents):\n-        x, x_dot = *primals, *tangents\n-        return from_(x), from_(x_dot)\n-\n-      def transpose(self, out_bar, _):\n-        return to(out_bar),\n-\n-    def to(x): return to_p.bind(x)\n-    to_p = ToMy('to_my')\n-\n-    def from_(x): return from_p.bind(x)\n-    from_p = FromMy('from_my')\n-\n-    def mul(x, y): return mul_p.bind(x, y)\n-    def add(x, y): return add_p.bind(x, y)\n-\n-    class MyMul(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, hi_x, hi_y):\n-        if hi_x != hi_y: raise Exception\n-        return hi_x, set()\n-\n-      def to_lojax(_, hi_x, hi_y):\n-        return MyArray(hi_x.arr * hi_y.arr)\n-\n-      def jvp(_, primals, tangents):\n-        (x, y), (x_dot, y_dot) = primals, tangents\n-        return mul(x, y), add(mul(x, y_dot), mul(x_dot, y))\n-\n-      def transpose(self, out_bar, x, y):\n-        assert ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)\n-        if ad.is_undefined_primal(x):\n-          return mul(out_bar, y), None\n-        else:\n-          return None, mul(x, out_bar)\n-\n-    class MyAdd(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, hi_x, hi_y):\n-        if hi_x != hi_y: raise Exception\n-        return hi_x, set()\n-\n-      def to_lojax(_, hi_x, hi_y):\n-        return MyArray(hi_x.arr + hi_y.arr)\n-\n-      def jvp(_, primals, tangents):\n-        assert False  # TODO\n-\n-      def transpose(self, out_bar, x, y):\n-        return out_bar, out_bar\n-\n-    mul_p = MyMul('my_mul')\n-    add_p = MyAdd('my_add')\n-\n-\n-    @jax.jit\n-    def f(x):\n-      return to(from_(x))\n-\n-    # test basic to/from jit\n-    a = MyArray(jnp.ones(()))\n-    b = f(a)  # don't crash\n-    self.assertIsInstance(b, MyArray)\n-    self.assertAllClose(b.arr, jnp.ones(()))\n-\n-    # test basic to/from autodiff\n-    b, b_dot = jax.jvp(f, (a,), (a,))\n-    self.assertIsInstance(b, MyArray)\n-    self.assertIsInstance(b_dot, MyArray)\n-\n-    # test mul jit and backward pass\n-\n-    @jax.jit\n-    def f(x):\n-      return mul(x, x)\n-\n-    b, f_vjp = jax.vjp(f, a)\n-    self.assertIn('MyTy', str(f_vjp))\n-    a_grad, = f_vjp(b)\n-    self.assertIsInstance(a_grad, MyArray)\n-    self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n-\n-  def test_box_autodiff(self):\n-    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n-    class BoxTy(core.AbstractValue):\n-      mutable = True\n-\n-      def to_tangent_aval(self):\n-        # NOTE not really used, for some reason we had to write it anyway\n-        return core.ShapedArray((), dtypes.float0)\n-\n-      def str_short(self, short_dtypes=False):\n-        return 'BoxTy'\n-\n-      def lower_val(self, box):\n-        return [box._val]\n-\n-      def raise_val(self, val):\n-        return Box(val)  # we're gonna mutate this\n-\n-      def lo_ty(self):\n-        return [core.ShapedArray((), jnp.dtype('float32'))]\n-\n-      def get(self, box):\n-        return [box._val]\n-\n-      def set(self, box, val):\n-        box._val = val\n-\n-    class Box:\n-      def __init__(self, val):\n-        self._val = val\n-      ty = BoxTy()\n-    core.pytype_aval_mappings[Box] = lambda b: b.ty\n-\n-\n-    class BoxSet(HiPrimitive):\n-      multiple_results = True\n-      def is_high(self) -> bool: return True\n-\n-      def abstract_eval(*_, **__):\n-        return [], set()\n-\n-      def to_lojax(_, box, val):\n-        box._val = val\n-        return []\n-\n-      def jvp(_, primals, tangents):\n-        assert False  # TODO\n-\n-      def transpose(_, *args):\n-        assert False  # TODO\n-    box_set_p = BoxSet('box_set')\n-\n-    class BoxGet(HiPrimitive):\n-      def is_high(self) -> bool: return True\n-\n-      def abstract_eval(*_, **__):\n-        return jnp.dtype('float32'), set()\n-\n-      def to_lojax(_, box):\n-        return box._val\n-\n-      def jvp(_, primals, tangents):\n-        assert False  # TODO\n-\n-      def transpose(_, *args):\n-        assert False  # TODO\n-    box_get_p = BoxGet('box_get')\n-\n-    class StashTangents(HiPrimitive):\n-      def is_high(self):\n-        return True\n-\n-      def abstract_eval(_, box_aval, x_aval):\n-        del box_aval\n-        return x_aval, set()\n-\n-      def to_lojax(_, box, x):\n-        assert False  # TODO\n-\n-      def jvp(_, primals, tangents):\n-        box, x = primals\n-        _, x_dot = tangents\n-        box_set(box, x_dot)\n-        return x, x_dot\n-\n-      def transpose(self, *args):\n-        assert False  # TODO\n-    stash_tangents_p = StashTangents('stash_tangents')\n-\n-    def box_set(box, val):\n-      box_set_p.bind(box, val)\n-\n-    def box_get(box):\n-      return box_get_p.bind(box)\n-\n-    def stash_tangents(box, x):\n-      return stash_tangents_p.bind(box, x)\n-\n-    @jax.jit\n-    def f(box, x):\n-      box_set(box, x)\n-\n-    box = Box(0.0)\n-    f(box, 1.)\n-    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n-\n-    @jax.jit\n-    def f(box, x):\n-      x = stash_tangents(box, x)\n-      return x\n-\n-    box = Box(0.0)\n-    jax.jvp(partial(f, box), (3.,), (5.,))\n-    self.assertAllClose(box_get(box), 5.0, check_dtypes=False)\n-\n-\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\ndiff --git a/tests/hijax_test.py b/tests/hijax_test.py\nnew file mode 100644\nindex 000000000000..21034d164d28\n--- /dev/null\n+++ b/tests/hijax_test.py\n@@ -0,0 +1,453 @@\n+# Copyright 2024 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+from functools import partial\n+import itertools as it\n+import unittest\n+\n+from absl.testing import absltest\n+\n+import jax\n+import jax.numpy as jnp\n+\n+from jax._src import config\n+from jax._src import core\n+from jax._src import dtypes\n+from jax._src.interpreters import ad\n+from jax._src.interpreters import partial_eval as pe\n+from jax._src import test_util as jtu\n+from jax._src.util import safe_zip, safe_map\n+\n+config.parse_flags_with_absl()\n+\n+map, unsafe_map = safe_map, map\n+zip, unsafe_zip = safe_zip, zip\n+\n+\n+# TODO(mattjj,dougalm): move HiPrimitive, Box, etc out of tests and into library\n+class HiPrimitive(core.Primitive):\n+  def __init__(self, name):\n+    self.name = name\n+    ad.primitive_jvps[self] = self.jvp\n+    ad.primitive_transposes[self] = self.transpose\n+    pe.custom_staging_rules[self] = self.staging\n+\n+  def staging(self, trace, *args, **kwargs):\n+    trace.frame.is_high = True\n+    return trace.default_process_primitive(self, args, kwargs)\n+\n+  def is_high(self, **params):\n+    return True\n+\n+  def abstract_eval(self, *arg_avals, **params):\n+    assert False, \"must override\"\n+\n+  def to_lojax(self, *lotypes_wrapped_in_hitypes, **params):\n+    assert False, \"must override\"\n+\n+  def jvp(self, primals, tangents, **params):\n+    assert False, \"must override\"\n+\n+  def transpose(self, *args, **params):\n+    assert False  # TODO\n+\n+\n+class BoxTy(core.AbstractValue):\n+  mutable = True\n+\n+  def __init__(self, leaf_avals, treedef):\n+    self._leaf_avals = leaf_avals  # hijax avals\n+    self._treedef = treedef\n+\n+  # aval interface: hashability and str_short\n+  def __hash__(self):\n+    return hash((self._leaf_avals, self._treedef))\n+\n+  def __eq__(self, other):\n+    return (isinstance(other, BoxTy) and self._leaf_avals == other._leaf_avals\n+            and self._treedef == other._treedef)\n+\n+  def str_short(self, short_dtypes=False):\n+    return 'BoxTy'\n+\n+  # hijax interface: lower val, raise val, and low type\n+  def lo_ty(self):\n+    return [lo_aval for hi_aval in self._leaf_avals for lo_aval in hi_aval.lo_ty()]\n+\n+  def lower_val(self, box):\n+    leaf_vals, treedef = jax.tree.flatten(box._val)\n+    assert treedef == self._treedef\n+    return [lo_val for hi_aval, hi_val in zip(self._leaf_avals, leaf_vals)\n+            for lo_val in hi_aval.lower_val(hi_val)]\n+\n+  def raise_val(self, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in self._leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    return Box(jax.tree.unflatten(self._treedef, hi_vals))  # will be mutated\n+\n+  # mutable interface: get/set\n+  def get(self, box):\n+    leaf_vals, treedef = jax.tree.flatten(box._val)\n+    assert treedef == self._treedef\n+    return [lo_val for hi_ty, hi_val in zip(self._leaf_avals, leaf_vals)\n+            for lo_val in hi_ty.lower_val(hi_val)]\n+\n+  def set(self, box, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in self._leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    box._val = jax.tree.unflatten(self._treedef, hi_vals)\n+\n+  # TODO placeholder thing\n+  def to_tangent_aval(self):\n+    return core.ShapedArray((), dtypes.float0)  # TODO revise placeholder\n+\n+class Box:  # noqa: F811\n+  def __init__(self, val):\n+    self._val = val\n+\n+  @property\n+  def ty(self):\n+    leaves, treedef = jax.tree.flatten(self._val)\n+    leaf_avals = tuple(map(core.typeof, leaves))\n+    return BoxTy(leaf_avals, treedef)\n+core.pytype_aval_mappings[Box] = lambda b: b.ty\n+\n+\n+class BoxSet(HiPrimitive):\n+  multiple_results = True\n+\n+  def is_high(self, *, treedef) -> bool: return True\n+\n+  def staging(self, trace, box, *leaves, treedef):\n+    super().staging(trace, box, *leaves, treedef=treedef)\n+    avals = tuple(t.aval for t in leaves)\n+    trace.frame.final_typechange_env[trace.getvar(box)] = BoxTy(avals, treedef)\n+\n+  def abstract_eval(self, box_ty, *leaf_avals, treedef):\n+    return [], set()  # TODO better typechecking...\n+\n+  def to_lojax(_, box, *leaves, treedef):\n+    box._val = jax.tree.unflatten(treedef, leaves)\n+    return []\n+\n+  def jvp(_, primals, tangents, *, treedef):\n+    assert False  # TODO\n+\n+  def transpose(_, *args, treedef):\n+    assert False  # TODO\n+box_set_p = BoxSet('box_set')\n+\n+def box_set(box, val):\n+  leaves, treedef = jax.tree.flatten(val)\n+  box_set_p.bind(box, *leaves, treedef=treedef)\n+\n+\n+class BoxGet(HiPrimitive):\n+  multiple_results = True\n+\n+  def is_high(self) -> bool: return True\n+\n+  def abstract_eval(self, box_ty):\n+    return box_ty._leaf_avals, set()\n+\n+  def to_lojax(_, box):\n+    return jax.tree.leaves(box._val)\n+\n+  def jvp(_, primals, tangents):\n+    assert False  # TODO\n+\n+  def transpose(_, *args):\n+    assert False  # TODO\n+box_get_p = BoxGet('box_get')\n+\n+def box_get(box):\n+  leaf_vals = box_get_p.bind(box)\n+  return jax.tree.unflatten(core.typeof(box)._treedef, leaf_vals)\n+\n+\n+class HijaxTest(jtu.JaxTestCase):\n+\n+  def test_custom_types_and_primitive(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+\n+    @dataclass(frozen=True)\n+    class MyArray:\n+      arr: jax.Array  # always f32\n+\n+    @dataclass(frozen=True)\n+    class MyTy(core.AbstractValue):\n+      mutable = False\n+\n+      def to_tangent_aval(self):\n+        return MyTy()\n+      def str_short(self, short_dtypes=False):\n+        return 'MyTy'\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n+        return [hi_val.arr]\n+      def raise_val(self, val) -> MyArray:\n+        return MyArray(val)\n+\n+      def __eq__(self, other): return isinstance(other, MyTy)\n+\n+      def vspace_zero(self):\n+        return MyArray(jnp.zeros((), 'float32'))\n+      def vspace_add(self, x, y):\n+        return add(x, y)\n+    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n+\n+    class ToMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, lo_aval):\n+        return MyTy(), set()\n+\n+      def to_lojax(_, lo):\n+        return MyArray(lo)\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return to(x), to(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return from_(out_bar),\n+\n+    class FromMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_aval):\n+        return hi_aval.lo_ty()[0], set()\n+\n+      def to_lojax(_, hi):\n+        return hi.arr\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return from_(x), from_(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return to(out_bar),\n+\n+    def to(x): return to_p.bind(x)\n+    to_p = ToMy('to_my')\n+\n+    def from_(x): return from_p.bind(x)\n+    from_p = FromMy('from_my')\n+\n+    def mul(x, y): return mul_p.bind(x, y)\n+    def add(x, y): return add_p.bind(x, y)\n+\n+    class MyMul(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr * hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        (x, y), (x_dot, y_dot) = primals, tangents\n+        return mul(x, y), add(mul(x, y_dot), mul(x_dot, y))\n+\n+      def transpose(self, out_bar, x, y):\n+        assert ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)\n+        if ad.is_undefined_primal(x):\n+          return mul(out_bar, y), None\n+        else:\n+          return None, mul(x, out_bar)\n+\n+    class MyAdd(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr + hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(self, out_bar, x, y):\n+        return out_bar, out_bar\n+\n+    mul_p = MyMul('my_mul')\n+    add_p = MyAdd('my_add')\n+\n+\n+    @jax.jit\n+    def f(x):\n+      return to(from_(x))\n+\n+    # test basic to/from jit\n+    a = MyArray(jnp.ones(()))\n+    b = f(a)  # don't crash\n+    self.assertIsInstance(b, MyArray)\n+    self.assertAllClose(b.arr, jnp.ones(()))\n+\n+    # test basic to/from autodiff\n+    b, b_dot = jax.jvp(f, (a,), (a,))\n+    self.assertIsInstance(b, MyArray)\n+    self.assertIsInstance(b_dot, MyArray)\n+\n+    # test mul jit and backward pass\n+\n+    @jax.jit\n+    def f(x):\n+      return mul(x, x)\n+\n+    b, f_vjp = jax.vjp(f, a)\n+    self.assertIn('MyTy', str(f_vjp))\n+    a_grad, = f_vjp(b)\n+    self.assertIsInstance(a_grad, MyArray)\n+    self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n+\n+  def test_box_autodiff(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+\n+    class StashTangents(HiPrimitive):\n+      def is_high(self):\n+        return True\n+\n+      def abstract_eval(_, box_aval, x_aval):\n+        del box_aval\n+        return x_aval, set()\n+\n+      def to_lojax(_, box, x):\n+        assert False  # TODO\n+\n+      def jvp(_, primals, tangents):\n+        box, x = primals\n+        _, x_dot = tangents\n+        box_set(box, x_dot)\n+        return x, x_dot\n+\n+      def transpose(self, *args):\n+        assert False  # TODO\n+    stash_tangents_p = StashTangents('stash_tangents')\n+\n+    def stash_tangents(box, x):\n+      return stash_tangents_p.bind(box, x)\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    box = Box(0.0)\n+    f(box, 1.)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+\n+    @jax.jit\n+    def f(box, x):\n+      x = stash_tangents(box, x)\n+      return x\n+\n+    box = Box(0.0)\n+    jax.jvp(partial(f, box), (3.,), (5.,))\n+    self.assertAllClose(box_get(box), 5.0, check_dtypes=False)\n+\n+  def test_type_changing_box(self):\n+    box = Box(jnp.arange(1))\n+    box_set(box, jnp.arange(2))\n+    self.assertLen(box._val, 2)\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    f(box, jnp.arange(3))\n+    self.assertLen(box._val, 3)\n+    f(box, jnp.arange(4))\n+    self.assertLen(box._val, 4)\n+\n+  def test_pytree_box(self):\n+    box = Box(None)\n+\n+    @jax.jit\n+    def f(box, x):\n+      assert tracing_ok\n+      val = box_get(box)\n+      if val is None:\n+        box_set(box, x)\n+      else:\n+        box_set(box, [x, x])\n+\n+    tracing_ok = True\n+    f(box, 1.0)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+    f(box, 2.0)\n+    self.assertAllClose(box_get(box), [2.0, 2.0], check_dtypes=False)\n+    f(box, 3.0)\n+    self.assertAllClose(box_get(box), [3.0, 3.0], check_dtypes=False)\n+    tracing_ok = False\n+    f(box, 4.0)\n+    self.assertAllClose(box_get(box), [4.0, 4.0], check_dtypes=False)\n+\n+  def test_pytree_of_hijaxtypes_box(self):\n+\n+    @dataclass(frozen=True)\n+    class MyArray:\n+      arr: jax.Array  # always f32\n+\n+    @dataclass(frozen=True)\n+    class MyTy(core.AbstractValue):\n+      mutable = False\n+\n+      def to_tangent_aval(self):\n+        return MyTy()\n+      def str_short(self, short_dtypes=False):\n+        return 'MyTy'\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n+        return [hi_val.arr]\n+      def raise_val(self, val) -> MyArray:\n+        return MyArray(val)\n+\n+      def __eq__(self, other): return isinstance(other, MyTy)\n+\n+    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n+\n+    box = Box([MyArray(jnp.float32(1)),\n+               MyArray(jnp.float32(2))])\n+\n+    @jax.jit\n+    def f(box):\n+      a, b = box_get(box)\n+      box_set(box, [b, a])\n+\n+    f(box)\n+    val = box_get(box)\n+    self.assertIsInstance(val, list)\n+    self.assertLen(val, 2)\n+    b_, a_ = val\n+    self.assertIsInstance(a_, MyArray)\n+    self.assertIsInstance(b_, MyArray)\n+    self.assertAllClose(a_.arr, 1, check_dtypes=False)\n+    self.assertAllClose(b_.arr, 2, check_dtypes=False)\n+\n+\n+if __name__ == '__main__':\n+  absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Added a test case to guard JAX ad, jax2tf in jax.lax.scan.\n\nPiperOrigin-RevId: 759808188",
        "output": [
            "```diff\nCommit: 4097840c58599971cde03c8d5044b7336bcad085\nDate: 2025-05-17T00:06:18Z\nURL: https://github.com/jax-ml/jax/commit/4097840c58599971cde03c8d5044b7336bcad085\nFiles changed: 1\nAdditions: +48, Deletions: -0\ndiff --git a/jax/experimental/jax2tf/tests/jax2tf_test.py b/jax/experimental/jax2tf/tests/jax2tf_test.py\nindex 3052b532cb97..db608adc3dde 100644\n--- a/jax/experimental/jax2tf/tests/jax2tf_test.py\n+++ b/jax/experimental/jax2tf/tests/jax2tf_test.py\n@@ -1694,6 +1694,54 @@ def f_jax(x):\n                                 \"Unsupported precision in dot_general\"):\n       jax2tf.convert(f_jax, native_serialization=False)(x)\n \n+  def test_jvp_through_loop(self):\n+    # Context: b/388929258\n+\n+    num_actions = 512\n+\n+    def tf_preprocessor(features):\n+      features[\"num_c_actions\"] = tf.constant(256, tf.int32)\n+      return features\n+\n+    def postprocessor(prob, features):\n+      actions = jnp.arange(num_actions, dtype=jnp.int32)\n+      r = actions // features[\"num_c_actions\"]\n+      c = actions - r * features[\"num_c_actions\"]\n+      rr = jnp.array([0.12, 0.3])[r] * prob\n+      rc = (jnp.arange(256) * 0.7)[c] * prob\n+      return rr, rc\n+\n+    def loop_step(features, params):\n+      features = jax2tf.call_tf(tf_preprocessor)(features)\n+      odds = features[\"f1\"] @ params[\"w1\"] + features[\"f2\"] @ params[\"w2\"]\n+      prob = jax.nn.sigmoid(odds)\n+      rr, rc = postprocessor(prob, features)\n+      new_f1 = jnp.mean(rr, keepdims=True)\n+      new_f2 = jnp.mean(rc, keepdims=True)\n+      return new_f1, new_f2\n+\n+    def loop(init_features, params):\n+      def body(carry, unused_x):\n+        f1, f2 = carry\n+        return loop_step({\"f1\": f1, \"f2\": f2}, params), None\n+\n+      (rr, rc), _ = jax.lax.scan(\n+          body, (init_features[\"f1\"], init_features[\"f2\"]), length=10\n+      )\n+      return rr, rc\n+\n+    def loss(features, params):\n+      rr, rc = loop(features, params)\n+      return jnp.mean((rr - rc) ** 2)\n+\n+    jax.grad(loss, argnums=(1,))(\n+        {\"f1\": jnp.array([0.5]), \"f2\": jnp.array([0.7])},\n+        {\n+            \"w1\": jnp.ones((1, num_actions)) * 0.01,\n+            \"w2\": jnp.ones((1, num_actions)) * 0.01,\n+        },\n+    )\n+\n \n @jtu.with_config(jax_enable_custom_prng=True)\n class Jax2tfWithCustomPRNGTest(tf_test_util.JaxToTfTestCase):\n\n```"
        ],
        "from_id": [
            "brills",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[hijax] type-changing boxes with pytree contents",
        "output": [
            "```diff\nCommit: 1f592543337c8e542e5ebb801c89b083928f6009\nDate: 2025-05-16T22:37:44Z\nURL: https://github.com/jax-ml/jax/commit/1f592543337c8e542e5ebb801c89b083928f6009\nFiles changed: 6\nAdditions: +512, Deletions: -314\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex ab97c7ff1c2c..e49173c3df45 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,7 +88,7 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info', '_is_high', '_mut_types']\n+               '_effects', '_debug_info', '_is_high', '_final_typechange_env']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -97,7 +97,7 @@ class Jaxpr:\n   _effects: Effects\n   _debug_info: DebugInfo\n   _is_high: bool\n-  _mut_types: dict[Var, Any]\n+  _final_typechange_env: dict[Var, Any]\n \n   @property\n   def constvars(self) -> list[Var]:\n@@ -128,8 +128,8 @@ def is_high(self) -> bool:\n     return self._is_high\n \n   @property\n-  def mut_types(self) -> dict[Var, Any]:\n-    return self._mut_types\n+  def final_typechange_env(self) -> dict[Var, Any]:\n+    return self._final_typechange_env\n \n   def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                outvars: Sequence[Atom], eqns: Sequence[JaxprEqn],\n@@ -139,7 +139,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n                is_high: bool = False,\n-               mut_types: dict | None = None,\n+               final_typechange_env: dict | None = None,\n                ):\n     \"\"\"\n     Args:\n@@ -165,7 +165,7 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n     self._is_high = is_high\n-    self._mut_types = mut_types or {}\n+    self._final_typechange_env = final_typechange_env or {}\n \n   def __str__(self):\n     return str(self.pretty_print())\n@@ -193,7 +193,8 @@ def replace(self, **kwargs):\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n         is_high=kwargs.pop(\"is_high\", self.is_high),\n-        mut_types=kwargs.pop(\"mut_types\", self.mut_types),\n+        final_typechange_env=kwargs.pop(\"final_typechange_env\",\n+                                        self.final_typechange_env),\n     )\n     if kwargs:\n       raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 9e875f43d831..f77db5443a86 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -1183,13 +1183,13 @@ def has_effects(effects) -> bool:\n   known_effects = make_jaxpr_effects(jaxpr.constvars, ins_known_and_ref_res,\n                                      known_outvars, known_eqns)\n   known_mut, staged_mut, ins_known_ = {}, {}, set(ins_known)  # type: ignore\n-  for v, t in jaxpr.mut_types.items():\n+  for v, t in jaxpr.final_typechange_env.items():\n     [staged_mut, known_mut][v in ins_known_][v] = t\n \n   # TODO(mattjj,necula): debug info should be updated here\n   jaxpr_known = jaxpr.replace(\n       invars=ins_known_and_ref_res, outvars=known_outvars,\n-      eqns=known_eqns, effects=known_effects, mut_types=known_mut)\n+      eqns=known_eqns, effects=known_effects, final_typechange_env=known_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_known)\n \n   _, ins_staged = partition_list(in_inst, jaxpr.invars)\n@@ -1200,7 +1200,7 @@ def has_effects(effects) -> bool:\n   # TODO(mattjj,necula): debug info should be updated here\n   jaxpr_staged = jaxpr.replace(\n       invars=staged_invars, outvars=outs_staged, eqns=staged_eqns,\n-      effects=staged_effects, mut_types=staged_mut)\n+      effects=staged_effects, final_typechange_env=staged_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_staged)\n \n   return (jaxpr_known, jaxpr_staged, out_unknowns, out_inst, len(residuals),\n@@ -1713,6 +1713,7 @@ class JaxprStackFrame:\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n   is_high: bool\n+  final_typechange_env: dict\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1728,6 +1729,7 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_vars = []\n     self.debug_info = debug_info\n     self.is_high = False\n+    self.final_typechange_env = {}\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1753,9 +1755,8 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n-    mut_types = {v: v.aval for v in invars if v.aval.mutable} if self.is_high else {}\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info, self.is_high, mut_types)\n+                  debug_info, self.is_high, self.final_typechange_env)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1872,6 +1873,8 @@ def new_arg(self, aval, source_info: SourceInfo):\n     self.frame.tracers.append(tracer)\n     self.frame.tracer_to_var[id(tracer)] = var = self.frame.newvar(aval)\n     self.frame.invars.append(var)\n+    if aval.mutable:\n+      self.frame.final_typechange_env[var] = aval\n     return tracer\n \n   def new_const(self, c, source_info: SourceInfo):\n@@ -2692,7 +2695,7 @@ def lower_traceable(jaxpr, *lo_args):\n   assert (problem := next(lo_args_, None)) is None\n   hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.mut_types.items()\n+  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.final_typechange_env.items()\n               for lo_val in ty.get(hi_args[in_idx[v]])]\n   lo_outs = [lo_val for t, hi_val in zip(jaxpr.out_avals, hi_outs)\n              for lo_val in t.lower_val(hi_val)]\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 5edd74fe74ef..10e7e697e706 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -1597,21 +1597,18 @@ def _is_high(jaxpr, **_) -> bool:\n   return jaxpr.jaxpr.is_high\n pjit_p.is_high = _is_high  # type: ignore\n \n-def _to_lojax(*hi_args, jaxpr, out_shardings, out_layouts, **params):\n-  num_mut = [len(ty.lo_ty()) for ty in jaxpr.jaxpr.mut_types.values()]\n-  out_shardings = (UNSPECIFIED,) * sum(num_mut) + out_shardings\n-  out_layouts = (None,) * sum(num_mut) + out_layouts\n+def _to_lojax( *hi_args, jaxpr, **params):\n+  params, num_mutants = _lojax_expand_params(jaxpr, **params)\n \n   lo_args = [lo_val for t, hi_val in zip(jaxpr.in_avals, hi_args)\n              for lo_val in t.lower_val(hi_val)]\n   lo_jaxpr = pe.lower_jaxpr(jaxpr)\n-  all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, out_shardings=out_shardings,\n-                         out_layouts=out_layouts, **params)\n-  out_mut, lo_outs = split_list(all_outs, [sum(num_mut)])\n+  all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, **params)\n+  out_mut, lo_outs = split_list(all_outs, [num_mutants])\n \n   out_mut_ = iter(out_mut)\n   in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n-  for var, ty in jaxpr.jaxpr.mut_types.items():\n+  for var, ty in jaxpr.jaxpr.final_typechange_env.items():\n     ty.set(hi_args[in_idx[var]], *it.islice(out_mut_, len(ty.lo_ty())))\n   assert next(out_mut_, None) is None\n \n@@ -1623,6 +1620,31 @@ def _to_lojax(*hi_args, jaxpr, out_shardings, out_layouts, **params):\n   return hi_outs\n pjit_p.to_lojax = _to_lojax\n \n+def _lojax_expand_params(\n+    hi_jaxpr, *, donated_invars, in_shardings, in_layouts, out_shardings,\n+    out_layouts, **params):\n+  # some pjit params match the length of hi_jaxpr.invars/outvars, so when\n+  # lowering we must expand them to match their number of lojax types\n+  def expand(hi_tys, xs):\n+    return tuple(y for hi, x in zip(hi_tys, xs) for y in (x,) * len(hi.lo_ty()))\n+  donated_invars = expand(hi_jaxpr.in_avals , donated_invars)\n+  in_shardings   = expand(hi_jaxpr.in_avals , in_shardings  )\n+  in_layouts     = expand(hi_jaxpr.in_avals , in_layouts    )\n+  out_shardings  = expand(hi_jaxpr.out_avals, out_shardings )\n+  out_layouts    = expand(hi_jaxpr.out_avals, out_layouts   )\n+\n+  # also, the lo_jaxpr has pure outputs corresponding to mutable hi_jaxpr types\n+  num_mutants = sum(len(hi_ty.lo_ty()) for hi_ty in\n+                    hi_jaxpr.jaxpr.final_typechange_env.values())\n+  out_shardings = (UNSPECIFIED,) * num_mutants + out_shardings\n+  out_layouts = (None,) * num_mutants + out_layouts\n+\n+  new_params = dict(params, donated_invars=donated_invars,\n+                    in_shardings=in_shardings, in_layouts=in_layouts,\n+                    out_shardings=out_shardings, out_layouts=out_layouts)\n+  return new_params, num_mutants\n+\n+\n def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n   # If device or backend is set, return the default layout. This is because you\n   # can pass arrays on cpu (with untiled layouts) to jit with backend='tpu'\ndiff --git a/tests/BUILD b/tests/BUILD\nindex c51d40715d15..35695c75c79f 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -1875,6 +1875,18 @@ jax_multiplatform_test(\n     ]),\n )\n \n+jax_multiplatform_test(\n+    name = \"hijax_test\",\n+    srcs = [\"hijax_test.py\"],\n+    deps = [\n+        \"//jax:experimental\",\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n+)\n+\n+\n jax_multiplatform_test(\n     name = \"colocated_python_test\",\n     srcs = [\"colocated_python_test.py\"],\ndiff --git a/tests/attrs_test.py b/tests/attrs_test.py\nindex b6cef7fec4dc..60a3753a7ba5 100644\n--- a/tests/attrs_test.py\n+++ b/tests/attrs_test.py\n@@ -15,9 +15,7 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n-from functools import partial\n import itertools as it\n-import unittest\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -27,10 +25,6 @@\n import jax.numpy as jnp\n \n from jax._src import config\n-from jax._src import core\n-from jax._src import dtypes\n-from jax._src.interpreters import ad\n-from jax._src.interpreters import partial_eval as pe\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip, safe_map\n \n@@ -1332,292 +1326,5 @@ def f(lst1, lst2):\n       f(b, b)\n \n \n-class HiPrimitive(core.Primitive):\n-  def __init__(self, name):\n-    self.name = name\n-    ad.primitive_jvps[self] = self.jvp\n-    ad.primitive_transposes[self] = self.transpose\n-    pe.custom_staging_rules[self] = self.staging\n-\n-  def staging(self, trace, *args, **kwargs):\n-    trace.frame.is_high = True\n-    return trace.default_process_primitive(self, args, kwargs)\n-\n-  def is_high(self, **params):\n-    return True\n-\n-  def abstract_eval(self, *arg_avals, **params):\n-    assert False, \"must override\"\n-\n-  def to_lojax(self, *lotypes_wrapped_in_hitypes, **params):\n-    assert False, \"must override\"\n-\n-  def jvp(self, primals, tangents, **params):\n-    assert False, \"must override\"\n-\n-  def transpose(self, *args, **params):\n-    assert False  # TODO\n-\n-\n-class HijaxTest(jtu.JaxTestCase):\n-\n-  def test_custom_types_and_primitive(self):\n-    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n-\n-    @dataclass(frozen=True)\n-    class MyArray:\n-      arr: jax.Array  # always f32\n-\n-    @dataclass(frozen=True)\n-    class MyTy(core.AbstractValue):\n-      mutable = False\n-\n-      def to_tangent_aval(self):\n-        return MyTy()\n-      def str_short(self, short_dtypes=False):\n-        return 'MyTy'\n-      def lo_ty(self):\n-        return [core.ShapedArray((), jnp.dtype('float32'))]\n-      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n-        return [hi_val.arr]\n-      def raise_val(self, val) -> MyArray:\n-        return MyArray(val)\n-\n-      def __eq__(self, other): return isinstance(other, MyTy)\n-\n-      def vspace_zero(self):\n-        return MyArray(jnp.zeros((), 'float32'))\n-      def vspace_add(self, x, y):\n-        return add(x, y)\n-\n-      def strip_weak_type(self): return self\n-      def normalize(self): return self\n-    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n-\n-    class ToMy(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, lo_aval):\n-        return MyTy(), set()\n-\n-      def to_lojax(_, lo):\n-        return MyArray(lo)\n-\n-      def jvp(_, primals, tangents):\n-        x, x_dot = *primals, *tangents\n-        return to(x), to(x_dot)\n-\n-      def transpose(self, out_bar, _):\n-        return from_(out_bar),\n-\n-    class FromMy(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, hi_aval):\n-        return hi_aval.lo_ty()[0], set()\n-\n-      def to_lojax(_, hi):\n-        return hi.arr\n-\n-      def jvp(_, primals, tangents):\n-        x, x_dot = *primals, *tangents\n-        return from_(x), from_(x_dot)\n-\n-      def transpose(self, out_bar, _):\n-        return to(out_bar),\n-\n-    def to(x): return to_p.bind(x)\n-    to_p = ToMy('to_my')\n-\n-    def from_(x): return from_p.bind(x)\n-    from_p = FromMy('from_my')\n-\n-    def mul(x, y): return mul_p.bind(x, y)\n-    def add(x, y): return add_p.bind(x, y)\n-\n-    class MyMul(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, hi_x, hi_y):\n-        if hi_x != hi_y: raise Exception\n-        return hi_x, set()\n-\n-      def to_lojax(_, hi_x, hi_y):\n-        return MyArray(hi_x.arr * hi_y.arr)\n-\n-      def jvp(_, primals, tangents):\n-        (x, y), (x_dot, y_dot) = primals, tangents\n-        return mul(x, y), add(mul(x, y_dot), mul(x_dot, y))\n-\n-      def transpose(self, out_bar, x, y):\n-        assert ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)\n-        if ad.is_undefined_primal(x):\n-          return mul(out_bar, y), None\n-        else:\n-          return None, mul(x, out_bar)\n-\n-    class MyAdd(HiPrimitive):\n-      def is_high(self): return True\n-\n-      def abstract_eval(_, hi_x, hi_y):\n-        if hi_x != hi_y: raise Exception\n-        return hi_x, set()\n-\n-      def to_lojax(_, hi_x, hi_y):\n-        return MyArray(hi_x.arr + hi_y.arr)\n-\n-      def jvp(_, primals, tangents):\n-        assert False  # TODO\n-\n-      def transpose(self, out_bar, x, y):\n-        return out_bar, out_bar\n-\n-    mul_p = MyMul('my_mul')\n-    add_p = MyAdd('my_add')\n-\n-\n-    @jax.jit\n-    def f(x):\n-      return to(from_(x))\n-\n-    # test basic to/from jit\n-    a = MyArray(jnp.ones(()))\n-    b = f(a)  # don't crash\n-    self.assertIsInstance(b, MyArray)\n-    self.assertAllClose(b.arr, jnp.ones(()))\n-\n-    # test basic to/from autodiff\n-    b, b_dot = jax.jvp(f, (a,), (a,))\n-    self.assertIsInstance(b, MyArray)\n-    self.assertIsInstance(b_dot, MyArray)\n-\n-    # test mul jit and backward pass\n-\n-    @jax.jit\n-    def f(x):\n-      return mul(x, x)\n-\n-    b, f_vjp = jax.vjp(f, a)\n-    self.assertIn('MyTy', str(f_vjp))\n-    a_grad, = f_vjp(b)\n-    self.assertIsInstance(a_grad, MyArray)\n-    self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n-\n-  def test_box_autodiff(self):\n-    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n-    class BoxTy(core.AbstractValue):\n-      mutable = True\n-\n-      def to_tangent_aval(self):\n-        # NOTE not really used, for some reason we had to write it anyway\n-        return core.ShapedArray((), dtypes.float0)\n-\n-      def str_short(self, short_dtypes=False):\n-        return 'BoxTy'\n-\n-      def lower_val(self, box):\n-        return [box._val]\n-\n-      def raise_val(self, val):\n-        return Box(val)  # we're gonna mutate this\n-\n-      def lo_ty(self):\n-        return [core.ShapedArray((), jnp.dtype('float32'))]\n-\n-      def get(self, box):\n-        return [box._val]\n-\n-      def set(self, box, val):\n-        box._val = val\n-\n-    class Box:\n-      def __init__(self, val):\n-        self._val = val\n-      ty = BoxTy()\n-    core.pytype_aval_mappings[Box] = lambda b: b.ty\n-\n-\n-    class BoxSet(HiPrimitive):\n-      multiple_results = True\n-      def is_high(self) -> bool: return True\n-\n-      def abstract_eval(*_, **__):\n-        return [], set()\n-\n-      def to_lojax(_, box, val):\n-        box._val = val\n-        return []\n-\n-      def jvp(_, primals, tangents):\n-        assert False  # TODO\n-\n-      def transpose(_, *args):\n-        assert False  # TODO\n-    box_set_p = BoxSet('box_set')\n-\n-    class BoxGet(HiPrimitive):\n-      def is_high(self) -> bool: return True\n-\n-      def abstract_eval(*_, **__):\n-        return jnp.dtype('float32'), set()\n-\n-      def to_lojax(_, box):\n-        return box._val\n-\n-      def jvp(_, primals, tangents):\n-        assert False  # TODO\n-\n-      def transpose(_, *args):\n-        assert False  # TODO\n-    box_get_p = BoxGet('box_get')\n-\n-    class StashTangents(HiPrimitive):\n-      def is_high(self):\n-        return True\n-\n-      def abstract_eval(_, box_aval, x_aval):\n-        del box_aval\n-        return x_aval, set()\n-\n-      def to_lojax(_, box, x):\n-        assert False  # TODO\n-\n-      def jvp(_, primals, tangents):\n-        box, x = primals\n-        _, x_dot = tangents\n-        box_set(box, x_dot)\n-        return x, x_dot\n-\n-      def transpose(self, *args):\n-        assert False  # TODO\n-    stash_tangents_p = StashTangents('stash_tangents')\n-\n-    def box_set(box, val):\n-      box_set_p.bind(box, val)\n-\n-    def box_get(box):\n-      return box_get_p.bind(box)\n-\n-    def stash_tangents(box, x):\n-      return stash_tangents_p.bind(box, x)\n-\n-    @jax.jit\n-    def f(box, x):\n-      box_set(box, x)\n-\n-    box = Box(0.0)\n-    f(box, 1.)\n-    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n-\n-    @jax.jit\n-    def f(box, x):\n-      x = stash_tangents(box, x)\n-      return x\n-\n-    box = Box(0.0)\n-    jax.jvp(partial(f, box), (3.,), (5.,))\n-    self.assertAllClose(box_get(box), 5.0, check_dtypes=False)\n-\n-\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\ndiff --git a/tests/hijax_test.py b/tests/hijax_test.py\nnew file mode 100644\nindex 000000000000..21034d164d28\n--- /dev/null\n+++ b/tests/hijax_test.py\n@@ -0,0 +1,453 @@\n+# Copyright 2024 The JAX Authors.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     https://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import annotations\n+\n+from dataclasses import dataclass\n+from functools import partial\n+import itertools as it\n+import unittest\n+\n+from absl.testing import absltest\n+\n+import jax\n+import jax.numpy as jnp\n+\n+from jax._src import config\n+from jax._src import core\n+from jax._src import dtypes\n+from jax._src.interpreters import ad\n+from jax._src.interpreters import partial_eval as pe\n+from jax._src import test_util as jtu\n+from jax._src.util import safe_zip, safe_map\n+\n+config.parse_flags_with_absl()\n+\n+map, unsafe_map = safe_map, map\n+zip, unsafe_zip = safe_zip, zip\n+\n+\n+# TODO(mattjj,dougalm): move HiPrimitive, Box, etc out of tests and into library\n+class HiPrimitive(core.Primitive):\n+  def __init__(self, name):\n+    self.name = name\n+    ad.primitive_jvps[self] = self.jvp\n+    ad.primitive_transposes[self] = self.transpose\n+    pe.custom_staging_rules[self] = self.staging\n+\n+  def staging(self, trace, *args, **kwargs):\n+    trace.frame.is_high = True\n+    return trace.default_process_primitive(self, args, kwargs)\n+\n+  def is_high(self, **params):\n+    return True\n+\n+  def abstract_eval(self, *arg_avals, **params):\n+    assert False, \"must override\"\n+\n+  def to_lojax(self, *lotypes_wrapped_in_hitypes, **params):\n+    assert False, \"must override\"\n+\n+  def jvp(self, primals, tangents, **params):\n+    assert False, \"must override\"\n+\n+  def transpose(self, *args, **params):\n+    assert False  # TODO\n+\n+\n+class BoxTy(core.AbstractValue):\n+  mutable = True\n+\n+  def __init__(self, leaf_avals, treedef):\n+    self._leaf_avals = leaf_avals  # hijax avals\n+    self._treedef = treedef\n+\n+  # aval interface: hashability and str_short\n+  def __hash__(self):\n+    return hash((self._leaf_avals, self._treedef))\n+\n+  def __eq__(self, other):\n+    return (isinstance(other, BoxTy) and self._leaf_avals == other._leaf_avals\n+            and self._treedef == other._treedef)\n+\n+  def str_short(self, short_dtypes=False):\n+    return 'BoxTy'\n+\n+  # hijax interface: lower val, raise val, and low type\n+  def lo_ty(self):\n+    return [lo_aval for hi_aval in self._leaf_avals for lo_aval in hi_aval.lo_ty()]\n+\n+  def lower_val(self, box):\n+    leaf_vals, treedef = jax.tree.flatten(box._val)\n+    assert treedef == self._treedef\n+    return [lo_val for hi_aval, hi_val in zip(self._leaf_avals, leaf_vals)\n+            for lo_val in hi_aval.lower_val(hi_val)]\n+\n+  def raise_val(self, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in self._leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    return Box(jax.tree.unflatten(self._treedef, hi_vals))  # will be mutated\n+\n+  # mutable interface: get/set\n+  def get(self, box):\n+    leaf_vals, treedef = jax.tree.flatten(box._val)\n+    assert treedef == self._treedef\n+    return [lo_val for hi_ty, hi_val in zip(self._leaf_avals, leaf_vals)\n+            for lo_val in hi_ty.lower_val(hi_val)]\n+\n+  def set(self, box, *lo_vals):\n+    lo_vals_ = iter(lo_vals)\n+    hi_vals = [hi_ty.raise_val(*it.islice(lo_vals_, len(hi_ty.lo_ty())))\n+               for hi_ty in self._leaf_avals]\n+    assert next(lo_vals_, None) is None\n+    box._val = jax.tree.unflatten(self._treedef, hi_vals)\n+\n+  # TODO placeholder thing\n+  def to_tangent_aval(self):\n+    return core.ShapedArray((), dtypes.float0)  # TODO revise placeholder\n+\n+class Box:  # noqa: F811\n+  def __init__(self, val):\n+    self._val = val\n+\n+  @property\n+  def ty(self):\n+    leaves, treedef = jax.tree.flatten(self._val)\n+    leaf_avals = tuple(map(core.typeof, leaves))\n+    return BoxTy(leaf_avals, treedef)\n+core.pytype_aval_mappings[Box] = lambda b: b.ty\n+\n+\n+class BoxSet(HiPrimitive):\n+  multiple_results = True\n+\n+  def is_high(self, *, treedef) -> bool: return True\n+\n+  def staging(self, trace, box, *leaves, treedef):\n+    super().staging(trace, box, *leaves, treedef=treedef)\n+    avals = tuple(t.aval for t in leaves)\n+    trace.frame.final_typechange_env[trace.getvar(box)] = BoxTy(avals, treedef)\n+\n+  def abstract_eval(self, box_ty, *leaf_avals, treedef):\n+    return [], set()  # TODO better typechecking...\n+\n+  def to_lojax(_, box, *leaves, treedef):\n+    box._val = jax.tree.unflatten(treedef, leaves)\n+    return []\n+\n+  def jvp(_, primals, tangents, *, treedef):\n+    assert False  # TODO\n+\n+  def transpose(_, *args, treedef):\n+    assert False  # TODO\n+box_set_p = BoxSet('box_set')\n+\n+def box_set(box, val):\n+  leaves, treedef = jax.tree.flatten(val)\n+  box_set_p.bind(box, *leaves, treedef=treedef)\n+\n+\n+class BoxGet(HiPrimitive):\n+  multiple_results = True\n+\n+  def is_high(self) -> bool: return True\n+\n+  def abstract_eval(self, box_ty):\n+    return box_ty._leaf_avals, set()\n+\n+  def to_lojax(_, box):\n+    return jax.tree.leaves(box._val)\n+\n+  def jvp(_, primals, tangents):\n+    assert False  # TODO\n+\n+  def transpose(_, *args):\n+    assert False  # TODO\n+box_get_p = BoxGet('box_get')\n+\n+def box_get(box):\n+  leaf_vals = box_get_p.bind(box)\n+  return jax.tree.unflatten(core.typeof(box)._treedef, leaf_vals)\n+\n+\n+class HijaxTest(jtu.JaxTestCase):\n+\n+  def test_custom_types_and_primitive(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+\n+    @dataclass(frozen=True)\n+    class MyArray:\n+      arr: jax.Array  # always f32\n+\n+    @dataclass(frozen=True)\n+    class MyTy(core.AbstractValue):\n+      mutable = False\n+\n+      def to_tangent_aval(self):\n+        return MyTy()\n+      def str_short(self, short_dtypes=False):\n+        return 'MyTy'\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n+        return [hi_val.arr]\n+      def raise_val(self, val) -> MyArray:\n+        return MyArray(val)\n+\n+      def __eq__(self, other): return isinstance(other, MyTy)\n+\n+      def vspace_zero(self):\n+        return MyArray(jnp.zeros((), 'float32'))\n+      def vspace_add(self, x, y):\n+        return add(x, y)\n+    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n+\n+    class ToMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, lo_aval):\n+        return MyTy(), set()\n+\n+      def to_lojax(_, lo):\n+        return MyArray(lo)\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return to(x), to(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return from_(out_bar),\n+\n+    class FromMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_aval):\n+        return hi_aval.lo_ty()[0], set()\n+\n+      def to_lojax(_, hi):\n+        return hi.arr\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return from_(x), from_(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return to(out_bar),\n+\n+    def to(x): return to_p.bind(x)\n+    to_p = ToMy('to_my')\n+\n+    def from_(x): return from_p.bind(x)\n+    from_p = FromMy('from_my')\n+\n+    def mul(x, y): return mul_p.bind(x, y)\n+    def add(x, y): return add_p.bind(x, y)\n+\n+    class MyMul(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr * hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        (x, y), (x_dot, y_dot) = primals, tangents\n+        return mul(x, y), add(mul(x, y_dot), mul(x_dot, y))\n+\n+      def transpose(self, out_bar, x, y):\n+        assert ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)\n+        if ad.is_undefined_primal(x):\n+          return mul(out_bar, y), None\n+        else:\n+          return None, mul(x, out_bar)\n+\n+    class MyAdd(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr + hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(self, out_bar, x, y):\n+        return out_bar, out_bar\n+\n+    mul_p = MyMul('my_mul')\n+    add_p = MyAdd('my_add')\n+\n+\n+    @jax.jit\n+    def f(x):\n+      return to(from_(x))\n+\n+    # test basic to/from jit\n+    a = MyArray(jnp.ones(()))\n+    b = f(a)  # don't crash\n+    self.assertIsInstance(b, MyArray)\n+    self.assertAllClose(b.arr, jnp.ones(()))\n+\n+    # test basic to/from autodiff\n+    b, b_dot = jax.jvp(f, (a,), (a,))\n+    self.assertIsInstance(b, MyArray)\n+    self.assertIsInstance(b_dot, MyArray)\n+\n+    # test mul jit and backward pass\n+\n+    @jax.jit\n+    def f(x):\n+      return mul(x, x)\n+\n+    b, f_vjp = jax.vjp(f, a)\n+    self.assertIn('MyTy', str(f_vjp))\n+    a_grad, = f_vjp(b)\n+    self.assertIsInstance(a_grad, MyArray)\n+    self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n+\n+  def test_box_autodiff(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+\n+    class StashTangents(HiPrimitive):\n+      def is_high(self):\n+        return True\n+\n+      def abstract_eval(_, box_aval, x_aval):\n+        del box_aval\n+        return x_aval, set()\n+\n+      def to_lojax(_, box, x):\n+        assert False  # TODO\n+\n+      def jvp(_, primals, tangents):\n+        box, x = primals\n+        _, x_dot = tangents\n+        box_set(box, x_dot)\n+        return x, x_dot\n+\n+      def transpose(self, *args):\n+        assert False  # TODO\n+    stash_tangents_p = StashTangents('stash_tangents')\n+\n+    def stash_tangents(box, x):\n+      return stash_tangents_p.bind(box, x)\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    box = Box(0.0)\n+    f(box, 1.)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+\n+    @jax.jit\n+    def f(box, x):\n+      x = stash_tangents(box, x)\n+      return x\n+\n+    box = Box(0.0)\n+    jax.jvp(partial(f, box), (3.,), (5.,))\n+    self.assertAllClose(box_get(box), 5.0, check_dtypes=False)\n+\n+  def test_type_changing_box(self):\n+    box = Box(jnp.arange(1))\n+    box_set(box, jnp.arange(2))\n+    self.assertLen(box._val, 2)\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    f(box, jnp.arange(3))\n+    self.assertLen(box._val, 3)\n+    f(box, jnp.arange(4))\n+    self.assertLen(box._val, 4)\n+\n+  def test_pytree_box(self):\n+    box = Box(None)\n+\n+    @jax.jit\n+    def f(box, x):\n+      assert tracing_ok\n+      val = box_get(box)\n+      if val is None:\n+        box_set(box, x)\n+      else:\n+        box_set(box, [x, x])\n+\n+    tracing_ok = True\n+    f(box, 1.0)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+    f(box, 2.0)\n+    self.assertAllClose(box_get(box), [2.0, 2.0], check_dtypes=False)\n+    f(box, 3.0)\n+    self.assertAllClose(box_get(box), [3.0, 3.0], check_dtypes=False)\n+    tracing_ok = False\n+    f(box, 4.0)\n+    self.assertAllClose(box_get(box), [4.0, 4.0], check_dtypes=False)\n+\n+  def test_pytree_of_hijaxtypes_box(self):\n+\n+    @dataclass(frozen=True)\n+    class MyArray:\n+      arr: jax.Array  # always f32\n+\n+    @dataclass(frozen=True)\n+    class MyTy(core.AbstractValue):\n+      mutable = False\n+\n+      def to_tangent_aval(self):\n+        return MyTy()\n+      def str_short(self, short_dtypes=False):\n+        return 'MyTy'\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n+        return [hi_val.arr]\n+      def raise_val(self, val) -> MyArray:\n+        return MyArray(val)\n+\n+      def __eq__(self, other): return isinstance(other, MyTy)\n+\n+    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n+\n+    box = Box([MyArray(jnp.float32(1)),\n+               MyArray(jnp.float32(2))])\n+\n+    @jax.jit\n+    def f(box):\n+      a, b = box_get(box)\n+      box_set(box, [b, a])\n+\n+    f(box)\n+    val = box_get(box)\n+    self.assertIsInstance(val, list)\n+    self.assertLen(val, 2)\n+    b_, a_ = val\n+    self.assertIsInstance(a_, MyArray)\n+    self.assertIsInstance(b_, MyArray)\n+    self.assertAllClose(a_.arr, 1, check_dtypes=False)\n+    self.assertAllClose(b_.arr, 2, check_dtypes=False)\n+\n+\n+if __name__ == '__main__':\n+  absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "Keep the serialized version of `BufferAssignmentProto` in `CompiledMemoryStats` to reduce its overheads\n\nMost users of `CompiledMemoryStats` do not use this field. So it is cheaper in terms of both CPU and RAM to keep it as a serialized string rather than a proto. If this continues to become a problem, we can consider inventing a separate executable API for buffer assignment.\n\nPiperOrigin-RevId: 759735092",
        "output": [
            "```diff\nCommit: 7412adec21c534f8e4bcc627552f28d162decc86\nDate: 2025-05-16T20:31:22Z\nURL: https://github.com/jax-ml/jax/commit/7412adec21c534f8e4bcc627552f28d162decc86\nFiles changed: 1\nAdditions: +4, Deletions: -1\ndiff --git a/jaxlib/xla.cc b/jaxlib/xla.cc\nindex 4020e061b3f4..3412766de6bd 100644\n--- a/jaxlib/xla.cc\n+++ b/jaxlib/xla.cc\n@@ -490,7 +490,10 @@ NB_MODULE(_jax, m) {\n               &CompiledMemoryStats::host_temp_size_in_bytes)\n       .def_prop_ro(\"serialized_buffer_assignment_proto\",\n                    [](const CompiledMemoryStats& cms) -> nb::bytes {\n-#if JAX_IFRT_VERSION_NUMBER >= 7\n+#if JAX_IFRT_VERSION_NUMBER >= 9\n+                     const std::string& s = cms.serialized_buffer_assignment;\n+                     return nb::bytes(s.data(), s.size());\n+#elif JAX_IFRT_VERSION_NUMBER >= 7\n                      if (cms.buffer_assignment.has_value()) {\n                        std::string s =\n                            cms.buffer_assignment->SerializeAsString();\n\n```"
        ],
        "from_id": [
            "junwhanahn",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic TPU][NFC] Consolidate `getIntConst`.\n\nPiperOrigin-RevId: 759728429",
        "output": [
            "```diff\nCommit: d47fc8d928905516a0d4dbfda6f70060068c779d\nDate: 2025-05-16T20:11:07Z\nURL: https://github.com/jax-ml/jax/commit/d47fc8d928905516a0d4dbfda6f70060068c779d\nFiles changed: 2\nAdditions: +25, Deletions: -36\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 656be0e677b0..ba1dfc95c66c 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -209,23 +209,18 @@ bool incrementIndex(const MutableArrayRef<int64_t> idx,\n   return false;\n }\n \n-FailureOr<int64_t> getIntConst(Value v, bool silent = false) {\n-  if (auto constant_op = v.getDefiningOp<arith::ConstantOp>()) {\n-    if (auto integer_attr = dyn_cast<IntegerAttr>(constant_op.getValue())) {\n-      return integer_attr.getValue().getSExtValue();\n-    }\n-  }\n-  if (silent) {\n-    return failure();\n+FailureOr<int64_t> expectIntConst(Value v) {\n+  if (auto cst = getIntConst(v)) {\n+    return cst.value();\n   }\n   return emitError(v.getLoc(), \"Expected an integer constant\");\n }\n \n-FailureOr<SmallVector<int64_t>> getIntConstsFromOperandRange(\n-    ValueRange vals, bool silent = false) {\n+FailureOr<SmallVector<int64_t>> expectIntConstsFromOperandRange(\n+    ValueRange vals) {\n   SmallVector<int64_t> res(vals.size());\n   for (int i = 0; i < vals.size(); ++i) {\n-    FAILUREOR_ASSIGN_OR_RETURN(res[i], getIntConst(vals[i], silent));\n+    FAILUREOR_ASSIGN_OR_RETURN(res[i], expectIntConst(vals[i]));\n   }\n   return res;\n }\n@@ -265,7 +260,7 @@ FailureOr<std::pair<Value, SmallVector<int64_t>>> sliceRef(\n   Value c0 = nullptr;\n   SmallVector<int64_t> indices_within_slice(indices.size() - tiling.size(), 0);\n   for (auto tiled_idx : indices.take_back(tiling.size())) {\n-    if (auto cst = getIntConst(tiled_idx, /*silent=*/true); succeeded(cst)) {\n+    if (auto cst = getIntConst(tiled_idx)) {\n       indices_within_slice.push_back(*cst);\n       if (!c0) {\n         c0 = builder.create<arith::ConstantOp>(i32,\n@@ -1548,7 +1543,7 @@ LogicalResult tpu_load_rule(RewriteContext &ctx, Operation &op,\n   }\n   FAILUREOR_ASSIGN_OR_RETURN(\n       const SmallVector<int64_t> indices,\n-      getIntConstsFromOperandRange(load_op.getIndices()));\n+      expectIntConstsFromOperandRange(load_op.getIndices()));\n   TPU_ASSERT_EQ_OP(indices.size(), 2);\n   if (indices[1] % ctx.target_shape[1] != 0) {\n     return op.emitOpError(\"Not implemented: Lane index is not a multiple of \")\n@@ -1606,8 +1601,8 @@ LogicalResult strided_op_rule_impl(RewriteContext &ctx, Operation &op,\n   if (strides[rank - 1] != 1) {\n     return op.emitOpError(\"Not Implemented: Stride on last dim is not 1\");\n   }\n-  auto last_idx = getIntConst(indices[rank - 1], /*silent=*/true);\n-  if (failed(last_idx)) {\n+  auto last_idx = getIntConst(indices[rank - 1]);\n+  if (!last_idx.has_value()) {\n     return op.emitOpError(\"Not Implemented: Dynamic index on last dim\");\n   } else if (last_idx.value() != 0) {\n     return op.emitOpError(\"Not Implemented: Index on last dim is not 0\");\n@@ -1975,7 +1970,7 @@ LogicalResult tpu_store_rule(RewriteContext &ctx, Operation &op,\n   tpu::StoreOp store_op = cast<tpu::StoreOp>(op);\n   FAILUREOR_ASSIGN_OR_RETURN(\n       const SmallVector<int64_t> indices,\n-      getIntConstsFromOperandRange(store_op.getIndices()));\n+      expectIntConstsFromOperandRange(store_op.getIndices()));\n   TPU_ASSERT_EQ_OP(indices.size(), 2);\n   if (indices[1] % ctx.target_shape[1] != 0) {\n     return op.emitOpError(\"Not implemented: Lane index is not a multiple of \")\n@@ -2143,15 +2138,14 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n     return op.emitOpError(\"Not implemented: unsupported layout for input\");\n   }\n   LayoutOffsets expected_offsets_out = layout_in.offsets();\n-  auto shift = getIntConst(amount, /*silent=*/true);\n-  const bool has_static_shift = succeeded(shift);\n+  auto shift = getIntConst(amount);\n   int rotated_tiled_dim = op.getDimension() - (op.getType().getRank() - 2);\n   bool has_padding_along_rotation =\n       (rotated_tiled_dim == 0 || rotated_tiled_dim == 1) &&\n       op.getType().getShape()[op.getDimension()] %\n               layout.tiling()[rotated_tiled_dim] !=\n           0;\n-  if (has_static_shift && has_padding_along_rotation) {\n+  if (shift.has_value() && has_padding_along_rotation) {\n     // We checked above that there are no implicit dims.\n     const int64_t dim_size = op.getType().getShape()[op.getDimension()];\n     // TODO(b/337384645): Currently we assume {0, 0} offsets in the input\n@@ -2173,7 +2167,7 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n   // TODO(b/411170715): Allow sublane rotation once the bug is fixed.\n   // TODO(b/337384645): Support non-zero stride.\n   if (has_padding_along_rotation &&\n-      (!has_static_shift ||\n+      (!shift.has_value() ||\n        (rotated_tiled_dim == 0 ||\n         (rotated_tiled_dim == 1 && op.getStride().value_or(0) != 0)))) {\n     return op.emitOpError(\"Not implemented: unsupported unaligned shape\");\n@@ -2200,19 +2194,19 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n         builder.getIntegerAttr(builder.getIndexType(), d));\n   };\n   auto modI = [&](const Value &v, unsigned d) -> Value {\n-    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+    if (auto cst = getIntConst(v)) {\n       return mlirI32Const(cst.value() % d);\n     }\n     return builder.create<arith::RemUIOp>(v, mlirI32Const(d));\n   };\n   auto divI = [&](const Value &v, unsigned d) -> Value {\n-    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+    if (auto cst = getIntConst(v)) {\n       return mlirI32Const(cst.value() / d);\n     }\n     return builder.create<arith::DivUIOp>(v, mlirI32Const(d));\n   };\n   auto addI = [&](const Value &v, unsigned d) -> Value {\n-    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+    if (auto cst = getIntConst(v)) {\n       return mlirI32Const(cst.value() + d);\n     }\n     return builder.create<arith::AddIOp>(v, mlirI32Const(d));\n@@ -2239,8 +2233,7 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n   auto getVmaskByPaddingEnd = [&](Value padding, int dim, int stride = 0) {\n     CHECK(dim == 0 || dim == 1);\n     Value padding_vreg;\n-    if (auto padding_cst = getIntConst(padding, /*silent=*/true);\n-        succeeded(padding_cst)) {\n+    if (auto padding_cst = getIntConst(padding)) {\n       CHECK_GE(padding_cst.value(), 0);\n       CHECK_LE(padding_cst.value(), ctx.target_shape[dim]);\n       padding_vreg = builder.create<arith::ConstantOp>(DenseElementsAttr::get(\n@@ -2269,8 +2262,7 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n   // and blend the data from contiguous vregs to emulate circular rotation.\n   auto rotateOnTilingDim = [&](const xla::Array<Value> &vregs,\n                                const Value &shift, int axis, int stride = 0) {\n-    if (auto shift_cst = getIntConst(shift, /*silent=*/true);\n-        succeeded(shift_cst)) {\n+    if (auto shift_cst = getIntConst(shift)) {\n       if (shift_cst.value() == 0 && stride == 0) {\n         return vregs;\n       }\n@@ -2395,8 +2387,7 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n     CHECK((tiling_dim != 1 && stride == 0) || (tiling_dim == 1 && stride >= 0));\n     SmallVector<xla::Array<Value>, 4> chunks;\n     // Handle rotation with static shift.\n-    if (auto shift_cst = getIntConst(shift, /*silent=*/true);\n-        succeeded(shift_cst)) {\n+    if (auto shift_cst = getIntConst(shift)) {\n       int64_t static_shift = shift_cst.value();\n       if (has_padding_along_rotation) {\n         return lazyRotate(vregs, static_shift, axis);\n@@ -2519,8 +2510,7 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n                                  vty.getDimSize(dim));\n         // After applying stride, we expect all shifts in a vreg are less or\n         // equal to the vreg's lane count for now.\n-        if (auto base_amount_cst = getIntConst(base_amount, /*silent=*/true);\n-            succeeded(base_amount_cst)) {\n+        if (auto base_amount_cst = getIntConst(base_amount)) {\n           int64_t static_base_amount = base_amount_cst.value();\n           auto max_shift_in_vreg = static_base_amount % ctx.target_shape[1] +\n                                    (ctx.target_shape[0] - 1) * stride;\n@@ -3163,7 +3153,7 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n   bool must_support_unaligned_dynamic_index = false;\n   if (load_op.getIndices().size() > 1) {\n     auto second_minor_idx = load_op.getIndices().take_back(2)[0];\n-    if (failed(getIntConst(second_minor_idx, /*silent=*/true)) &&\n+    if (!getIntConst(second_minor_idx).has_value() &&\n         !isGuaranteedDivisible(second_minor_idx, memref_tiling[0])) {\n       must_support_unaligned_dynamic_index = true;\n     }\n@@ -3196,7 +3186,7 @@ LogicalResult vector_load_rule(RewriteContext &ctx, Operation &op,\n   }\n \n   auto add_idx = [&](const Value &v, int64_t d) -> Value {\n-    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+    if (auto cst = getIntConst(v)) {\n       return IdxConst(cst.value() + d, builder, op.getLoc());\n     }\n     return builder.create<arith::AddIOp>(v, IdxConst(d, builder, op.getLoc()));\n@@ -4476,7 +4466,7 @@ LogicalResult vector_store_impl(RewriteContext &ctx, Op store_op,\n   bool must_support_unaligned_dynamic_index = false;\n   if (store_op.getIndices().size() > 1) {\n     auto second_minor_idx = store_op.getIndices().take_back(2)[0];\n-    if (failed(getIntConst(second_minor_idx, /*silent=*/true)) &&\n+    if (!getIntConst(second_minor_idx).has_value() &&\n         !isGuaranteedDivisible(second_minor_idx, memref_tiling[0])) {\n       must_support_unaligned_dynamic_index = true;\n     }\n@@ -4507,7 +4497,7 @@ LogicalResult vector_store_impl(RewriteContext &ctx, Op store_op,\n   }\n \n   auto add_idx = [&](const Value &v, int64_t d) -> Value {\n-    if (auto cst = getIntConst(v, /*silent=*/true); succeeded(cst)) {\n+    if (auto cst = getIntConst(v)) {\n       return IdxConst(cst.value() + d, builder, op.getLoc());\n     }\n     return builder.create<arith::AddIOp>(v, IdxConst(d, builder, op.getLoc()));\ndiff --git a/jaxlib/mosaic/dialect/tpu/util.h b/jaxlib/mosaic/dialect/tpu/util.h\nindex 000cb4411e62..ac83d95b715e 100644\n--- a/jaxlib/mosaic/dialect/tpu/util.h\n+++ b/jaxlib/mosaic/dialect/tpu/util.h\n@@ -283,7 +283,6 @@ inline arith::ConstantOp I32Const(int32_t value, ArrayRef<int64_t> shape,\n                builder.getIntegerAttr(builder.getI32Type(), value)));\n }\n \n-// TODO(jevinjiang): consolidate this with getIntConst in apply-vector-layout.\n std::optional<int64_t> getIntConst(Value v);\n }  // namespace mlir::tpu\n \n\n```"
        ],
        "from_id": [
            "bythew3i",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28778 from jakevdp:mypy-version-bump\n\nPiperOrigin-RevId: 759668066",
        "output": [
            "```diff\nCommit: 993deb852f531539d0b6376248039971a0af8c7c\nDate: 2025-05-16T17:32:30Z\nURL: https://github.com/jax-ml/jax/commit/993deb852f531539d0b6376248039971a0af8c7c\nFiles changed: 3\nAdditions: +4, Deletions: -4\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex a6697076404f..46deb8eb4879 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -36,7 +36,7 @@ repos:\n   - id: ruff\n \n - repo: https://github.com/pre-commit/mirrors-mypy\n-  rev: 'bbc3dc1f890007061f18f17e2334f216ea9e5df7'  # frozen: v1.14.1\n+  rev: 'f40886d54c729f533f864ed6ce584e920feb0af7'  # frozen: v1.15.0\n   hooks:\n   - id: mypy\n     files: (jax/|tests/typing_test\\.py)\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex aa9bfe9529ce..f26b4222b23b 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -264,7 +264,7 @@ def _get_batched_exception(self) -> BatchedError | None:\n       cur_effect = None\n       for error_effect, code in self._code.items():\n         if self._pred[error_effect][idx]:   # type: ignore\n-          if min_code is None or code[idx] < min_code:\n+          if min_code is None or code[idx] < min_code:  # type: ignore[index]\n             min_code = code[idx]   # type: ignore\n             cur_effect = error_effect\n \ndiff --git a/jax/_src/export/_export.py b/jax/_src/export/_export.py\nindex d5a328bb8e05..c0ca1e108590 100644\n--- a/jax/_src/export/_export.py\n+++ b/jax/_src/export/_export.py\n@@ -760,8 +760,8 @@ def export_sharding(s: LoweringSharding,\n         elif cur_mesh.shape_tuple != sharding.mesh.shape_tuple:\n           raise ValueError(\n               \"Mesh for all inputs/outputs should be equal. Got one mesh \"\n-              f\"{cur_mesh} on an array {cur_arg._aval} at \"\n-              f\"{shape_poly.args_kwargs_path_to_str(cur_k_path)} and another mesh: \"\n+              f\"{cur_mesh} on an array {cur_arg._aval} at \"  # type: ignore[union-attr]\n+              f\"{shape_poly.args_kwargs_path_to_str(cur_k_path)} and another mesh: \"  # type: ignore[arg-type]\n               f\"{sharding.mesh}' on a tensor {arg._aval} at \"\n               f\"{shape_poly.args_kwargs_path_to_str(k_path)}\")\n     if cur_mesh and isinstance(cur_mesh, mesh_lib.Mesh):\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Skip tests that need stdout capture when using pytest\n\nThe prints all go to the stdout captured by pytest instead of being\nintercepted by `jtu`.\n\nPiperOrigin-RevId: 759656514",
        "output": [
            "```diff\nCommit: abda2872ce6562807012ad927534d06b8a515f9c\nDate: 2025-05-16T17:04:09Z\nURL: https://github.com/jax-ml/jax/commit/abda2872ce6562807012ad927534d06b8a515f9c\nFiles changed: 2\nAdditions: +6, Deletions: -0\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 03ded0ac446c..62f377f031a0 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -20,6 +20,7 @@\n import itertools\n import math\n import operator\n+import sys\n import re\n import unittest\n \n@@ -236,6 +237,8 @@ def setUp(self):\n \n   @contextlib.contextmanager\n   def capture_stdout(self):\n+    if \"pytest\" in sys.modules:\n+      self.skipTest(\"pytest interacts badly with GPU stdout capture\")\n     if mosaic_gpu_lib is None:\n       raise ValueError(\"Running tests but missing Mosaic GPU extension\")\n     with jtu.capture_stdout() as stdout:\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 40539463e007..dc35f03843f7 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -19,6 +19,7 @@\n import operator\n import os\n import re\n+import sys\n import tempfile\n from typing import ClassVar\n \n@@ -106,6 +107,8 @@ def pallas_call(self, *args, **kwargs):\n \n   @contextlib.contextmanager\n   def capture_stdout(self):\n+    if \"pytest\" in sys.modules:\n+      self.skipTest(\"pytest interacts badly with GPU stdout capture\")\n     if mosaic_gpu_lib is None:\n       raise ValueError(\"Running tests but missing Mosaic GPU extension\")\n     with jtu.capture_stdout() as stdout:\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas/Mosaic GPU] Fix `get_p` lowering to handle `RefUnion`s correctly.\n\nIn the case of transformed refs, it's possible that a transform ends up\nchanging the dtype of the reference considered. This typically happens when\nextracting an aliased ref out of a `RefUnion`, but it could also happen if\nwe were to handle `RefBitcast`s. As a result, querying the dtype of refs by\nquerying their `aval` is not a safe operation.\n\nPiperOrigin-RevId: 759624846",
        "output": [
            "```diff\nCommit: 997978143b57e9e6a237cd16c596e4e74424c90b\nDate: 2025-05-16T15:28:43Z\nURL: https://github.com/jax-ml/jax/commit/997978143b57e9e6a237cd16c596e4e74424c90b\nFiles changed: 3\nAdditions: +70, Deletions: -27\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 9ead4f16c1a6..b611ea4c17f9 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -812,7 +812,6 @@ def body(launch_ctx: mgpu.LaunchContext, *buffers: ir.Value):\n         mesh=jax_mesh,\n     )\n     del runtime_smem, grouped_barriers, runtime_barriers\n-\n     _ = lower_jaxpr_to_mosaic_gpu(\n         module_ctx, launch_ctx, jaxpr, buffers_gmem, consts\n     )\n@@ -1288,8 +1287,7 @@ def _get_lowering_rule(ctx: LoweringRuleContext, x_ref, *leaves, tree):\n \n   if not isinstance(x_ref, ir.Value) and ir.MemRefType.isinstance(x_ref):\n     raise TypeError(f\"Can only load from references (got {x_ref}).\")\n-\n-  x_aval = ctx.avals_in[0]\n+  dtype = ctx.avals_out[0].dtype\n \n   transforms = jax.tree.unflatten(tree, leaves)\n   x_smem, transforms = _handle_transforms(\n@@ -1300,21 +1298,21 @@ def _get_lowering_rule(ctx: LoweringRuleContext, x_ref, *leaves, tree):\n     case (gpu_core.UnswizzleRef(swizzle), gpu_core.UntileRef(tiling)):\n       if tiling != (\n           8,\n-          (swizzle * 8) // pallas_utils.dtype_bitwidth(x_aval.dtype),\n+          (swizzle * 8) // pallas_utils.dtype_bitwidth(dtype),\n       ):\n         raise NotImplementedError(\"Tiling does not fit swizzle\")\n       return mgpu.FragmentedArray.load_tiled(\n-          x_smem, is_signed=mgpu_utils.is_signed(x_aval.dtype), swizzle=swizzle\n+          x_smem, is_signed=mgpu_utils.is_signed(dtype), swizzle=swizzle\n       )\n     case ():\n       # Handle scalar indexing.\n       if not ctx.avals_out[0].shape:\n-        is_signed = mgpu_utils.is_signed(x_aval.dtype)\n+        is_signed = mgpu_utils.is_signed(dtype)\n         val = memref_dialect.load(x_smem, [])\n         return mgpu.FragmentedArray.splat(val, shape=(), is_signed=is_signed)\n \n       return mgpu.FragmentedArray.load_strided(\n-          x_smem, is_signed=mgpu_utils.is_signed(x_aval.dtype)\n+          x_smem, is_signed=mgpu_utils.is_signed(dtype)\n       )\n     case _:\n       raise NotImplementedError(f\"Unsupported transforms: {transforms}\")\n@@ -1325,12 +1323,11 @@ def _get_lowering_rule_wg(ctx: LoweringRuleContext, x_smem, *leaves, tree):\n   if not isinstance(x_smem, ir.Value) and ir.MemRefType.isinstance(x_smem):\n     raise TypeError(f\"Can only load from references (got {x_smem}).\")\n \n-  x_aval = ctx.avals_in[0]\n-\n   transforms = jax.tree.unflatten(tree, leaves)\n   x_smem, transforms = _handle_transforms(\n       ctx, x_smem, transforms, allow_peer_refs=True\n   )\n+  mlir_dtype = ir.MemRefType(x_smem.type).element_type\n \n   if transforms:\n     raise NotImplementedError(\n@@ -1338,7 +1335,7 @@ def _get_lowering_rule_wg(ctx: LoweringRuleContext, x_smem, *leaves, tree):\n     )\n \n   shape = ctx.avals_out[0].shape\n-  ty = ir.VectorType.get(shape, mgpu_utils.dtype_to_ir_type(x_aval.dtype))\n+  ty = ir.VectorType.get(shape, mlir_dtype)\n   if shape:\n     zero_index = arith_dialect.constant(ir.IndexType.get(), 0)\n     indices = [zero_index for _ in range(len(shape))]\n@@ -1374,7 +1371,8 @@ def _swap_lowering_rule(\n   transforms = jax.tree.unflatten(tree, leaves)\n   transposed_value = value.layout == mgpu.WGMMA_TRANSPOSED_LAYOUT\n   x_smem, transforms = _handle_transforms(\n-      ctx, x_ref, transforms, handle_transposes=not transposed_value, allow_peer_refs=True\n+      ctx, x_ref, transforms, handle_transposes=not transposed_value,\n+      allow_peer_refs=True\n   )\n   mgpu.warpgroup_barrier()  # Make sure reads have completed before we write.\n   match transforms:\n@@ -1437,16 +1435,15 @@ def _swap_lowering_rule_wg(\n   if not ir.MemRefType.isinstance(x_smem.type):\n     raise TypeError(f\"Can only store to references (got {x_smem}).\")\n \n-  x_aval = ctx.avals_in[0]\n-\n   transforms = jax.tree.unflatten(tree, leaves)\n-  x_smem, transforms = _handle_transforms(ctx, x_smem, transforms, allow_peer_refs=True)\n+  x_smem, transforms = _handle_transforms(\n+      ctx, x_smem, transforms, allow_peer_refs=True)\n   if transforms:\n     raise NotImplementedError(\n         \"Transforms are not yet implemented for warpgroup semantics\"\n     )\n-\n-  ty = ir.VectorType.get(shape, mgpu_utils.dtype_to_ir_type(x_aval.dtype))\n+  x_mlir_dtype = ir.MemRefType(x_smem.type).element_type\n+  ty = ir.VectorType.get(shape, x_mlir_dtype)\n   if shape:\n     zero_index = arith_dialect.constant(ir.IndexType.get(), 0)\n     indices = [zero_index for _ in range(len(shape))]\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex f199b7b245c6..0e9319972949 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -85,7 +85,7 @@ def _load_p_lowering_rule(\n   if not isinstance(x_ref, ir.Value) or not ir.MemRefType.isinstance(x_ref.type):\n     raise TypeError(f\"Can only load from references (got {x_ref}).\")\n \n-  x_aval = ctx.avals_in[0]\n+  out_aval = ctx.avals_out[0]\n \n   transforms = jax.tree.unflatten(args_tree, leaves)\n   x_ref, transforms = lowering._handle_transforms(ctx, x_ref, transforms)\n@@ -93,10 +93,10 @@ def _load_p_lowering_rule(\n   if layout is not None:\n     layout = layout.to_mgpu()\n \n-  is_signed = mgpu_utils.is_signed(x_aval.dtype)\n+  is_signed = mgpu_utils.is_signed(out_aval.dtype)\n   match transforms:\n     case (gpu_core.UnswizzleRef(swizzle), gpu_core.UntileRef(tiling)):\n-      if tiling != (8, swizzle // x_aval.dtype.itemsize):\n+      if tiling != (8, swizzle // out_aval.dtype.itemsize):\n         raise NotImplementedError(\"Tiling does not fit swizzle\")\n       return mgpu.FragmentedArray.load_tiled(\n           x_ref,\n@@ -106,8 +106,8 @@ def _load_p_lowering_rule(\n       )\n     case ():\n       # Handle scalar indexing.\n-      if not ctx.avals_out[0].shape:\n-        is_signed = mgpu_utils.is_signed(x_aval.dtype)\n+      if not out_aval.shape:\n+        is_signed = mgpu_utils.is_signed(out_aval.dtype)\n         val = memref_dialect.load(x_ref, [])\n         return mgpu.FragmentedArray.splat(\n             val, shape=(), layout=layout, is_signed=is_signed\n@@ -259,7 +259,9 @@ def _copy_smem_to_gmem_lowering(\n   )\n   src_transforms = src_transforms_treedef.unflatten(flat_src_transforms)\n   dst_transforms = dst_transforms_treedef.unflatten(flat_dst_transforms)\n-  src, src_transforms = lowering._handle_transforms(ctx, src, src_transforms, handle_transposes=False)\n+  src, src_transforms = lowering._handle_transforms(\n+      ctx, src, src_transforms, handle_transposes=False\n+  )\n   copy_params = _extract_gmem_copy_params(dst_transforms) | _extract_smem_copy_params(src_transforms)\n   if ctx.module_ctx.lowering_semantics == mgpu.LoweringSemantics.Lane:\n     ctx.launch_ctx.async_copy(\n@@ -475,7 +477,9 @@ def _copy_gmem_to_smem_lowering(\n   )\n   src_transforms = src_transforms_treedef.unflatten(flat_src_transforms)\n   dst_transforms = dst_transforms_treedef.unflatten(flat_dst_transforms)\n-  dst, dst_transforms = lowering._handle_transforms(ctx, dst, dst_transforms, handle_transposes=False)\n+  dst, dst_transforms = lowering._handle_transforms(\n+      ctx, dst, dst_transforms, handle_transposes=False\n+  )\n   copy_params = _extract_smem_copy_params(dst_transforms) | _extract_gmem_copy_params(src_transforms)\n   barrier_indexer = _extract_barrier_indexer(\n       barrier_transforms_treedef.unflatten(flat_barrier_transforms)\n@@ -921,7 +925,6 @@ def _wgmma_lowering(\n     a_transforms_tree,\n     b_transforms_tree,\n ):\n-  _, a_aval, *_ = ctx.avals_in\n   lhs_swizzle: int | None = None\n   if a_transforms_tree is not None:\n     a_transforms_leaves, b_transforms_leaves = util.split_list(\n@@ -942,7 +945,8 @@ def _wgmma_lowering(\n         lhs_transpose = True\n       case _:\n         raise ValueError(f\"WGMMA lhs has unsupported transforms: {a_transforms}.\")\n-    swizzle_elems = lhs_swizzle // a_aval.dtype.itemsize\n+    a_mlir_dtype = ir.MemRefType(a.type).element_type\n+    swizzle_elems = lhs_swizzle // mgpu_utils.bytewidth(a_mlir_dtype)\n     if tiling != (8, swizzle_elems):\n       raise NotImplementedError(\"WGMMA lhs tiling does not fit swizzle\")\n   else:\n@@ -991,7 +995,8 @@ def _wgmma_lowering(\n       raise ValueError(f\"WGMMA rhs has unsupported transforms: {b_transforms}.\")\n \n   if lhs_swizzle is not None:\n-    swizzle_elems = rhs_swizzle // a_aval.dtype.itemsize\n+    b_mlir_dtype = ir.MemRefType(b.type).element_type\n+    swizzle_elems = rhs_swizzle // mgpu_utils.bytewidth(b_mlir_dtype)\n     if rhs_swizzle != lhs_swizzle:\n       raise NotImplementedError(\"WGMMA rhs swizzle must match lhs swizzle\")\n     if rhs_tiling != (8, swizzle_elems):\n@@ -1917,7 +1922,9 @@ def _inline_mgpu_lowering_rule(\n       assert transforms is None\n       continue\n     assert isinstance(aval, pallas_core.AbstractMemoryRef)\n-    a, user_transforms = lowering._handle_transforms(ctx, a, transforms, handle_transposes=False)\n+    a, user_transforms = lowering._handle_transforms(\n+        ctx, a, transforms, handle_transposes=False\n+    )\n     # Transforms that do not originate from a MemoryRefTransform are\n     # applied implicitly (eg by emit-pipeline) and therefore we do not\n     # expect the user to pass them to the type. The transforms not\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 9d259097f90a..40539463e007 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1671,6 +1671,45 @@ def unpack_i4_as_i8(x):\n     test_as_i8 = jax.lax.convert_element_type(kernel(x), new_dtype=jnp.int8)\n     np.testing.assert_array_equal(test_as_i8[:256], unpack_i4_as_i8(x))\n \n+  def test_smem_aliasing_works_for_quantization(self):\n+    self.skip_if_wg_semantics()\n+    shape = (64, 256)\n+    large_ty, small_ty = jnp.bfloat16, jnp.uint4\n+    large_swizzle = plgpu.SwizzleTransform(64 * jnp.finfo(large_ty).bits // 8)\n+    small_swizzle = plgpu.SwizzleTransform(64 * jnp.iinfo(small_ty).bits // 8)\n+    tiling = plgpu.TilingTransform((8, 64))\n+\n+    def kernel(x_gmem, o_gmem):\n+      return pl.run_scoped(\n+          functools.partial(scoped_kernel, x_gmem, o_gmem),\n+          plgpu.RefUnion(\n+              plgpu.SMEM(shape, large_ty, transforms=(tiling, large_swizzle)),\n+              plgpu.SMEM(shape, small_ty, transforms=(tiling, small_swizzle))\n+          ),\n+          plgpu.Barrier(1, num_barriers=1),\n+      )\n+\n+    def scoped_kernel(x_gmem, o_gmem, aliased_ref, barrier):\n+      ref_large_ty, ref_small_ty = aliased_ref\n+      plgpu.copy_gmem_to_smem(x_gmem, ref_small_ty, barrier=barrier)\n+      plgpu.barrier_wait(barrier)\n+      ref_large_ty[...] = ref_small_ty[...].astype(ref_large_ty.dtype) * 3\n+      plgpu.commit_smem()\n+      plgpu.copy_smem_to_gmem(ref_large_ty, o_gmem)\n+      plgpu.wait_smem_to_gmem(0, wait_read_only=True)\n+\n+    kernel_fn = self.pallas_call(\n+        kernel,\n+        in_specs=[pl.BlockSpec(memory_space=plgpu.GMEM)],\n+        out_specs=pl.BlockSpec(memory_space=plgpu.GMEM),\n+        out_shape=jax.ShapeDtypeStruct(shape, large_ty),\n+        grid=(1, 1),\n+    )\n+    key = jax.random.key(42)\n+    x = jax.random.randint(key, shape, 0, 4).astype(small_ty)\n+    expected = x * 3\n+    np.testing.assert_array_equal(kernel_fn(x), expected)\n+\n   def test_assigning_to_ref_union_raises(self):\n     @functools.partial(\n         self.pallas_call,\n\n```"
        ],
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Strip leading zeros from ML_WHEEL_GIT_HASH.\n\nThey end up being stripped by setuptools, which leads to a mismatch\nbetween expected and actual wheel names, which is fatal, as Bazel is\nexpecting only a particular name, not to mention other issues.\n\nhttps://peps.python.org/pep-0440/\n\nPiperOrigin-RevId: 759610274",
        "output": [
            "```diff\nCommit: 6523226e701ee08ed9e65f28c74dd01c8a49728f\nDate: 2025-05-16T14:41:54Z\nURL: https://github.com/jax-ml/jax/commit/6523226e701ee08ed9e65f28c74dd01c8a49728f\nFiles changed: 1\nAdditions: +4, Deletions: -1\ndiff --git a/build/build.py b/build/build.py\nindex 4a7c745ce9d9..b65f7a49dd8f 100755\n--- a/build/build.py\n+++ b/build/build.py\n@@ -637,7 +637,10 @@ async def main():\n       if \"ML_WHEEL_BUILD_DATE\" in option:\n         wheel_build_date = option.split(\"=\")[-1].replace(\"-\", \"\")\n       if \"ML_WHEEL_GIT_HASH\" in option:\n-        wheel_git_hash = option.split(\"=\")[-1][:9]\n+        # Strip leading zeros as they end up being stripped by setuptools,\n+        # which leads to a mismatch between expected and actual wheel names\n+        # https://peps.python.org/pep-0440/\n+        wheel_git_hash = option.split(\"=\")[-1][:9].lstrip('0')\n \n   with open(\".jax_configure.bazelrc\", \"w\") as f:\n     jax_configure_options = utils.get_jax_configure_bazel_options(wheel_build_command_base.get_command_as_list(), args.use_new_wheel_build_rule)\n\n```"
        ],
        "from_id": [
            "belitskiy",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[mosaic_gpu] Added support for attaching source information to the PTX\n\nThe implementation currently forces O=0 due to a suspected bug in the NVPTX\nbackend.\n\nTo get source information\n\n* Set MOSAIC_GPU_LINE_INFO=1\n* Run with --jax_include_full_tracebacks_in_locations=true\n\nPiperOrigin-RevId: 759608368",
        "output": [
            "```diff\nCommit: 73be65e1bc35f754db3b994f7cf4cd2556269646\nDate: 2025-05-16T14:35:24Z\nURL: https://github.com/jax-ml/jax/commit/73be65e1bc35f754db3b994f7cf4cd2556269646\nFiles changed: 2\nAdditions: +25, Deletions: -7\ndiff --git a/jaxlib/mosaic/gpu/BUILD b/jaxlib/mosaic/gpu/BUILD\nindex 115d0c47cc52..66f13bdac7f5 100644\n--- a/jaxlib/mosaic/gpu/BUILD\n+++ b/jaxlib/mosaic/gpu/BUILD\n@@ -149,8 +149,6 @@ cc_library(\n         \":nvshmem\",\n         \":passes\",\n         \":target\",\n-        \"//jaxlib/cuda:cuda_vendor\",\n-        \"//jaxlib/mosaic/dialect/gpu:mosaic_gpu\",\n         \"@com_google_absl//absl/base\",\n         \"@com_google_absl//absl/base:core_headers\",\n         \"@com_google_absl//absl/cleanup\",\n@@ -200,11 +198,16 @@ cc_library(\n         \"@llvm-project//mlir:UBToLLVM\",\n         \"@llvm-project//mlir:VectorDialect\",\n         \"@llvm-project//mlir:VectorToLLVM\",\n-        \"@tsl//tsl/profiler/lib:traceme\",\n+        \"//jaxlib/cuda:cuda_vendor\",\n+        \"//jaxlib/mosaic/dialect/gpu:mosaic_gpu\",\n         \"@xla//xla/ffi\",\n         \"@xla//xla/ffi:ffi_api\",\n         \"@xla//xla/service:custom_call_status\",\n         \"@xla//xla/service:custom_call_target_registry\",\n+        \"@tsl//tsl/profiler/lib:traceme\",\n+        # TODO(slebedev): Remove once enable-line-info is merged into the upstream\n+        # ensure-debug-info-scope-on-llvm-func pass in MLIR.\n+        \"@triton//:TritonLLVMIR\",\n     ],\n     alwayslink = True,\n )\ndiff --git a/jaxlib/mosaic/gpu/custom_call.cc b/jaxlib/mosaic/gpu/custom_call.cc\nindex 7c93d54aff9e..27175c3773e6 100644\n--- a/jaxlib/mosaic/gpu/custom_call.cc\n+++ b/jaxlib/mosaic/gpu/custom_call.cc\n@@ -100,6 +100,7 @@ limitations under the License.\n #include \"xla/service/custom_call_status.h\"\n #include \"xla/service/custom_call_target_registry.h\"\n #include \"tsl/profiler/lib/traceme.h\"\n+#include \"triton/Target/LLVMIR/Passes.h\"\n \n namespace {\n \n@@ -174,8 +175,10 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n     mosaic::gpu::registerConvertGpuToLLVMPass();\n     mosaic::gpu::registerByvalInsertionPass();\n     mlir::arith::registerArithExpandOpsPass();\n+    mlir::registerLLVMDIScopePass();\n     return true;\n   });\n+  bool emit_line_info = getenv(\"MOSAIC_GPU_LINE_INFO\") != nullptr;\n   return mlir::parsePassPipeline(absl::StrCat(\n       R\"(\n       builtin.module(\n@@ -188,23 +191,35 @@ mlir::FailureOr<mlir::OpPassManager> GetPassPipeline(\n         convert-scf-to-cf,\n         convert-nvvm-to-llvm,\n         expand-strided-metadata,\n-        nvvm-attach-target{O=3 chip=)\",\n-      sm, R\"( fast=false features=+)\", ptx_isa,\n+        nvvm-attach-target{)\",\n+      // TODO(slebedev): Always use O=3 once\n+      // https://github.com/llvm/llvm-project/pull/140146 is merged.\n+      emit_line_info ? \"O=0\" : \"O=3\", \" chip=\", sm, \" fast=false features=+\",\n+      ptx_isa,\n       R\"( ftz=false  module= triple=nvptx64-nvidia-cuda},\n         lower-affine,\n         convert-arith-to-llvm{index-bitwidth=0},\n         convert-index-to-llvm{index-bitwidth=64},\n         canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true},\n         cse,\n-        gpu.module(strip-debuginfo),\n+        )\",\n+      emit_line_info ? \"\" : \"gpu.module(strip-debuginfo),\",\n+      R\"(\n         gpu.module(convert-gpu-to-nvvm{has-redux=false index-bitwidth=64 use-bare-ptr-memref-call-conv=false}),\n         gpu.module(canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true}),\n         gpu.module(cse),\n         gpu.module(mosaic-byval-insertion),\n         gpu.module(reconcile-unrealized-casts),\n         mosaic-convert-gpu-to-llvm,\n+        )\",\n+      // TODO(slebedev): Switch to the ensure-debug-info-scope-on-llvm-func\n+      // pass in MLIR once Triton upstreams its changes.\n+      emit_line_info ? \"enable-line-info,\" : \"\",\n+      R\"(\n         gpu-module-to-binary{format=)\" +\n-      mlir::gpu::stringifyCompilationTarget(target).str() + (!nvshmem_path.empty() ? R\"( l=)\" + nvshmem_path : \"\")  + R\"(},\n+          mlir::gpu::stringifyCompilationTarget(target).str() +\n+          (!nvshmem_path.empty() ? R\"( l=)\" + nvshmem_path : \"\") +\n+          (emit_line_info ? \"  opts=-lineinfo\" : \"\") + R\"(},\n         convert-math-to-llvm{approximate-log1p=true},\n         canonicalize{max-iterations=10 max-num-rewrites=-1 region-simplify=normal test-convergence=false top-down=true},\n         cse,\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[JAX] Add ticks around input to clearify in the error message. Currently it looks like this.\n\n```\nValueError: Pytree for `in_specs` and inputs do not match. There are 1 mismatches, including:\n    * `in_specs` is a tuple of length 1 but inputs is a tuple of length 4, so the lengths do not match\n\n```\n\nPiperOrigin-RevId: 759499528",
        "output": [
            "```diff\nCommit: 8f919a17f7f19555a911cc76ce4c9722a5228208\nDate: 2025-05-16T08:09:07Z\nURL: https://github.com/jax-ml/jax/commit/8f919a17f7f19555a911cc76ce4c9722a5228208\nFiles changed: 2\nAdditions: +3, Deletions: -3\ndiff --git a/jax/_src/pallas/core.py b/jax/_src/pallas/core.py\nindex f68393a7de54..709bb4640241 100644\n--- a/jax/_src/pallas/core.py\n+++ b/jax/_src/pallas/core.py\n@@ -1077,7 +1077,7 @@ def get_grid_mapping(\n     if in_specs_tree != in_tree:\n       raise ValueError(\n           pytreedef_mismatch_err_msg(\"`in_specs`\", in_specs_tree,\n-                                     \"inputs\", in_tree))\n+                                     \"`inputs`\", in_tree))\n   else:\n     flat_in_specs = [no_block_spec] * len(in_avals)\n \ndiff --git a/tests/pallas/pallas_test.py b/tests/pallas/pallas_test.py\nindex 725b3adb4388..1114153b16c2 100644\n--- a/tests/pallas/pallas_test.py\n+++ b/tests/pallas/pallas_test.py\n@@ -1081,10 +1081,10 @@ def test_pallas_call_in_specs_mismatch_inputs(self):\n                                    pl.BlockSpec((4,), lambda: 0)])\n     with self.assertRaisesRegex(\n         ValueError,\n-        re.compile(\"Pytree for `in_specs` and inputs do not match. \"\n+        re.compile(\"Pytree for `in_specs` and `inputs` do not match. \"\n                    \"There are 1 mismatches, including:\"\n                    \".* at \\\\[1\\\\], `in_specs` is a pytree leaf but \"\n-                   \"inputs is a.*\", re.DOTALL)):\n+                   \"`inputs` is a.*\", re.DOTALL)):\n       f(a, dict(a=a))\n \n   def test_pallas_call_index_map_wrong_number_of_arguments(self):\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28781 from mattjj:hijax\n\nPiperOrigin-RevId: 759336328",
        "output": [
            "```diff\nCommit: 5b2f399c6befde3b23210613fe1c021da8886c50\nDate: 2025-05-15T22:55:27Z\nURL: https://github.com/jax-ml/jax/commit/5b2f399c6befde3b23210613fe1c021da8886c50\nFiles changed: 7\nAdditions: +434, Deletions: -35\ndiff --git a/jax/_src/ad_util.py b/jax/_src/ad_util.py\nindex 8cfd7b214338..4e9616e48375 100644\n--- a/jax/_src/ad_util.py\n+++ b/jax/_src/ad_util.py\n@@ -31,6 +31,9 @@\n map = safe_map\n \n def add_jaxvals(x: ArrayLike, y: ArrayLike) -> Array:\n+  ty = core.typeof(x)\n+  if hasattr(ty, 'vspace_add'):  # TODO(mattjj,dougalm): revise away hasattr\n+    return ty.vspace_add(x, y)\n   x, y = core.standard_insert_pvary(x, y)\n   return add_jaxvals_p.bind(x, y)\n \n@@ -48,6 +51,8 @@ def add_abstract(x, y):\n   return x\n \n def zeros_like_aval(aval: core.AbstractValue) -> Array:\n+  if hasattr(aval, 'vspace_zero'):  # TODO(mattjj,dougalm): revise away hasattr\n+    return aval.vspace_zero()\n   return aval_zeros_likers[type(aval)](aval)\n aval_zeros_likers: dict[type, Callable[[Any], Array]] = {}\n \ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 154ff5132a39..33802e494304 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -540,17 +540,18 @@ def _check_input_dtype_revderiv(name, holomorphic, allow_int, x):\n     if not dtypes.issubdtype(aval.dtype, np.complexfloating):\n       raise TypeError(f\"{name} with holomorphic=True requires inputs with complex dtype, \"\n                       f\"but got {aval.dtype.name}.\")\n-  if (dtypes.issubdtype(aval.dtype, dtypes.extended) or\n-      dtypes.issubdtype(aval.dtype, np.integer) or\n-      dtypes.issubdtype(aval.dtype, np.bool_)):\n-    if not allow_int:\n-      raise TypeError(f\"{name} requires real- or complex-valued inputs (input dtype \"\n-                      f\"that is a sub-dtype of np.inexact), but got {aval.dtype.name}. \"\n-                      \"If you want to use Boolean- or integer-valued inputs, use vjp \"\n-                      \"or set allow_int to True.\")\n-  elif not dtypes.issubdtype(aval.dtype, np.inexact):\n-    raise TypeError(f\"{name} requires numerical-valued inputs (input dtype that is a \"\n-                    f\"sub-dtype of np.bool_ or np.number), but got {aval.dtype.name}.\")\n+  if isinstance(aval, ShapedArray):\n+    if (dtypes.issubdtype(aval.dtype, dtypes.extended) or\n+        dtypes.issubdtype(aval.dtype, np.integer) or\n+        dtypes.issubdtype(aval.dtype, np.bool_)):\n+      if not allow_int:\n+        raise TypeError(f\"{name} requires real- or complex-valued inputs (input dtype \"\n+                        f\"that is a sub-dtype of np.inexact), but got {aval.dtype.name}. \"\n+                        \"If you want to use Boolean- or integer-valued inputs, use vjp \"\n+                        \"or set allow_int to True.\")\n+    elif not dtypes.issubdtype(aval.dtype, np.inexact):\n+      raise TypeError(f\"{name} requires numerical-valued inputs (input dtype that is a \"\n+                      f\"sub-dtype of np.bool_ or np.number), but got {aval.dtype.name}.\")\n _check_input_dtype_grad = partial(_check_input_dtype_revderiv, \"grad\")\n \n def _check_output_dtype_revderiv(name, holomorphic, x):\n@@ -1873,6 +1874,7 @@ def _jvp(fun: lu.WrappedFun, primals, tangents, has_aux=False):\n                     f\"structure; primals have tree structure {tree_def} whereas tangents have \"\n                     f\"tree structure {tree_def_2}.\")\n   for p, t in zip(ps_flat, ts_flat):\n+    if not isinstance(core.typeof(p), ShapedArray): continue\n     if core.primal_dtype_to_tangent_dtype(_dtype(p)) != _dtype(t):\n       raise TypeError(\"primal and tangent arguments to jax.jvp do not match; \"\n                       \"dtypes must be equal, or in case of int/bool primal dtype \"\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex c730e1c289ae..ab97c7ff1c2c 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,7 +88,7 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info']\n+               '_effects', '_debug_info', '_is_high', '_mut_types']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -96,6 +96,8 @@ class Jaxpr:\n   _eqns: list[JaxprEqn]\n   _effects: Effects\n   _debug_info: DebugInfo\n+  _is_high: bool\n+  _mut_types: dict[Var, Any]\n \n   @property\n   def constvars(self) -> list[Var]:\n@@ -121,6 +123,14 @@ def effects(self) -> Effects:\n   def debug_info(self) -> DebugInfo:\n     return self._debug_info\n \n+  @property\n+  def is_high(self) -> bool:\n+    return self._is_high\n+\n+  @property\n+  def mut_types(self) -> dict[Var, Any]:\n+    return self._mut_types\n+\n   def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                outvars: Sequence[Atom], eqns: Sequence[JaxprEqn],\n                effects: Effects = no_effects,\n@@ -128,6 +138,8 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # compatibility we have to allow calls when the debug_info\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n+               is_high: bool = False,\n+               mut_types: dict | None = None,\n                ):\n     \"\"\"\n     Args:\n@@ -152,6 +164,8 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # TODO(necula): re-enable these safety checks\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n+    self._is_high = is_high\n+    self._mut_types = mut_types or {}\n \n   def __str__(self):\n     return str(self.pretty_print())\n@@ -178,6 +192,8 @@ def replace(self, **kwargs):\n         eqns=kwargs.pop(\"eqns\", self.eqns),\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n+        is_high=kwargs.pop(\"is_high\", self.is_high),\n+        mut_types=kwargs.pop(\"mut_types\", self.mut_types),\n     )\n     if kwargs:\n       raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n@@ -517,7 +533,7 @@ def _true_bind(self, *args, **params):\n     for arg in args:\n       if isinstance(arg, Tracer) and not arg._trace.is_valid():\n         raise escaped_tracer_error(arg)\n-    # TODO: figure out how to handle function arguments\n+    # TODO: figure out how to handle function arguments for this assert\n     # assert (not config.enable_checks.value or\n     #         all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n \n@@ -525,6 +541,10 @@ def _true_bind(self, *args, **params):\n     # is called frequently and it's slightly faster to avoid using a context\n     # manager object.\n     prev_trace = trace_ctx.trace\n+\n+    if self.is_high(**params) and prev_trace.requires_low:\n+      return self.to_lojax(*args, **params)  # type: ignore\n+\n     trace_ctx.set_trace(eval_trace)\n     try:\n       return self.bind_with_trace(prev_trace, args, params)\n@@ -561,6 +581,9 @@ def abstract_eval(self, *args, **params):\n   def get_bind_params(self, params):\n     return [], params\n \n+  def is_high(self, **params) -> bool:\n+    return False\n+\n \n def _effect_free_abstract_eval(abstract_eval):\n   def abstract_eval_(*args, **kwargs):\n@@ -627,12 +650,13 @@ def check_avals_context_mesh(avals, prim_name):\n TracerType = TypeVar('TracerType', bound='Tracer')\n \n class Trace(Generic[TracerType]):\n-  __slots__ = (\"__weakref__\", \"_invalidated\", \"_weakref\")\n+  __slots__ = (\"__weakref__\", \"_invalidated\", \"_weakref\", \"requires_low\")\n \n   def __init__(self):\n     self._invalidated = False\n     # We frequently need a weakref to a trace, so let's precompute one.\n     self._weakref = weakref.ref(self)\n+    self.requires_low = True\n \n   def process_primitive(self, primitive, tracers, params):\n     raise NotImplementedError(\"must override\")\n@@ -1445,6 +1469,8 @@ def definitely_equal(x, y):\n \n class AbstractValue:\n   __slots__: list[str] = []\n+  is_high = False\n+  mutable = False\n \n   def to_tangent_aval(self):\n     raise NotImplementedError(\"must override\")\n@@ -1948,6 +1974,10 @@ def __init__(self, shape, dtype, weak_type=False, *, sharding=None,\n     self.sharding = get_sharding(sharding, self.shape)\n     self.vma = get_vma(vma, self.sharding.mesh)\n \n+  def lower_val(self, val): return [val]\n+  def raise_val(self, val): return val\n+  def lo_ty(self): return [self]\n+\n   def update(self, shape=None, dtype=None, weak_type=None, **kwargs):\n     if shape is None:\n       shape = self.shape\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 45705382efa0..090022c9b6a4 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -73,6 +73,7 @@ def jvp(fun: lu.WrappedFun, has_aux=False, instantiate=True,\n def jvpfun(f: Callable, instantiate, transform_stack, primals, tangents):\n   tag = core.TraceTag()\n   tangents = [Zero.from_primal_value(t) if not isinstance(t, Zero)\n+              and isinstance(core.typeof(t), core.ShapedArray)\n               and dtype(t) == float0 else t for t in tangents]\n   ctx = (source_info_util.transform_name_stack('jvp') if transform_stack\n          else contextlib.nullcontext())\n@@ -475,6 +476,7 @@ def __init__(self, parent_trace, tag):\n     super().__init__()\n     self.tag = tag\n     self.parent_trace = parent_trace\n+    self.requires_low = False\n \n   def to_primal_tangent_pair(self, val):\n     if isinstance(val, JVPTracer) and val._trace.tag is self.tag:\n@@ -606,7 +608,8 @@ def process_custom_transpose(self, prim, call, tracers, **params):\n     return map(partial(maybe_jvp_tracer, self), ps_out, ts_out)\n \n def maybe_jvp_tracer(trace, primal, tangent):\n-  if type(tangent) is Zero or dtype(tangent) == float0:\n+  if (type(tangent) is Zero or\n+      core.typeof(tangent) is core.ShapedArray and dtype(tangent) == float0):\n     return primal\n   else:\n     return JVPTracer(trace, primal, tangent)\n@@ -641,6 +644,7 @@ def _primal_tangent_shapes_match(primal, tangent):\n   if type(tangent) is not Zero:\n     primal_aval = get_aval(primal).strip_weak_type()\n     tangent_aval = get_aval(tangent).strip_weak_type()\n+    if not isinstance(primal_aval, core.ShapedArray): return  # TODO(mattjj,dougalm)\n     assert core.definitely_equal_shape(primal_aval.shape, tangent_aval.shape), (primal_aval.shape, tangent_aval.shape)\n     expected_tangent_dtype = core.primal_dtype_to_tangent_dtype(primal_aval.dtype)\n     assert expected_tangent_dtype == tangent_aval.dtype, (expected_tangent_dtype, tangent_aval.dtype)\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 5866b0c5f8eb..9e875f43d831 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -155,6 +155,7 @@ def __init__(self, parent_trace:Trace, name_stack: source_info_util.NameStack, t\n     self.name_stack = name_stack\n     self.tag = tag\n     self.parent_trace = parent_trace\n+    self.requires_low = False\n \n   def to_jaxpr_tracer(self, x):\n     if isinstance(x, JaxprTracer) and x._trace.tag is self.tag:\n@@ -899,9 +900,8 @@ def convert_envvars_to_constvars(jaxpr: Jaxpr, num_env_vars: int) -> Jaxpr:\n     raise NotImplementedError\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   env_vars, invars = split_list(jaxpr.invars, [num_env_vars])\n-  converted_jaxpr = Jaxpr(constvars=jaxpr.constvars + env_vars,\n-                          invars=invars, outvars=jaxpr.outvars, eqns=jaxpr.eqns,\n-                          effects=jaxpr.effects, debug_info=jaxpr.debug_info)\n+  converted_jaxpr = jaxpr.replace(constvars=jaxpr.constvars + env_vars,\n+                                  invars=invars)\n   config.enable_checks.value and core.check_jaxpr(converted_jaxpr)\n   return converted_jaxpr\n \n@@ -1173,6 +1173,7 @@ def has_effects(effects) -> bool:\n   out_unknowns = map(op.or_, out_unknowns, ensure_out_unknowns)\n   out_inst     = map(op.or_, out_inst,     ensure_out_inst)\n \n+\n   ins_known, _ = partition_list(in_unknowns, jaxpr.invars)\n   outs_known, _ = partition_list(out_unknowns, jaxpr.outvars)\n   ref_res_is_input = [r in ins_known for r in residual_refs]\n@@ -1181,8 +1182,14 @@ def has_effects(effects) -> bool:\n   known_outvars = [*outs_known, *residuals]\n   known_effects = make_jaxpr_effects(jaxpr.constvars, ins_known_and_ref_res,\n                                      known_outvars, known_eqns)\n-  jaxpr_known = Jaxpr(jaxpr.constvars, ins_known_and_ref_res, known_outvars,\n-                      known_eqns, known_effects, jaxpr.debug_info)\n+  known_mut, staged_mut, ins_known_ = {}, {}, set(ins_known)  # type: ignore\n+  for v, t in jaxpr.mut_types.items():\n+    [staged_mut, known_mut][v in ins_known_][v] = t\n+\n+  # TODO(mattjj,necula): debug info should be updated here\n+  jaxpr_known = jaxpr.replace(\n+      invars=ins_known_and_ref_res, outvars=known_outvars,\n+      eqns=known_eqns, effects=known_effects, mut_types=known_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_known)\n \n   _, ins_staged = partition_list(in_inst, jaxpr.invars)\n@@ -1190,9 +1197,10 @@ def has_effects(effects) -> bool:\n   staged_invars = [*residuals, *non_input_res_refs, *ins_staged]\n   staged_effects = make_jaxpr_effects(jaxpr.constvars, staged_invars,\n                                       outs_staged, staged_eqns)\n-  jaxpr_staged = Jaxpr(jaxpr.constvars, staged_invars,\n-                       outs_staged, staged_eqns, staged_effects,\n-                       jaxpr.debug_info)\n+  # TODO(mattjj,necula): debug info should be updated here\n+  jaxpr_staged = jaxpr.replace(\n+      invars=staged_invars, outvars=outs_staged, eqns=staged_eqns,\n+      effects=staged_effects, mut_types=staged_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_staged)\n \n   return (jaxpr_known, jaxpr_staged, out_unknowns, out_inst, len(residuals),\n@@ -1483,7 +1491,8 @@ def write(x: Atom, b: bool) -> None:\n       jaxpr.debug_info.traced_for, jaxpr.debug_info.func_src_info,\n       jaxpr.debug_info.filter_arg_names(used_inputs),\n       jaxpr.debug_info.filter_result_paths(used_outputs))\n-  new_jaxpr = Jaxpr(jaxpr.constvars, invars, outvars, eqns, jaxpr_effects, dbg)\n+  new_jaxpr = jaxpr.replace(invars=invars, outvars=outvars, eqns=eqns,\n+                            effects=jaxpr_effects, debug_info=dbg)\n   config.enable_checks.value and core.check_jaxpr(new_jaxpr)\n \n   return new_jaxpr, used_inputs\n@@ -1561,9 +1570,8 @@ def _move_binders_to_front(closed_jaxpr: ClosedJaxpr, to_move: tuple[bool, ...]\n   new_invars = _move_to_front(invars, to_move)\n   new_effs = _renumber_effects(\n       (*constvars, *new_invars), (*constvars, *invars), closed_jaxpr.jaxpr.effects)\n-  new_jaxpr = Jaxpr(constvars, new_invars, closed_jaxpr.jaxpr.outvars,\n-                    closed_jaxpr.jaxpr.eqns, new_effs,\n-                    closed_jaxpr.jaxpr.debug_info)\n+  new_jaxpr = closed_jaxpr.jaxpr.replace(\n+      constvars=constvars, invars=new_invars, effects=new_effs)\n   new_closed_jaxpr = core.ClosedJaxpr(new_jaxpr, closed_jaxpr.consts)\n   return new_closed_jaxpr\n \n@@ -1704,6 +1712,7 @@ class JaxprStackFrame:\n   attrs_inits: list\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n+  is_high: bool\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1718,6 +1727,7 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_inits = []\n     self.attrs_vars = []\n     self.debug_info = debug_info\n+    self.is_high = False\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1743,8 +1753,9 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n+    mut_types = {v: v.aval for v in invars if v.aval.mutable} if self.is_high else {}\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info)\n+                  debug_info, self.is_high, mut_types)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1831,8 +1842,9 @@ def vars(atom: Atom) -> list[Var]:\n class DynamicJaxprTrace(core.Trace):\n   __slots__ = (\"frame\", \"tag\")\n \n-  def __init__(self, debug_info: core.DebugInfo):\n+  def __init__(self, debug_info: core.DebugInfo, lower=False):\n     super().__init__()\n+    self.requires_low = lower\n     self.frame = JaxprStackFrame(debug_info)\n \n   def invalidate(self):\n@@ -2193,10 +2205,11 @@ def trace_to_jaxpr_dynamic(\n     in_avals: Sequence[AbstractValue],\n     *,\n     keep_inputs: list[bool] | None = None,\n+    lower: bool = False,\n ) -> tuple[Jaxpr, list[AbstractValue], list[Any],\n            list[tuple[PyTreeDef, PyTreeDef, tuple[Any, str, AttrKind]]]]:\n   keep_inputs = [True] * len(in_avals) if keep_inputs is None else keep_inputs\n-  trace = DynamicJaxprTrace(fun.debug_info)\n+  trace = DynamicJaxprTrace(fun.debug_info, lower=lower)\n   with core.ensure_no_leaks(trace), source_info_util.reset_name_stack():\n     source_info = source_info_util.current()\n     in_tracers = _input_type_to_tracers(\n@@ -2418,8 +2431,7 @@ def _add_implicit_outputs(jaxpr: Jaxpr) -> tuple[Jaxpr, OutputType]:\n   kept_outs = [False] * len(impl_outvars) + [True] * len(expl_outvars)\n   out_type = tuple(zip(out_avals, kept_outs))\n \n-  new_jaxpr = Jaxpr(jaxpr.constvars, jaxpr.invars, outvars, jaxpr.eqns,\n-                    jaxpr.effects, jaxpr.debug_info)\n+  new_jaxpr = jaxpr.replace(outvars=outvars)\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   return new_jaxpr, out_type\n \n@@ -2663,3 +2675,25 @@ def _linearize_of_pmap_hack(f: lu.WrappedFun, jaxpr, consts) -> tuple[Jaxpr, lis\n     _, jaxpr = f.f.closure\n     return convert_constvars_jaxpr(jaxpr), []\n   return jaxpr, consts\n+\n+\n+@weakref_lru_cache\n+def lower_jaxpr(hi_jaxpr):\n+  in_avals = [lo_ty for t in hi_jaxpr.in_avals for lo_ty in t.lo_ty()]\n+  f = lu.wrap_init(partial(lower_traceable, hi_jaxpr),\n+                   debug_info=hi_jaxpr.jaxpr.debug_info)\n+  lo_jaxpr, _, consts, () = trace_to_jaxpr_dynamic(f, in_avals, lower=True)\n+  return core.ClosedJaxpr(lo_jaxpr, consts)\n+\n+def lower_traceable(jaxpr, *lo_args):\n+  lo_args_ = iter(lo_args)\n+  hi_args = [t.raise_val(*it.islice(lo_args_, len(t.lo_ty())))\n+             for t in jaxpr.in_avals]\n+  assert (problem := next(lo_args_, None)) is None\n+  hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n+  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n+  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.mut_types.items()\n+              for lo_val in ty.get(hi_args[in_idx[v]])]\n+  lo_outs = [lo_val for t, hi_val in zip(jaxpr.out_avals, hi_outs)\n+             for lo_val in t.lower_val(hi_val)]\n+  return mut_outs + lo_outs\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex ca77b659a08d..5edd74fe74ef 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -20,7 +20,7 @@\n import dataclasses\n from functools import partial\n import inspect\n-import itertools\n+import itertools as it\n import logging\n import weakref\n from typing import NamedTuple, Any, Union, cast\n@@ -188,7 +188,8 @@ def _python_pjit_helper(fun: Callable, jit_info: PjitInfo, *args, **kwargs):\n     args_flat = [*init_states, *args_flat]\n \n   try:\n-    if core.trace_state_clean() and not config.debug_key_reuse.value:\n+    if (core.trace_state_clean() and not config.debug_key_reuse.value\n+        and not p.params['jaxpr'].jaxpr.is_high):\n       args_flat = map(core.full_lower, args_flat)\n       core.check_eval_args(args_flat)\n       out_flat, compiled, profiler = _pjit_call_impl_python(*args_flat, **p.params)\n@@ -1592,6 +1593,36 @@ def check_aval_layout_compatibility(\n pjit_p.multiple_results = True\n pjit_p.skip_canonicalization = True\n \n+def _is_high(jaxpr, **_) -> bool:\n+  return jaxpr.jaxpr.is_high\n+pjit_p.is_high = _is_high  # type: ignore\n+\n+def _to_lojax(*hi_args, jaxpr, out_shardings, out_layouts, **params):\n+  num_mut = [len(ty.lo_ty()) for ty in jaxpr.jaxpr.mut_types.values()]\n+  out_shardings = (UNSPECIFIED,) * sum(num_mut) + out_shardings\n+  out_layouts = (None,) * sum(num_mut) + out_layouts\n+\n+  lo_args = [lo_val for t, hi_val in zip(jaxpr.in_avals, hi_args)\n+             for lo_val in t.lower_val(hi_val)]\n+  lo_jaxpr = pe.lower_jaxpr(jaxpr)\n+  all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, out_shardings=out_shardings,\n+                         out_layouts=out_layouts, **params)\n+  out_mut, lo_outs = split_list(all_outs, [sum(num_mut)])\n+\n+  out_mut_ = iter(out_mut)\n+  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n+  for var, ty in jaxpr.jaxpr.mut_types.items():\n+    ty.set(hi_args[in_idx[var]], *it.islice(out_mut_, len(ty.lo_ty())))\n+  assert next(out_mut_, None) is None\n+\n+  lo_outs_ = iter(lo_outs)\n+  hi_outs = [t.raise_val(*it.islice(lo_outs_, len(t.lo_ty())))\n+             for t in jaxpr.out_avals]\n+  assert next(lo_outs_, None) is None\n+\n+  return hi_outs\n+pjit_p.to_lojax = _to_lojax\n+\n def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n   # If device or backend is set, return the default layout. This is because you\n   # can pass arrays on cpu (with untiled layouts) to jit with backend='tpu'\n@@ -3233,7 +3264,7 @@ def _flatten_boxes(dbg, args, kwargs):\n     return args, kwargs, []\n   box_data = []\n   id_first_occurrences = {}\n-  idxs = itertools.count()\n+  idxs = it.count()\n   def visit(x):\n     i = next(idxs)\n     if (isinstance(x, (Box, List)) and\ndiff --git a/tests/attrs_test.py b/tests/attrs_test.py\nindex 60a3753a7ba5..b6cef7fec4dc 100644\n--- a/tests/attrs_test.py\n+++ b/tests/attrs_test.py\n@@ -15,7 +15,9 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n+from functools import partial\n import itertools as it\n+import unittest\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -25,6 +27,10 @@\n import jax.numpy as jnp\n \n from jax._src import config\n+from jax._src import core\n+from jax._src import dtypes\n+from jax._src.interpreters import ad\n+from jax._src.interpreters import partial_eval as pe\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip, safe_map\n \n@@ -1326,5 +1332,292 @@ def f(lst1, lst2):\n       f(b, b)\n \n \n+class HiPrimitive(core.Primitive):\n+  def __init__(self, name):\n+    self.name = name\n+    ad.primitive_jvps[self] = self.jvp\n+    ad.primitive_transposes[self] = self.transpose\n+    pe.custom_staging_rules[self] = self.staging\n+\n+  def staging(self, trace, *args, **kwargs):\n+    trace.frame.is_high = True\n+    return trace.default_process_primitive(self, args, kwargs)\n+\n+  def is_high(self, **params):\n+    return True\n+\n+  def abstract_eval(self, *arg_avals, **params):\n+    assert False, \"must override\"\n+\n+  def to_lojax(self, *lotypes_wrapped_in_hitypes, **params):\n+    assert False, \"must override\"\n+\n+  def jvp(self, primals, tangents, **params):\n+    assert False, \"must override\"\n+\n+  def transpose(self, *args, **params):\n+    assert False  # TODO\n+\n+\n+class HijaxTest(jtu.JaxTestCase):\n+\n+  def test_custom_types_and_primitive(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+\n+    @dataclass(frozen=True)\n+    class MyArray:\n+      arr: jax.Array  # always f32\n+\n+    @dataclass(frozen=True)\n+    class MyTy(core.AbstractValue):\n+      mutable = False\n+\n+      def to_tangent_aval(self):\n+        return MyTy()\n+      def str_short(self, short_dtypes=False):\n+        return 'MyTy'\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n+        return [hi_val.arr]\n+      def raise_val(self, val) -> MyArray:\n+        return MyArray(val)\n+\n+      def __eq__(self, other): return isinstance(other, MyTy)\n+\n+      def vspace_zero(self):\n+        return MyArray(jnp.zeros((), 'float32'))\n+      def vspace_add(self, x, y):\n+        return add(x, y)\n+\n+      def strip_weak_type(self): return self\n+      def normalize(self): return self\n+    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n+\n+    class ToMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, lo_aval):\n+        return MyTy(), set()\n+\n+      def to_lojax(_, lo):\n+        return MyArray(lo)\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return to(x), to(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return from_(out_bar),\n+\n+    class FromMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_aval):\n+        return hi_aval.lo_ty()[0], set()\n+\n+      def to_lojax(_, hi):\n+        return hi.arr\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return from_(x), from_(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return to(out_bar),\n+\n+    def to(x): return to_p.bind(x)\n+    to_p = ToMy('to_my')\n+\n+    def from_(x): return from_p.bind(x)\n+    from_p = FromMy('from_my')\n+\n+    def mul(x, y): return mul_p.bind(x, y)\n+    def add(x, y): return add_p.bind(x, y)\n+\n+    class MyMul(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr * hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        (x, y), (x_dot, y_dot) = primals, tangents\n+        return mul(x, y), add(mul(x, y_dot), mul(x_dot, y))\n+\n+      def transpose(self, out_bar, x, y):\n+        assert ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)\n+        if ad.is_undefined_primal(x):\n+          return mul(out_bar, y), None\n+        else:\n+          return None, mul(x, out_bar)\n+\n+    class MyAdd(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr + hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(self, out_bar, x, y):\n+        return out_bar, out_bar\n+\n+    mul_p = MyMul('my_mul')\n+    add_p = MyAdd('my_add')\n+\n+\n+    @jax.jit\n+    def f(x):\n+      return to(from_(x))\n+\n+    # test basic to/from jit\n+    a = MyArray(jnp.ones(()))\n+    b = f(a)  # don't crash\n+    self.assertIsInstance(b, MyArray)\n+    self.assertAllClose(b.arr, jnp.ones(()))\n+\n+    # test basic to/from autodiff\n+    b, b_dot = jax.jvp(f, (a,), (a,))\n+    self.assertIsInstance(b, MyArray)\n+    self.assertIsInstance(b_dot, MyArray)\n+\n+    # test mul jit and backward pass\n+\n+    @jax.jit\n+    def f(x):\n+      return mul(x, x)\n+\n+    b, f_vjp = jax.vjp(f, a)\n+    self.assertIn('MyTy', str(f_vjp))\n+    a_grad, = f_vjp(b)\n+    self.assertIsInstance(a_grad, MyArray)\n+    self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n+\n+  def test_box_autodiff(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+    class BoxTy(core.AbstractValue):\n+      mutable = True\n+\n+      def to_tangent_aval(self):\n+        # NOTE not really used, for some reason we had to write it anyway\n+        return core.ShapedArray((), dtypes.float0)\n+\n+      def str_short(self, short_dtypes=False):\n+        return 'BoxTy'\n+\n+      def lower_val(self, box):\n+        return [box._val]\n+\n+      def raise_val(self, val):\n+        return Box(val)  # we're gonna mutate this\n+\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+\n+      def get(self, box):\n+        return [box._val]\n+\n+      def set(self, box, val):\n+        box._val = val\n+\n+    class Box:\n+      def __init__(self, val):\n+        self._val = val\n+      ty = BoxTy()\n+    core.pytype_aval_mappings[Box] = lambda b: b.ty\n+\n+\n+    class BoxSet(HiPrimitive):\n+      multiple_results = True\n+      def is_high(self) -> bool: return True\n+\n+      def abstract_eval(*_, **__):\n+        return [], set()\n+\n+      def to_lojax(_, box, val):\n+        box._val = val\n+        return []\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(_, *args):\n+        assert False  # TODO\n+    box_set_p = BoxSet('box_set')\n+\n+    class BoxGet(HiPrimitive):\n+      def is_high(self) -> bool: return True\n+\n+      def abstract_eval(*_, **__):\n+        return jnp.dtype('float32'), set()\n+\n+      def to_lojax(_, box):\n+        return box._val\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(_, *args):\n+        assert False  # TODO\n+    box_get_p = BoxGet('box_get')\n+\n+    class StashTangents(HiPrimitive):\n+      def is_high(self):\n+        return True\n+\n+      def abstract_eval(_, box_aval, x_aval):\n+        del box_aval\n+        return x_aval, set()\n+\n+      def to_lojax(_, box, x):\n+        assert False  # TODO\n+\n+      def jvp(_, primals, tangents):\n+        box, x = primals\n+        _, x_dot = tangents\n+        box_set(box, x_dot)\n+        return x, x_dot\n+\n+      def transpose(self, *args):\n+        assert False  # TODO\n+    stash_tangents_p = StashTangents('stash_tangents')\n+\n+    def box_set(box, val):\n+      box_set_p.bind(box, val)\n+\n+    def box_get(box):\n+      return box_get_p.bind(box)\n+\n+    def stash_tangents(box, x):\n+      return stash_tangents_p.bind(box, x)\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    box = Box(0.0)\n+    f(box, 1.)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+\n+    @jax.jit\n+    def f(box, x):\n+      x = stash_tangents(box, x)\n+      return x\n+\n+    box = Box(0.0)\n+    jax.jvp(partial(f, box), (3.,), (5.,))\n+    self.assertAllClose(box_get(box), 5.0, check_dtypes=False)\n+\n+\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[hijax] landing prototype pieces\n\nshouldn't affect existing behaviors, or trace time\n\nThe main implementation ideas:\n* each Trace is tagged with a `requires_low: bool`\n* each Jaxpr\n  * is tagged with an `is_high: bool`, default False but set True while tracing\n    if any hijax primitives are encountered\n  * includes an `mut_types: dict[Var, HijaxType]` indicating final types for\n    type-changing mutable hijax types\n* each AbstractValue is tagged by a `mutable: bool` which is read to populate\n  `mut_types`\n* each Primitive\n  * has an `is_high(**params) -> bool` method (depends on params for HOPs)\n  * has a `to_lojax(*args, **params)` method taking and returning\n    hijaxtypes-wrapping-lowtracers\n* in `Primitive.bind`, we check if `prim.is_high(**params) and\n  trace.requires_low`, and if so we call `prim.to_lojax`\n\nCo-authored-by: Dougal Maclaurin <dougalm@google.com>",
        "output": [
            "```diff\nCommit: a043de0e7ee4cfe3c793b16b43d4626efdc55fc1\nDate: 2025-05-15T22:08:48Z\nURL: https://github.com/jax-ml/jax/commit/a043de0e7ee4cfe3c793b16b43d4626efdc55fc1\nFiles changed: 7\nAdditions: +434, Deletions: -35\ndiff --git a/jax/_src/ad_util.py b/jax/_src/ad_util.py\nindex 8cfd7b214338..4e9616e48375 100644\n--- a/jax/_src/ad_util.py\n+++ b/jax/_src/ad_util.py\n@@ -31,6 +31,9 @@\n map = safe_map\n \n def add_jaxvals(x: ArrayLike, y: ArrayLike) -> Array:\n+  ty = core.typeof(x)\n+  if hasattr(ty, 'vspace_add'):  # TODO(mattjj,dougalm): revise away hasattr\n+    return ty.vspace_add(x, y)\n   x, y = core.standard_insert_pvary(x, y)\n   return add_jaxvals_p.bind(x, y)\n \n@@ -48,6 +51,8 @@ def add_abstract(x, y):\n   return x\n \n def zeros_like_aval(aval: core.AbstractValue) -> Array:\n+  if hasattr(aval, 'vspace_zero'):  # TODO(mattjj,dougalm): revise away hasattr\n+    return aval.vspace_zero()\n   return aval_zeros_likers[type(aval)](aval)\n aval_zeros_likers: dict[type, Callable[[Any], Array]] = {}\n \ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 154ff5132a39..33802e494304 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -540,17 +540,18 @@ def _check_input_dtype_revderiv(name, holomorphic, allow_int, x):\n     if not dtypes.issubdtype(aval.dtype, np.complexfloating):\n       raise TypeError(f\"{name} with holomorphic=True requires inputs with complex dtype, \"\n                       f\"but got {aval.dtype.name}.\")\n-  if (dtypes.issubdtype(aval.dtype, dtypes.extended) or\n-      dtypes.issubdtype(aval.dtype, np.integer) or\n-      dtypes.issubdtype(aval.dtype, np.bool_)):\n-    if not allow_int:\n-      raise TypeError(f\"{name} requires real- or complex-valued inputs (input dtype \"\n-                      f\"that is a sub-dtype of np.inexact), but got {aval.dtype.name}. \"\n-                      \"If you want to use Boolean- or integer-valued inputs, use vjp \"\n-                      \"or set allow_int to True.\")\n-  elif not dtypes.issubdtype(aval.dtype, np.inexact):\n-    raise TypeError(f\"{name} requires numerical-valued inputs (input dtype that is a \"\n-                    f\"sub-dtype of np.bool_ or np.number), but got {aval.dtype.name}.\")\n+  if isinstance(aval, ShapedArray):\n+    if (dtypes.issubdtype(aval.dtype, dtypes.extended) or\n+        dtypes.issubdtype(aval.dtype, np.integer) or\n+        dtypes.issubdtype(aval.dtype, np.bool_)):\n+      if not allow_int:\n+        raise TypeError(f\"{name} requires real- or complex-valued inputs (input dtype \"\n+                        f\"that is a sub-dtype of np.inexact), but got {aval.dtype.name}. \"\n+                        \"If you want to use Boolean- or integer-valued inputs, use vjp \"\n+                        \"or set allow_int to True.\")\n+    elif not dtypes.issubdtype(aval.dtype, np.inexact):\n+      raise TypeError(f\"{name} requires numerical-valued inputs (input dtype that is a \"\n+                      f\"sub-dtype of np.bool_ or np.number), but got {aval.dtype.name}.\")\n _check_input_dtype_grad = partial(_check_input_dtype_revderiv, \"grad\")\n \n def _check_output_dtype_revderiv(name, holomorphic, x):\n@@ -1873,6 +1874,7 @@ def _jvp(fun: lu.WrappedFun, primals, tangents, has_aux=False):\n                     f\"structure; primals have tree structure {tree_def} whereas tangents have \"\n                     f\"tree structure {tree_def_2}.\")\n   for p, t in zip(ps_flat, ts_flat):\n+    if not isinstance(core.typeof(p), ShapedArray): continue\n     if core.primal_dtype_to_tangent_dtype(_dtype(p)) != _dtype(t):\n       raise TypeError(\"primal and tangent arguments to jax.jvp do not match; \"\n                       \"dtypes must be equal, or in case of int/bool primal dtype \"\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex e004263abe71..12e2763d7202 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -88,7 +88,7 @@\n \n class Jaxpr:\n   __slots__ = ['__weakref__', '_constvars', '_invars', '_outvars', '_eqns',\n-               '_effects', '_debug_info']\n+               '_effects', '_debug_info', '_is_high', '_mut_types']\n \n   _constvars: list[Var]\n   _invars: list[Var]\n@@ -96,6 +96,8 @@ class Jaxpr:\n   _eqns: list[JaxprEqn]\n   _effects: Effects\n   _debug_info: DebugInfo\n+  _is_high: bool\n+  _mut_types: dict[Var, Any]\n \n   @property\n   def constvars(self) -> list[Var]:\n@@ -121,6 +123,14 @@ def effects(self) -> Effects:\n   def debug_info(self) -> DebugInfo:\n     return self._debug_info\n \n+  @property\n+  def is_high(self) -> bool:\n+    return self._is_high\n+\n+  @property\n+  def mut_types(self) -> dict[Var, Any]:\n+    return self._mut_types\n+\n   def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                outvars: Sequence[Atom], eqns: Sequence[JaxprEqn],\n                effects: Effects = no_effects,\n@@ -128,6 +138,8 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n                # compatibility we have to allow calls when the debug_info\n                # is missing.\n                debug_info: DebugInfo = None,  # type: ignore[annotation-type-mismatch,assignment]\n+               is_high: bool = False,\n+               mut_types: dict | None = None,\n                ):\n     \"\"\"\n     Args:\n@@ -152,6 +164,8 @@ def __init__(self, constvars: Sequence[Var], invars: Sequence[Var],\n     # TODO(necula): re-enable these safety checks\n     # assert (len(debug_info.arg_names) == len(invars)), (debug_info, invars)\n     # assert (len(debug_info.result_paths) == len(outvars)), (debug_info, outvars)\n+    self._is_high = is_high\n+    self._mut_types = mut_types or {}\n \n   def __str__(self):\n     return str(self.pretty_print())\n@@ -178,6 +192,8 @@ def replace(self, **kwargs):\n         eqns=kwargs.pop(\"eqns\", self.eqns),\n         effects=kwargs.pop(\"effects\", self.effects),\n         debug_info=kwargs.pop(\"debug_info\", self.debug_info),\n+        is_high=kwargs.pop(\"is_high\", self.is_high),\n+        mut_types=kwargs.pop(\"mut_types\", self.mut_types),\n     )\n     if kwargs:\n       raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n@@ -517,7 +533,7 @@ def _true_bind(self, *args, **params):\n     for arg in args:\n       if isinstance(arg, Tracer) and not arg._trace.is_valid():\n         raise escaped_tracer_error(arg)\n-    # TODO: figure out how to handle function arguments\n+    # TODO: figure out how to handle function arguments for this assert\n     # assert (not config.enable_checks.value or\n     #         all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n \n@@ -525,6 +541,10 @@ def _true_bind(self, *args, **params):\n     # is called frequently and it's slightly faster to avoid using a context\n     # manager object.\n     prev_trace = trace_ctx.trace\n+\n+    if self.is_high(**params) and prev_trace.requires_low:\n+      return self.to_lojax(*args, **params)  # type: ignore\n+\n     trace_ctx.set_trace(eval_trace)\n     try:\n       return self.bind_with_trace(prev_trace, args, params)\n@@ -561,6 +581,9 @@ def abstract_eval(self, *args, **params):\n   def get_bind_params(self, params):\n     return [], params\n \n+  def is_high(self, **params) -> bool:\n+    return False\n+\n \n def _effect_free_abstract_eval(abstract_eval):\n   def abstract_eval_(*args, **kwargs):\n@@ -627,12 +650,13 @@ def check_avals_context_mesh(avals, prim_name):\n TracerType = TypeVar('TracerType', bound='Tracer')\n \n class Trace(Generic[TracerType]):\n-  __slots__ = (\"__weakref__\", \"_invalidated\", \"_weakref\")\n+  __slots__ = (\"__weakref__\", \"_invalidated\", \"_weakref\", \"requires_low\")\n \n   def __init__(self):\n     self._invalidated = False\n     # We frequently need a weakref to a trace, so let's precompute one.\n     self._weakref = weakref.ref(self)\n+    self.requires_low = True\n \n   def process_primitive(self, primitive, tracers, params):\n     raise NotImplementedError(\"must override\")\n@@ -1445,6 +1469,8 @@ def definitely_equal(x, y):\n \n class AbstractValue:\n   __slots__: list[str] = []\n+  is_high = False\n+  mutable = False\n \n   def to_tangent_aval(self):\n     raise NotImplementedError(\"must override\")\n@@ -1948,6 +1974,10 @@ def __init__(self, shape, dtype, weak_type=False, *, sharding=None,\n     self.sharding = get_sharding(sharding, self.shape)\n     self.vma = get_vma(vma, self.sharding.mesh)\n \n+  def lower_val(self, val): return [val]\n+  def raise_val(self, val): return val\n+  def lo_ty(self): return [self]\n+\n   def update(self, shape=None, dtype=None, weak_type=None, **kwargs):\n     if shape is None:\n       shape = self.shape\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 45705382efa0..090022c9b6a4 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -73,6 +73,7 @@ def jvp(fun: lu.WrappedFun, has_aux=False, instantiate=True,\n def jvpfun(f: Callable, instantiate, transform_stack, primals, tangents):\n   tag = core.TraceTag()\n   tangents = [Zero.from_primal_value(t) if not isinstance(t, Zero)\n+              and isinstance(core.typeof(t), core.ShapedArray)\n               and dtype(t) == float0 else t for t in tangents]\n   ctx = (source_info_util.transform_name_stack('jvp') if transform_stack\n          else contextlib.nullcontext())\n@@ -475,6 +476,7 @@ def __init__(self, parent_trace, tag):\n     super().__init__()\n     self.tag = tag\n     self.parent_trace = parent_trace\n+    self.requires_low = False\n \n   def to_primal_tangent_pair(self, val):\n     if isinstance(val, JVPTracer) and val._trace.tag is self.tag:\n@@ -606,7 +608,8 @@ def process_custom_transpose(self, prim, call, tracers, **params):\n     return map(partial(maybe_jvp_tracer, self), ps_out, ts_out)\n \n def maybe_jvp_tracer(trace, primal, tangent):\n-  if type(tangent) is Zero or dtype(tangent) == float0:\n+  if (type(tangent) is Zero or\n+      core.typeof(tangent) is core.ShapedArray and dtype(tangent) == float0):\n     return primal\n   else:\n     return JVPTracer(trace, primal, tangent)\n@@ -641,6 +644,7 @@ def _primal_tangent_shapes_match(primal, tangent):\n   if type(tangent) is not Zero:\n     primal_aval = get_aval(primal).strip_weak_type()\n     tangent_aval = get_aval(tangent).strip_weak_type()\n+    if not isinstance(primal_aval, core.ShapedArray): return  # TODO(mattjj,dougalm)\n     assert core.definitely_equal_shape(primal_aval.shape, tangent_aval.shape), (primal_aval.shape, tangent_aval.shape)\n     expected_tangent_dtype = core.primal_dtype_to_tangent_dtype(primal_aval.dtype)\n     assert expected_tangent_dtype == tangent_aval.dtype, (expected_tangent_dtype, tangent_aval.dtype)\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 5866b0c5f8eb..9e875f43d831 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -155,6 +155,7 @@ def __init__(self, parent_trace:Trace, name_stack: source_info_util.NameStack, t\n     self.name_stack = name_stack\n     self.tag = tag\n     self.parent_trace = parent_trace\n+    self.requires_low = False\n \n   def to_jaxpr_tracer(self, x):\n     if isinstance(x, JaxprTracer) and x._trace.tag is self.tag:\n@@ -899,9 +900,8 @@ def convert_envvars_to_constvars(jaxpr: Jaxpr, num_env_vars: int) -> Jaxpr:\n     raise NotImplementedError\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   env_vars, invars = split_list(jaxpr.invars, [num_env_vars])\n-  converted_jaxpr = Jaxpr(constvars=jaxpr.constvars + env_vars,\n-                          invars=invars, outvars=jaxpr.outvars, eqns=jaxpr.eqns,\n-                          effects=jaxpr.effects, debug_info=jaxpr.debug_info)\n+  converted_jaxpr = jaxpr.replace(constvars=jaxpr.constvars + env_vars,\n+                                  invars=invars)\n   config.enable_checks.value and core.check_jaxpr(converted_jaxpr)\n   return converted_jaxpr\n \n@@ -1173,6 +1173,7 @@ def has_effects(effects) -> bool:\n   out_unknowns = map(op.or_, out_unknowns, ensure_out_unknowns)\n   out_inst     = map(op.or_, out_inst,     ensure_out_inst)\n \n+\n   ins_known, _ = partition_list(in_unknowns, jaxpr.invars)\n   outs_known, _ = partition_list(out_unknowns, jaxpr.outvars)\n   ref_res_is_input = [r in ins_known for r in residual_refs]\n@@ -1181,8 +1182,14 @@ def has_effects(effects) -> bool:\n   known_outvars = [*outs_known, *residuals]\n   known_effects = make_jaxpr_effects(jaxpr.constvars, ins_known_and_ref_res,\n                                      known_outvars, known_eqns)\n-  jaxpr_known = Jaxpr(jaxpr.constvars, ins_known_and_ref_res, known_outvars,\n-                      known_eqns, known_effects, jaxpr.debug_info)\n+  known_mut, staged_mut, ins_known_ = {}, {}, set(ins_known)  # type: ignore\n+  for v, t in jaxpr.mut_types.items():\n+    [staged_mut, known_mut][v in ins_known_][v] = t\n+\n+  # TODO(mattjj,necula): debug info should be updated here\n+  jaxpr_known = jaxpr.replace(\n+      invars=ins_known_and_ref_res, outvars=known_outvars,\n+      eqns=known_eqns, effects=known_effects, mut_types=known_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_known)\n \n   _, ins_staged = partition_list(in_inst, jaxpr.invars)\n@@ -1190,9 +1197,10 @@ def has_effects(effects) -> bool:\n   staged_invars = [*residuals, *non_input_res_refs, *ins_staged]\n   staged_effects = make_jaxpr_effects(jaxpr.constvars, staged_invars,\n                                       outs_staged, staged_eqns)\n-  jaxpr_staged = Jaxpr(jaxpr.constvars, staged_invars,\n-                       outs_staged, staged_eqns, staged_effects,\n-                       jaxpr.debug_info)\n+  # TODO(mattjj,necula): debug info should be updated here\n+  jaxpr_staged = jaxpr.replace(\n+      invars=staged_invars, outvars=outs_staged, eqns=staged_eqns,\n+      effects=staged_effects, mut_types=staged_mut)\n   config.enable_checks.value and core.check_jaxpr(jaxpr_staged)\n \n   return (jaxpr_known, jaxpr_staged, out_unknowns, out_inst, len(residuals),\n@@ -1483,7 +1491,8 @@ def write(x: Atom, b: bool) -> None:\n       jaxpr.debug_info.traced_for, jaxpr.debug_info.func_src_info,\n       jaxpr.debug_info.filter_arg_names(used_inputs),\n       jaxpr.debug_info.filter_result_paths(used_outputs))\n-  new_jaxpr = Jaxpr(jaxpr.constvars, invars, outvars, eqns, jaxpr_effects, dbg)\n+  new_jaxpr = jaxpr.replace(invars=invars, outvars=outvars, eqns=eqns,\n+                            effects=jaxpr_effects, debug_info=dbg)\n   config.enable_checks.value and core.check_jaxpr(new_jaxpr)\n \n   return new_jaxpr, used_inputs\n@@ -1561,9 +1570,8 @@ def _move_binders_to_front(closed_jaxpr: ClosedJaxpr, to_move: tuple[bool, ...]\n   new_invars = _move_to_front(invars, to_move)\n   new_effs = _renumber_effects(\n       (*constvars, *new_invars), (*constvars, *invars), closed_jaxpr.jaxpr.effects)\n-  new_jaxpr = Jaxpr(constvars, new_invars, closed_jaxpr.jaxpr.outvars,\n-                    closed_jaxpr.jaxpr.eqns, new_effs,\n-                    closed_jaxpr.jaxpr.debug_info)\n+  new_jaxpr = closed_jaxpr.jaxpr.replace(\n+      constvars=constvars, invars=new_invars, effects=new_effs)\n   new_closed_jaxpr = core.ClosedJaxpr(new_jaxpr, closed_jaxpr.consts)\n   return new_closed_jaxpr\n \n@@ -1704,6 +1712,7 @@ class JaxprStackFrame:\n   attrs_inits: list\n   attrs_vars: list[Var]\n   debug_info: core.DebugInfo\n+  is_high: bool\n \n   def __init__(self, debug_info: core.DebugInfo):\n     self.gensym = core.gensym()\n@@ -1718,6 +1727,7 @@ def __init__(self, debug_info: core.DebugInfo):\n     self.attrs_inits = []\n     self.attrs_vars = []\n     self.debug_info = debug_info\n+    self.is_high = False\n \n   def add_eqn(self, eqn: core.JaxprEqn):\n     self.eqns.append(eqn)\n@@ -1743,8 +1753,9 @@ def to_jaxpr(\n     outvars = state_outvars + explicit_outvars\n     constvars, constvals = unzip2(self.constvar_to_val.items())\n     jaxpr_effects = make_jaxpr_effects(constvars, self.invars, explicit_outvars, self.eqns)\n+    mut_types = {v: v.aval for v in invars if v.aval.mutable} if self.is_high else {}\n     jaxpr = Jaxpr(constvars, invars, outvars, self.eqns, jaxpr_effects,\n-                  debug_info)\n+                  debug_info, self.is_high, mut_types)\n     jaxpr, constvals = _drop_unused_vars(jaxpr, constvals)\n     init_trees = [tree_structure(init_val) for init_val in self.attrs_inits]\n     return jaxpr, list(constvals), zip(init_trees, end_trees, self.attrs_tracked)\n@@ -1831,8 +1842,9 @@ def vars(atom: Atom) -> list[Var]:\n class DynamicJaxprTrace(core.Trace):\n   __slots__ = (\"frame\", \"tag\")\n \n-  def __init__(self, debug_info: core.DebugInfo):\n+  def __init__(self, debug_info: core.DebugInfo, lower=False):\n     super().__init__()\n+    self.requires_low = lower\n     self.frame = JaxprStackFrame(debug_info)\n \n   def invalidate(self):\n@@ -2193,10 +2205,11 @@ def trace_to_jaxpr_dynamic(\n     in_avals: Sequence[AbstractValue],\n     *,\n     keep_inputs: list[bool] | None = None,\n+    lower: bool = False,\n ) -> tuple[Jaxpr, list[AbstractValue], list[Any],\n            list[tuple[PyTreeDef, PyTreeDef, tuple[Any, str, AttrKind]]]]:\n   keep_inputs = [True] * len(in_avals) if keep_inputs is None else keep_inputs\n-  trace = DynamicJaxprTrace(fun.debug_info)\n+  trace = DynamicJaxprTrace(fun.debug_info, lower=lower)\n   with core.ensure_no_leaks(trace), source_info_util.reset_name_stack():\n     source_info = source_info_util.current()\n     in_tracers = _input_type_to_tracers(\n@@ -2418,8 +2431,7 @@ def _add_implicit_outputs(jaxpr: Jaxpr) -> tuple[Jaxpr, OutputType]:\n   kept_outs = [False] * len(impl_outvars) + [True] * len(expl_outvars)\n   out_type = tuple(zip(out_avals, kept_outs))\n \n-  new_jaxpr = Jaxpr(jaxpr.constvars, jaxpr.invars, outvars, jaxpr.eqns,\n-                    jaxpr.effects, jaxpr.debug_info)\n+  new_jaxpr = jaxpr.replace(outvars=outvars)\n   config.enable_checks.value and core.check_jaxpr(jaxpr)\n   return new_jaxpr, out_type\n \n@@ -2663,3 +2675,25 @@ def _linearize_of_pmap_hack(f: lu.WrappedFun, jaxpr, consts) -> tuple[Jaxpr, lis\n     _, jaxpr = f.f.closure\n     return convert_constvars_jaxpr(jaxpr), []\n   return jaxpr, consts\n+\n+\n+@weakref_lru_cache\n+def lower_jaxpr(hi_jaxpr):\n+  in_avals = [lo_ty for t in hi_jaxpr.in_avals for lo_ty in t.lo_ty()]\n+  f = lu.wrap_init(partial(lower_traceable, hi_jaxpr),\n+                   debug_info=hi_jaxpr.jaxpr.debug_info)\n+  lo_jaxpr, _, consts, () = trace_to_jaxpr_dynamic(f, in_avals, lower=True)\n+  return core.ClosedJaxpr(lo_jaxpr, consts)\n+\n+def lower_traceable(jaxpr, *lo_args):\n+  lo_args_ = iter(lo_args)\n+  hi_args = [t.raise_val(*it.islice(lo_args_, len(t.lo_ty())))\n+             for t in jaxpr.in_avals]\n+  assert (problem := next(lo_args_, None)) is None\n+  hi_outs = core.jaxpr_as_fun(jaxpr)(*hi_args)\n+  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n+  mut_outs = [lo_val for v, ty in jaxpr.jaxpr.mut_types.items()\n+              for lo_val in ty.get(hi_args[in_idx[v]])]\n+  lo_outs = [lo_val for t, hi_val in zip(jaxpr.out_avals, hi_outs)\n+             for lo_val in t.lower_val(hi_val)]\n+  return mut_outs + lo_outs\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex ca77b659a08d..5edd74fe74ef 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -20,7 +20,7 @@\n import dataclasses\n from functools import partial\n import inspect\n-import itertools\n+import itertools as it\n import logging\n import weakref\n from typing import NamedTuple, Any, Union, cast\n@@ -188,7 +188,8 @@ def _python_pjit_helper(fun: Callable, jit_info: PjitInfo, *args, **kwargs):\n     args_flat = [*init_states, *args_flat]\n \n   try:\n-    if core.trace_state_clean() and not config.debug_key_reuse.value:\n+    if (core.trace_state_clean() and not config.debug_key_reuse.value\n+        and not p.params['jaxpr'].jaxpr.is_high):\n       args_flat = map(core.full_lower, args_flat)\n       core.check_eval_args(args_flat)\n       out_flat, compiled, profiler = _pjit_call_impl_python(*args_flat, **p.params)\n@@ -1592,6 +1593,36 @@ def check_aval_layout_compatibility(\n pjit_p.multiple_results = True\n pjit_p.skip_canonicalization = True\n \n+def _is_high(jaxpr, **_) -> bool:\n+  return jaxpr.jaxpr.is_high\n+pjit_p.is_high = _is_high  # type: ignore\n+\n+def _to_lojax(*hi_args, jaxpr, out_shardings, out_layouts, **params):\n+  num_mut = [len(ty.lo_ty()) for ty in jaxpr.jaxpr.mut_types.values()]\n+  out_shardings = (UNSPECIFIED,) * sum(num_mut) + out_shardings\n+  out_layouts = (None,) * sum(num_mut) + out_layouts\n+\n+  lo_args = [lo_val for t, hi_val in zip(jaxpr.in_avals, hi_args)\n+             for lo_val in t.lower_val(hi_val)]\n+  lo_jaxpr = pe.lower_jaxpr(jaxpr)\n+  all_outs = pjit_p.bind(*lo_args, jaxpr=lo_jaxpr, out_shardings=out_shardings,\n+                         out_layouts=out_layouts, **params)\n+  out_mut, lo_outs = split_list(all_outs, [sum(num_mut)])\n+\n+  out_mut_ = iter(out_mut)\n+  in_idx = {v: i for i, v in enumerate(jaxpr.jaxpr.invars)}\n+  for var, ty in jaxpr.jaxpr.mut_types.items():\n+    ty.set(hi_args[in_idx[var]], *it.islice(out_mut_, len(ty.lo_ty())))\n+  assert next(out_mut_, None) is None\n+\n+  lo_outs_ = iter(lo_outs)\n+  hi_outs = [t.raise_val(*it.islice(lo_outs_, len(t.lo_ty())))\n+             for t in jaxpr.out_avals]\n+  assert next(lo_outs_, None) is None\n+\n+  return hi_outs\n+pjit_p.to_lojax = _to_lojax\n+\n def _resolve_in_layouts(args, jit_in_layouts, resolved_in_shardings, in_avals):\n   # If device or backend is set, return the default layout. This is because you\n   # can pass arrays on cpu (with untiled layouts) to jit with backend='tpu'\n@@ -3233,7 +3264,7 @@ def _flatten_boxes(dbg, args, kwargs):\n     return args, kwargs, []\n   box_data = []\n   id_first_occurrences = {}\n-  idxs = itertools.count()\n+  idxs = it.count()\n   def visit(x):\n     i = next(idxs)\n     if (isinstance(x, (Box, List)) and\ndiff --git a/tests/attrs_test.py b/tests/attrs_test.py\nindex 60a3753a7ba5..b6cef7fec4dc 100644\n--- a/tests/attrs_test.py\n+++ b/tests/attrs_test.py\n@@ -15,7 +15,9 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n+from functools import partial\n import itertools as it\n+import unittest\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -25,6 +27,10 @@\n import jax.numpy as jnp\n \n from jax._src import config\n+from jax._src import core\n+from jax._src import dtypes\n+from jax._src.interpreters import ad\n+from jax._src.interpreters import partial_eval as pe\n from jax._src import test_util as jtu\n from jax._src.util import safe_zip, safe_map\n \n@@ -1326,5 +1332,292 @@ def f(lst1, lst2):\n       f(b, b)\n \n \n+class HiPrimitive(core.Primitive):\n+  def __init__(self, name):\n+    self.name = name\n+    ad.primitive_jvps[self] = self.jvp\n+    ad.primitive_transposes[self] = self.transpose\n+    pe.custom_staging_rules[self] = self.staging\n+\n+  def staging(self, trace, *args, **kwargs):\n+    trace.frame.is_high = True\n+    return trace.default_process_primitive(self, args, kwargs)\n+\n+  def is_high(self, **params):\n+    return True\n+\n+  def abstract_eval(self, *arg_avals, **params):\n+    assert False, \"must override\"\n+\n+  def to_lojax(self, *lotypes_wrapped_in_hitypes, **params):\n+    assert False, \"must override\"\n+\n+  def jvp(self, primals, tangents, **params):\n+    assert False, \"must override\"\n+\n+  def transpose(self, *args, **params):\n+    assert False  # TODO\n+\n+\n+class HijaxTest(jtu.JaxTestCase):\n+\n+  def test_custom_types_and_primitive(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+\n+    @dataclass(frozen=True)\n+    class MyArray:\n+      arr: jax.Array  # always f32\n+\n+    @dataclass(frozen=True)\n+    class MyTy(core.AbstractValue):\n+      mutable = False\n+\n+      def to_tangent_aval(self):\n+        return MyTy()\n+      def str_short(self, short_dtypes=False):\n+        return 'MyTy'\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+      def lower_val(self, hi_val: MyArray) -> list[jax.Array]:\n+        return [hi_val.arr]\n+      def raise_val(self, val) -> MyArray:\n+        return MyArray(val)\n+\n+      def __eq__(self, other): return isinstance(other, MyTy)\n+\n+      def vspace_zero(self):\n+        return MyArray(jnp.zeros((), 'float32'))\n+      def vspace_add(self, x, y):\n+        return add(x, y)\n+\n+      def strip_weak_type(self): return self\n+      def normalize(self): return self\n+    core.pytype_aval_mappings[MyArray] = lambda _: MyTy()\n+\n+    class ToMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, lo_aval):\n+        return MyTy(), set()\n+\n+      def to_lojax(_, lo):\n+        return MyArray(lo)\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return to(x), to(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return from_(out_bar),\n+\n+    class FromMy(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_aval):\n+        return hi_aval.lo_ty()[0], set()\n+\n+      def to_lojax(_, hi):\n+        return hi.arr\n+\n+      def jvp(_, primals, tangents):\n+        x, x_dot = *primals, *tangents\n+        return from_(x), from_(x_dot)\n+\n+      def transpose(self, out_bar, _):\n+        return to(out_bar),\n+\n+    def to(x): return to_p.bind(x)\n+    to_p = ToMy('to_my')\n+\n+    def from_(x): return from_p.bind(x)\n+    from_p = FromMy('from_my')\n+\n+    def mul(x, y): return mul_p.bind(x, y)\n+    def add(x, y): return add_p.bind(x, y)\n+\n+    class MyMul(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr * hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        (x, y), (x_dot, y_dot) = primals, tangents\n+        return mul(x, y), add(mul(x, y_dot), mul(x_dot, y))\n+\n+      def transpose(self, out_bar, x, y):\n+        assert ad.is_undefined_primal(x) ^ ad.is_undefined_primal(y)\n+        if ad.is_undefined_primal(x):\n+          return mul(out_bar, y), None\n+        else:\n+          return None, mul(x, out_bar)\n+\n+    class MyAdd(HiPrimitive):\n+      def is_high(self): return True\n+\n+      def abstract_eval(_, hi_x, hi_y):\n+        if hi_x != hi_y: raise Exception\n+        return hi_x, set()\n+\n+      def to_lojax(_, hi_x, hi_y):\n+        return MyArray(hi_x.arr + hi_y.arr)\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(self, out_bar, x, y):\n+        return out_bar, out_bar\n+\n+    mul_p = MyMul('my_mul')\n+    add_p = MyAdd('my_add')\n+\n+\n+    @jax.jit\n+    def f(x):\n+      return to(from_(x))\n+\n+    # test basic to/from jit\n+    a = MyArray(jnp.ones(()))\n+    b = f(a)  # don't crash\n+    self.assertIsInstance(b, MyArray)\n+    self.assertAllClose(b.arr, jnp.ones(()))\n+\n+    # test basic to/from autodiff\n+    b, b_dot = jax.jvp(f, (a,), (a,))\n+    self.assertIsInstance(b, MyArray)\n+    self.assertIsInstance(b_dot, MyArray)\n+\n+    # test mul jit and backward pass\n+\n+    @jax.jit\n+    def f(x):\n+      return mul(x, x)\n+\n+    b, f_vjp = jax.vjp(f, a)\n+    self.assertIn('MyTy', str(f_vjp))\n+    a_grad, = f_vjp(b)\n+    self.assertIsInstance(a_grad, MyArray)\n+    self.assertAllClose(a_grad.arr, 2.0, check_dtypes=False)\n+\n+  def test_box_autodiff(self):\n+    if config.enable_x64.value: raise unittest.SkipTest(\"no x64\")\n+    class BoxTy(core.AbstractValue):\n+      mutable = True\n+\n+      def to_tangent_aval(self):\n+        # NOTE not really used, for some reason we had to write it anyway\n+        return core.ShapedArray((), dtypes.float0)\n+\n+      def str_short(self, short_dtypes=False):\n+        return 'BoxTy'\n+\n+      def lower_val(self, box):\n+        return [box._val]\n+\n+      def raise_val(self, val):\n+        return Box(val)  # we're gonna mutate this\n+\n+      def lo_ty(self):\n+        return [core.ShapedArray((), jnp.dtype('float32'))]\n+\n+      def get(self, box):\n+        return [box._val]\n+\n+      def set(self, box, val):\n+        box._val = val\n+\n+    class Box:\n+      def __init__(self, val):\n+        self._val = val\n+      ty = BoxTy()\n+    core.pytype_aval_mappings[Box] = lambda b: b.ty\n+\n+\n+    class BoxSet(HiPrimitive):\n+      multiple_results = True\n+      def is_high(self) -> bool: return True\n+\n+      def abstract_eval(*_, **__):\n+        return [], set()\n+\n+      def to_lojax(_, box, val):\n+        box._val = val\n+        return []\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(_, *args):\n+        assert False  # TODO\n+    box_set_p = BoxSet('box_set')\n+\n+    class BoxGet(HiPrimitive):\n+      def is_high(self) -> bool: return True\n+\n+      def abstract_eval(*_, **__):\n+        return jnp.dtype('float32'), set()\n+\n+      def to_lojax(_, box):\n+        return box._val\n+\n+      def jvp(_, primals, tangents):\n+        assert False  # TODO\n+\n+      def transpose(_, *args):\n+        assert False  # TODO\n+    box_get_p = BoxGet('box_get')\n+\n+    class StashTangents(HiPrimitive):\n+      def is_high(self):\n+        return True\n+\n+      def abstract_eval(_, box_aval, x_aval):\n+        del box_aval\n+        return x_aval, set()\n+\n+      def to_lojax(_, box, x):\n+        assert False  # TODO\n+\n+      def jvp(_, primals, tangents):\n+        box, x = primals\n+        _, x_dot = tangents\n+        box_set(box, x_dot)\n+        return x, x_dot\n+\n+      def transpose(self, *args):\n+        assert False  # TODO\n+    stash_tangents_p = StashTangents('stash_tangents')\n+\n+    def box_set(box, val):\n+      box_set_p.bind(box, val)\n+\n+    def box_get(box):\n+      return box_get_p.bind(box)\n+\n+    def stash_tangents(box, x):\n+      return stash_tangents_p.bind(box, x)\n+\n+    @jax.jit\n+    def f(box, x):\n+      box_set(box, x)\n+\n+    box = Box(0.0)\n+    f(box, 1.)\n+    self.assertAllClose(box_get(box), 1.0, check_dtypes=False)\n+\n+    @jax.jit\n+    def f(box, x):\n+      x = stash_tangents(box, x)\n+      return x\n+\n+    box = Box(0.0)\n+    jax.jvp(partial(f, box), (3.,), (5.,))\n+    self.assertAllClose(box_get(box), 5.0, check_dtypes=False)\n+\n+\n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "[pre-commit] update mypy to v1.15.0",
        "output": [
            "```diff\nCommit: 8ff60c3422e3ac93e5c2de31e4f7ea7fbd90ba92\nDate: 2025-05-15T21:32:00Z\nURL: https://github.com/jax-ml/jax/commit/8ff60c3422e3ac93e5c2de31e4f7ea7fbd90ba92\nFiles changed: 3\nAdditions: +4, Deletions: -4\ndiff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml\nindex a6697076404f..46deb8eb4879 100644\n--- a/.pre-commit-config.yaml\n+++ b/.pre-commit-config.yaml\n@@ -36,7 +36,7 @@ repos:\n   - id: ruff\n \n - repo: https://github.com/pre-commit/mirrors-mypy\n-  rev: 'bbc3dc1f890007061f18f17e2334f216ea9e5df7'  # frozen: v1.14.1\n+  rev: 'f40886d54c729f533f864ed6ce584e920feb0af7'  # frozen: v1.15.0\n   hooks:\n   - id: mypy\n     files: (jax/|tests/typing_test\\.py)\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex aa9bfe9529ce..f26b4222b23b 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -264,7 +264,7 @@ def _get_batched_exception(self) -> BatchedError | None:\n       cur_effect = None\n       for error_effect, code in self._code.items():\n         if self._pred[error_effect][idx]:   # type: ignore\n-          if min_code is None or code[idx] < min_code:\n+          if min_code is None or code[idx] < min_code:  # type: ignore[index]\n             min_code = code[idx]   # type: ignore\n             cur_effect = error_effect\n \ndiff --git a/jax/_src/export/_export.py b/jax/_src/export/_export.py\nindex d5a328bb8e05..c0ca1e108590 100644\n--- a/jax/_src/export/_export.py\n+++ b/jax/_src/export/_export.py\n@@ -760,8 +760,8 @@ def export_sharding(s: LoweringSharding,\n         elif cur_mesh.shape_tuple != sharding.mesh.shape_tuple:\n           raise ValueError(\n               \"Mesh for all inputs/outputs should be equal. Got one mesh \"\n-              f\"{cur_mesh} on an array {cur_arg._aval} at \"\n-              f\"{shape_poly.args_kwargs_path_to_str(cur_k_path)} and another mesh: \"\n+              f\"{cur_mesh} on an array {cur_arg._aval} at \"  # type: ignore[union-attr]\n+              f\"{shape_poly.args_kwargs_path_to_str(cur_k_path)} and another mesh: \"  # type: ignore[arg-type]\n               f\"{sharding.mesh}' on a tensor {arg._aval} at \"\n               f\"{shape_poly.args_kwargs_path_to_str(k_path)}\")\n     if cur_mesh and isinstance(cur_mesh, mesh_lib.Mesh):\n\n```"
        ],
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Merge pull request #28721 from jakevdp:fix-typestubs\n\nPiperOrigin-RevId: 759294851",
        "output": [
            "```diff\nCommit: 3c55db3db8c27fe7d0d289192fe1088c598b936f\nDate: 2025-05-15T21:12:26Z\nURL: https://github.com/jax-ml/jax/commit/3c55db3db8c27fe7d0d289192fe1088c598b936f\nFiles changed: 1\nAdditions: +31, Deletions: -32\ndiff --git a/jax/numpy/__init__.pyi b/jax/numpy/__init__.pyi\nindex c52ce2628cda..4db407861f34 100644\n--- a/jax/numpy/__init__.pyi\n+++ b/jax/numpy/__init__.pyi\n@@ -253,7 +253,8 @@ def broadcast_shapes(*shapes: Sequence[int]) -> tuple[int, ...]: ...\n def broadcast_shapes(*shapes: Sequence[int | _core.Tracer]\n                      ) -> tuple[int | _core.Tracer, ...]: ...\n \n-def broadcast_to(array: ArrayLike, shape: DimSize | Shape) -> Array: ...\n+def broadcast_to(array: ArrayLike, shape: DimSize | Shape, *,\n+                 out_sharding: NamedSharding | P | None = None) -> Array: ...\n c_: _CClass\n can_cast = _np.can_cast\n def cbrt(x: ArrayLike, /) -> Array: ...\n@@ -267,6 +268,7 @@ def clip(\n     /,\n     min: ArrayLike | None = ...,\n     max: ArrayLike | None = ...,\n+    *,\n     a: ArrayLike | DeprecatedArg | None = ...,\n     a_min: ArrayLike | DeprecatedArg | None = ...,\n     a_max: ArrayLike | DeprecatedArg | None = ...\n@@ -278,7 +280,7 @@ complex128: Any\n complex64: Any\n complex_: Any\n complexfloating = _np.complexfloating\n-def compress(condition: ArrayLike, a: ArrayLike, axis: int | None = ...,\n+def compress(condition: ArrayLike, a: ArrayLike, axis: int | None = ..., *,\n              size: int | None = ..., fill_value: ArrayLike = ..., out: None = ...) -> Array: ...\n def concat(arrays: Sequence[ArrayLike], /, *, axis: int | None = 0) -> Array: ...\n def concatenate(\n@@ -314,9 +316,9 @@ def cross(\n     axis: int | None = ...,\n ) -> Array: ...\n csingle: Any\n-def cumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def cumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n             out: None = ...) -> Array: ...\n-def cumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def cumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n            out: None = ...) -> Array: ...\n def cumulative_prod(x: ArrayLike, /, *, axis: int | None = ...,\n                     dtype: DTypeLike | None = ...,\n@@ -371,7 +373,6 @@ def einsum(\n     optimize: str | builtins.bool | list[tuple[int, ...]] = ...,\n     precision: PrecisionLike = ...,\n     preferred_element_type: DTypeLike | None = ...,\n-    _use_xeinsum: builtins.bool = False,\n     _dot_general: Callable[..., Array] = ...,\n     out_sharding: NamedSharding | P | None = ...,\n ) -> Array: ...\n@@ -385,7 +386,6 @@ def einsum(\n     optimize: str | builtins.bool | list[tuple[int, ...]] = ...,\n     precision: PrecisionLike = ...,\n     preferred_element_type: DTypeLike | None = ...,\n-    _use_xeinsum: builtins.bool = False,\n     _dot_general: Callable[..., Array] = ...,\n     out_sharding: NamedSharding | P | None = ...,\n ) -> Array: ...\n@@ -397,7 +397,6 @@ def einsum(\n     optimize: str | builtins.bool | list[tuple[int, ...]] = ...,\n     precision: PrecisionLike = ...,\n     preferred_element_type: DTypeLike | None = ...,\n-    _use_xeinsum: builtins.bool = ...,\n     _dot_general: Callable[..., Array] = ...,\n     out_sharding: NamedSharding | P | None = ...,\n ) -> Array: ...\n@@ -422,7 +421,7 @@ def einsum_path(\n     optimize: str | builtins.bool | list[tuple[int, ...]] =  ...,\n ) -> tuple[list[tuple[int, ...]], Any]: ...\n \n-def empty(shape: Any, dtype: DTypeLike | None = ...,\n+def empty(shape: Any, dtype: DTypeLike | None = ..., *,\n           device: _Device | _Sharding | None = ...) -> Array: ...\n def empty_like(prototype: ArrayLike | DuckTypedArray,\n                dtype: DTypeLike | None = ...,\n@@ -579,17 +578,17 @@ def intersect1d(ar1: ArrayLike, ar2: ArrayLike, assume_unique: builtins.bool = .\n def invert(x: ArrayLike, /) -> Array: ...\n def isclose(a: ArrayLike, b: ArrayLike, rtol: ArrayLike = ...,\n             atol: ArrayLike = ..., equal_nan: builtins.bool = ...) -> Array: ...\n-def iscomplex(m: ArrayLike) -> Array: ...\n+def iscomplex(x: ArrayLike) -> Array: ...\n def iscomplexobj(x: Any) -> builtins.bool: ...\n def isdtype(dtype: DTypeLike, kind: DType | str | tuple[DType | str, ...]) -> builtins.bool: ...\n def isfinite(x: ArrayLike, /) -> Array: ...\n-def isin(element: ArrayLike, test_elements: ArrayLike,\n-         assume_unique: builtins.bool = ..., invert: builtins.bool = ..., method: str = ...) -> Array: ...\n+def isin(element: ArrayLike, test_elements: ArrayLike, assume_unique: builtins.bool = ...,\n+         invert: builtins.bool = ..., *, method: str = ...) -> Array: ...\n def isinf(x: ArrayLike, /) -> Array: ...\n def isnan(x: ArrayLike, /) -> Array: ...\n def isneginf(x: ArrayLike, /) -> Array: ...\n def isposinf(x: ArrayLike, /) -> Array: ...\n-def isreal(m: ArrayLike) -> Array: ...\n+def isreal(x: ArrayLike) -> Array: ...\n def isrealobj(x: Any) -> builtins.bool: ...\n def isscalar(element: Any) -> builtins.bool: ...\n def issubdtype(arg1: DTypeLike, arg2: DTypeLike) -> builtins.bool: ...\n@@ -644,7 +643,7 @@ def logspace(start: ArrayLike, stop: ArrayLike, num: int = ...,\n              endpoint: builtins.bool = ..., base: ArrayLike = ...,\n              dtype: DTypeLike | None = ..., axis: int = ...) -> Array: ...\n def mask_indices(\n-    n: int, mask_func: Callable, k: int = ...\n+    n: int, mask_func: Callable, k: int = ..., *, size: int | None = ...\n ) -> tuple[Array, ...]: ...\n def matmul(\n     a: ArrayLike, b: ArrayLike, *, precision: PrecisionLike = ...,\n@@ -655,7 +654,7 @@ def max(a: ArrayLike, axis: _Axis = ..., out: None = ...,\n         keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n         where: ArrayLike | None = ...) -> Array: ...\n def maximum(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n-def mean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def mean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n          out: None = ..., keepdims: builtins.bool = ..., *,\n          where: ArrayLike | None = ...) -> Array: ...\n def median(a: ArrayLike, axis: int | tuple[int, ...] | None = ...,\n@@ -689,14 +688,14 @@ def nanargmin(\n     out: None = ...,\n     keepdims: builtins.bool | None = ...,\n ) -> Array: ...\n-def nancumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def nancumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n                out: None = ...) -> Array: ...\n-def nancumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def nancumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n                out: None = ...) -> Array: ...\n def nanmax(a: ArrayLike, axis: _Axis = ..., out: None = ...,\n            keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n            where: ArrayLike | None = ...) -> Array: ...\n-def nanmean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nanmean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n             out: None = ...,\n             keepdims: builtins.bool = ...,\n             where: ArrayLike | None = ...) -> Array: ...\n@@ -710,21 +709,21 @@ def nanpercentile(a: ArrayLike, q: ArrayLike,\n                   axis: int | tuple[int, ...] | None = ...,\n                   out: None = ..., overwrite_input: builtins.bool = ..., method: str = ...,\n                   keepdims: builtins.bool = ..., *, interpolation: DeprecatedArg | str = ...) -> Array: ...\n-def nanprod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nanprod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n             out: None = ...,\n             keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n             where: ArrayLike | None = ...) -> Array: ...\n def nanquantile(a: ArrayLike, q: ArrayLike, axis: int | tuple[int, ...] | None = ...,\n                 out: None = ..., overwrite_input: builtins.bool = ..., method: str = ...,\n                 keepdims: builtins.bool = ..., *, interpolation: DeprecatedArg | str = ...) -> Array: ...\n-def nanstd(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ..., out: None = ...,\n-           ddof: int = ..., keepdims: builtins.bool = ...,\n+def nanstd(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n+           out: None = ..., ddof: int = ..., keepdims: builtins.bool = ...,\n            where: ArrayLike | None = ...) -> Array: ...\n-def nansum(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nansum(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n            out: None = ..., keepdims: builtins.bool = ...,\n            initial: ArrayLike | None = ...,\n            where: ArrayLike | None = ...) -> Array: ...\n-def nanvar(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nanvar(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n            out: None = ...,\n            ddof: int = 0, keepdims: builtins.bool = False,\n            where: ArrayLike | None = ...) -> Array: ...\n@@ -740,7 +739,7 @@ def not_equal(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n number = _np.number\n object_ = _np.object_\n ogrid: _Ogrid\n-def ones(shape: Any, dtype: DTypeLike | None = ...,\n+def ones(shape: Any, dtype: DTypeLike | None = ..., *,\n          device: _Device | _Sharding | None = ...) -> Array: ...\n def ones_like(a: ArrayLike | DuckTypedArray,\n               dtype: DTypeLike | None = ...,\n@@ -782,7 +781,7 @@ def positive(x: ArrayLike, /) -> Array: ...\n def pow(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n def power(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n printoptions = _np.printoptions\n-def prod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def prod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n          out: None = ..., keepdims: builtins.bool = ...,\n          initial: ArrayLike | None = ..., where: ArrayLike | None = ...,\n          promote_integers: builtins.bool = ...) -> Array: ...\n@@ -805,7 +804,6 @@ def ravel_multi_index(multi_index: Sequence[ArrayLike], dims: Sequence[int],\n                       mode: str = ..., order: str = ...) -> Array: ...\n def real(x: ArrayLike, /) -> Array: ...\n def reciprocal(x: ArrayLike, /) -> Array: ...\n-register_jax_array_methods: Any\n def remainder(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n def repeat(a: ArrayLike, repeats: ArrayLike, axis: int | None = ..., *,\n            total_repeat_length: int | None = ...,\n@@ -844,7 +842,8 @@ def setdiff1d(\n     size: int | None = ...,\n     fill_value: ArrayLike | None = ...,\n ) -> Array: ...\n-def setxor1d(ar1: ArrayLike, ar2: ArrayLike, assume_unique: builtins.bool = ...) -> Array: ...\n+def setxor1d(ar1: ArrayLike, ar2: ArrayLike, assume_unique: builtins.bool = ..., *,\n+             size: int | None = ..., fill_value: ArrayLike | None = ...) -> Array: ...\n def shape(a: ArrayLike | SupportsShape) -> tuple[int, ...]: ...\n def sign(x: ArrayLike, /) -> Array: ...\n def signbit(x: ArrayLike, /) -> Array: ...\n@@ -882,14 +881,14 @@ def stack(\n     out: None = ...,\n     dtype: DTypeLike | None = ...,\n ) -> Array: ...\n-def std(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def std(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n         out: None = ..., ddof: int = ..., keepdims: builtins.bool = ..., *,\n         where: ArrayLike | None = ..., correction: int | float | None = ...) -> Array: ...\n subtract: BinaryUfunc\n def sum(\n     a: ArrayLike,\n     axis: _Axis = ...,\n-    dtype: DTypeLike = ...,\n+    dtype: DTypeLike | None = ...,\n     out: None = ...,\n     keepdims: builtins.bool = ...,\n     initial: ArrayLike | None = ...,\n@@ -927,7 +926,7 @@ def transpose(a: ArrayLike, axes: Sequence[int] | None = ...) -> Array: ...\n def trapezoid(y: ArrayLike, x: ArrayLike | None = None, dx: ArrayLike = ...,\n               axis: int = ...) -> Array: ...\n def tri(\n-    N: int, M: int | None = ..., k: int = ..., dtype: DTypeLike = ...\n+    N: int, M: int | None = ..., k: int = ..., dtype: DTypeLike | None = ...\n ) -> Array: ...\n def tril(m: ArrayLike, k: int = ...) -> Array: ...\n def tril_indices(\n@@ -970,7 +969,7 @@ class _UniqueInverseResult(NamedTuple):\n def unique(ar: ArrayLike, return_index: builtins.bool = ..., return_inverse: builtins.bool = ...,\n            return_counts: builtins.bool = ..., axis: int | None = ...,\n            *, equal_nan: builtins.bool = ..., size: int | None = ...,\n-           fill_value: ArrayLike | None = ...\n+           fill_value: ArrayLike | None = ..., sorted: bool = ...,\n ): ...\n def unique_all(x: ArrayLike, /, *, size: int | None = ...,\n                fill_value: ArrayLike | None = ...) -> _UniqueAllResult: ...\n@@ -994,7 +993,7 @@ def unwrap(p: ArrayLike, discont: ArrayLike | None = ...,\n def vander(\n     x: ArrayLike, N: int | None = ..., increasing: builtins.bool = ...\n ) -> Array: ...\n-def var(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def var(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n         out: None = ..., ddof: int = ..., keepdims: builtins.bool = ..., *,\n         where: ArrayLike | None = ..., correction: int | float | None = ...) -> Array: ...\n def vdot(\n@@ -1029,7 +1028,7 @@ def where(condition: ArrayLike, x: ArrayLike | None = ...,\n           fill_value: None | ArrayLike | tuple[ArrayLike, ...] = ...\n           ) -> Array | tuple[Array, ...]: ...\n \n-def zeros(shape: Any, dtype: DTypeLike | None = ...,\n+def zeros(shape: Any, dtype: DTypeLike | None = ..., *,\n           device: _Device | _Sharding | None = ...) -> Array: ...\n def zeros_like(a: ArrayLike | DuckTypedArray,\n                dtype: DTypeLike | None = ...,\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28317 from yhtang:pr-add-k8s-docs\n\nPiperOrigin-RevId: 759252455",
        "output": [
            "```diff\nCommit: 72f540eb7f3d0355eb43114a8770e93727860259\nDate: 2025-05-15T19:25:23Z\nURL: https://github.com/jax-ml/jax/commit/72f540eb7f3d0355eb43114a8770e93727860259\nFiles changed: 3\nAdditions: +89, Deletions: -2\ndiff --git a/.github/workflows/k8s.yaml b/.github/workflows/k8s.yaml\nindex 470a899a187e..5756b1afbbd2 100644\n--- a/.github/workflows/k8s.yaml\n+++ b/.github/workflows/k8s.yaml\n@@ -1,4 +1,4 @@\n-name: Distributed run using K8s Jobset\n+name: Multi-process run using K8s\n \n on:\n   push:\ndiff --git a/docs/multi_process.md b/docs/multi_process.md\nindex 8ecc51cc2557..f8c2566ca872 100644\n--- a/docs/multi_process.md\n+++ b/docs/multi_process.md\n@@ -307,6 +307,83 @@ what it prints:\n \n Woohoo, look at all those TPU cores!\n \n+### Kubernetes Example\n+\n+Running multi-controller JAX on a Kubernetes cluster is almost identical in spirit to the GPU and TPU examples above: every pod runs the same Python program, JAX discovers its peers, and the cluster behaves like one giant machine.\n+\n+1. **Container image** - start from a JAX-enabled image, e.g. one of the public JAX AI images on Google Artifact Registry ([TPU][google-artifact-tpu] / [GPU][google-artifact-gpu]) or NVIDIA ([NGC][nvidia-ngc] / [JAX-Toolbox][nvidia-jax-toolbox]).\n+\n+2. **Workload type** - use either a [JobSet][k8s-jobset] or an [indexed Job][k8s-indexed-job]. Each replica corresponds to one JAX process.\n+\n+3. **Service Account** - JAX needs permission to list the pods that belong to the job so that processes discover their peers. A minimal RBAC setup is provided in [examples/k8s/svc-acct.yaml][rbac-svc-acct].\n+\n+Below is a [minimal JobSet][minimal-jobset] that launches two replicas. Replace the placeholders - \n+image, GPU count, and any private registry secrets - with values that match your environment.\n+\n+```yaml\n+apiVersion: jobset.x-k8s.io/v1alpha2\n+kind: JobSet\n+metadata:\n+  name: jaxjob\n+spec:\n+  replicatedJobs:\n+  - name: workers\n+    template:\n+      spec:\n+        parallelism: 2\n+        completions: 2\n+        backoffLimit: 0\n+        template:\n+          spec:\n+            serviceAccountName: jax-job-sa  # kubectl apply -f svc-acct.yaml\n+            restartPolicy: Never\n+            imagePullSecrets:\n+              # https://k8s.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n+            - name: null\n+            containers:\n+            - name: main\n+              image: null  # e.g. ghcr.io/nvidia/jax:jax\n+              imagePullPolicy: Always\n+              resources:\n+                limits:\n+                  cpu: 1\n+                  # https://k8s.io/docs/tasks/manage-gpus/scheduling-gpus/\n+                  nvidia.com/gpu: null\n+              command: \n+                - python\n+              args:\n+                - -c\n+                - |\n+                  import jax\n+                  jax.distributed.initialize()\n+                  print(jax.devices())\n+                  print(jax.local_devices())\n+                  assert jax.process_count() > 1\n+                  assert len(jax.devices()) > len(jax.local_devices())\n+```\n+\n+Apply the manifest and watch the pods complete:\n+\n+```bash\n+$ kubectl apply -f example.yaml\n+$ kubectl get pods -l jobset.sigs.k8s.io/jobset-name=jaxjob\n+NAME                       READY   STATUS      RESTARTS   AGE\n+jaxjob-workers-0-0-xpx8l   0/1     Completed   0          8m32s\n+jaxjob-workers-0-1-ddkq8   0/1     Completed   0          8m32s\n+```\n+\n+When the job finishes, inspect the logs to confirm that every process saw all accelerators:\n+\n+```bash\n+$ kubectl logs -l jobset.sigs.k8s.io/jobset-name=jaxjob\n+[CudaDevice(id=0), CudaDevice(id=1)]\n+[CudaDevice(id=0)]\n+[CudaDevice(id=0), CudaDevice(id=1)]\n+[CudaDevice(id=1)]\n+```\n+\n+Every pod should have the same set of global devices and a different set of local devices. At this point, you can replace the inline script with your real JAX program.\n+\n Once the processes are set up, we can start building global {class}`jax.Array`s\n and running computations. The remaining Python code examples in this tutorial\n are meant to be run on all processes simultaneously, after running\n@@ -580,3 +657,11 @@ assert (np.all(\n [distributed_arrays]: https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html\n [gpu_machines]: https://cloud.google.com/compute/docs/gpus\n [unified_sharding]: https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html\n+[google-artifact-tpu]: https://console.cloud.google.com/artifacts/docker/cloud-tpu-images/us/jax-ai-image/tpu\n+[google-artifact-gpu]: https://console.cloud.google.com/artifacts/docker/deeplearning-images/us-central1/jax-ai-image/gpu\n+[nvidia-ngc]: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/jax\n+[nvidia-jax-toolbox]: https://github.com/NVIDIA/JAX-Toolbox\n+[k8s-jobset]: https://github.com/kubernetes-sigs/jobset\n+[k8s-indexed-job]: https://kubernetes.io/docs/concepts/workloads/controllers/job/#parallel-jobs\n+[rbac-svc-acct]: https://github.com/jax-ml/jax/blob/main/examples/k8s/svc-acct.yaml\n+[minimal-jobset]: https://github.com/jax-ml/jax/blob/main/examples/k8s/example.yaml\ndiff --git a/jax/_src/clusters/k8s_cluster.py b/jax/_src/clusters/k8s_cluster.py\nindex b40b39cade34..af1b7c020eed 100644\n--- a/jax/_src/clusters/k8s_cluster.py\n+++ b/jax/_src/clusters/k8s_cluster.py\n@@ -107,7 +107,9 @@ def _handle_api_exception(cls):\n           \"this job does not have the permission for pod introspection. Please \"\n           \"either grant the default SA permission to read pod info, or create a \"\n           \"dedicated service account with the permission and associated with \"\n-          \"the job. For more details, see <PLACERHOLDER_LINK>.\",\n+          \"the job. For an example on setting up the service account, see the \"\n+          \"example/k8s directory in the JAX repo. For more details, please refer to \"\n+          \"https://docs.jax.dev/en/latest/multi_process.html#kubernetes-example\",\n           width=80\n         ))\n       raise RuntimeError('\\n'.join(err_msg)) from e\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28757 from mattjj:custom-vjp-aval-mismatch-extra\n\nPiperOrigin-RevId: 759203972",
        "output": [
            "```diff\nCommit: 053326344be62a35071f41bd45e9a3e9318d1f0b\nDate: 2025-05-15T17:30:45Z\nURL: https://github.com/jax-ml/jax/commit/053326344be62a35071f41bd45e9a3e9318d1f0b\nFiles changed: 5\nAdditions: +27, Deletions: -26\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex e004263abe71..c730e1c289ae 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -2777,6 +2777,26 @@ def typematch(t1: AbstractValue, t2: AbstractValue) -> bool:\n   else:\n     return False\n \n+def aval_mismatch_extra(a1: AbstractValue, a2: AbstractValue) -> str:\n+  assert not typematch(a1, a2)\n+  if isinstance(a1, ShapedArray) and isinstance(a2, ShapedArray):\n+    mismatches = []\n+    if a1.dtype != a2.dtype:\n+      mismatches.append('the dtypes do not match')\n+    if a1.shape != a2.shape:\n+      mismatches.append('the shapes do not match')\n+    if a1.vma != a2.vma:\n+      mismatches.append('the varying manual axes do not match')\n+    # TODO(yashkatariya,mattjj): add check for sharding-in-types mismatch\n+\n+    if len(mismatches) == 0:\n+      return ''\n+    elif len(mismatches) == 1:\n+      return ', so ' + mismatches[0]\n+    else:\n+      return ', so ' + ', '.join(mismatches[:-1]) + ', and ' + mismatches[-1]\n+  return ''\n+\n class JaxprTypeError(TypeError): pass\n \n custom_typechecks: dict[Primitive, Callable] = {}\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex 7b81c4e86889..9b28595e1835 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -917,7 +917,8 @@ def append(x, d):\n                \"shape/dtypes as the args tuple of the primal function, but at \"\n                f\"output{keystr(kp)} the bwd rule produced an output of \"\n                f\"shape/dtype {a_.str_short()} corresponding \"\n-               f\"to an input of shape/dtype {a.str_short()}.\")\n+               f\"to an input of shape/dtype {a.str_short()}\"\n+               f\"{core.aval_mismatch_extra(a, a_)}\")\n         raise ValueError(msg)\n       results.append(ct)\n   return results\ndiff --git a/jax/_src/lax/control_flow/common.py b/jax/_src/lax/control_flow/common.py\nindex 87dbcd8d3f32..b75cbf6ac708 100644\n--- a/jax/_src/lax/control_flow/common.py\n+++ b/jax/_src/lax/control_flow/common.py\n@@ -260,23 +260,3 @@ def _show_diff(array1, array2):\n def _avals_short(avals):\n   to_str = lambda aval: getattr(aval, 'str_short', partial(str, aval))()\n   return ' '.join(map(to_str, avals))\n-\n-def _aval_mismatch_extra(a1: core.AbstractValue, a2: core.AbstractValue) -> str:\n-  assert not core.typematch(a1, a2)\n-  if isinstance(a1, core.ShapedArray) and isinstance(a2, core.ShapedArray):\n-    mismatches = []\n-    if a1.dtype != a2.dtype:\n-      mismatches.append('the dtypes do not match')\n-    if a1.shape != a2.shape:\n-      mismatches.append('the shapes do not match')\n-    if a1.vma != a2.vma:\n-      mismatches.append('the varying manual axes do not match')\n-    # TODO(yashkatariya,mattjj): add check for sharding-in-types mismatch\n-\n-    if len(mismatches) == 0:\n-      return ''\n-    elif len(mismatches) == 1:\n-      return ', so ' + mismatches[0]\n-    else:\n-      return ', so ' + ', '.join(mismatches[:-1]) + ', and ' + mismatches[-1]\n-  return ''\ndiff --git a/jax/_src/lax/control_flow/conditionals.py b/jax/_src/lax/control_flow/conditionals.py\nindex d875989921d0..741636c47e31 100644\n--- a/jax/_src/lax/control_flow/conditionals.py\n+++ b/jax/_src/lax/control_flow/conditionals.py\n@@ -53,8 +53,8 @@\n import numpy as np\n \n from jax._src.lax.control_flow.common import (\n-    _avals_short, _typecheck_param, _aval_mismatch_extra,\n-    _initial_style_jaxprs_with_common_consts, _make_closed_jaxpr, _prune_zeros)\n+    _avals_short, _typecheck_param, _initial_style_jaxprs_with_common_consts,\n+    _make_closed_jaxpr, _prune_zeros)\n \n map, unsafe_map = safe_map, map\n \n@@ -351,7 +351,7 @@ def _check_branch_outputs(\n   if not all(map(core.typematch, out_avals1, out_avals2)):\n     diffs = [f'the output of {name1}{component(p)} has type {a1.str_short()}'\n              f' but the corresponding output of {name2} has type '\n-             f'{a2.str_short()}{_aval_mismatch_extra(a1, a2)}'\n+             f'{a2.str_short()}{core.aval_mismatch_extra(a1, a2)}'\n              for p, a1, a2 in zip(paths, out_avals1, out_avals2)\n              if not core.typematch(a1, a2)]\n     if len(diffs) == 0:\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex c85a23b6b199..7efe3294fdca 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -51,7 +51,7 @@\n from jax._src.lax.control_flow.common import (\n     _avals_short, _initial_style_jaxpr,\n     _initial_style_jaxpr_attrs, _make_closed_jaxpr_attrs, _prune_zeros,\n-    _typecheck_param, _aval_mismatch_extra)\n+    _typecheck_param)\n from jax._src.lax.other import logaddexp\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n@@ -478,7 +478,7 @@ def _check_carry_type(name, body_fun, in_carry, out_carry_tree, out_avals):\n   if not all(_map(core.typematch, in_avals, out_avals)):\n     diffs = [f'{component(path)} has type {in_aval.str_short()}'\n              ' but the corresponding output carry component has type '\n-             f'{out_aval.str_short()}{_aval_mismatch_extra(in_aval, out_aval)}'\n+             f'{out_aval.str_short()}{core.aval_mismatch_extra(in_aval, out_aval)}'\n              for path, in_aval, out_aval in zip(paths, in_avals, out_avals)\n              if not core.typematch(in_aval, out_aval)]\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "improve error message with when custom_vjp bwd rule produces wrong shape/dtype",
        "output": [
            "```diff\nCommit: 0984dc8bbcb9406a86111c700cb7cbbb3faedbe8\nDate: 2025-05-15T16:46:32Z\nURL: https://github.com/jax-ml/jax/commit/0984dc8bbcb9406a86111c700cb7cbbb3faedbe8\nFiles changed: 5\nAdditions: +27, Deletions: -26\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex e004263abe71..c730e1c289ae 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -2777,6 +2777,26 @@ def typematch(t1: AbstractValue, t2: AbstractValue) -> bool:\n   else:\n     return False\n \n+def aval_mismatch_extra(a1: AbstractValue, a2: AbstractValue) -> str:\n+  assert not typematch(a1, a2)\n+  if isinstance(a1, ShapedArray) and isinstance(a2, ShapedArray):\n+    mismatches = []\n+    if a1.dtype != a2.dtype:\n+      mismatches.append('the dtypes do not match')\n+    if a1.shape != a2.shape:\n+      mismatches.append('the shapes do not match')\n+    if a1.vma != a2.vma:\n+      mismatches.append('the varying manual axes do not match')\n+    # TODO(yashkatariya,mattjj): add check for sharding-in-types mismatch\n+\n+    if len(mismatches) == 0:\n+      return ''\n+    elif len(mismatches) == 1:\n+      return ', so ' + mismatches[0]\n+    else:\n+      return ', so ' + ', '.join(mismatches[:-1]) + ', and ' + mismatches[-1]\n+  return ''\n+\n class JaxprTypeError(TypeError): pass\n \n custom_typechecks: dict[Primitive, Callable] = {}\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex 7b81c4e86889..9b28595e1835 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -917,7 +917,8 @@ def append(x, d):\n                \"shape/dtypes as the args tuple of the primal function, but at \"\n                f\"output{keystr(kp)} the bwd rule produced an output of \"\n                f\"shape/dtype {a_.str_short()} corresponding \"\n-               f\"to an input of shape/dtype {a.str_short()}.\")\n+               f\"to an input of shape/dtype {a.str_short()}\"\n+               f\"{core.aval_mismatch_extra(a, a_)}\")\n         raise ValueError(msg)\n       results.append(ct)\n   return results\ndiff --git a/jax/_src/lax/control_flow/common.py b/jax/_src/lax/control_flow/common.py\nindex 87dbcd8d3f32..b75cbf6ac708 100644\n--- a/jax/_src/lax/control_flow/common.py\n+++ b/jax/_src/lax/control_flow/common.py\n@@ -260,23 +260,3 @@ def _show_diff(array1, array2):\n def _avals_short(avals):\n   to_str = lambda aval: getattr(aval, 'str_short', partial(str, aval))()\n   return ' '.join(map(to_str, avals))\n-\n-def _aval_mismatch_extra(a1: core.AbstractValue, a2: core.AbstractValue) -> str:\n-  assert not core.typematch(a1, a2)\n-  if isinstance(a1, core.ShapedArray) and isinstance(a2, core.ShapedArray):\n-    mismatches = []\n-    if a1.dtype != a2.dtype:\n-      mismatches.append('the dtypes do not match')\n-    if a1.shape != a2.shape:\n-      mismatches.append('the shapes do not match')\n-    if a1.vma != a2.vma:\n-      mismatches.append('the varying manual axes do not match')\n-    # TODO(yashkatariya,mattjj): add check for sharding-in-types mismatch\n-\n-    if len(mismatches) == 0:\n-      return ''\n-    elif len(mismatches) == 1:\n-      return ', so ' + mismatches[0]\n-    else:\n-      return ', so ' + ', '.join(mismatches[:-1]) + ', and ' + mismatches[-1]\n-  return ''\ndiff --git a/jax/_src/lax/control_flow/conditionals.py b/jax/_src/lax/control_flow/conditionals.py\nindex d875989921d0..741636c47e31 100644\n--- a/jax/_src/lax/control_flow/conditionals.py\n+++ b/jax/_src/lax/control_flow/conditionals.py\n@@ -53,8 +53,8 @@\n import numpy as np\n \n from jax._src.lax.control_flow.common import (\n-    _avals_short, _typecheck_param, _aval_mismatch_extra,\n-    _initial_style_jaxprs_with_common_consts, _make_closed_jaxpr, _prune_zeros)\n+    _avals_short, _typecheck_param, _initial_style_jaxprs_with_common_consts,\n+    _make_closed_jaxpr, _prune_zeros)\n \n map, unsafe_map = safe_map, map\n \n@@ -351,7 +351,7 @@ def _check_branch_outputs(\n   if not all(map(core.typematch, out_avals1, out_avals2)):\n     diffs = [f'the output of {name1}{component(p)} has type {a1.str_short()}'\n              f' but the corresponding output of {name2} has type '\n-             f'{a2.str_short()}{_aval_mismatch_extra(a1, a2)}'\n+             f'{a2.str_short()}{core.aval_mismatch_extra(a1, a2)}'\n              for p, a1, a2 in zip(paths, out_avals1, out_avals2)\n              if not core.typematch(a1, a2)]\n     if len(diffs) == 0:\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex c85a23b6b199..7efe3294fdca 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -51,7 +51,7 @@\n from jax._src.lax.control_flow.common import (\n     _avals_short, _initial_style_jaxpr,\n     _initial_style_jaxpr_attrs, _make_closed_jaxpr_attrs, _prune_zeros,\n-    _typecheck_param, _aval_mismatch_extra)\n+    _typecheck_param)\n from jax._src.lax.other import logaddexp\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n@@ -478,7 +478,7 @@ def _check_carry_type(name, body_fun, in_carry, out_carry_tree, out_avals):\n   if not all(_map(core.typematch, in_avals, out_avals)):\n     diffs = [f'{component(path)} has type {in_aval.str_short()}'\n              ' but the corresponding output carry component has type '\n-             f'{out_aval.str_short()}{_aval_mismatch_extra(in_aval, out_aval)}'\n+             f'{out_aval.str_short()}{core.aval_mismatch_extra(in_aval, out_aval)}'\n              for path, in_aval, out_aval in zip(paths, in_avals, out_avals)\n              if not core.typematch(in_aval, out_aval)]\n \n\n```"
        ],
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "#sdy Fix incorrect sharding on a token during a callback.\n\nThe \"add a token\" part of the `callback` primitive's MLIR lowering was incorrectly adding a ranked sharding by using the sharding of a ranked tensor. So instead create an unranked sharding explicitly\n\nPiperOrigin-RevId: 759135477",
        "output": [
            "```diff\nCommit: afdf51d797da6b851b80d40fda856040e75c641e\nDate: 2025-05-15T14:15:14Z\nURL: https://github.com/jax-ml/jax/commit/afdf51d797da6b851b80d40fda856040e75c641e\nFiles changed: 1\nAdditions: +13, Deletions: -10\ndiff --git a/jax/_src/callback.py b/jax/_src/callback.py\nindex 7fcccac14950..5b5ec593a550 100644\n--- a/jax/_src/callback.py\n+++ b/jax/_src/callback.py\n@@ -40,7 +40,7 @@\n from jax._src.lib import xla_client as xc\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n-from jax._src.sharding_impls import SdyArray, SdyArrayList, SingleDeviceSharding\n+from jax._src.sharding_impls import SdyArray, SdyArrayList, SdyDim, SingleDeviceSharding\n from jax._src.typing import DeprecatedArg\n import numpy as np\n \n@@ -157,11 +157,11 @@ def _callback_op_sharding(\n       ndim = 0\n       if avals_out and isinstance(avals_out[0], core.ShapedArray):\n         ndim = avals_out[0].ndim\n-      op_sharding = sharding_impls.SdyArrayList([\n-          sharding_impls.SdyArray(\n+      op_sharding = SdyArrayList([\n+          SdyArray(\n               mesh_shape=(),\n               dim_shardings=[\n-                  sharding_impls.SdyDim(axes=[], is_open=False)\n+                  SdyDim(axes=[], is_open=False)\n               ] * ndim,\n               logical_device_ids=())])\n     else:\n@@ -199,8 +199,8 @@ def _callback_op_sharding(\n       # number of result ops. If there are no result ops, we need 1 shardy\n       # annotation.\n       num_sdy_shardings = max(1, len(avals_out))\n-      op_sharding = sharding_impls.SdyArrayList(num_sdy_shardings * [\n-          sharding_impls.SdyArray(\n+      op_sharding = SdyArrayList(num_sdy_shardings * [\n+          SdyArray(\n               mesh_shape=(),\n               dim_shardings=[],\n               logical_device_ids=(device_index,))])\n@@ -838,14 +838,17 @@ def _wrapped_callback(token, *args):  # type: ignore  # pylint: disable=function\n         config.use_shardy_partitioner.value\n         and sharding is not None\n         and len(ctx.avals_out) > 0\n-        and isinstance(sharding, sharding_impls.SdyArrayList)\n+        and isinstance(sharding, SdyArrayList)\n     ):\n       # Add a sharding annotation for the token if we have at least one\n       # output. Otherwise, the single shardy annotation required of all ops\n       # (even those without any results) can annotate the token.\n-      sharding = sharding_impls.SdyArrayList(\n-          [*sharding.shardings, sharding.shardings[-1]]\n-      )\n+      sharding = SdyArrayList([\n+          SdyArray(\n+              mesh_shape=(),\n+              dim_shardings=[],\n+              logical_device_ids=()),\n+          *sharding.shardings])\n     ctx = dataclasses.replace(\n         ctx,\n         avals_in=[core.abstract_token, *ctx.avals_in],\n\n```"
        ],
        "from_id": [
            "bartchr808",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "#sdy Properly handle token types in JAX and `ManualComputationOp`.\n\nWe weren't handling them correctly meaning you couldn't use a `shard_map`/`ManualComputationOp` which has callbacks inside.\n\nPiperOrigin-RevId: 759072597",
        "output": [
            "```diff\nCommit: 0a0368bb2add564122617bb68caf731942d41279\nDate: 2025-05-15T10:37:53Z\nURL: https://github.com/jax-ml/jax/commit/0a0368bb2add564122617bb68caf731942d41279\nFiles changed: 6\nAdditions: +84, Deletions: -27\ndiff --git a/jax/_src/callback.py b/jax/_src/callback.py\nindex bc233b634f3c..7fcccac14950 100644\n--- a/jax/_src/callback.py\n+++ b/jax/_src/callback.py\n@@ -154,13 +154,15 @@ def _callback_op_sharding(\n           \" computations\"\n       )\n     if config.use_shardy_partitioner.value:\n-      assert len(avals_out) == 1\n+      ndim = 0\n+      if avals_out and isinstance(avals_out[0], core.ShapedArray):\n+        ndim = avals_out[0].ndim\n       op_sharding = sharding_impls.SdyArrayList([\n           sharding_impls.SdyArray(\n               mesh_shape=(),\n               dim_shardings=[\n                   sharding_impls.SdyDim(axes=[], is_open=False)\n-              ] * avals_out[0].ndim,\n+              ] * ndim,\n               logical_device_ids=())])\n     else:\n       op_sharding = xc.OpSharding()  # type: ignore[assignment]\ndiff --git a/jax/_src/debugging.py b/jax/_src/debugging.py\nindex c2febf752b92..e931a6edb9b3 100644\n--- a/jax/_src/debugging.py\n+++ b/jax/_src/debugging.py\n@@ -164,14 +164,16 @@ def debug_callback_lowering(ctx, *args, effect, partitioned, callback, **params)\n       # If we have fully manual sharding during lowering, that means the JAX\n       # program has per-device semantics, so we run the callback on each device.\n       if config.use_shardy_partitioner.value:\n-        assert len(ctx.avals_out) == 1\n+        ndim = 0\n+        if ctx.avals_out and isinstance(ctx.avals_out[0], core.ShapedArray):\n+          ndim = ctx.avals_out[0].ndim\n         sharding = sharding_impls.SdyArrayList([\n             sharding_impls.SdyArray(\n                 mesh_shape=(),\n                 dim_shardings=[\n                     sharding_impls.SdyDim(axes=[], is_open=False)\n-                ] * ctx.avals_out[0].ndim,\n-                logical_device_ids=())])\n+                ] * ndim,\n+                logical_device_ids=(0,))])\n       else:\n         sharding = xc.OpSharding()\n         sharding.type = xc.OpSharding.Type.MANUAL\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex abcc2ca0acf1..ac529a667d04 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -44,7 +44,7 @@\n                            get_abstract_mesh, get_concrete_mesh)\n from jax._src.api import _shared_code_pmap, _prepare_pmap\n from jax._src.lib.mlir import ir\n-from jax._src.lib.mlir.dialects import sdy\n+from jax._src.lib.mlir.dialects import hlo, sdy\n from jax._src.util import (HashableFunction, HashablePartial, unzip2,\n                            as_hashable_function, memoize, partition_list,\n                            merge_lists, split_list, subs_list2)\n@@ -786,6 +786,13 @@ def _shardy_shard_map_sharding(\n   return sdy_sharding\n \n \n+def _shardy_shard_map_token_sharding(\n+    ctx: mlir.LoweringRuleContext, mesh\n+  ) -> ir.Attribute:\n+  ns = _make_scoped_manual_sharding(ctx, mesh, {})\n+  return ns._to_sdy_sharding(0)\n+\n+\n def _shard_map_lowering_shardy(\n     ctx, in_nodes, jaxpr, mesh, in_names, out_names, manual_axes, check_vma):\n   axis_ctx = ctx.module_context.axis_context\n@@ -799,36 +806,70 @@ def _shard_map_lowering_shardy(\n   new_axis_context = sharding_impls.SPMDAxisContext(mesh, manual_axes)\n   sub_ctx = ctx.module_context.replace(axis_context=new_axis_context)\n \n+  tokens = [ctx.tokens_in.get(eff) for eff in ctx.tokens_in.effects()]\n+  num_tokens = len(tokens)\n   manual_axes = order_wrt_mesh(mesh, shardy_manual_axes)\n   if np.prod([mesh.shape[a] for a in manual_axes]) == 1:\n     # No need for a `ManualComputationOp` if all manual axes are size 1.\n     with _extend_axis_env(mesh, manual_axes), config._check_vma(check_vma):\n-      out_nodes, _ = mlir.jaxpr_subcomp(\n-          sub_ctx, jaxpr, ctx.name_stack, mlir.TokenSet(), (), *in_nodes,\n+      args = (*ctx.dim_var_values, *tokens, *in_nodes)\n+      out_nodes, tokens_out = mlir.jaxpr_subcomp(\n+          sub_ctx, jaxpr, ctx.name_stack,\n+          mlir.TokenSet(zip(ctx.tokens_in.effects(), in_nodes[:num_tokens])),\n+        (), *args[num_tokens:],\n           dim_var_values=ctx.dim_var_values)\n-    return out_nodes\n+      num_tokens = len(tokens_out.effects())\n+      tokens_out = tokens_out.update_tokens(mlir.TokenSet(zip(\n+          ctx.tokens_in.effects(), out_nodes[:num_tokens])))\n+      ctx.set_tokens_out(tokens_out)\n+    return out_nodes[num_tokens:]\n \n-  in_shardings = sharding_impls.SdyArrayList(map(\n+  in_shardings = list(map(\n       partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n-      in_names, ctx.avals_in)).build()\n-  out_shardings = sharding_impls.SdyArrayList(map(\n+      in_names, ctx.avals_in))\n+  num_dim_vars = len(ctx.dim_var_values)\n+  in_shardings = ([_shardy_shard_map_token_sharding(ctx, mesh)]\n+                  * (num_tokens + num_dim_vars) + in_shardings)\n+  in_shardings = sharding_impls.SdyArrayList(in_shardings).build()\n+\n+  out_shardings = list(map(\n       partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n-      out_names, ctx.avals_out)).build()\n-  output_types = map(mlir.aval_to_ir_type, ctx.avals_out)\n+      out_names, ctx.avals_out))\n+  out_shardings = [\n+      _shardy_shard_map_token_sharding(ctx, mesh)] * num_tokens + out_shardings\n+  out_shardings = sharding_impls.SdyArrayList(out_shardings).build()\n+\n+  output_types = ([hlo.TokenType.get()] * num_tokens +\n+                  list(map(mlir.aval_to_ir_type, ctx.avals_out)))\n+\n+  args = (*ctx.dim_var_values, *tokens, *in_nodes)\n   manual_computation_op = sdy.ManualComputationOp(\n-      output_types, in_nodes, in_shardings, out_shardings,\n+      output_types,\n+      mlir.flatten_ir_values(args),\n+      in_shardings, out_shardings,\n       sdy.ManualAxesAttr.get(\n           ir.ArrayAttr.get([ir.StringAttr.get(i) for i in manual_axes])))\n   block = ir.Block.create_at_start(\n-      manual_computation_op.body, map(mlir.aval_to_ir_type, in_avals_))\n+      manual_computation_op.body,\n+      (*(i if isinstance(i, ir.Type) else i.type for i in ctx.dim_var_values),\n+       *([hlo.TokenType.get()] * num_tokens),\n+       *map(mlir.aval_to_ir_type, in_avals_)))\n   with (ir.InsertionPoint(block), _extend_axis_env(mesh, manual_axes),\n         config._check_vma(check_vma)):\n-    out_nodes_, _ = mlir.jaxpr_subcomp(\n-        sub_ctx, jaxpr, ctx.name_stack, mlir.TokenSet(), (), *block.arguments,\n+    out_nodes_, tokens_out = mlir.jaxpr_subcomp(\n+        sub_ctx, jaxpr, ctx.name_stack,\n+        mlir.TokenSet(zip(\n+            ctx.tokens_in.effects(), block.arguments[:num_tokens])),\n+        (), *block.arguments[num_tokens+num_dim_vars:],\n         dim_var_values=ctx.dim_var_values)\n-    sdy.ReturnOp([ir.Value(x) for x in out_nodes_])\n+    sdy.ReturnOp([ir.Value(x) for x in (*[v for _, v in tokens_out.items()],\n+                                        *out_nodes_)])\n+    num_tokens = len(tokens_out.effects())\n+    tokens_out = tokens_out.update_tokens(mlir.TokenSet(zip(\n+        ctx.tokens_in.effects(), manual_computation_op.results[:num_tokens])))\n+    ctx.set_tokens_out(tokens_out)\n \n-  return manual_computation_op.results\n+  return manual_computation_op.results[num_tokens:]\n \n \n def _shard_map_lowering(ctx, *in_nodes, jaxpr, mesh, in_names, out_names,\n@@ -846,7 +887,8 @@ def _shard_map_lowering(ctx, *in_nodes, jaxpr, mesh, in_names, out_names,\n   with _extend_axis_env(mesh, manual_axes), config._check_vma(check_vma):\n     out_nodes_, tokens_out = mlir.call_lowering(\n         \"shmap_body\", ctx.name_stack, jaxpr, None, sub_ctx, in_avals_,\n-        out_avals_, ctx.tokens_in, *in_nodes_, dim_var_values=ctx.dim_var_values,\n+        out_avals_, ctx.tokens_in, *in_nodes_,\n+        dim_var_values=ctx.dim_var_values,\n         arg_names=map(_pspec_mhlo_attrs, in_names, in_avals_),\n         result_names=map(_pspec_mhlo_attrs, out_names, out_avals_))\n   ctx.set_tokens_out(tokens_out)\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex d1c8ec7f050d..7d87705e20bd 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -4485,8 +4485,6 @@ def test_in_out_shardings_unconstrained_error(self):\n               in_shardings=NamedSharding(mesh, P(P.UNCONSTRAINED, 'x')))\n \n   def test_empty_io_callback_under_shard_map(self):\n-    if config.use_shardy_partitioner.value:\n-      self.skipTest(\"TODO(b/384938613): Failing under shardy.\")\n     mesh = jtu.create_mesh((4,), 'i')\n \n     def empty_callback(x):\ndiff --git a/tests/python_callback_test.py b/tests/python_callback_test.py\nindex 9f7336548d12..9a3b26530044 100644\n--- a/tests/python_callback_test.py\n+++ b/tests/python_callback_test.py\n@@ -1364,8 +1364,6 @@ def f_base(i, x):\n     self.assertEqual(_collected, expected)\n \n   def test_can_shard_io_callback_manually(self):\n-    if config.use_shardy_partitioner.value:\n-      self.skipTest(\"TODO(b/384938613): Failing under shardy.\")\n \n     mesh = Mesh(np.array(jax.devices()), axis_names=('x',))\n \ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 00d437aadb08..2fdc846a356b 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -869,8 +869,6 @@ def test_shmap_abstract_mesh_errors(self):\n   @jtu.run_on_devices('cpu', 'gpu', 'tpu')\n   @jtu.thread_unsafe_test()\n   def test_debug_print_jit(self, jit):\n-    if config.use_shardy_partitioner.value:\n-      self.skipTest('TODO(b/384938613): Failing under shardy')\n     mesh = Mesh(jax.devices(), ('i',))\n \n     @partial(shard_map, mesh=mesh, in_specs=P('i'), out_specs=P('i'))\n@@ -892,6 +890,23 @@ def f(x):\n     for i in range(len(jax.devices())):\n       self.assertIn(f'instance {i} has value', output())\n \n+  @jtu.run_on_devices('cpu', 'gpu', 'tpu')\n+  @jtu.thread_unsafe_test()\n+  def test_debug_print_jit_partial_auto(self):\n+    mesh = jtu.create_mesh((2,2), ('x', 'y'))\n+\n+    @partial(shard_map, mesh=mesh, in_specs=P('x'), out_specs=P('x'),\n+             axis_names=frozenset({'x'}))\n+    def f(x):\n+      idx = jax.lax.axis_index('x')\n+      jax.debug.print(\"instance {i} has value x={x}\", i=idx, x=x)\n+      y = jnp.cos(x)\n+      return y\n+\n+    f = jax.jit(f)\n+    x = jnp.arange(2 * len(jax.devices()))\n+    f(x)  # don't crash!\n+\n   def test_debug_print_eager(self):\n     mesh = Mesh(jax.devices(), ('i',))\n \n\n```"
        ],
        "from_id": [
            "bartchr808",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas] Do not emit verbose lowering errors by default\n\nThe errors are too verbose and mostly not very useful.\n\nPiperOrigin-RevId: 759025165",
        "output": [
            "```diff\nCommit: 7cbdc3c2defe2bb1fdec55b7157a0d2b43fcd27b\nDate: 2025-05-15T07:55:23Z\nURL: https://github.com/jax-ml/jax/commit/7cbdc3c2defe2bb1fdec55b7157a0d2b43fcd27b\nFiles changed: 2\nAdditions: +1, Deletions: -5\ndiff --git a/jax/_src/pallas/pallas_call.py b/jax/_src/pallas/pallas_call.py\nindex def8efd472c6..2d27bd3cc485 100644\n--- a/jax/_src/pallas/pallas_call.py\n+++ b/jax/_src/pallas/pallas_call.py\n@@ -1235,7 +1235,7 @@ def _trace_kernel_to_jaxpr(\n \n _PALLAS_VERBOSE_ERRORS = config.bool_flag(\n     \"jax_pallas_verbose_errors\",\n-    default=config.bool_env(\"JAX_PALLAS_VERBOSE_ERRORS\", True),\n+    default=config.bool_env(\"JAX_PALLAS_VERBOSE_ERRORS\", False),\n     help=(\n         \"If True, print verbose error messages for Pallas kernels.\"\n     ),\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 21ab7ea1a482..109af4213b81 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -162,7 +162,6 @@ jax_multiplatform_test(\n     ],\n     env = {\n         \"JAX_PALLAS_USE_MOSAIC_GPU\": \"1\",\n-        \"JAX_PALLAS_VERBOSE_ERRORS\": \"0\",\n     },\n     shard_count = 16,\n     tags = [\n@@ -239,9 +238,6 @@ jax_multiplatform_test(\n         \"gpu_h100_x32\",\n         \"gpu_h100\",\n     ],\n-    env = {\n-        \"JAX_PALLAS_VERBOSE_ERRORS\": \"0\",\n-    },\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_mosaic_gpu\",  # build_cleaner: keep\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix CI build failure on Mac.\n\nWe must not depend on the nvidia_nvshmem_cu12 pip package directly since it does not exist on Windows and Mac platforms.\n\nPiperOrigin-RevId: 758917499",
        "output": [
            "```diff\nCommit: ec72f173cf98d95d1537b1f3b9f6720e2a032203\nDate: 2025-05-15T01:37:44Z\nURL: https://github.com/jax-ml/jax/commit/ec72f173cf98d95d1537b1f3b9f6720e2a032203\nFiles changed: 2\nAdditions: +3, Deletions: -2\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 3f73a4b9e68f..4018bff873bd 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -887,7 +887,6 @@ pytype_strict_library(\n py_library_providing_imports_info(\n     name = \"mosaic_gpu\",\n     srcs = glob([\"experimental/mosaic/gpu/*.py\"]),\n-    data = py_deps(\"libnvshmem_device\"),\n     visibility = [\n         \":mosaic_gpu_users\",\n     ],\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex 1d4e24720c2e..a8dc67eb3804 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -100,7 +100,6 @@ _py_deps = {\n     \"tensorstore\": get_optional_dep(\"@pypi//tensorstore\"),\n     \"torch\": [],\n     \"zstandard\": get_zstandard(),\n-    \"libnvshmem_device\": [\"@pypi//nvidia_nvshmem_cu12\"],\n }\n \n def all_py_deps(excluded = []):\n@@ -188,14 +187,17 @@ def _gpu_test_deps():\n             \"//jaxlib/cuda:gpu_only_test_deps\",\n             \"//jaxlib/rocm:gpu_only_test_deps\",\n             \"//jax_plugins:gpu_plugin_only_test_deps\",\n+            \"@pypi//nvidia_nvshmem_cu12\",\n         ],\n         \"//jax:config_build_jaxlib_false\": [\n             \"@pypi//jax_cuda12_plugin\",\n             \"@pypi//jax_cuda12_pjrt\",\n+            \"@pypi//nvidia_nvshmem_cu12\",\n         ],\n         \"//jax:config_build_jaxlib_wheel\": [\n             \"//jaxlib/tools:jax_cuda_plugin_py_import\",\n             \"//jaxlib/tools:jax_cuda_pjrt_py_import\",\n+            \"@pypi//nvidia_nvshmem_cu12\",\n         ],\n     })\n \n\n```"
        ],
        "from_id": [
            "hawkinsp",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28755 from hawkinsp:plugins\n\nPiperOrigin-RevId: 758915292",
        "output": [
            "```diff\nCommit: 0f91c40a54069784fc9e1742f72d4a0dbeb6fcf5\nDate: 2025-05-15T01:30:25Z\nURL: https://github.com/jax-ml/jax/commit/0f91c40a54069784fc9e1742f72d4a0dbeb6fcf5\nFiles changed: 3\nAdditions: +150, Deletions: -136\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex a0c30132c169..9fd4e50304d0 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -21,6 +21,8 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n     given its name.\n \n * Changes\n+  * Additional checking for the versions of CUDA package dependencies was\n+    reenabled, having been accidentally disabled in a previous release.\n   * JAX nightly packages are now published to artifact registry. To install\n     these packages, see the [JAX installation guide](https://docs.jax.dev/en/latest/installation.html#jax-nightly-installation).\n   * `jax.sharding.PartitionSpec` no longer inherits from a tuple.\ndiff --git a/jax/_src/xla_bridge.py b/jax/_src/xla_bridge.py\nindex 72a16d5fbe5c..ce0c36fdcca4 100644\n--- a/jax/_src/xla_bridge.py\n+++ b/jax/_src/xla_bridge.py\n@@ -31,7 +31,6 @@\n import pkgutil\n import platform as py_platform\n import threading\n-import traceback\n from typing import Any, Sequence, Union\n import warnings\n \n@@ -311,141 +310,6 @@ def _check_cuda_compute_capability(devices_to_check):\n       )\n \n \n-def _check_cuda_versions(raise_on_first_error: bool = False,\n-                         debug: bool = False):\n-  assert cuda_versions is not None\n-  results: list[dict[str, Any]] = []\n-\n-  def _make_msg(name: str,\n-                runtime_version: int,\n-                build_version: int,\n-                min_supported: int,\n-                debug_msg: bool = False):\n-    if debug_msg:\n-      return (f\"Package: {name}\\n\"\n-              f\"Version JAX was built against: {build_version}\\n\"\n-              f\"Minimum supported: {min_supported}\\n\"\n-              f\"Installed version: {runtime_version}\")\n-    if min_supported:\n-      req_str = (f\"The local installation version must be no lower than \"\n-                 f\"{min_supported}.\")\n-    else:\n-      req_str = (\"The local installation must be the same version as \"\n-                 \"the version against which JAX was built.\")\n-    msg = (f\"Outdated {name} installation found.\\n\"\n-           f\"Version JAX was built against: {build_version}\\n\"\n-           f\"Minimum supported: {min_supported}\\n\"\n-           f\"Installed version: {runtime_version}\\n\"\n-           f\"{req_str}\")\n-    return msg\n-\n-  def _version_check(name: str,\n-                     get_version,\n-                     get_build_version,\n-                     scale_for_comparison: int = 1,\n-                     min_supported_version: int = 0):\n-    \"\"\"Checks the runtime CUDA component version against the JAX one.\n-\n-    Args:\n-      name: Of the CUDA component.\n-      get_version: A function to get the local runtime version of the component.\n-      get_build_version: A function to get the build version of the component.\n-      scale_for_comparison: For rounding down a version to ignore patch/minor.\n-      min_supported_version: An absolute minimum version required. Must be\n-        passed without rounding down.\n-\n-    Raises:\n-      RuntimeError: If the component is not found, or is of unsupported version,\n-        and if raising the error is not deferred till later.\n-    \"\"\"\n-\n-    build_version = get_build_version()\n-    try:\n-      version = get_version()\n-    except Exception as e:\n-      err_msg = f\"Unable to load {name}. Is it installed?\"\n-      if raise_on_first_error:\n-        raise RuntimeError(err_msg) from e\n-      err_msg += f\"\\n{traceback.format_exc()}\"\n-      results.append({\"name\": name, \"installed\": False, \"msg\": err_msg})\n-      return\n-\n-    if not min_supported_version:\n-      min_supported_version = build_version // scale_for_comparison\n-    passed = min_supported_version <= version\n-\n-    if not passed or debug:\n-      msg = _make_msg(name=name,\n-                      runtime_version=version,\n-                      build_version=build_version,\n-                      min_supported=min_supported_version,\n-                      debug_msg=passed)\n-      if not passed and raise_on_first_error:\n-        raise RuntimeError(msg)\n-      else:\n-        record = {\"name\": name,\n-                  \"installed\": True,\n-                  \"msg\": msg,\n-                  \"passed\": passed,\n-                  \"build_version\": build_version,\n-                  \"version\": version,\n-                  \"minimum_supported\": min_supported_version}\n-        results.append(record)\n-\n-  _version_check(\"CUDA\", cuda_versions.cuda_runtime_get_version,\n-                 cuda_versions.cuda_runtime_build_version,\n-                 scale_for_comparison=10,\n-                 min_supported_version=12010)\n-  _version_check(\n-      \"cuDNN\",\n-      cuda_versions.cudnn_get_version,\n-      cuda_versions.cudnn_build_version,\n-      # NVIDIA promise both backwards and forwards compatibility for cuDNN patch\n-      # versions:\n-      # https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#api-compat\n-      scale_for_comparison=100,\n-      min_supported_version=9100\n-  )\n-  _version_check(\"cuFFT\", cuda_versions.cufft_get_version,\n-                 cuda_versions.cufft_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100)\n-  _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n-                 cuda_versions.cusolver_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=11400)\n-  _version_check(\"cuPTI\", cuda_versions.cupti_get_version,\n-                 cuda_versions.cupti_build_version,\n-                 min_supported_version=18)\n-  _version_check(\"cuBLAS\", cuda_versions.cublas_get_version,\n-                 cuda_versions.cublas_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=120100)\n-  _version_check(\"cuSPARSE\", cuda_versions.cusparse_get_version,\n-                 cuda_versions.cusparse_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=12100)\n-\n-  errors = []\n-  debug_results = []\n-  for result in results:\n-    message: str = result['msg']\n-    if not result['installed'] or not result['passed']:\n-      errors.append(message)\n-    else:\n-      debug_results.append(message)\n-\n-  join_str = f'\\n{\"-\" * 50}\\n'\n-  if debug_results:\n-    print(f'CUDA components status (debug):\\n'\n-          f'{join_str.join(debug_results)}')\n-  if errors:\n-    raise RuntimeError(f'Unable to use CUDA because of the '\n-                       f'following issues with CUDA components:\\n'\n-                       f'{join_str.join(errors)}')\n \n def get_num_nodes_from_gpu_topology(topology: str) -> int:\n     try:\ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 1be29326c95f..9df7fc69ff1a 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -17,6 +17,8 @@\n import logging\n import os\n import pathlib\n+import traceback\n+from typing import Any\n \n from jax._src.lib import triton\n from jax._src.lib import xla_client\n@@ -29,8 +31,12 @@\n     cuda_plugin_extension = importlib.import_module(\n         f'{pkg_name}.cuda_plugin_extension'\n     )\n+    cuda_versions = importlib.import_module(\n+        f'{pkg_name}._versions'\n+    )\n   except ImportError:\n     cuda_plugin_extension = None\n+    cuda_versions = None\n   else:\n     break\n \n@@ -76,11 +82,153 @@ def _get_library_path():\n   return None\n \n \n+def _check_cuda_versions(raise_on_first_error: bool = False,\n+                         debug: bool = False):\n+  assert cuda_versions is not None\n+  results: list[dict[str, Any]] = []\n+\n+  def _make_msg(name: str,\n+                runtime_version: int,\n+                build_version: int,\n+                min_supported: int,\n+                debug_msg: bool = False):\n+    if debug_msg:\n+      return (f\"Package: {name}\\n\"\n+              f\"Version JAX was built against: {build_version}\\n\"\n+              f\"Minimum supported: {min_supported}\\n\"\n+              f\"Installed version: {runtime_version}\")\n+    if min_supported:\n+      req_str = (f\"The local installation version must be no lower than \"\n+                 f\"{min_supported}.\")\n+    else:\n+      req_str = (\"The local installation must be the same version as \"\n+                 \"the version against which JAX was built.\")\n+    msg = (f\"Outdated {name} installation found.\\n\"\n+           f\"Version JAX was built against: {build_version}\\n\"\n+           f\"Minimum supported: {min_supported}\\n\"\n+           f\"Installed version: {runtime_version}\\n\"\n+           f\"{req_str}\")\n+    return msg\n+\n+  def _version_check(name: str,\n+                     get_version,\n+                     get_build_version,\n+                     scale_for_comparison: int = 1,\n+                     min_supported_version: int = 0):\n+    \"\"\"Checks the runtime CUDA component version against the JAX one.\n+\n+    Args:\n+      name: Of the CUDA component.\n+      get_version: A function to get the local runtime version of the component.\n+      get_build_version: A function to get the build version of the component.\n+      scale_for_comparison: For rounding down a version to ignore patch/minor.\n+      min_supported_version: An absolute minimum version required. Must be\n+        passed without rounding down.\n+\n+    Raises:\n+      RuntimeError: If the component is not found, or is of unsupported version,\n+        and if raising the error is not deferred till later.\n+    \"\"\"\n+\n+    build_version = get_build_version()\n+    try:\n+      version = get_version()\n+    except Exception as e:\n+      err_msg = f\"Unable to load {name}. Is it installed?\"\n+      if raise_on_first_error:\n+        raise RuntimeError(err_msg) from e\n+      err_msg += f\"\\n{traceback.format_exc()}\"\n+      results.append({\"name\": name, \"installed\": False, \"msg\": err_msg})\n+      return\n+\n+    if not min_supported_version:\n+      min_supported_version = build_version // scale_for_comparison\n+    passed = min_supported_version <= version\n+\n+    if not passed or debug:\n+      msg = _make_msg(name=name,\n+                      runtime_version=version,\n+                      build_version=build_version,\n+                      min_supported=min_supported_version,\n+                      debug_msg=passed)\n+      if not passed and raise_on_first_error:\n+        raise RuntimeError(msg)\n+      else:\n+        record = {\"name\": name,\n+                  \"installed\": True,\n+                  \"msg\": msg,\n+                  \"passed\": passed,\n+                  \"build_version\": build_version,\n+                  \"version\": version,\n+                  \"minimum_supported\": min_supported_version}\n+        results.append(record)\n+\n+  _version_check(\"CUDA\", cuda_versions.cuda_runtime_get_version,\n+                 cuda_versions.cuda_runtime_build_version,\n+                 scale_for_comparison=10,\n+                 min_supported_version=12010)\n+  _version_check(\n+      \"cuDNN\",\n+      cuda_versions.cudnn_get_version,\n+      cuda_versions.cudnn_build_version,\n+      # NVIDIA promise both backwards and forwards compatibility for cuDNN patch\n+      # versions:\n+      # https://docs.nvidia.com/deeplearning/cudnn/backend/latest/developer/forward-compatibility.html#cudnn-api-compatibility\n+      scale_for_comparison=100,\n+  )\n+  _version_check(\"cuFFT\", cuda_versions.cufft_get_version,\n+                 cuda_versions.cufft_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100)\n+  _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n+                 cuda_versions.cusolver_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100,\n+                 min_supported_version=11400)\n+  _version_check(\"cuPTI\", cuda_versions.cupti_get_version,\n+                 cuda_versions.cupti_build_version,\n+                 min_supported_version=18)\n+  _version_check(\"cuBLAS\", cuda_versions.cublas_get_version,\n+                 cuda_versions.cublas_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100,\n+                 min_supported_version=120100)\n+  _version_check(\"cuSPARSE\", cuda_versions.cusparse_get_version,\n+                 cuda_versions.cusparse_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100,\n+                 min_supported_version=12100)\n+\n+  errors = []\n+  debug_results = []\n+  for result in results:\n+    message: str = result['msg']\n+    if not result['installed'] or not result['passed']:\n+      errors.append(message)\n+    else:\n+      debug_results.append(message)\n+\n+  join_str = f'\\n{\"-\" * 50}\\n'\n+  if debug_results:\n+    print(f'CUDA components status (debug):\\n'\n+          f'{join_str.join(debug_results)}')\n+  if errors:\n+    raise RuntimeError(f'Unable to use CUDA because of the '\n+                       f'following issues with CUDA components:\\n'\n+                       f'{join_str.join(errors)}')\n+\n+\n def initialize():\n   path = _get_library_path()\n   if path is None:\n     return\n \n+  if not os.getenv(\"JAX_SKIP_CUDA_CONSTRAINTS_CHECK\"):\n+    _check_cuda_versions(raise_on_first_error=True)\n+  else:\n+    print('Skipped CUDA versions constraints check due to the '\n+          'JAX_SKIP_CUDA_CONSTRAINTS_CHECK env var being set.')\n+\n   options = xla_client.generate_pjrt_gpu_plugin_options()\n   c_api = xb.register_plugin(\n       'cuda', priority=500, library_path=str(path), options=options\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[JAX] Fix unhashable slice in api_test\n\n`slice` is not hashable before Python 3.12. This change mitigates it by\nconverting it into a hash value.\n\nPiperOrigin-RevId: 758905560",
        "output": [
            "```diff\nCommit: 9a1535e210dcb29f2b0f7494a773ed9aee5b9049\nDate: 2025-05-15T00:58:02Z\nURL: https://github.com/jax-ml/jax/commit/9a1535e210dcb29f2b0f7494a773ed9aee5b9049\nFiles changed: 1\nAdditions: +5, Deletions: -2\ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 1fd192a52525..3fe3d6fa7514 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -54,6 +54,7 @@\n from jax._src import xla_bridge\n from jax._src import debugging\n from jax._src import pjit as pjit_lib\n+from jax._src import sharding_impls\n from jax._src.ad_checkpoint import saved_residuals\n from jax._src.interpreters import ad as ad_internal\n from jax._src.interpreters import mlir\n@@ -2047,10 +2048,12 @@ def test_internal_device_put_assembled(self):\n     per_device_arrs = {\n         # Use uncommitted arrays that are not aligned with the destination\n         # sharding so that we trigger `BatchedDevicePut`.\n-        index: jnp.array(arr[index])\n+        sharding_impls.hashed_index(index): jnp.array(arr[index])\n         for _, index in sharding.devices_indices_map(arr.shape).items()\n     }\n-    data_callback = lambda index: per_device_arrs[index]\n+    data_callback = lambda index: per_device_arrs[\n+        sharding_impls.hashed_index(index)\n+    ]\n     with jtu.count_internal_device_puts() as counts:\n       jax.make_array_from_callback(arr.shape, sharding, data_callback)\n     self.assertEqual(\n\n```"
        ],
        "from_id": [
            "hyeontaek",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add numpy and absl/testing dep to custom_api_test. Fixes https://github.com/jax-ml/jax/actions/runs/15031061909/job/42243435305\n\nPiperOrigin-RevId: 758898284",
        "output": [
            "```diff\nCommit: bf4fda96a936aab5adc6430763419c3bdb3d9495\nDate: 2025-05-15T00:32:41Z\nURL: https://github.com/jax-ml/jax/commit/bf4fda96a936aab5adc6430763419c3bdb3d9495\nFiles changed: 2\nAdditions: +9, Deletions: -2\ndiff --git a/tests/BUILD b/tests/BUILD\nindex e70d4593e8fc..1d1f3c28b239 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -50,7 +50,10 @@ jax_multiplatform_test(\n     shard_count = 10,\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 49a05ee487f0..21ab7ea1a482 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -427,7 +427,11 @@ jax_multiplatform_test(\n         \"//jax:extend\",\n         \"//jax:pallas_mosaic_gpu\",\n         \"//jax:test_multiprocess\",\n-    ] + py_deps(\"portpicker\"),\n+    ] + py_deps([\n+        \"portpicker\",\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reenable CUDA version checks from Python.\n\nThese had been accidentally broken at some point in the plugin\nswitchover..",
        "output": [
            "```diff\nCommit: 011639cf3621c52c00ffc1a24abf7f4dacf19966\nDate: 2025-05-14T21:35:56Z\nURL: https://github.com/jax-ml/jax/commit/011639cf3621c52c00ffc1a24abf7f4dacf19966\nFiles changed: 3\nAdditions: +150, Deletions: -136\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex a0c30132c169..9fd4e50304d0 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -21,6 +21,8 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n     given its name.\n \n * Changes\n+  * Additional checking for the versions of CUDA package dependencies was\n+    reenabled, having been accidentally disabled in a previous release.\n   * JAX nightly packages are now published to artifact registry. To install\n     these packages, see the [JAX installation guide](https://docs.jax.dev/en/latest/installation.html#jax-nightly-installation).\n   * `jax.sharding.PartitionSpec` no longer inherits from a tuple.\ndiff --git a/jax/_src/xla_bridge.py b/jax/_src/xla_bridge.py\nindex 72a16d5fbe5c..ce0c36fdcca4 100644\n--- a/jax/_src/xla_bridge.py\n+++ b/jax/_src/xla_bridge.py\n@@ -31,7 +31,6 @@\n import pkgutil\n import platform as py_platform\n import threading\n-import traceback\n from typing import Any, Sequence, Union\n import warnings\n \n@@ -311,141 +310,6 @@ def _check_cuda_compute_capability(devices_to_check):\n       )\n \n \n-def _check_cuda_versions(raise_on_first_error: bool = False,\n-                         debug: bool = False):\n-  assert cuda_versions is not None\n-  results: list[dict[str, Any]] = []\n-\n-  def _make_msg(name: str,\n-                runtime_version: int,\n-                build_version: int,\n-                min_supported: int,\n-                debug_msg: bool = False):\n-    if debug_msg:\n-      return (f\"Package: {name}\\n\"\n-              f\"Version JAX was built against: {build_version}\\n\"\n-              f\"Minimum supported: {min_supported}\\n\"\n-              f\"Installed version: {runtime_version}\")\n-    if min_supported:\n-      req_str = (f\"The local installation version must be no lower than \"\n-                 f\"{min_supported}.\")\n-    else:\n-      req_str = (\"The local installation must be the same version as \"\n-                 \"the version against which JAX was built.\")\n-    msg = (f\"Outdated {name} installation found.\\n\"\n-           f\"Version JAX was built against: {build_version}\\n\"\n-           f\"Minimum supported: {min_supported}\\n\"\n-           f\"Installed version: {runtime_version}\\n\"\n-           f\"{req_str}\")\n-    return msg\n-\n-  def _version_check(name: str,\n-                     get_version,\n-                     get_build_version,\n-                     scale_for_comparison: int = 1,\n-                     min_supported_version: int = 0):\n-    \"\"\"Checks the runtime CUDA component version against the JAX one.\n-\n-    Args:\n-      name: Of the CUDA component.\n-      get_version: A function to get the local runtime version of the component.\n-      get_build_version: A function to get the build version of the component.\n-      scale_for_comparison: For rounding down a version to ignore patch/minor.\n-      min_supported_version: An absolute minimum version required. Must be\n-        passed without rounding down.\n-\n-    Raises:\n-      RuntimeError: If the component is not found, or is of unsupported version,\n-        and if raising the error is not deferred till later.\n-    \"\"\"\n-\n-    build_version = get_build_version()\n-    try:\n-      version = get_version()\n-    except Exception as e:\n-      err_msg = f\"Unable to load {name}. Is it installed?\"\n-      if raise_on_first_error:\n-        raise RuntimeError(err_msg) from e\n-      err_msg += f\"\\n{traceback.format_exc()}\"\n-      results.append({\"name\": name, \"installed\": False, \"msg\": err_msg})\n-      return\n-\n-    if not min_supported_version:\n-      min_supported_version = build_version // scale_for_comparison\n-    passed = min_supported_version <= version\n-\n-    if not passed or debug:\n-      msg = _make_msg(name=name,\n-                      runtime_version=version,\n-                      build_version=build_version,\n-                      min_supported=min_supported_version,\n-                      debug_msg=passed)\n-      if not passed and raise_on_first_error:\n-        raise RuntimeError(msg)\n-      else:\n-        record = {\"name\": name,\n-                  \"installed\": True,\n-                  \"msg\": msg,\n-                  \"passed\": passed,\n-                  \"build_version\": build_version,\n-                  \"version\": version,\n-                  \"minimum_supported\": min_supported_version}\n-        results.append(record)\n-\n-  _version_check(\"CUDA\", cuda_versions.cuda_runtime_get_version,\n-                 cuda_versions.cuda_runtime_build_version,\n-                 scale_for_comparison=10,\n-                 min_supported_version=12010)\n-  _version_check(\n-      \"cuDNN\",\n-      cuda_versions.cudnn_get_version,\n-      cuda_versions.cudnn_build_version,\n-      # NVIDIA promise both backwards and forwards compatibility for cuDNN patch\n-      # versions:\n-      # https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#api-compat\n-      scale_for_comparison=100,\n-      min_supported_version=9100\n-  )\n-  _version_check(\"cuFFT\", cuda_versions.cufft_get_version,\n-                 cuda_versions.cufft_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100)\n-  _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n-                 cuda_versions.cusolver_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=11400)\n-  _version_check(\"cuPTI\", cuda_versions.cupti_get_version,\n-                 cuda_versions.cupti_build_version,\n-                 min_supported_version=18)\n-  _version_check(\"cuBLAS\", cuda_versions.cublas_get_version,\n-                 cuda_versions.cublas_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=120100)\n-  _version_check(\"cuSPARSE\", cuda_versions.cusparse_get_version,\n-                 cuda_versions.cusparse_build_version,\n-                 # Ignore patch versions.\n-                 scale_for_comparison=100,\n-                 min_supported_version=12100)\n-\n-  errors = []\n-  debug_results = []\n-  for result in results:\n-    message: str = result['msg']\n-    if not result['installed'] or not result['passed']:\n-      errors.append(message)\n-    else:\n-      debug_results.append(message)\n-\n-  join_str = f'\\n{\"-\" * 50}\\n'\n-  if debug_results:\n-    print(f'CUDA components status (debug):\\n'\n-          f'{join_str.join(debug_results)}')\n-  if errors:\n-    raise RuntimeError(f'Unable to use CUDA because of the '\n-                       f'following issues with CUDA components:\\n'\n-                       f'{join_str.join(errors)}')\n \n def get_num_nodes_from_gpu_topology(topology: str) -> int:\n     try:\ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 1be29326c95f..9df7fc69ff1a 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -17,6 +17,8 @@\n import logging\n import os\n import pathlib\n+import traceback\n+from typing import Any\n \n from jax._src.lib import triton\n from jax._src.lib import xla_client\n@@ -29,8 +31,12 @@\n     cuda_plugin_extension = importlib.import_module(\n         f'{pkg_name}.cuda_plugin_extension'\n     )\n+    cuda_versions = importlib.import_module(\n+        f'{pkg_name}._versions'\n+    )\n   except ImportError:\n     cuda_plugin_extension = None\n+    cuda_versions = None\n   else:\n     break\n \n@@ -76,11 +82,153 @@ def _get_library_path():\n   return None\n \n \n+def _check_cuda_versions(raise_on_first_error: bool = False,\n+                         debug: bool = False):\n+  assert cuda_versions is not None\n+  results: list[dict[str, Any]] = []\n+\n+  def _make_msg(name: str,\n+                runtime_version: int,\n+                build_version: int,\n+                min_supported: int,\n+                debug_msg: bool = False):\n+    if debug_msg:\n+      return (f\"Package: {name}\\n\"\n+              f\"Version JAX was built against: {build_version}\\n\"\n+              f\"Minimum supported: {min_supported}\\n\"\n+              f\"Installed version: {runtime_version}\")\n+    if min_supported:\n+      req_str = (f\"The local installation version must be no lower than \"\n+                 f\"{min_supported}.\")\n+    else:\n+      req_str = (\"The local installation must be the same version as \"\n+                 \"the version against which JAX was built.\")\n+    msg = (f\"Outdated {name} installation found.\\n\"\n+           f\"Version JAX was built against: {build_version}\\n\"\n+           f\"Minimum supported: {min_supported}\\n\"\n+           f\"Installed version: {runtime_version}\\n\"\n+           f\"{req_str}\")\n+    return msg\n+\n+  def _version_check(name: str,\n+                     get_version,\n+                     get_build_version,\n+                     scale_for_comparison: int = 1,\n+                     min_supported_version: int = 0):\n+    \"\"\"Checks the runtime CUDA component version against the JAX one.\n+\n+    Args:\n+      name: Of the CUDA component.\n+      get_version: A function to get the local runtime version of the component.\n+      get_build_version: A function to get the build version of the component.\n+      scale_for_comparison: For rounding down a version to ignore patch/minor.\n+      min_supported_version: An absolute minimum version required. Must be\n+        passed without rounding down.\n+\n+    Raises:\n+      RuntimeError: If the component is not found, or is of unsupported version,\n+        and if raising the error is not deferred till later.\n+    \"\"\"\n+\n+    build_version = get_build_version()\n+    try:\n+      version = get_version()\n+    except Exception as e:\n+      err_msg = f\"Unable to load {name}. Is it installed?\"\n+      if raise_on_first_error:\n+        raise RuntimeError(err_msg) from e\n+      err_msg += f\"\\n{traceback.format_exc()}\"\n+      results.append({\"name\": name, \"installed\": False, \"msg\": err_msg})\n+      return\n+\n+    if not min_supported_version:\n+      min_supported_version = build_version // scale_for_comparison\n+    passed = min_supported_version <= version\n+\n+    if not passed or debug:\n+      msg = _make_msg(name=name,\n+                      runtime_version=version,\n+                      build_version=build_version,\n+                      min_supported=min_supported_version,\n+                      debug_msg=passed)\n+      if not passed and raise_on_first_error:\n+        raise RuntimeError(msg)\n+      else:\n+        record = {\"name\": name,\n+                  \"installed\": True,\n+                  \"msg\": msg,\n+                  \"passed\": passed,\n+                  \"build_version\": build_version,\n+                  \"version\": version,\n+                  \"minimum_supported\": min_supported_version}\n+        results.append(record)\n+\n+  _version_check(\"CUDA\", cuda_versions.cuda_runtime_get_version,\n+                 cuda_versions.cuda_runtime_build_version,\n+                 scale_for_comparison=10,\n+                 min_supported_version=12010)\n+  _version_check(\n+      \"cuDNN\",\n+      cuda_versions.cudnn_get_version,\n+      cuda_versions.cudnn_build_version,\n+      # NVIDIA promise both backwards and forwards compatibility for cuDNN patch\n+      # versions:\n+      # https://docs.nvidia.com/deeplearning/cudnn/backend/latest/developer/forward-compatibility.html#cudnn-api-compatibility\n+      scale_for_comparison=100,\n+  )\n+  _version_check(\"cuFFT\", cuda_versions.cufft_get_version,\n+                 cuda_versions.cufft_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100)\n+  _version_check(\"cuSOLVER\", cuda_versions.cusolver_get_version,\n+                 cuda_versions.cusolver_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100,\n+                 min_supported_version=11400)\n+  _version_check(\"cuPTI\", cuda_versions.cupti_get_version,\n+                 cuda_versions.cupti_build_version,\n+                 min_supported_version=18)\n+  _version_check(\"cuBLAS\", cuda_versions.cublas_get_version,\n+                 cuda_versions.cublas_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100,\n+                 min_supported_version=120100)\n+  _version_check(\"cuSPARSE\", cuda_versions.cusparse_get_version,\n+                 cuda_versions.cusparse_build_version,\n+                 # Ignore patch versions.\n+                 scale_for_comparison=100,\n+                 min_supported_version=12100)\n+\n+  errors = []\n+  debug_results = []\n+  for result in results:\n+    message: str = result['msg']\n+    if not result['installed'] or not result['passed']:\n+      errors.append(message)\n+    else:\n+      debug_results.append(message)\n+\n+  join_str = f'\\n{\"-\" * 50}\\n'\n+  if debug_results:\n+    print(f'CUDA components status (debug):\\n'\n+          f'{join_str.join(debug_results)}')\n+  if errors:\n+    raise RuntimeError(f'Unable to use CUDA because of the '\n+                       f'following issues with CUDA components:\\n'\n+                       f'{join_str.join(errors)}')\n+\n+\n def initialize():\n   path = _get_library_path()\n   if path is None:\n     return\n \n+  if not os.getenv(\"JAX_SKIP_CUDA_CONSTRAINTS_CHECK\"):\n+    _check_cuda_versions(raise_on_first_error=True)\n+  else:\n+    print('Skipped CUDA versions constraints check due to the '\n+          'JAX_SKIP_CUDA_CONSTRAINTS_CHECK env var being set.')\n+\n   options = xla_client.generate_pjrt_gpu_plugin_options()\n   c_api = xb.register_plugin(\n       'cuda', priority=500, library_path=str(path), options=options\n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Merge pull request #28753 from hawkinsp:plugins\n\nPiperOrigin-RevId: 758833461",
        "output": [
            "```diff\nCommit: 7e8fa0d888319edb76194ebf393d014bc0d88330\nDate: 2025-05-14T21:24:35Z\nURL: https://github.com/jax-ml/jax/commit/7e8fa0d888319edb76194ebf393d014bc0d88330\nFiles changed: 4\nAdditions: +48, Deletions: -113\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 6afcdf2892f4..3f73a4b9e68f 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -101,17 +101,22 @@ string_flag(\n )\n \n config_setting(\n-    name = \"disable_jaxlib_and_jax_build\",\n+    name = \"config_build_jax_true\",\n+    flag_values = {\n+        \":build_jax\": \"true\",\n+    },\n+)\n+\n+config_setting(\n+    name = \"config_build_jax_false\",\n     flag_values = {\n-        \":build_jaxlib\": \"false\",\n         \":build_jax\": \"false\",\n     },\n )\n \n config_setting(\n-    name = \"enable_jaxlib_and_jax_py_import\",\n+    name = \"config_build_jax_wheel\",\n     flag_values = {\n-        \":build_jaxlib\": \"wheel\",\n         \":build_jax\": \"wheel\",\n     },\n )\ndiff --git a/jax_plugins/cuda/BUILD.bazel b/jax_plugins/cuda/BUILD.bazel\nindex c3c20f536cff..7070bf6bc495 100644\n--- a/jax_plugins/cuda/BUILD.bazel\n+++ b/jax_plugins/cuda/BUILD.bazel\n@@ -14,7 +14,6 @@\n \n load(\n     \"//jaxlib:jax.bzl\",\n-    \"if_windows\",\n     \"py_library_providing_imports_info\",\n     \"pytype_library\",\n )\n@@ -58,35 +57,3 @@ py_library_providing_imports_info(\n     data = [\":pjrt_c_api_gpu_plugin.so\"],\n     lib_rule = pytype_library,\n )\n-\n-config_setting(\n-    name = \"disable_jaxlib_for_cpu_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"false\",\n-        \"@local_config_cuda//:enable_cuda\": \"False\",\n-    },\n-)\n-\n-config_setting(\n-    name = \"disable_jaxlib_for_cuda12_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"false\",\n-        \"@local_config_cuda//:enable_cuda\": \"True\",\n-    },\n-)\n-\n-config_setting(\n-    name = \"enable_py_import_for_cpu_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"wheel\",\n-        \"@local_config_cuda//:enable_cuda\": \"False\",\n-    },\n-)\n-\n-config_setting(\n-    name = \"enable_py_import_for_cuda12_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"wheel\",\n-        \"@local_config_cuda//:enable_cuda\": \"True\",\n-    },\n-)\ndiff --git a/jax_plugins/rocm/BUILD.bazel b/jax_plugins/rocm/BUILD.bazel\nindex 15e9e627830e..7ee0726e7960 100644\n--- a/jax_plugins/rocm/BUILD.bazel\n+++ b/jax_plugins/rocm/BUILD.bazel\n@@ -16,7 +16,6 @@ licenses([\"notice\"])\n \n load(\n   \"//jaxlib:jax.bzl\",\n-  \"if_windows\",\n   \"py_library_providing_imports_info\",\n   \"pytype_library\",\n )\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex a48c44f406f2..1d4e24720c2e 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -167,70 +167,36 @@ def if_building_jaxlib(\n       if_building: the source code targets to depend on in case we don't depend on the jaxlib wheels\n       if_not_building: the wheels to depend on if we are not depending directly on //jaxlib.\n     \"\"\"\n-\n     return select({\n         \"//jax:config_build_jaxlib_true\": if_building,\n         \"//jax:config_build_jaxlib_false\": if_not_building,\n         \"//jax:config_build_jaxlib_wheel\": [],\n     })\n \n-def _get_test_deps(deps, backend_independent):\n-    \"\"\"Returns the test deps for the given backend.\n-\n-    Args:\n-      deps: the full list of test dependencies\n-      backend_independent: whether the test is backend independent\n+def _cpu_test_deps():\n+    \"\"\"Returns the test depencies needed for a CPU-only JAX test.\"\"\"\n+    return select({\n+        \"//jax:config_build_jaxlib_true\": [],\n+        \"//jax:config_build_jaxlib_false\": [\"@pypi//jaxlib\"],\n+        \"//jax:config_build_jaxlib_wheel\": [\"//jaxlib/tools:jaxlib_py_import\"],\n+    })\n \n-    Returns:\n-      A list of test deps for the given backend.\n-        For CPU builds:\n-          If --//jax:build_jaxlib=true, returns pypi test deps.\n-          If --//jax:build_jaxlib=false, returns jaxlib pypi wheel dep and pypi test deps.\n-          If --//jax:build_jaxlib=wheel, returns jaxlib py_import dep and pypi test deps.\n-        For GPU builds:\n-          If --//jax:build_jaxlib=true, returns pypi test deps and gpu build deps.\n-          If --//jax:build_jaxlib=false, returns jaxlib, jax-cuda-plugin,\n-            jax-cuda-pjrt pypi wheel deps and pypi test deps.\n-          If --//jax:build_jaxlib=wheel, returns jaxlib,\n-            jax-cuda-plugin, jax-cuda-pjrt py_import deps and pypi test deps.\n-    \"\"\"\n-    gpu_build_deps = [\n-        \"//jaxlib/cuda:gpu_only_test_deps\",\n-        \"//jaxlib/rocm:gpu_only_test_deps\",\n-        \"//jax_plugins:gpu_plugin_only_test_deps\",\n-    ]\n-    pypi_test_deps = [d for d in deps if d.startswith(\"@pypi//\")]\n-\n-    gpu_py_imports = [\n-        \"//jaxlib/tools:jaxlib_py_import\",\n-        \"//jaxlib/tools:jax_cuda_plugin_py_import\",\n-        \"//jaxlib/tools:jax_cuda_pjrt_py_import\",\n-    ] + pypi_test_deps\n-    cpu_py_imports = [\n-        \"//jaxlib/tools:jaxlib_py_import\",\n-    ] + pypi_test_deps\n-    jaxlib_pypi_wheel_deps = [\n-        \"@pypi//jaxlib\",\n-    ] + pypi_test_deps\n-\n-    if backend_independent:\n-        test_deps = pypi_test_deps\n-        gpu_pypi_wheel_deps = jaxlib_pypi_wheel_deps\n-        gpu_py_import_deps = cpu_py_imports\n-    else:\n-        test_deps = gpu_build_deps + pypi_test_deps\n-        gpu_pypi_wheel_deps = jaxlib_pypi_wheel_deps + [\n+def _gpu_test_deps():\n+    \"\"\"Returns the additional dependencies needed for a GPU test.\"\"\"\n+    return select({\n+        \"//jax:config_build_jaxlib_true\": [\n+            \"//jaxlib/cuda:gpu_only_test_deps\",\n+            \"//jaxlib/rocm:gpu_only_test_deps\",\n+            \"//jax_plugins:gpu_plugin_only_test_deps\",\n+        ],\n+        \"//jax:config_build_jaxlib_false\": [\n             \"@pypi//jax_cuda12_plugin\",\n             \"@pypi//jax_cuda12_pjrt\",\n-        ]\n-        gpu_py_import_deps = gpu_py_imports\n-\n-    return select({\n-        \"//jax:config_build_jaxlib_true\": test_deps,\n-        \"//jax_plugins/cuda:disable_jaxlib_for_cpu_build\": jaxlib_pypi_wheel_deps,\n-        \"//jax_plugins/cuda:disable_jaxlib_for_cuda12_build\": gpu_pypi_wheel_deps,\n-        \"//jax_plugins/cuda:enable_py_import_for_cpu_build\": cpu_py_imports,\n-        \"//jax_plugins/cuda:enable_py_import_for_cuda12_build\": gpu_py_import_deps,\n+        ],\n+        \"//jax:config_build_jaxlib_wheel\": [\n+            \"//jaxlib/tools:jax_cuda_plugin_py_import\",\n+            \"//jaxlib/tools:jax_cuda_pjrt_py_import\",\n+        ],\n     })\n \n def _get_jax_test_deps(deps):\n@@ -246,28 +212,23 @@ def _get_jax_test_deps(deps):\n       If --//jax:build_jax=false, returns jax pypi wheel dep and transitive pypi test deps.\n       If --//jax:build_jax=wheel, returns jax py_import dep and transitive pypi test deps.\n     \"\"\"\n-    jax_build_deps = [d for d in deps if not d.startswith(\"@pypi//\")]\n+    non_pypi_deps = [d for d in deps if not d.startswith(\"@pypi//\")]\n \n     # A lot of tests don't have explicit dependencies on scipy, ml_dtypes, etc. But the tests\n     # transitively depends on them via //jax. So we need to make sure that these dependencies are\n     # included in the test when JAX is built from source.\n-    jax_transitive_pypi_test_deps = {k: \"true\" for k in py_deps([\n+    pypi_deps = depset([d for d in deps if d.startswith(\"@pypi//\")])\n+    pypi_deps = depset(py_deps([\n         \"ml_dtypes\",\n         \"scipy\",\n         \"opt_einsum\",\n         \"flatbuffers\",\n-    ])}\n+    ]), transitive = [pypi_deps]).to_list()\n \n-    # Remove the pypi deps that are already provided by _get_test_deps().\n-    for d in deps:\n-        if d.startswith(\"@pypi//\") and jax_transitive_pypi_test_deps.get(d):\n-            jax_transitive_pypi_test_deps.pop(d)\n-    return select({\n-        \"//jax:disable_jaxlib_and_jax_build\": [\"//:jax_wheel_with_internal_test_util\"] +\n-                                              jax_transitive_pypi_test_deps.keys(),\n-        \"//jax:enable_jaxlib_and_jax_py_import\": [\"//:jax_py_import\"] +\n-                                                 jax_transitive_pypi_test_deps.keys(),\n-        \"//conditions:default\": jax_build_deps + jax_transitive_pypi_test_deps.keys(),\n+    return pypi_deps + select({\n+        \"//jax:config_build_jax_false\": [\"//:jax_wheel_with_internal_test_util\"],\n+        \"//jax:config_build_jax_wheel\": [\"//:jax_py_import\"],\n+        \"//jax:config_build_jax_true\": non_pypi_deps,\n     })\n \n # buildifier: disable=function-docstring\n@@ -316,18 +277,21 @@ def jax_multiplatform_test(\n         test_tags = list(tags) + [\"jax_test_%s\" % backend] + backend_tags.get(backend, [])\n         if enable_backends != None and backend not in enable_backends and not any([config.startswith(backend) for config in enable_configs]):\n             test_tags.append(\"manual\")\n+        test_deps = _cpu_test_deps() + _get_jax_test_deps([\n+            \"//jax\",\n+            \"//jax:test_util\",\n+        ] + deps)\n         if backend == \"gpu\":\n+            test_deps += _gpu_test_deps()\n             test_tags += tf_cuda_tests_tags()\n+        elif backend == \"tpu\":\n+            test_deps += [\"@pypi//libtpu\"]\n         native.py_test(\n             name = name + \"_\" + backend,\n             srcs = srcs,\n             args = test_args,\n             env = env,\n-            deps = _get_test_deps(deps, backend_independent = False) +\n-                   _get_jax_test_deps([\n-                       \"//jax\",\n-                       \"//jax:test_util\",\n-                   ] + deps),\n+            deps = test_deps,\n             data = data,\n             shard_count = test_shards,\n             tags = test_tags,\n@@ -620,13 +584,13 @@ def jax_py_test(\n     env = dict(env)\n     env.setdefault(\"PYTHONWARNINGS\", \"error\")\n     deps = kwargs.get(\"deps\", [])\n-    test_deps = _get_test_deps(deps, backend_independent = True) + _get_jax_test_deps(deps)\n+    test_deps = _cpu_test_deps() + _get_jax_test_deps(deps)\n     kwargs[\"deps\"] = test_deps\n     py_test(name = name, env = env, **kwargs)\n \n def pytype_test(name, **kwargs):\n     deps = kwargs.get(\"deps\", [])\n-    test_deps = _get_test_deps(deps, backend_independent = True) + _get_jax_test_deps(deps)\n+    test_deps = _cpu_test_deps() + _get_jax_test_deps(deps)\n     kwargs[\"deps\"] = test_deps\n     native.py_test(name = name, **kwargs)\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add use_raw_buffers which allows switching the implementation to\nhold references to raw buffers instead of PjRtBuffers.\n\nThis fixes an issue where the buffers can be deleted before\nthe transfer is complete, but introduces another problem where\nif they are donated it will now silently read from donated arrays.\n\nOnce the underlying runtime exposes usage holds properly, this\nnew codepath should take a usage hold and the old pjrtbuffer\npath should be removed.\n\nPiperOrigin-RevId: 758819621",
        "output": [
            "```diff\nCommit: 5a6957fa75db31c9f63df8788f12f47b0d060f44\nDate: 2025-05-14T20:51:03Z\nURL: https://github.com/jax-ml/jax/commit/5a6957fa75db31c9f63df8788f12f47b0d060f44\nFiles changed: 2\nAdditions: +57, Deletions: -8\ndiff --git a/jaxlib/BUILD b/jaxlib/BUILD\nindex add6dbd7d92a..7047ddc3edd6 100644\n--- a/jaxlib/BUILD\n+++ b/jaxlib/BUILD\n@@ -1033,6 +1033,7 @@ cc_library(\n         \"@xla//xla/pjrt:status_casters\",\n         \"@xla//xla/python:nb_numpy\",\n         \"@xla//xla/python:types\",\n+        \"@xla//xla/python:version\",\n         \"@xla//xla/python/ifrt\",\n         \"@xla//xla/python/pjrt_ifrt\",\n         \"@xla//xla/python/pjrt_ifrt:pjrt_dtype\",\ndiff --git a/jaxlib/py_socket_transfer.cc b/jaxlib/py_socket_transfer.cc\nindex 114e3c14874d..fde63df8da47 100644\n--- a/jaxlib/py_socket_transfer.cc\n+++ b/jaxlib/py_socket_transfer.cc\n@@ -60,6 +60,7 @@ limitations under the License.\n #include \"xla/python/transfer/streaming_ifrt.h\"\n #include \"xla/python/transfer/transfer_socket.pb.h\"\n #include \"xla/python/types.h\"\n+#include \"xla/python/version.h\"\n #include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/tsl/platform/statusor.h\"\n #include \"xla/util.h\"\n@@ -109,6 +110,7 @@ absl::StatusOr<xla::PjRtMemorySpace*> MemorySpaceFromSharding(\n   }\n }\n \n+#if JAX_IFRT_VERSION_NUMBER < 8\n class IfrtArrayEntry : public PullTable::Entry {\n  public:\n   struct BufferRef {\n@@ -153,10 +155,48 @@ class IfrtArrayEntry : public PullTable::Entry {\n   std::shared_ptr<PremappedCopierState> state_;\n   size_t xfer_size_;\n };\n+#endif\n \n-absl::StatusOr<tsl::RCReference<IfrtArrayEntry>> CreatePullEntry(\n+absl::StatusOr<tsl::RCReference<PullTable::Entry>> CreatePullEntry(\n     const std::vector<xla::ifrt::ArrayRef>& arrs,\n-    std::shared_ptr<PremappedCopierState> state, size_t xfer_size) {\n+    std::shared_ptr<PremappedCopierState> state, size_t xfer_size,\n+    bool use_raw_buffers) {\n+#if JAX_IFRT_VERSION_NUMBER >= 8\n+  if (use_raw_buffers) {\n+    std::vector<RawBufferEntry::BufferRef> refs;\n+    for (auto& arr : arrs) {\n+      auto* pjrt_arr = llvm::dyn_cast_or_null<xla::ifrt::PjRtArray>(arr.get());\n+      if (pjrt_arr == nullptr) {\n+        return absl::InvalidArgumentError(\n+            \"Cannot remote transfer non-pjrt arrays.\");\n+      }\n+      for (auto& pjrt_buf : pjrt_arr->pjrt_buffers()) {\n+        TF_ASSIGN_OR_RETURN(size_t buf_size,\n+                            pjrt_buf->GetOnDeviceSizeInBytes());\n+        TF_ASSIGN_OR_RETURN(\n+            auto raw_buffer,\n+            xla::PjRtRawBuffer::CreateRawAliasOfBuffer(pjrt_buf.get()));\n+        refs.push_back(\n+            {pjrt_buf->GetReadyFuture(), std::move(raw_buffer), buf_size});\n+      }\n+    }\n+    return tsl::MakeRef<RawBufferEntry>(std::move(refs), state, xfer_size);\n+  }\n+\n+  std::vector<PjRtBufferEntry::BufferRef> refs;\n+  for (auto& arr : arrs) {\n+    auto* pjrt_arr = llvm::dyn_cast_or_null<xla::ifrt::PjRtArray>(arr.get());\n+    if (pjrt_arr == nullptr) {\n+      return absl::InvalidArgumentError(\n+          \"Cannot remote transfer non-pjrt arrays.\");\n+    }\n+    for (auto& pjrt_buf : pjrt_arr->pjrt_buffers()) {\n+      TF_ASSIGN_OR_RETURN(size_t buf_size, pjrt_buf->GetOnDeviceSizeInBytes());\n+      refs.push_back({pjrt_buf, buf_size});\n+    }\n+  }\n+  return tsl::MakeRef<PjRtBufferEntry>(std::move(refs), state, xfer_size);\n+#else\n   std::vector<IfrtArrayEntry::BufferRef> refs;\n   for (auto& arr : arrs) {\n     auto* pjrt_arr = llvm::dyn_cast_or_null<xla::ifrt::PjRtArray>(arr.get());\n@@ -170,6 +210,7 @@ absl::StatusOr<tsl::RCReference<IfrtArrayEntry>> CreatePullEntry(\n     }\n   }\n   return tsl::MakeRef<IfrtArrayEntry>(std::move(refs), state, xfer_size);\n+#endif\n }\n \n class PyTransferServerConnection {\n@@ -195,7 +236,8 @@ class PyTransferServer {\n   absl::Status Start(xla::ifrt::Client* client, size_t max_num_parallel_copies,\n                      size_t xfer_size, const SocketAddress& addr,\n                      const std::vector<SocketAddress>& transport_addresses,\n-                     bool supports_pinned_allocator) {\n+                     bool supports_pinned_allocator, bool use_raw_buffers) {\n+    use_raw_buffers_ = use_raw_buffers;\n     std::shared_ptr<BulkTransportFactory> factory;\n     if (transport_addresses.empty()) {\n       factory = BulkTransportFactory::CreateLocal();\n@@ -235,8 +277,9 @@ class PyTransferServer {\n   }\n \n   void AwaitPull(uint64_t uuid, const std::vector<xla::ifrt::ArrayRef>& arrs) {\n-    server_->AwaitPull(uuid, xla::ValueOrThrow(CreatePullEntry(\n-                                 arrs, premapped_copier_, xfer_size_)));\n+    server_->AwaitPull(\n+        uuid, xla::ValueOrThrow(CreatePullEntry(arrs, premapped_copier_,\n+                                                xfer_size_, use_raw_buffers_)));\n   }\n \n   size_t xfer_size() { return xfer_size_; }\n@@ -249,6 +292,7 @@ class PyTransferServer {\n   std::shared_ptr<SocketServer> server_;\n   std::shared_ptr<PremappedCopierState> premapped_copier_;\n   size_t xfer_size_;\n+  bool use_raw_buffers_ = false;\n };\n \n absl::StatusOr<xla::ifrt::ArraySpec> ArraySpecFromShapeDtypeStruct(\n@@ -394,7 +438,8 @@ void RegisterTransferServerTypes(nanobind::module_& m) {\n       [](xla::nb_class_ptr<xla::PyClient> py_client, std::string address,\n          std::vector<std::string> transport_addresses_str,\n          size_t max_num_parallel_copies, size_t transfer_size,\n-         bool supports_pinned_allocator) -> PyTransferServer {\n+         bool supports_pinned_allocator,\n+         bool use_raw_buffers) -> PyTransferServer {\n         PyTransferServer result;\n         std::vector<SocketAddress> transport_addresses;\n         transport_addresses.reserve(transport_addresses_str.size());\n@@ -405,7 +450,7 @@ void RegisterTransferServerTypes(nanobind::module_& m) {\n         xla::ThrowIfError(result.Start(\n             py_client->ifrt_client(), max_num_parallel_copies, transfer_size,\n             xla::ValueOrThrow(SocketAddress::Parse(address)),\n-            transport_addresses, supports_pinned_allocator));\n+            transport_addresses, supports_pinned_allocator, use_raw_buffers));\n         return result;\n       },\n       nb::arg(\"client\"), nb::arg(\"address\") = SocketAddress().ToString(),\n@@ -413,7 +458,10 @@ void RegisterTransferServerTypes(nanobind::module_& m) {\n       nb::arg(\"max_num_parallel_copies\") = 8,\n       nb::arg(\"transfer_size\") = 256 * 1024 * 1024,\n       // Dual pinning not confirmed to be supported.\n-      nb::arg(\"supports_pinned_allocator\") = false);\n+      nb::arg(\"supports_pinned_allocator\") = false,\n+      // Technically unsafe (because a future donation won't wait for the\n+      // transfer to complete).\n+      nb::arg(\"use_raw_buffers\") = false);\n }\n \n }  // namespace aux\n\n```"
        ],
        "from_id": [
            "pschuh",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Simplify the Bazel logic used to add dependencies to tests.\n\nA lot of this logic was confusing phrased as conditions over both CPU\nand GPU build flags. But we can decompose it:\n* dependencies we add for CPU tests, and\n* additional dependencies we add for GPU tests.\n\nWhile we are here, also add the necessary pypi dependency for TPU tests.",
        "output": [
            "```diff\nCommit: 4102cf09ec9b30651f12c22f8537f007d6f7e129\nDate: 2025-05-14T20:41:32Z\nURL: https://github.com/jax-ml/jax/commit/4102cf09ec9b30651f12c22f8537f007d6f7e129\nFiles changed: 4\nAdditions: +48, Deletions: -113\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 6afcdf2892f4..3f73a4b9e68f 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -101,17 +101,22 @@ string_flag(\n )\n \n config_setting(\n-    name = \"disable_jaxlib_and_jax_build\",\n+    name = \"config_build_jax_true\",\n+    flag_values = {\n+        \":build_jax\": \"true\",\n+    },\n+)\n+\n+config_setting(\n+    name = \"config_build_jax_false\",\n     flag_values = {\n-        \":build_jaxlib\": \"false\",\n         \":build_jax\": \"false\",\n     },\n )\n \n config_setting(\n-    name = \"enable_jaxlib_and_jax_py_import\",\n+    name = \"config_build_jax_wheel\",\n     flag_values = {\n-        \":build_jaxlib\": \"wheel\",\n         \":build_jax\": \"wheel\",\n     },\n )\ndiff --git a/jax_plugins/cuda/BUILD.bazel b/jax_plugins/cuda/BUILD.bazel\nindex c3c20f536cff..7070bf6bc495 100644\n--- a/jax_plugins/cuda/BUILD.bazel\n+++ b/jax_plugins/cuda/BUILD.bazel\n@@ -14,7 +14,6 @@\n \n load(\n     \"//jaxlib:jax.bzl\",\n-    \"if_windows\",\n     \"py_library_providing_imports_info\",\n     \"pytype_library\",\n )\n@@ -58,35 +57,3 @@ py_library_providing_imports_info(\n     data = [\":pjrt_c_api_gpu_plugin.so\"],\n     lib_rule = pytype_library,\n )\n-\n-config_setting(\n-    name = \"disable_jaxlib_for_cpu_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"false\",\n-        \"@local_config_cuda//:enable_cuda\": \"False\",\n-    },\n-)\n-\n-config_setting(\n-    name = \"disable_jaxlib_for_cuda12_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"false\",\n-        \"@local_config_cuda//:enable_cuda\": \"True\",\n-    },\n-)\n-\n-config_setting(\n-    name = \"enable_py_import_for_cpu_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"wheel\",\n-        \"@local_config_cuda//:enable_cuda\": \"False\",\n-    },\n-)\n-\n-config_setting(\n-    name = \"enable_py_import_for_cuda12_build\",\n-    flag_values = {\n-        \"//jax:build_jaxlib\": \"wheel\",\n-        \"@local_config_cuda//:enable_cuda\": \"True\",\n-    },\n-)\ndiff --git a/jax_plugins/rocm/BUILD.bazel b/jax_plugins/rocm/BUILD.bazel\nindex 15e9e627830e..7ee0726e7960 100644\n--- a/jax_plugins/rocm/BUILD.bazel\n+++ b/jax_plugins/rocm/BUILD.bazel\n@@ -16,7 +16,6 @@ licenses([\"notice\"])\n \n load(\n   \"//jaxlib:jax.bzl\",\n-  \"if_windows\",\n   \"py_library_providing_imports_info\",\n   \"pytype_library\",\n )\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex a48c44f406f2..1d4e24720c2e 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -167,70 +167,36 @@ def if_building_jaxlib(\n       if_building: the source code targets to depend on in case we don't depend on the jaxlib wheels\n       if_not_building: the wheels to depend on if we are not depending directly on //jaxlib.\n     \"\"\"\n-\n     return select({\n         \"//jax:config_build_jaxlib_true\": if_building,\n         \"//jax:config_build_jaxlib_false\": if_not_building,\n         \"//jax:config_build_jaxlib_wheel\": [],\n     })\n \n-def _get_test_deps(deps, backend_independent):\n-    \"\"\"Returns the test deps for the given backend.\n-\n-    Args:\n-      deps: the full list of test dependencies\n-      backend_independent: whether the test is backend independent\n+def _cpu_test_deps():\n+    \"\"\"Returns the test depencies needed for a CPU-only JAX test.\"\"\"\n+    return select({\n+        \"//jax:config_build_jaxlib_true\": [],\n+        \"//jax:config_build_jaxlib_false\": [\"@pypi//jaxlib\"],\n+        \"//jax:config_build_jaxlib_wheel\": [\"//jaxlib/tools:jaxlib_py_import\"],\n+    })\n \n-    Returns:\n-      A list of test deps for the given backend.\n-        For CPU builds:\n-          If --//jax:build_jaxlib=true, returns pypi test deps.\n-          If --//jax:build_jaxlib=false, returns jaxlib pypi wheel dep and pypi test deps.\n-          If --//jax:build_jaxlib=wheel, returns jaxlib py_import dep and pypi test deps.\n-        For GPU builds:\n-          If --//jax:build_jaxlib=true, returns pypi test deps and gpu build deps.\n-          If --//jax:build_jaxlib=false, returns jaxlib, jax-cuda-plugin,\n-            jax-cuda-pjrt pypi wheel deps and pypi test deps.\n-          If --//jax:build_jaxlib=wheel, returns jaxlib,\n-            jax-cuda-plugin, jax-cuda-pjrt py_import deps and pypi test deps.\n-    \"\"\"\n-    gpu_build_deps = [\n-        \"//jaxlib/cuda:gpu_only_test_deps\",\n-        \"//jaxlib/rocm:gpu_only_test_deps\",\n-        \"//jax_plugins:gpu_plugin_only_test_deps\",\n-    ]\n-    pypi_test_deps = [d for d in deps if d.startswith(\"@pypi//\")]\n-\n-    gpu_py_imports = [\n-        \"//jaxlib/tools:jaxlib_py_import\",\n-        \"//jaxlib/tools:jax_cuda_plugin_py_import\",\n-        \"//jaxlib/tools:jax_cuda_pjrt_py_import\",\n-    ] + pypi_test_deps\n-    cpu_py_imports = [\n-        \"//jaxlib/tools:jaxlib_py_import\",\n-    ] + pypi_test_deps\n-    jaxlib_pypi_wheel_deps = [\n-        \"@pypi//jaxlib\",\n-    ] + pypi_test_deps\n-\n-    if backend_independent:\n-        test_deps = pypi_test_deps\n-        gpu_pypi_wheel_deps = jaxlib_pypi_wheel_deps\n-        gpu_py_import_deps = cpu_py_imports\n-    else:\n-        test_deps = gpu_build_deps + pypi_test_deps\n-        gpu_pypi_wheel_deps = jaxlib_pypi_wheel_deps + [\n+def _gpu_test_deps():\n+    \"\"\"Returns the additional dependencies needed for a GPU test.\"\"\"\n+    return select({\n+        \"//jax:config_build_jaxlib_true\": [\n+            \"//jaxlib/cuda:gpu_only_test_deps\",\n+            \"//jaxlib/rocm:gpu_only_test_deps\",\n+            \"//jax_plugins:gpu_plugin_only_test_deps\",\n+        ],\n+        \"//jax:config_build_jaxlib_false\": [\n             \"@pypi//jax_cuda12_plugin\",\n             \"@pypi//jax_cuda12_pjrt\",\n-        ]\n-        gpu_py_import_deps = gpu_py_imports\n-\n-    return select({\n-        \"//jax:config_build_jaxlib_true\": test_deps,\n-        \"//jax_plugins/cuda:disable_jaxlib_for_cpu_build\": jaxlib_pypi_wheel_deps,\n-        \"//jax_plugins/cuda:disable_jaxlib_for_cuda12_build\": gpu_pypi_wheel_deps,\n-        \"//jax_plugins/cuda:enable_py_import_for_cpu_build\": cpu_py_imports,\n-        \"//jax_plugins/cuda:enable_py_import_for_cuda12_build\": gpu_py_import_deps,\n+        ],\n+        \"//jax:config_build_jaxlib_wheel\": [\n+            \"//jaxlib/tools:jax_cuda_plugin_py_import\",\n+            \"//jaxlib/tools:jax_cuda_pjrt_py_import\",\n+        ],\n     })\n \n def _get_jax_test_deps(deps):\n@@ -246,28 +212,23 @@ def _get_jax_test_deps(deps):\n       If --//jax:build_jax=false, returns jax pypi wheel dep and transitive pypi test deps.\n       If --//jax:build_jax=wheel, returns jax py_import dep and transitive pypi test deps.\n     \"\"\"\n-    jax_build_deps = [d for d in deps if not d.startswith(\"@pypi//\")]\n+    non_pypi_deps = [d for d in deps if not d.startswith(\"@pypi//\")]\n \n     # A lot of tests don't have explicit dependencies on scipy, ml_dtypes, etc. But the tests\n     # transitively depends on them via //jax. So we need to make sure that these dependencies are\n     # included in the test when JAX is built from source.\n-    jax_transitive_pypi_test_deps = {k: \"true\" for k in py_deps([\n+    pypi_deps = depset([d for d in deps if d.startswith(\"@pypi//\")])\n+    pypi_deps = depset(py_deps([\n         \"ml_dtypes\",\n         \"scipy\",\n         \"opt_einsum\",\n         \"flatbuffers\",\n-    ])}\n+    ]), transitive = [pypi_deps]).to_list()\n \n-    # Remove the pypi deps that are already provided by _get_test_deps().\n-    for d in deps:\n-        if d.startswith(\"@pypi//\") and jax_transitive_pypi_test_deps.get(d):\n-            jax_transitive_pypi_test_deps.pop(d)\n-    return select({\n-        \"//jax:disable_jaxlib_and_jax_build\": [\"//:jax_wheel_with_internal_test_util\"] +\n-                                              jax_transitive_pypi_test_deps.keys(),\n-        \"//jax:enable_jaxlib_and_jax_py_import\": [\"//:jax_py_import\"] +\n-                                                 jax_transitive_pypi_test_deps.keys(),\n-        \"//conditions:default\": jax_build_deps + jax_transitive_pypi_test_deps.keys(),\n+    return pypi_deps + select({\n+        \"//jax:config_build_jax_false\": [\"//:jax_wheel_with_internal_test_util\"],\n+        \"//jax:config_build_jax_wheel\": [\"//:jax_py_import\"],\n+        \"//jax:config_build_jax_true\": non_pypi_deps,\n     })\n \n # buildifier: disable=function-docstring\n@@ -316,18 +277,21 @@ def jax_multiplatform_test(\n         test_tags = list(tags) + [\"jax_test_%s\" % backend] + backend_tags.get(backend, [])\n         if enable_backends != None and backend not in enable_backends and not any([config.startswith(backend) for config in enable_configs]):\n             test_tags.append(\"manual\")\n+        test_deps = _cpu_test_deps() + _get_jax_test_deps([\n+            \"//jax\",\n+            \"//jax:test_util\",\n+        ] + deps)\n         if backend == \"gpu\":\n+            test_deps += _gpu_test_deps()\n             test_tags += tf_cuda_tests_tags()\n+        elif backend == \"tpu\":\n+            test_deps += [\"@pypi//libtpu\"]\n         native.py_test(\n             name = name + \"_\" + backend,\n             srcs = srcs,\n             args = test_args,\n             env = env,\n-            deps = _get_test_deps(deps, backend_independent = False) +\n-                   _get_jax_test_deps([\n-                       \"//jax\",\n-                       \"//jax:test_util\",\n-                   ] + deps),\n+            deps = test_deps,\n             data = data,\n             shard_count = test_shards,\n             tags = test_tags,\n@@ -620,13 +584,13 @@ def jax_py_test(\n     env = dict(env)\n     env.setdefault(\"PYTHONWARNINGS\", \"error\")\n     deps = kwargs.get(\"deps\", [])\n-    test_deps = _get_test_deps(deps, backend_independent = True) + _get_jax_test_deps(deps)\n+    test_deps = _cpu_test_deps() + _get_jax_test_deps(deps)\n     kwargs[\"deps\"] = test_deps\n     py_test(name = name, env = env, **kwargs)\n \n def pytype_test(name, **kwargs):\n     deps = kwargs.get(\"deps\", [])\n-    test_deps = _get_test_deps(deps, backend_independent = True) + _get_jax_test_deps(deps)\n+    test_deps = _cpu_test_deps() + _get_jax_test_deps(deps)\n     kwargs[\"deps\"] = test_deps\n     native.py_test(name = name, **kwargs)\n \n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Merge pull request #28748 from dfm:scan-lin\n\nPiperOrigin-RevId: 758806795",
        "output": [
            "```diff\nCommit: 43e3385f297287f358ec46a5b99b6a2d2f046b54\nDate: 2025-05-14T20:18:51Z\nURL: https://github.com/jax-ml/jax/commit/43e3385f297287f358ec46a5b99b6a2d2f046b54\nFiles changed: 2\nAdditions: +116, Deletions: -0\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 05e9c010dc51..c85a23b6b199 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -683,6 +683,108 @@ def _scan_jvp(primals, tangents, reverse, length, jaxpr, num_consts, num_carry,\n                   for p, nz in zip(primals_out, nonzeros_out)]\n   return primals_out, tangents_out\n \n+def _scan_linearization(nzs, *primals_in, reverse: bool, length: int,\n+                        num_consts: int, num_carry: int,\n+                        jaxpr: core.ClosedJaxpr, linear: Sequence[bool],\n+                        unroll: int, _split_transpose: bool):\n+  const_nz, init_nz, xs_nz = split_list(nzs, [num_consts, num_carry])\n+  carry_nz = init_nz\n+  for _ in range(1 + num_carry):\n+    nzs = const_nz + carry_nz + xs_nz\n+    primal_jaxpr, num_res, nzs_out, tangent_jaxpr = ad.linearize_jaxpr(jaxpr, nzs)\n+    carry_nz_out = nzs_out[:num_carry]\n+    if carry_nz_out == carry_nz:\n+      break\n+    else:\n+      carry_nz = _map(operator.or_, carry_nz, carry_nz_out)\n+  else:\n+    assert False, \"Fixpoint not reached\"\n+\n+  # The linearize_jaxpr function produces primal_jaxpr with num_res residuals\n+  # output at the front, and tangent_jaxpr with num_res residuals input at the\n+  # back. We could move all the residuals to the back and treat them as\n+  # extensive outputs, but this would be wasteful for residuals that are\n+  # loop invariant, or forwarded extensive inputs.\n+\n+  # First, for residuals that are forwarded constants, we move those to the\n+  # front in the tangent_jaxpr to treat them as intensive inputs.\n+  in_fwd = pe._jaxpr_forwarding(primal_jaxpr.jaxpr)\n+  primal_jaxpr, tangent_jaxpr, intensive_res, in_fwd = _const_to_intensive_res_forwarding(\n+      primal_jaxpr, tangent_jaxpr, num_res, num_consts, primals_in, in_fwd)\n+  num_intensive_res = len(intensive_res)\n+  num_res -= num_intensive_res\n+\n+  # After pruning the intensive residuals, the rest get moved to the back and\n+  # handled as extensive outputs from the primal.\n+  num_out = len(nzs_out)\n+  primal_jaxpr = pe.move_outvars_to_back(\n+      primal_jaxpr, [True] * num_res + [False] * num_out)\n+  in_fwd = in_fwd[num_res:] + in_fwd[:num_res]\n+\n+  # Then, any residuals or other extensive outputs that are forwarded extensive\n+  # inputs, we remove them from the primal jaxpr, and manually forward them.\n+  in_fwd = [in_idx if out_idx >= num_carry and in_idx is not None and\n+            in_idx >= num_consts + num_carry else None\n+            for out_idx, in_idx in enumerate(in_fwd)]\n+  primal_jaxpr = pe.prune_closed_jaxpr_outputs(primal_jaxpr,\n+                                               [i is None for i in in_fwd])\n+\n+  out = scan_p.bind(*primals_in, jaxpr=primal_jaxpr, reverse=reverse,\n+                    length=length, num_consts=num_consts, num_carry=num_carry,\n+                    linear=linear, unroll=unroll, _split_transpose=_split_transpose)\n+  out_ = iter(out)\n+  all_out = [next(out_) if f is None else _maybe_put(primals_in[f]) for f in in_fwd]\n+  assert next(out_, None) is None\n+  primals_out, extensive_res = split_list(all_out, [len(all_out) - num_res])\n+  res = [*intensive_res, *extensive_res]\n+\n+  def tangent_fun(res, *tangents):\n+    intensive_res, extensive_res = split_list(res, [num_intensive_res])\n+    nz_tangents = [ad.instantiate_zeros(x) for nz, x in zip(nzs, tangents) if nz]\n+    tangent_linear = (\n+        (False,) * len(intensive_res) +\n+        (True,) * len(nz_tangents) +\n+        (False,) * len(extensive_res)\n+    )\n+    tangent_num_consts = len(intensive_res) + sum(nzs[:num_consts])\n+    tangent_num_carry = sum(nzs[num_consts:num_consts + num_carry])\n+    nz_tangents_out = scan_p.bind(*intensive_res, *nz_tangents, *extensive_res,\n+                                  jaxpr=tangent_jaxpr,\n+                                  reverse=reverse, length=length,\n+                                  num_consts=tangent_num_consts,\n+                                  num_carry=tangent_num_carry,\n+                                  linear=tangent_linear, unroll=unroll,\n+                                  _split_transpose=_split_transpose)\n+    tangent_avals_out = [v.aval.to_tangent_aval() for v in jaxpr.jaxpr.outvars]\n+    nz_tangents_out_ = iter(nz_tangents_out)\n+    tangents_out = [next(nz_tangents_out_) if nz else ad.Zero(aval)\n+                    for aval, nz in zip(tangent_avals_out, nzs_out)]\n+    assert next(nz_tangents_out_, None) is None\n+    return tangents_out\n+\n+  return primals_out, nzs_out, res, tangent_fun\n+\n+def _const_to_intensive_res_forwarding(\n+    primal_jaxpr: core.ClosedJaxpr,\n+    tangent_jaxpr: core.ClosedJaxpr,\n+    num_res: int,\n+    num_consts: int,\n+    primals_in: Sequence[Any],\n+    in_fwd: list[int | None]\n+) -> tuple[core.ClosedJaxpr, core.ClosedJaxpr, list[Any], list[int | None]]:\n+  const_to_res = [in_idx if in_idx is not None and in_idx < num_consts else None\n+                  for in_idx in in_fwd[:num_res]]\n+  new_in_fwd = [f for c, f in zip(const_to_res, in_fwd[:num_res]) if c is None]\n+  new_in_fwd += in_fwd[num_res:]\n+  intensive_res = [primals_in[f] for f in const_to_res if f is not None]\n+  num_out = len(primal_jaxpr.out_avals) - num_res\n+  primal_jaxpr = pe.prune_closed_jaxpr_outputs(\n+      primal_jaxpr, [i is None for i in const_to_res] + [True] * num_out)\n+  num_nz = len(tangent_jaxpr.in_avals) - num_res\n+  tangent_jaxpr = pe.move_binders_to_front(\n+      tangent_jaxpr, [False] * num_nz + [i is not None for i in const_to_res])\n+  return primal_jaxpr, tangent_jaxpr, intensive_res, new_in_fwd\n+\n def _scan_partial_eval(trace, *tracers, reverse: bool,\n                        length: int, num_consts: int, num_carry: int,\n                        jaxpr: core.ClosedJaxpr, linear: Sequence[bool],\n@@ -1385,6 +1487,7 @@ def arrange_jaxpr_args_for_wrapped(args):\n scan_p.def_effectful_abstract_eval(_scan_abstract_eval)\n ad.primitive_jvps[scan_p] = _scan_jvp\n ad.primitive_transposes[scan_p] = _scan_transpose\n+ad.primitive_linearizations[scan_p] = _scan_linearization\n pe.custom_partial_eval_rules[scan_p] = _scan_partial_eval\n xla.register_initial_style_primitive(scan_p)\n mlir.register_lowering(scan_p,\ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex d32d761ee1fa..54dff47fea32 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -28,6 +28,7 @@\n \n import jax\n from jax._src import core\n+from jax._src import config\n from jax import dtypes\n from jax import lax\n from jax import random\n@@ -3298,6 +3299,18 @@ def body_fun(c, _):\n     outs_ref = body_fun(body_fun(init_vals, [x[0] for x in xs])[0], [x[1] for x in xs])[0]\n     self.assertAllClose(outs, outs_ref, check_dtypes=False)\n \n+  def test_scan_diff_of_print(self):\n+    # ref: https://github.com/jax-ml/jax/issues/28738\n+    def f(c, _):\n+      jax.debug.print(\"c = {c}\", c=c, ordered=True)\n+      return c + 1, None\n+    def g(x):\n+      return jax.lax.scan(f, x, length=2)[0]\n+    with config.use_direct_linearize(True):\n+      jaxpr = jax.make_jaxpr(jax.value_and_grad(g))(1.0)\n+    eqn_jaxpr = jaxpr.eqns[0].params[\"jaxpr\"]\n+    self.assertIn(\"debug_callback\", [e.primitive.name for e in eqn_jaxpr.eqns])\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28751 from hawkinsp:plugins\n\nPiperOrigin-RevId: 758801504",
        "output": [
            "```diff\nCommit: db99793ace5034ceaf93c840a2764376a247a1a1\nDate: 2025-05-14T20:07:06Z\nURL: https://github.com/jax-ml/jax/commit/db99793ace5034ceaf93c840a2764376a247a1a1\nFiles changed: 8\nAdditions: +52, Deletions: -41\ndiff --git a/jax_plugins/cuda/BUILD.bazel b/jax_plugins/cuda/BUILD.bazel\nindex 6566cfc62b0c..c3c20f536cff 100644\n--- a/jax_plugins/cuda/BUILD.bazel\n+++ b/jax_plugins/cuda/BUILD.bazel\n@@ -34,15 +34,28 @@ exports_files([\n     \"setup.py\",\n ])\n \n+cc_binary(\n+    name = \"pjrt_c_api_gpu_plugin.so\",\n+    linkopts = [\n+        \"-Wl,--version-script,$(location :gpu_version_script.lds)\",\n+        \"-Wl,--no-undefined\",\n+    ],\n+    linkshared = True,\n+    deps = [\n+        \":gpu_version_script.lds\",\n+        \"//jaxlib/mosaic/gpu:custom_call\",\n+        \"@xla//xla/pjrt/c:pjrt_c_api_gpu\",\n+        \"@xla//xla/service:gpu_plugin\",\n+        \"@xla//xla/stream_executor:cuda_platform\",\n+    ],\n+)\n+\n py_library_providing_imports_info(\n     name = \"cuda_plugin\",\n     srcs = [\n         \"__init__.py\",\n     ],\n-    data = if_windows(\n-        [\"@xla//xla/pjrt/c/pjrt_c_api_gpu_plugin.pyd\"],\n-        [\"//jaxlib/tools:pjrt_c_api_gpu_plugin.so\"],\n-    ),\n+    data = [\":pjrt_c_api_gpu_plugin.so\"],\n     lib_rule = pytype_library,\n )\n \ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 4891fbeb3332..1be29326c95f 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -51,7 +51,7 @@ def _get_library_path():\n     runfiles_dir = os.getenv('RUNFILES_DIR', None)\n     if runfiles_dir:\n       local_path = os.path.join(\n-          runfiles_dir, '__main__/jaxlib/tools/pjrt_c_api_gpu_plugin.so'\n+          runfiles_dir, '__main__/jax_plugins/cuda/pjrt_c_api_gpu_plugin.so'\n       )\n \n   if os.path.exists(local_path):\ndiff --git a/jaxlib/tools/gpu_version_script.lds b/jax_plugins/cuda/gpu_version_script.lds\nsimilarity index 100%\nrename from jaxlib/tools/gpu_version_script.lds\nrename to jax_plugins/cuda/gpu_version_script.lds\ndiff --git a/jax_plugins/rocm/BUILD.bazel b/jax_plugins/rocm/BUILD.bazel\nindex 6e265bcd18cf..15e9e627830e 100644\n--- a/jax_plugins/rocm/BUILD.bazel\n+++ b/jax_plugins/rocm/BUILD.bazel\n@@ -34,14 +34,26 @@ exports_files([\n     \"setup.py\",\n ])\n \n+cc_binary(\n+    name = \"pjrt_c_api_gpu_plugin.so\",\n+    linkopts = [\n+        \"-Wl,--version-script,$(location :gpu_version_script.lds)\",\n+        \"-Wl,--no-undefined\",\n+    ],\n+    linkshared = True,\n+    deps = [\n+        \":gpu_version_script.lds\",\n+        \"@xla//xla/pjrt/c:pjrt_c_api_gpu\",\n+        \"@xla//xla/service:gpu_plugin\",\n+        \"@xla//xla/stream_executor:rocm_platform\",\n+    ],\n+)\n+\n py_library_providing_imports_info(\n     name = \"rocm_plugin\",\n     srcs = [\n         \"__init__.py\",\n     ],\n-    data = if_windows(\n-        [\"@xla//xla/pjrt/c/pjrt_c_api_gpu_plugin.pyd\"],\n-        [\"@xla//xla/pjrt/c:pjrt_c_api_gpu_plugin.so\"],\n-    ),\n+    data = [\":pjrt_c_api_gpu_plugin.so\"],\n     lib_rule = pytype_library,\n )\ndiff --git a/jax_plugins/rocm/__init__.py b/jax_plugins/rocm/__init__.py\nindex 0b1b077acfcd..cf2a625fa783 100644\n--- a/jax_plugins/rocm/__init__.py\n+++ b/jax_plugins/rocm/__init__.py\n@@ -51,7 +51,7 @@ def _get_library_path():\n     runfiles_dir = os.getenv('RUNFILES_DIR', None)\n     if runfiles_dir:\n       local_path = pathlib.Path(\n-          os.path.join(runfiles_dir, 'xla/xla/pjrt/c/pjrt_c_api_gpu_plugin.so')\n+          os.path.join(runfiles_dir, '__main__/jax_plugins/rocm/pjrt_c_api_gpu_plugin.so')\n       )\n \n   if local_path.exists():\ndiff --git a/jax_plugins/rocm/gpu_version_script.lds b/jax_plugins/rocm/gpu_version_script.lds\nnew file mode 100644\nindex 000000000000..cbac4549bde3\n--- /dev/null\n+++ b/jax_plugins/rocm/gpu_version_script.lds\n@@ -0,0 +1,9 @@\n+VERS_1.0 {\n+  global:\n+    extern \"C\" {\n+      GetPjrtApi;\n+    };\n+\n+  local:\n+    *;\n+};\ndiff --git a/jaxlib/tools/BUILD.bazel b/jaxlib/tools/BUILD.bazel\nindex 219096836ffc..22bae26a4420 100644\n--- a/jaxlib/tools/BUILD.bazel\n+++ b/jaxlib/tools/BUILD.bazel\n@@ -64,10 +64,10 @@ py_binary(\n         \"LICENSE.txt\",\n         \"//jaxlib\",\n         \"//jaxlib:README.md\",\n+        \"//jaxlib:_jax\",\n         \"//jaxlib:jaxlib_binaries\",\n         \"//jaxlib:setup.py\",\n         \"//jaxlib:xla_client.py\",\n-        \"//jaxlib:_jax\",\n         \"@xla//xla/ffi/api:api.h\",\n         \"@xla//xla/ffi/api:c_api.h\",\n         \"@xla//xla/ffi/api:ffi.h\",\n@@ -90,35 +90,15 @@ jax_py_test(\n     ],\n )\n \n-cc_binary(\n-    name = \"pjrt_c_api_gpu_plugin.so\",\n-    linkopts = [\n-        \"-Wl,--version-script,$(location :gpu_version_script.lds)\",\n-        \"-Wl,--no-undefined\",\n-    ],\n-    linkshared = True,\n-    deps = [\n-        \":gpu_version_script.lds\",\n-        \"@xla//xla/pjrt/c:pjrt_c_api_gpu\",\n-        \"@xla//xla/pjrt/c:pjrt_c_api_gpu_version_script.lds\",\n-        \"@xla//xla/service:gpu_plugin\",\n-    ] + if_cuda([\n-        \"//jaxlib/mosaic/gpu:custom_call\",\n-        \"@xla//xla/stream_executor:cuda_platform\",\n-    ]) + if_rocm([\n-        \"@xla//xla/stream_executor:rocm_platform\",\n-    ]),\n-)\n-\n py_binary(\n     name = \"build_gpu_plugin_wheel\",\n     srcs = [\"build_gpu_plugin_wheel.py\"],\n     data = [\n         \"LICENSE.txt\",\n-        \":pjrt_c_api_gpu_plugin.so\",\n     ] + if_cuda([\n         \"//jaxlib:version\",\n         \"//jaxlib/cuda:cuda_gpu_support\",\n+        \"//jax_plugins/cuda:pjrt_c_api_gpu_plugin.so\",\n         \"//jax_plugins/cuda:pyproject.toml\",\n         \"//jax_plugins/cuda:setup.py\",\n         \"//jax_plugins/cuda:__init__.py\",\n@@ -126,6 +106,7 @@ py_binary(\n     ]) + if_rocm([\n         \"//jaxlib:version\",\n         \"//jaxlib/rocm:rocm_gpu_support\",\n+        \"//jax_plugins/rocm:pjrt_c_api_gpu_plugin.so\",\n         \"//jax_plugins/rocm:pyproject.toml\",\n         \"//jax_plugins/rocm:setup.py\",\n         \"//jax_plugins/rocm:__init__.py\",\n@@ -387,10 +368,6 @@ jax_wheel(\n )\n \n # JAX PJRT wheel targets.\n-pytype_strict_library(\n-    name = \"pjrt_c_api_gpu_plugin_so\",\n-    data = [\":pjrt_c_api_gpu_plugin.so\"],\n-)\n \n py_binary(\n     name = \"build_gpu_plugin_wheel_tool\",\n@@ -407,12 +384,12 @@ py_binary(\n \n wheel_sources(\n     name = \"jax_pjrt_sources\",\n-    data_srcs = [\n-        \":pjrt_c_api_gpu_plugin_so\",\n-    ] + if_cuda([\n+    data_srcs = if_cuda([\n+        \"//jax_plugins/cuda:cuda_plugin\",\n         \"//jaxlib/cuda:cuda_gpu_support\",\n         \"@local_config_cuda//cuda:cuda-nvvm\",\n     ]) + if_rocm([\n+        \"//jax_plugins/rocm:rocm_plugin\",\n         \"//jaxlib/rocm:rocm_gpu_support\",\n     ]),\n     py_srcs = [\ndiff --git a/jaxlib/tools/build_gpu_plugin_wheel.py b/jaxlib/tools/build_gpu_plugin_wheel.py\nindex 337bedab4591..68e08d89338e 100644\n--- a/jaxlib/tools/build_gpu_plugin_wheel.py\n+++ b/jaxlib/tools/build_gpu_plugin_wheel.py\n@@ -120,7 +120,7 @@ def prepare_cuda_plugin_wheel(\n       ],\n   )\n   copy_files(\n-      f\"{source_file_prefix}jaxlib/tools/pjrt_c_api_gpu_plugin.so\",\n+      f\"{source_file_prefix}jax_plugins/cuda/pjrt_c_api_gpu_plugin.so\",\n       dst_dir=plugin_dir,\n       dst_filename=\"xla_cuda_plugin.so\",\n   )\n@@ -158,7 +158,7 @@ def prepare_rocm_plugin_wheel(\n       ],\n   )\n   copy_files(\n-      f\"{source_file_prefix}jaxlib/tools/pjrt_c_api_gpu_plugin.so\",\n+      f\"{source_file_prefix}jax_plugins/rocm/pjrt_c_api_gpu_plugin.so\",\n       dst_dir=plugin_dir,\n       dst_filename=\"xla_rocm_plugin.so\",\n   )\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add a linearization rule for scan.",
        "output": [
            "```diff\nCommit: f0e00a6658709067951446868894ea300b365c8b\nDate: 2025-05-14T19:33:09Z\nURL: https://github.com/jax-ml/jax/commit/f0e00a6658709067951446868894ea300b365c8b\nFiles changed: 2\nAdditions: +116, Deletions: -0\ndiff --git a/jax/_src/lax/control_flow/loops.py b/jax/_src/lax/control_flow/loops.py\nindex 05e9c010dc51..c85a23b6b199 100644\n--- a/jax/_src/lax/control_flow/loops.py\n+++ b/jax/_src/lax/control_flow/loops.py\n@@ -683,6 +683,108 @@ def _scan_jvp(primals, tangents, reverse, length, jaxpr, num_consts, num_carry,\n                   for p, nz in zip(primals_out, nonzeros_out)]\n   return primals_out, tangents_out\n \n+def _scan_linearization(nzs, *primals_in, reverse: bool, length: int,\n+                        num_consts: int, num_carry: int,\n+                        jaxpr: core.ClosedJaxpr, linear: Sequence[bool],\n+                        unroll: int, _split_transpose: bool):\n+  const_nz, init_nz, xs_nz = split_list(nzs, [num_consts, num_carry])\n+  carry_nz = init_nz\n+  for _ in range(1 + num_carry):\n+    nzs = const_nz + carry_nz + xs_nz\n+    primal_jaxpr, num_res, nzs_out, tangent_jaxpr = ad.linearize_jaxpr(jaxpr, nzs)\n+    carry_nz_out = nzs_out[:num_carry]\n+    if carry_nz_out == carry_nz:\n+      break\n+    else:\n+      carry_nz = _map(operator.or_, carry_nz, carry_nz_out)\n+  else:\n+    assert False, \"Fixpoint not reached\"\n+\n+  # The linearize_jaxpr function produces primal_jaxpr with num_res residuals\n+  # output at the front, and tangent_jaxpr with num_res residuals input at the\n+  # back. We could move all the residuals to the back and treat them as\n+  # extensive outputs, but this would be wasteful for residuals that are\n+  # loop invariant, or forwarded extensive inputs.\n+\n+  # First, for residuals that are forwarded constants, we move those to the\n+  # front in the tangent_jaxpr to treat them as intensive inputs.\n+  in_fwd = pe._jaxpr_forwarding(primal_jaxpr.jaxpr)\n+  primal_jaxpr, tangent_jaxpr, intensive_res, in_fwd = _const_to_intensive_res_forwarding(\n+      primal_jaxpr, tangent_jaxpr, num_res, num_consts, primals_in, in_fwd)\n+  num_intensive_res = len(intensive_res)\n+  num_res -= num_intensive_res\n+\n+  # After pruning the intensive residuals, the rest get moved to the back and\n+  # handled as extensive outputs from the primal.\n+  num_out = len(nzs_out)\n+  primal_jaxpr = pe.move_outvars_to_back(\n+      primal_jaxpr, [True] * num_res + [False] * num_out)\n+  in_fwd = in_fwd[num_res:] + in_fwd[:num_res]\n+\n+  # Then, any residuals or other extensive outputs that are forwarded extensive\n+  # inputs, we remove them from the primal jaxpr, and manually forward them.\n+  in_fwd = [in_idx if out_idx >= num_carry and in_idx is not None and\n+            in_idx >= num_consts + num_carry else None\n+            for out_idx, in_idx in enumerate(in_fwd)]\n+  primal_jaxpr = pe.prune_closed_jaxpr_outputs(primal_jaxpr,\n+                                               [i is None for i in in_fwd])\n+\n+  out = scan_p.bind(*primals_in, jaxpr=primal_jaxpr, reverse=reverse,\n+                    length=length, num_consts=num_consts, num_carry=num_carry,\n+                    linear=linear, unroll=unroll, _split_transpose=_split_transpose)\n+  out_ = iter(out)\n+  all_out = [next(out_) if f is None else _maybe_put(primals_in[f]) for f in in_fwd]\n+  assert next(out_, None) is None\n+  primals_out, extensive_res = split_list(all_out, [len(all_out) - num_res])\n+  res = [*intensive_res, *extensive_res]\n+\n+  def tangent_fun(res, *tangents):\n+    intensive_res, extensive_res = split_list(res, [num_intensive_res])\n+    nz_tangents = [ad.instantiate_zeros(x) for nz, x in zip(nzs, tangents) if nz]\n+    tangent_linear = (\n+        (False,) * len(intensive_res) +\n+        (True,) * len(nz_tangents) +\n+        (False,) * len(extensive_res)\n+    )\n+    tangent_num_consts = len(intensive_res) + sum(nzs[:num_consts])\n+    tangent_num_carry = sum(nzs[num_consts:num_consts + num_carry])\n+    nz_tangents_out = scan_p.bind(*intensive_res, *nz_tangents, *extensive_res,\n+                                  jaxpr=tangent_jaxpr,\n+                                  reverse=reverse, length=length,\n+                                  num_consts=tangent_num_consts,\n+                                  num_carry=tangent_num_carry,\n+                                  linear=tangent_linear, unroll=unroll,\n+                                  _split_transpose=_split_transpose)\n+    tangent_avals_out = [v.aval.to_tangent_aval() for v in jaxpr.jaxpr.outvars]\n+    nz_tangents_out_ = iter(nz_tangents_out)\n+    tangents_out = [next(nz_tangents_out_) if nz else ad.Zero(aval)\n+                    for aval, nz in zip(tangent_avals_out, nzs_out)]\n+    assert next(nz_tangents_out_, None) is None\n+    return tangents_out\n+\n+  return primals_out, nzs_out, res, tangent_fun\n+\n+def _const_to_intensive_res_forwarding(\n+    primal_jaxpr: core.ClosedJaxpr,\n+    tangent_jaxpr: core.ClosedJaxpr,\n+    num_res: int,\n+    num_consts: int,\n+    primals_in: Sequence[Any],\n+    in_fwd: list[int | None]\n+) -> tuple[core.ClosedJaxpr, core.ClosedJaxpr, list[Any], list[int | None]]:\n+  const_to_res = [in_idx if in_idx is not None and in_idx < num_consts else None\n+                  for in_idx in in_fwd[:num_res]]\n+  new_in_fwd = [f for c, f in zip(const_to_res, in_fwd[:num_res]) if c is None]\n+  new_in_fwd += in_fwd[num_res:]\n+  intensive_res = [primals_in[f] for f in const_to_res if f is not None]\n+  num_out = len(primal_jaxpr.out_avals) - num_res\n+  primal_jaxpr = pe.prune_closed_jaxpr_outputs(\n+      primal_jaxpr, [i is None for i in const_to_res] + [True] * num_out)\n+  num_nz = len(tangent_jaxpr.in_avals) - num_res\n+  tangent_jaxpr = pe.move_binders_to_front(\n+      tangent_jaxpr, [False] * num_nz + [i is not None for i in const_to_res])\n+  return primal_jaxpr, tangent_jaxpr, intensive_res, new_in_fwd\n+\n def _scan_partial_eval(trace, *tracers, reverse: bool,\n                        length: int, num_consts: int, num_carry: int,\n                        jaxpr: core.ClosedJaxpr, linear: Sequence[bool],\n@@ -1385,6 +1487,7 @@ def arrange_jaxpr_args_for_wrapped(args):\n scan_p.def_effectful_abstract_eval(_scan_abstract_eval)\n ad.primitive_jvps[scan_p] = _scan_jvp\n ad.primitive_transposes[scan_p] = _scan_transpose\n+ad.primitive_linearizations[scan_p] = _scan_linearization\n pe.custom_partial_eval_rules[scan_p] = _scan_partial_eval\n xla.register_initial_style_primitive(scan_p)\n mlir.register_lowering(scan_p,\ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex d32d761ee1fa..54dff47fea32 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -28,6 +28,7 @@\n \n import jax\n from jax._src import core\n+from jax._src import config\n from jax import dtypes\n from jax import lax\n from jax import random\n@@ -3298,6 +3299,18 @@ def body_fun(c, _):\n     outs_ref = body_fun(body_fun(init_vals, [x[0] for x in xs])[0], [x[1] for x in xs])[0]\n     self.assertAllClose(outs, outs_ref, check_dtypes=False)\n \n+  def test_scan_diff_of_print(self):\n+    # ref: https://github.com/jax-ml/jax/issues/28738\n+    def f(c, _):\n+      jax.debug.print(\"c = {c}\", c=c, ordered=True)\n+      return c + 1, None\n+    def g(x):\n+      return jax.lax.scan(f, x, length=2)[0]\n+    with config.use_direct_linearize(True):\n+      jaxpr = jax.make_jaxpr(jax.value_and_grad(g))(1.0)\n+    eqn_jaxpr = jaxpr.eqns[0].params[\"jaxpr\"]\n+    self.assertIn(\"debug_callback\", [e.primitive.name for e in eqn_jaxpr.eqns])\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Move definition of pjrt_c_api_gpu_plugin.so into jax_plugins/{cuda,rocm}\n\nThis is simpler and should work better for a bazel submodule.",
        "output": [
            "```diff\nCommit: 183425cf05da44cb5496db3046409af812747e5d\nDate: 2025-05-14T19:30:52Z\nURL: https://github.com/jax-ml/jax/commit/183425cf05da44cb5496db3046409af812747e5d\nFiles changed: 8\nAdditions: +52, Deletions: -41\ndiff --git a/jax_plugins/cuda/BUILD.bazel b/jax_plugins/cuda/BUILD.bazel\nindex 6566cfc62b0c..c3c20f536cff 100644\n--- a/jax_plugins/cuda/BUILD.bazel\n+++ b/jax_plugins/cuda/BUILD.bazel\n@@ -34,15 +34,28 @@ exports_files([\n     \"setup.py\",\n ])\n \n+cc_binary(\n+    name = \"pjrt_c_api_gpu_plugin.so\",\n+    linkopts = [\n+        \"-Wl,--version-script,$(location :gpu_version_script.lds)\",\n+        \"-Wl,--no-undefined\",\n+    ],\n+    linkshared = True,\n+    deps = [\n+        \":gpu_version_script.lds\",\n+        \"//jaxlib/mosaic/gpu:custom_call\",\n+        \"@xla//xla/pjrt/c:pjrt_c_api_gpu\",\n+        \"@xla//xla/service:gpu_plugin\",\n+        \"@xla//xla/stream_executor:cuda_platform\",\n+    ],\n+)\n+\n py_library_providing_imports_info(\n     name = \"cuda_plugin\",\n     srcs = [\n         \"__init__.py\",\n     ],\n-    data = if_windows(\n-        [\"@xla//xla/pjrt/c/pjrt_c_api_gpu_plugin.pyd\"],\n-        [\"//jaxlib/tools:pjrt_c_api_gpu_plugin.so\"],\n-    ),\n+    data = [\":pjrt_c_api_gpu_plugin.so\"],\n     lib_rule = pytype_library,\n )\n \ndiff --git a/jax_plugins/cuda/__init__.py b/jax_plugins/cuda/__init__.py\nindex 4891fbeb3332..1be29326c95f 100644\n--- a/jax_plugins/cuda/__init__.py\n+++ b/jax_plugins/cuda/__init__.py\n@@ -51,7 +51,7 @@ def _get_library_path():\n     runfiles_dir = os.getenv('RUNFILES_DIR', None)\n     if runfiles_dir:\n       local_path = os.path.join(\n-          runfiles_dir, '__main__/jaxlib/tools/pjrt_c_api_gpu_plugin.so'\n+          runfiles_dir, '__main__/jax_plugins/cuda/pjrt_c_api_gpu_plugin.so'\n       )\n \n   if os.path.exists(local_path):\ndiff --git a/jaxlib/tools/gpu_version_script.lds b/jax_plugins/cuda/gpu_version_script.lds\nsimilarity index 100%\nrename from jaxlib/tools/gpu_version_script.lds\nrename to jax_plugins/cuda/gpu_version_script.lds\ndiff --git a/jax_plugins/rocm/BUILD.bazel b/jax_plugins/rocm/BUILD.bazel\nindex 6e265bcd18cf..15e9e627830e 100644\n--- a/jax_plugins/rocm/BUILD.bazel\n+++ b/jax_plugins/rocm/BUILD.bazel\n@@ -34,14 +34,26 @@ exports_files([\n     \"setup.py\",\n ])\n \n+cc_binary(\n+    name = \"pjrt_c_api_gpu_plugin.so\",\n+    linkopts = [\n+        \"-Wl,--version-script,$(location :gpu_version_script.lds)\",\n+        \"-Wl,--no-undefined\",\n+    ],\n+    linkshared = True,\n+    deps = [\n+        \":gpu_version_script.lds\",\n+        \"@xla//xla/pjrt/c:pjrt_c_api_gpu\",\n+        \"@xla//xla/service:gpu_plugin\",\n+        \"@xla//xla/stream_executor:rocm_platform\",\n+    ],\n+)\n+\n py_library_providing_imports_info(\n     name = \"rocm_plugin\",\n     srcs = [\n         \"__init__.py\",\n     ],\n-    data = if_windows(\n-        [\"@xla//xla/pjrt/c/pjrt_c_api_gpu_plugin.pyd\"],\n-        [\"@xla//xla/pjrt/c:pjrt_c_api_gpu_plugin.so\"],\n-    ),\n+    data = [\":pjrt_c_api_gpu_plugin.so\"],\n     lib_rule = pytype_library,\n )\ndiff --git a/jax_plugins/rocm/__init__.py b/jax_plugins/rocm/__init__.py\nindex 0b1b077acfcd..cf2a625fa783 100644\n--- a/jax_plugins/rocm/__init__.py\n+++ b/jax_plugins/rocm/__init__.py\n@@ -51,7 +51,7 @@ def _get_library_path():\n     runfiles_dir = os.getenv('RUNFILES_DIR', None)\n     if runfiles_dir:\n       local_path = pathlib.Path(\n-          os.path.join(runfiles_dir, 'xla/xla/pjrt/c/pjrt_c_api_gpu_plugin.so')\n+          os.path.join(runfiles_dir, '__main__/jax_plugins/rocm/pjrt_c_api_gpu_plugin.so')\n       )\n \n   if local_path.exists():\ndiff --git a/jax_plugins/rocm/gpu_version_script.lds b/jax_plugins/rocm/gpu_version_script.lds\nnew file mode 100644\nindex 000000000000..cbac4549bde3\n--- /dev/null\n+++ b/jax_plugins/rocm/gpu_version_script.lds\n@@ -0,0 +1,9 @@\n+VERS_1.0 {\n+  global:\n+    extern \"C\" {\n+      GetPjrtApi;\n+    };\n+\n+  local:\n+    *;\n+};\ndiff --git a/jaxlib/tools/BUILD.bazel b/jaxlib/tools/BUILD.bazel\nindex 219096836ffc..22bae26a4420 100644\n--- a/jaxlib/tools/BUILD.bazel\n+++ b/jaxlib/tools/BUILD.bazel\n@@ -64,10 +64,10 @@ py_binary(\n         \"LICENSE.txt\",\n         \"//jaxlib\",\n         \"//jaxlib:README.md\",\n+        \"//jaxlib:_jax\",\n         \"//jaxlib:jaxlib_binaries\",\n         \"//jaxlib:setup.py\",\n         \"//jaxlib:xla_client.py\",\n-        \"//jaxlib:_jax\",\n         \"@xla//xla/ffi/api:api.h\",\n         \"@xla//xla/ffi/api:c_api.h\",\n         \"@xla//xla/ffi/api:ffi.h\",\n@@ -90,35 +90,15 @@ jax_py_test(\n     ],\n )\n \n-cc_binary(\n-    name = \"pjrt_c_api_gpu_plugin.so\",\n-    linkopts = [\n-        \"-Wl,--version-script,$(location :gpu_version_script.lds)\",\n-        \"-Wl,--no-undefined\",\n-    ],\n-    linkshared = True,\n-    deps = [\n-        \":gpu_version_script.lds\",\n-        \"@xla//xla/pjrt/c:pjrt_c_api_gpu\",\n-        \"@xla//xla/pjrt/c:pjrt_c_api_gpu_version_script.lds\",\n-        \"@xla//xla/service:gpu_plugin\",\n-    ] + if_cuda([\n-        \"//jaxlib/mosaic/gpu:custom_call\",\n-        \"@xla//xla/stream_executor:cuda_platform\",\n-    ]) + if_rocm([\n-        \"@xla//xla/stream_executor:rocm_platform\",\n-    ]),\n-)\n-\n py_binary(\n     name = \"build_gpu_plugin_wheel\",\n     srcs = [\"build_gpu_plugin_wheel.py\"],\n     data = [\n         \"LICENSE.txt\",\n-        \":pjrt_c_api_gpu_plugin.so\",\n     ] + if_cuda([\n         \"//jaxlib:version\",\n         \"//jaxlib/cuda:cuda_gpu_support\",\n+        \"//jax_plugins/cuda:pjrt_c_api_gpu_plugin.so\",\n         \"//jax_plugins/cuda:pyproject.toml\",\n         \"//jax_plugins/cuda:setup.py\",\n         \"//jax_plugins/cuda:__init__.py\",\n@@ -126,6 +106,7 @@ py_binary(\n     ]) + if_rocm([\n         \"//jaxlib:version\",\n         \"//jaxlib/rocm:rocm_gpu_support\",\n+        \"//jax_plugins/rocm:pjrt_c_api_gpu_plugin.so\",\n         \"//jax_plugins/rocm:pyproject.toml\",\n         \"//jax_plugins/rocm:setup.py\",\n         \"//jax_plugins/rocm:__init__.py\",\n@@ -387,10 +368,6 @@ jax_wheel(\n )\n \n # JAX PJRT wheel targets.\n-pytype_strict_library(\n-    name = \"pjrt_c_api_gpu_plugin_so\",\n-    data = [\":pjrt_c_api_gpu_plugin.so\"],\n-)\n \n py_binary(\n     name = \"build_gpu_plugin_wheel_tool\",\n@@ -407,12 +384,12 @@ py_binary(\n \n wheel_sources(\n     name = \"jax_pjrt_sources\",\n-    data_srcs = [\n-        \":pjrt_c_api_gpu_plugin_so\",\n-    ] + if_cuda([\n+    data_srcs = if_cuda([\n+        \"//jax_plugins/cuda:cuda_plugin\",\n         \"//jaxlib/cuda:cuda_gpu_support\",\n         \"@local_config_cuda//cuda:cuda-nvvm\",\n     ]) + if_rocm([\n+        \"//jax_plugins/rocm:rocm_plugin\",\n         \"//jaxlib/rocm:rocm_gpu_support\",\n     ]),\n     py_srcs = [\ndiff --git a/jaxlib/tools/build_gpu_plugin_wheel.py b/jaxlib/tools/build_gpu_plugin_wheel.py\nindex 337bedab4591..68e08d89338e 100644\n--- a/jaxlib/tools/build_gpu_plugin_wheel.py\n+++ b/jaxlib/tools/build_gpu_plugin_wheel.py\n@@ -120,7 +120,7 @@ def prepare_cuda_plugin_wheel(\n       ],\n   )\n   copy_files(\n-      f\"{source_file_prefix}jaxlib/tools/pjrt_c_api_gpu_plugin.so\",\n+      f\"{source_file_prefix}jax_plugins/cuda/pjrt_c_api_gpu_plugin.so\",\n       dst_dir=plugin_dir,\n       dst_filename=\"xla_cuda_plugin.so\",\n   )\n@@ -158,7 +158,7 @@ def prepare_rocm_plugin_wheel(\n       ],\n   )\n   copy_files(\n-      f\"{source_file_prefix}jaxlib/tools/pjrt_c_api_gpu_plugin.so\",\n+      f\"{source_file_prefix}jax_plugins/rocm/pjrt_c_api_gpu_plugin.so\",\n       dst_dir=plugin_dir,\n       dst_filename=\"xla_rocm_plugin.so\",\n   )\n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Merge pull request #28747 from jakevdp:canon-dtype-jax-array\n\nPiperOrigin-RevId: 758785125",
        "output": [
            "```diff\nCommit: 5d628e0fd38d2064c9c476da8ab72fa3e24bf15f\nDate: 2025-05-14T19:20:31Z\nURL: https://github.com/jax-ml/jax/commit/5d628e0fd38d2064c9c476da8ab72fa3e24bf15f\nFiles changed: 2\nAdditions: +8, Deletions: -0\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 5f1cf1670729..6afcdf2892f4 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -1180,6 +1180,7 @@ pytype_strict_library(\n         \":abstract_arrays\",\n         \":config\",\n         \":core\",\n+        \":deprecations\",\n         \":dtypes\",\n         \":sharding_impls\",\n         \":source_info_util\",\ndiff --git a/jax/_src/interpreters/xla.py b/jax/_src/interpreters/xla.py\nindex 7fbb22923e0f..73a57f935f5d 100644\n--- a/jax/_src/interpreters/xla.py\n+++ b/jax/_src/interpreters/xla.py\n@@ -23,6 +23,7 @@\n import numpy as np\n \n from jax._src import core\n+from jax._src import deprecations\n from jax._src import dtypes\n from jax._src.abstract_arrays import numpy_scalar_types\n from jax._src.util import safe_zip, safe_map\n@@ -100,6 +101,12 @@ def canonicalize_dtype(x):\n     handler = canonicalize_dtype_handlers.get(typ)\n     if handler: return handler(x)\n   if hasattr(x, '__jax_array__'):\n+    deprecations.warn(\n+      'jax-abstract-dunder-array',\n+      ('Triggering of __jax_array__() during abstractification is deprecated.'\n+       ' To avoid this error, either explicitly convert your object using'\n+       ' jax.numpy.array(), or register your object as a pytree.'),\n+      stacklevel=6)\n     return canonicalize_dtype(x.__jax_array__())\n   raise InvalidInputException(\n       f\"Argument '{x}' of type {type(x)} is not a valid JAX type.\")\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28750 from hawkinsp:requ\n\nPiperOrigin-RevId: 758780857",
        "output": [
            "```diff\nCommit: c8b920bab4465cb148276a49839e8a30b6aabc92\nDate: 2025-05-14T19:09:29Z\nURL: https://github.com/jax-ml/jax/commit/c8b920bab4465cb148276a49839e8a30b6aabc92\nFiles changed: 2\nAdditions: +8, Deletions: -0\ndiff --git a/WORKSPACE b/WORKSPACE\nindex 903085714e65..f389afe2263f 100644\n--- a/WORKSPACE\n+++ b/WORKSPACE\n@@ -14,6 +14,8 @@ python_init_repositories(\n     default_python_version = \"system\",\n     local_wheel_dist_folder = \"../dist\",\n     local_wheel_inclusion_list = [\n+        \"ml_dtypes*\",\n+        \"ml-dtypes*\",\n         \"numpy*\",\n         \"scipy*\",\n         \"jax-*\",\ndiff --git a/build/requirements_lock_3_14_ft.txt b/build/requirements_lock_3_14_ft.txt\nindex e50305f4fa48..6eedf149f5fa 100644\n--- a/build/requirements_lock_3_14_ft.txt\n+++ b/build/requirements_lock_3_14_ft.txt\n@@ -19,3 +19,9 @@ flatbuffers==24.12.23\n ml-dtypes==0.5.1\n \n opt-einsum==3.4.0\n+\n+build==1.2.2.post1\n+setuptools==80.0.0\n+wheel==0.45.1\n+pyproject-hooks==1.2.0\n+packaging==25.0\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add requirements needed for building wheels under Python 3.14t.\n\nAlso allow ml_dtypes as a local wheel override.",
        "output": [
            "```diff\nCommit: e968cb6b04d31ac7a045f119de027ee17fc16e1e\nDate: 2025-05-14T18:36:57Z\nURL: https://github.com/jax-ml/jax/commit/e968cb6b04d31ac7a045f119de027ee17fc16e1e\nFiles changed: 2\nAdditions: +8, Deletions: -0\ndiff --git a/WORKSPACE b/WORKSPACE\nindex 903085714e65..f389afe2263f 100644\n--- a/WORKSPACE\n+++ b/WORKSPACE\n@@ -14,6 +14,8 @@ python_init_repositories(\n     default_python_version = \"system\",\n     local_wheel_dist_folder = \"../dist\",\n     local_wheel_inclusion_list = [\n+        \"ml_dtypes*\",\n+        \"ml-dtypes*\",\n         \"numpy*\",\n         \"scipy*\",\n         \"jax-*\",\ndiff --git a/build/requirements_lock_3_14_ft.txt b/build/requirements_lock_3_14_ft.txt\nindex e50305f4fa48..6eedf149f5fa 100644\n--- a/build/requirements_lock_3_14_ft.txt\n+++ b/build/requirements_lock_3_14_ft.txt\n@@ -19,3 +19,9 @@ flatbuffers==24.12.23\n ml-dtypes==0.5.1\n \n opt-einsum==3.4.0\n+\n+build==1.2.2.post1\n+setuptools==80.0.0\n+wheel==0.45.1\n+pyproject-hooks==1.2.0\n+packaging==25.0\n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Warn when __jax_array__ is seen in xla dtype canonicalization",
        "output": [
            "```diff\nCommit: a5361315fd874cf4d5e278e20a9ab2de521d8df4\nDate: 2025-05-14T18:08:48Z\nURL: https://github.com/jax-ml/jax/commit/a5361315fd874cf4d5e278e20a9ab2de521d8df4\nFiles changed: 2\nAdditions: +8, Deletions: -0\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 5f1cf1670729..6afcdf2892f4 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -1180,6 +1180,7 @@ pytype_strict_library(\n         \":abstract_arrays\",\n         \":config\",\n         \":core\",\n+        \":deprecations\",\n         \":dtypes\",\n         \":sharding_impls\",\n         \":source_info_util\",\ndiff --git a/jax/_src/interpreters/xla.py b/jax/_src/interpreters/xla.py\nindex 7fbb22923e0f..73a57f935f5d 100644\n--- a/jax/_src/interpreters/xla.py\n+++ b/jax/_src/interpreters/xla.py\n@@ -23,6 +23,7 @@\n import numpy as np\n \n from jax._src import core\n+from jax._src import deprecations\n from jax._src import dtypes\n from jax._src.abstract_arrays import numpy_scalar_types\n from jax._src.util import safe_zip, safe_map\n@@ -100,6 +101,12 @@ def canonicalize_dtype(x):\n     handler = canonicalize_dtype_handlers.get(typ)\n     if handler: return handler(x)\n   if hasattr(x, '__jax_array__'):\n+    deprecations.warn(\n+      'jax-abstract-dunder-array',\n+      ('Triggering of __jax_array__() during abstractification is deprecated.'\n+       ' To avoid this error, either explicitly convert your object using'\n+       ' jax.numpy.array(), or register your object as a pytree.'),\n+      stacklevel=6)\n     return canonicalize_dtype(x.__jax_array__())\n   raise InvalidInputException(\n       f\"Argument '{x}' of type {type(x)} is not a valid JAX type.\")\n\n```"
        ],
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Partial in `repeats` if `total_repeat_length is None` since repeats is expected to be a constant.\n\nThis fixes an error in explicit sharding mode where we were converting repeats to a Tracer instead of it being a concrete value.\n\nPiperOrigin-RevId: 758743044",
        "output": [
            "```diff\nCommit: 5abc510b434884639b9c3f48ba379172d443dba6\nDate: 2025-05-14T17:36:30Z\nURL: https://github.com/jax-ml/jax/commit/5abc510b434884639b9c3f48ba379172d443dba6\nFiles changed: 2\nAdditions: +22, Deletions: -8\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex 266aad4954ba..0bd287dadd51 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -6698,9 +6698,8 @@ def repeat(a: ArrayLike, repeats: ArrayLike, axis: int | None = None, *,\n            [3, 3, 4, 4, 4, 4, 4]], dtype=int32)\n   \"\"\"\n   if out_sharding is not None:\n-    return auto_axes(\n-        partial(_repeat, axis=axis, total_repeat_length=total_repeat_length),\n-        out_sharding=out_sharding)(a, repeats)\n+    return _auto_repeat(_repeat, a, repeats, axis, total_repeat_length,\n+                        out_sharding)\n   ctx_mesh = get_abstract_mesh()\n   if ctx_mesh._are_all_axes_explicit:\n     aval = core.typeof(a)\n@@ -6710,17 +6709,26 @@ def repeat(a: ArrayLike, repeats: ArrayLike, axis: int | None = None, *,\n     assert axis is not None and aval.sharding.spec[axis] is None\n     out_sharding = (NamedSharding(ctx_mesh, P())\n                     if aval.sharding.mesh.empty else aval.sharding)\n-    return auto_axes(\n-        partial(_repeat, axis=axis, total_repeat_length=total_repeat_length),\n-        out_sharding=out_sharding)(a, repeats)\n+    return _auto_repeat(_repeat, a, repeats, axis, total_repeat_length,\n+                        out_sharding)\n   try:\n-    return _repeat(a, repeats, axis=axis,\n+    return _repeat(a, repeats=repeats, axis=axis,\n                    total_repeat_length=total_repeat_length)\n   except core.ShardingTypeError as e:\n     raise ValueError(\n         \"Please pass sharding to `jnp.repeat` via `out_sharding` parameter.\")\n \n-def _repeat(a: ArrayLike, repeats: ArrayLike, *, axis: int | None = None,\n+def _auto_repeat(fun, a, repeats, axis, total_repeat_length, out_sharding):\n+  if total_repeat_length is None:\n+    return auto_axes(partial(fun, repeats=repeats, axis=axis,\n+                             total_repeat_length=total_repeat_length),\n+                     out_sharding=out_sharding)(a)\n+  else:\n+    return auto_axes(\n+        partial(fun, axis=axis, total_repeat_length=total_repeat_length),\n+        out_sharding=out_sharding)(a, repeats=repeats)\n+\n+def _repeat(a: ArrayLike, *, repeats: ArrayLike, axis: int | None = None,\n             total_repeat_length: int | None = None) -> Array:\n   if core.is_dim(repeats):\n     util.check_arraylike(\"repeat\", a)\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex e374b0b15a7b..d1c8ec7f050d 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7617,6 +7617,12 @@ def test_jnp_repeat(self, mesh):\n     out = jnp.repeat(a, np.array((2,2,2,2)) - 1, axis=0, out_sharding=P('x'))\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('x', None)))\n \n+    a = jax.device_put(jnp.eye(16).reshape(16, 16), P('x'))\n+    @jax.jit\n+    def f(x):\n+      return jnp.repeat(x, 3, axis=-1)\n+    f(a)\n+\n   @jtu.with_explicit_mesh((2,), ('x',))\n   def test_scatter_gather(self, mesh):\n     x = np.random.uniform(size=(mesh.size * 2, 3))\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28732 from vfdev-5:remove-tsan-suppresions\n\nPiperOrigin-RevId: 758726514",
        "output": [
            "```diff\nCommit: bf0f6eee628a9a3912112c9835ce34ff504a58f5\nDate: 2025-05-14T17:02:20Z\nURL: https://github.com/jax-ml/jax/commit/bf0f6eee628a9a3912112c9835ce34ff504a58f5\nFiles changed: 2\nAdditions: +0, Deletions: -8\ndiff --git a/.github/workflows/tsan-suppressions_3.13.txt b/.github/workflows/tsan-suppressions_3.13.txt\nindex e82699036e92..aec94dfef004 100644\n--- a/.github/workflows/tsan-suppressions_3.13.txt\n+++ b/.github/workflows/tsan-suppressions_3.13.txt\n@@ -40,7 +40,3 @@ race:gemm_oncopy\n # https://github.com/python/cpython/issues/132245\n race:split_keys_entry_added\n race_top:dict_dict_merge\n-\n-# https://github.com/python/cpython/issues/132013\n-# Fixed on 3.14 and not backported to 3.13\n-race_top:frozenset_hash\n\\ No newline at end of file\ndiff --git a/.github/workflows/tsan-suppressions_3.14.txt b/.github/workflows/tsan-suppressions_3.14.txt\nindex ec4d81c987d0..ec5102502a2b 100644\n--- a/.github/workflows/tsan-suppressions_3.14.txt\n+++ b/.github/workflows/tsan-suppressions_3.14.txt\n@@ -18,7 +18,3 @@ race:dscal_k_\n race:scal_k_\n race:gemm_beta\n race:gemm_oncopy\n-\n-# https://github.com/python/cpython/issues/132214\n-# Should be fixed\n-# race_top:update_one_slot\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28635 from ROCm:skip-csr-mat-tests-rocm63\n\nPiperOrigin-RevId: 758726482",
        "output": [
            "```diff\nCommit: d1a1a5530cd0fa22311da905748592495551482c\nDate: 2025-05-14T16:58:00Z\nURL: https://github.com/jax-ml/jax/commit/d1a1a5530cd0fa22311da905748592495551482c\nFiles changed: 1\nAdditions: +27, Deletions: -0\ndiff --git a/tests/sparse_test.py b/tests/sparse_test.py\nindex 71437fd0e028..97a156f9f6f5 100644\n--- a/tests/sparse_test.py\n+++ b/tests/sparse_test.py\n@@ -16,6 +16,8 @@\n from functools import partial\n import itertools\n import math\n+import os\n+from pathlib import Path\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -42,6 +44,15 @@\n import numpy as np\n import scipy.sparse\n \n+def get_rocm_version():\n+  rocm_path = os.environ.get(\"ROCM_PATH\", \"/opt/rocm\")\n+  version_path = Path(rocm_path) / \".info\" / \"version\"\n+  if not version_path.exists():\n+    raise FileNotFoundError(f\"Expected ROCm version file at {version_path}\")\n+  version_str = version_path.read_text().strip()\n+  major, minor, *_ = version_str.split(\".\")\n+  return int(major), int(minor)\n+\n jax.config.parse_flags_with_absl()\n \n all_dtypes = jtu.dtypes.integer + jtu.dtypes.floating + jtu.dtypes.complex\n@@ -208,6 +219,14 @@ def test_csr_fromdense(self, shape, dtype):\n     transpose=[True, False],\n   )\n   def test_csr_matvec(self, shape, dtype, transpose):\n+    if (\n+        jtu.is_device_rocm() and\n+        get_rocm_version() < (6, 4) and\n+        dtype in (jtu.dtypes.floating + jtu.dtypes.complex)\n+    ):\n+      # TODO: Remove this check when ROCm 6.4+ is the minimum supported version\n+      self.skipTest(\"ROCm <6.4 bug: NaN propagation when beta==0 (fixed in ROCm 6.4.0)\")\n+\n     op = lambda M: M.T if transpose else M\n \n     v_rng = jtu.rand_default(self.rng())\n@@ -228,6 +247,14 @@ def test_csr_matvec(self, shape, dtype, transpose):\n       transpose=[True, False],\n   )\n   def test_csr_matmat(self, shape, dtype, transpose):\n+    if (\n+        jtu.is_device_rocm() and\n+        get_rocm_version() < (6, 4) and\n+        dtype in (jtu.dtypes.floating + jtu.dtypes.complex)\n+    ):\n+      # TODO: Remove this check when ROCm 6.4+ is the minimum supported version\n+      self.skipTest(\"ROCm <6.4 bug: NaN propagation when beta==0 (fixed in ROCm 6.4.0)\")\n+\n     op = lambda M: M.T if transpose else M\n \n     B_rng = jtu.rand_default(self.rng())\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Lower unreduced to shardy. GSPMD doesn't support unreduced.\n\nPiperOrigin-RevId: 758719692",
        "output": [
            "```diff\nCommit: 0b70c2323df506811fb4d5290df72ca5ee328c55\nDate: 2025-05-14T16:41:11Z\nURL: https://github.com/jax-ml/jax/commit/0b70c2323df506811fb4d5290df72ca5ee328c55\nFiles changed: 8\nAdditions: +64, Deletions: -45\ndiff --git a/jax/_src/callback.py b/jax/_src/callback.py\nindex 06b36ce5c880..bc233b634f3c 100644\n--- a/jax/_src/callback.py\n+++ b/jax/_src/callback.py\n@@ -158,7 +158,7 @@ def _callback_op_sharding(\n       op_sharding = sharding_impls.SdyArrayList([\n           sharding_impls.SdyArray(\n               mesh_shape=(),\n-              dimension_shardings=[\n+              dim_shardings=[\n                   sharding_impls.SdyDim(axes=[], is_open=False)\n               ] * avals_out[0].ndim,\n               logical_device_ids=())])\n@@ -200,7 +200,7 @@ def _callback_op_sharding(\n       op_sharding = sharding_impls.SdyArrayList(num_sdy_shardings * [\n           sharding_impls.SdyArray(\n               mesh_shape=(),\n-              dimension_shardings=[],\n+              dim_shardings=[],\n               logical_device_ids=(device_index,))])\n     else:\n       op_sharding = xc.OpSharding()  # type: ignore[assignment]\n@@ -610,7 +610,7 @@ def send_to_host(\n       assert len(sharding.shardings) >= 1\n       sharding = SdyArrayList([\n           SdyArray(\n-              mesh_shape=(), dimension_shardings=[],\n+              mesh_shape=(), dim_shardings=[],\n               logical_device_ids=sharding.shardings[0].logical_device_ids)])\n     mlir.set_sharding(send_op, sharding)\n   return send_op.result\n@@ -645,7 +645,7 @@ def receive_from_host(\n       sharding = SdyArrayList([\n           sharding.shardings[0],\n           SdyArray(\n-              mesh_shape=(), dimension_shardings=[],\n+              mesh_shape=(), dim_shardings=[],\n               logical_device_ids=sharding.shardings[0].logical_device_ids)])\n     mlir.set_sharding(recv_op, sharding)\n   # Token should be at the end of the results\ndiff --git a/jax/_src/debugging.py b/jax/_src/debugging.py\nindex 63abcbef331e..c2febf752b92 100644\n--- a/jax/_src/debugging.py\n+++ b/jax/_src/debugging.py\n@@ -168,7 +168,7 @@ def debug_callback_lowering(ctx, *args, effect, partitioned, callback, **params)\n         sharding = sharding_impls.SdyArrayList([\n             sharding_impls.SdyArray(\n                 mesh_shape=(),\n-                dimension_shardings=[\n+                dim_shardings=[\n                     sharding_impls.SdyDim(axes=[], is_open=False)\n                 ] * ctx.avals_out[0].ndim,\n                 logical_device_ids=())])\n@@ -184,7 +184,7 @@ def debug_callback_lowering(ctx, *args, effect, partitioned, callback, **params)\n     if config.use_shardy_partitioner.value:\n       sharding = sharding_impls.SdyArrayList([\n           sharding_impls.SdyArray(\n-              mesh_shape=(), dimension_shardings=[], logical_device_ids=(0,))])\n+              mesh_shape=(), dim_shardings=[], logical_device_ids=(0,))])\n     else:\n       sharding = xc.OpSharding()\n       sharding.type = xc.OpSharding.Type.MAXIMAL\ndiff --git a/jax/_src/interpreters/mlir.py b/jax/_src/interpreters/mlir.py\nindex 94418f0b958b..0256057b8b09 100644\n--- a/jax/_src/interpreters/mlir.py\n+++ b/jax/_src/interpreters/mlir.py\n@@ -1841,7 +1841,7 @@ def replicate_trailing_dims(ctx, val: ir.Value, aval) -> ir.Value:\n     physical_ndim = core.physical_aval(aval).ndim\n     s = SdyArray(\n         mesh_shape=None,\n-        dimension_shardings=[\n+        dim_shardings=[\n             sharding_impls.SdyDim(axes=[], is_open=i < aval.ndim)\n             for i in range(physical_ndim)\n         ])\ndiff --git a/jax/_src/named_sharding.py b/jax/_src/named_sharding.py\nindex 45ae1c124a22..faf0b2a9f2b2 100644\n--- a/jax/_src/named_sharding.py\n+++ b/jax/_src/named_sharding.py\n@@ -44,7 +44,8 @@ def __init__(self, mesh: mesh_lib.Mesh):\n   def _to_sdy_sharding(self, ndim: int) -> SdyArray:\n     dim_shardings = [SdyDim(axes=[], is_open=True)\n                      for _ in range(ndim)]\n-    return SdyArray(self.mesh.shape_tuple, dim_shardings)\n+    return SdyArray(mesh_shape=self.mesh.shape_tuple,\n+                    dim_shardings=dim_shardings)\n \n class UnspecifiedValue:\n   def __repr__(self):\n@@ -244,8 +245,10 @@ def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n       else:\n         dim_spec = dim_spec if isinstance(dim_spec, tuple) else (dim_spec,)\n         dim_shardings[i].axes = dim_spec\n-    return SdyArray(self.mesh.shape_tuple, dim_shardings,\n-                    self._logical_device_ids)\n+    return SdyArray(mesh_shape=self.mesh.shape_tuple,\n+                    dim_shardings=dim_shardings,\n+                    logical_device_ids=self._logical_device_ids,\n+                    unreduced_axes=self.spec.unreduced)\n \n NamedSharding.__module__ = 'jax.sharding'\n \n@@ -285,13 +288,21 @@ def _custom_repr(self):\n     priority_repr = '' if self.priority is None else f'p{self.priority}'\n     return f'{{{axes_repr}{open_repr}}}{priority_repr}'\n \n+def _get_axes(axes, mesh_shape):\n+  if not axes:\n+    return ()\n+  assert mesh_shape is not None\n+  # Sort wrt mesh axis names so order is deterministic and doesn't hang in\n+  # McJAX.\n+  return tuple(n for n, _ in mesh_shape if n in axes)\n \n-@dataclasses.dataclass\n+@dataclasses.dataclass(kw_only=True)\n class SdyArray:\n   mesh_shape: tuple[tuple[str, int], ...] | None\n-  dimension_shardings: Sequence[SdyDim]\n+  dim_shardings: Sequence[SdyDim]\n   logical_device_ids: tuple[int, ...] | None = None\n   replicated_axes: tuple[str, ...] = ()\n+  unreduced_axes: tuple[str, ...] = ()\n \n   def build(self) -> sdy.TensorShardingAttr:\n     if self.mesh_shape is None:\n@@ -302,14 +313,18 @@ def build(self) -> sdy.TensorShardingAttr:\n       mesh_attr = sdy.MeshAttr.get(\n           [sdy.MeshAxisAttr.get(name, size) for name, size in self.mesh_shape],\n           ldi)\n+\n+    replicated_axes = _get_axes(self.replicated_axes, self.mesh_shape)\n+    unreduced_axes = _get_axes(self.unreduced_axes, self.mesh_shape)\n     return sdy.TensorShardingAttr.get(\n         mesh_attr,\n-        [dim_sharding.build() for dim_sharding in self.dimension_shardings],\n-        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in self.replicated_axes])\n+        [dim_sharding.build() for dim_sharding in self.dim_shardings],\n+        replicated_axes=[sdy.AxisRefAttr.get(axis) for axis in replicated_axes],\n+        unreduced_axes=[sdy.AxisRefAttr.get(axis) for axis in unreduced_axes])\n \n   def __repr__(self):\n     dim_sharding_repr = ', '.join(\n-        d._custom_repr() for d in self.dimension_shardings)\n+        d._custom_repr() for d in self.dim_shardings)\n     device_id_repr = (f', device_ids={self.logical_device_ids}'\n                       if self.logical_device_ids is not None else '')\n     rar = (f', replicated_axes={self.replicated_axes}'\n@@ -317,6 +332,26 @@ def __repr__(self):\n     return f\"SdyArray([{dim_sharding_repr}]{device_id_repr}{rar})\"\n \n \n+# TODO(yashkatariya): Upstream this into `_to_sdy_sharding` maybe with an extra\n+# parameter to it `_to_sdy_sharding(self, ndim, modify_wrt_axis_types=False)`\n+def modify_sdy_sharding_wrt_axis_types(sdy_sharding: SdyArray, mesh):\n+  if mesh._any_axis_auto:\n+    dim_shardings, used_axes = [], []  # type: ignore\n+    for d in sdy_sharding.dim_shardings:\n+      # TODO(yashkatariya): Maybe if any mesh axis is auto, mark all axes as open?\n+      dim_shardings.append(SdyDim(axes=[], is_open=True)\n+                           if not d.axes and not d.is_open else d)\n+      used_axes.extend(d.axes)\n+    remaining_axes = set(mesh.axis_names) - set(used_axes)\n+    replicated_axes = tuple(r for r in remaining_axes\n+                            if mesh._name_to_type[r] == mesh_lib.AxisType.Explicit)\n+    return SdyArray(mesh_shape=sdy_sharding.mesh_shape,\n+                    dim_shardings=dim_shardings,\n+                    logical_device_ids=sdy_sharding.logical_device_ids,\n+                    replicated_axes=replicated_axes)\n+  return sdy_sharding\n+\n+\n @cache(max_size=4096, trace_context_in_key=False)\n def named_sharding_to_xla_hlo_sharding(\n     self, num_dimensions: int) -> xc.HloSharding:\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 3eb46da890f2..abcc2ca0acf1 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -781,7 +781,7 @@ def _shardy_shard_map_sharding(\n     aval_in = core.physical_aval(aval_in)\n   sdy_sharding = ns._to_sdy_sharding(aval_in.ndim)\n   if len(manual_axes) < len(mesh.axis_names):\n-    for dim_sharding in sdy_sharding.dimension_shardings:\n+    for dim_sharding in sdy_sharding.dim_shardings:\n       dim_sharding.is_open = True\n   return sdy_sharding\n \ndiff --git a/jax/_src/sharding_impls.py b/jax/_src/sharding_impls.py\nindex f7f0ebd2cc26..982af82c5c4d 100644\n--- a/jax/_src/sharding_impls.py\n+++ b/jax/_src/sharding_impls.py\n@@ -38,7 +38,8 @@\n     SdyArray, SdyDim, UnspecifiedValue, AUTO,\n     _check_unique_resources, NamedSharding, UNSPECIFIED,\n     ArrayMapping, ArrayMappingOrAutoOrUnspecified, get_array_mapping,\n-    array_mapping_to_axis_resources, named_sharding_to_xla_hlo_sharding)\n+    array_mapping_to_axis_resources, named_sharding_to_xla_hlo_sharding,\n+    modify_sdy_sharding_wrt_axis_types)\n from jax._src.op_shardings import (\n     are_op_shardings_equal, get_num_ways_dim_sharded, is_op_sharding_replicated)\n from jax._src.partition_spec import PartitionSpec\n@@ -95,27 +96,6 @@ def build(self) -> sdy.TensorShardingPerValueAttr:\n         [sharding.build() for sharding in self.shardings])\n \n \n-# TODO(yashkatariya): Upstream this into `_to_sdy_sharding` maybe with an extra\n-# parameter to it `_to_sdy_sharding(self, ndim, modify_wrt_axis_types=False)`\n-def modify_sdy_sharding_wrt_axis_types(sdy_sharding: SdyArray, mesh):\n-  if mesh._any_axis_auto:\n-    dim_shardings, used_axes = [], []  # type: ignore\n-    for d in sdy_sharding.dimension_shardings:\n-      # TODO(yashkatariya): Maybe if any mesh axis is auto, mark all axes as open?\n-      dim_shardings.append(SdyDim(axes=[], is_open=True)\n-                           if not d.axes and not d.is_open else d)\n-      used_axes.extend(d.axes)\n-    remaining_axes = set(mesh.axis_names) - set(used_axes)\n-    # Sort wrt mesh axis names so order is deterministic and doesn't hang in\n-    # McJAX.\n-    remaining_axes = [n for n in mesh.axis_names if n in remaining_axes]\n-    replicated_axes = tuple(r for r in remaining_axes\n-                            if mesh._name_to_type[r] == mesh_lib.AxisType.Explicit)\n-    return SdyArray(sdy_sharding.mesh_shape, dim_shardings,\n-                    sdy_sharding.logical_device_ids, replicated_axes)\n-  return sdy_sharding\n-\n-\n replicated_hlo_sharding = xc.HloSharding.replicate()\n \n \n@@ -188,7 +168,7 @@ def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n   def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n     sdy_dim_sharding = [SdyDim(axes=[], is_open=False)\n                         for _ in range(num_dimensions)]\n-    return SdyArray(None, sdy_dim_sharding)\n+    return SdyArray(mesh_shape=None, dim_shardings=sdy_dim_sharding)\n \n   @property\n   def is_fully_replicated(self) -> bool:\ndiff --git a/tests/array_test.py b/tests/array_test.py\nindex 0bab37c07bff..1691c3acc749 100644\n--- a/tests/array_test.py\n+++ b/tests/array_test.py\n@@ -1477,8 +1477,8 @@ def test_long_axis_names(self):\n     self.assertEqual(\n         sdy_sharding,\n         SdyArray(\n-            mesh.shape_tuple,\n-            [SdyDim(\n+            mesh_shape=mesh.shape_tuple,\n+            dim_shardings=[SdyDim(\n              ('sequence', 'data'), False),\n              SdyDim(('model',), False),\n              SdyDim([], False)]))\n@@ -1497,8 +1497,8 @@ def test_unconstrained(self):\n     self.assertEqual(\n         sdy_sharding,\n         SdyArray(\n-            mesh.shape_tuple,\n-            [SdyDim([], False),\n+            mesh_shape=mesh.shape_tuple,\n+            dim_shardings=[SdyDim([], False),\n              SdyDim([], True),\n              SdyDim(('x',), False)]))\n     with ir.Context() as ctx:\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 9b658dd8c604..e374b0b15a7b 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7735,6 +7735,7 @@ def f(x):\n     f(arr)  # doesn't crash\n     jax.jit(f)(arr)  # doesn't crash\n \n+  @config.use_shardy_partitioner(True)\n   @jtu.with_explicit_mesh((2, 2), ('x', 'y'))\n   def test_unreduced_basic(self, mesh):\n     np_inp = np.arange(16).reshape(8, 2)\n@@ -7758,7 +7759,10 @@ def f(x, y, a, b):\n       self.assertEqual(out.aval.sharding.spec, P('x', None))\n       return out\n \n-    f.trace(x, y, a, b)  # doesn't crash\n+    traced = f.trace(x, y, a, b)\n+    lowered_text = traced.lower().as_text()\n+    self.assertIn('unreduced={\"y\"}', lowered_text)\n+    self.assertTrue(lowered_text.count('unreduced={\"y\"}') == 3)\n \n   @jtu.with_explicit_mesh((2, 2, 1), ('x', 'y', 'z'))\n   def test_dot_general_unreduced_error(self, mesh):\n@@ -8595,7 +8599,7 @@ def f(x, y):\n   def test_array_sharding_repr_with_priority(self):\n     sharding = sharding_impls.SdyArray(\n         mesh_shape=(('data', 4), ('model', 8), ('expert', 2)),\n-        dimension_shardings=[\n+        dim_shardings=[\n             sharding_impls.SdyDim(axes=['data', 'expert'], is_open=False),\n             sharding_impls.SdyDim(axes=['model'], is_open=True, priority=2)])\n     self.assertEqual(repr(sharding), \"SdyArray([{'data', 'expert'}, {'model', ?}p2])\")\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Skip CSR matmat/matvec float tests on ROCm <6.4 (NaN issue with beta==0).\n\nCherry-picked from ROCm fork (commit 5dbfa9d1bcb3629658c2ec9addf45ac389f17305).\n\nAdded TODOs to remove this check when ROCm 6.4+ is the minimum supported version.",
        "output": [
            "```diff\nCommit: 0af3e8205307db0e12259930de0ce63d50f18f2e\nDate: 2025-05-14T15:03:11Z\nURL: https://github.com/jax-ml/jax/commit/0af3e8205307db0e12259930de0ce63d50f18f2e\nFiles changed: 1\nAdditions: +27, Deletions: -0\ndiff --git a/tests/sparse_test.py b/tests/sparse_test.py\nindex 71437fd0e028..97a156f9f6f5 100644\n--- a/tests/sparse_test.py\n+++ b/tests/sparse_test.py\n@@ -16,6 +16,8 @@\n from functools import partial\n import itertools\n import math\n+import os\n+from pathlib import Path\n \n from absl.testing import absltest\n from absl.testing import parameterized\n@@ -42,6 +44,15 @@\n import numpy as np\n import scipy.sparse\n \n+def get_rocm_version():\n+  rocm_path = os.environ.get(\"ROCM_PATH\", \"/opt/rocm\")\n+  version_path = Path(rocm_path) / \".info\" / \"version\"\n+  if not version_path.exists():\n+    raise FileNotFoundError(f\"Expected ROCm version file at {version_path}\")\n+  version_str = version_path.read_text().strip()\n+  major, minor, *_ = version_str.split(\".\")\n+  return int(major), int(minor)\n+\n jax.config.parse_flags_with_absl()\n \n all_dtypes = jtu.dtypes.integer + jtu.dtypes.floating + jtu.dtypes.complex\n@@ -208,6 +219,14 @@ def test_csr_fromdense(self, shape, dtype):\n     transpose=[True, False],\n   )\n   def test_csr_matvec(self, shape, dtype, transpose):\n+    if (\n+        jtu.is_device_rocm() and\n+        get_rocm_version() < (6, 4) and\n+        dtype in (jtu.dtypes.floating + jtu.dtypes.complex)\n+    ):\n+      # TODO: Remove this check when ROCm 6.4+ is the minimum supported version\n+      self.skipTest(\"ROCm <6.4 bug: NaN propagation when beta==0 (fixed in ROCm 6.4.0)\")\n+\n     op = lambda M: M.T if transpose else M\n \n     v_rng = jtu.rand_default(self.rng())\n@@ -228,6 +247,14 @@ def test_csr_matvec(self, shape, dtype, transpose):\n       transpose=[True, False],\n   )\n   def test_csr_matmat(self, shape, dtype, transpose):\n+    if (\n+        jtu.is_device_rocm() and\n+        get_rocm_version() < (6, 4) and\n+        dtype in (jtu.dtypes.floating + jtu.dtypes.complex)\n+    ):\n+      # TODO: Remove this check when ROCm 6.4+ is the minimum supported version\n+      self.skipTest(\"ROCm <6.4 bug: NaN propagation when beta==0 (fixed in ROCm 6.4.0)\")\n+\n     op = lambda M: M.T if transpose else M\n \n     B_rng = jtu.rand_default(self.rng())\n\n```"
        ],
        "from_id": [
            "psanal35"
        ]
    },
    {
        "text_input": "Rename `SdyArraySharding -> SdyArray` and `SdyDimSharding -> SdyDim` since these are not `Sharding` from a JAX POV.\n\nPiperOrigin-RevId: 758679923",
        "output": [
            "```diff\nCommit: 3142bc3b464dec165008257c62d93afa4e97d919\nDate: 2025-05-14T14:46:24Z\nURL: https://github.com/jax-ml/jax/commit/3142bc3b464dec165008257c62d93afa4e97d919\nFiles changed: 8\nAdditions: +78, Deletions: -78\ndiff --git a/jax/_src/callback.py b/jax/_src/callback.py\nindex d23389af16eb..06b36ce5c880 100644\n--- a/jax/_src/callback.py\n+++ b/jax/_src/callback.py\n@@ -40,7 +40,7 @@\n from jax._src.lib import xla_client as xc\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n-from jax._src.sharding_impls import SdyArraySharding, SdyArrayShardingList, SingleDeviceSharding\n+from jax._src.sharding_impls import SdyArray, SdyArrayList, SingleDeviceSharding\n from jax._src.typing import DeprecatedArg\n import numpy as np\n \n@@ -155,11 +155,11 @@ def _callback_op_sharding(\n       )\n     if config.use_shardy_partitioner.value:\n       assert len(avals_out) == 1\n-      op_sharding = sharding_impls.SdyArrayShardingList([\n-          sharding_impls.SdyArraySharding(\n+      op_sharding = sharding_impls.SdyArrayList([\n+          sharding_impls.SdyArray(\n               mesh_shape=(),\n               dimension_shardings=[\n-                  sharding_impls.SdyDimSharding(axes=[], is_open=False)\n+                  sharding_impls.SdyDim(axes=[], is_open=False)\n               ] * avals_out[0].ndim,\n               logical_device_ids=())])\n     else:\n@@ -197,8 +197,8 @@ def _callback_op_sharding(\n       # number of result ops. If there are no result ops, we need 1 shardy\n       # annotation.\n       num_sdy_shardings = max(1, len(avals_out))\n-      op_sharding = sharding_impls.SdyArrayShardingList(num_sdy_shardings * [\n-          sharding_impls.SdyArraySharding(\n+      op_sharding = sharding_impls.SdyArrayList(num_sdy_shardings * [\n+          sharding_impls.SdyArray(\n               mesh_shape=(),\n               dimension_shardings=[],\n               logical_device_ids=(device_index,))])\n@@ -590,7 +590,7 @@ def send_to_host(\n     operand: Any,\n     name: str,\n     *,\n-    sharding: SdyArrayShardingList | xc.OpSharding | None = None,\n+    sharding: SdyArrayList | xc.OpSharding | None = None,\n ) -> ir.Value:\n   channel_handle = hlo.ChannelHandle.get(channel, mlir.SEND_TO_HOST_TYPE)\n   send_op = hlo.SendOp([operand], token, channel_handle,\n@@ -606,10 +606,10 @@ def send_to_host(\n       # we need to create an equivalent sharding with no dimensions. If there\n       # are multiple shardings, just grab the first one since all these\n       # shardings should be the same.\n-      assert isinstance(sharding, SdyArrayShardingList)\n+      assert isinstance(sharding, SdyArrayList)\n       assert len(sharding.shardings) >= 1\n-      sharding = SdyArrayShardingList([\n-          SdyArraySharding(\n+      sharding = SdyArrayList([\n+          SdyArray(\n               mesh_shape=(), dimension_shardings=[],\n               logical_device_ids=sharding.shardings[0].logical_device_ids)])\n     mlir.set_sharding(send_op, sharding)\n@@ -622,7 +622,7 @@ def receive_from_host(\n     out_aval: core.ShapedArray,\n     name: str,\n     *,\n-    sharding: SdyArrayShardingList | xc.OpSharding | None = None,\n+    sharding: SdyArrayList | xc.OpSharding | None = None,\n ) -> tuple[ir.Value, ir.Value]:\n   channel_handle = hlo.ChannelHandle.get(channel, mlir.RECV_FROM_HOST_TYPE)\n   recv_op = hlo.RecvOp([mlir.aval_to_ir_type(out_aval),\n@@ -634,7 +634,7 @@ def receive_from_host(\n           _xla_host_transfer_rendezvous=ir.StringAttr.get(str(name))))\n   if sharding is not None:\n     if config.use_shardy_partitioner.value:\n-      assert isinstance(sharding, SdyArrayShardingList)\n+      assert isinstance(sharding, SdyArrayList)\n       assert len(sharding.shardings) >= 1\n        # `RecvOp`'s last argument is a `TokenType`. Since Shardy requires the\n       # number of shardings to match the number of results, but JAX only sees\n@@ -642,9 +642,9 @@ def receive_from_host(\n       # Note that even if a function returns N results, we will end up with N\n       # `RecvOp`s, so we only need to get the first sharding. All shardings are\n       # the same anyways, operating on the same single device ID.\n-      sharding = SdyArrayShardingList([\n+      sharding = SdyArrayList([\n           sharding.shardings[0],\n-          SdyArraySharding(\n+          SdyArray(\n               mesh_shape=(), dimension_shardings=[],\n               logical_device_ids=sharding.shardings[0].logical_device_ids)])\n     mlir.set_sharding(recv_op, sharding)\n@@ -683,7 +683,7 @@ def _emit_tpu_python_callback(\n     result_avals: Sequence[core.ShapedArray],\n     result_shapes: Sequence[xc.Shape],\n     *,\n-    sharding: SdyArrayShardingList | xc.OpSharding | None = None,\n+    sharding: SdyArrayList | xc.OpSharding | None = None,\n ) -> tuple[Sequence[ir.Value], Any]:\n   token = token or hlo.create_token()\n   _wrapped_callback = callback\n@@ -738,7 +738,7 @@ def emit_python_callback(\n     *,\n     has_side_effect: bool,\n     partitioned: bool = False,\n-    sharding: SdyArrayShardingList | xc.OpSharding | None = None,\n+    sharding: SdyArrayList | xc.OpSharding | None = None,\n ) -> tuple[Sequence[mlir.IrValues], Any, Any]:\n   \"\"\"Emits MLIR that calls back to a provided Python function.\n \n@@ -836,12 +836,12 @@ def _wrapped_callback(token, *args):  # type: ignore  # pylint: disable=function\n         config.use_shardy_partitioner.value\n         and sharding is not None\n         and len(ctx.avals_out) > 0\n-        and isinstance(sharding, sharding_impls.SdyArrayShardingList)\n+        and isinstance(sharding, sharding_impls.SdyArrayList)\n     ):\n       # Add a sharding annotation for the token if we have at least one\n       # output. Otherwise, the single shardy annotation required of all ops\n       # (even those without any results) can annotate the token.\n-      sharding = sharding_impls.SdyArrayShardingList(\n+      sharding = sharding_impls.SdyArrayList(\n           [*sharding.shardings, sharding.shardings[-1]]\n       )\n     ctx = dataclasses.replace(\ndiff --git a/jax/_src/debugging.py b/jax/_src/debugging.py\nindex 29cbb01511e9..63abcbef331e 100644\n--- a/jax/_src/debugging.py\n+++ b/jax/_src/debugging.py\n@@ -165,11 +165,11 @@ def debug_callback_lowering(ctx, *args, effect, partitioned, callback, **params)\n       # program has per-device semantics, so we run the callback on each device.\n       if config.use_shardy_partitioner.value:\n         assert len(ctx.avals_out) == 1\n-        sharding = sharding_impls.SdyArrayShardingList([\n-            sharding_impls.SdyArraySharding(\n+        sharding = sharding_impls.SdyArrayList([\n+            sharding_impls.SdyArray(\n                 mesh_shape=(),\n                 dimension_shardings=[\n-                    sharding_impls.SdyDimSharding(axes=[], is_open=False)\n+                    sharding_impls.SdyDim(axes=[], is_open=False)\n                 ] * ctx.avals_out[0].ndim,\n                 logical_device_ids=())])\n       else:\n@@ -182,8 +182,8 @@ def debug_callback_lowering(ctx, *args, effect, partitioned, callback, **params)\n     # program has bulk array semantics, so we run the callback with a MAXIMAL\n     # sharding and hence execute it only once on the full logical value).\n     if config.use_shardy_partitioner.value:\n-      sharding = sharding_impls.SdyArrayShardingList([\n-          sharding_impls.SdyArraySharding(\n+      sharding = sharding_impls.SdyArrayList([\n+          sharding_impls.SdyArray(\n               mesh_shape=(), dimension_shardings=[], logical_device_ids=(0,))])\n     else:\n       sharding = xc.OpSharding()\ndiff --git a/jax/_src/interpreters/mlir.py b/jax/_src/interpreters/mlir.py\nindex f6ef5787ccbf..94418f0b958b 100644\n--- a/jax/_src/interpreters/mlir.py\n+++ b/jax/_src/interpreters/mlir.py\n@@ -57,7 +57,7 @@\n from jax._src.partition_spec import PartitionSpec\n from jax._src.sharding import Sharding as JSharding\n from jax._src.sharding_impls import ( AUTO, NamedSharding,\n-                                     SdyArraySharding, SdyArrayShardingList,\n+                                     SdyArray, SdyArrayList,\n                                      modify_sdy_sharding_wrt_axis_types)\n from jax._src.state.types import AbstractRef\n from jax._src.util import foreach\n@@ -1034,7 +1034,7 @@ def add_manual_axes(axis_ctx: sharding_impls.SPMDAxisContext, sharding, ndim):\n def _to_physical_op_sharding(\n     ctx: ModuleContext,\n     aval: core.AbstractValue, sharding: JSharding | AUTO | None,\n-) -> xc.OpSharding | SdyArraySharding | None:\n+) -> xc.OpSharding | SdyArray | None:\n   if sharding is None:\n     return None\n   if all_unconstrained(sharding, aval):\n@@ -1839,10 +1839,10 @@ def replicate_trailing_dims(ctx, val: ir.Value, aval) -> ir.Value:\n   assert isinstance(aval, (core.ShapedArray, core.DShapedArray))\n   if config.use_shardy_partitioner.value:\n     physical_ndim = core.physical_aval(aval).ndim\n-    s = SdyArraySharding(\n+    s = SdyArray(\n         mesh_shape=None,\n         dimension_shardings=[\n-            sharding_impls.SdyDimSharding(axes=[], is_open=i < aval.ndim)\n+            sharding_impls.SdyDim(axes=[], is_open=i < aval.ndim)\n             for i in range(physical_ndim)\n         ])\n     return wrap_with_sharding_op(ctx, val, aval, s)\n@@ -2665,7 +2665,7 @@ def _wrap_with_spmd_op(name: str,\n                        ctx: LoweringRuleContext,\n                        x: ir.Value,\n                        aval_out: core.AbstractValue,\n-                       sharding: xc.OpSharding | SdyArraySharding,\n+                       sharding: xc.OpSharding | SdyArray,\n                        unspecified_dims: set[int] | None = None,\n                        has_side_effect: bool = False,\n                        allow_shardy_lowering: bool = False):\n@@ -2730,7 +2730,7 @@ def lower_with_sharding_in_types(ctx, op, aval, sharding_proto=None):\n     return wrap_with_sharding_op(ctx, op, aval, proto, unspecified_dims)\n \n \n-def set_sharding(op, sharding: xc.OpSharding | SdyArraySharding | SdyArrayShardingList):\n+def set_sharding(op, sharding: xc.OpSharding | SdyArray | SdyArrayList):\n   if config.use_shardy_partitioner.value:\n     op.attributes[\"sdy.sharding\"] = get_sharding_attr(sharding)\n   else:\n@@ -2738,7 +2738,7 @@ def set_sharding(op, sharding: xc.OpSharding | SdyArraySharding | SdyArrayShardi\n \n \n def get_sharding_attr(\n-    sharding: xc.OpSharding | SdyArraySharding | SdyArrayShardingList\n+    sharding: xc.OpSharding | SdyArray | SdyArrayList\n ) -> ir.Attribute:\n   if config.use_shardy_partitioner.value:\n     return sharding.build()  # type: ignore\ndiff --git a/jax/_src/named_sharding.py b/jax/_src/named_sharding.py\nindex 3dfcbd29fc96..45ae1c124a22 100644\n--- a/jax/_src/named_sharding.py\n+++ b/jax/_src/named_sharding.py\n@@ -41,10 +41,10 @@ class AUTO:\n   def __init__(self, mesh: mesh_lib.Mesh):\n     self.mesh = mesh\n \n-  def _to_sdy_sharding(self, ndim: int) -> SdyArraySharding:\n-    dim_shardings = [SdyDimSharding(axes=[], is_open=True)\n+  def _to_sdy_sharding(self, ndim: int) -> SdyArray:\n+    dim_shardings = [SdyDim(axes=[], is_open=True)\n                      for _ in range(ndim)]\n-    return SdyArraySharding(self.mesh.shape_tuple, dim_shardings)\n+    return SdyArray(self.mesh.shape_tuple, dim_shardings)\n \n class UnspecifiedValue:\n   def __repr__(self):\n@@ -232,8 +232,8 @@ def with_spec(self, spec: PartitionSpec | Sequence[Any]) -> NamedSharding:\n   def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n     return named_sharding_to_xla_hlo_sharding(self, num_dimensions)\n \n-  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArraySharding:\n-    dim_shardings = [SdyDimSharding(axes=[], is_open=False)\n+  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n+    dim_shardings = [SdyDim(axes=[], is_open=False)\n                      for _ in range(num_dimensions)]\n     for i, dim_spec in enumerate(self.spec):\n       if dim_spec is PartitionSpec.UNCONSTRAINED:\n@@ -244,8 +244,8 @@ def _to_sdy_sharding(self, num_dimensions: int) -> SdyArraySharding:\n       else:\n         dim_spec = dim_spec if isinstance(dim_spec, tuple) else (dim_spec,)\n         dim_shardings[i].axes = dim_spec\n-    return SdyArraySharding(self.mesh.shape_tuple, dim_shardings,\n-                            self._logical_device_ids)\n+    return SdyArray(self.mesh.shape_tuple, dim_shardings,\n+                    self._logical_device_ids)\n \n NamedSharding.__module__ = 'jax.sharding'\n \n@@ -264,7 +264,7 @@ def get_array_mapping(\n   return d\n \n @dataclasses.dataclass\n-class SdyDimSharding:\n+class SdyDim:\n   axes: Sequence[str]\n   is_open: bool\n   priority: int | None = None\n@@ -275,7 +275,7 @@ def build(self) -> sdy.DimensionShardingAttr:\n         is_closed=not self.is_open, priority=self.priority)\n \n   def __repr__(self):\n-    return f'SdyDimSharding({self._custom_repr()})'\n+    return f'SdyDim({self._custom_repr()})'\n \n   def _custom_repr(self):\n     axes_repr = ', '.join(f\"'{a}'\" for a in self.axes)\n@@ -287,9 +287,9 @@ def _custom_repr(self):\n \n \n @dataclasses.dataclass\n-class SdyArraySharding:\n+class SdyArray:\n   mesh_shape: tuple[tuple[str, int], ...] | None\n-  dimension_shardings: Sequence[SdyDimSharding]\n+  dimension_shardings: Sequence[SdyDim]\n   logical_device_ids: tuple[int, ...] | None = None\n   replicated_axes: tuple[str, ...] = ()\n \n@@ -314,7 +314,7 @@ def __repr__(self):\n                       if self.logical_device_ids is not None else '')\n     rar = (f', replicated_axes={self.replicated_axes}'\n            if self.replicated_axes else '')\n-    return f\"SdyArraySharding([{dim_sharding_repr}]{device_id_repr}{rar})\"\n+    return f\"SdyArray([{dim_sharding_repr}]{device_id_repr}{rar})\"\n \n \n @cache(max_size=4096, trace_context_in_key=False)\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex b772a3de239e..3eb46da890f2 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -773,7 +773,7 @@ def _valid_repeats(mesh: Mesh, vma: Set[AxisName], names: AxisNames) -> bool:\n \n def _shardy_shard_map_sharding(\n     ctx: mlir.LoweringRuleContext, mesh, manual_axes, names, aval_in\n-) -> sharding_impls.SdyArraySharding:\n+) -> sharding_impls.SdyArray:\n   axes = {name: i for i, ns in names.items() for name in ns}\n   ns = _make_scoped_manual_sharding(ctx, mesh, axes)\n   if dtypes.issubdtype(aval_in.dtype, dtypes.extended):\n@@ -808,10 +808,10 @@ def _shard_map_lowering_shardy(\n           dim_var_values=ctx.dim_var_values)\n     return out_nodes\n \n-  in_shardings = sharding_impls.SdyArrayShardingList(map(\n+  in_shardings = sharding_impls.SdyArrayList(map(\n       partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n       in_names, ctx.avals_in)).build()\n-  out_shardings = sharding_impls.SdyArrayShardingList(map(\n+  out_shardings = sharding_impls.SdyArrayList(map(\n       partial(_shardy_shard_map_sharding, ctx, mesh, manual_axes),\n       out_names, ctx.avals_out)).build()\n   output_types = map(mlir.aval_to_ir_type, ctx.avals_out)\ndiff --git a/jax/_src/sharding_impls.py b/jax/_src/sharding_impls.py\nindex 2394e9e18f38..f7f0ebd2cc26 100644\n--- a/jax/_src/sharding_impls.py\n+++ b/jax/_src/sharding_impls.py\n@@ -35,7 +35,7 @@\n from jax._src.lib import xla_client as xc\n from jax._src.lib.mlir.dialects import sdy\n from jax._src.named_sharding import (  # noqa: F401\n-    SdyArraySharding, SdyDimSharding, UnspecifiedValue, AUTO,\n+    SdyArray, SdyDim, UnspecifiedValue, AUTO,\n     _check_unique_resources, NamedSharding, UNSPECIFIED,\n     ArrayMapping, ArrayMappingOrAutoOrUnspecified, get_array_mapping,\n     array_mapping_to_axis_resources, named_sharding_to_xla_hlo_sharding)\n@@ -87,8 +87,8 @@ def device_replica_id_map(sharding, global_shape: Shape) -> Mapping[Device, int]\n \n \n @dataclasses.dataclass\n-class SdyArrayShardingList:\n-  shardings: Sequence[SdyArraySharding]\n+class SdyArrayList:\n+  shardings: Sequence[SdyArray]\n \n   def build(self) -> sdy.TensorShardingPerValueAttr:\n     return sdy.TensorShardingPerValueAttr.get(\n@@ -97,12 +97,12 @@ def build(self) -> sdy.TensorShardingPerValueAttr:\n \n # TODO(yashkatariya): Upstream this into `_to_sdy_sharding` maybe with an extra\n # parameter to it `_to_sdy_sharding(self, ndim, modify_wrt_axis_types=False)`\n-def modify_sdy_sharding_wrt_axis_types(sdy_sharding: SdyArraySharding, mesh):\n+def modify_sdy_sharding_wrt_axis_types(sdy_sharding: SdyArray, mesh):\n   if mesh._any_axis_auto:\n     dim_shardings, used_axes = [], []  # type: ignore\n     for d in sdy_sharding.dimension_shardings:\n       # TODO(yashkatariya): Maybe if any mesh axis is auto, mark all axes as open?\n-      dim_shardings.append(SdyDimSharding(axes=[], is_open=True)\n+      dim_shardings.append(SdyDim(axes=[], is_open=True)\n                            if not d.axes and not d.is_open else d)\n       used_axes.extend(d.axes)\n     remaining_axes = set(mesh.axis_names) - set(used_axes)\n@@ -111,8 +111,8 @@ def modify_sdy_sharding_wrt_axis_types(sdy_sharding: SdyArraySharding, mesh):\n     remaining_axes = [n for n in mesh.axis_names if n in remaining_axes]\n     replicated_axes = tuple(r for r in remaining_axes\n                             if mesh._name_to_type[r] == mesh_lib.AxisType.Explicit)\n-    return SdyArraySharding(sdy_sharding.mesh_shape, dim_shardings,\n-                            sdy_sharding.logical_device_ids, replicated_axes)\n+    return SdyArray(sdy_sharding.mesh_shape, dim_shardings,\n+                    sdy_sharding.logical_device_ids, replicated_axes)\n   return sdy_sharding\n \n \n@@ -185,10 +185,10 @@ def _device_assignment(self) -> XLADeviceAssignment:\n   def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n     return replicated_hlo_sharding\n \n-  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArraySharding:\n-    sdy_dim_sharding = [SdyDimSharding(axes=[], is_open=False)\n+  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n+    sdy_dim_sharding = [SdyDim(axes=[], is_open=False)\n                         for _ in range(num_dimensions)]\n-    return SdyArraySharding(None, sdy_dim_sharding)\n+    return SdyArray(None, sdy_dim_sharding)\n \n   @property\n   def is_fully_replicated(self) -> bool:\n@@ -330,8 +330,8 @@ def with_memory_kind(self, kind: str):\n   def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n     raise NotImplementedError(\"pmap doesn't use OpSharding.\")\n \n-  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArraySharding:\n-    raise NotImplementedError(\"pmap doesn't use SdyArraySharding.\")\n+  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n+    raise NotImplementedError(\"pmap doesn't use SdyArray.\")\n \n   @functools.cached_property\n   def is_fully_replicated(self) -> bool:\n@@ -540,9 +540,9 @@ def _device_assignment(self) -> XLADeviceAssignment:\n   def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n     return _positional_sharding_to_xla_hlo_sharding(self, num_dimensions)\n \n-  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArraySharding:\n+  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n     raise NotImplementedError(\n-        \"PositionalSharding can't be converted to an SdyArraySharding.\")\n+        \"PositionalSharding can't be converted to an SdyArray.\")\n \n   @functools.cached_property\n   def is_fully_addressable(self) -> bool:\n@@ -657,9 +657,9 @@ def _device_assignment(self) -> XLADeviceAssignment:\n   def _to_xla_hlo_sharding(self, num_dimensions: int) -> xc.HloSharding:\n     return self._hlo_sharding\n \n-  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArraySharding:\n+  def _to_sdy_sharding(self, num_dimensions: int) -> SdyArray:\n     raise NotImplementedError(\n-        \"GSPMDSharding can't be converted to SdyArraySharding.\")\n+        \"GSPMDSharding can't be converted to SdyArray.\")\n \n   @functools.cached_property\n   def is_fully_replicated(self) -> bool:\ndiff --git a/tests/array_test.py b/tests/array_test.py\nindex b951f7f6b4cd..0bab37c07bff 100644\n--- a/tests/array_test.py\n+++ b/tests/array_test.py\n@@ -35,8 +35,8 @@\n from jax._src.sharding import common_devices_indices_map\n from jax._src.sharding_impls import (\n     _op_sharding_to_pos_sharding, pmap_sharding_devices_indices_map,\n-    NamedSharding, GSPMDSharding, PositionalSharding, SdyDimSharding,\n-    SdyArraySharding)\n+    NamedSharding, GSPMDSharding, PositionalSharding, SdyDim,\n+    SdyArray)\n from jax.experimental.pjit import pjit\n from jax.experimental import multihost_utils\n from jax.sharding import PartitionSpec as P\n@@ -1476,12 +1476,12 @@ def test_long_axis_names(self):\n     sdy_sharding = s._to_sdy_sharding(3)\n     self.assertEqual(\n         sdy_sharding,\n-        SdyArraySharding(\n+        SdyArray(\n             mesh.shape_tuple,\n-            [SdyDimSharding(\n+            [SdyDim(\n              ('sequence', 'data'), False),\n-             SdyDimSharding(('model',), False),\n-             SdyDimSharding([], False)]))\n+             SdyDim(('model',), False),\n+             SdyDim([], False)]))\n     with ir.Context() as ctx:\n       dialects.sdy.register_dialect(ctx)\n       self.assertEqual(\n@@ -1496,11 +1496,11 @@ def test_unconstrained(self):\n     sdy_sharding = s._to_sdy_sharding(3)\n     self.assertEqual(\n         sdy_sharding,\n-        SdyArraySharding(\n+        SdyArray(\n             mesh.shape_tuple,\n-            [SdyDimSharding([], False),\n-             SdyDimSharding([], True),\n-             SdyDimSharding(('x',), False)]))\n+            [SdyDim([], False),\n+             SdyDim([], True),\n+             SdyDim(('x',), False)]))\n     with ir.Context() as ctx:\n       dialects.sdy.register_dialect(ctx)\n       self.assertEqual(\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 22a4d4f70f8c..9b658dd8c604 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -8593,26 +8593,26 @@ def f(x, y):\n     self.assertIn('sdy.mesh @mesh = <[\"x\"=8]>', lowered_str)\n \n   def test_array_sharding_repr_with_priority(self):\n-    sharding = sharding_impls.SdyArraySharding(\n+    sharding = sharding_impls.SdyArray(\n         mesh_shape=(('data', 4), ('model', 8), ('expert', 2)),\n         dimension_shardings=[\n-            sharding_impls.SdyDimSharding(axes=['data', 'expert'], is_open=False),\n-            sharding_impls.SdyDimSharding(axes=['model'], is_open=True, priority=2)])\n-    self.assertEqual(repr(sharding), \"SdyArraySharding([{'data', 'expert'}, {'model', ?}p2])\")\n+            sharding_impls.SdyDim(axes=['data', 'expert'], is_open=False),\n+            sharding_impls.SdyDim(axes=['model'], is_open=True, priority=2)])\n+    self.assertEqual(repr(sharding), \"SdyArray([{'data', 'expert'}, {'model', ?}p2])\")\n \n   def test_array_sharding_repr_with_logical_ids(self):\n     abstract_mesh = jax.sharding.AbstractMesh((4, 8, 2), ('x', 'y', 'z'))\n     ns = NamedSharding(abstract_mesh, P(('x', 'y'), 'z', P.UNCONSTRAINED, None),\n                        _logical_device_ids=[4, 5, 6, 7, 0, 1, 2, 3])\n     self.assertEqual(repr(ns._to_sdy_sharding(4)),\n-                     \"SdyArraySharding([{'x', 'y'}, {'z'}, {?}, {}], \"\n+                     \"SdyArray([{'x', 'y'}, {'z'}, {?}, {}], \"\n                      \"device_ids=[4, 5, 6, 7, 0, 1, 2, 3])\")\n \n   def test_dimension_sharding_repr(self):\n-    dim_sharding = sharding_impls.SdyDimSharding(\n+    dim_sharding = sharding_impls.SdyDim(\n         axes=['data', 'model'], is_open=True, priority=2)\n     self.assertEqual(repr(dim_sharding),\n-                     \"SdyDimSharding({'data', 'model', ?}p2)\")\n+                     \"SdyDim({'data', 'model', ?}p2)\")\n \n   def test_tensor_dialect(self):\n     # While this doesn't emit any `mlir::TensorDialect` ops, some pass in the\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Replace gsutil command with gcloud storage commands.\n\nGCP recommends using `gcloud storage` instead of `gsutil`\n\nhttps://cloud.google.com/storage/docs/gsutil\n\nPiperOrigin-RevId: 758672654",
        "output": [
            "```diff\nCommit: abc79f3d590360a48245d051c62fdc23351a90d6\nDate: 2025-05-14T14:23:53Z\nURL: https://github.com/jax-ml/jax/commit/abc79f3d590360a48245d051c62fdc23351a90d6\nFiles changed: 6\nAdditions: +21, Deletions: -21\ndiff --git a/.github/workflows/bazel_cuda_non_rbe.yml b/.github/workflows/bazel_cuda_non_rbe.yml\nindex 72878ad7aacb..677d8d869a22 100644\n--- a/.github/workflows/bazel_cuda_non_rbe.yml\n+++ b/.github/workflows/bazel_cuda_non_rbe.yml\n@@ -84,12 +84,12 @@ jobs:\n         continue-on-error: true\n         run: |\n           mkdir -p $(pwd)/dist\n-          gsutil -m cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n+          gcloud storage cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n \n           if [[ ${{ inputs.jaxlib-version }} == \"head\" ]]; then\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*plugin*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*pjrt*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*plugin*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*pjrt*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n           elif [[ ${{ inputs.jaxlib-version }} == \"pypi_latest\" ]]; then\n             PYTHON=python${{ inputs.python }}\n             $PYTHON -m pip download jaxlib jax-cuda12-pjrt jax-cuda12-plugin --dest $(pwd)/dist/\ndiff --git a/.github/workflows/build_artifacts.yml b/.github/workflows/build_artifacts.yml\nindex 37a791784506..1b534ee3b6fc 100644\n--- a/.github/workflows/build_artifacts.yml\n+++ b/.github/workflows/build_artifacts.yml\n@@ -136,13 +136,13 @@ jobs:\n       - name: Upload artifacts to a GCS bucket (non-Windows runs)\n         if: >-\n           ${{ inputs.upload_artifacts_to_gcs && !contains(inputs.runner, 'windows-x86') }}\n-        run:  gsutil -m cp -r \"$(pwd)/dist/*.whl\" \"${{ inputs.gcs_upload_uri }}\"/\n+        run:  gcloud storage cp -r \"$(pwd)/dist/*.whl\" \"${{ inputs.gcs_upload_uri }}\"/\n       # Set shell to cmd to avoid path errors when using gcloud commands on Windows\n       - name: Upload artifacts to a GCS bucket (Windows runs)\n         if: >-\n           ${{ inputs.upload_artifacts_to_gcs &&  contains(inputs.runner, 'windows-x86') }}\n         shell: cmd\n-        run:  gsutil -m cp -r \"dist/*.whl\" \"${{ inputs.gcs_upload_uri }}\"/\n+        run:  gcloud storage cp -r \"dist/*.whl\" \"${{ inputs.gcs_upload_uri }}\"/\n       - name: Store the GCS upload URI as an output\n         id: store-gcs-upload-uri\n         if: ${{ inputs.upload_artifacts_to_gcs }}\ndiff --git a/.github/workflows/pytest_cpu.yml b/.github/workflows/pytest_cpu.yml\nindex bdce2b684803..fc4633110667 100644\n--- a/.github/workflows/pytest_cpu.yml\n+++ b/.github/workflows/pytest_cpu.yml\n@@ -96,12 +96,12 @@ jobs:\n         if: ${{ !contains(inputs.runner, 'windows-x86') }}\n         run: |\n           mkdir -p $(pwd)/dist\n-          gsutil -m cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n+          gcloud storage cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n \n           if [[ \"${{ inputs.download-jax-only-from-gcs }}\" == \"1\" ]]; then\n             echo \"JAX only release. Only downloading the jax wheel from the release bucket.\"\n           else\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n           fi\n       - name: Download wheels from GCS (Windows runs)\n         id: download-wheel-artifacts-w\n@@ -113,14 +113,14 @@ jobs:\n         shell: cmd\n         run: |\n           mkdir dist\n-          @REM Use `call` so that we can run sequential gsutil commands on Windows\n+          @REM Use `call` so that we can run sequential gcloud storage commands on Windows\n           @REM See https://github.com/GoogleCloudPlatform/gsutil/issues/233#issuecomment-196150652\n-          call gsutil -m cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl dist/\n+          call gcloud storage cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl dist/\n \n           if \"${{ inputs.download-jax-only-from-gcs }}\"==\"1\" (\n             echo \"JAX only release. Only downloading the jax wheel from the release bucket.\"\n           ) else (\n-            call gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*%PYTHON_MAJOR_MINOR%*%OS%*%ARCH%*.whl\" dist/\n+            call gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*%PYTHON_MAJOR_MINOR%*%OS%*%ARCH%*.whl\" dist/\n           )\n       - name: Skip the test run if the wheel artifacts were not downloaded successfully\n         if: steps.download-wheel-artifacts-nw.outcome == 'failure' || steps.download-wheel-artifacts-w.outcome == 'failure'\ndiff --git a/.github/workflows/pytest_cuda.yml b/.github/workflows/pytest_cuda.yml\nindex 4df752310ace..af034ab09991 100644\n--- a/.github/workflows/pytest_cuda.yml\n+++ b/.github/workflows/pytest_cuda.yml\n@@ -93,7 +93,7 @@ jobs:\n         continue-on-error: true\n         run: |\n           mkdir -p $(pwd)/dist\n-          gsutil -m cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n+          gcloud storage cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n \n           # Do not download the jaxlib and CUDA plugin artifacts if we are testing a jax only\n           # release.\n@@ -104,9 +104,9 @@ jobs:\n             # required dependency of jax so that gets installed automatically.\n             echo \"JAXCI_ADDITIONAL_WHEELS_INSTALL_FROM_PYPI=jax_cuda_pypi\">> $GITHUB_ENV\n           else\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*plugin*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*pjrt*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*plugin*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/ &&\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jax*cuda*pjrt*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n           fi\n       - name: Skip the test run if the wheel artifacts were not downloaded successfully\n         if: steps.download-wheel-artifacts.outcome == 'failure'\ndiff --git a/.github/workflows/pytest_tpu.yml b/.github/workflows/pytest_tpu.yml\nindex 55a0b4cc1a5f..2d4d2925bd2f 100644\n--- a/.github/workflows/pytest_tpu.yml\n+++ b/.github/workflows/pytest_tpu.yml\n@@ -114,11 +114,11 @@ jobs:\n         continue-on-error: true\n         run: |\n           mkdir -p $(pwd)/dist\n-          gsutil -m cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n+          gcloud storage cp -r \"${{ inputs.gcs_download_uri }}\"/jax*py3*none*any.whl $(pwd)/dist/\n           if [[ \"${{ inputs.download-jax-only-from-gcs }}\" == \"1\" ]]; then\n             echo \"JAX only release. Only downloading the jax wheel from the release bucket.\"\n           else\n-            gsutil -m cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${{ inputs.gcs_download_uri }}/jaxlib*${PYTHON_MAJOR_MINOR}*${OS}*${ARCH}*.whl\" $(pwd)/dist/\n           fi\n       - name: Skip the test run if the wheel artifacts were not downloaded successfully\n         if: steps.download-wheel-artifacts.outcome == 'failure'\ndiff --git a/.github/workflows/wheel_tests_nightly_release.yml b/.github/workflows/wheel_tests_nightly_release.yml\nindex 8d597b84f735..3e616a894d13 100644\n--- a/.github/workflows/wheel_tests_nightly_release.yml\n+++ b/.github/workflows/wheel_tests_nightly_release.yml\n@@ -171,15 +171,15 @@ jobs:\n           python_major_minor=$(echo \"${python_major_minor//-nogil/t}\" | tr -d '.')\n           python_major_minor=\"cp${python_major_minor%t}-cp${python_major_minor}-\"\n \n-          gsutil -m cp -r \"${final_gcs_download_uri}\"/jax*py3*none*any.whl $(pwd)/dist/\n+          gcloud storage cp -r \"${final_gcs_download_uri}\"/jax*py3*none*any.whl $(pwd)/dist/\n \n           jax_wheel=$(ls dist/jax*py3*none*any.whl 2>/dev/null)\n           echo \"JAX_WHEEL=$jax_wheel\" >> $GITHUB_ENV\n \n           if [[ \"${{ inputs.download-jax-only-from-gcs }}\" != \"1\" ]]; then\n-            gsutil -m cp -r \"${final_gcs_download_uri}/jaxlib*${python_major_minor}*linux*x86_64*.whl\" $(pwd)/dist/\n-            gsutil -m cp -r \"${final_gcs_download_uri}/jax*cuda*plugin*${python_major_minor}*linux*x86_64*.whl\" $(pwd)/dist/\n-            gsutil -m cp -r \"${final_gcs_download_uri}/jax*cuda*pjrt*linux*x86_64*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${final_gcs_download_uri}/jaxlib*${python_major_minor}*linux*x86_64*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${final_gcs_download_uri}/jax*cuda*plugin*${python_major_minor}*linux*x86_64*.whl\" $(pwd)/dist/\n+            gcloud storage cp -r \"${final_gcs_download_uri}/jax*cuda*pjrt*linux*x86_64*.whl\" $(pwd)/dist/\n \n             jaxlib_wheel=$(ls dist/jaxlib*${python_major_minor}*linux*x86_64*.whl 2>/dev/null)\n             jax_cuda_plugin_wheel=$(ls dist/jax*cuda*plugin*${python_major_minor}*linux*x86_64*.whl 2>/dev/null)\n\n```"
        ],
        "from_id": [
            "nitins17",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas/Mosaic GPU] Optimize the construction of output buffers for `plgpu.kernel`.\n\nPreviously to this change, we would zero initialize all buffers---which was a\nbig overhead. Instead, we now generate an empty custom call in order to only\ngenerate an allocation.\n\nPiperOrigin-RevId: 758659576",
        "output": [
            "```diff\nCommit: 7f5b6e7d02656d3b5f116fe557fcdc0c365f88ee\nDate: 2025-05-14T13:42:49Z\nURL: https://github.com/jax-ml/jax/commit/7f5b6e7d02656d3b5f116fe557fcdc0c365f88ee\nFiles changed: 1\nAdditions: +14, Deletions: -1\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 21c78720e812..e4fe1a7035a7 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -33,6 +33,7 @@\n from jax._src import tree_util\n from jax._src.lib.mlir.dialects import arith as arith_dialect\n from jax._src.pallas import core as pallas_core\n+from jax._src.pallas import pallas_call\n from jax._src.pallas import primitives as pallas_primitives\n import jax._src.pallas.utils as pallas_utils\n from jax._src.state import discharge as state_discharge\n@@ -194,12 +195,24 @@ def cmap_body():\n           mesh, compiler_params=compiler_params\n       )(cmap_body)\n     _, outs = state_discharge.run_state(stateful)(\n-        (operands, jax.tree.map(jnp.zeros_like, out_shape))\n+        (operands, empty_like(out_shape))\n     )\n     return outs[0] if unwrap_out else outs\n   return wrapper\n \n \n+def empty_like(shape):\n+  return pallas_call.pallas_call(\n+      lambda *_: None,\n+      out_shape=shape,\n+      out_specs=jax.tree.map(\n+          lambda _: pallas_core.BlockSpec(memory_space=GPUMemorySpace.GMEM),\n+          shape,\n+      ),\n+      backend=\"mosaic_gpu\",\n+  )()\n+\n+\n def _is_known_divisible(value, divisor, fuel=10) -> bool:\n   \"\"\"Returns True if the value is statically known to be divisible by the divisor.\"\"\"\n   if fuel < 0:\n\n```"
        ],
        "from_id": [
            "bchetioui",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Removed few tsan cpython suppressions as fixed",
        "output": [
            "```diff\nCommit: 08a0485a49378a8ba688cc06a9da925cafdb4c42\nDate: 2025-05-14T11:47:35Z\nURL: https://github.com/jax-ml/jax/commit/08a0485a49378a8ba688cc06a9da925cafdb4c42\nFiles changed: 2\nAdditions: +0, Deletions: -8\ndiff --git a/.github/workflows/tsan-suppressions_3.13.txt b/.github/workflows/tsan-suppressions_3.13.txt\nindex e82699036e92..aec94dfef004 100644\n--- a/.github/workflows/tsan-suppressions_3.13.txt\n+++ b/.github/workflows/tsan-suppressions_3.13.txt\n@@ -40,7 +40,3 @@ race:gemm_oncopy\n # https://github.com/python/cpython/issues/132245\n race:split_keys_entry_added\n race_top:dict_dict_merge\n-\n-# https://github.com/python/cpython/issues/132013\n-# Fixed on 3.14 and not backported to 3.13\n-race_top:frozenset_hash\n\\ No newline at end of file\ndiff --git a/.github/workflows/tsan-suppressions_3.14.txt b/.github/workflows/tsan-suppressions_3.14.txt\nindex ec4d81c987d0..ec5102502a2b 100644\n--- a/.github/workflows/tsan-suppressions_3.14.txt\n+++ b/.github/workflows/tsan-suppressions_3.14.txt\n@@ -18,7 +18,3 @@ race:dscal_k_\n race:scal_k_\n race:gemm_beta\n race:gemm_oncopy\n-\n-# https://github.com/python/cpython/issues/132214\n-# Should be fixed\n-# race_top:update_one_slot\n\n```"
        ],
        "from_id": [
            "vfdev-5",
            "noreply@github.com"
        ]
    },
    {
        "text_input": "Simplify if_building_jaxlib macro.\n\nI don't believe the GPU case of this macro ever matters, so this can be a condition strictly about how we're building jaxlib. No behavioral changes intended.\n\nPiperOrigin-RevId: 758624138",
        "output": [
            "```diff\nCommit: 67e1d5c2c8baf4face571e637deddd363cd864b9\nDate: 2025-05-14T11:38:27Z\nURL: https://github.com/jax-ml/jax/commit/67e1d5c2c8baf4face571e637deddd363cd864b9\nFiles changed: 2\nAdditions: +30, Deletions: -28\ndiff --git a/jax/BUILD b/jax/BUILD\nindex 16bc9de6935e..5f1cf1670729 100644\n--- a/jax/BUILD\n+++ b/jax/BUILD\n@@ -64,12 +64,26 @@ string_flag(\n )\n \n config_setting(\n-    name = \"enable_jaxlib_build\",\n+    name = \"config_build_jaxlib_true\",\n     flag_values = {\n         \":build_jaxlib\": \"true\",\n     },\n )\n \n+config_setting(\n+    name = \"config_build_jaxlib_false\",\n+    flag_values = {\n+        \":build_jaxlib\": \"false\",\n+    },\n+)\n+\n+config_setting(\n+    name = \"config_build_jaxlib_wheel\",\n+    flag_values = {\n+        \":build_jaxlib\": \"wheel\",\n+    },\n+)\n+\n # The flag controls whether jax should be built by Bazel.\n # If \":build_jax=true\", then jax will be built.\n # If \":build_jax=false\", then jax is not built. It is assumed that the pre-built jax wheel\n@@ -212,7 +226,6 @@ py_library(\n             \":jax\",\n         ],\n         if_not_building = [],\n-        if_not_building_for_cpu = [],\n     ) + py_deps(\"numpy\"),\n )\n \n@@ -229,7 +242,6 @@ py_library(\n             \"//jax/_src/lib\",\n         ],\n         if_not_building = [],\n-        if_not_building_for_cpu = [],\n     ) + py_deps(\"numpy\"),\n )\n \n@@ -243,7 +255,6 @@ py_library(\n             \":test_util\",\n         ],\n         if_not_building = [],\n-        if_not_building_for_cpu = [],\n     ),\n )\n \n@@ -259,7 +270,6 @@ py_library(\n             \":test_util\",\n         ],\n         if_not_building = [],\n-        if_not_building_for_cpu = [],\n     ) + py_deps(\"numpy\"),\n )\n \ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex e739b681a029..a48c44f406f2 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -158,28 +158,20 @@ def if_building_jaxlib(\n         if_building,\n         if_not_building = [\n             \"@pypi//jaxlib\",\n-            \"@pypi//jax_cuda12_plugin\",\n-            \"@pypi//jax_cuda12_pjrt\",\n-        ],\n-        if_not_building_for_cpu = [\n-            \"@pypi//jaxlib\",\n         ]):\n-    \"\"\"Adds jaxlib and jaxlib cuda plugin wheels as dependencies instead of depending on sources.\n+    \"\"\"Adds jaxlib wheels as dependencies instead of depending on sources.\n \n     This allows us to test prebuilt versions of jaxlib wheels against the rest of the JAX codebase.\n \n     Args:\n       if_building: the source code targets to depend on in case we don't depend on the jaxlib wheels\n-      if_not_building: the wheels to depend on including gpu-specific plugins in case of\n-                       gpu-enabled builds\n-      if_not_building_for_cpu: the wheels to depend on in case of cpu-only builds\n+      if_not_building: the wheels to depend on if we are not depending directly on //jaxlib.\n     \"\"\"\n \n     return select({\n-        \"//jax:enable_jaxlib_build\": if_building,\n-        \"//jax_plugins/cuda:disable_jaxlib_for_cpu_build\": if_not_building_for_cpu,\n-        \"//jax_plugins/cuda:disable_jaxlib_for_cuda12_build\": if_not_building,\n-        \"//conditions:default\": [],\n+        \"//jax:config_build_jaxlib_true\": if_building,\n+        \"//jax:config_build_jaxlib_false\": if_not_building,\n+        \"//jax:config_build_jaxlib_wheel\": [],\n     })\n \n def _get_test_deps(deps, backend_independent):\n@@ -192,14 +184,14 @@ def _get_test_deps(deps, backend_independent):\n     Returns:\n       A list of test deps for the given backend.\n         For CPU builds:\n-          If --//jax:enable_jaxlib_build=true, returns pypi test deps.\n-          If --//jax:enable_jaxlib_build=false, returns jaxlib pypi wheel dep and pypi test deps.\n-          If --//jax:enable_jaxlib_build=wheel, returns jaxlib py_import dep and pypi test deps.\n+          If --//jax:build_jaxlib=true, returns pypi test deps.\n+          If --//jax:build_jaxlib=false, returns jaxlib pypi wheel dep and pypi test deps.\n+          If --//jax:build_jaxlib=wheel, returns jaxlib py_import dep and pypi test deps.\n         For GPU builds:\n-          If --//jax:enable_jaxlib_build=true, returns pypi test deps and gpu build deps.\n-          If --//jax:enable_jaxlib_build=false, returns jaxlib, jax-cuda-plugin,\n+          If --//jax:build_jaxlib=true, returns pypi test deps and gpu build deps.\n+          If --//jax:build_jaxlib=false, returns jaxlib, jax-cuda-plugin,\n             jax-cuda-pjrt pypi wheel deps and pypi test deps.\n-          If --//jax:enable_jaxlib_build=wheel, returns jaxlib,\n+          If --//jax:build_jaxlib=wheel, returns jaxlib,\n             jax-cuda-plugin, jax-cuda-pjrt py_import deps and pypi test deps.\n     \"\"\"\n     gpu_build_deps = [\n@@ -234,7 +226,7 @@ def _get_test_deps(deps, backend_independent):\n         gpu_py_import_deps = gpu_py_imports\n \n     return select({\n-        \"//jax:enable_jaxlib_build\": test_deps,\n+        \"//jax:config_build_jaxlib_true\": test_deps,\n         \"//jax_plugins/cuda:disable_jaxlib_for_cpu_build\": jaxlib_pypi_wheel_deps,\n         \"//jax_plugins/cuda:disable_jaxlib_for_cuda12_build\": gpu_pypi_wheel_deps,\n         \"//jax_plugins/cuda:enable_py_import_for_cpu_build\": cpu_py_imports,\n@@ -250,9 +242,9 @@ def _get_jax_test_deps(deps):\n     Returns:\n       A list of jax test deps.\n \n-      If --//jax:enable_jax_build=true, returns jax build deps.\n-      If --//jax:enable_jax_build=false, returns jax pypi wheel dep and transitive pypi test deps.\n-      If --//jax:enable_jax_build=wheel, returns jax py_import dep and transitive pypi test deps.\n+      If --//jax:build_jax=true, returns jax build deps.\n+      If --//jax:build_jax=false, returns jax pypi wheel dep and transitive pypi test deps.\n+      If --//jax:build_jax=wheel, returns jax py_import dep and transitive pypi test deps.\n     \"\"\"\n     jax_build_deps = [d for d in deps if not d.startswith(\"@pypi//\")]\n \n\n```"
        ],
        "from_id": [
            "hawkinsp",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Prepare to make DmaCopyChunk movable.\n\nPiperOrigin-RevId: 758436699",
        "output": [
            "```diff\nCommit: 9b2043e4f2b434d640a9460fc730a7b7300d9d82\nDate: 2025-05-14T00:16:38Z\nURL: https://github.com/jax-ml/jax/commit/9b2043e4f2b434d640a9460fc730a7b7300d9d82\nFiles changed: 1\nAdditions: +3, Deletions: -3\ndiff --git a/jaxlib/py_socket_transfer.cc b/jaxlib/py_socket_transfer.cc\nindex ed2a4f4c204a..114e3c14874d 100644\n--- a/jaxlib/py_socket_transfer.cc\n+++ b/jaxlib/py_socket_transfer.cc\n@@ -132,9 +132,9 @@ class IfrtArrayEntry : public PullTable::Entry {\n             std::min(xfer_size_, arrs_[bid].buf_size - i * xfer_size_));\n         bool is_largest = blob.size + blob.offset == arrs_[bid].buf_size;\n         state_->ScheduleCopy(\n-            blob, [req_id, state, copier_state = state_, is_largest](\n-                      PremappedCopierState* copier_state_ptr, void* buf,\n-                      const DmaCopyChunk& chunk) {\n+            std::move(blob), [req_id, state, copier_state = state_, is_largest](\n+                                 PremappedCopierState* copier_state_ptr,\n+                                 void* buf, const DmaCopyChunk& chunk) {\n               state->Send(\n                   req_id, buf, chunk.offset, chunk.size, is_largest,\n                   [copier_state, buf]() { copier_state->ReturnBuffer(buf); });\n\n```"
        ],
        "from_id": [
            "pschuh",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Global find/replace in jax for s/divisble/divisible.\n\nSpotted this in an error message, then saw there were a few other places with this typo.\n\nPiperOrigin-RevId: 758416484",
        "output": [
            "```diff\nCommit: e2c4a7f53fb95ae30ef2ff74fcb7107e05a94aa9\nDate: 2025-05-13T23:16:50Z\nURL: https://github.com/jax-ml/jax/commit/e2c4a7f53fb95ae30ef2ff74fcb7107e05a94aa9\nFiles changed: 3\nAdditions: +5, Deletions: -4\ndiff --git a/jax/_src/lax/slicing.py b/jax/_src/lax/slicing.py\nindex 9f4645dca975..ad8a2cf0b315 100644\n--- a/jax/_src/lax/slicing.py\n+++ b/jax/_src/lax/slicing.py\n@@ -1353,9 +1353,10 @@ def _get_sharding_for_varying_out_shape(out_shape, operand, name):\n     if (op_sh != out_sh and op_spec is not None and\n         out_sh % _get_sub_spec_size(mesh, op_spec) != 0):\n       raise core.ShardingTypeError(\n-          f\"{name} on sharded dims where out dim ({out_sh}) is not divisble by\"\n+          f\"{name} on sharded dims where out dim ({out_sh}) is not divisible by\"\n           f\" mesh axes ({_get_sub_spec_size(mesh, op_spec)}) with spec\"\n-          f\" ({op_spec}) is not implemented.\")\n+          f\" ({op_spec}) is not implemented.\"\n+      )\n   # TODO(yashkatariya): Returning operand.sharding as is may or may not move\n   # data. So think about how to avoid it which might include creating a new\n   # mesh? For example:\ndiff --git a/jaxlib/mosaic/gpu/runtime.cc b/jaxlib/mosaic/gpu/runtime.cc\nindex cb48a20dc3d5..da7b0159d7b2 100644\n--- a/jaxlib/mosaic/gpu/runtime.cc\n+++ b/jaxlib/mosaic/gpu/runtime.cc\n@@ -115,7 +115,7 @@ void mosaic_gpu_init_tma_desc(CUtensorMap *tma_desc, void *base_addr,\n     if (tma_stride_i % 16 != 0 || tma_stride_i >= static_cast<cuuint64_t>(1)\n                                                       << 40) {\n       fprintf(stderr,\n-              \"Byte strides must be divisble by 16 and less than 2**40, but \"\n+              \"Byte strides must be divisible by 16 and less than 2**40, but \"\n               \"got %ld (item stride = %ld, item size = %ld) at index %ld\\n\",\n               tma_stride_i, strides[rank - 1], elem_bytewidth, rank - i - 2);\n       abort();\ndiff --git a/tests/pallas/pallas_jumble_test.py b/tests/pallas/pallas_jumble_test.py\nindex 509ef08a987f..0a2994a84a8f 100644\n--- a/tests/pallas/pallas_jumble_test.py\n+++ b/tests/pallas/pallas_jumble_test.py\n@@ -354,7 +354,7 @@ def invoke_kernel(x):\n \n     with self.assertRaisesRegex(\n         ValueError,\n-        \"Ragged input shape must be evenly divisble by the grid\"  # noqa: W605\n+        \"Ragged input shape must be evenly divisible by the grid\"  # noqa: W605\n         \" size at the ragged dimension 2\",\n     ):\n       jax.vmap(\n\n```"
        ],
        "from_id": [
            "jkr26",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Deprecate the no-op custom_jvp_call_jaxpr_p import stub.\n\nThe `custom_jvp_call_jaxpr_p` primitive has not been used for a long time, and the existing object is just an import stub. Let's try to clean up some @mattjj TODOs!\n\nSadly, since this lives in the public API, I think we need to do a full deprecation cycle, so let's at least get that started!\n\nPiperOrigin-RevId: 758415957",
        "output": [
            "```diff\nCommit: c7a4e34bea51921b1d359be37ef0b5d59de944ab\nDate: 2025-05-13T23:14:55Z\nURL: https://github.com/jax-ml/jax/commit/c7a4e34bea51921b1d359be37ef0b5d59de944ab\nFiles changed: 3\nAdditions: +24, Deletions: -2\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex 518e854b5bb1..a0c30132c169 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -27,6 +27,10 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n   * `jax.ShapeDtypeStruct` is immutable now. Please use `.update` method to\n     update your `ShapeDtypeStruct` instead of doing in-place updates.\n \n+* Deprecations\n+  * `jax.custom_derivatives.custom_jvp_call_jaxpr_p` is deprecated, and will be\n+    removed in JAX v0.7.0.\n+\n ## JAX 0.6.0 (April 16, 2025)\n \n * Breaking changes\ndiff --git a/jax/custom_derivatives.py b/jax/custom_derivatives.py\nindex b768b687dfad..6674046dd8e8 100644\n--- a/jax/custom_derivatives.py\n+++ b/jax/custom_derivatives.py\n@@ -23,7 +23,7 @@\n   custom_gradient as custom_gradient,\n   custom_jvp as custom_jvp,\n   custom_jvp_call_p as custom_jvp_call_p,\n-  custom_jvp_call_jaxpr_p as custom_jvp_call_jaxpr_p,\n+  custom_jvp_call_jaxpr_p as _custom_jvp_call_jaxpr_p,\n   custom_vjp as custom_vjp,\n   custom_vjp_call_p as custom_vjp_call_p,\n   custom_vjp_primal_tree_values as custom_vjp_primal_tree_values,\n@@ -36,3 +36,22 @@\n   SymbolicZero as SymbolicZero,\n   zero_from_primal as zero_from_primal\n )\n+\n+_deprecations = {\n+    # Added May 12, 2025\n+    \"custom_jvp_call_jaxpr_p\": (\n+      (\"jax.custom_derivatives.custom_jvp_call_jaxpr_p is deprecated, use \"\n+       \"jax.extend.core.primitives.custom_jvp_call_p instead.\"),\n+      _custom_jvp_call_jaxpr_p,\n+    ),\n+}\n+\n+import typing\n+if typing.TYPE_CHECKING:\n+  custom_jvp_call_jaxpr_p = _custom_jvp_call_jaxpr_p\n+else:\n+  from jax._src.deprecations import deprecation_getattr as _deprecation_getattr\n+  __getattr__ = _deprecation_getattr(__name__, _deprecations)\n+  del _deprecation_getattr\n+del typing\n+del _custom_jvp_call_jaxpr_p\ndiff --git a/jax/extend/core/primitives.py b/jax/extend/core/primitives.py\nindex 30350dace637..515dd3e11dcf 100644\n--- a/jax/extend/core/primitives.py\n+++ b/jax/extend/core/primitives.py\n@@ -24,7 +24,6 @@\n \n from jax._src.custom_derivatives import (\n   custom_jvp_call_p as custom_jvp_call_p,\n-  custom_jvp_call_jaxpr_p as custom_jvp_call_jaxpr_p,\n   custom_vjp_call_p as custom_vjp_call_p,\n )\n \n\n```"
        ],
        "from_id": [
            "dfm",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28722 from jakevdp:jaxlib-api-deprecation\n\nPiperOrigin-RevId: 758401882",
        "output": [
            "```diff\nCommit: e6b47535adae06e51efdb0c7f289478450229727\nDate: 2025-05-13T22:36:46Z\nURL: https://github.com/jax-ml/jax/commit/e6b47535adae06e51efdb0c7f289478450229727\nFiles changed: 1\nAdditions: +5, Deletions: -0\ndiff --git a/docs/api_compatibility.md b/docs/api_compatibility.md\nindex 9dca1fc08f50..dda86e2e5d31 100644\n--- a/docs/api_compatibility.md\n+++ b/docs/api_compatibility.md\n@@ -59,6 +59,11 @@ Any API or import path prefixed with an underscore is explicitly private,\n and may change without warning between JAX releases. We are working to move\n all private APIs into `jax._src` to make these expectations more clear.\n \n+### jaxlib\n+Any import path in the `jaxlib` package is considered private, and may change\n+without warning between releases. Some APIs defined in `jaxlib` have public\n+aliases in the `jax` package.\n+\n ### Legacy internal APIs\n In addition, there are several legacy modules that currently expose some\n private APIs without an underscore, including:\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic TPU] Fold reshape (..., M, N, 128) -> (..., M, N * 128) with any tiling and dtype to load.\n\nNow we should support decent number of reshape that match with this pattern. And it is much more efficient to fold reshape (retiling + re-pack) into load.\n\nTake bf16(256, 8, 128) -> bf16(256, 8 * 128) as example, this cl emits 148 bundles and compared to before 630 bundles - about 4.2x speedup.\n\nPiperOrigin-RevId: 758398675",
        "output": [
            "```diff\nCommit: 7887298da20d00af7db58a524beeeebdef4f776a\nDate: 2025-05-13T22:28:02Z\nURL: https://github.com/jax-ml/jax/commit/7887298da20d00af7db58a524beeeebdef4f776a\nFiles changed: 6\nAdditions: +277, Deletions: -15\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu.td b/jaxlib/mosaic/dialect/tpu/tpu.td\nindex 226fc6285192..29ce9c84de07 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu.td\n+++ b/jaxlib/mosaic/dialect/tpu/tpu.td\n@@ -1006,6 +1006,8 @@ def CanonicalizeMosaicPass : Pass<\"tpu-canonicalize-mosaic\", \"::mlir::func::Func\n   let options = [\n     Option<\"hardware_generation\", \"hardware-generation\", \"int\", /*default=*/\"-1\", \"\">,\n     Option<\"compatibility_mode\", \"compatibility-mode\", \"bool\", /*default=*/\"1\", \"\">,\n+    Option<\"lane_count\", \"lane-count\", \"int\", /*default=*/\"128\", \"\">,\n+    Option<\"sublane_count\", \"sublane-count\", \"int\", /*default=*/\"8\", \"\">,\n   ];\n }\n \ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu_dialect.h b/jaxlib/mosaic/dialect/tpu/tpu_dialect.h\nindex 2afaf08f29ed..798386b92744 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu_dialect.h\n+++ b/jaxlib/mosaic/dialect/tpu/tpu_dialect.h\n@@ -74,7 +74,8 @@ std::unique_ptr<OperationPass<func::FuncOp>> createInferMemRefLayoutPass(\n     const TpuTilingFlags &tpu_tiling_flags = {});\n \n std::unique_ptr<OperationPass<func::FuncOp>> createCanonicalizeMosaicPass(\n-    int hardware_generation = -1, bool compatibility_mode = true);\n+    int hardware_generation = -1, bool compatibility_mode = true,\n+    std::array<int64_t, 2> target_shape = {8, 128});\n \n std::unique_ptr<OperationPass<func::FuncOp>> createInferVectorLayoutPass(\n     int hardware_generation = -1,\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\nindex 71e48539f4dd..645e6d615722 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n@@ -14,6 +14,7 @@ limitations under the License.\n ==============================================================================*/\n \n #include <algorithm>\n+#include <array>\n #include <cstdint>\n #include <functional>\n #include <limits>\n@@ -66,6 +67,8 @@ struct CanonicalizeContext {\n   bool compatibility_mode;\n \n   int hardware_generation;\n+\n+  std::array<int64_t, 2> target_shape;\n };\n \n bool need_elementwise_canonicalization(const CanonicalizeContext &ctx,\n@@ -753,6 +756,149 @@ LogicalResult canonicalize_vector_transpose(const CanonicalizeContext &ctx,\n   return success();\n }\n \n+LogicalResult canonicalize_reshape(const CanonicalizeContext &ctx,\n+                                   Operation &raw_op) {\n+  auto op = cast<vector::ShapeCastOp>(raw_op);\n+  // We can canonicalize some reshape(load(x)) -> strided load + ALU ops.\n+  auto src = op.getSource();\n+  auto src_ty = src.getType();\n+  auto tgt_ty = op.getType();\n+  if (auto load_op = src.getDefiningOp<vector::LoadOp>()) {\n+    // Pattern match (..., M, N, 128) -> (..., M, N * 128).\n+    // This reshape can be folded into the load for any dtype and tiling\n+    // as long as the minormost dim is 128 and N is aligned to packing. The\n+    // pseudo code is:\n+    // ```\n+    // src_ref: (M, N, 128) with src_ty\n+    //\n+    // def load_to_reshape(src_ref):\n+    //   b_ref = src_ref.bitcast(i32) # i32[M, N / packing, 128]\n+    //   r_ref = b_ref.reshape(M * N / packing, 128)\n+    //   chunks = []\n+    //   for i in range(N / packing):\n+    //     v = r_ref[i::N / packing, :] # i32[M, 128]\n+    //     for j in range(packing):\n+    //       chunk = v >> (j * bitwidth)\n+    //       chunks.append(chunk)\n+    //   res = concat(chunks, axis=-1) # i32[M, N * 128]\n+    //   # int_src_ty refers to int type with the same bitwidth as src_ty.\n+    //   res = res.astype(int_src_ty) # Trigger i32 -> int_src_ty packing.\n+    //   return bitcast(res, src_ty) # src_ty[M, N * 128]\n+    // ```\n+    // TODO(jevinjiang): we can extend this to support folding more dims to last\n+    // dim not just last 2 dims.\n+    auto bitwidth = src_ty.getElementTypeBitWidth();\n+    auto packing = 32 / bitwidth;\n+    if (packing <= 0) {\n+      return op.emitOpError(\"Unsupported bitwidth = \") << bitwidth;\n+    }\n+    // Memref bitcast is not supported if HW generation is below 4. We don't\n+    // return failure because we will rely on vector reshape.\n+    if ((ctx.hardware_generation < 4 && packing > 1) ||\n+        (ctx.hardware_generation == 4 && packing > 2)) {\n+      return success();\n+    }\n+    auto ref = load_op.getBase();\n+    auto indices = load_op.getIndices();\n+    auto ref_shape = ref.getType().getShape();\n+    auto src_shape = src_ty.getShape();\n+    auto tgt_shape = tgt_ty.getShape();\n+    int ref_rank = ref_shape.size();\n+    int src_rank = src_shape.size();\n+    int tgt_rank = tgt_shape.size();\n+    if (ref_rank != src_rank) {\n+      return op.emitOpError(\"Loaded vector rank and memref rank mismatch\");\n+    }\n+    // Check the memref's eligibility.\n+    if (!isContiguousMemref(ref) || ref_rank <= 2 ||\n+        // TODO(jevinjiang): add support for partial load on last 2 dims where\n+        // last 2 indices are not necessarily 0 or load shape is not full.\n+        getIntConst(indices[ref_rank - 1]) != 0 ||\n+        getIntConst(indices[ref_rank - 2]) != 0 ||\n+        ref_shape[ref_rank - 1] != src_shape[src_rank - 1] ||\n+        ref_shape[ref_rank - 2] != src_shape[src_rank - 2]) {\n+      return success();\n+    }\n+    // Check the reshape's eligibility.\n+    if (src_rank != tgt_rank + 1 || src_shape[src_rank - 2] % packing != 0 ||\n+        src_shape[src_rank - 1] != ctx.target_shape[1] ||\n+        src_shape[src_rank - 2] * src_shape[src_rank - 1] !=\n+            tgt_shape[tgt_rank - 1]) {\n+      return success();\n+    }\n+    // At this point, the pattern is matched.\n+    ImplicitLocOpBuilder builder(op->getLoc(), op.getOperation());\n+    auto loc = op.getLoc();\n+    // First, we bitcast and reshape src ref from (..., M, N, 128) to\n+    // i32(..., M * N / packing, 128).\n+    SmallVector<int64_t> bitcast_shape(ref_shape);\n+    // TODO(jevinjiang): once we have memref pad op, we can use ceiling\n+    // division to ref_shape[ref_rank - 2] and packing to get sublane_cnt.\n+    CHECK_EQ(ref_shape[ref_rank - 2] % packing, 0);\n+    auto i32_2nd_minor_size = ref_shape[ref_rank - 2] / packing;\n+    bitcast_shape[ref_rank - 2] = i32_2nd_minor_size;\n+    auto i32_ref = builder.create<tpu::MemRefBitcastOp>(\n+        MemRefType::get(bitcast_shape, builder.getI32Type()), ref);\n+\n+    SmallVector<int64_t> reshape_shape(ref_shape.begin(),\n+                                       ref_shape.begin() + tgt_rank);\n+    reshape_shape[tgt_rank - 1] = ctx.target_shape[1];\n+    reshape_shape[tgt_rank - 2] = ref_shape[ref_rank - 3] * i32_2nd_minor_size;\n+    auto reshape_ref = builder.create<tpu::MemRefReshapeOp>(\n+        MemRefType::get(reshape_shape, builder.getI32Type()), i32_ref);\n+\n+    // We also need to transform the indices while transforming the memref.\n+    SmallVector<Value> new_indices(indices.begin(), indices.begin() + tgt_rank);\n+    new_indices[tgt_rank - 1] = IdxConst(0, builder, loc);\n+    new_indices[tgt_rank - 2] = builder.create<arith::MulIOp>(\n+        builder.getIndexType(), indices[ref_rank - 3],\n+        IdxConst(i32_2nd_minor_size, builder, loc));\n+    // Then, we strided load the bitcasted ref by stride (N / packing).\n+    int stride = i32_2nd_minor_size;\n+    // Expect to hold src_shape[src_rank - 2] number of chunks which have the\n+    // shape (..., src_shape[src_rank - 3], 128) and wait to be concatenated\n+    // along the last dim.\n+    SmallVector<Value> chunks(src_shape[src_rank - 2]);\n+    SmallVector<int64_t> chunk_shape(tgt_shape);\n+    chunk_shape[tgt_rank - 1] = ctx.target_shape[1];\n+    SmallVector<int32_t> strides(tgt_rank, 1);\n+    strides[tgt_rank - 2] = stride;\n+    auto tgt_2nd_minor_idx = new_indices[tgt_rank - 2];\n+    for (int i = 0; i < stride; ++i) {\n+      new_indices[tgt_rank - 2] = builder.create<arith::AddIOp>(\n+          builder.getIndexType(), tgt_2nd_minor_idx, IdxConst(i, builder, loc));\n+      auto chunk = builder.create<tpu::StridedLoadOp>(\n+          VectorType::get(chunk_shape, builder.getI32Type()), reshape_ref,\n+          new_indices, strides);\n+      for (int j = 0; j < packing; ++j) {\n+        int idx = i * packing + j;\n+        chunks[idx] = builder.create<arith::ShRUIOp>(\n+            chunk.getType(), chunk,\n+            I32Const(j * bitwidth, chunk_shape, builder, loc));\n+      }\n+    }\n+    // Concatenate the chunks along the last dim to get i32(..., M, N * 128).\n+    CHECK_GT(chunks.size(), 0);\n+    Value i32_tgt = chunks[0];\n+    if (chunks.size() > 1) {\n+      i32_tgt = builder.create<tpu::ConcatenateOp>(\n+          VectorType::get(tgt_shape, builder.getI32Type()), chunks,\n+          /*dimension=*/tgt_rank - 1);\n+    }\n+    Value tgt = i32_tgt;\n+    // Convert to target dtype.\n+    if (packing > 1) {\n+      tgt = builder.create<arith::TruncIOp>(\n+          VectorType::get(tgt_shape, builder.getIntegerType(bitwidth)),\n+          i32_tgt);\n+    }\n+    tgt = builder.create<arith::BitcastOp>(tgt_ty, tgt);\n+    op.replaceAllUsesWith(tgt);\n+    op.erase();\n+  }\n+  return success();\n+}\n+\n using canonicalize_rule_type =\n     std::function<LogicalResult(const CanonicalizeContext &ctx, Operation &op)>;\n \n@@ -764,6 +910,7 @@ const llvm::StringMap<canonicalize_rule_type> &rules() {\n       {vector::MultiDimReductionOp::getOperationName(),\n        canonicalize_multi_dim_reduction},\n       {vector::TransposeOp::getOperationName(), canonicalize_vector_transpose},\n+      {vector::ShapeCastOp::getOperationName(), canonicalize_reshape},\n       {arith::SelectOp::getOperationName(), canonicalize_select},\n       {arith::FPToSIOp::getOperationName(), canonicalize_fptosi},\n       {arith::SIToFPOp::getOperationName(), canonicalize_sitofp},\n@@ -808,12 +955,15 @@ bool need_elementwise_canonicalization(const CanonicalizeContext &ctx,\n \n class MosaicCanonicalizer {\n  public:\n-  MosaicCanonicalizer(int hardware_generation, bool compatibility_mode)\n+  MosaicCanonicalizer(int hardware_generation, bool compatibility_mode,\n+                      std::array<int64_t, 2> target_shape)\n       : hardware_generation_(hardware_generation),\n-        compatibility_mode_(compatibility_mode) {}\n+        compatibility_mode_(compatibility_mode),\n+        target_shape_(target_shape) {}\n \n   int hardware_generation_;\n   bool compatibility_mode_;\n+  std::array<int64_t, 2> target_shape_;\n \n   LogicalResult canonicalize(func::FuncOp op) {\n     if (!op.getBody().hasOneBlock()) {\n@@ -834,7 +984,8 @@ class MosaicCanonicalizer {\n   }\n \n   LogicalResult canonicalizeOp(Operation &any_op) {\n-    CanonicalizeContext ctx({compatibility_mode_, hardware_generation_});\n+    CanonicalizeContext ctx(\n+        {compatibility_mode_, hardware_generation_, target_shape_});\n     // We must iterate over the op first, because canonicalization can cause\n     // us to .erase() an op, and accessing getRegions on it after is not sound.\n     // Invariant - top level ops with regions may never be invalidated.\n@@ -859,14 +1010,18 @@ class MosaicCanonicalizer {\n \n struct CanonicalizeMosaicPass\n     : public impl::CanonicalizeMosaicPassBase<CanonicalizeMosaicPass> {\n-  CanonicalizeMosaicPass(int hardware_generation_p, bool compatibility_mode_p)\n+  CanonicalizeMosaicPass(int hardware_generation_p, bool compatibility_mode_p,\n+                         std::array<int64_t, 2> target_shape)\n       : compatibility_mode_(compatibility_mode_p) {\n     this->hardware_generation = hardware_generation_p;\n+    this->sublane_count = target_shape[0];\n+    this->lane_count = target_shape[1];\n   }\n \n   void runOnOperation() override {\n     func::FuncOp func = getOperation();\n-    MosaicCanonicalizer vlc(hardware_generation, compatibility_mode_);\n+    MosaicCanonicalizer vlc(hardware_generation, compatibility_mode_,\n+                            {sublane_count, lane_count});\n     if (vlc.canonicalize(func).failed()) {\n       signalPassFailure();\n     }\n@@ -878,9 +1033,10 @@ struct CanonicalizeMosaicPass\n }  // namespace\n \n std::unique_ptr<OperationPass<func::FuncOp>> createCanonicalizeMosaicPass(\n-    int hardware_generation, bool compatibility_mode) {\n-  return std::make_unique<CanonicalizeMosaicPass>(hardware_generation,\n-                                                  compatibility_mode);\n+    int hardware_generation, bool compatibility_mode,\n+    std::array<int64_t, 2> target_shape) {\n+  return std::make_unique<CanonicalizeMosaicPass>(\n+      hardware_generation, compatibility_mode, target_shape);\n }\n \n }  // namespace mlir::tpu\ndiff --git a/jaxlib/mosaic/dialect/tpu/util.cc b/jaxlib/mosaic/dialect/tpu/util.cc\nindex bb42c678bbf6..b562f81ad534 100644\n--- a/jaxlib/mosaic/dialect/tpu/util.cc\n+++ b/jaxlib/mosaic/dialect/tpu/util.cc\n@@ -27,7 +27,9 @@ limitations under the License.\n #include \"llvm/ADT/STLExtras.h\"\n #include \"llvm/Support/MathExtras.h\"\n #include \"llvm/Support/raw_ostream.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/IR/Attributes.h\"\n+#include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinTypes.h\"\n #include \"mlir/IR/Value.h\"\n@@ -159,6 +161,17 @@ bool canReinterpretToUntiledMemref(TypedValue<MemRefType> tiled_memref,\n          *(tiled_layout.getTileStrides().end() - 2) == 1;\n }\n \n+bool isContiguousMemref(TypedValue<MemRefType> memref) {\n+  auto memref_ty = getMemRefType(memref);\n+  if (auto tiled_layout =\n+          dyn_cast<tpu::TiledLayoutAttr>(memref_ty.getLayout())) {\n+    auto contiguous_tile_strides = ComputeTileStrides(\n+        memref_ty, tiled_layout.getTiles().front().dimensions());\n+    return contiguous_tile_strides == tiled_layout.getTileStrides();\n+  }\n+  return true;\n+}\n+\n bool HasMemorySpace(MemRefType ty, tpu::MemorySpace space) {\n   auto memory_space =\n       dyn_cast_or_null<tpu::MemorySpaceAttr>(ty.getMemorySpace());\n@@ -278,4 +291,14 @@ void setLayout(Operation *op, ArrayRef<Layout> in, ArrayRef<Layout> out) {\n   setInLayout(op, in);\n   setOutLayout(op, out);\n }\n+\n+std::optional<int64_t> getIntConst(Value v) {\n+  if (auto const_op = v.getDefiningOp<arith::ConstantOp>()) {\n+    if (auto cst_attr = dyn_cast<IntegerAttr>(const_op.getValue())) {\n+      return cst_attr.getValue().getSExtValue();\n+    }\n+  }\n+  return std::nullopt;\n+}\n+\n }  // namespace mlir::tpu\ndiff --git a/jaxlib/mosaic/dialect/tpu/util.h b/jaxlib/mosaic/dialect/tpu/util.h\nindex b9aea1b087dc..000cb4411e62 100644\n--- a/jaxlib/mosaic/dialect/tpu/util.h\n+++ b/jaxlib/mosaic/dialect/tpu/util.h\n@@ -195,11 +195,6 @@ ArrayRef<std::remove_const_t<T>> toArrayRef(absl::Span<T> span) {\n   return ArrayRef<std::remove_const_t<T>>(span.data(), span.size());\n }\n \n-inline arith::ConstantOp IdxConst(int64_t idx, OpBuilder &builder,\n-                                  Location loc) {\n-  return builder.create<arith::ConstantOp>(loc, builder.getIndexType(),\n-                                           builder.getIndexAttr(idx));\n-}\n \n // Debug only util.\n template <typename T>\n@@ -242,6 +237,8 @@ bool canReinterpretToUntiledMemref(TypedValue<MemRefType> tiled_memref,\n                                    const std::array<int64_t, 2> &target_shape,\n                                    bool allow_minormost_padding = false);\n \n+bool isContiguousMemref(TypedValue<MemRefType> memref);\n+\n // Determines whether the given MemRefType has the given memory space.\n bool HasMemorySpace(MemRefType ty, tpu::MemorySpace space);\n \n@@ -264,6 +261,30 @@ void setLayout(Operation *op, Layout in, Layout out);\n void setLayout(Operation *op, ArrayRef<Layout> in, Layout out);\n void setLayout(Operation *op, Layout in, ArrayRef<Layout> out);\n void setLayout(Operation *op, ArrayRef<Layout> in, ArrayRef<Layout> out);\n+\n+// Helper functions to create constants.\n+inline arith::ConstantOp IdxConst(int64_t idx, OpBuilder &builder,\n+                                  Location loc) {\n+  return builder.create<arith::ConstantOp>(loc, builder.getIndexType(),\n+                                           builder.getIndexAttr(idx));\n+}\n+\n+inline arith::ConstantOp I32Const(int32_t value, OpBuilder &builder,\n+                                  Location loc) {\n+  return builder.create<arith::ConstantOp>(loc, builder.getI32Type(),\n+                                           builder.getI32IntegerAttr(value));\n+}\n+\n+inline arith::ConstantOp I32Const(int32_t value, ArrayRef<int64_t> shape,\n+                                  OpBuilder &builder, Location loc) {\n+  return builder.create<arith::ConstantOp>(\n+      loc, DenseElementsAttr::get(\n+               VectorType::get(shape, builder.getI32Type()),\n+               builder.getIntegerAttr(builder.getI32Type(), value)));\n+}\n+\n+// TODO(jevinjiang): consolidate this with getIntConst in apply-vector-layout.\n+std::optional<int64_t> getIntConst(Value v);\n }  // namespace mlir::tpu\n \n #endif  // THIRD_PARTY_PY_JAX_JAXLIB_MOSAIC_DIALECT_TPU_UTIL_H_\ndiff --git a/tests/pallas/tpu_ops_test.py b/tests/pallas/tpu_ops_test.py\nindex 1fb0bc24701b..de87126ebd3f 100644\n--- a/tests/pallas/tpu_ops_test.py\n+++ b/tests/pallas/tpu_ops_test.py\n@@ -38,16 +38,36 @@\n jax.config.parse_flags_with_absl()\n jtu.setup_hypothesis(max_examples=100)\n \n-_JAX_DTYPES = (\n+_JAX_DTYPES_NO_BOOL = (\n     jnp.float32,\n     jnp.bfloat16,\n     jnp.int32,\n     jnp.int16,\n     jnp.int8,\n+    jnp.int4,\n+    jnp.float8_e5m2,\n+)\n+\n+_JAX_DTYPES = (\n+    *_JAX_DTYPES_NO_BOOL,\n     jnp.bool_,\n )\n \n \n+def rand(\n+    shape: tuple[int, ...], dtype: np.dtype | jnp.dtype, seed: int = 1234\n+) -> np.ndarray:\n+  \"\"\"A helper function to generate random data for testing.\"\"\"\n+  rng = np.random.Generator(np.random.Philox(counter=0, key=seed))\n+  if jnp.issubdtype(dtype, jnp.floating):\n+    return rng.normal(size=shape).astype(dtype)\n+  if jnp.issubdtype(dtype, jnp.integer):\n+    return rng.integers(\n+        jnp.iinfo(dtype).min, jnp.iinfo(dtype).max, shape, dtype=np.int32\n+    ).astype(dtype)\n+  raise NotImplementedError(f\"Unsupported random data generation for {dtype=}\")\n+\n+\n class PallasBaseTest(jtu.JaxTestCase):\n   INTERPRET = False\n \n@@ -511,6 +531,45 @@ def kernel(src, tgt):\n         output[tuple(slice(0, d) for d in src_shape)], x\n     )\n \n+  # TODO(jevinjiang): we need to support strided load for bool.\n+  @parameterized.product(dtype=_JAX_DTYPES_NO_BOOL)\n+  @hp.given(\n+      slice_start=hps.integers(0, 3),\n+      slice_size=hps.integers(1, 3),\n+      m=hps.integers(1, 32),\n+      # Need to make sure the 2nd minor has no padding.\n+      n=hps.sampled_from([1, 2, 4, 8, 16, 24, 32]),\n+  )\n+  @hp.settings(max_examples=20)  # 20 examples for each dtype.\n+  def test_load_to_reshape(self, dtype, slice_start, slice_size, m, n):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 15):\n+      self.skipTest(\"Requires libtpu built after 2025-05-15\")\n+    bitwidth = pallas_utils.dtype_bitwidth(dtype)\n+    if jtu.get_tpu_version() < 4 and bitwidth != 32:\n+      self.skipTest(\"Requires TPUv4+ for non-32-bit types\")\n+    if jtu.get_tpu_version() == 4 and bitwidth <= 8:\n+      self.skipTest(\"Int8 is not supported on this target\")\n+    packing = 32 // bitwidth\n+    n *= packing\n+    slices = (\n+        slice(slice_start, slice_start + slice_size),\n+        slice(slice_start, slice_start + m),\n+        slice(None),\n+        slice(None),\n+    )\n+    inp_shape = (8, 64, n, 128)\n+    out_shape = (slice_size, m, n * 128)\n+\n+    def kernel(inp_ref, out_ref):\n+      inp = inp_ref[slices]\n+      out_ref[...] = inp.reshape(out_shape)\n+\n+    inp = rand(inp_shape, dtype, seed=1234)\n+    run = pl.pallas_call(kernel, jax.ShapeDtypeStruct(out_shape, dtype))\n+    output = run(inp)\n+    expected = inp[slices].reshape(out_shape)\n+    np.testing.assert_array_equal(output, expected)\n+\n \n @jtu.thread_unsafe_test_class()  # hypothesis is not thread safe\n class OpsInterpretTest(OpsTest):\n\n```"
        ],
        "from_id": [
            "bythew3i",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28661 from jakevdp:jax-array-jep\n\nPiperOrigin-RevId: 758381022",
        "output": [
            "```diff\nCommit: 0c10daedea6e7e0fc4f164e8fe515b7f1c379bb8\nDate: 2025-05-13T21:38:49Z\nURL: https://github.com/jax-ml/jax/commit/0c10daedea6e7e0fc4f164e8fe515b7f1c379bb8\nFiles changed: 2\nAdditions: +215, Deletions: -0\ndiff --git a/docs/jep/28661-jax-array-protocol.md b/docs/jep/28661-jax-array-protocol.md\nnew file mode 100644\nindex 000000000000..e05d69d2822d\n--- /dev/null\n+++ b/docs/jep/28661-jax-array-protocol.md\n@@ -0,0 +1,214 @@\n+# JEP 28661: Supporting the `__jax_array__` protocol\n+\n+[@jakevdp](http://github.com/jakevdp), *May 2025*\n+\n+An occasional user request is for the ability to define custom array-like objects that\n+work with jax APIs. JAX currently has a partial implementation of a mechanism that does\n+this via a `__jax_array__` method defined on the custom object. This was never intended\n+to be a load-bearing public API (see the discussion at {jax-issue}`#4725`), but has\n+become essential to packages like Keras and flax, which explicitly document the ability\n+to use their custom array objects with jax functions. This JEP proposes a design for\n+full, documented support of the `__jax_array__` protocol.\n+\n+## Levels of array extensibility\n+Requests for extensibility of JAX arrays come in a few flavors:\n+\n+### Level 1 Extensibility: polymorphic inputs\n+What I’ll call \"Level 1\" extensibility is the desire that JAX APIs accept polymorphic inputs.\n+That is, a user desires behavior like this:\n+\n+```python\n+class CustomArray:\n+  data: numpy.ndarray\n+  ...\n+\n+x = CustomArray(np.arange(5))\n+result = jnp.sin(x)  # Converts `x` to JAX array and returns a JAX array\n+```\n+\n+Under this extensibility model, JAX functions would accept CustomArray objects as inputs,\n+implicitly converting them to `jax.Array` objects for the sake of computation.\n+This is similar to the functionality offered by NumPy via the `__array__` method, and in\n+JAX (in many but not all cases) via the `__jax_array__` method.\n+\n+This is the mode of extensibility that has been requested by the maintainers of `flax.nnx`\n+and others. The current implementation is also used by JAX internally for the case of\n+symbolic dimensions.\n+\n+### Level 2 extensibility: polymorphic outputs\n+What I’ll call \"Level 2\" extensibility is the desire that JAX APIs should not only accept\n+polymorphic inputs, but also wrap outputs to match the class of the input.\n+That is, a user desires behavior like this:\n+\n+```python\n+class CustomArray:\n+  data: numpy.ndarray\n+  ...\n+\n+x = CustomArray(np.arange(5))\n+result = jnp.sin(x)  # returns a new CustomArray\n+```\n+\n+Under this extensibility model, JAX functions would not only accept custom objects\n+as inputs, but have some protocol to determine how to correctly re-wrap outputs with\n+the same class. In NumPy, this sort of functionality is offered in varying degrees by\n+the special `__array_ufunc__`, `__array_wrap__`, and `__array_function__` protocols,\n+which allow user-defined objects to customize how NumPy API functions operate on\n+arbitrary inputs and map input types to outputs.\n+JAX does not currently have any equivalent to these interfaces in NumPy.\n+\n+This is the mode of extensibility that has been requested by the maintainers of `keras`,\n+among others.\n+\n+### Level 3 extensibility: subclassing `Array`\n+\n+What I’ll call \"Level 3\" extensibility is the desire that the JAX array object itself\n+could be subclassable. NumPy provides some APIs that allow this\n+(see [Subclassing ndarray](https://numpy.org/devdocs/user/basics.subclassing.html)) but\n+this sort of approach would take some extra thought in JAX due to the need for\n+representing array objects abstractly via tracing.\n+\n+This mode of extensibility has occasionally been requested by users who want to add\n+special metadata to JAX arrays, such as units of measurement.\n+\n+## Synopsis\n+\n+For the sake of this proposal, we will stick with the simplest, level 1 extensibility\n+model. The proposed interface is the one currently non-uniformly supported by a number\n+of JAX APIs, the `__jax_array__` method. Its usage looks something like this:\n+\n+```python\n+import jax\n+import jax.numpy as jnp\n+import numpy as np\n+\n+class CustomArray:\n+  data: np.ndarray\n+\n+  def __init__(self, data: np.ndarray):\n+    self.data = data\n+\n+  def __jax_array__(self) -> jax.Array:\n+    return jnp.asarray(self.data)\n+\n+arr = CustomArray(np.arange(5))\n+result = jnp.multiply(arr, 2)\n+print(repr(result))\n+# Array([0, 2, 4, 6, 8], dtype=int32)\n+```\n+\n+We may revisit other extensibility levels in the future.\n+\n+## Design challenges\n+\n+JAX presents some interesting design challenges related to this kind of extensibility,\n+which have not been fully explored previously. We’ll discuss them in turn here:\n+\n+### Priority of `__jax_array__` vs. PyTree flattening\n+JAX already has a supported mechanism for registering custom objects, namely pytree\n+registration (see [Extending pytrees](https://docs.jax.dev/en/latest/pytrees.html#extending-pytrees)).\n+If we also support __jax_array__, which one should take precedence?\n+\n+To put this more concretely, what should be the result of this code?\n+\n+```python\n+@jax.jit\n+def f(x):\n+  print(\"is JAX array:\", isinstance(x, jax.Array))\n+\n+f(CustomArray(...))\n+```\n+\n+If we choose to prioritize `__jax_array__` at the JIT boundary, then the output of this\n+function would be:\n+```\n+is JAX array: True\n+```\n+That is, at the JIT boundary, the `CustomArray` object would be converted into a\n+`__jax_array__`, and its shape and dtype would be used to construct a standard JAX\n+tracer for the function.\n+\n+If we choose to prioritize pytree flattening at the JIT boundary, then the output of\n+this function would be:\n+```\n+type(x)=CustomArray\n+```\n+That is, at the JIT boundary, the `CustomArray` object is flattened, and then unflattened\n+before being passed to the JIT-compiled function for tracing. If `CustomArray` has been\n+registered as a pytree, it will generally contain traced arrays as its attributes, and\n+when x is passed to any JAX API that supports `__jax_array__`, these traced attributes\n+will be converted to a single traced array according to the logic specified in the method.\n+\n+There are deeper consequences here for how other transformations like vmap and grad work\n+when encountering custom objects: for example, if we prioritize pytree flattening, vmap\n+would operate over the dimensions of the flattened contents of the custom object, while\n+if we prioritize `__jax_array__`, vmap would operate over the converted array dimensions.\n+\n+This also has consequences when it comes to JIT invariance: consider a function like this:\n+```python\n+def f(x):\n+  if isinstance(x, CustomArray):\n+    return x.custom_method()\n+  else:\n+    # do something else\n+    ...\n+\n+result1 = f(x)\n+result2 = jax.jit(f)(x)\n+```\n+If `jit` consumes `x` via pytree flattening, the results should agree for a well-specified\n+flattening rule. If `jit` consumes `x` via `__jax_array__`, the results will differ because\n+`x` is no longer a CustomArray within the JIT-compiled version of the function.\n+\n+#### Synopsis\n+As of JAX v0.6.0, transformations prioritize `__jax_array__` when it is available. This status\n+quo can lead to confusion around lack of JIT invariance, and the current implementation in practice\n+leads to subtle bugs in the case of automatic differentiation, where the forward and backward pass\n+do not treat inputs consistently.\n+\n+Because the pytree extensibility mechanism already exists for the case of customizing\n+transformations, it seems most straightforward if transformations act only via this\n+mechanism: that is, **we propose to remove `__jax_array__` parsing during abstractification.**\n+This approach will preserve object identity through transformations, and give the user the\n+most possible flexibility. If the user wants to opt-in to array conversion semantics, that\n+is always possible by explicitly casting their input via jnp.asarray, which will trigger the \n+`__jax_array__` protocol.\n+\n+### Which APIs should support `__jax_array__`?\n+JAX has a number of different levels of API, from the level of explicit primitive binding\n+(e.g. `jax.lax.add_p.bind(x, y)`) to the `jax.lax` APIs (e.g. `jax.lax.add(x, y)`) to the\n+`jax.numpy` APIs (e.g. `jax.numpy.add(x, y)`). Which of these API categories should handle\n+implicit conversion via `__jax_array__`?\n+\n+In order to limit the scope of the change and the required testing, I propose that `__jax_array__`\n+only be explicitly supported in `jax.numpy` APIs: after all, it is inspired by the` __array__`\n+protocol which is supported by the NumPy package. We could always expand this in the future to\n+`jax.lax` APIs if needed.\n+\n+This is in line with the current state of the package, where `__jax_array__` handling is mainly\n+within the input validation utilities used by `jax.numpy` APIs.\n+\n+## Implementation\n+With these design choices in mind, we plan to implement this as follows:\n+\n+- **Adding runtime support to `jax.numpy`**: This is likely the easiest part, as most\n+  `jax.numpy` functions use a common internal utility (`ensure_arraylike`) to validate\n+  inputs and convert them to array. This utility already supports `__jax_array__`, and\n+  so most jax.numpy APIs are already compliant.\n+- **Adding test coverage**:  To ensure compliance across the APIs, we should add a new\n+  test scaffold that calls every `jax.numpy` API with custom inputs and validates correct\n+  behavior.\n+- **Deprecating `__jax_array__` during abstractification**: Currently JAX's abstractification\n+  pass, used in `jit` and other transformations, does parse the `__jax_array__` protocol,\n+  and this is not the behavior we want long-term. We need to deprecate this behavior, and\n+  ensure that downstream packages that rely on it can move toward pytree registration or\n+  explicit array conversion where necessary.\n+- **Adding type annotations**: the type interface for jax.numpy functions is in\n+  `jax/numpy/__init__.pyi`, and we’ll need to change each input type from `ArrayLike` to\n+  `ArrayLike | SupportsJAXArray`, where the latter is a protocol with a `__jax_array__`\n+  method. We cannot add this directly to the `ArrayLike` definition, because `ArrayLike`\n+  is used in contexts where `__jax_array__` should not be supported.\n+- **Documentation**: once the above support is added, we should add a documentation section\n+  on array extensibility that outlines exactly what to expect regarding the `__jax_array__`\n+  protocol, with examples of how it can be used in conjunction with pytree registration\n+  in order to effectively work with user-defined types.\ndiff --git a/docs/jep/index.rst b/docs/jep/index.rst\nindex 1c4ecbb3411f..2ba85a5f4a8d 100644\n--- a/docs/jep/index.rst\n+++ b/docs/jep/index.rst\n@@ -52,6 +52,7 @@ Then create a pull request that adds a file named\n   17111: Efficient transposition of `shard_map` (and other maps) <17111-shmap-transpose>\n   18137: Scope of JAX NumPy & SciPy Wrappers <18137-numpy-scipy-scope>\n   25516: Effort-based versioning <25516-effver>\n+  28661: Supporting the `__jax_array__` protocol <28661-jax-array-protocol>\n \n \n Several early JEPs were converted in hindsight from other documentation,\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Avoid duplication of Bazel dependencies between //jax/_src/lib and //jaxlib.\n\nPut all dependencies on //jaxlib and make //jax/_src/lib a pure forwarding rule.\n\nPiperOrigin-RevId: 758378774",
        "output": [
            "```diff\nCommit: b2549761e1cdee37b4d990bb28e97fdab7a99fe7\nDate: 2025-05-13T21:33:37Z\nURL: https://github.com/jax-ml/jax/commit/b2549761e1cdee37b4d990bb28e97fdab7a99fe7\nFiles changed: 2\nAdditions: +35, Deletions: -46\ndiff --git a/jax/_src/lib/BUILD b/jax/_src/lib/BUILD\nindex 4bbc861432aa..e0b5ea607501 100644\n--- a/jax/_src/lib/BUILD\n+++ b/jax/_src/lib/BUILD\n@@ -40,30 +40,5 @@ py_library_providing_imports_info(\n         \"//jax:version\",\n     ] + if_building_jaxlib([\n         \"//jaxlib\",\n-        \"//jaxlib/mosaic/python:gpu_dialect\",\n-        \"//jaxlib/mosaic/python:tpu_dialect\",\n-        \"//jaxlib:cpu_feature_guard\",\n-        \"//jaxlib:utils\",\n-        \"//jaxlib:weakref_lru_cache\",\n-        \"//jaxlib:xla_client\",\n-        \"//jaxlib:_jax\",\n-        \"//jaxlib/triton\",\n-        \"//jaxlib/mlir/_mlir_libs:register_jax_dialects\",\n-        \"//jaxlib/mlir:arithmetic_dialect\",\n-        \"//jaxlib/mlir:builtin_dialect\",\n-        \"//jaxlib/mlir:chlo_dialect\",\n-        \"//jaxlib/mlir:control_flow_dialect\",\n-        \"//jaxlib/mlir:func_dialect\",\n-        \"//jaxlib/mlir:ir\",\n-        \"//jaxlib/mlir:math_dialect\",\n-        \"//jaxlib/mlir:memref_dialect\",\n-        \"//jaxlib/mlir:mhlo_dialect\",\n-        \"//jaxlib/mlir:pass_manager\",\n-        \"//jaxlib/mlir:scf_dialect\",\n-        \"//jaxlib/mlir:sdy_dialect\",\n-        \"//jaxlib/mlir:sparse_tensor_dialect\",\n-        \"//jaxlib/mlir:stablehlo_dialect\",\n-        \"//jaxlib/mlir:vector_dialect\",\n-        \"@xla//xla/python:_profiler\",\n     ]),\n )\ndiff --git a/jaxlib/BUILD b/jaxlib/BUILD\nindex e0fb2699a25e..add6dbd7d92a 100644\n--- a/jaxlib/BUILD\n+++ b/jaxlib/BUILD\n@@ -22,7 +22,6 @@ load(\n     \"nanobind_extension\",\n     \"proto_library\",\n     \"py_deps\",\n-    \"py_library_providing_imports_info\",\n     \"py_strict_test\",\n     \"pytype_library\",\n     \"pytype_strict_library\",\n@@ -49,33 +48,17 @@ package_group(\n     ],\n )\n \n-py_library_providing_imports_info(\n+pytype_strict_library(\n     name = \"jaxlib\",\n-    srcs = [\n-        \"cpu_sparse.py\",\n-        \"gpu_common_utils.py\",\n-        \"gpu_linalg.py\",\n-        \"gpu_prng.py\",\n-        \"gpu_rnn.py\",\n-        \"gpu_solver.py\",\n-        \"gpu_sparse.py\",\n-        \"gpu_triton.py\",\n-        \"hlo_helpers.py\",\n-        \"init.py\",\n-        \"lapack.py\",\n-        \"plugin_support.py\",\n-        \"xla_client.py\",\n-        \":version\",\n-    ],\n     data = [\":ffi_headers\"],\n-    lib_rule = pytype_library,\n     deps = [\n+        \":_jax\",\n         \":cpu_feature_guard\",\n         \":jax\",\n+        \":jaxlib_files\",\n         \":utils\",\n         \":weakref_lru_cache\",\n-        \"//jaxlib:_jax\",\n-        \"//jaxlib:xla_client\",\n+        \":xla_client\",\n         \"//jaxlib/cpu:_lapack\",\n         \"//jaxlib/cpu:_sparse\",\n         \"//jaxlib/mlir\",\n@@ -98,8 +81,39 @@ py_library_providing_imports_info(\n         \"//jaxlib/mlir:sparse_tensor_dialect\",\n         \"//jaxlib/mlir:stablehlo_dialect\",\n         \"//jaxlib/mlir:vector_dialect\",\n+        \"//jaxlib/mlir/_mlir_libs:register_jax_dialects\",\n         \"//jaxlib/mosaic\",\n+        \"//jaxlib/mosaic/python:gpu_dialect\",\n+        \"//jaxlib/mosaic/python:tpu_dialect\",\n         \"//jaxlib/triton\",\n+        \"@xla//xla/python:_profiler\",\n+    ],\n+)\n+\n+pytype_library(\n+    name = \"jaxlib_files\",\n+    srcs = [\n+        \"cpu_sparse.py\",\n+        \"gpu_common_utils.py\",\n+        \"gpu_linalg.py\",\n+        \"gpu_prng.py\",\n+        \"gpu_rnn.py\",\n+        \"gpu_solver.py\",\n+        \"gpu_sparse.py\",\n+        \"gpu_triton.py\",\n+        \"hlo_helpers.py\",\n+        \"init.py\",\n+        \"lapack.py\",\n+        \"plugin_support.py\",\n+        \"xla_client.py\",\n+        \":version\",\n+    ],\n+    deps = [\n+        \":_jax\",\n+        \"//jaxlib/cpu:_lapack\",\n+        \"//jaxlib/cpu:_sparse\",\n+        \"//jaxlib/mlir:ir\",\n+        \"//jaxlib/mlir:stablehlo_dialect\",\n     ],\n )\n \n\n```"
        ],
        "from_id": [
            "hawkinsp",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[doc] mention jaxlib in the API compatibility doc",
        "output": [
            "```diff\nCommit: 6971e125a904ec6af6336c823154f0eda2d7f86a\nDate: 2025-05-13T21:18:03Z\nURL: https://github.com/jax-ml/jax/commit/6971e125a904ec6af6336c823154f0eda2d7f86a\nFiles changed: 1\nAdditions: +5, Deletions: -0\ndiff --git a/docs/api_compatibility.md b/docs/api_compatibility.md\nindex 9dca1fc08f50..dda86e2e5d31 100644\n--- a/docs/api_compatibility.md\n+++ b/docs/api_compatibility.md\n@@ -59,6 +59,11 @@ Any API or import path prefixed with an underscore is explicitly private,\n and may change without warning between JAX releases. We are working to move\n all private APIs into `jax._src` to make these expectations more clear.\n \n+### jaxlib\n+Any import path in the `jaxlib` package is considered private, and may change\n+without warning between releases. Some APIs defined in `jaxlib` have public\n+aliases in the `jax` package.\n+\n ### Legacy internal APIs\n In addition, there are several legacy modules that currently expose some\n private APIs without an underscore, including:\n\n```"
        ],
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "jax.numpy: make type stubs consistent with runtime",
        "output": [
            "```diff\nCommit: b5a595571bb21ae5e8a93f9f0150cf289be4f423\nDate: 2025-05-13T21:07:33Z\nURL: https://github.com/jax-ml/jax/commit/b5a595571bb21ae5e8a93f9f0150cf289be4f423\nFiles changed: 1\nAdditions: +31, Deletions: -32\ndiff --git a/jax/numpy/__init__.pyi b/jax/numpy/__init__.pyi\nindex c52ce2628cda..4db407861f34 100644\n--- a/jax/numpy/__init__.pyi\n+++ b/jax/numpy/__init__.pyi\n@@ -253,7 +253,8 @@ def broadcast_shapes(*shapes: Sequence[int]) -> tuple[int, ...]: ...\n def broadcast_shapes(*shapes: Sequence[int | _core.Tracer]\n                      ) -> tuple[int | _core.Tracer, ...]: ...\n \n-def broadcast_to(array: ArrayLike, shape: DimSize | Shape) -> Array: ...\n+def broadcast_to(array: ArrayLike, shape: DimSize | Shape, *,\n+                 out_sharding: NamedSharding | P | None = None) -> Array: ...\n c_: _CClass\n can_cast = _np.can_cast\n def cbrt(x: ArrayLike, /) -> Array: ...\n@@ -267,6 +268,7 @@ def clip(\n     /,\n     min: ArrayLike | None = ...,\n     max: ArrayLike | None = ...,\n+    *,\n     a: ArrayLike | DeprecatedArg | None = ...,\n     a_min: ArrayLike | DeprecatedArg | None = ...,\n     a_max: ArrayLike | DeprecatedArg | None = ...\n@@ -278,7 +280,7 @@ complex128: Any\n complex64: Any\n complex_: Any\n complexfloating = _np.complexfloating\n-def compress(condition: ArrayLike, a: ArrayLike, axis: int | None = ...,\n+def compress(condition: ArrayLike, a: ArrayLike, axis: int | None = ..., *,\n              size: int | None = ..., fill_value: ArrayLike = ..., out: None = ...) -> Array: ...\n def concat(arrays: Sequence[ArrayLike], /, *, axis: int | None = 0) -> Array: ...\n def concatenate(\n@@ -314,9 +316,9 @@ def cross(\n     axis: int | None = ...,\n ) -> Array: ...\n csingle: Any\n-def cumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def cumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n             out: None = ...) -> Array: ...\n-def cumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def cumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n            out: None = ...) -> Array: ...\n def cumulative_prod(x: ArrayLike, /, *, axis: int | None = ...,\n                     dtype: DTypeLike | None = ...,\n@@ -371,7 +373,6 @@ def einsum(\n     optimize: str | builtins.bool | list[tuple[int, ...]] = ...,\n     precision: PrecisionLike = ...,\n     preferred_element_type: DTypeLike | None = ...,\n-    _use_xeinsum: builtins.bool = False,\n     _dot_general: Callable[..., Array] = ...,\n     out_sharding: NamedSharding | P | None = ...,\n ) -> Array: ...\n@@ -385,7 +386,6 @@ def einsum(\n     optimize: str | builtins.bool | list[tuple[int, ...]] = ...,\n     precision: PrecisionLike = ...,\n     preferred_element_type: DTypeLike | None = ...,\n-    _use_xeinsum: builtins.bool = False,\n     _dot_general: Callable[..., Array] = ...,\n     out_sharding: NamedSharding | P | None = ...,\n ) -> Array: ...\n@@ -397,7 +397,6 @@ def einsum(\n     optimize: str | builtins.bool | list[tuple[int, ...]] = ...,\n     precision: PrecisionLike = ...,\n     preferred_element_type: DTypeLike | None = ...,\n-    _use_xeinsum: builtins.bool = ...,\n     _dot_general: Callable[..., Array] = ...,\n     out_sharding: NamedSharding | P | None = ...,\n ) -> Array: ...\n@@ -422,7 +421,7 @@ def einsum_path(\n     optimize: str | builtins.bool | list[tuple[int, ...]] =  ...,\n ) -> tuple[list[tuple[int, ...]], Any]: ...\n \n-def empty(shape: Any, dtype: DTypeLike | None = ...,\n+def empty(shape: Any, dtype: DTypeLike | None = ..., *,\n           device: _Device | _Sharding | None = ...) -> Array: ...\n def empty_like(prototype: ArrayLike | DuckTypedArray,\n                dtype: DTypeLike | None = ...,\n@@ -579,17 +578,17 @@ def intersect1d(ar1: ArrayLike, ar2: ArrayLike, assume_unique: builtins.bool = .\n def invert(x: ArrayLike, /) -> Array: ...\n def isclose(a: ArrayLike, b: ArrayLike, rtol: ArrayLike = ...,\n             atol: ArrayLike = ..., equal_nan: builtins.bool = ...) -> Array: ...\n-def iscomplex(m: ArrayLike) -> Array: ...\n+def iscomplex(x: ArrayLike) -> Array: ...\n def iscomplexobj(x: Any) -> builtins.bool: ...\n def isdtype(dtype: DTypeLike, kind: DType | str | tuple[DType | str, ...]) -> builtins.bool: ...\n def isfinite(x: ArrayLike, /) -> Array: ...\n-def isin(element: ArrayLike, test_elements: ArrayLike,\n-         assume_unique: builtins.bool = ..., invert: builtins.bool = ..., method: str = ...) -> Array: ...\n+def isin(element: ArrayLike, test_elements: ArrayLike, assume_unique: builtins.bool = ...,\n+         invert: builtins.bool = ..., *, method: str = ...) -> Array: ...\n def isinf(x: ArrayLike, /) -> Array: ...\n def isnan(x: ArrayLike, /) -> Array: ...\n def isneginf(x: ArrayLike, /) -> Array: ...\n def isposinf(x: ArrayLike, /) -> Array: ...\n-def isreal(m: ArrayLike) -> Array: ...\n+def isreal(x: ArrayLike) -> Array: ...\n def isrealobj(x: Any) -> builtins.bool: ...\n def isscalar(element: Any) -> builtins.bool: ...\n def issubdtype(arg1: DTypeLike, arg2: DTypeLike) -> builtins.bool: ...\n@@ -644,7 +643,7 @@ def logspace(start: ArrayLike, stop: ArrayLike, num: int = ...,\n              endpoint: builtins.bool = ..., base: ArrayLike = ...,\n              dtype: DTypeLike | None = ..., axis: int = ...) -> Array: ...\n def mask_indices(\n-    n: int, mask_func: Callable, k: int = ...\n+    n: int, mask_func: Callable, k: int = ..., *, size: int | None = ...\n ) -> tuple[Array, ...]: ...\n def matmul(\n     a: ArrayLike, b: ArrayLike, *, precision: PrecisionLike = ...,\n@@ -655,7 +654,7 @@ def max(a: ArrayLike, axis: _Axis = ..., out: None = ...,\n         keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n         where: ArrayLike | None = ...) -> Array: ...\n def maximum(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n-def mean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def mean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n          out: None = ..., keepdims: builtins.bool = ..., *,\n          where: ArrayLike | None = ...) -> Array: ...\n def median(a: ArrayLike, axis: int | tuple[int, ...] | None = ...,\n@@ -689,14 +688,14 @@ def nanargmin(\n     out: None = ...,\n     keepdims: builtins.bool | None = ...,\n ) -> Array: ...\n-def nancumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def nancumprod(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n                out: None = ...) -> Array: ...\n-def nancumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike = ...,\n+def nancumsum(a: ArrayLike, axis: int | None = ..., dtype: DTypeLike | None = ...,\n                out: None = ...) -> Array: ...\n def nanmax(a: ArrayLike, axis: _Axis = ..., out: None = ...,\n            keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n            where: ArrayLike | None = ...) -> Array: ...\n-def nanmean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nanmean(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n             out: None = ...,\n             keepdims: builtins.bool = ...,\n             where: ArrayLike | None = ...) -> Array: ...\n@@ -710,21 +709,21 @@ def nanpercentile(a: ArrayLike, q: ArrayLike,\n                   axis: int | tuple[int, ...] | None = ...,\n                   out: None = ..., overwrite_input: builtins.bool = ..., method: str = ...,\n                   keepdims: builtins.bool = ..., *, interpolation: DeprecatedArg | str = ...) -> Array: ...\n-def nanprod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nanprod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n             out: None = ...,\n             keepdims: builtins.bool = ..., initial: ArrayLike | None = ...,\n             where: ArrayLike | None = ...) -> Array: ...\n def nanquantile(a: ArrayLike, q: ArrayLike, axis: int | tuple[int, ...] | None = ...,\n                 out: None = ..., overwrite_input: builtins.bool = ..., method: str = ...,\n                 keepdims: builtins.bool = ..., *, interpolation: DeprecatedArg | str = ...) -> Array: ...\n-def nanstd(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ..., out: None = ...,\n-           ddof: int = ..., keepdims: builtins.bool = ...,\n+def nanstd(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n+           out: None = ..., ddof: int = ..., keepdims: builtins.bool = ...,\n            where: ArrayLike | None = ...) -> Array: ...\n-def nansum(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nansum(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n            out: None = ..., keepdims: builtins.bool = ...,\n            initial: ArrayLike | None = ...,\n            where: ArrayLike | None = ...) -> Array: ...\n-def nanvar(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def nanvar(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n            out: None = ...,\n            ddof: int = 0, keepdims: builtins.bool = False,\n            where: ArrayLike | None = ...) -> Array: ...\n@@ -740,7 +739,7 @@ def not_equal(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n number = _np.number\n object_ = _np.object_\n ogrid: _Ogrid\n-def ones(shape: Any, dtype: DTypeLike | None = ...,\n+def ones(shape: Any, dtype: DTypeLike | None = ..., *,\n          device: _Device | _Sharding | None = ...) -> Array: ...\n def ones_like(a: ArrayLike | DuckTypedArray,\n               dtype: DTypeLike | None = ...,\n@@ -782,7 +781,7 @@ def positive(x: ArrayLike, /) -> Array: ...\n def pow(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n def power(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n printoptions = _np.printoptions\n-def prod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def prod(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n          out: None = ..., keepdims: builtins.bool = ...,\n          initial: ArrayLike | None = ..., where: ArrayLike | None = ...,\n          promote_integers: builtins.bool = ...) -> Array: ...\n@@ -805,7 +804,6 @@ def ravel_multi_index(multi_index: Sequence[ArrayLike], dims: Sequence[int],\n                       mode: str = ..., order: str = ...) -> Array: ...\n def real(x: ArrayLike, /) -> Array: ...\n def reciprocal(x: ArrayLike, /) -> Array: ...\n-register_jax_array_methods: Any\n def remainder(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n def repeat(a: ArrayLike, repeats: ArrayLike, axis: int | None = ..., *,\n            total_repeat_length: int | None = ...,\n@@ -844,7 +842,8 @@ def setdiff1d(\n     size: int | None = ...,\n     fill_value: ArrayLike | None = ...,\n ) -> Array: ...\n-def setxor1d(ar1: ArrayLike, ar2: ArrayLike, assume_unique: builtins.bool = ...) -> Array: ...\n+def setxor1d(ar1: ArrayLike, ar2: ArrayLike, assume_unique: builtins.bool = ..., *,\n+             size: int | None = ..., fill_value: ArrayLike | None = ...) -> Array: ...\n def shape(a: ArrayLike | SupportsShape) -> tuple[int, ...]: ...\n def sign(x: ArrayLike, /) -> Array: ...\n def signbit(x: ArrayLike, /) -> Array: ...\n@@ -882,14 +881,14 @@ def stack(\n     out: None = ...,\n     dtype: DTypeLike | None = ...,\n ) -> Array: ...\n-def std(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def std(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n         out: None = ..., ddof: int = ..., keepdims: builtins.bool = ..., *,\n         where: ArrayLike | None = ..., correction: int | float | None = ...) -> Array: ...\n subtract: BinaryUfunc\n def sum(\n     a: ArrayLike,\n     axis: _Axis = ...,\n-    dtype: DTypeLike = ...,\n+    dtype: DTypeLike | None = ...,\n     out: None = ...,\n     keepdims: builtins.bool = ...,\n     initial: ArrayLike | None = ...,\n@@ -927,7 +926,7 @@ def transpose(a: ArrayLike, axes: Sequence[int] | None = ...) -> Array: ...\n def trapezoid(y: ArrayLike, x: ArrayLike | None = None, dx: ArrayLike = ...,\n               axis: int = ...) -> Array: ...\n def tri(\n-    N: int, M: int | None = ..., k: int = ..., dtype: DTypeLike = ...\n+    N: int, M: int | None = ..., k: int = ..., dtype: DTypeLike | None = ...\n ) -> Array: ...\n def tril(m: ArrayLike, k: int = ...) -> Array: ...\n def tril_indices(\n@@ -970,7 +969,7 @@ class _UniqueInverseResult(NamedTuple):\n def unique(ar: ArrayLike, return_index: builtins.bool = ..., return_inverse: builtins.bool = ...,\n            return_counts: builtins.bool = ..., axis: int | None = ...,\n            *, equal_nan: builtins.bool = ..., size: int | None = ...,\n-           fill_value: ArrayLike | None = ...\n+           fill_value: ArrayLike | None = ..., sorted: bool = ...,\n ): ...\n def unique_all(x: ArrayLike, /, *, size: int | None = ...,\n                fill_value: ArrayLike | None = ...) -> _UniqueAllResult: ...\n@@ -994,7 +993,7 @@ def unwrap(p: ArrayLike, discont: ArrayLike | None = ...,\n def vander(\n     x: ArrayLike, N: int | None = ..., increasing: builtins.bool = ...\n ) -> Array: ...\n-def var(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike = ...,\n+def var(a: ArrayLike, axis: _Axis = ..., dtype: DTypeLike | None = ...,\n         out: None = ..., ddof: int = ..., keepdims: builtins.bool = ..., *,\n         where: ArrayLike | None = ..., correction: int | float | None = ...) -> Array: ...\n def vdot(\n@@ -1029,7 +1028,7 @@ def where(condition: ArrayLike, x: ArrayLike | None = ...,\n           fill_value: None | ArrayLike | tuple[ArrayLike, ...] = ...\n           ) -> Array | tuple[Array, ...]: ...\n \n-def zeros(shape: Any, dtype: DTypeLike | None = ...,\n+def zeros(shape: Any, dtype: DTypeLike | None = ..., *,\n           device: _Device | _Sharding | None = ...) -> Array: ...\n def zeros_like(a: ArrayLike | DuckTypedArray,\n                dtype: DTypeLike | None = ...,\n\n```"
        ],
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Merge pull request #28355 from jakevdp:jax-array-abstract\n\nPiperOrigin-RevId: 758360297",
        "output": [
            "```diff\nCommit: e55dadd29a818ffedf079598bb787a5688f66a06\nDate: 2025-05-13T20:48:27Z\nURL: https://github.com/jax-ml/jax/commit/e55dadd29a818ffedf079598bb787a5688f66a06\nFiles changed: 5\nAdditions: +41, Deletions: -2\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex da9efdb0eacc..e004263abe71 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -34,6 +34,7 @@\n \n import numpy as np\n \n+from jax._src import deprecations\n from jax._src import dtypes\n from jax._src import config\n from jax._src import effects\n@@ -1554,6 +1555,12 @@ def shaped_abstractify(x):\n   if isinstance(x, AbstractValue):\n     return x\n   if hasattr(x, '__jax_array__'):\n+    deprecations.warn(\n+      'jax-abstract-dunder-array',\n+      ('Triggering of __jax_array__() during abstractification is deprecated.'\n+       ' To avoid this error, either explicitly convert your object using'\n+       ' jax.numpy.array(), or register your object as a pytree.'),\n+      stacklevel=6)\n     return shaped_abstractify(x.__jax_array__())\n   if hasattr(x, 'dtype'):\n     aval = ShapedArray(np.shape(x), x.dtype,\n@@ -1578,6 +1585,12 @@ def get_aval(x):\n     if (aval_fn := pytype_aval_mappings.get(t)):\n       return aval_fn(x)\n   if hasattr(x, '__jax_array__'):\n+    deprecations.warn(\n+      'jax-abstract-dunder-array',\n+      ('Triggering of __jax_array__() during abstractification is deprecated.'\n+       ' To avoid this error, either explicitly convert your object using'\n+       ' jax.numpy.array(), or register your object as a pytree.'),\n+      stacklevel=6)\n     return get_aval(x.__jax_array__())\n   raise TypeError(f\"Argument '{x}' of type '{typ}' is not a valid JAX type\")\n \ndiff --git a/jax/_src/deprecations.py b/jax/_src/deprecations.py\nindex 329491b1e8a8..4e5e22745658 100644\n--- a/jax/_src/deprecations.py\n+++ b/jax/_src/deprecations.py\n@@ -135,3 +135,4 @@ def warn(deprecation_id: str, message: str, stacklevel: int) -> None:\n register('jax-numpy-trimzeros-not-1d-array')\n register('jax-scipy-special-sph-harm')\n register('jax-jit-positional-args')\n+register('jax-abstract-dunder-array')\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex 2a1ec7227439..266aad4954ba 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -1237,7 +1237,7 @@ def permute_dims(a: ArrayLike, /, axes: tuple[int, ...]) -> Array:\n            [2, 5],\n            [3, 6]], dtype=int32)\n   \"\"\"\n-  util.check_arraylike(\"permute_dims\", a)\n+  a = util.ensure_arraylike(\"permute_dims\", a)\n   return lax.transpose(a, axes)\n \n \ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 5f775b46fb16..1fd192a52525 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -4036,6 +4036,9 @@ def test_default_device(self):\n   def test_dunder_jax_array(self):\n     # https://github.com/jax-ml/jax/pull/4725\n \n+    @partial(jax.tree_util.register_dataclass,\n+             data_fields=['jax_val'],\n+             meta_fields=[])\n     class AlexArray:\n       def __init__(self, jax_val):\n         self.jax_val = jax_val\n@@ -4045,10 +4048,16 @@ def __jax_array__(self):\n       shape = property(lambda self: self.jax_val.shape)\n \n     x = AlexArray(jnp.array([1., 2., 3.]))\n+\n+    y = jax.jit(lambda x: x)(x)\n+    self.assertIsInstance(x, AlexArray)\n+    self.assertArraysEqual(jnp.asarray(x), jnp.asarray(y))\n+\n     y = jnp.sin(x)\n     self.assertAllClose(y, jnp.sin(jnp.array([1., 2., 3.])))\n     y = api.grad(api.jit(lambda x: jnp.sin(x).sum()))(x)\n-    self.assertAllClose(y, jnp.cos(jnp.array([1., 2., 3.])))\n+    self.assertIsInstance(y, AlexArray)\n+    self.assertAllClose(jnp.asarray(y), jnp.cos(jnp.array([1., 2., 3.])))\n \n     x = AlexArray(jnp.array([[1., 2., 3.]]))\n     y = api.pmap(jnp.sin)(x)\n@@ -4066,6 +4075,19 @@ def __jax_array__(self):\n     a2 = jnp.array(((x, x), [x, x]))\n     self.assertAllClose(np.array(((1, 1), (1, 1))), a2)\n \n+  def test_dunder_jax_array_warnings(self):\n+    class AlexArray:\n+      def __init__(self, jax_val):\n+        self.jax_val = jax_val\n+      def __jax_array__(self):\n+        return self.jax_val\n+\n+    f = jax.jit(lambda x: x)\n+    a = AlexArray(jnp.arange(4))\n+    msg = r\"Triggering of __jax_array__\\(\\) during abstractification is deprecated.\"\n+    with self.assertDeprecationWarnsOrRaises('jax-abstract-dunder-array', msg):\n+      f(a)\n+\n   @jtu.thread_unsafe_test()  # count_jit_tracing_cache_miss() isn't thread-safe\n   def test_eval_shape_weak_type(self):\n     # https://github.com/jax-ml/jax/issues/23302\ndiff --git a/tests/array_extensibility_test.py b/tests/array_extensibility_test.py\nindex 91e2a1d9cf6d..36726659c2f9 100644\n--- a/tests/array_extensibility_test.py\n+++ b/tests/array_extensibility_test.py\n@@ -29,6 +29,9 @@\n config.parse_flags_with_absl()\n \n \n+@functools.partial(jax.tree_util.register_dataclass,\n+                   data_fields=['x'],\n+                   meta_fields=[])\n class JaxArrayWrapper:\n   \"\"\"Class that provides a __jax_array__ method.\"\"\"\n   x: ArrayLike\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add direct `pypi` dependencies to the JAX test targets.\n\nPiperOrigin-RevId: 758330650",
        "output": [
            "```diff\nCommit: 725d0f64addd67b242691012854a582d2f14ce6c\nDate: 2025-05-13T19:34:09Z\nURL: https://github.com/jax-ml/jax/commit/725d0f64addd67b242691012854a582d2f14ce6c\nFiles changed: 4\nAdditions: +674, Deletions: -124\ndiff --git a/jaxlib/jax.bzl b/jaxlib/jax.bzl\nindex a8fe2b50344b..e739b681a029 100644\n--- a/jaxlib/jax.bzl\n+++ b/jaxlib/jax.bzl\n@@ -256,18 +256,13 @@ def _get_jax_test_deps(deps):\n     \"\"\"\n     jax_build_deps = [d for d in deps if not d.startswith(\"@pypi//\")]\n \n-    # A lot of tests don't have explicit dependencies on absl/testing, numpy, etc. But the tests\n+    # A lot of tests don't have explicit dependencies on scipy, ml_dtypes, etc. But the tests\n     # transitively depends on them via //jax. So we need to make sure that these dependencies are\n     # included in the test when JAX is built from source.\n-    # TODO(ybaturina): Add individual dependencies for each test and remove this block.\n     jax_transitive_pypi_test_deps = {k: \"true\" for k in py_deps([\n-        \"absl/testing\",\n-        \"numpy\",\n         \"ml_dtypes\",\n         \"scipy\",\n         \"opt_einsum\",\n-        \"hypothesis\",\n-        \"cloudpickle\",\n         \"flatbuffers\",\n     ])}\n \ndiff --git a/tests/BUILD b/tests/BUILD\nindex 4c6369bee5de..e70d4593e8fc 100644\n--- a/tests/BUILD\n+++ b/tests/BUILD\n@@ -38,7 +38,10 @@ jax_multiplatform_test(\n     shard_count = 10,\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -61,12 +64,16 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu_ops\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"device_test\",\n     srcs = [\"device_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -75,12 +82,16 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"absl/testing\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"api_util_test\",\n     srcs = [\"api_util_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -98,7 +109,10 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"absl/testing\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -112,7 +126,11 @@ jax_multiplatform_test(\n         \"gpu_h100x2\",\n     ],\n     tags = [\"multiaccelerator\"],\n-    deps = py_deps(\"tensorflow_core\"),\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"tensorflow_core\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -121,6 +139,10 @@ jax_multiplatform_test(\n     shard_count = {\n         \"gpu\": 5,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -132,7 +154,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental_buffer_callback\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -151,11 +176,19 @@ jax_multiplatform_test(\n         \"cpu\": 5,\n         \"gpu\": 10,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"debug_nans_test\",\n     srcs = [\"debug_nans_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -164,14 +197,20 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"portpicker\"),\n+    ] + py_deps([\n+        \"portpicker\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"distributed_test\",\n     srcs = [\"distributed_test.py\"],\n     enable_backends = [\"gpu\"],\n-    deps = py_deps(\"portpicker\"),\n+    deps = py_deps([\n+        \"portpicker\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_py_test(\n@@ -184,12 +223,19 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"portpicker\"),\n+    ] + py_deps([\n+        \"portpicker\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"dtypes_test\",\n     srcs = [\"dtypes_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -199,12 +245,13 @@ jax_multiplatform_test(\n     enable_configs = [\n         \"cpu\",\n     ],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n     name = \"extend_test\",\n     srcs = [\"extend_test.py\"],\n-    deps = [\"//jax:extend\"],\n+    deps = [\"//jax:extend\"] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -214,7 +261,10 @@ jax_multiplatform_test(\n         \"gpu_h100x2\",\n     ],\n     # TODO(dfm): Remove after removal of jex.ffi imports.\n-    deps = [\"//jax:extend\"],\n+    deps = [\"//jax:extend\"] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -231,11 +281,19 @@ jax_multiplatform_test(\n         \"cpu\": 20,\n         \"gpu\": 10,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"generated_fun_test\",\n     srcs = [\"generated_fun_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -246,6 +304,7 @@ jax_multiplatform_test(\n         \"XLA_PYTHON_CLIENT_PREALLOCATE\": \"0\",\n     },\n     main = \"gpu_memory_flags_test.py\",\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -255,6 +314,7 @@ jax_multiplatform_test(\n     env = {\n         \"XLA_PYTHON_CLIENT_PREALLOCATE\": \"1\",\n     },\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -269,7 +329,12 @@ jax_multiplatform_test(\n     },\n     deps = [\n         \"//jax:experimental_sparse\",\n-    ] + py_deps(\"matplotlib\"),\n+    ] + py_deps([\n+        \"matplotlib\",\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -280,6 +345,11 @@ jax_multiplatform_test(\n         \"gpu\": 10,\n         \"tpu\": 15,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -288,7 +358,7 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -306,7 +376,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -329,7 +402,10 @@ jax_multiplatform_test(\n     tags = [\"multiaccelerator\"],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -345,7 +421,10 @@ jax_multiplatform_test(\n     tags = [\"multiaccelerator\"],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -359,7 +438,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -375,7 +457,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -390,7 +475,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -406,7 +494,7 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -422,7 +510,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:experimental\",\n         \"//jax:internal_test_util\",\n-    ],\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -431,7 +522,10 @@ jax_multiplatform_test(\n     tags = [\"multiaccelerator\"],\n     deps = [\n         \"//jax:experimental\",\n-    ] + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -443,18 +537,31 @@ jax_multiplatform_test(\n         \"tpu\": 8,\n     },\n     tags = [\"noasan\"],  # Linking TF causes a linker OOM.\n-    deps = py_deps(\"pil\") + py_deps(\"tensorflow_core\"),\n+    deps = py_deps([\n+        \"pil\",\n+        \"tensorflow_core\",\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"infeed_test\",\n     srcs = [\"infeed_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"jax_jit_test\",\n     srcs = [\"jax_jit_test.py\"],\n     main = \"jax_jit_test.py\",\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -464,7 +571,10 @@ jax_py_test(\n         \"//jax:test_util\",\n         \"//jax/experimental/jax2tf\",\n         \"//jax/tools:jax_to_ir\",\n-    ] + py_deps(\"tensorflow_core\"),\n+    ] + py_deps([\n+        \"tensorflow_core\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_py_test(\n@@ -474,7 +584,7 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:jaxpr_util\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -487,7 +597,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:jet\",\n         \"//jax:stax\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -498,16 +611,28 @@ jax_multiplatform_test(\n         \"gpu\": 30,\n         \"tpu\": 20,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"custom_root_test\",\n     srcs = [\"custom_root_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"custom_linear_solve_test\",\n     srcs = [\"custom_linear_solve_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -526,6 +651,10 @@ jax_multiplatform_test(\n         \"noasan\",  # Test times out on all backends\n         \"test_cpu_thunks\",\n     ],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -536,6 +665,10 @@ jax_multiplatform_test(\n         \"gpu\": 30,\n         \"tpu\": 40,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -546,6 +679,10 @@ jax_multiplatform_test(\n         \"gpu\": 20,\n         \"tpu\": 20,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -556,11 +693,19 @@ jax_multiplatform_test(\n         \"gpu\": 10,\n         \"tpu\": 10,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"lax_numpy_einsum_test\",\n     srcs = [\"lax_numpy_einsum_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -571,11 +716,19 @@ jax_multiplatform_test(\n         \"gpu\": 5,\n         \"tpu\": 5,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"lax_numpy_vectorize_test\",\n     srcs = [\"lax_numpy_vectorize_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -586,7 +739,11 @@ jax_multiplatform_test(\n         \"gpu\": 20,\n         \"tpu\": 8,\n     },\n-    deps = py_deps(\"numpy\") + py_deps(\"scipy\") + py_deps(\"absl/testing\"),\n+    deps = py_deps([\n+        \"numpy\",\n+        \"scipy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -600,6 +757,11 @@ jax_multiplatform_test(\n         \"gpu\": 5,\n         \"tpu\": 5,\n     },\n+    deps = py_deps([\n+        \"numpy\",\n+        \"scipy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -617,7 +779,11 @@ jax_multiplatform_test(\n         \"tpu\": 20,\n     },\n     tags = [\"noasan\"],  # Times out under asan.\n-    deps = py_deps(\"numpy\") + py_deps(\"scipy\") + py_deps(\"absl/testing\"),\n+    deps = py_deps([\n+        \"numpy\",\n+        \"scipy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -630,7 +796,10 @@ jax_multiplatform_test(\n     },\n     deps = [\n         \"//jax:internal_test_util\",\n-    ] + py_deps(\"numpy\") + py_deps(\"scipy\") + py_deps(\"absl/testing\"),\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -648,7 +817,11 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:internal_test_util\",\n         \"//jax:lax_reference\",\n-    ] + py_deps(\"numpy\") + py_deps(\"mpmath\"),\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+        \"mpmath\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -659,7 +832,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:internal_test_util\",\n         \"//jax:lax_reference\",\n-    ] + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -670,6 +846,10 @@ jax_multiplatform_test(\n         \"gpu\": 30,\n         \"tpu\": 20,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -680,7 +860,10 @@ jax_multiplatform_test(\n         \"gpu\": 40,\n         \"tpu\": 40,\n     },\n-    deps = [\"//jax:internal_test_util\"] + py_deps(\"numpy\") + py_deps(\"absl/testing\"),\n+    deps = [\"//jax:internal_test_util\"] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -691,7 +874,10 @@ jax_multiplatform_test(\n         \"gpu\": 40,\n         \"tpu\": 40,\n     },\n-    deps = [\"//jax:internal_test_util\"] + py_deps(\"numpy\") + py_deps(\"absl/testing\"),\n+    deps = [\"//jax:internal_test_util\"] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_py_test(\n@@ -702,7 +888,7 @@ jax_py_test(\n     deps = [\n         \"//jax:internal_test_util\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -713,7 +899,7 @@ jax_py_test(\n     deps = [\n         \"//jax:internal_test_util\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -733,6 +919,11 @@ jax_multiplatform_test(\n         \"gpu\": 40,\n         \"tpu\": 40,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -753,24 +944,36 @@ jax_multiplatform_test(\n     tags = [\n         \"multiaccelerator\",\n     ],\n+    deps = py_deps([\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"magma_linalg_test\",\n     srcs = [\"magma_linalg_test.py\"],\n     enable_backends = [\"gpu\"],\n-    deps = py_deps(\"magma\"),\n+    deps = py_deps([\n+        \"magma\",\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"cholesky_update_test\",\n     srcs = [\"cholesky_update_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"metadata_test\",\n     srcs = [\"metadata_test.py\"],\n     enable_backends = [\"cpu\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -779,7 +982,7 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -789,12 +992,17 @@ jax_multiplatform_test(\n         \"tpu_v3_x4\",\n         \"gpu_h100x2\",\n     ],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"multi_device_test\",\n     srcs = [\"multi_device_test.py\"],\n     enable_backends = [\"cpu\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -813,12 +1021,20 @@ jax_multiplatform_test(\n         \"tpu\": 10,\n         \"gpu\": 10,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"optimizers_test\",\n     srcs = [\"optimizers_test.py\"],\n-    deps = [\"//jax:optimizers\"],\n+    deps = [\"//jax:optimizers\"] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -826,7 +1042,11 @@ jax_multiplatform_test(\n     srcs = [\"pickle_test.py\"],\n     deps = [\n         \"//jax:experimental\",\n-    ] + py_deps(\"cloudpickle\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"cloudpickle\",\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -850,7 +1070,10 @@ jax_multiplatform_test(\n     tags = [\"multiaccelerator\"],\n     deps = [\n         \"//jax:internal_test_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -868,12 +1091,21 @@ jax_multiplatform_test(\n     # in this case there's not a good place to do it, see b/197635968#comment19\n     # for details.\n     tags = [\"nomsan\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"heap_profiler_test\",\n     srcs = [\"heap_profiler_test.py\"],\n     enable_backends = [\"cpu\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -892,7 +1124,7 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:profiler\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -907,7 +1139,10 @@ jax_multiplatform_test(\n         \"nomsan\",  # TODO(b/355237462): msan false-positives in torch?\n         \"not_build:arm\",\n     ],\n-    deps = py_deps(\"torch\"),\n+    deps = py_deps([\n+        \"torch\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -921,11 +1156,19 @@ jax_multiplatform_test(\n         ],\n     },\n     shard_count = 8,\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"random_test\",\n     srcs = [\"random_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -951,6 +1194,11 @@ jax_multiplatform_test(\n         \"tpu\": 40,\n     },\n     tags = [\"noasan\"],  # Times out\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n # TODO(b/199564969): remove once we always enable_custom_prng\n@@ -959,6 +1207,7 @@ jax_multiplatform_test(\n     srcs = [\"random_test.py\"],\n     args = [\"--jax_enable_custom_prng=true\"],\n     main = \"random_test.py\",\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -972,21 +1221,41 @@ jax_multiplatform_test(\n         ],  # Times out on TPU with asan/tsan/msan.\n     },\n     shard_count = 12,\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"scipy_interpolate_test\",\n     srcs = [\"scipy_interpolate_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"scipy_ndimage_test\",\n     srcs = [\"scipy_ndimage_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"scipy_optimize_test\",\n     srcs = [\"scipy_optimize_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1012,6 +1281,11 @@ jax_multiplatform_test(\n         \"gpu\": 40,\n         \"tpu\": 50,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1021,7 +1295,11 @@ jax_multiplatform_test(\n         \"cpu\": 4,\n         \"gpu\": 4,\n     },\n-    deps = py_deps(\"scipy\"),\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1039,6 +1317,11 @@ jax_multiplatform_test(\n         \"noasan\",\n         \"notsan\",\n     ],  # Times out\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1071,7 +1354,11 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:experimental_sparse\",\n         \"//jax:sparse_test_util\",\n-    ] + py_deps(\"scipy\"),\n+    ] + py_deps([\n+        \"scipy\",\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1108,7 +1395,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:experimental_sparse\",\n         \"//jax:sparse_test_util\",\n-    ] + py_deps(\"scipy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1133,12 +1423,16 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:experimental_sparse\",\n         \"//jax:sparse_test_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"stack_test\",\n     srcs = [\"stack_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1149,33 +1443,50 @@ jax_multiplatform_test(\n         \"gpu\": 2,\n         \"tpu\": 4,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"error_check_test\",\n     srcs = [\"error_check_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n     name = \"jax_numpy_error_test\",\n     srcs = [\"jax_numpy_error_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n     name = \"stax_test\",\n     srcs = [\"stax_test.py\"],\n-    deps = [\"//jax:stax\"],\n+    deps = [\"//jax:stax\"] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"linear_search_test\",\n     srcs = [\"third_party/scipy/line_search_test.py\"],\n     main = \"third_party/scipy/line_search_test.py\",\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"blocked_sampler_test\",\n     srcs = [\"blocked_sampler_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1184,7 +1495,11 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"cloudpickle\",\n+    ]),\n )\n \n pytype_test(\n@@ -1193,7 +1508,11 @@ pytype_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+        \"//jax:typing\",\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1202,7 +1521,7 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1211,7 +1530,7 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1230,7 +1549,9 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:compiler\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"absl/logging\"),\n+    ] + py_deps([\n+        \"absl/logging\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1240,7 +1561,10 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:lru_cache\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"filelock\"),\n+    ] + py_deps([\n+        \"filelock\",\n+        \"absl/logging\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1249,7 +1573,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:compilation_cache_internal\",\n         \"//jax:compiler\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1258,7 +1585,7 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:cache_key\",\n         \"//jax:compiler\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1267,18 +1594,27 @@ jax_multiplatform_test(\n     shard_count = {\n         \"cpu\": 10,\n     },\n-    deps = [\"//jax:ode\"],\n+    deps = [\"//jax:ode\"] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"scipy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"key_reuse_test\",\n     srcs = [\"key_reuse_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"roofline_test\",\n     srcs = [\"roofline_test.py\"],\n     enable_backends = [\"cpu\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1286,7 +1622,10 @@ jax_multiplatform_test(\n     srcs = [\"x64_context_test.py\"],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1297,6 +1636,10 @@ jax_multiplatform_test(\n         \"gpu\": 5,\n         \"tpu\": 10,\n     },\n+    deps = py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1306,17 +1649,26 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:mesh_utils\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"transfer_guard_test\",\n     srcs = [\"transfer_guard_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"cloudpickle\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"garbage_collection_guard_test\",\n     srcs = [\"garbage_collection_guard_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1342,6 +1694,10 @@ jax_multiplatform_test(\n         \"tpu_v4_x4\",\n     ],\n     tags = [\"multiaccelerator\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1356,6 +1712,10 @@ jax_multiplatform_test(\n         \"gpu_h100_shardy\",\n         \"tpu_v3_x4_shardy\",\n     ],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1374,7 +1734,10 @@ jax_multiplatform_test(\n     tags = [\"multiaccelerator\"],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1390,6 +1753,10 @@ jax_multiplatform_test(\n         \"tpu_v3_x4\",\n         \"tpu_v4_x4\",\n     ],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1411,12 +1778,20 @@ jax_multiplatform_test(\n         \"gpu\": 2,\n         \"tpu\": 2,\n     },\n-    deps = py_deps(\"hypothesis\"),\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"mutable_array_test\",\n     srcs = [\"mutable_array_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1425,6 +1800,10 @@ jax_multiplatform_test(\n     shard_count = {\n         \"tpu\": 20,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1445,7 +1824,7 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1469,12 +1848,16 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:experimental\",\n         \"//jax:tree_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"clear_backends_test\",\n     srcs = [\"clear_backends_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1482,7 +1865,10 @@ jax_multiplatform_test(\n     srcs = [\"attrs_test.py\"],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1491,7 +1877,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:experimental_colocated_python\",\n         \"//jax/extend:ifrt_programs\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1504,7 +1893,10 @@ jax_multiplatform_test(\n     shard_count = 15,\n     deps = [\n         \"//jax:rnn\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1514,7 +1906,7 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:mosaic\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1523,7 +1915,7 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1532,12 +1924,13 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n     name = \"logging_test\",\n     srcs = [\"logging_test.py\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1550,6 +1943,10 @@ jax_multiplatform_test(\n         \"tpu_v3_x4\",\n     ],\n     tags = [],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1574,7 +1971,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:internal_test_harnesses\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1596,7 +1996,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:internal_test_harnesses\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1610,7 +2013,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:internal_export_back_compat_test_data\",\n         \"//jax:internal_export_back_compat_test_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1618,12 +2024,16 @@ jax_multiplatform_test(\n     srcs = [\"fused_attention_stablehlo_test.py\"],\n     enable_backends = [\"gpu\"],\n     tags = [\"multiaccelerator\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"xla_metadata_test\",\n     srcs = [\"xla_metadata_test.py\"],\n-    deps = [\"//jax:experimental\"],\n+    deps = [\"//jax:experimental\"] + py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1637,7 +2047,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:experimental\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1646,7 +2059,7 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1656,7 +2069,7 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:source_mapper\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -1665,12 +2078,19 @@ jax_py_test(\n     deps = [\n         \"//jax\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n     name = \"string_array_test\",\n     srcs = [\"string_array_test.py\"],\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -1682,6 +2102,7 @@ jax_multiplatform_test(\n         \"gpu_h100\",\n     ],\n     tags = [\"multiaccelerator\"],\n+    deps = py_deps(\"absl/testing\"),\n )\n \n jax_multiplatform_test(\n@@ -1691,6 +2112,10 @@ jax_multiplatform_test(\n     shard_count = {\n         \"gpu\": 4,\n     },\n+    deps = py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -1700,7 +2125,7 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:experimental\",\n         \"//jax:test_util\",\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n exports_files(\ndiff --git a/tests/mosaic/BUILD b/tests/mosaic/BUILD\nindex 24acb1b9a3f2..75e1df335f6f 100644\n--- a/tests/mosaic/BUILD\n+++ b/tests/mosaic/BUILD\n@@ -41,7 +41,11 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:mosaic_gpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\") + py_deps(\"hypothesis\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -53,7 +57,10 @@ jax_multiplatform_test(\n     tags = [\"multiaccelerator\"],\n     deps = [\n         \"//jax:mosaic_gpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_py_test(\n@@ -83,7 +90,10 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:mosaic_gpu\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"absl/testing\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -95,7 +105,11 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:mosaic_gpu\",\n         \"//jax/experimental/mosaic/gpu/examples:matmul\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\") + py_deps(\"hypothesis\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -110,7 +124,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:mosaic_gpu\",\n-    ] + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"numpy\",\n+        \"absl/testing\",\n+    ]),\n )\n \n jax_multiplatform_test(\ndiff --git a/tests/pallas/BUILD b/tests/pallas/BUILD\nindex 3769da27a1eb..49a05ee487f0 100644\n--- a/tests/pallas/BUILD\n+++ b/tests/pallas/BUILD\n@@ -54,7 +54,10 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu_ops\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -68,7 +71,10 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu_ops\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -88,7 +94,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -124,7 +133,11 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"hypothesis\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"hypothesis\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -162,7 +175,11 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_mosaic_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"hypothesis\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"hypothesis\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -182,7 +199,11 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_tpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"hypothesis\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"hypothesis\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -202,7 +223,10 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu_ops\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -221,7 +245,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_mosaic_gpu\",  # build_cleaner: keep\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -240,7 +267,7 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_mosaic_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu_ops\",  # build_cleaner: keep\n-    ],\n+    ] + py_deps(\"absl/testing\"),\n )\n \n jax_py_test(\n@@ -253,7 +280,10 @@ jax_py_test(\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu\",  # build_cleaner: keep\n         \"//jax:test_util\",\n-    ] + jax_gpu_support_deps,\n+    ] + jax_gpu_support_deps + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -273,7 +303,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu\",  # build_cleaner: keep\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -294,7 +327,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu\",  # build_cleaner: keep\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -307,7 +343,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_tpu\",\n         \"//jax/_src/pallas/mosaic:random\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -321,7 +360,11 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\") + py_deps(\"hypothesis\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -360,7 +403,10 @@ jax_multiplatform_test(\n         \"//jax:extend\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -398,7 +444,11 @@ jax_multiplatform_test(\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"hypothesis\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"hypothesis\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -415,7 +465,10 @@ jax_multiplatform_test(\n         \"//jax:extend\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -436,7 +489,11 @@ jax_multiplatform_test(\n         \"//jax:extend\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"hypothesis\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -449,7 +506,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:pallas_tpu\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -464,7 +524,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:extend\",\n         \"//jax:pallas_tpu\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -481,7 +544,10 @@ jax_multiplatform_test(\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n         \"//jax/_src/pallas/mosaic:random\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -494,7 +560,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_tpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -507,7 +576,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_tpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -525,7 +597,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -543,7 +618,10 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -560,7 +638,11 @@ jax_multiplatform_test(\n     ],\n     deps = [\n         \"//jax:pallas_tpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\") + py_deps(\"hypothesis\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -575,7 +657,10 @@ jax_multiplatform_test(\n         \"//jax:extend\",\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n-    ],\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n # This test doesn't need a TPU; it only tests numpy-using helpers.\n@@ -588,7 +673,11 @@ jax_py_test(\n         \"//jax\",\n         \"//jax:pallas_tpu_ops\",\n         \"//jax:test_util\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\") + py_deps(\"hypothesis\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+        \"hypothesis\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -606,7 +695,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_gpu\",  # build_cleaner: keep\n         \"//jax:pallas_gpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -629,7 +721,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_gpu\",\n         \"//jax:pallas_gpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -647,7 +742,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_gpu\",\n         \"//jax:pallas_gpu_ops\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -663,7 +761,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_mosaic_gpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -680,7 +781,10 @@ jax_multiplatform_test(\n         \"//jax:pallas\",\n         \"//jax:pallas_experimental_gpu_ops\",\n         \"//jax:pallas_mosaic_gpu\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -701,7 +805,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax/_src/pallas/fuser\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -722,7 +829,10 @@ jax_multiplatform_test(\n     deps = [\n         \"//jax:pallas\",\n         \"//jax:pallas_fuser\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n \n jax_multiplatform_test(\n@@ -758,5 +868,8 @@ jax_multiplatform_test(\n         \"//jax:pallas_tpu\",\n         \"//jax:pallas_tpu_ops\",\n         \"//jax/_src/pallas/fuser\",\n-    ] + py_deps(\"absl/testing\") + py_deps(\"numpy\"),\n+    ] + py_deps([\n+        \"absl/testing\",\n+        \"numpy\",\n+    ]),\n )\n\n```"
        ],
        "from_id": [
            "hawkinsp",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Use DmaCopyChunk::Make because directly assigning the struct\nfully constrains the implementation.\n\nPiperOrigin-RevId: 758330465",
        "output": [
            "```diff\nCommit: 0e4f213e9c84b1f59a9d1be84c63cd1b5e4dfd2b\nDate: 2025-05-13T19:32:09Z\nURL: https://github.com/jax-ml/jax/commit/0e4f213e9c84b1f59a9d1be84c63cd1b5e4dfd2b\nFiles changed: 1\nAdditions: +3, Deletions: -6\ndiff --git a/jaxlib/py_socket_transfer.cc b/jaxlib/py_socket_transfer.cc\nindex 491e90d778cf..ed2a4f4c204a 100644\n--- a/jaxlib/py_socket_transfer.cc\n+++ b/jaxlib/py_socket_transfer.cc\n@@ -127,12 +127,9 @@ class IfrtArrayEntry : public PullTable::Entry {\n       auto req_id = base_req_id;\n       ++base_req_id;\n       for (size_t i = 0; i * xfer_size_ < arrs_[bid].buf_size; ++i) {\n-        DmaCopyChunk blob;\n-        blob.arr = std::move(arrs_[bid].arr);\n-        blob.buffer = arrs_[bid].buffer;\n-        blob.buffer_id = bid;\n-        blob.offset = i * xfer_size_;\n-        blob.size = std::min(xfer_size_, arrs_[bid].buf_size - blob.offset);\n+        DmaCopyChunk blob = DmaCopyChunk::Make(\n+            std::move(arrs_[bid].arr), arrs_[bid].buffer, bid, i * xfer_size_,\n+            std::min(xfer_size_, arrs_[bid].buf_size - i * xfer_size_));\n         bool is_largest = blob.size + blob.offset == arrs_[bid].buf_size;\n         state_->ScheduleCopy(\n             blob, [req_id, state, copier_state = state_, is_largest](\n\n```"
        ],
        "from_id": [
            "pschuh",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[JAX] Make fully replicated sharding to avoid materializing the same host buffers\n\nThis change recognizes fully replicated shardings (based on JAX sharding), and\nmaterialize only one host buffer. This saves the cost of repeatedly\nmaterializing the same host buffer for multiple devices, and makes it\nstreamlined to create a multi-device IFRT array.\n\nClean up `JAX_IFRT_VERSION_NUMBER < 2` since we are well past it.\n\nPiperOrigin-RevId: 758320113",
        "output": [
            "```diff\nCommit: ef1b3e9231a929ed509628c080e48d516a5e173d\nDate: 2025-05-13T19:07:40Z\nURL: https://github.com/jax-ml/jax/commit/ef1b3e9231a929ed509628c080e48d516a5e173d\nFiles changed: 7\nAdditions: +210, Deletions: -28\ndiff --git a/jax/_src/test_util.py b/jax/_src/test_util.py\nindex c584ffefa4f2..bb1ef6595ec3 100644\n--- a/jax/_src/test_util.py\n+++ b/jax/_src/test_util.py\n@@ -54,6 +54,7 @@\n from jax._src import mesh as mesh_lib\n from jax._src.cloud_tpu_init import running_in_cloud_tpu_vm\n from jax._src.interpreters import mlir\n+from jax._src.lib import jaxlib_extension_version\n from jax._src.lib.mlir.dialects import hlo\n from jax._src.numpy.util import promote_dtypes, promote_dtypes_inexact\n from jax._src.public_test_util import (  # noqa: F401\n@@ -354,6 +355,20 @@ def assert_num_jit_and_pmap_compilations(times):\n     raise AssertionError(f\"Expected exactly {times} XLA compilations, \"\n                          f\"but executed {count()}\")\n \n+@contextmanager\n+def count_internal_device_puts():\n+  if jaxlib_extension_version >= 341:\n+    before = jax._src.lib._jax.get_internal_device_put_info()\n+  counts = {}\n+  try:\n+    yield lambda: counts\n+  finally:\n+    if jaxlib_extension_version >= 341:\n+      after = jax._src.lib._jax.get_internal_device_put_info()\n+      for k, v in after.items():\n+        diff = v - before.get(k, 0)\n+        if diff != 0:\n+          counts[k] = diff\n \n def jaxlib_version() -> tuple[int, ...]:\n   return _jaxlib.version\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 6f4f952be9c3..c9c25e172161 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -989,3 +989,5 @@ def approx_top_k_reduction_output_size(\n     aggregate_to_topk: bool | None = ...,\n     input_size_override: int | None = ...,\n ) -> tuple[int, int]: ...\n+\n+def get_internal_device_put_info() -> dict[str, int]: ...\ndiff --git a/jaxlib/py_values.cc b/jaxlib/py_values.cc\nindex 81f6523d3e14..6ea5c272eea3 100644\n--- a/jaxlib/py_values.cc\n+++ b/jaxlib/py_values.cc\n@@ -25,6 +25,7 @@ limitations under the License.\n #include <optional>\n #include <string>\n #include <type_traits>\n+#include <unordered_map>\n #include <utility>\n #include <variant>\n #include <vector>\n@@ -78,6 +79,12 @@ namespace xla {\n \n namespace {\n \n+// Gets the thread-local instance.\n+static DevicePutInfo& GetDevicePutInfo() {\n+  thread_local DevicePutInfo device_put_info;\n+  return device_put_info;\n+}\n+\n // Prepared data for creating a single shard of an array. Holds a single-device\n // IFRT array or a host buffer.\n struct Shard {\n@@ -147,6 +154,27 @@ using DevicePutHandler = std::function<absl::StatusOr<ShardFn>(\n     nb::handle obj, ifrt::Client* client, ifrt::Device* to_device,\n     ifrt::MemoryKind to_memory_kind, const DevicePutOptions& options)>;\n \n+// Shared logic that makes an IFRT array (either single-device or multi-device)\n+// from a fully-replicated `shard` that is created from a host buffer (not from\n+// an existing IFRT array). `shard` will be consumed.\n+//\n+// `user_context` will be used for a new IFRT array created.\n+//\n+// Expected to be called without holding GIL.\n+absl::StatusOr<tsl::RCReference<ifrt::Array>>\n+MakeIfrtArrayFromFullyReplicatedShard(\n+    ifrt::Client* ifrt_client, ifrt::ShardingRef ifrt_sharding, Shard& shard,\n+    tsl::RCReference<ifrt::UserContext> user_context) {\n+  auto host_buffer_shard = std::get<ifrt::Client::HostBuffer>(\n+      std::move(shard.ifrt_array_or_host_buffer));\n+  return ifrt_client->MakeArrayFromHostBuffer(\n+      host_buffer_shard.data, host_buffer_shard.dtype,\n+      std::move(host_buffer_shard.shape),\n+      std::move(host_buffer_shard.byte_strides), std::move(ifrt_sharding),\n+      shard.host_buffer_semantics, std::move(host_buffer_shard.on_done),\n+      std::move(user_context));\n+}\n+\n // Shared logic that makes a single-device IFRT array from a `shard`. `shard`\n // will be consumed.\n //\n@@ -161,18 +189,11 @@ absl::StatusOr<ifrt::ArrayRef> MakeSingleDeviceIfrtArrayFromShard(\n   if (auto* ifrt_array =\n           std::get_if<ifrt::ArrayRef>(&shard.ifrt_array_or_host_buffer)) {\n     return std::move(*ifrt_array);\n-  } else {\n-    auto host_buffer_shard = std::get<ifrt::Client::HostBuffer>(\n-        std::move(shard.ifrt_array_or_host_buffer));\n-    ifrt::ShardingRef ifrt_sharding =\n-        ifrt::SingleDeviceSharding::Create(ifrt_device, ifrt_memory_kind);\n-    return ifrt_client->MakeArrayFromHostBuffer(\n-        host_buffer_shard.data, host_buffer_shard.dtype,\n-        std::move(host_buffer_shard.shape),\n-        std::move(host_buffer_shard.byte_strides), std::move(ifrt_sharding),\n-        shard.host_buffer_semantics, std::move(host_buffer_shard.on_done),\n-        std::move(user_context));\n   }\n+  ifrt::ShardingRef ifrt_sharding =\n+      ifrt::SingleDeviceSharding::Create(ifrt_device, ifrt_memory_kind);\n+  return MakeIfrtArrayFromFullyReplicatedShard(\n+      ifrt_client, std::move(ifrt_sharding), shard, std::move(user_context));\n }\n \n // Makes an IFRT Array from `shards` using a batched array creation API (fast\n@@ -587,10 +608,12 @@ absl::StatusOr<ShardFn> MakeShardFn(nb::handle arg, ifrt::Client* client,\n                                     ifrt::Device* to_device,\n                                     ifrt::MemoryKind to_memory_kind,\n                                     const DevicePutOptions& options) {\n-  using PyObjectDeviceHandlerMap = absl::flat_hash_map<PyObject*, DevicePutHandler>;\n+  using PyObjectDeviceHandlerMap =\n+      absl::flat_hash_map<PyObject*, DevicePutHandler>;\n \n-  auto init_fn = [](){\n-    std::unique_ptr<PyObjectDeviceHandlerMap> p = std::make_unique<PyObjectDeviceHandlerMap>();\n+  auto init_fn = []() {\n+    std::unique_ptr<PyObjectDeviceHandlerMap> p =\n+        std::make_unique<PyObjectDeviceHandlerMap>();\n \n     const NumpyScalarTypes& dtypes = GetNumpyScalarTypes();\n     // Python scalar types.\n@@ -660,7 +683,8 @@ absl::StatusOr<ShardFn> MakeShardFn(nb::handle arg, ifrt::Client* client,\n     (*p)[dtypes.np_intc.ptr()] = HandleNumpyScalar<int32_t>;\n     return p;\n   };\n-  const PyObjectDeviceHandlerMap& handlers = xla::SafeStaticInit<PyObjectDeviceHandlerMap>(init_fn);\n+  const PyObjectDeviceHandlerMap& handlers =\n+      xla::SafeStaticInit<PyObjectDeviceHandlerMap>(init_fn);\n \n   if (arg.type().ptr() == PyArray::type().ptr()) {\n     auto array = nb::borrow<PyArray>(arg);\n@@ -895,6 +919,7 @@ absl::StatusOr<DevicePutResult> DevicePutWithDevice(\n     ifrt::Device* ifrt_device, ifrt::MemoryKind ifrt_memory_kind,\n     const DevicePutOptions& options) {\n   tsl::profiler::TraceMe traceme(\"DevicePut\");\n+  ++GetDevicePutInfo().device_put_with_device;\n \n   if (!ifrt_device->IsAddressable()) {\n     return InvalidArgument(\"Cannot copy array to non-addressable device: %s\",\n@@ -924,6 +949,7 @@ absl::StatusOr<DevicePutResult> DevicePutWithSharding(\n     absl::Span<const int64_t> shape, nanobind::handle sharding,\n     const DevicePutOptions& options) {\n   tsl::profiler::TraceMe traceme(\"DevicePutWithSharding\");\n+  ++GetDevicePutInfo().device_put_with_sharding;\n \n   TF_ASSIGN_OR_RETURN(ifrt::DeviceListRef ifrt_device_list,\n                       GetIfrtDeviceList(sharding));\n@@ -973,12 +999,19 @@ absl::StatusOr<DevicePutResult> DevicePutWithSharding(\n   }\n \n   ifrt::ShardingRef ifrt_sharding;\n+  bool is_fully_replicated;\n   if (is_pmap_sharding) {\n     CHECK(!shard_fns.empty());\n     // IFRT Sharding will be determined once we discover the shard shape.\n+    is_fully_replicated = false;\n   } else {\n     TF_ASSIGN_OR_RETURN(ifrt_sharding,\n                         GetIfrtHloSharding(sharding, ifrt_shape));\n+    // Fully-replicated shardings enable additional optimizations of using a\n+    // single host buffer.\n+    // TODO(hyeontaek): Enable a similar optimization for partially replicated\n+    // cases to reduce the number of host buffers to obtain.\n+    is_fully_replicated = ifrt_sharding->IsFullyReplicated();\n   }\n   tsl::RCReference<ifrt::UserContext> ifrt_user_context =\n       ifrt_client->CreateUserContext();\n@@ -988,12 +1021,6 @@ absl::StatusOr<DevicePutResult> DevicePutWithSharding(\n   // Whether to build an IFRT array from host buffers as a single batch. We do\n   // not batch any shard is already an IFRT array.\n   bool should_batch = true;\n-#if JAX_IFRT_VERSION_NUMBER < 2\n-  // PjRt-IFRT would fail `xla::ifrt::Client::MakeArrayFromHostBuffer()` invoked\n-  // by `xla::ifrt::ClientMakeArraysFromHostBufferShards()` for a fully\n-  // replicated sharding if the sharding has any non-addressable device.\n-  should_batch = false;\n-#endif\n \n   std::vector<Shard> shards;\n   shards.reserve(shard_fns.size());\n@@ -1004,7 +1031,15 @@ absl::StatusOr<DevicePutResult> DevicePutWithSharding(\n       should_batch = false;\n     }\n     shards.push_back(std::move(shard));\n+    if (should_batch && is_fully_replicated) {\n+      // We need only one host buffer for a fully-replicated array.\n+      break;\n+    }\n   }\n+  // While we have finished calling `shard_fns`, we cannot destroy them until we\n+  // make a call to IFRT array creation. Destroying `shard_fns` would release\n+  // host buffers prematurely and can cause the array creation API to see\n+  // garbage data.\n \n   // TODO(emilyaf): Remove the following and just use ifrt_dtype when tokens are\n   // supported.\n@@ -1021,12 +1056,22 @@ absl::StatusOr<DevicePutResult> DevicePutWithSharding(\n \n   ifrt::ArrayRef ifrt_array;\n   if (should_batch) {\n-    TF_ASSIGN_OR_RETURN(ifrt_array,\n-                        MakeIfrtArrayFromShardsInBatch(\n-                            ifrt_client, ifrt_dtype, std::move(ifrt_shape),\n-                            std::move(ifrt_sharding), absl::MakeSpan(shards),\n-                            std::move(ifrt_user_context)));\n+    if (is_fully_replicated && shards.size() == 1) {\n+      ++GetDevicePutInfo().device_put_fully_replicated;\n+      TF_ASSIGN_OR_RETURN(\n+          ifrt_array, MakeIfrtArrayFromFullyReplicatedShard(\n+                          ifrt_client, std::move(ifrt_sharding), shards.front(),\n+                          std::move(ifrt_user_context)));\n+    } else {\n+      ++GetDevicePutInfo().device_put_batched;\n+      TF_ASSIGN_OR_RETURN(ifrt_array,\n+                          MakeIfrtArrayFromShardsInBatch(\n+                              ifrt_client, ifrt_dtype, std::move(ifrt_shape),\n+                              std::move(ifrt_sharding), absl::MakeSpan(shards),\n+                              std::move(ifrt_user_context)));\n+    }\n   } else {\n+    ++GetDevicePutInfo().device_put_assembled;\n     TF_ASSIGN_OR_RETURN(\n         ifrt_array, MakeIfrtArrayFromShardsWithAssembly(\n                         ifrt_client, ifrt_dtype, std::move(ifrt_shape),\n@@ -1038,4 +1083,15 @@ absl::StatusOr<DevicePutResult> DevicePutWithSharding(\n   return DevicePutResult(std::move(ifrt_array), weak_type);\n }\n \n+std::unordered_map<std::string, int64_t> DevicePutInfo::GetInfo() {\n+  const DevicePutInfo& info = GetDevicePutInfo();\n+  return std::unordered_map<std::string, int64_t>({\n+      {\"device_put_with_device\", info.device_put_with_device},\n+      {\"device_put_with_sharding\", info.device_put_with_sharding},\n+      {\"device_put_fully_replicated\", info.device_put_fully_replicated},\n+      {\"device_put_batched\", info.device_put_batched},\n+      {\"device_put_assembled\", info.device_put_assembled},\n+  });\n+}\n+\n }  // namespace xla\ndiff --git a/jaxlib/py_values.h b/jaxlib/py_values.h\nindex 64a83aa66ab9..d74cf9668a99 100644\n--- a/jaxlib/py_values.h\n+++ b/jaxlib/py_values.h\n@@ -21,6 +21,7 @@ limitations under the License.\n #include <cstdint>\n #include <string>\n #include <tuple>\n+#include <unordered_map>\n #include <utility>\n \n #include \"absl/container/inlined_vector.h\"\n@@ -32,7 +33,6 @@ limitations under the License.\n #include \"xla/python/ifrt/device.h\"\n #include \"xla/python/ifrt/memory.h\"\n #include \"xla/python/nb_numpy.h\"\n-#include \"xla/tsl/concurrency/ref_count.h\"\n #include \"xla/xla_data.pb.h\"\n \n namespace xla {\n@@ -136,6 +136,26 @@ H AbslHashValue(H h, const xla::PyArgSignature& s) {\n   return h;\n }\n \n+// Tracks the number of DevicePut calls and subcases. For testing.\n+struct DevicePutInfo {\n+  // DevicePutWithDevice call count.\n+  int device_put_with_device = 0;\n+\n+  // DevicePutWithSharding call count.\n+  int device_put_with_sharding = 0;\n+\n+  // DevicePutWithSharding with a fully replicated sharding.\n+  int device_put_fully_replicated = 0;\n+  // DevicePutWithSharding that made a batched array creation call.\n+  int device_put_batched = 0;\n+  // DevicePutWithSharding that made per-shard creation calls followed by an\n+  // assembly call.\n+  int device_put_assembled = 0;\n+\n+  // Returns a map of the counters for the current thread.\n+  static std::unordered_map<std::string, int64_t> GetInfo();\n+};\n+\n }  // namespace xla\n \n #endif  // JAXLIB_PY_VALUES_H_\ndiff --git a/jaxlib/xla.cc b/jaxlib/xla.cc\nindex adf6f3c98297..4020e061b3f4 100644\n--- a/jaxlib/xla.cc\n+++ b/jaxlib/xla.cc\n@@ -45,6 +45,7 @@ limitations under the License.\n #include \"nanobind/stl/string.h\"  // IWYU pragma: keep\n #include \"nanobind/stl/string_view.h\"  // IWYU pragma: keep\n #include \"nanobind/stl/unique_ptr.h\"  // IWYU pragma: keep\n+#include \"nanobind/stl/unordered_map.h\"  // IWYU pragma: keep\n #include \"nanobind/stl/variant.h\"  // IWYU pragma: keep\n #include \"nanobind/stl/vector.h\"  // IWYU pragma: keep\n #include \"jaxlib/ffi.h\"\n@@ -975,6 +976,9 @@ NB_MODULE(_jax, m) {\n         nb::arg(\"recall_target\"), nb::arg(\"aggregate_to_topk\") = true,\n         nb::arg(\"input_size_override\") = -1);\n \n+  m.def(\"get_internal_device_put_info\",\n+        []() { return DevicePutInfo::GetInfo(); });\n+\n }  // NOLINT(readability/fn_size)\n \n }  // namespace xla\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 6aaae11c139d..69e168de9c2d 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 340\n+_version = 341\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 15966c678d87..5f775b46fb16 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -60,6 +60,7 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.compilation_cache import is_persistent_cache_enabled\n from jax._src.lib import _jax\n+from jax._src.lib import jaxlib_extension_version\n import jax._src.util as jax_util\n from jax.ad_checkpoint import checkpoint_name, checkpoint as new_checkpoint\n from jax.errors import (UnexpectedTracerError, TracerIntegerConversionError,\n@@ -1972,6 +1973,90 @@ def test_device_put_sharding_mismatched_tree_different_leaf_count(self):\n     ):\n       jax.device_put((x, y, z), device=(s1, s2))\n \n+  def test_internal_device_put_with_device(self):\n+    if jaxlib_extension_version < 341:\n+      raise unittest.SkipTest(\n+          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n+          \" DevicePut calls\")\n+\n+    # Hitting the cache for a single-device jitted execution while using a numpy\n+    # array calls internal `DevicePutWithDevice`.\n+    f = jax.jit(lambda x: x + 1)\n+    f(np.arange(8))\n+\n+    with jtu.count_internal_device_puts() as counts:\n+      f(np.arange(8))\n+    self.assertEqual(counts(), {\"device_put_with_device\": 1})\n+\n+  def test_internal_device_put_fully_replicated(self):\n+    if jaxlib_extension_version < 341:\n+      raise unittest.SkipTest(\n+          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n+          \" DevicePut calls\")\n+    if jax.device_count() < 2:\n+      raise unittest.SkipTest(\"Test requires >= 2 devices\")\n+\n+    # Creating an array from a numpy array with a fully-replicated sharding\n+    # calls internal `DevicePutWithSharding`, taking the fully-replicated sub\n+    # case.\n+    mesh = jax.sharding.Mesh(np.array(jax.devices()[:2]), \"x\")\n+    sharding = jax.NamedSharding(mesh, P())\n+\n+    with jtu.count_internal_device_puts() as counts:\n+      jax.device_put(np.arange(8), sharding)\n+    self.assertEqual(\n+        counts(),\n+        {\"device_put_with_sharding\": 1, \"device_put_fully_replicated\": 1},\n+    )\n+\n+  def test_internal_device_put_batched(self):\n+    if jaxlib_extension_version < 341:\n+      raise unittest.SkipTest(\n+          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n+          \" DevicePut calls\")\n+    if jax.device_count() < 2:\n+      raise unittest.SkipTest(\"Test requires >= 2 devices\")\n+\n+    # Creating an array from a numpy array with a non-fully-replicated sharding\n+    # calls internal `DevicePutWithSharding`, performing batched creation of a\n+    # multi-shard array.\n+    mesh = jax.sharding.Mesh(np.array(jax.devices()[:2]), \"x\")\n+    sharding = jax.NamedSharding(mesh, P(\"x\"))\n+\n+    with jtu.count_internal_device_puts() as counts:\n+      jax.device_put(np.arange(8), sharding)\n+    self.assertEqual(\n+        counts(), {\"device_put_with_sharding\": 1, \"device_put_batched\": 1}\n+    )\n+\n+  def test_internal_device_put_assembled(self):\n+    if jaxlib_extension_version < 341:\n+      raise unittest.SkipTest(\n+          \"Test requires jaxlib extension version >= 341 for tracking low-level\"\n+          \" DevicePut calls\")\n+    if jax.device_count() < 2:\n+      raise unittest.SkipTest(\"Test requires >= 2 devices\")\n+\n+    # Creating an array from per-device JAX arrays calls internal\n+    # `DevicePutWithSharding`, performing per-shard array adoption followed by\n+    # assembly.\n+    mesh = jax.sharding.Mesh(np.array(jax.devices()[:2]), \"x\")\n+    sharding = jax.NamedSharding(mesh, P(\"x\"))\n+\n+    arr = np.arange(8)\n+    per_device_arrs = {\n+        # Use uncommitted arrays that are not aligned with the destination\n+        # sharding so that we trigger `BatchedDevicePut`.\n+        index: jnp.array(arr[index])\n+        for _, index in sharding.devices_indices_map(arr.shape).items()\n+    }\n+    data_callback = lambda index: per_device_arrs[index]\n+    with jtu.count_internal_device_puts() as counts:\n+      jax.make_array_from_callback(arr.shape, sharding, data_callback)\n+    self.assertEqual(\n+        counts(), {\"device_put_with_sharding\": 1, \"device_put_assembled\": 1}\n+    )\n+\n   def test_device_put_custom_type_not_accepting_none_leaves(self):\n \n     class CustomNode(list):\n\n```"
        ],
        "from_id": [
            "hyeontaek",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28715 from dfm:custom-vjp-pp\n\nPiperOrigin-RevId: 758306384",
        "output": [
            "```diff\nCommit: d6608988b3470e283c7a4a61739df23cb9d6b2a9\nDate: 2025-05-13T18:35:56Z\nURL: https://github.com/jax-ml/jax/commit/d6608988b3470e283c7a4a61739df23cb9d6b2a9\nFiles changed: 2\nAdditions: +46, Deletions: -0\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex dcd893f44123..7b81c4e86889 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -1046,6 +1046,23 @@ def dce_bwd(*args):\n   return list(used_ins), new_eqn\n pe.dce_rules[custom_vjp_call_p] = _custom_vjp_call_dce\n \n+\n+def _custom_vjp_call_pp_rule(eqn: core.JaxprEqn,\n+                             context: core.JaxprPpContext,\n+                             settings: core.JaxprPpSettings) -> core.pp.Doc:\n+  params = dict(eqn.params)\n+  if not params[\"num_consts\"]:\n+    params.pop(\"num_consts\")\n+  params.pop(\"out_trees\")\n+  params[\"fwd\"] = params.pop(\"fwd_jaxpr_thunk\").debug_info.func_name\n+  params[\"bwd\"] = params.pop(\"bwd\").debug_info.func_name\n+  names = sorted(params)\n+  params[\"name\"] = params[\"call_jaxpr\"].jaxpr.debug_info.func_name\n+  return core._pp_eqn(eqn.replace(params=params), context, settings,\n+                      params=[\"name\"] + names)\n+\n+core.pp_eqn_rules[custom_vjp_call_p] = _custom_vjp_call_pp_rule\n+\n batching.primitive_batchers[ad.custom_lin_p] = ad.raise_custom_vjp_error_on_jvp\n mlir.register_lowering(ad.custom_lin_p, ad.raise_custom_vjp_error_on_jvp)\n \ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 73dc2fbefcaa..9d10b40c6030 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3117,6 +3117,35 @@ def f_bwd(res, cts):\n     ):\n       f(0.5, 0.1, z=1.0)\n \n+  def test_pretty_print(self):\n+    @jax.custom_vjp\n+    def f(x):\n+      return x + 1\n+\n+    def f_fwd(x):\n+      return f(x), ()\n+\n+    def f_bwd(_, g):\n+      return g\n+    f.defvjp(f_fwd, f_bwd)\n+\n+    x = jnp.array([4.2], dtype=jnp.float32)\n+    jaxpr = jax.make_jaxpr(f)(x)\n+    actual = jaxpr.pretty_print(use_color=False)\n+    expected = textwrap.dedent(\n+        \"\"\"\n+        { lambda ; a:f32[1]. let\n+            b:f32[1] = custom_vjp_call[\n+              name=f\n+              bwd=f_bwd\n+              call_jaxpr={ lambda ; c:f32[1]. let d:f32[1] = add c 1.0:f32[] in (d,) }\n+              fwd=f_fwd\n+              symbolic_zeros=False\n+            ] a\n+          in (b,) }\n+        \"\"\").strip()\n+    self.assertEqual(actual, expected)\n+\n \n def transpose_unary(f, x_example):\n   def transposed(y):\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas] Allow f8 casting tests on TPUv5-.\n\nPiperOrigin-RevId: 758287085",
        "output": [
            "```diff\nCommit: 78f89b8bb4742fd27ba0aa155b4f6f31c0a1d8db\nDate: 2025-05-13T17:52:03Z\nURL: https://github.com/jax-ml/jax/commit/78f89b8bb4742fd27ba0aa155b4f6f31c0a1d8db\nFiles changed: 1\nAdditions: +16, Deletions: -4\ndiff --git a/tests/pallas/ops_test.py b/tests/pallas/ops_test.py\nindex 61ebc19e018f..9bb6d31d15e1 100644\n--- a/tests/pallas/ops_test.py\n+++ b/tests/pallas/ops_test.py\n@@ -594,10 +594,16 @@ def kernel(x_ref, y_ref):\n   def test_cast_from_32bit(self, from_dtype, to_dtype, data):\n     sut_is_mosaic_gpu = jtu.test_device_matches([\"gpu\"]) and use_mosaic_gpu\n     if to_dtype in {\"float8_e4m3b11fnuz\", \"float8_e5m2\", \"float8_e4m3fn\"}:\n-      if not jtu.test_device_matches([\"tpu\"]) or jtu.get_tpu_version() < 5:\n+      if not jtu.test_device_matches([\"tpu\"]):\n         self.skipTest(\"Not supported on this hardware\")\n-      if not jtu.if_cloud_tpu_at_least(2025, 3, 8):\n+      if jtu.get_tpu_version() >= 5 and not jtu.if_cloud_tpu_at_least(\n+          2025, 3, 8\n+      ):\n         self.skipTest(\"Test requires libtpu from 2025/3/8 or later\")\n+      if jtu.get_tpu_version() < 5 and not jtu.if_cloud_tpu_at_least(\n+          2025, 5, 15\n+      ):\n+        self.skipTest(\"Test requires libtpu from 2025/5/15 or later\")\n     if from_dtype in {\"int2\", \"uint2\"} or to_dtype in {\"int2\", \"uint2\"}:\n       if jtu.test_device_matches([\"tpu\"]) and not jtu.if_cloud_tpu_at_least(\n           2025, 4, 1\n@@ -721,10 +727,16 @@ def test_cast_from_sub_32bit(self, from_dtype, to_dtype, randomize):\n         \"float8_e5m2\",\n         \"float8_e4m3fn\",\n     } or to_dtype in {\"float8_e4m3b11fnuz\", \"float8_e5m2\", \"float8_e4m3fn\"}:\n-      if not jtu.test_device_matches([\"tpu\"]) or jtu.get_tpu_version() < 5:\n+      if not jtu.test_device_matches([\"tpu\"]):\n         self.skipTest(\"Not supported on this hardware\")\n-      if not jtu.if_cloud_tpu_at_least(2025, 3, 9):\n+      if jtu.get_tpu_version() >= 5 and not jtu.if_cloud_tpu_at_least(\n+          2025, 3, 9\n+      ):\n         self.skipTest(\"Test requires libtpu from 2025/3/9 or later\")\n+      if jtu.get_tpu_version() < 5 and not jtu.if_cloud_tpu_at_least(\n+          2025, 5, 15\n+      ):\n+        self.skipTest(\"Test requires libtpu from 2025/5/15 or later\")\n     if from_dtype == \"int2\" and to_dtype == \"bool\":\n       self.skipTest(\n           \"TODO(b/343490729): XLA compare(s2, s2) yields wrong results\"\n\n```"
        ],
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add a pretty printing rule for custom_vjp.",
        "output": [
            "```diff\nCommit: 71692fcbedf162c586ec8847d34938151ffa1f79\nDate: 2025-05-13T17:51:31Z\nURL: https://github.com/jax-ml/jax/commit/71692fcbedf162c586ec8847d34938151ffa1f79\nFiles changed: 2\nAdditions: +46, Deletions: -0\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex dcd893f44123..7b81c4e86889 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -1046,6 +1046,23 @@ def dce_bwd(*args):\n   return list(used_ins), new_eqn\n pe.dce_rules[custom_vjp_call_p] = _custom_vjp_call_dce\n \n+\n+def _custom_vjp_call_pp_rule(eqn: core.JaxprEqn,\n+                             context: core.JaxprPpContext,\n+                             settings: core.JaxprPpSettings) -> core.pp.Doc:\n+  params = dict(eqn.params)\n+  if not params[\"num_consts\"]:\n+    params.pop(\"num_consts\")\n+  params.pop(\"out_trees\")\n+  params[\"fwd\"] = params.pop(\"fwd_jaxpr_thunk\").debug_info.func_name\n+  params[\"bwd\"] = params.pop(\"bwd\").debug_info.func_name\n+  names = sorted(params)\n+  params[\"name\"] = params[\"call_jaxpr\"].jaxpr.debug_info.func_name\n+  return core._pp_eqn(eqn.replace(params=params), context, settings,\n+                      params=[\"name\"] + names)\n+\n+core.pp_eqn_rules[custom_vjp_call_p] = _custom_vjp_call_pp_rule\n+\n batching.primitive_batchers[ad.custom_lin_p] = ad.raise_custom_vjp_error_on_jvp\n mlir.register_lowering(ad.custom_lin_p, ad.raise_custom_vjp_error_on_jvp)\n \ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 73dc2fbefcaa..9d10b40c6030 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3117,6 +3117,35 @@ def f_bwd(res, cts):\n     ):\n       f(0.5, 0.1, z=1.0)\n \n+  def test_pretty_print(self):\n+    @jax.custom_vjp\n+    def f(x):\n+      return x + 1\n+\n+    def f_fwd(x):\n+      return f(x), ()\n+\n+    def f_bwd(_, g):\n+      return g\n+    f.defvjp(f_fwd, f_bwd)\n+\n+    x = jnp.array([4.2], dtype=jnp.float32)\n+    jaxpr = jax.make_jaxpr(f)(x)\n+    actual = jaxpr.pretty_print(use_color=False)\n+    expected = textwrap.dedent(\n+        \"\"\"\n+        { lambda ; a:f32[1]. let\n+            b:f32[1] = custom_vjp_call[\n+              name=f\n+              bwd=f_bwd\n+              call_jaxpr={ lambda ; c:f32[1]. let d:f32[1] = add c 1.0:f32[] in (d,) }\n+              fwd=f_fwd\n+              symbolic_zeros=False\n+            ] a\n+          in (b,) }\n+        \"\"\").strip()\n+    self.assertEqual(actual, expected)\n+\n \n def transpose_unary(f, x_example):\n   def transposed(y):\n\n```"
        ],
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Merge pull request #28711 from dfm:custom-vjp-symb-zeros\n\nPiperOrigin-RevId: 758272898",
        "output": [
            "```diff\nCommit: 8060ca2e8dd12172b96f4fedcc42e953e7f20d0a\nDate: 2025-05-13T17:20:08Z\nURL: https://github.com/jax-ml/jax/commit/8060ca2e8dd12172b96f4fedcc42e953e7f20d0a\nFiles changed: 1\nAdditions: +11, Deletions: -9\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 435e9027f5b3..45705382efa0 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -565,12 +565,12 @@ def process_custom_vjp_call(self, prim, fun, fwd, bwd, tracers, out_trees,\n     _, res_tree = out_trees()\n     res, primals_out = split_list(res_and_primals_out, [res_tree.num_leaves])\n     avals_out = [core.get_aval(x).to_tangent_aval() for x in primals_out]\n-    # TODO(frostig,mattjj): avoid instantiating zeros when we don't have to!\n+    in_zeros = [type(t) is Zero for t in tangents_in]\n+    nz_tangents_in = [t for z, t in zip(in_zeros, tangents_in) if not z]\n     with core.set_current_trace(self.parent_trace):\n-      tangents_in = map(instantiate_zeros, tangents_in)\n       tangents_out = custom_lin_p.bind(\n-        *res, *tangents_in, num_res=res_tree.num_leaves, bwd=bwd,\n-        out_avals=avals_out, symbolic_zeros=symbolic_zeros)\n+          *res, *nz_tangents_in, num_res=res_tree.num_leaves, bwd=bwd,\n+          out_avals=avals_out, symbolic_zeros=symbolic_zeros, in_zeros=in_zeros)\n     return map(partial(maybe_jvp_tracer, self), primals_out, tangents_out)\n \n   def process_custom_transpose(self, prim, call, tracers, **params):\n@@ -734,11 +734,12 @@ def process_custom_vjp_call(self, prim, fun, fwd,\n     res, primals_out = split_list(res_and_primals_out, [res_tree.num_leaves])\n     avals_out = [core.get_aval(x).to_tangent_aval() for x in primals_out]\n \n-    tangents_in_zeros = map(instantiate_zeros, tangents_in)\n+    in_zeros = [type(t) is Zero for t in tangents_in]\n+    nz_tangents_in = [t for z, t in zip(in_zeros, tangents_in) if not z]\n     with core.set_current_trace(self.tangent_trace):\n       tangents_out = custom_lin_p.bind(\n-        *res, *tangents_in_zeros, num_res=res_tree.num_leaves, bwd=bwd,\n-        out_avals=avals_out, symbolic_zeros=symbolic_zeros)\n+          *res, *nz_tangents_in, num_res=res_tree.num_leaves, bwd=bwd,\n+          out_avals=avals_out, symbolic_zeros=symbolic_zeros, in_zeros=in_zeros)\n     tangent_nzs_out = [type(t) is not Zero for t in tangents_out]\n     return map(partial(maybe_linearize_tracer, self), primals_out, tangent_nzs_out, tangents_out)\n \n@@ -1223,7 +1224,7 @@ def raise_custom_vjp_error_on_jvp(*_, **__):\n \n def _custom_lin_transpose(cts_out, *invals, num_res,\n                           bwd: lu.WrappedFun, out_avals,\n-                          symbolic_zeros):\n+                          symbolic_zeros, in_zeros):\n   res, _ = split_list(invals, [num_res])\n   if symbolic_zeros:\n     cts_out = map(replace_internal_symbolic_zeros, cts_out)\n@@ -1231,7 +1232,8 @@ def _custom_lin_transpose(cts_out, *invals, num_res,\n     cts_out = map(instantiate_zeros, cts_out)\n   cts_in = bwd.call_wrapped(*res, *cts_out)\n   cts_in = map(replace_rule_output_symbolic_zeros, cts_in)\n-  return [None] * num_res + list(cts_in)\n+  nz_cts_in, _ = partition_list(in_zeros, cts_in)\n+  return [None] * num_res + nz_cts_in\n primitive_transposes[custom_lin_p] = _custom_lin_transpose\n \n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Don't instantiate zeros passed to custom_lin_p.",
        "output": [
            "```diff\nCommit: df66c2fdc538a5b0d8e7d052a96ceaa6258a9da5\nDate: 2025-05-13T16:57:47Z\nURL: https://github.com/jax-ml/jax/commit/df66c2fdc538a5b0d8e7d052a96ceaa6258a9da5\nFiles changed: 1\nAdditions: +11, Deletions: -9\ndiff --git a/jax/_src/interpreters/ad.py b/jax/_src/interpreters/ad.py\nindex 435e9027f5b3..45705382efa0 100644\n--- a/jax/_src/interpreters/ad.py\n+++ b/jax/_src/interpreters/ad.py\n@@ -565,12 +565,12 @@ def process_custom_vjp_call(self, prim, fun, fwd, bwd, tracers, out_trees,\n     _, res_tree = out_trees()\n     res, primals_out = split_list(res_and_primals_out, [res_tree.num_leaves])\n     avals_out = [core.get_aval(x).to_tangent_aval() for x in primals_out]\n-    # TODO(frostig,mattjj): avoid instantiating zeros when we don't have to!\n+    in_zeros = [type(t) is Zero for t in tangents_in]\n+    nz_tangents_in = [t for z, t in zip(in_zeros, tangents_in) if not z]\n     with core.set_current_trace(self.parent_trace):\n-      tangents_in = map(instantiate_zeros, tangents_in)\n       tangents_out = custom_lin_p.bind(\n-        *res, *tangents_in, num_res=res_tree.num_leaves, bwd=bwd,\n-        out_avals=avals_out, symbolic_zeros=symbolic_zeros)\n+          *res, *nz_tangents_in, num_res=res_tree.num_leaves, bwd=bwd,\n+          out_avals=avals_out, symbolic_zeros=symbolic_zeros, in_zeros=in_zeros)\n     return map(partial(maybe_jvp_tracer, self), primals_out, tangents_out)\n \n   def process_custom_transpose(self, prim, call, tracers, **params):\n@@ -734,11 +734,12 @@ def process_custom_vjp_call(self, prim, fun, fwd,\n     res, primals_out = split_list(res_and_primals_out, [res_tree.num_leaves])\n     avals_out = [core.get_aval(x).to_tangent_aval() for x in primals_out]\n \n-    tangents_in_zeros = map(instantiate_zeros, tangents_in)\n+    in_zeros = [type(t) is Zero for t in tangents_in]\n+    nz_tangents_in = [t for z, t in zip(in_zeros, tangents_in) if not z]\n     with core.set_current_trace(self.tangent_trace):\n       tangents_out = custom_lin_p.bind(\n-        *res, *tangents_in_zeros, num_res=res_tree.num_leaves, bwd=bwd,\n-        out_avals=avals_out, symbolic_zeros=symbolic_zeros)\n+          *res, *nz_tangents_in, num_res=res_tree.num_leaves, bwd=bwd,\n+          out_avals=avals_out, symbolic_zeros=symbolic_zeros, in_zeros=in_zeros)\n     tangent_nzs_out = [type(t) is not Zero for t in tangents_out]\n     return map(partial(maybe_linearize_tracer, self), primals_out, tangent_nzs_out, tangents_out)\n \n@@ -1223,7 +1224,7 @@ def raise_custom_vjp_error_on_jvp(*_, **__):\n \n def _custom_lin_transpose(cts_out, *invals, num_res,\n                           bwd: lu.WrappedFun, out_avals,\n-                          symbolic_zeros):\n+                          symbolic_zeros, in_zeros):\n   res, _ = split_list(invals, [num_res])\n   if symbolic_zeros:\n     cts_out = map(replace_internal_symbolic_zeros, cts_out)\n@@ -1231,7 +1232,8 @@ def _custom_lin_transpose(cts_out, *invals, num_res,\n     cts_out = map(instantiate_zeros, cts_out)\n   cts_in = bwd.call_wrapped(*res, *cts_out)\n   cts_in = map(replace_rule_output_symbolic_zeros, cts_in)\n-  return [None] * num_res + list(cts_in)\n+  nz_cts_in, _ = partition_list(in_zeros, cts_in)\n+  return [None] * num_res + nz_cts_in\n primitive_transposes[custom_lin_p] = _custom_lin_transpose\n \n \n\n```"
        ],
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "[Mosaic] Make `tpu.relayout` an explicit operation, merge in existing behavior, stop calling relayout() in apply.\n\nThis change should reduce complexity and make it easier to see what happened in a graph.\n\nNote - there are still cases where certain relayout() calls are not ops yet, those will be migrated in the future. Specifically, see the note in the CL around force_relayout.\n\nAdded helper methods to generate full like vectors.\n\nFollowup for subsequent CLs: Simplify Relayout rule in future CLs, maybe break up into smaller sub relayouts with nice names.\n\nFollowup for subsequent CLs: Unify transpose in here\n\nPiperOrigin-RevId: 758240646",
        "output": [
            "```diff\nCommit: 123022cae08d83c4d53ac77481b5c2391f003794\nDate: 2025-05-13T15:56:29Z\nURL: https://github.com/jax-ml/jax/commit/123022cae08d83c4d53ac77481b5c2391f003794\nFiles changed: 2\nAdditions: +40, Deletions: -31\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex d625e8bf4d6f..656be0e677b0 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -6811,6 +6811,7 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n   FAILUREOR_ASSIGN_OR_RETURN(\n       xla::Array<Value> src_tiles,\n       disassemble(builder, src, v, target_shape, /*use_implicit_shape=*/true));\n+\n   if (is_mask_pack) {\n     std::vector<int64_t> vmsks_shape(src_tiles.dimensions().begin(),\n                                      src_tiles.dimensions().end());\n@@ -6855,6 +6856,7 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n   auto assemble_with_mask_check = [&](xla::Array<Value> &tiles,\n                                       bool use_implicit_shape = false) {\n \n+\n     if (is_mask) {\n       auto zeros_tile = builder.create<arith::ConstantOp>(\n           tiles.begin()->getLoc(),\n@@ -6985,34 +6987,18 @@ LogicalResult tpu_relayout_rule(RewriteContext &ctx, Operation &op,\n \n   auto in_layout_array_attr =\n       tpu_relayout_op->getAttrOfType<ArrayAttr>(\"in_layout\");\n-  if (!in_layout_array_attr || in_layout_array_attr.empty()) {\n-    return tpu_relayout_op.emitOpError(\n-        \"missing or empty 'in_layout' attribute\");\n-  }\n   auto src_vla = dyn_cast<tpu::VectorLayoutAttr>(in_layout_array_attr[0]);\n-  if (!src_vla) {\n-    return tpu_relayout_op.emitOpError(\n-        \"'in_layout' attribute is not a VectorLayoutAttr\");\n-  }\n   VectorLayout src_layout = src_vla.getLayout().value();\n \n   auto out_layout_array_attr =\n       tpu_relayout_op->getAttrOfType<ArrayAttr>(\"out_layout\");\n-  if (!out_layout_array_attr || out_layout_array_attr.empty()) {\n-    return tpu_relayout_op.emitOpError(\n-        \"missing or empty 'out_layout' attribute\");\n-  }\n   auto dst_vla = dyn_cast<tpu::VectorLayoutAttr>(out_layout_array_attr[0]);\n-  if (!dst_vla) {\n-    return tpu_relayout_op.emitOpError(\n-        \"'out_layout' attribute is not a VectorLayoutAttr\");\n-  }\n   VectorLayout dst_layout = dst_vla.getLayout().value();\n \n   if (src_layout == dst_layout) {\n-    tpu_relayout_op.replaceAllUsesWith(tpu_relayout_op.getInput());\n-    tpu_relayout_op.erase();\n-    return success();\n+    return op.emitError(\n+        \"Source and destination layouts are the same - did you forget to run \"\n+        \"relayout-insertion-pass?\");\n   }\n \n   OpBuilder builder(&op);\n@@ -7079,9 +7065,6 @@ const llvm::StringMap<rule_type> &rules() {\n   return *rules;\n }\n \n-// TODO(apaszke): Implement a debug mode that inserts additional assertions.\n-// For example, we should verify that ops that were supposed to generate\n-// replicated outputs satisfy that requirement.\n LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n   // When an operation does not have any operands, the layout_in tuple is empty.\n   // If one of the operands is not of vector type, the corresponding entry in\n@@ -7117,14 +7100,11 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n                                  getOutLayouts(*def_op, ctx.target_shape));\n       const Layout lo = def_layouts[res_idx];\n       TPU_ASSERT_OP(lo.has_value());\n-      if (*lo == *li) {\n-        continue;\n+      if (*lo != *li) {\n+        return op.emitError(\n+            \"Invariant violation: Input layout does not match output layout - \"\n+            \"did you forget to run relayout-insertion?\");\n       }\n-      OpBuilder builder(&op);\n-      FAILUREOR_ASSIGN_OR_RETURN(\n-          Value new_v, relayout(ctx, builder, vector_operand, /*src=*/*lo,\n-                                /*dst=*/*li));\n-      op.setOperand(idx, new_v);\n     }\n   }\n \n@@ -7132,7 +7112,8 @@ LogicalResult applyLayoutOp(RewriteContext &ctx, Operation &op) {\n   // support for offsets outside of the first tile. When support is more broad,\n   // any op without support should check it within their own rule.\n   if (!isa<arith::TruncFOp, arith::TruncIOp, vector::BroadcastOp,\n-           vector::ExtractStridedSliceOp, vector::ShapeCastOp>(op)) {\n+           vector::ExtractStridedSliceOp, vector::ShapeCastOp, tpu::RelayoutOp>(\n+          op)) {\n     for (const Layout &layout : layouts_in) {\n       if (layout && layout->offsets()[1].has_value() &&\n           layout->offsets()[1].value() >= layout->tiling()[1]) {\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/relayout_insertion.cc b/jaxlib/mosaic/dialect/tpu/transforms/relayout_insertion.cc\nindex 6ddf8bd5ce66..178b97876b49 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/relayout_insertion.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/relayout_insertion.cc\n@@ -119,7 +119,26 @@ FailureOr<TypedValue<VectorType>> relayout(\n               dst_bitwidth_layout);\n     return cast<TypedValue<VectorType>>(cmp_op.getResult());\n   }\n-  return v;\n+  // Fall through to generic relayout.\n+  auto relayout_op =\n+      builder.create<tpu::RelayoutOp>(v.getLoc(), v.getType(), v);\n+  setLayout(relayout_op, src, dst);\n+\n+  return cast<TypedValue<VectorType>>(relayout_op.getResult());\n+}\n+\n+LogicalResult insertRelayout(Operation &op, int hardware_generation,\n+                             std::array<int64_t, 2> target_shape);\n+\n+LogicalResult insertRelayoutBlock(Block &block, int hardware_generation,\n+                                  const std::array<int64_t, 2> target_shape) {\n+  // We'll be modifying the block, so use early increment.\n+  for (Operation &op : make_early_inc_range(block)) {\n+    if (failed(insertRelayout(op, hardware_generation, target_shape))) {\n+      return failure();\n+    }\n+  }\n+  return success();\n }\n \n // TODO(jevinjiang): make relayout to an op so we don't need decide when to\n@@ -167,6 +186,15 @@ LogicalResult insertRelayout(Operation &op, int hardware_generation,\n                               /*dst=*/*li, hardware_generation, target_shape));\n     op.setOperand(idx, new_v);\n   }\n+\n+  for (auto &region : op.getRegions()) {\n+    for (auto &block : region.getBlocks()) {\n+      if (failed(\n+              insertRelayoutBlock(block, hardware_generation, target_shape))) {\n+        return failure();\n+      }\n+    }\n+  }\n   return success();\n }\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28589 from dfm:consolidate-custom-vjp-primitives\n\nPiperOrigin-RevId: 758235138",
        "output": [
            "```diff\nCommit: 1ad9eae3a28d2254500a4aa52e0af7d649a939ee\nDate: 2025-05-13T15:41:13Z\nURL: https://github.com/jax-ml/jax/commit/1ad9eae3a28d2254500a4aa52e0af7d649a939ee\nFiles changed: 8\nAdditions: +111, Deletions: -159\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex 144cbaf5cd21..aa9bfe9529ce 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -1079,17 +1079,17 @@ def jvp(*xs):\n     return [*primal_errs, *out_primals, *tangent_errs, *out_tangents]\n   return lu.wrap_init(jvp, debug_info=jvp_jaxpr_fun.debug_info)\n \n-def custom_vjp_call_jaxpr_rule(in_err, enabled_errors, *in_vals,\n-                               fun_jaxpr: core.ClosedJaxpr,\n-                               fwd_jaxpr_thunk, num_consts,\n-                               bwd: lu.WrappedFun, out_trees,\n-                               symbolic_zeros: bool):\n+def custom_vjp_call_rule(in_err, enabled_errors, *in_vals,\n+                         call_jaxpr: core.ClosedJaxpr,\n+                         fwd_jaxpr_thunk, num_consts,\n+                         bwd: lu.WrappedFun, out_trees,\n+                         symbolic_zeros: bool):\n   err_vals, err_tree = jtu.tree_flatten(in_err)\n   num_errs = err_tree.num_leaves\n   checkified_fun = lu.wrap_init(\n-      functools.partial(checkify_jaxpr_flat, fun_jaxpr.jaxpr,\n-                        fun_jaxpr.consts, enabled_errors, err_tree),\n-      debug_info=fun_jaxpr.jaxpr.debug_info)\n+      functools.partial(checkify_jaxpr_flat, call_jaxpr.jaxpr,\n+                        call_jaxpr.consts, enabled_errors, err_tree),\n+      debug_info=call_jaxpr.jaxpr.debug_info)\n   checkified_fun, fun_metadata = _flatten_and_get_error_metadata_thunk(\n       checkified_fun)\n \n@@ -1097,13 +1097,13 @@ def checkified_fwd(*args):\n     # TODO(lenamartens, sharadmv): why not checkify here?\n     xs, zeros = args[::2], args[1::2]\n     xs, zeros = xs[num_errs:], zeros[num_errs:]\n-    fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk(*zeros)\n+    fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk.call_wrapped(*zeros)\n     xs_without_consts = xs[num_consts:]\n     return core.eval_jaxpr(fwd_jaxpr, fwd_consts, *xs_without_consts)\n \n   # TODO(necula): the fwd result_paths are not quite the same as fun_jaxpr\n   checkified_fwd_wrapped = lu.wrap_init(checkified_fwd,\n-                                        debug_info=fun_jaxpr.jaxpr.debug_info)\n+                                        debug_info=fwd_jaxpr_thunk.debug_info)\n   bwd_ = lu.wrap_init(lambda *args: (*(None,)*num_errs, *bwd.call_wrapped(*args)),\n                       debug_info=bwd.debug_info)\n   checkified_fwd_wrapped, fwd_out_tree = flatten_fun_output(checkified_fwd_wrapped)\n@@ -1118,7 +1118,7 @@ def checkified_fwd(*args):\n   else:\n     out_err, out_vals = in_err, all_outs\n   return out_err, out_vals\n-error_checks[custom_derivatives.custom_vjp_call_jaxpr_p] = custom_vjp_call_jaxpr_rule\n+error_checks[custom_derivatives.custom_vjp_call_p] = custom_vjp_call_rule\n \n \n def check_discharge_rule(error, enabled_errors, *args, err_tree, debug):\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex dc8fc90e3d1f..dcd893f44123 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -425,16 +425,14 @@ def _custom_jvp_call_typecheck(_, *in_avals, call_jaxpr, jvp_jaxpr_fun,\n   return call_jaxpr.out_avals, call_jaxpr.effects\n core.custom_typechecks[custom_jvp_call_p] = _custom_jvp_call_typecheck\n \n-def _custom_jvp_call_mlir_translation(ctx, *args, call_jaxpr, jvp_jaxpr_fun,\n-                                      num_consts, symbolic_zeros):\n-  del jvp_jaxpr_fun, num_consts, symbolic_zeros\n+def _custom_jvp_vjp_call_lowering(ctx, *args, call_jaxpr, **_):\n   consts = mlir._ir_consts(call_jaxpr.consts)\n   out, tokens = mlir.jaxpr_subcomp(ctx.module_context, call_jaxpr.jaxpr,\n                                    ctx.name_stack, ctx.tokens_in, consts,\n                                    *args, dim_var_values=ctx.dim_var_values)\n   ctx.set_tokens_out(tokens)\n   return out\n-mlir.register_lowering(custom_jvp_call_p, _custom_jvp_call_mlir_translation)\n+mlir.register_lowering(custom_jvp_call_p, _custom_jvp_vjp_call_lowering)\n \n # If a (multi)linear function is defined with a custom jvp, then\n # custom_jvp_call_ can appear in jaxprs to be transposed. Since it's already\n@@ -936,8 +934,8 @@ def _temporary_dtype_exception(a, a_) -> bool:\n def _temporary_shape_exception(a, a_) -> bool:\n   return config.custom_vjp_disable_shape_check.value\n \n-class CustomVJPCallPrimitive(core.CallPrimitive):\n-  initial_style: core.Primitive\n+class CustomVJPCallPrimitive(core.Primitive):\n+  multiple_results = True\n \n   def bind(self, *args, **params):\n     return self._true_bind(*args, **params)\n@@ -946,107 +944,70 @@ def bind_with_trace(self, trace, args, params):\n     fun, fwd, bwd, tracers = args[0], args[1], args[2], args[3:]\n     return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n \n-custom_vjp_call_p = CustomVJPCallPrimitive('custom_vjp_call')\n+  def impl(self, fun, fwd, bwd, *args):\n+    raise NotImplementedError\n+\n+  def get_bind_params(self, params):\n+    new_params = dict(params)\n+    call_jaxpr: core.ClosedJaxpr = new_params.pop('call_jaxpr')\n+    num_consts: int = new_params.pop('num_consts')\n+    fwd_jaxpr_thunk = new_params.pop('fwd_jaxpr_thunk')\n+    fun = lu.wrap_init(core.jaxpr_as_fun(call_jaxpr),\n+                       debug_info=call_jaxpr.jaxpr.debug_info)\n+    fwd = lift_fwd(num_consts, fwd_jaxpr_thunk)\n+    const_avals, _ = split_list(call_jaxpr.in_avals, [num_consts])\n+    bwd = _handle_consts_in_bwd(new_params.pop('bwd'), const_avals)\n+    return [fun, fwd, bwd], new_params\n+\n+def lift_fwd(num_consts: int, fwd_jaxpr_thunk: lu.WrappedFun) -> lu.WrappedFun:\n+  def fwd(*args):\n+    vals, zeros = args[::2], args[1::2]\n+    assert len(vals) == len(zeros)\n+    _, primals = split_list(vals, [num_consts])\n+    const_zeros, in_zeros = split_list(zeros, [num_consts])\n+    if any(const_zeros):\n+      raise ad.CustomVJPException()\n+    fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk.call_wrapped(*in_zeros)\n+    return core.eval_jaxpr(fwd_jaxpr, fwd_consts, *primals)\n+  return lu.wrap_init(fwd, debug_info=fwd_jaxpr_thunk.debug_info)\n \n-def _custom_vjp_call_jaxpr_impl(*args, fun_jaxpr, **_):\n-  return core.jaxpr_as_fun(fun_jaxpr)(*args)\n+@lu.transformation2\n+def _handle_consts_in_bwd(f, const_avals, *args):\n+  return [Zero(a) for a in const_avals] + list(f(*args))\n \n-def _custom_vjp_call_jaxpr_abstract_eval(*_, fun_jaxpr, **__):\n-  disallowed_effects = effects.custom_derivatives_allowed_effects.filter_not_in(fun_jaxpr.effects)\n+custom_vjp_call_p = CustomVJPCallPrimitive('custom_vjp_call')\n+mlir.register_lowering(custom_vjp_call_p, _custom_jvp_vjp_call_lowering)\n+\n+def _custom_vjp_call_typecheck(_, *in_avals, call_jaxpr, **kwargs):\n+  del in_avals, kwargs\n+  disallowed_effects = effects.custom_derivatives_allowed_effects.filter_not_in(\n+      call_jaxpr.effects)\n   if disallowed_effects:\n     raise NotImplementedError(\n         f'Effects not supported in `custom_vjp`: {disallowed_effects}')\n-  return fun_jaxpr.out_avals, fun_jaxpr.effects\n-\n-custom_vjp_call_jaxpr_p = core.Primitive('custom_vjp_call_jaxpr')\n-custom_vjp_call_jaxpr_p.multiple_results = True\n-custom_vjp_call_jaxpr_p.def_impl(_custom_vjp_call_jaxpr_impl)\n-custom_vjp_call_jaxpr_p.def_effectful_abstract_eval(_custom_vjp_call_jaxpr_abstract_eval)\n-CustomVJPCallPrimitive.initial_style = custom_vjp_call_jaxpr_p\n-\n-mlir.register_lowering(custom_vjp_call_jaxpr_p, mlir.lower_fun(\n-    _custom_vjp_call_jaxpr_impl, multiple_results=True))\n-\n-def _custom_vjp_call_jaxpr_jvp(\n-    primals, tangents, *, fun_jaxpr: core.ClosedJaxpr,\n-    fwd_jaxpr_thunk: Callable[..., tuple[core.Jaxpr, Sequence[Any]]],\n-    num_consts: int, bwd: lu.WrappedFun,\n-    out_trees: Callable[[], Sequence[PyTreeDef]],\n-    symbolic_zeros: bool):\n-  _, args = split_list(primals, [num_consts])\n-  consts_dot, args_dot = split_list(tangents, [num_consts])\n-  if any(type(t) is not Zero for t in consts_dot):\n-    raise ad.CustomVJPException()\n-  zeros = [type(t) is not Zero for t in args_dot]\n-  fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk(*zeros)  # consts can be tracers!\n-  _, res_tree = out_trees()\n-  res_and_primals_out = core.eval_jaxpr(fwd_jaxpr, fwd_consts, *args)\n-  res, primals_out = split_list(res_and_primals_out, [res_tree.num_leaves])\n-  avals_out = [core.get_aval(x).to_tangent_aval() for x in primals_out]\n-  args_dot = map(ad.instantiate_zeros, args_dot)\n-  tangents_out = ad.custom_lin_p.bind(\n-      *res, *args_dot, num_res=res_tree.num_leaves, bwd=bwd,\n-      out_avals=avals_out, symbolic_zeros=symbolic_zeros)\n-  tangents_out = map(lax.tie_p.bind, primals_out, tangents_out)\n-  return primals_out, tangents_out\n-ad.primitive_jvps[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_jvp\n-\n-def _custom_vjp_call_jaxpr_vmap(\n-    axis_data, args, in_dims, *,\n-    fun_jaxpr: core.ClosedJaxpr,\n-    fwd_jaxpr_thunk: Callable[..., tuple[core.Jaxpr, Sequence[Any]]],\n-    num_consts: int, bwd: lu.WrappedFun,\n-    out_trees: Callable, symbolic_zeros: bool):\n-  args = [batching.moveaxis(x, d, 0) if d is not not_mapped and d != 0\n-          else x for x, d in zip(args, in_dims)]\n-  in_batched = [d is not not_mapped for d in in_dims]\n-  _, args_batched = split_list(in_batched, [num_consts])\n-  batched_fun_jaxpr, out_batched = batching.batch_jaxpr(\n-      fun_jaxpr, axis_data, in_batched, False)\n-  out_dims1 = [0 if b else not_mapped for b in out_batched]\n-  out_dims2 = []\n-\n-  @pe._memoize\n-  def batched_fwd_jaxpr_thunk(*zeros):\n-    fwd_jaxpr = core.ClosedJaxpr(*fwd_jaxpr_thunk(*zeros))  # consts can be tracers\n-    batched_fwd_jaxpr, out_batched = batching.batch_jaxpr(\n-        fwd_jaxpr, axis_data, args_batched, False)\n-    out_dims2.append([0 if b else not_mapped for b in out_batched])\n-    return batched_fwd_jaxpr.jaxpr, batched_fwd_jaxpr.consts\n-\n-  fwd_args_batched = [0 if b else not_mapped for b in args_batched]\n-  fwd_out_dims = lambda: out_dims2[0]\n-  tag = core.TraceTag()\n-  batched_bwd = batching.batch_custom_vjp_bwd(\n-    bwd, tag, axis_data, fwd_out_dims, fwd_args_batched)\n-\n-  batched_outs = custom_vjp_call_jaxpr_p.bind(\n-      *args, fun_jaxpr=batched_fun_jaxpr,\n-      fwd_jaxpr_thunk=batched_fwd_jaxpr_thunk, bwd=batched_bwd,\n-      num_consts=num_consts, out_trees=out_trees, symbolic_zeros=symbolic_zeros)\n-  out_dims = out_dims2[0] if out_dims2 else out_dims1\n-  return batched_outs, out_dims\n-batching.fancy_primitive_batchers[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_vmap\n+  return call_jaxpr.out_avals, call_jaxpr.effects\n+core.custom_typechecks[custom_vjp_call_p] = _custom_vjp_call_typecheck\n \n-def _custom_vjp_call_jaxpr_dce(\n+def _custom_vjp_call_dce(\n     used_outs: Sequence[bool], eqn: core.JaxprEqn\n ) -> tuple[list[bool], core.JaxprEqn | None]:\n   if not any(used_outs) and not pe.has_effects(eqn):\n     return [False] * len(eqn.invars), None\n-  fun_jaxpr: core.ClosedJaxpr = eqn.params[\"fun_jaxpr\"]\n+  call_jaxpr: core.ClosedJaxpr = eqn.params[\"call_jaxpr\"]\n   fwd_jaxpr_thunk = eqn.params[\"fwd_jaxpr_thunk\"]\n   bwd: lu.WrappedFun = eqn.params[\"bwd\"]\n   out_trees: Callable[[], Sequence[PyTreeDef]] = eqn.params[\"out_trees\"]\n   symbolic_zeros: bool = eqn.params[\"symbolic_zeros\"]\n-  dce_fun_jaxpr: core.ClosedJaxpr\n+  dce_call_jaxpr: core.ClosedJaxpr\n   used_ins: Sequence[bool]\n-  dce_fun_jaxpr, used_ins = _cached_closed_call_dce_instantiate(\n-      fun_jaxpr, tuple(used_outs))\n+  dce_call_jaxpr, used_ins = _cached_closed_call_dce_instantiate(\n+      call_jaxpr, tuple(used_outs))\n   assert all(used_ins)\n \n+  @partial(lu.wrap_init, debug_info=fwd_jaxpr_thunk.debug_info)\n   @pe._memoize\n   def dce_fwd_jaxpr_thunk(*zeros):\n-    fwd_jaxpr = core.ClosedJaxpr(*fwd_jaxpr_thunk(*zeros))\n+    fwd_jaxpr = core.ClosedJaxpr(*fwd_jaxpr_thunk.call_wrapped(*zeros))\n     _, res_tree = out_trees()\n     num_res = res_tree.num_leaves\n     dce_fwd_jaxpr, _ = _cached_closed_call_dce_instantiate(\n@@ -1058,7 +1019,7 @@ def dce_bwd(*args):\n     res, cts = split_list(args, [res_tree.num_leaves])\n     cts_ = iter(cts)\n     all_cts = []\n-    for used, aval in zip(used_outs, fun_jaxpr.out_avals):\n+    for used, aval in zip(used_outs, call_jaxpr.out_avals):\n       if used:\n         all_cts.append(next(cts_))\n       else:\n@@ -1075,17 +1036,15 @@ def dce_bwd(*args):\n   outvars = [v for used, v in zip(used_outs, eqn.outvars) if used]\n   new_params = dict(\n       eqn.params,\n-      fun_jaxpr=dce_fun_jaxpr,\n+      call_jaxpr=dce_call_jaxpr,\n       fwd_jaxpr_thunk=dce_fwd_jaxpr_thunk,\n       bwd=dce_bwd_wrapped,\n   )\n   new_eqn = pe.new_jaxpr_eqn(\n-      eqn.invars, outvars, eqn.primitive, new_params, dce_fun_jaxpr.effects,\n+      eqn.invars, outvars, eqn.primitive, new_params, dce_call_jaxpr.effects,\n       eqn.source_info, eqn.ctx)\n   return list(used_ins), new_eqn\n-pe.dce_rules[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_dce\n-\n-xla.register_initial_style_primitive(custom_vjp_call_jaxpr_p)\n+pe.dce_rules[custom_vjp_call_p] = _custom_vjp_call_dce\n \n batching.primitive_batchers[ad.custom_lin_p] = ad.raise_custom_vjp_error_on_jvp\n mlir.register_lowering(ad.custom_lin_p, ad.raise_custom_vjp_error_on_jvp)\n@@ -1586,7 +1545,6 @@ def jvp(primals, tangents):\n # TODO(mattjj): remove these stubs, which exist to avoid breaking internal users\n custom_jvp_call_jaxpr_p = core.Primitive(\"custom_jvp_call_jaxpr\")\n \n-\n # The following is a helper for optimizing the behavior of custom_vjp when used\n # under remat. This is really only useful when the `fwd` function to custom_vjp\n # executes a black box kernel. Otherwise, DCE will perform this optimization\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 64226a789cde..5866b0c5f8eb 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -434,49 +434,45 @@ def process_custom_vjp_call(self, prim, f, fwd, bwd, tracers, out_trees, symboli\n     if all(t.is_known() for t in tracers):\n       vals = [t.pval[1] for t in tracers]\n       with core.set_current_trace(self.parent_trace):\n-        return prim.bind(f, fwd, bwd, *vals, out_trees=out_trees, symbolic_zeros=symbolic_zeros)\n-    else:\n-      # TODO(mattjj): remove non-ad users of partial eval, then drop this case.\n-      # We stage out the whole thing, i.e. no nontrivial partial evaluation.\n-      tracers = map(self.instantiate_const_abstracted, tracers)\n-      # Because we instantiate all tracers, in_knowns is all False.\n-      in_knowns, in_avals, () = partition_pvals([t.pval for t in tracers])\n-      f = trace_to_subjaxpr_nounits(f, self, True, f.debug_info)\n-      f, aux = partial_eval_wrapper_nounits(f, (*in_knowns,), (*in_avals,))\n-      with core.set_current_trace(self.parent_trace):\n-        out_flat = prim.bind(f, fwd, bwd, out_trees=out_trees,\n-                             symbolic_zeros=symbolic_zeros)\n-      out_knowns, out_avals, jaxpr, env = aux()\n-      out_consts, res = split_list(out_flat, [len(out_flat)-len(jaxpr.constvars)])\n-      res_tracers = map(self.new_instantiated_const, res)\n-      env_tracers = map(self.to_jaxpr_tracer, env)\n-      out_tracers = [JaxprTracer(self, PartialVal.unknown(a), None)\n-                    for a in out_avals]\n-      closed_jaxpr = core.ClosedJaxpr(convert_constvars_jaxpr(jaxpr), ())\n-\n-      @_memoize\n-      def fwd_jaxpr_thunk(*zeros):\n-        fwd_ = _interleave_fun(fwd, zeros)\n-        fwd_ = trace_to_subjaxpr_nounits(fwd_, self, True, fwd_.debug_info)\n-        fwd_, aux = partial_eval_wrapper_nounits(fwd_, (*in_knowns,), (*in_avals,))\n-        out_flat = fwd_.call_wrapped()\n-        out_knowns, out_avals, jaxpr, env = aux()\n-        _, res = split_list(out_flat, [len(out_flat)-len(jaxpr.constvars)])\n-        converted_jaxpr = convert_envvars_to_constvars(jaxpr, len(env))\n-        return converted_jaxpr, (*res, *env)\n+        return prim.bind(f, fwd, bwd, *vals, out_trees=out_trees,\n+                         symbolic_zeros=symbolic_zeros)\n+\n+    tracers = map(self.instantiate_const, tracers)\n+    in_knowns = (False,) * len(tracers)\n+    in_avals = tuple(t.aval for t in tracers)\n+    f_ = trace_to_subjaxpr_nounits2(f, self.tag, f.debug_info, True)\n+    f_, aux = partial_eval_wrapper_nounits(f_, in_knowns, in_avals)\n+    params = dict(out_trees=out_trees, symbolic_zeros=symbolic_zeros)\n+    res = prim.bind_with_trace(self.parent_trace, (f_, fwd, bwd), params)\n+    out_knowns, out_avals, jaxpr, env = aux()\n+    assert not any(out_knowns)\n+    res_tracers = map(self.instantiate_const, map(self.new_const, res))\n+    env_tracers = map(self.to_jaxpr_tracer, env)\n+    out_tracers = [JaxprTracer(self, PartialVal.unknown(a), None)\n+                   for a in out_avals]\n+    closed_jaxpr = close_jaxpr(convert_constvars_jaxpr(jaxpr))\n+\n+    @partial(lu.wrap_init, debug_info=fwd.debug_info)\n+    @_memoize\n+    def fwd_jaxpr_thunk(*zeros):\n+      fwd_ = _interleave_fun(fwd, zeros)\n+      fwd_jaxpr, _, consts, () = trace_to_jaxpr_dynamic(fwd_, in_avals)\n+      return fwd_jaxpr, consts\n \n     name_stack = self._current_truncated_name_stack()\n     source = source_info_util.current().replace(name_stack=name_stack)\n+    params = dict(\n+        call_jaxpr=closed_jaxpr,\n+        fwd_jaxpr_thunk=fwd_jaxpr_thunk,\n+        num_consts=len(res) + len(env),\n+        bwd=bwd,\n+        out_trees=out_trees,\n+        symbolic_zeros=symbolic_zeros\n+    )\n     eqn = new_eqn_recipe((*res_tracers, *env_tracers, *tracers),\n-                         out_tracers, prim.initial_style,\n-                         dict(fun_jaxpr=closed_jaxpr,\n-                              fwd_jaxpr_thunk=fwd_jaxpr_thunk,\n-                              num_consts=len(res) + len(env),\n-                              bwd=bwd, out_trees=out_trees,\n-                              symbolic_zeros=symbolic_zeros),\n-                         jaxpr.effects, source)\n+                         out_tracers, prim, params, jaxpr.effects, source)\n     for t in out_tracers: t.recipe = eqn\n-    return merge_lists(out_knowns, out_tracers, out_consts)\n+    return out_tracers\n \n def partition_pvals(\n     pvals: list[PartialVal]\n@@ -2050,6 +2046,7 @@ def process_custom_jvp_call(self, prim, fun: lu.WrappedFun,\n     fun_jaxpr, out_avals, consts, () = trace_to_jaxpr_dynamic(fun, in_avals)\n     closed_fun_jaxpr = core.ClosedJaxpr(convert_constvars_jaxpr(fun_jaxpr), ())\n \n+    @partial(lu.wrap_init, debug_info=jvp.debug_info)\n     @_memoize\n     def jvp_jaxpr_thunk(*in_zeros):\n       for store in jvp.stores: store and store.reset()\n@@ -2065,8 +2062,7 @@ def jvp_jaxpr_thunk(*in_zeros):\n     outvars = map(self.makevar, out_tracers)\n     eqn = new_jaxpr_eqn([*constvars, *invars], outvars, prim,\n                         dict(call_jaxpr=closed_fun_jaxpr,\n-                             jvp_jaxpr_fun=lu.wrap_init(jvp_jaxpr_thunk,\n-                                                        debug_info=jvp.debug_info),\n+                             jvp_jaxpr_fun=jvp_jaxpr_thunk,\n                              num_consts=len(consts),\n                              symbolic_zeros=symbolic_zeros),\n                         fun_jaxpr.effects,\n@@ -2086,6 +2082,7 @@ def process_custom_vjp_call(self, prim: core.Primitive,\n     fun_jaxpr, out_avals, consts, _ = trace_to_jaxpr_dynamic(fun, in_avals)\n     closed_fun_jaxpr = core.ClosedJaxpr(convert_constvars_jaxpr(fun_jaxpr), ())\n \n+    @partial(lu.wrap_init, debug_info=fwd.debug_info)\n     @_memoize\n     def fwd_jaxpr_from_zeros(*zeros):\n       for store in fwd.stores: store and store.reset()\n@@ -2098,9 +2095,8 @@ def fwd_jaxpr_from_zeros(*zeros):\n     invars = map(self.getvar, tracers)\n     constvars = map(self.getvar, map(to_jaxpr_tracer, consts))\n     outvars = map(self.makevar, out_tracers)\n-    eqn = new_jaxpr_eqn([*constvars, *invars], outvars,\n-                        prim.initial_style,  # pytype: disable=attribute-error\n-                        dict(fun_jaxpr=closed_fun_jaxpr,\n+    eqn = new_jaxpr_eqn([*constvars, *invars], outvars, prim,\n+                        dict(call_jaxpr=closed_fun_jaxpr,\n                              fwd_jaxpr_thunk=fwd_jaxpr_from_zeros,\n                              num_consts=len(consts),\n                              bwd=bwd, out_trees=out_trees,\ndiff --git a/jax/_src/pallas/cost_estimate.py b/jax/_src/pallas/cost_estimate.py\nindex 3b82d3095f64..93bcf5348b24 100644\n--- a/jax/_src/pallas/cost_estimate.py\n+++ b/jax/_src/pallas/cost_estimate.py\n@@ -238,15 +238,15 @@ def _pjit_cost_rule(ctx, *, jaxpr: jax_core.ClosedJaxpr, **_):\n   )\n register_cost_rule(pjit.pjit_p, _pjit_cost_rule)\n \n-def _custom_vjp_rule(ctx, *, fun_jaxpr: jax_core.ClosedJaxpr, **_):\n+def _custom_vjp_rule(ctx, *, call_jaxpr: jax_core.ClosedJaxpr, **_):\n   del ctx\n-  inner_cost = cost_estimate_jaxpr(fun_jaxpr)\n+  inner_cost = cost_estimate_jaxpr(call_jaxpr)\n   return CostEstimate(\n       flops=inner_cost.flops,\n       transcendentals=inner_cost.transcendentals,\n       bytes_accessed=inner_cost.bytes_accessed,\n   )\n-register_cost_rule(custom_derivatives.custom_vjp_call_jaxpr_p, _custom_vjp_rule)\n+register_cost_rule(custom_derivatives.custom_vjp_call_p, _custom_vjp_rule)\n \n def _run_state_rule(*_, jaxpr: jax_core.Jaxpr, **_2):\n   inner_cost = cost_estimate_jaxpr(pe.close_jaxpr(jaxpr))\ndiff --git a/jax/custom_derivatives.py b/jax/custom_derivatives.py\nindex 3628ae4aaa6e..b768b687dfad 100644\n--- a/jax/custom_derivatives.py\n+++ b/jax/custom_derivatives.py\n@@ -26,7 +26,6 @@\n   custom_jvp_call_jaxpr_p as custom_jvp_call_jaxpr_p,\n   custom_vjp as custom_vjp,\n   custom_vjp_call_p as custom_vjp_call_p,\n-  custom_vjp_call_jaxpr_p as custom_vjp_call_jaxpr_p,\n   custom_vjp_primal_tree_values as custom_vjp_primal_tree_values,\n   CustomVJPPrimal as CustomVJPPrimal,\n   linear_call as linear_call,\ndiff --git a/jax/experimental/jax2tf/jax2tf.py b/jax/experimental/jax2tf/jax2tf.py\nindex 786e021e2ff0..536bf1f201f0 100644\n--- a/jax/experimental/jax2tf/jax2tf.py\n+++ b/jax/experimental/jax2tf/jax2tf.py\n@@ -3461,14 +3461,14 @@ def _custom_jvp_call(*args: TfVal, call_jaxpr: core.ClosedJaxpr,\n tf_impl[custom_derivatives.custom_jvp_call_p] = _custom_jvp_call\n \n \n-def _custom_vjp_call_jaxpr(*args: TfVal, fun_jaxpr: core.ClosedJaxpr,\n-                           **_) -> Sequence[TfVal]:\n+def _custom_vjp_call(*args: TfVal, call_jaxpr: core.ClosedJaxpr,\n+                     **_) -> Sequence[TfVal]:\n   # TODO(necula): ensure that there is no AD transformation in scope\n-  return _interpret_jaxpr(fun_jaxpr, *args, extra_name_stack=\"custom_vjp\",\n+  return _interpret_jaxpr(call_jaxpr, *args, extra_name_stack=\"custom_vjp\",\n                           fresh_constant_cache=False)\n \n \n-tf_impl[custom_derivatives.custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr\n+tf_impl[custom_derivatives.custom_vjp_call_p] = _custom_vjp_call\n \n \n def _custom_lin(*args: TfVal, **_) -> Sequence[TfVal]:\ndiff --git a/jax/extend/core/primitives.py b/jax/extend/core/primitives.py\nindex 60d8cd24a949..30350dace637 100644\n--- a/jax/extend/core/primitives.py\n+++ b/jax/extend/core/primitives.py\n@@ -26,7 +26,6 @@\n   custom_jvp_call_p as custom_jvp_call_p,\n   custom_jvp_call_jaxpr_p as custom_jvp_call_jaxpr_p,\n   custom_vjp_call_p as custom_vjp_call_p,\n-  custom_vjp_call_jaxpr_p as custom_vjp_call_jaxpr_p,\n )\n \n from jax._src.dispatch import device_put_p as device_put_p\ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 72c14634a9c8..73dc2fbefcaa 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3065,7 +3065,7 @@ def check_jaxpr(jaxpr, used_outs, includes, excludes):\n       if not dce_jaxpr.eqns:\n         assert not includes\n         return\n-      call_jaxpr = dce_jaxpr.eqns[0].params[\"fun_jaxpr\"]\n+      call_jaxpr = dce_jaxpr.eqns[0].params[\"call_jaxpr\"]\n       for prim in includes:\n         assert any(eqn.primitive == prim for eqn in call_jaxpr.eqns)\n       for prim in excludes:\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add layout inference and lowering for `scf.WhileOp` and enable tests.\n\nPiperOrigin-RevId: 758205399",
        "output": [
            "```diff\nCommit: 66476a0b4c20753d79b0960bc476167589691e67\nDate: 2025-05-13T14:12:52Z\nURL: https://github.com/jax-ml/jax/commit/66476a0b4c20753d79b0960bc476167589691e67\nFiles changed: 4\nAdditions: +243, Deletions: -62\ndiff --git a/jax/experimental/mosaic/gpu/dialect_lowering.py b/jax/experimental/mosaic/gpu/dialect_lowering.py\nindex c1506bde32ea..320ae32607e9 100644\n--- a/jax/experimental/mosaic/gpu/dialect_lowering.py\n+++ b/jax/experimental/mosaic/gpu/dialect_lowering.py\n@@ -1123,6 +1123,54 @@ def _unflatten_ir_values(\n   return result\n \n \n+def _move_scf_block_to_block_with_flattened_arguments(\n+    ctx: LoweringContext,\n+    old_block: ir.Block,\n+    new_block: ir.Block,\n+    last_op_type: type[ir.OpView],\n+    args_template: Sequence[_VectorTemplate | None],\n+    *new_leading_args: Sequence[ir.Value],\n+) -> Sequence[_VectorTemplate | None]:\n+  \"\"\"Moves the operations from `old_block` to `new_block`.\n+\n+  The input arguments to the block, if any, are flattened using the provided\n+  `args_template`, except for any new_leading_args which are simply prepended\n+  to the flattened arguments and must be part of the template.\n+\n+  The last operation of the old block must be of type `last_op_type` which\n+  is expected to be either a `scf.YieldOp` or a `scf.ConditionOp`. This\n+  operation is recreated with flattened output arguments.\n+  \"\"\"\n+  out_template = None\n+  with ir.InsertionPoint(new_block):\n+    new_carry = _unflatten_ir_values(new_block.arguments[len(new_leading_args):], args_template)\n+    new_args = new_leading_args + tuple(new_carry)\n+    for old_arg, new_arg in zip(old_block.arguments, new_args, strict=True):\n+      old_arg.replace_all_uses_with(new_arg)\n+    for op in [*old_block]:\n+      if not isinstance(op, last_op_type):\n+        mgpu.private_operation_remove_from_parent(op)\n+        mgpu.private_block_append_owned_operation(new_block, op)\n+        ctx.lower_op(op)\n+      else:\n+        assert out_template is None\n+        layouts = (\n+            inference_utils.in_layouts(op)\n+            if inference_utils.has_in_layouts_set(op)\n+            else []\n+        )\n+        if isinstance(op, scf.YieldOp):\n+          flat_operands, out_template = _flatten_ir_values(op.operands, layouts)\n+          scf.yield_(flat_operands)\n+        elif isinstance(op, scf.ConditionOp):\n+          flat_carry, out_template = _flatten_ir_values(op.args, layouts)\n+          scf.condition(op.condition, flat_carry)\n+        else:\n+          raise NotImplementedError(f\"Unsupported op type: {op}\")\n+        op.erase()\n+  assert out_template is not None\n+  return out_template\n+\n @_register_lowering(scf.ForOp)\n def _for_op_lowering_rule(\n     ctx: LoweringContext, for_op: scf.ForOp\n@@ -1145,33 +1193,78 @@ def _for_op_lowering_rule(\n       for_op.step,\n       flat_init_args,\n   )\n-  with ir.InsertionPoint(new_for_op.body):\n-    recreated_carry = _unflatten_ir_values(\n-        new_for_op.body.arguments[1:], args_template\n-    )\n-    ops_to_lower = []\n-    for op in [*for_op.body]:\n-      if op == yield_op:\n-        continue\n-      mgpu.private_operation_remove_from_parent(op)\n-      mgpu.private_block_append_owned_operation(new_for_op.body, op)\n-      ops_to_lower.append(op)\n-    new_args = (new_for_op.induction_variable, *recreated_carry)\n-    for old_carry, new_carry in zip(for_op.body.arguments, new_args, strict=True):\n-      old_carry.replace_all_uses_with(new_carry)\n-\n-  for op in ops_to_lower:\n-    with ir.InsertionPoint(op):\n-      ctx.lower_op(op)\n \n-  with ir.InsertionPoint(new_for_op.body):\n-    flat_operands, _ = _flatten_ir_values(yield_op.operands, in_layouts)\n-    yield_op.erase()\n-    scf.yield_(flat_operands)\n+  _move_scf_block_to_block_with_flattened_arguments(\n+      ctx,\n+      for_op.body,\n+      new_for_op.body,\n+      scf.YieldOp,\n+      args_template,\n+      new_for_op.induction_variable,\n+  )\n \n   return _unflatten_ir_values(new_for_op.results, args_template)\n \n \n+@_register_lowering(scf.WhileOp)\n+def _while_op_lowering_rule(\n+    ctx: LoweringContext, while_op: scf.WhileOp\n+) -> MlirLoweringRuleResult:\n+  if not inference_utils.should_have_layout(while_op):\n+    return _traverse_op_lowering_rule(ctx, while_op)\n+\n+  before_block = while_op.before.blocks[0]\n+  after_block = while_op.after.blocks[0]\n+  condition_op = before_block.operations[len(before_block.operations) - 1]\n+  yield_op = after_block.operations[len(after_block.operations) - 1]\n+\n+  in_layouts = inference_utils.in_layouts(while_op)\n+  out_layouts = inference_utils.out_layouts(while_op)\n+\n+  if in_layouts:\n+    yield_layouts = inference_utils.in_layouts(yield_op)\n+    if in_layouts != yield_layouts:\n+      raise ValueError(\n+          f\"Input layouts {in_layouts} do not match yield layouts\"\n+          f\" {yield_layouts}\"\n+      )\n+\n+  if out_layouts:\n+    condition_layouts = inference_utils.in_layouts(condition_op)\n+    if out_layouts != condition_layouts:\n+      raise ValueError(\n+          f\"Output layouts {out_layouts} do not match condition layouts\"\n+          f\" {condition_layouts}\"\n+      )\n+\n+  flat_inits, inits_template = _flatten_ir_values(while_op.inits, in_layouts)\n+  result_types = _infer_flat_result_types(while_op, out_layouts)\n+  new_while_op = scf.WhileOp(result_types, flat_inits)\n+\n+  # Before block\n+  init_types = [v.type for v in flat_inits]\n+  new_before_block = new_while_op.before.blocks.append(*init_types)\n+  results_template = _move_scf_block_to_block_with_flattened_arguments(\n+      ctx,\n+      before_block,\n+      new_before_block,\n+      scf.ConditionOp,\n+      inits_template,\n+  )\n+\n+  # After block\n+  new_after_block = new_while_op.after.blocks.append(*result_types)\n+  _move_scf_block_to_block_with_flattened_arguments(\n+      ctx,\n+      after_block,\n+      new_after_block,\n+      scf.YieldOp,\n+      results_template,\n+  )\n+\n+  return _unflatten_ir_values(new_while_op.results, results_template)\n+\n+\n def _infer_flat_result_types(\n     op: ir.OpView, out_layouts: Sequence[ir.Attribute]\n ) -> Sequence[ir.Type]:\n@@ -1221,19 +1314,9 @@ def _index_switch_op_lowering_rule(\n   ):\n     [block] = region.blocks\n     new_block = new_region.blocks.append()\n-    with ir.InsertionPoint(new_block):\n-      for op in [*block]:\n-        if not isinstance(op, scf.YieldOp):\n-          mgpu.private_operation_remove_from_parent(op)\n-          mgpu.private_block_append_owned_operation(new_block, op)\n-          ctx.lower_op(op)\n-          continue\n-        if inference_utils.in_layouts(op) != out_layouts:\n-          raise ValueError(\"Layout mismatch\")\n-        flat_results, results_template = _flatten_ir_values(\n-            op.operands, out_layouts\n-        )\n-        scf.yield_(flat_results)\n+    results_template = _move_scf_block_to_block_with_flattened_arguments(\n+        ctx, block, new_block, scf.YieldOp, []\n+    )\n   return _unflatten_ir_values(new_switch_op.results, results_template)\n \n \ndiff --git a/jax/experimental/mosaic/gpu/layout_inference.py b/jax/experimental/mosaic/gpu/layout_inference.py\nindex b39dc933ce9d..c010bf181bce 100644\n--- a/jax/experimental/mosaic/gpu/layout_inference.py\n+++ b/jax/experimental/mosaic/gpu/layout_inference.py\n@@ -336,38 +336,61 @@ def _infer_constant_op_layout(constant_op: arith.ConstantOp) -> OptionalLayouts:\n   return [], [layout]\n \n \n-@partial(_add_layout_inference_rule, scf.YieldOp)\n-def _infer_yield_op_layout(op: scf.YieldOp) -> OptionalLayouts:\n+def _layouts_from_values(values: Sequence[ir.Value]) -> list[ir.Attribute] | None:\n   layouts = []\n-  for result in op.results_:\n-    if not ir.VectorType.isinstance(result.type):\n+  for value in values:\n+    if not ir.VectorType.isinstance(value.type):\n       continue\n-    if (layout := inference_utils.value_layout(result)) is not None:\n+    if (layout := inference_utils.value_layout(value)) is not None:\n       if layouts_lib.is_splat_fragmented_layout(layout):\n         return None\n       layouts.append(layout)\n     else:\n       # Not all layouts could be inferred for vector ops. Return for now.\n       return None\n+  return layouts\n \n+@partial(_add_layout_inference_rule, scf.YieldOp)\n+def _infer_yield_op_layout(op: scf.YieldOp) -> OptionalLayouts:\n+  layouts = _layouts_from_values(op.results_)\n+  if layouts is None:\n+    return None\n   return (layouts, [])\n \n \n+@partial(_add_layout_inference_rule, scf.ConditionOp)\n+def _infer_condition_op_layout(op: scf.ConditionOp) -> OptionalLayouts:\n+  layouts = _layouts_from_values(op.args)\n+  if layouts is None:\n+    return None\n+  return (layouts, [])\n+\n+\n+def _last_op(region: ir.Region, expected_op_type: type[ir.OpView]):\n+  [block] = region.blocks\n+  last_op = block.operations[len(block.operations) - 1]\n+  assert isinstance(last_op, expected_op_type)\n+  return last_op\n+\n+\n+def _infer_from_op(op: ir.OpView) -> list[ir.Attribute] | None:\n+  if not inference_utils.has_in_layouts_set(op):\n+    return None\n+  in_layouts = list(inference_utils.in_layouts(op))\n+  if any(\n+      layouts_lib.is_splat_fragmented_layout(layout)\n+      for layout in in_layouts\n+  ):\n+    return None\n+  return in_layouts\n+\n+\n def _infer_from_yield_ops(op: ir.Operation) -> list[ir.Attribute] | None:\n   candidates = []\n   for region in op.regions:\n-    [block] = region.blocks\n-    yield_op = block.operations[len(block.operations) - 1]\n-    assert isinstance(yield_op, scf.YieldOp)\n-    if not inference_utils.has_in_layouts_set(yield_op):\n-      continue\n-    yield_layouts = inference_utils.in_layouts(yield_op)\n-    if any(\n-        layouts_lib.is_splat_fragmented_layout(layout)\n-        for layout in yield_layouts\n-    ):\n-      continue\n-    candidates.append(yield_layouts)\n+    yield_layouts = _infer_from_op(_last_op(region, scf.YieldOp))\n+    if yield_layouts is not None:\n+      candidates.append(yield_layouts)\n   if not candidates:\n     return None\n   return [_choose_representative_layout(set(c)) for c in zip(*candidates)]\n@@ -382,6 +405,27 @@ def _infer_for_op_layout(op: scf.ForOp) -> OptionalLayouts:\n   return None\n \n \n+@partial(_add_layout_inference_rule, scf.WhileOp)\n+def _infer_while_op_layout(op: scf.WhileOp) -> OptionalLayouts:\n+  # TODO(dasenov): we don't attempt to propagate from outside for the moment.\n+\n+  # Note that the inputs or results do not necessarily contain vector types. If\n+  # there is no vector type, the corresponding layouts (in_layouts or\n+  # out_layouts) should be an empty list.\n+\n+  yield_op = _last_op(op.after, scf.YieldOp)\n+  needs_in_layouts = inference_utils.should_have_layout(yield_op)\n+  in_layouts = _infer_from_op(yield_op) if needs_in_layouts else []\n+\n+  condition_op = _last_op(op.before, scf.ConditionOp)\n+  needs_out_layouts = inference_utils.should_have_layout(condition_op)\n+  out_layouts = _infer_from_op(condition_op) if needs_out_layouts else []\n+\n+  if in_layouts is None or out_layouts is None:\n+    return None\n+  return in_layouts, out_layouts\n+\n+\n @partial(_add_layout_inference_rule, scf.IfOp)\n def _infer_if_op_layout(op: scf.IfOp) -> OptionalLayouts:\n   if layouts := _infer_from_yield_ops(op):\ndiff --git a/tests/mosaic/gpu_layout_inference_test.py b/tests/mosaic/gpu_layout_inference_test.py\nindex 315ae2659ab6..038766542f3b 100644\n--- a/tests/mosaic/gpu_layout_inference_test.py\n+++ b/tests/mosaic/gpu_layout_inference_test.py\n@@ -429,6 +429,59 @@ def body(lower_bound, upper_bound, step, a, b, c):\n     self.assertSequenceEqual(for_op.attributes[\"in_layouts\"], [wgmma_layout])\n     self.assertSequenceEqual(for_op.attributes[\"out_layouts\"], [wgmma_layout])\n \n+  @parameterized.parameters(\n+      ((), None, (), None),\n+      ((64, 32), mgpu.WGMMA_LAYOUT, (), None),\n+      ((), None, (64, 32), mgpu.WGMMA_LAYOUT),\n+      ((64,), mgpu.WGMMA_ROW_LAYOUT, (64, 32), mgpu.WGMMA_LAYOUT),\n+  )\n+  def test_infer_while_op_layouts(\n+      self, init_shape, init_layout, result_shape, result_layout\n+  ):\n+    if init_shape:\n+      in_type = ir.VectorType.get(init_shape, ir.F32Type.get())\n+    else:\n+      in_type = ir.F32Type.get()\n+\n+    if result_shape:\n+      out_type = ir.VectorType.get(result_shape, ir.F32Type.get())\n+    else:\n+      out_type = ir.F32Type.get()\n+\n+    while_op = condition_op = yield_op = None\n+\n+    def body(condition, init, result):\n+      nonlocal while_op, condition_op, yield_op\n+      while_op = scf.WhileOp([out_type], [init])\n+      before_block = while_op.before.blocks.append(init.type)\n+      with ir.InsertionPoint(before_block):\n+        condition_op = scf.ConditionOp(condition, [result])\n+\n+      after_block = while_op.after.blocks.append(out_type)\n+      with ir.InsertionPoint(after_block):\n+        yield_op = scf.YieldOp([init])\n+\n+    with ir.InsertionPoint(self.module.body):\n+      i1 = ir.IntegerType.get_signless(1)\n+      func.FuncOp.from_py_func(i1, in_type, out_type)(body)\n+\n+    [f] = self.module.body.operations\n+    f_layouts = []\n+    if init_layout:\n+      f_layouts.append(layouts.to_layout_attr(init_layout))\n+    if result_layout:\n+      f_layouts.append(layouts.to_layout_attr(result_layout))\n+    if f_layouts:\n+      f.attributes[\"in_layouts\"] = ir.ArrayAttr.get(f_layouts)\n+\n+    mgpu.infer_layout(self.module)\n+\n+    if init_layout or result_layout:\n+      init_layouts = [layouts.to_layout_attr(init_layout)] if init_layout else []\n+      result_layouts = [layouts.to_layout_attr(result_layout)] if result_layout else []\n+      self.assertSequenceEqual(while_op.attributes[\"in_layouts\"], init_layouts)\n+      self.assertSequenceEqual(while_op.attributes[\"out_layouts\"], result_layouts)\n+\n   def test_infer_layout_has_no_layout_for_non_vector_types(self):\n     shape = (32, 4)\n     elt_ty = ir.BF16Type.get()\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 873854266782..9d259097f90a 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1218,8 +1218,6 @@ def body(idx, _):\n     np.testing.assert_array_equal(kernel(x, y), x + y)\n \n   def test_while_loop(self):\n-    self.skip_if_wg_semantics()\n-\n     @functools.partial(\n         self.pallas_call, out_shape=jax.ShapeDtypeStruct([128], jnp.int32)\n     )\n@@ -1242,8 +1240,6 @@ def body(acc):\n     )\n \n   def test_while_loop_layout_mismatch(self):\n-    self.skip_if_wg_semantics()  # while and conditional are not yet supported.\n-\n     @functools.partial(\n         self.pallas_call, out_shape=jax.ShapeDtypeStruct([128], jnp.int32)\n     )\n@@ -1261,8 +1257,17 @@ def body(acc):\n \n       _ = jax.lax.while_loop(cond, body, o_ref[...])\n \n-    with self.assertRaisesRegex(ValueError, \"has layout .*, when it should be\"):\n-      kernel()\n+    if self.LOWERING_SEMANTICS == plgpu.LoweringSemantics.Warpgroup:\n+      with self.assertRaisesRegex(\n+          NotImplementedError,\n+          \"Cannot convert from WGStridedFragLayout.* to TiledLayout\",\n+      ):\n+        kernel()\n+    else:\n+      with self.assertRaisesRegex(\n+          ValueError, \"has layout .*, when it should be\"\n+      ):\n+        kernel()\n \n   def test_cond(self):\n     @functools.partial(\n@@ -1722,10 +1727,6 @@ class PallasCallSm90ATest(PallasSm90ATest):\n \n   @parameterized.parameters(False, True)\n   def test_fori_loop_accumulator(self, force_while):\n-    if force_while:\n-      # Layout inference and lowering for 'while' are not yet implemented for\n-      # warpgroup semantics.\n-      self.skip_if_wg_semantics()\n     if self.LOWERING_SEMANTICS == plgpu.LoweringSemantics.Lane:\n       transforms = (plgpu.TilingTransform((8, 64)), plgpu.SwizzleTransform(128))\n     else:\n\n```"
        ],
        "from_id": [
            "dimitar-asenov",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Nuked a debug test added by accident\n\nPiperOrigin-RevId: 758144215",
        "output": [
            "```diff\nCommit: 0d6ad8aee0fe786109e2e37b2b0e92eed712c950\nDate: 2025-05-13T10:39:06Z\nURL: https://github.com/jax-ml/jax/commit/0d6ad8aee0fe786109e2e37b2b0e92eed712c950\nFiles changed: 1\nAdditions: +0, Deletions: -14\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex a5719b01f4ad..873854266782 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -3321,20 +3321,6 @@ def do_wgmma(acc_ref):\n \n     np.testing.assert_allclose(kernel(x, x), x @ x)\n \n-  def test_debug_bug(self):\n-    dtype = jnp.float16\n-    @functools.partial(\n-        self.pallas_call,\n-        out_shape=jax.ShapeDtypeStruct([256], dtype),\n-    )\n-    def kernel(o_ref):\n-      kv_step = jnp.asarray(0)\n-      @pl.when(kv_step < -2)\n-      def dp():\n-        pl.debug_print(\"foo\")\n-      o_ref[...] = jnp.zeros_like(o_ref)\n-    kernel()\n-\n   # TODO(apaszke): Clusters and multicast\n \n \n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Update tridiagonal solve kernels on GPU to properly use the FFI.\n\nThis fixes https://github.com/jax-ml/jax/issues/28544 by using the batched algorithms directly when possible. It also adds complex dtype and batch partitioning support to tridiagonal solves on GPU.\n\nPiperOrigin-RevId: 758129745",
        "output": [
            "```diff\nCommit: 91de2e39c161e3f26e8d58ab7071a189903563f8\nDate: 2025-05-13T09:46:51Z\nURL: https://github.com/jax-ml/jax/commit/91de2e39c161e3f26e8d58ab7071a189903563f8\nFiles changed: 11\nAdditions: +270, Deletions: -37\ndiff --git a/jax/_src/lax/linalg.py b/jax/_src/lax/linalg.py\nindex dd86c22432d8..857b115b06d8 100644\n--- a/jax/_src/lax/linalg.py\n+++ b/jax/_src/lax/linalg.py\n@@ -48,7 +48,7 @@\n from jax._src.lib import gpu_solver\n from jax._src.lib import gpu_sparse\n from jax._src.lib import lapack\n-from jax._src.lib import version as jaxlib_version\n+from jax._src.lib import version as jaxlib_version, jaxlib_extension_version\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import chlo\n from jax._src.lib.mlir.dialects import hlo\n@@ -2530,29 +2530,33 @@ def _tridiagonal_solve_shape_rule(dl_shape, d_shape, du_shape, b_shape, **_):\n   return b_shape\n \n def _tridiagonal_solve_gpu_lowering(ctx, dl, d, du, b, *, target_name_prefix):\n-  _, _, _, b_aval = ctx.avals_in\n-  *batch_dims, m, n = b_aval.shape\n-  batch_size = math.prod(batch_dims)\n-\n-  mod = gpu_sparse._cusparse if target_name_prefix == \"cu\" else gpu_sparse._hipsparse\n-  assert mod is not None\n-  opaque = mod.build_gtsv2_descriptor(batch_size, m, n, m)\n-  if b_aval.dtype == np.float32:\n-    buffer_size = mod.gtsv2_f32_buffer_size(m, n, m)\n-    target_name = \"sparse_gtsv2_f32_ffi\"\n-  elif b_aval.dtype == np.float64:\n-    buffer_size = mod.gtsv2_f64_buffer_size(m, n, m)\n-    target_name = \"sparse_gtsv2_f64_ffi\"\n-  else:\n-    raise NotImplementedError(\n-        \"tridiagonal_solve is only implemented for float32 and float64 on GPU.\")\n-\n-  buffer_aval = core.ShapedArray(shape=(buffer_size,), dtype=np.int8)\n-  sub_ctx = ctx.replace(avals_out=[*ctx.avals_out, buffer_aval])\n-  rule = _linalg_ffi_lowering(\n-      f\"{target_name_prefix}{target_name}\", operand_output_aliases={3: 0},\n-      batch_partitionable=False)\n-  return rule(sub_ctx, dl, d, du, b, opaque=opaque)[:1]\n+  if jaxlib_extension_version < 340:\n+    _, _, _, b_aval = ctx.avals_in\n+    *batch_dims, m, n = b_aval.shape\n+    batch_size = math.prod(batch_dims)\n+    mod = gpu_sparse._cusparse if target_name_prefix == \"cu\" else gpu_sparse._hipsparse\n+    assert mod is not None\n+    opaque = mod.build_gtsv2_descriptor(batch_size, m, n, m)\n+    if b_aval.dtype == np.float32:\n+      buffer_size = mod.gtsv2_f32_buffer_size(m, n, m)\n+      target_name = \"sparse_gtsv2_f32_ffi\"\n+    elif b_aval.dtype == np.float64:\n+      buffer_size = mod.gtsv2_f64_buffer_size(m, n, m)\n+      target_name = \"sparse_gtsv2_f64_ffi\"\n+    else:\n+      raise NotImplementedError(\n+          \"tridiagonal_solve is only implemented for float32 and float64 on GPU.\")\n+\n+    buffer_aval = core.ShapedArray(shape=(buffer_size,), dtype=np.int8)\n+    sub_ctx = ctx.replace(avals_out=[*ctx.avals_out, buffer_aval])\n+    rule = _linalg_ffi_lowering(\n+        f\"{target_name_prefix}{target_name}\", operand_output_aliases={3: 0},\n+        batch_partitionable=False)\n+    return rule(sub_ctx, dl, d, du, b, opaque=opaque)[:1]\n+\n+  target_name = f\"{target_name_prefix}sparse_gtsv2_ffi\"\n+  rule = _linalg_ffi_lowering(target_name, operand_output_aliases={3: 0})\n+  return rule(ctx, dl, d, du, b)\n \n def _tridiagonal_solve_cpu_lowering(ctx, dl, d, du, b, **kwargs):\n   del kwargs  # unused\ndiff --git a/jaxlib/cuda/BUILD b/jaxlib/cuda/BUILD\nindex 2cc1476b637e..eabb3157ecca 100644\n--- a/jaxlib/cuda/BUILD\n+++ b/jaxlib/cuda/BUILD\n@@ -259,11 +259,13 @@ cc_library(\n         \":cuda_gpu_kernel_helpers\",\n         \":cuda_vendor\",\n         \":ffi_wrapper\",\n+        \"//jaxlib:ffi_helpers\",\n         \"//jaxlib:kernel_helpers\",\n         \"//jaxlib/gpu:handle_pool\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@local_config_cuda//cuda:cuda_headers\",\n         \"@xla//xla/ffi/api:ffi\",\ndiff --git a/jaxlib/gpu/sparse.cc b/jaxlib/gpu/sparse.cc\nindex 592c0f454a55..0190ba776de5 100644\n--- a/jaxlib/gpu/sparse.cc\n+++ b/jaxlib/gpu/sparse.cc\n@@ -614,6 +614,8 @@ nb::dict Registrations() {\n       EncapsulateFfiHandler(gtsv2_f32_ffi);\n   dict[JAX_GPU_PREFIX \"sparse_gtsv2_f64_ffi\"] =\n       EncapsulateFfiHandler(gtsv2_f64_ffi);\n+  dict[JAX_GPU_PREFIX \"sparse_gtsv2_ffi\"] = EncapsulateFfiHandler(kGtsv2);\n+\n   // TODO(tomhennigan): Add support for gtsv2 complex 32/64.\n   return dict;\n }\ndiff --git a/jaxlib/gpu/sparse_kernels.cc b/jaxlib/gpu/sparse_kernels.cc\nindex a9c08317e066..363321e3ca8b 100644\n--- a/jaxlib/gpu/sparse_kernels.cc\n+++ b/jaxlib/gpu/sparse_kernels.cc\n@@ -16,20 +16,29 @@ limitations under the License.\n #include \"jaxlib/gpu/sparse_kernels.h\"\n \n #include <cstddef>\n+#include <cstdint>\n #include <cstring>\n #include <string>\n \n #include \"absl/status/status.h\"\n #include \"absl/status/statusor.h\"\n #include \"absl/strings/str_cat.h\"\n+#include \"absl/strings/str_format.h\"\n #include \"absl/synchronization/mutex.h\"\n+#include \"jaxlib/ffi_helpers.h\"\n #include \"jaxlib/gpu/ffi_wrapper.h\"\n #include \"jaxlib/gpu/gpu_kernel_helpers.h\"\n #include \"jaxlib/gpu/handle_pool.h\"\n #include \"jaxlib/gpu/vendor.h\"\n #include \"jaxlib/kernel_helpers.h\"\n+#include \"xla/ffi/api/ffi.h\"\n #include \"xla/service/custom_call_status.h\"\n \n+#define JAX_FFI_RETURN_IF_GPU_ERROR(...) \\\n+  FFI_RETURN_IF_ERROR_STATUS(JAX_AS_STATUS(__VA_ARGS__))\n+\n+namespace ffi = ::xla::ffi;\n+\n namespace jax {\n \n template <>\n@@ -641,5 +650,162 @@ void gtsv2_f64(gpuStream_t stream, void** buffers, const char* opaque,\n   }\n }\n \n+template <typename T, typename BufferSizeF, typename KernelF>\n+ffi::Error Gtsv2Impl(BufferSizeF getBufferSize, KernelF kernel, int64_t batch,\n+                     int64_t rows, int64_t cols, gpuStream_t stream,\n+                     ffi::ScratchAllocator& scratch, ffi::AnyBuffer dl,\n+                     ffi::AnyBuffer d, ffi::AnyBuffer du, ffi::AnyBuffer b,\n+                     ffi::Result<ffi::AnyBuffer> out) {\n+  FFI_ASSIGN_OR_RETURN(auto m, MaybeCastNoOverflow<int>(rows));\n+  FFI_ASSIGN_OR_RETURN(auto n, MaybeCastNoOverflow<int>(cols));\n+\n+  FFI_ASSIGN_OR_RETURN(auto handle, SparseHandlePool::Borrow(stream));\n+  size_t buffer_size_in_bytes;\n+  JAX_FFI_RETURN_IF_GPU_ERROR(getBufferSize(handle.get(), m, n, nullptr,\n+                                            nullptr, nullptr, nullptr, m,\n+                                            &buffer_size_in_bytes));\n+  auto maybe_workspace = scratch.Allocate(buffer_size_in_bytes);\n+  if (!maybe_workspace.has_value()) {\n+    return ffi::Error::Internal(\"Unable to allocate workspace for gtsv2\");\n+  }\n+  void* workspace = maybe_workspace.value();\n+\n+  auto dl_data = static_cast<T*>(dl.untyped_data());\n+  auto d_data = static_cast<T*>(d.untyped_data());\n+  auto du_data = static_cast<T*>(du.untyped_data());\n+  auto b_data = static_cast<T*>(b.untyped_data());\n+  auto out_data = static_cast<T*>(out->untyped_data());\n+  if (b_data != out_data) {\n+    JAX_FFI_RETURN_IF_GPU_ERROR(gpuMemcpyAsync(\n+        out_data, b_data, b.size_bytes(), gpuMemcpyDeviceToDevice, stream));\n+  }\n+\n+  for (int64_t i = 0; i < batch; ++i) {\n+    JAX_FFI_RETURN_IF_GPU_ERROR(kernel(handle.get(), m, n, dl_data, d_data,\n+                                       du_data, out_data, m, workspace));\n+    dl_data += m;\n+    d_data += m;\n+    du_data += m;\n+    out_data += m * n;\n+  }\n+  return ffi::Error::Success();\n+}\n+\n+template <typename T, typename BufferSizeF, typename KernelF>\n+ffi::Error Gtsv2BatchedImpl(BufferSizeF getBufferSize, KernelF kernel,\n+                            int64_t batch, int64_t rows, gpuStream_t stream,\n+                            ffi::ScratchAllocator& scratch, ffi::AnyBuffer dl,\n+                            ffi::AnyBuffer d, ffi::AnyBuffer du,\n+                            ffi::AnyBuffer b, ffi::Result<ffi::AnyBuffer> out) {\n+  FFI_ASSIGN_OR_RETURN(auto batch_count, MaybeCastNoOverflow<int>(batch));\n+  FFI_ASSIGN_OR_RETURN(auto m, MaybeCastNoOverflow<int>(rows));\n+\n+  FFI_ASSIGN_OR_RETURN(auto handle, SparseHandlePool::Borrow(stream));\n+  size_t buffer_size_in_bytes;\n+  JAX_FFI_RETURN_IF_GPU_ERROR(getBufferSize(handle.get(), m, nullptr, nullptr,\n+                                            nullptr, nullptr, batch_count, m,\n+                                            &buffer_size_in_bytes));\n+  auto maybe_workspace = scratch.Allocate(buffer_size_in_bytes);\n+  if (!maybe_workspace.has_value()) {\n+    return ffi::Error::Internal(\"Unable to allocate workspace for gtsv2\");\n+  }\n+  void* workspace = maybe_workspace.value();\n+\n+  auto dl_data = static_cast<T*>(dl.untyped_data());\n+  auto d_data = static_cast<T*>(d.untyped_data());\n+  auto du_data = static_cast<T*>(du.untyped_data());\n+  auto b_data = static_cast<T*>(b.untyped_data());\n+  auto out_data = static_cast<T*>(out->untyped_data());\n+  if (b_data != out_data) {\n+    JAX_FFI_RETURN_IF_GPU_ERROR(gpuMemcpyAsync(\n+        out_data, b_data, b.size_bytes(), gpuMemcpyDeviceToDevice, stream));\n+  }\n+\n+  JAX_FFI_RETURN_IF_GPU_ERROR(kernel(handle.get(), m, dl_data, d_data, du_data,\n+                                     out_data, batch_count, m, workspace));\n+  return ffi::Error::Success();\n+}\n+\n+ffi::Error Gtsv2(gpuStream_t stream, ffi::ScratchAllocator scratch,\n+                 ffi::AnyBuffer dl, ffi::AnyBuffer d, ffi::AnyBuffer du,\n+                 ffi::AnyBuffer b, ffi::Result<ffi::AnyBuffer> out) {\n+  auto dataType = dl.element_type();\n+  if (dataType != d.element_type() || dataType != du.element_type() ||\n+      dataType != b.element_type() || dataType != out->element_type()) {\n+    return ffi::Error::InvalidArgument(\n+        \"The inputs and outputs to gtsv2 must have the same element type\");\n+  }\n+  FFI_ASSIGN_OR_RETURN((auto [batch, rows, cols]),\n+                       SplitBatch2D(b.dimensions()));\n+  FFI_RETURN_IF_ERROR(\n+      CheckShape(out->dimensions(), {batch, rows, cols}, \"out\", \"gtsv2\"));\n+  FFI_RETURN_IF_ERROR(\n+      CheckShape(dl.dimensions(), {batch, rows}, \"dl\", \"gtsv2\"));\n+  FFI_RETURN_IF_ERROR(CheckShape(d.dimensions(), {batch, rows}, \"d\", \"gtsv2\"));\n+  FFI_RETURN_IF_ERROR(\n+      CheckShape(du.dimensions(), {batch, rows}, \"du\", \"gtsv2\"));\n+  if (batch > 1 && cols == 1) {\n+    switch (dataType) {\n+      case ffi::F32:\n+        return Gtsv2BatchedImpl<float>(\n+            gpusparseSgtsv2StridedBatch_bufferSizeExt,\n+            gpusparseSgtsv2StridedBatch, batch, rows, stream, scratch, dl, d,\n+            du, b, out);\n+      case ffi::F64:\n+        return Gtsv2BatchedImpl<double>(\n+            gpusparseDgtsv2StridedBatch_bufferSizeExt,\n+            gpusparseDgtsv2StridedBatch, batch, rows, stream, scratch, dl, d,\n+            du, b, out);\n+      case ffi::C64:\n+        return Gtsv2BatchedImpl<gpuComplex>(\n+            gpusparseCgtsv2StridedBatch_bufferSizeExt,\n+            gpusparseCgtsv2StridedBatch, batch, rows, stream, scratch, dl, d,\n+            du, b, out);\n+      case ffi::C128:\n+        return Gtsv2BatchedImpl<gpuDoubleComplex>(\n+            gpusparseZgtsv2StridedBatch_bufferSizeExt,\n+            gpusparseZgtsv2StridedBatch, batch, rows, stream, scratch, dl, d,\n+            du, b, out);\n+      default:\n+        break;\n+    }\n+\n+  } else {\n+    switch (dataType) {\n+      case ffi::F32:\n+        return Gtsv2Impl<float>(gpusparseSgtsv2_bufferSizeExt, gpusparseSgtsv2,\n+                                batch, rows, cols, stream, scratch, dl, d, du,\n+                                b, out);\n+      case ffi::F64:\n+        return Gtsv2Impl<double>(gpusparseDgtsv2_bufferSizeExt, gpusparseDgtsv2,\n+                                 batch, rows, cols, stream, scratch, dl, d, du,\n+                                 b, out);\n+      case ffi::C64:\n+        return Gtsv2Impl<gpuComplex>(gpusparseCgtsv2_bufferSizeExt,\n+                                     gpusparseCgtsv2, batch, rows, cols, stream,\n+                                     scratch, dl, d, du, b, out);\n+      case ffi::C128:\n+        return Gtsv2Impl<gpuDoubleComplex>(gpusparseZgtsv2_bufferSizeExt,\n+                                           gpusparseZgtsv2, batch, rows, cols,\n+                                           stream, scratch, dl, d, du, b, out);\n+      default:\n+        break;\n+    }\n+  }\n+  return ffi::Error::InvalidArgument(absl::StrFormat(\n+      \"Unsupported dtype %s in gtsv2\", absl::FormatStreamed(dataType)));\n+}\n+\n+XLA_FFI_DEFINE_HANDLER_SYMBOL(kGtsv2, Gtsv2,\n+                              ffi::Ffi::Bind()\n+                                  .Ctx<ffi::PlatformStream<gpuStream_t>>()\n+                                  .Ctx<ffi::ScratchAllocator>()\n+                                  .Arg<ffi::AnyBuffer>()  // dl\n+                                  .Arg<ffi::AnyBuffer>()  // d\n+                                  .Arg<ffi::AnyBuffer>()  // du\n+                                  .Arg<ffi::AnyBuffer>()  // b\n+                                  .Ret<ffi::AnyBuffer>()  // out\n+);\n+\n }  // namespace JAX_GPU_NAMESPACE\n }  // namespace jax\ndiff --git a/jaxlib/gpu/sparse_kernels.h b/jaxlib/gpu/sparse_kernels.h\nindex d735c320307c..3b365872f591 100644\n--- a/jaxlib/gpu/sparse_kernels.h\n+++ b/jaxlib/gpu/sparse_kernels.h\n@@ -157,6 +157,7 @@ XLA_FFI_DECLARE_HANDLER_SYMBOL(CooMatvecFfi);\n XLA_FFI_DECLARE_HANDLER_SYMBOL(CooMatmatFfi);\n XLA_FFI_DECLARE_HANDLER_SYMBOL(gtsv2_f32_ffi);\n XLA_FFI_DECLARE_HANDLER_SYMBOL(gtsv2_f64_ffi);\n+XLA_FFI_DECLARE_HANDLER_SYMBOL(kGtsv2);\n \n }  // namespace JAX_GPU_NAMESPACE\n }  // namespace jax\ndiff --git a/jaxlib/gpu/vendor.h b/jaxlib/gpu/vendor.h\nindex 5deb8d4c650a..b96552f81bd1 100644\n--- a/jaxlib/gpu/vendor.h\n+++ b/jaxlib/gpu/vendor.h\n@@ -152,7 +152,8 @@ typedef cusparseDnVecDescr_t gpusparseDnVecDescr_t;\n #define GPUDNN_STATUS_SUCCESS CUDNN_STATUS_SUCCESS\n #define GPUDNN_WGRAD_MODE_ADD CUDNN_WGRAD_MODE_ADD\n #define GPUDNN_RNN_ALGO_STANDARD CUDNN_RNN_ALGO_STANDARD\n-#define GPUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED\n+#define GPUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED \\\n+  CUDNN_RNN_DATA_LAYOUT_BATCH_MAJOR_UNPACKED\n #define GPUDNN_RNN_PADDED_IO_ENABLED CUDNN_RNN_PADDED_IO_ENABLED\n #define GPUDNN_DEFAULT_MATH CUDNN_DEFAULT_MATH\n #define GPUDNN_FMA_MATH CUDNN_FMA_MATH\n@@ -289,10 +290,28 @@ typedef cusparseDnVecDescr_t gpusparseDnVecDescr_t;\n #define gpusparseSpMM_bufferSize cusparseSpMM_bufferSize\n #define gpusparseSpMV cusparseSpMV\n #define gpusparseSpMV_bufferSize cusparseSpMV_bufferSize\n+\n #define gpusparseSgtsv2 cusparseSgtsv2\n #define gpusparseDgtsv2 cusparseDgtsv2\n+#define gpusparseCgtsv2 cusparseCgtsv2\n+#define gpusparseZgtsv2 cusparseZgtsv2\n #define gpusparseSgtsv2_bufferSizeExt cusparseSgtsv2_bufferSizeExt\n #define gpusparseDgtsv2_bufferSizeExt cusparseDgtsv2_bufferSizeExt\n+#define gpusparseCgtsv2_bufferSizeExt cusparseCgtsv2_bufferSizeExt\n+#define gpusparseZgtsv2_bufferSizeExt cusparseZgtsv2_bufferSizeExt\n+\n+#define gpusparseSgtsv2StridedBatch_bufferSizeExt \\\n+  cusparseSgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseDgtsv2StridedBatch_bufferSizeExt \\\n+  cusparseDgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseCgtsv2StridedBatch_bufferSizeExt \\\n+  cusparseCgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseZgtsv2StridedBatch_bufferSizeExt \\\n+  cusparseZgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseSgtsv2StridedBatch cusparseSgtsv2StridedBatch\n+#define gpusparseDgtsv2StridedBatch cusparseDgtsv2StridedBatch\n+#define gpusparseCgtsv2StridedBatch cusparseCgtsv2StridedBatch\n+#define gpusparseZgtsv2StridedBatch cusparseZgtsv2StridedBatch\n \n #define GPUSPARSE_INDEX_16U CUSPARSE_INDEX_16U\n #define GPUSPARSE_INDEX_32I CUSPARSE_INDEX_32I\n@@ -636,10 +655,28 @@ typedef hipsparseDnVecDescr_t gpusparseDnVecDescr_t;\n #define gpusparseSpMM_bufferSize hipsparseSpMM_bufferSize\n #define gpusparseSpMV hipsparseSpMV\n #define gpusparseSpMV_bufferSize hipsparseSpMV_bufferSize\n+\n #define gpusparseSgtsv2 hipsparseSgtsv2\n #define gpusparseDgtsv2 hipsparseDgtsv2\n+#define gpusparseCgtsv2 hipsparseCgtsv2\n+#define gpusparseZgtsv2 hipsparseZgtsv2\n #define gpusparseSgtsv2_bufferSizeExt hipsparseSgtsv2_bufferSizeExt\n #define gpusparseDgtsv2_bufferSizeExt hipsparseDgtsv2_bufferSizeExt\n+#define gpusparseCgtsv2_bufferSizeExt hipsparseCgtsv2_bufferSizeExt\n+#define gpusparseZgtsv2_bufferSizeExt hipsparseZgtsv2_bufferSizeExt\n+\n+#define gpusparseSgtsv2StridedBatch_bufferSizeExt \\\n+  hipsparseSgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseDgtsv2StridedBatch_bufferSizeExt \\\n+  hipsparseDgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseCgtsv2StridedBatch_bufferSizeExt \\\n+  hipsparseCgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseZgtsv2StridedBatch_bufferSizeExt \\\n+  hipsparseZgtsv2StridedBatch_bufferSizeExt\n+#define gpusparseSgtsv2StridedBatch hipsparseSgtsv2StridedBatch\n+#define gpusparseDgtsv2StridedBatch hipsparseDgtsv2StridedBatch\n+#define gpusparseCgtsv2StridedBatch hipsparseCgtsv2StridedBatch\n+#define gpusparseZgtsv2StridedBatch hipsparseZgtsv2StridedBatch\n \n #define GPUSPARSE_INDEX_16U HIPSPARSE_INDEX_16U\n #define GPUSPARSE_INDEX_32I HIPSPARSE_INDEX_32I\ndiff --git a/jaxlib/gpu_sparse.py b/jaxlib/gpu_sparse.py\nindex af03eb6e6a8a..bf1dc6f64ec1 100644\n--- a/jaxlib/gpu_sparse.py\n+++ b/jaxlib/gpu_sparse.py\n@@ -35,3 +35,12 @@ def registrations() -> dict[str, list[tuple[str, Any, int]]]:\n           for name, value in module.registrations().items()\n       )\n   return registrations  # pytype: disable=bad-return-type\n+\n+def batch_partitionable_targets() -> list[str]:\n+  targets: list[str] = []\n+  for module in [_cusparse, _hipsparse]:\n+    if module:\n+      targets.extend(\n+          name for name in module.registrations() if name.endswith(\"gtsv2_ffi\")\n+      )\n+  return targets\ndiff --git a/jaxlib/rocm/BUILD b/jaxlib/rocm/BUILD\nindex 75406174dd93..d0468d72d1b3 100644\n--- a/jaxlib/rocm/BUILD\n+++ b/jaxlib/rocm/BUILD\n@@ -244,11 +244,13 @@ cc_library(\n         \":ffi_wrapper\",\n         \":hip_gpu_kernel_helpers\",\n         \":hip_vendor\",\n+        \"//jaxlib:ffi_helpers\",\n         \"//jaxlib:kernel_helpers\",\n         \"//jaxlib/gpu:handle_pool\",\n         \"@com_google_absl//absl/status\",\n         \"@com_google_absl//absl/status:statusor\",\n         \"@com_google_absl//absl/strings\",\n+        \"@com_google_absl//absl/strings:str_format\",\n         \"@com_google_absl//absl/synchronization\",\n         \"@local_config_rocm//rocm:hipsparse\",\n         \"@local_config_rocm//rocm:rocm_headers\",\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 449dfa653286..6aaae11c139d 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 339\n+_version = 340\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\ndiff --git a/tests/linalg_sharding_test.py b/tests/linalg_sharding_test.py\nindex 2f190cdc5ad6..e68e94e16494 100644\n--- a/tests/linalg_sharding_test.py\n+++ b/tests/linalg_sharding_test.py\n@@ -22,6 +22,7 @@\n from jax import lax\n from jax._src import config\n from jax._src import test_util as jtu\n+from jax._src.lib import jaxlib_extension_version\n from jax.sharding import PartitionSpec as P\n \n config.parse_flags_with_absl()\n@@ -31,13 +32,8 @@\n complex_types = jtu.dtypes.complex\n \n \n+# These functions are only supported on CPU.\n CPU_ONLY_FUN_AND_SHAPES = [\n-    # The GPU kernel for this function still uses an opaque descriptor to\n-    # encode the input shapes so it is not partitionable.\n-    # TODO(danfm): Update the kernel and enable this test on GPU.\n-    (lax.linalg.tridiagonal_solve, ((6,), (6,), (6,), (6, 4))),\n-\n-    # These functions are only supported on CPU.\n     (lax.linalg.hessenberg, ((6, 6),)),\n     (lax.linalg.schur, ((6, 6),)),\n ]\n@@ -51,6 +47,7 @@\n     (lax.linalg.svd, ((10, 6),)),\n     (lax.linalg.triangular_solve, ((6, 6), (4, 6))),\n     (lax.linalg.tridiagonal, ((6, 6),)),\n+    (lax.linalg.tridiagonal_solve, ((6,), (6,), (6,), (6, 4))),\n ]\n \n ALL_FUN_AND_SHAPES = CPU_ONLY_FUN_AND_SHAPES + CPU_AND_GPU_FUN_AND_SHAPES\n@@ -73,6 +70,11 @@ def get_fun_and_shapes(self, fun_and_shapes, grad=False):\n         self.skipTest(\n             f\"Partitioning {fun_and_shapes[0].__name__} only supported on GPU \"\n             \"when shardy is enabled.\")\n+      if (fun_and_shapes[0] == lax.linalg.tridiagonal_solve and\n+          jaxlib_extension_version < 340):\n+        self.skipTest(\n+            f\"Partitioning {fun_and_shapes[0].__name__} on GPU, requires a \"\n+            \"more recent jaxlib version.\")\n     if not grad:\n       return fun_and_shapes\n \n@@ -178,7 +180,9 @@ def jvp_fun(primals, tangents):\n         (primals_sharded, tangents),\n     ]:\n       _, actual = jvp_fun_jit(*args)\n-      self.assertAllClose(actual, expected, atol={np.float64: 1e-12})\n+      self.assertAllClose(actual, expected, rtol={\n+          np.float32: 1e-4, np.float64: 1e-11, np.complex64: 1e-4,\n+          np.complex128: 1e-11})\n       hlo = jvp_fun_jit.lower(primals_sharded, tangents_sharded).compile()\n       self.assertNotIn(\"all-\", hlo.as_text())\n \n@@ -199,7 +203,9 @@ def test_batch_axis_sharding_vjp(self, fun_and_shapes, dtype):\n     vjp_fun_jit = jax.jit(vjp_fun)\n     expected = vjp_fun(tangents)\n     actual = vjp_fun_jit(tangents_sharded)\n-    self.assertAllClose(actual, expected)\n+    self.assertAllClose(actual, expected, rtol={\n+          np.float32: 1e-4, np.float64: 1e-11, np.complex64: 1e-4,\n+          np.complex128: 1e-11})\n     hlo = vjp_fun_jit.lower(tangents_sharded).compile()\n     self.assertNotIn(\"all-\", hlo.as_text())\n \ndiff --git a/tests/linalg_test.py b/tests/linalg_test.py\nindex 033ca989c8e7..a9f81ec04560 100644\n--- a/tests/linalg_test.py\n+++ b/tests/linalg_test.py\n@@ -33,6 +33,7 @@\n from jax._src.lax import linalg as lax_linalg\n from jax._src import test_util as jtu\n from jax._src import xla_bridge\n+from jax._src.lib import jaxlib_extension_version\n from jax._src.numpy.util import promote_dtypes_inexact\n \n config.parse_flags_with_absl()\n@@ -2202,7 +2203,7 @@ def testSelect(self, dtype):\n   @jtu.sample_product(shape=[(3,), (3, 4), (3, 4, 5)],\n                       dtype=float_types + complex_types)\n   def test_tridiagonal_solve(self, shape, dtype):\n-    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]):\n+    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]) and jaxlib_extension_version < 340:\n       self.skipTest(\"Data type not supported on GPU\")\n     rng = self.rng()\n     d = 1.0 + jtu.rand_positive(rng)(shape, dtype)\n@@ -2217,7 +2218,10 @@ def build_tri(dl, d, du):\n       build_tri = jax.vmap(build_tri)\n \n     a = build_tri(dl, d, du)\n-    self.assertAllClose(a @ x, b, atol=5e-5, rtol=1e-4)\n+    with jax.default_matmul_precision(\"float32\"):\n+      self.assertAllClose(a @ x, b, atol={\n+          np.float32: 1e-3, np.float64: 1e-10, np.complex64: 1e-3,\n+          np.complex128: 1e-10})\n \n   def test_tridiagonal_solve_endpoints(self):\n     # tridagonal_solve shouldn't depend on the endpoints being explicitly zero.\n@@ -2238,7 +2242,7 @@ def test_tridiagonal_solve_endpoints(self):\n \n   @jtu.sample_product(shape=[(3,), (3, 4)], dtype=float_types + complex_types)\n   def test_tridiagonal_solve_grad(self, shape, dtype):\n-    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]):\n+    if dtype not in float_types and jtu.test_device_matches([\"gpu\"]) and jaxlib_extension_version < 340:\n       self.skipTest(\"Data type not supported on GPU\")\n     rng = self.rng()\n     d = 1.0 + jtu.rand_positive(rng)(shape, dtype)\n\n```"
        ],
        "from_id": [
            "dfm",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Consolidate initial/final style custom_vjp primitives into one.",
        "output": [
            "```diff\nCommit: 74938be8456050fec5032682f1463ea36983e1de\nDate: 2025-05-13T09:39:08Z\nURL: https://github.com/jax-ml/jax/commit/74938be8456050fec5032682f1463ea36983e1de\nFiles changed: 8\nAdditions: +111, Deletions: -159\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex 144cbaf5cd21..aa9bfe9529ce 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -1079,17 +1079,17 @@ def jvp(*xs):\n     return [*primal_errs, *out_primals, *tangent_errs, *out_tangents]\n   return lu.wrap_init(jvp, debug_info=jvp_jaxpr_fun.debug_info)\n \n-def custom_vjp_call_jaxpr_rule(in_err, enabled_errors, *in_vals,\n-                               fun_jaxpr: core.ClosedJaxpr,\n-                               fwd_jaxpr_thunk, num_consts,\n-                               bwd: lu.WrappedFun, out_trees,\n-                               symbolic_zeros: bool):\n+def custom_vjp_call_rule(in_err, enabled_errors, *in_vals,\n+                         call_jaxpr: core.ClosedJaxpr,\n+                         fwd_jaxpr_thunk, num_consts,\n+                         bwd: lu.WrappedFun, out_trees,\n+                         symbolic_zeros: bool):\n   err_vals, err_tree = jtu.tree_flatten(in_err)\n   num_errs = err_tree.num_leaves\n   checkified_fun = lu.wrap_init(\n-      functools.partial(checkify_jaxpr_flat, fun_jaxpr.jaxpr,\n-                        fun_jaxpr.consts, enabled_errors, err_tree),\n-      debug_info=fun_jaxpr.jaxpr.debug_info)\n+      functools.partial(checkify_jaxpr_flat, call_jaxpr.jaxpr,\n+                        call_jaxpr.consts, enabled_errors, err_tree),\n+      debug_info=call_jaxpr.jaxpr.debug_info)\n   checkified_fun, fun_metadata = _flatten_and_get_error_metadata_thunk(\n       checkified_fun)\n \n@@ -1097,13 +1097,13 @@ def checkified_fwd(*args):\n     # TODO(lenamartens, sharadmv): why not checkify here?\n     xs, zeros = args[::2], args[1::2]\n     xs, zeros = xs[num_errs:], zeros[num_errs:]\n-    fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk(*zeros)\n+    fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk.call_wrapped(*zeros)\n     xs_without_consts = xs[num_consts:]\n     return core.eval_jaxpr(fwd_jaxpr, fwd_consts, *xs_without_consts)\n \n   # TODO(necula): the fwd result_paths are not quite the same as fun_jaxpr\n   checkified_fwd_wrapped = lu.wrap_init(checkified_fwd,\n-                                        debug_info=fun_jaxpr.jaxpr.debug_info)\n+                                        debug_info=fwd_jaxpr_thunk.debug_info)\n   bwd_ = lu.wrap_init(lambda *args: (*(None,)*num_errs, *bwd.call_wrapped(*args)),\n                       debug_info=bwd.debug_info)\n   checkified_fwd_wrapped, fwd_out_tree = flatten_fun_output(checkified_fwd_wrapped)\n@@ -1118,7 +1118,7 @@ def checkified_fwd(*args):\n   else:\n     out_err, out_vals = in_err, all_outs\n   return out_err, out_vals\n-error_checks[custom_derivatives.custom_vjp_call_jaxpr_p] = custom_vjp_call_jaxpr_rule\n+error_checks[custom_derivatives.custom_vjp_call_p] = custom_vjp_call_rule\n \n \n def check_discharge_rule(error, enabled_errors, *args, err_tree, debug):\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex dc8fc90e3d1f..dcd893f44123 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -425,16 +425,14 @@ def _custom_jvp_call_typecheck(_, *in_avals, call_jaxpr, jvp_jaxpr_fun,\n   return call_jaxpr.out_avals, call_jaxpr.effects\n core.custom_typechecks[custom_jvp_call_p] = _custom_jvp_call_typecheck\n \n-def _custom_jvp_call_mlir_translation(ctx, *args, call_jaxpr, jvp_jaxpr_fun,\n-                                      num_consts, symbolic_zeros):\n-  del jvp_jaxpr_fun, num_consts, symbolic_zeros\n+def _custom_jvp_vjp_call_lowering(ctx, *args, call_jaxpr, **_):\n   consts = mlir._ir_consts(call_jaxpr.consts)\n   out, tokens = mlir.jaxpr_subcomp(ctx.module_context, call_jaxpr.jaxpr,\n                                    ctx.name_stack, ctx.tokens_in, consts,\n                                    *args, dim_var_values=ctx.dim_var_values)\n   ctx.set_tokens_out(tokens)\n   return out\n-mlir.register_lowering(custom_jvp_call_p, _custom_jvp_call_mlir_translation)\n+mlir.register_lowering(custom_jvp_call_p, _custom_jvp_vjp_call_lowering)\n \n # If a (multi)linear function is defined with a custom jvp, then\n # custom_jvp_call_ can appear in jaxprs to be transposed. Since it's already\n@@ -936,8 +934,8 @@ def _temporary_dtype_exception(a, a_) -> bool:\n def _temporary_shape_exception(a, a_) -> bool:\n   return config.custom_vjp_disable_shape_check.value\n \n-class CustomVJPCallPrimitive(core.CallPrimitive):\n-  initial_style: core.Primitive\n+class CustomVJPCallPrimitive(core.Primitive):\n+  multiple_results = True\n \n   def bind(self, *args, **params):\n     return self._true_bind(*args, **params)\n@@ -946,107 +944,70 @@ def bind_with_trace(self, trace, args, params):\n     fun, fwd, bwd, tracers = args[0], args[1], args[2], args[3:]\n     return trace.process_custom_vjp_call(self, fun, fwd, bwd, tracers, **params)\n \n-custom_vjp_call_p = CustomVJPCallPrimitive('custom_vjp_call')\n+  def impl(self, fun, fwd, bwd, *args):\n+    raise NotImplementedError\n+\n+  def get_bind_params(self, params):\n+    new_params = dict(params)\n+    call_jaxpr: core.ClosedJaxpr = new_params.pop('call_jaxpr')\n+    num_consts: int = new_params.pop('num_consts')\n+    fwd_jaxpr_thunk = new_params.pop('fwd_jaxpr_thunk')\n+    fun = lu.wrap_init(core.jaxpr_as_fun(call_jaxpr),\n+                       debug_info=call_jaxpr.jaxpr.debug_info)\n+    fwd = lift_fwd(num_consts, fwd_jaxpr_thunk)\n+    const_avals, _ = split_list(call_jaxpr.in_avals, [num_consts])\n+    bwd = _handle_consts_in_bwd(new_params.pop('bwd'), const_avals)\n+    return [fun, fwd, bwd], new_params\n+\n+def lift_fwd(num_consts: int, fwd_jaxpr_thunk: lu.WrappedFun) -> lu.WrappedFun:\n+  def fwd(*args):\n+    vals, zeros = args[::2], args[1::2]\n+    assert len(vals) == len(zeros)\n+    _, primals = split_list(vals, [num_consts])\n+    const_zeros, in_zeros = split_list(zeros, [num_consts])\n+    if any(const_zeros):\n+      raise ad.CustomVJPException()\n+    fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk.call_wrapped(*in_zeros)\n+    return core.eval_jaxpr(fwd_jaxpr, fwd_consts, *primals)\n+  return lu.wrap_init(fwd, debug_info=fwd_jaxpr_thunk.debug_info)\n \n-def _custom_vjp_call_jaxpr_impl(*args, fun_jaxpr, **_):\n-  return core.jaxpr_as_fun(fun_jaxpr)(*args)\n+@lu.transformation2\n+def _handle_consts_in_bwd(f, const_avals, *args):\n+  return [Zero(a) for a in const_avals] + list(f(*args))\n \n-def _custom_vjp_call_jaxpr_abstract_eval(*_, fun_jaxpr, **__):\n-  disallowed_effects = effects.custom_derivatives_allowed_effects.filter_not_in(fun_jaxpr.effects)\n+custom_vjp_call_p = CustomVJPCallPrimitive('custom_vjp_call')\n+mlir.register_lowering(custom_vjp_call_p, _custom_jvp_vjp_call_lowering)\n+\n+def _custom_vjp_call_typecheck(_, *in_avals, call_jaxpr, **kwargs):\n+  del in_avals, kwargs\n+  disallowed_effects = effects.custom_derivatives_allowed_effects.filter_not_in(\n+      call_jaxpr.effects)\n   if disallowed_effects:\n     raise NotImplementedError(\n         f'Effects not supported in `custom_vjp`: {disallowed_effects}')\n-  return fun_jaxpr.out_avals, fun_jaxpr.effects\n-\n-custom_vjp_call_jaxpr_p = core.Primitive('custom_vjp_call_jaxpr')\n-custom_vjp_call_jaxpr_p.multiple_results = True\n-custom_vjp_call_jaxpr_p.def_impl(_custom_vjp_call_jaxpr_impl)\n-custom_vjp_call_jaxpr_p.def_effectful_abstract_eval(_custom_vjp_call_jaxpr_abstract_eval)\n-CustomVJPCallPrimitive.initial_style = custom_vjp_call_jaxpr_p\n-\n-mlir.register_lowering(custom_vjp_call_jaxpr_p, mlir.lower_fun(\n-    _custom_vjp_call_jaxpr_impl, multiple_results=True))\n-\n-def _custom_vjp_call_jaxpr_jvp(\n-    primals, tangents, *, fun_jaxpr: core.ClosedJaxpr,\n-    fwd_jaxpr_thunk: Callable[..., tuple[core.Jaxpr, Sequence[Any]]],\n-    num_consts: int, bwd: lu.WrappedFun,\n-    out_trees: Callable[[], Sequence[PyTreeDef]],\n-    symbolic_zeros: bool):\n-  _, args = split_list(primals, [num_consts])\n-  consts_dot, args_dot = split_list(tangents, [num_consts])\n-  if any(type(t) is not Zero for t in consts_dot):\n-    raise ad.CustomVJPException()\n-  zeros = [type(t) is not Zero for t in args_dot]\n-  fwd_jaxpr, fwd_consts = fwd_jaxpr_thunk(*zeros)  # consts can be tracers!\n-  _, res_tree = out_trees()\n-  res_and_primals_out = core.eval_jaxpr(fwd_jaxpr, fwd_consts, *args)\n-  res, primals_out = split_list(res_and_primals_out, [res_tree.num_leaves])\n-  avals_out = [core.get_aval(x).to_tangent_aval() for x in primals_out]\n-  args_dot = map(ad.instantiate_zeros, args_dot)\n-  tangents_out = ad.custom_lin_p.bind(\n-      *res, *args_dot, num_res=res_tree.num_leaves, bwd=bwd,\n-      out_avals=avals_out, symbolic_zeros=symbolic_zeros)\n-  tangents_out = map(lax.tie_p.bind, primals_out, tangents_out)\n-  return primals_out, tangents_out\n-ad.primitive_jvps[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_jvp\n-\n-def _custom_vjp_call_jaxpr_vmap(\n-    axis_data, args, in_dims, *,\n-    fun_jaxpr: core.ClosedJaxpr,\n-    fwd_jaxpr_thunk: Callable[..., tuple[core.Jaxpr, Sequence[Any]]],\n-    num_consts: int, bwd: lu.WrappedFun,\n-    out_trees: Callable, symbolic_zeros: bool):\n-  args = [batching.moveaxis(x, d, 0) if d is not not_mapped and d != 0\n-          else x for x, d in zip(args, in_dims)]\n-  in_batched = [d is not not_mapped for d in in_dims]\n-  _, args_batched = split_list(in_batched, [num_consts])\n-  batched_fun_jaxpr, out_batched = batching.batch_jaxpr(\n-      fun_jaxpr, axis_data, in_batched, False)\n-  out_dims1 = [0 if b else not_mapped for b in out_batched]\n-  out_dims2 = []\n-\n-  @pe._memoize\n-  def batched_fwd_jaxpr_thunk(*zeros):\n-    fwd_jaxpr = core.ClosedJaxpr(*fwd_jaxpr_thunk(*zeros))  # consts can be tracers\n-    batched_fwd_jaxpr, out_batched = batching.batch_jaxpr(\n-        fwd_jaxpr, axis_data, args_batched, False)\n-    out_dims2.append([0 if b else not_mapped for b in out_batched])\n-    return batched_fwd_jaxpr.jaxpr, batched_fwd_jaxpr.consts\n-\n-  fwd_args_batched = [0 if b else not_mapped for b in args_batched]\n-  fwd_out_dims = lambda: out_dims2[0]\n-  tag = core.TraceTag()\n-  batched_bwd = batching.batch_custom_vjp_bwd(\n-    bwd, tag, axis_data, fwd_out_dims, fwd_args_batched)\n-\n-  batched_outs = custom_vjp_call_jaxpr_p.bind(\n-      *args, fun_jaxpr=batched_fun_jaxpr,\n-      fwd_jaxpr_thunk=batched_fwd_jaxpr_thunk, bwd=batched_bwd,\n-      num_consts=num_consts, out_trees=out_trees, symbolic_zeros=symbolic_zeros)\n-  out_dims = out_dims2[0] if out_dims2 else out_dims1\n-  return batched_outs, out_dims\n-batching.fancy_primitive_batchers[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_vmap\n+  return call_jaxpr.out_avals, call_jaxpr.effects\n+core.custom_typechecks[custom_vjp_call_p] = _custom_vjp_call_typecheck\n \n-def _custom_vjp_call_jaxpr_dce(\n+def _custom_vjp_call_dce(\n     used_outs: Sequence[bool], eqn: core.JaxprEqn\n ) -> tuple[list[bool], core.JaxprEqn | None]:\n   if not any(used_outs) and not pe.has_effects(eqn):\n     return [False] * len(eqn.invars), None\n-  fun_jaxpr: core.ClosedJaxpr = eqn.params[\"fun_jaxpr\"]\n+  call_jaxpr: core.ClosedJaxpr = eqn.params[\"call_jaxpr\"]\n   fwd_jaxpr_thunk = eqn.params[\"fwd_jaxpr_thunk\"]\n   bwd: lu.WrappedFun = eqn.params[\"bwd\"]\n   out_trees: Callable[[], Sequence[PyTreeDef]] = eqn.params[\"out_trees\"]\n   symbolic_zeros: bool = eqn.params[\"symbolic_zeros\"]\n-  dce_fun_jaxpr: core.ClosedJaxpr\n+  dce_call_jaxpr: core.ClosedJaxpr\n   used_ins: Sequence[bool]\n-  dce_fun_jaxpr, used_ins = _cached_closed_call_dce_instantiate(\n-      fun_jaxpr, tuple(used_outs))\n+  dce_call_jaxpr, used_ins = _cached_closed_call_dce_instantiate(\n+      call_jaxpr, tuple(used_outs))\n   assert all(used_ins)\n \n+  @partial(lu.wrap_init, debug_info=fwd_jaxpr_thunk.debug_info)\n   @pe._memoize\n   def dce_fwd_jaxpr_thunk(*zeros):\n-    fwd_jaxpr = core.ClosedJaxpr(*fwd_jaxpr_thunk(*zeros))\n+    fwd_jaxpr = core.ClosedJaxpr(*fwd_jaxpr_thunk.call_wrapped(*zeros))\n     _, res_tree = out_trees()\n     num_res = res_tree.num_leaves\n     dce_fwd_jaxpr, _ = _cached_closed_call_dce_instantiate(\n@@ -1058,7 +1019,7 @@ def dce_bwd(*args):\n     res, cts = split_list(args, [res_tree.num_leaves])\n     cts_ = iter(cts)\n     all_cts = []\n-    for used, aval in zip(used_outs, fun_jaxpr.out_avals):\n+    for used, aval in zip(used_outs, call_jaxpr.out_avals):\n       if used:\n         all_cts.append(next(cts_))\n       else:\n@@ -1075,17 +1036,15 @@ def dce_bwd(*args):\n   outvars = [v for used, v in zip(used_outs, eqn.outvars) if used]\n   new_params = dict(\n       eqn.params,\n-      fun_jaxpr=dce_fun_jaxpr,\n+      call_jaxpr=dce_call_jaxpr,\n       fwd_jaxpr_thunk=dce_fwd_jaxpr_thunk,\n       bwd=dce_bwd_wrapped,\n   )\n   new_eqn = pe.new_jaxpr_eqn(\n-      eqn.invars, outvars, eqn.primitive, new_params, dce_fun_jaxpr.effects,\n+      eqn.invars, outvars, eqn.primitive, new_params, dce_call_jaxpr.effects,\n       eqn.source_info, eqn.ctx)\n   return list(used_ins), new_eqn\n-pe.dce_rules[custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr_dce\n-\n-xla.register_initial_style_primitive(custom_vjp_call_jaxpr_p)\n+pe.dce_rules[custom_vjp_call_p] = _custom_vjp_call_dce\n \n batching.primitive_batchers[ad.custom_lin_p] = ad.raise_custom_vjp_error_on_jvp\n mlir.register_lowering(ad.custom_lin_p, ad.raise_custom_vjp_error_on_jvp)\n@@ -1586,7 +1545,6 @@ def jvp(primals, tangents):\n # TODO(mattjj): remove these stubs, which exist to avoid breaking internal users\n custom_jvp_call_jaxpr_p = core.Primitive(\"custom_jvp_call_jaxpr\")\n \n-\n # The following is a helper for optimizing the behavior of custom_vjp when used\n # under remat. This is really only useful when the `fwd` function to custom_vjp\n # executes a black box kernel. Otherwise, DCE will perform this optimization\ndiff --git a/jax/_src/interpreters/partial_eval.py b/jax/_src/interpreters/partial_eval.py\nindex 64226a789cde..5866b0c5f8eb 100644\n--- a/jax/_src/interpreters/partial_eval.py\n+++ b/jax/_src/interpreters/partial_eval.py\n@@ -434,49 +434,45 @@ def process_custom_vjp_call(self, prim, f, fwd, bwd, tracers, out_trees, symboli\n     if all(t.is_known() for t in tracers):\n       vals = [t.pval[1] for t in tracers]\n       with core.set_current_trace(self.parent_trace):\n-        return prim.bind(f, fwd, bwd, *vals, out_trees=out_trees, symbolic_zeros=symbolic_zeros)\n-    else:\n-      # TODO(mattjj): remove non-ad users of partial eval, then drop this case.\n-      # We stage out the whole thing, i.e. no nontrivial partial evaluation.\n-      tracers = map(self.instantiate_const_abstracted, tracers)\n-      # Because we instantiate all tracers, in_knowns is all False.\n-      in_knowns, in_avals, () = partition_pvals([t.pval for t in tracers])\n-      f = trace_to_subjaxpr_nounits(f, self, True, f.debug_info)\n-      f, aux = partial_eval_wrapper_nounits(f, (*in_knowns,), (*in_avals,))\n-      with core.set_current_trace(self.parent_trace):\n-        out_flat = prim.bind(f, fwd, bwd, out_trees=out_trees,\n-                             symbolic_zeros=symbolic_zeros)\n-      out_knowns, out_avals, jaxpr, env = aux()\n-      out_consts, res = split_list(out_flat, [len(out_flat)-len(jaxpr.constvars)])\n-      res_tracers = map(self.new_instantiated_const, res)\n-      env_tracers = map(self.to_jaxpr_tracer, env)\n-      out_tracers = [JaxprTracer(self, PartialVal.unknown(a), None)\n-                    for a in out_avals]\n-      closed_jaxpr = core.ClosedJaxpr(convert_constvars_jaxpr(jaxpr), ())\n-\n-      @_memoize\n-      def fwd_jaxpr_thunk(*zeros):\n-        fwd_ = _interleave_fun(fwd, zeros)\n-        fwd_ = trace_to_subjaxpr_nounits(fwd_, self, True, fwd_.debug_info)\n-        fwd_, aux = partial_eval_wrapper_nounits(fwd_, (*in_knowns,), (*in_avals,))\n-        out_flat = fwd_.call_wrapped()\n-        out_knowns, out_avals, jaxpr, env = aux()\n-        _, res = split_list(out_flat, [len(out_flat)-len(jaxpr.constvars)])\n-        converted_jaxpr = convert_envvars_to_constvars(jaxpr, len(env))\n-        return converted_jaxpr, (*res, *env)\n+        return prim.bind(f, fwd, bwd, *vals, out_trees=out_trees,\n+                         symbolic_zeros=symbolic_zeros)\n+\n+    tracers = map(self.instantiate_const, tracers)\n+    in_knowns = (False,) * len(tracers)\n+    in_avals = tuple(t.aval for t in tracers)\n+    f_ = trace_to_subjaxpr_nounits2(f, self.tag, f.debug_info, True)\n+    f_, aux = partial_eval_wrapper_nounits(f_, in_knowns, in_avals)\n+    params = dict(out_trees=out_trees, symbolic_zeros=symbolic_zeros)\n+    res = prim.bind_with_trace(self.parent_trace, (f_, fwd, bwd), params)\n+    out_knowns, out_avals, jaxpr, env = aux()\n+    assert not any(out_knowns)\n+    res_tracers = map(self.instantiate_const, map(self.new_const, res))\n+    env_tracers = map(self.to_jaxpr_tracer, env)\n+    out_tracers = [JaxprTracer(self, PartialVal.unknown(a), None)\n+                   for a in out_avals]\n+    closed_jaxpr = close_jaxpr(convert_constvars_jaxpr(jaxpr))\n+\n+    @partial(lu.wrap_init, debug_info=fwd.debug_info)\n+    @_memoize\n+    def fwd_jaxpr_thunk(*zeros):\n+      fwd_ = _interleave_fun(fwd, zeros)\n+      fwd_jaxpr, _, consts, () = trace_to_jaxpr_dynamic(fwd_, in_avals)\n+      return fwd_jaxpr, consts\n \n     name_stack = self._current_truncated_name_stack()\n     source = source_info_util.current().replace(name_stack=name_stack)\n+    params = dict(\n+        call_jaxpr=closed_jaxpr,\n+        fwd_jaxpr_thunk=fwd_jaxpr_thunk,\n+        num_consts=len(res) + len(env),\n+        bwd=bwd,\n+        out_trees=out_trees,\n+        symbolic_zeros=symbolic_zeros\n+    )\n     eqn = new_eqn_recipe((*res_tracers, *env_tracers, *tracers),\n-                         out_tracers, prim.initial_style,\n-                         dict(fun_jaxpr=closed_jaxpr,\n-                              fwd_jaxpr_thunk=fwd_jaxpr_thunk,\n-                              num_consts=len(res) + len(env),\n-                              bwd=bwd, out_trees=out_trees,\n-                              symbolic_zeros=symbolic_zeros),\n-                         jaxpr.effects, source)\n+                         out_tracers, prim, params, jaxpr.effects, source)\n     for t in out_tracers: t.recipe = eqn\n-    return merge_lists(out_knowns, out_tracers, out_consts)\n+    return out_tracers\n \n def partition_pvals(\n     pvals: list[PartialVal]\n@@ -2050,6 +2046,7 @@ def process_custom_jvp_call(self, prim, fun: lu.WrappedFun,\n     fun_jaxpr, out_avals, consts, () = trace_to_jaxpr_dynamic(fun, in_avals)\n     closed_fun_jaxpr = core.ClosedJaxpr(convert_constvars_jaxpr(fun_jaxpr), ())\n \n+    @partial(lu.wrap_init, debug_info=jvp.debug_info)\n     @_memoize\n     def jvp_jaxpr_thunk(*in_zeros):\n       for store in jvp.stores: store and store.reset()\n@@ -2065,8 +2062,7 @@ def jvp_jaxpr_thunk(*in_zeros):\n     outvars = map(self.makevar, out_tracers)\n     eqn = new_jaxpr_eqn([*constvars, *invars], outvars, prim,\n                         dict(call_jaxpr=closed_fun_jaxpr,\n-                             jvp_jaxpr_fun=lu.wrap_init(jvp_jaxpr_thunk,\n-                                                        debug_info=jvp.debug_info),\n+                             jvp_jaxpr_fun=jvp_jaxpr_thunk,\n                              num_consts=len(consts),\n                              symbolic_zeros=symbolic_zeros),\n                         fun_jaxpr.effects,\n@@ -2086,6 +2082,7 @@ def process_custom_vjp_call(self, prim: core.Primitive,\n     fun_jaxpr, out_avals, consts, _ = trace_to_jaxpr_dynamic(fun, in_avals)\n     closed_fun_jaxpr = core.ClosedJaxpr(convert_constvars_jaxpr(fun_jaxpr), ())\n \n+    @partial(lu.wrap_init, debug_info=fwd.debug_info)\n     @_memoize\n     def fwd_jaxpr_from_zeros(*zeros):\n       for store in fwd.stores: store and store.reset()\n@@ -2098,9 +2095,8 @@ def fwd_jaxpr_from_zeros(*zeros):\n     invars = map(self.getvar, tracers)\n     constvars = map(self.getvar, map(to_jaxpr_tracer, consts))\n     outvars = map(self.makevar, out_tracers)\n-    eqn = new_jaxpr_eqn([*constvars, *invars], outvars,\n-                        prim.initial_style,  # pytype: disable=attribute-error\n-                        dict(fun_jaxpr=closed_fun_jaxpr,\n+    eqn = new_jaxpr_eqn([*constvars, *invars], outvars, prim,\n+                        dict(call_jaxpr=closed_fun_jaxpr,\n                              fwd_jaxpr_thunk=fwd_jaxpr_from_zeros,\n                              num_consts=len(consts),\n                              bwd=bwd, out_trees=out_trees,\ndiff --git a/jax/_src/pallas/cost_estimate.py b/jax/_src/pallas/cost_estimate.py\nindex 3b82d3095f64..93bcf5348b24 100644\n--- a/jax/_src/pallas/cost_estimate.py\n+++ b/jax/_src/pallas/cost_estimate.py\n@@ -238,15 +238,15 @@ def _pjit_cost_rule(ctx, *, jaxpr: jax_core.ClosedJaxpr, **_):\n   )\n register_cost_rule(pjit.pjit_p, _pjit_cost_rule)\n \n-def _custom_vjp_rule(ctx, *, fun_jaxpr: jax_core.ClosedJaxpr, **_):\n+def _custom_vjp_rule(ctx, *, call_jaxpr: jax_core.ClosedJaxpr, **_):\n   del ctx\n-  inner_cost = cost_estimate_jaxpr(fun_jaxpr)\n+  inner_cost = cost_estimate_jaxpr(call_jaxpr)\n   return CostEstimate(\n       flops=inner_cost.flops,\n       transcendentals=inner_cost.transcendentals,\n       bytes_accessed=inner_cost.bytes_accessed,\n   )\n-register_cost_rule(custom_derivatives.custom_vjp_call_jaxpr_p, _custom_vjp_rule)\n+register_cost_rule(custom_derivatives.custom_vjp_call_p, _custom_vjp_rule)\n \n def _run_state_rule(*_, jaxpr: jax_core.Jaxpr, **_2):\n   inner_cost = cost_estimate_jaxpr(pe.close_jaxpr(jaxpr))\ndiff --git a/jax/custom_derivatives.py b/jax/custom_derivatives.py\nindex 3628ae4aaa6e..b768b687dfad 100644\n--- a/jax/custom_derivatives.py\n+++ b/jax/custom_derivatives.py\n@@ -26,7 +26,6 @@\n   custom_jvp_call_jaxpr_p as custom_jvp_call_jaxpr_p,\n   custom_vjp as custom_vjp,\n   custom_vjp_call_p as custom_vjp_call_p,\n-  custom_vjp_call_jaxpr_p as custom_vjp_call_jaxpr_p,\n   custom_vjp_primal_tree_values as custom_vjp_primal_tree_values,\n   CustomVJPPrimal as CustomVJPPrimal,\n   linear_call as linear_call,\ndiff --git a/jax/experimental/jax2tf/jax2tf.py b/jax/experimental/jax2tf/jax2tf.py\nindex 786e021e2ff0..536bf1f201f0 100644\n--- a/jax/experimental/jax2tf/jax2tf.py\n+++ b/jax/experimental/jax2tf/jax2tf.py\n@@ -3461,14 +3461,14 @@ def _custom_jvp_call(*args: TfVal, call_jaxpr: core.ClosedJaxpr,\n tf_impl[custom_derivatives.custom_jvp_call_p] = _custom_jvp_call\n \n \n-def _custom_vjp_call_jaxpr(*args: TfVal, fun_jaxpr: core.ClosedJaxpr,\n-                           **_) -> Sequence[TfVal]:\n+def _custom_vjp_call(*args: TfVal, call_jaxpr: core.ClosedJaxpr,\n+                     **_) -> Sequence[TfVal]:\n   # TODO(necula): ensure that there is no AD transformation in scope\n-  return _interpret_jaxpr(fun_jaxpr, *args, extra_name_stack=\"custom_vjp\",\n+  return _interpret_jaxpr(call_jaxpr, *args, extra_name_stack=\"custom_vjp\",\n                           fresh_constant_cache=False)\n \n \n-tf_impl[custom_derivatives.custom_vjp_call_jaxpr_p] = _custom_vjp_call_jaxpr\n+tf_impl[custom_derivatives.custom_vjp_call_p] = _custom_vjp_call\n \n \n def _custom_lin(*args: TfVal, **_) -> Sequence[TfVal]:\ndiff --git a/jax/extend/core/primitives.py b/jax/extend/core/primitives.py\nindex 60d8cd24a949..30350dace637 100644\n--- a/jax/extend/core/primitives.py\n+++ b/jax/extend/core/primitives.py\n@@ -26,7 +26,6 @@\n   custom_jvp_call_p as custom_jvp_call_p,\n   custom_jvp_call_jaxpr_p as custom_jvp_call_jaxpr_p,\n   custom_vjp_call_p as custom_vjp_call_p,\n-  custom_vjp_call_jaxpr_p as custom_vjp_call_jaxpr_p,\n )\n \n from jax._src.dispatch import device_put_p as device_put_p\ndiff --git a/tests/custom_api_test.py b/tests/custom_api_test.py\nindex 72c14634a9c8..73dc2fbefcaa 100644\n--- a/tests/custom_api_test.py\n+++ b/tests/custom_api_test.py\n@@ -3065,7 +3065,7 @@ def check_jaxpr(jaxpr, used_outs, includes, excludes):\n       if not dce_jaxpr.eqns:\n         assert not includes\n         return\n-      call_jaxpr = dce_jaxpr.eqns[0].params[\"fun_jaxpr\"]\n+      call_jaxpr = dce_jaxpr.eqns[0].params[\"call_jaxpr\"]\n       for prim in includes:\n         assert any(eqn.primitive == prim for eqn in call_jaxpr.eqns)\n       for prim in excludes:\n\n```"
        ],
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Merge pull request #28607 from gnecula:fix_platform_dependent\n\nPiperOrigin-RevId: 758086717",
        "output": [
            "```diff\nCommit: 97a354dfc67fe8e2230257735b6aab1434e8b15e\nDate: 2025-05-13T07:28:43Z\nURL: https://github.com/jax-ml/jax/commit/97a354dfc67fe8e2230257735b6aab1434e8b15e\nFiles changed: 8\nAdditions: +201, Deletions: -90\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex 5a6456762db7..144cbaf5cd21 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -759,7 +759,8 @@ def jaxpr_to_checkify_jaxpr(\n   out_tree, error_effects = metadata()\n   return checked_jaxpr, out_tree, error_effects\n \n-def cond_error_check(error: Error, enabled_errors, index, *ops, branches):\n+def cond_error_check(error: Error, enabled_errors, index, *ops,\n+                     branches, **params):\n   # Get the error-effects out of all branches so the cond can be called with\n   # a merged error with all these effects.\n   err_vals, err_tree = jtu.tree_flatten(error)\n@@ -780,7 +781,7 @@ def get_error_effects_from_jaxpr(jxpr):\n \n   err_and_outs = lax.cond_p.bind(\n       index, *err_vals, *ops,\n-      branches=tuple(new_branches))\n+      branches=tuple(new_branches), **params)\n \n   # we need to merge metadata across out_trees (a tuple)\n   err0, out = tree_unflatten(out_trees[0], err_and_outs)\ndiff --git a/jax/_src/interpreters/mlir.py b/jax/_src/interpreters/mlir.py\nindex e9deb8d3fff9..f6ef5787ccbf 100644\n--- a/jax/_src/interpreters/mlir.py\n+++ b/jax/_src/interpreters/mlir.py\n@@ -2080,6 +2080,11 @@ def _platforms_for_eqn_ctx(eqn_ctx: core.JaxprEqnContext | None\n     return ('tpu',)\n   return ()\n \n+def _platforms_for_eqn(ctx: LoweringRuleContext) -> tuple[str, ...]:\n+  \"\"\"The lowering platforms for the current eqn\"\"\"\n+  return tuple((_platforms_for_eqn_ctx(ctx.jaxpr_eqn_ctx) or\n+               ctx.platforms or ctx.module_context.platforms))\n+\n \n def lower_per_platform(ctx: LoweringRuleContext,\n                        description: str,\n@@ -2122,8 +2127,7 @@ def lower_per_platform(ctx: LoweringRuleContext,\n    rule_args: the args of the lowering rules.\n    rule_kwargs: the kwargs of the lowering rules.\n   \"\"\"\n-  platforms: Sequence[str] = (_platforms_for_eqn_ctx(ctx.jaxpr_eqn_ctx) or\n-                              ctx.platforms or ctx.module_context.platforms)\n+  platforms: Sequence[str] = _platforms_for_eqn(ctx)\n   # Special case the common case (single-platform lowering)\n   if len(platforms) == 1:\n     rule = platform_rules.get(platforms[0], default_rule)\ndiff --git a/jax/_src/lax/control_flow/__init__.py b/jax/_src/lax/control_flow/__init__.py\nindex f89e4d53a476..44ee94e14ca2 100644\n--- a/jax/_src/lax/control_flow/__init__.py\n+++ b/jax/_src/lax/control_flow/__init__.py\n@@ -34,6 +34,7 @@\n     while_p as while_p,\n )\n from jax._src.lax.control_flow.conditionals import (\n+    BranchesPlatforms as BranchesPlatforms,\n     cond as cond,\n     cond_p as cond_p,\n     switch as switch,\ndiff --git a/jax/_src/lax/control_flow/conditionals.py b/jax/_src/lax/control_flow/conditionals.py\nindex 99fa72421ea1..d875989921d0 100644\n--- a/jax/_src/lax/control_flow/conditionals.py\n+++ b/jax/_src/lax/control_flow/conditionals.py\n@@ -46,6 +46,7 @@\n from jax._src.interpreters import xla\n from jax._src.lax import lax\n from jax._src.traceback_util import api_boundary\n+from jax._src.typing import ArrayLike\n from jax._src.util import safe_map, split_list, partition_list, unzip2\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n@@ -127,9 +128,17 @@ def switch(index, branches, *operands):\n   lo = np.array(0, np.int32)\n   hi = np.array(len(branches) - 1, np.int32)\n   index = lax.clamp(lo, index, hi)\n+  return _switch_internal(index, branches, operands,\n+                          branches_platforms=None)\n \n+\n+def _switch_internal(\n+    index: ArrayLike,\n+    branches: Sequence[Callable],\n+    operands: Sequence[ArrayLike], *,\n+    branches_platforms: BranchesPlatforms | None):\n   if (config.disable_jit.value and core.is_concrete(index)):\n-    return branches[int(index)](*operands)\n+    return branches[int(index)](*operands)  # type: ignore\n \n   dbgs = [api_util.debug_info(\"switch\", branch, operands, {})\n           for branch in branches]\n@@ -159,7 +168,10 @@ def switch(index, branches, *operands):\n     raise NotImplementedError(\n         f'Effects not supported in `switch`: {disallowed_effects}')\n   jaxprs = [replace_jaxpr_effects(jaxpr, joined_effects) for jaxpr in jaxprs]\n-  out = cond_p.bind(index, *consts, *ops, branches=tuple(jaxprs))\n+  params = dict(branches=tuple(jaxprs))\n+  if branches_platforms is not None:\n+    params[\"branches_platforms\"] = branches_platforms\n+  out = cond_p.bind(index, *consts, *ops, **params)\n   out_ = iter(out)\n \n   all_inputs = [*consts, *ops]\n@@ -464,7 +476,7 @@ def _bcast_select_n(pred, *cases):\n     pred = lax.broadcast_in_dim(pred, np.shape(cases[0]), idx)\n   return lax.select_n(pred, *cases)\n \n-def _cond_batching_rule(axis_data, args, dims, branches):\n+def _cond_batching_rule(axis_data, args, dims, *, branches, **params):\n   index, *ops = args\n   index_dim, *op_dims = dims\n   # TODO(sharadmv): clean this up by adding a specific blocklist\n@@ -480,6 +492,9 @@ def _cond_batching_rule(axis_data, args, dims, branches):\n \n \n   if index_dim is not batching.not_mapped:\n+    assert \"branches_platforms\" not in params, (\n+        \"The index of a cond with branches_platforms should be a \"\n+        \"platform_index and should never be mapped\")\n     # Convert to a lax.select. While we could get away with not broadcasting\n     # some operands yet, because all outputs must be broadcast together anyway\n     # for the select we broadcast the input operands for simplicity and leave\n@@ -518,10 +533,11 @@ def _cond_batching_rule(axis_data, args, dims, branches):\n         for jaxpr in branches)\n \n     out_dims = [0 if b else batching.not_mapped for b in out_bat]\n-    out = cond_p.bind(index, *ops, branches=branches_batched)\n+    out = cond_p.bind(index, *ops, branches=branches_batched,\n+                      **params)\n     return out, out_dims\n \n-def _cond_jvp(primals, tangents, branches):\n+def _cond_jvp(primals, tangents, *, branches, **params):\n   nonzeros = [type(t) is not ad_util.Zero for t in tangents]\n \n   index_nz, *ops_nz = nonzeros\n@@ -538,14 +554,15 @@ def _cond_jvp(primals, tangents, branches):\n   _, *ops_dot = tangents\n   ops_dot = _prune_zeros(ops_dot)\n \n-  out = cond_p.bind(index, *ops, *ops_dot, branches=branches_jvp)\n+  out = cond_p.bind(index, *ops, *ops_dot, branches=branches_jvp,\n+                    **params)\n   out_primals, out_tangents = split_list(out, [len(out_nz)])\n   out_tangents_iter = iter(out_tangents)\n   out_tangents = [next(out_tangents_iter) if nz else ad_util.Zero.from_primal_value(p)\n                   for p, nz in zip(out_primals, out_nz)]\n   return out_primals, out_tangents\n \n-def _cond_partial_eval(trace, *tracers, branches):\n+def _cond_partial_eval(trace, *tracers, branches, **params):\n   in_unknowns = [t.pval[0] is not None for t in tracers]\n   index_uk, *ops_uk = in_unknowns\n   if any(isinstance(eff, RefEffect) for branch in branches for eff in\n@@ -556,7 +573,7 @@ def _cond_partial_eval(trace, *tracers, branches):\n   if index_uk:\n     # When the branch index is unknown, we stage out the whole cond.\n     # TODO(mattjj): remove this path when old remat is removed\n-    params = dict(branches=branches)\n+    params = dict(branches=branches, **params)\n     return trace.default_process_primitive(cond_p, tracers, params)\n \n   branches_out_uks = []\n@@ -586,7 +603,8 @@ def _cond_partial_eval(trace, *tracers, branches):\n              for j in branches_known[1:])\n \n   in_consts = [t.pval.get_known() for t in tracers if t.pval.is_known()]\n-  out_consts_res = cond_p.bind(*in_consts, branches=branches_known)\n+  out_consts_res = cond_p.bind(*in_consts, branches=branches_known,\n+                               **params)\n   out_consts, res = split_list(out_consts_res, [len(out_consts_res) - num_res])\n \n   index_tracer = trace.instantiate_const(tracers[0])\n@@ -595,7 +613,7 @@ def _cond_partial_eval(trace, *tracers, branches):\n   res_tracers = map(trace.new_instantiated_const, res)\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(aval), None)\n                  for aval in branches_unknown[0].out_avals]\n-  params = dict(branches=branches_unknown)\n+  params = dict(branches=branches_unknown, **params)\n   name_stack = source_info_util.current_name_stack()[len(trace.name_stack):]\n   source = source_info_util.current().replace(name_stack=name_stack)\n   eqn = pe.new_eqn_recipe(\n@@ -608,6 +626,7 @@ def _cond_partial_eval(trace, *tracers, branches):\n def _cond_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n   index_uk, *ops_uk = unks_in\n   branches = eqn.params['branches']\n+  eqn_rest_params = dict(k_v for k_v in eqn.params.items() if k_v[0] != 'branches')\n \n   # Instantiate all inputs (b/c jaxpr_staged will take all inputs).\n   new_inst = [x for x, inst in zip(eqn.invars, inst_in)\n@@ -664,7 +683,7 @@ def _cond_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n   # Build the known eqn.\n   ins_known, _ = partition_list(unks_in, eqn.invars)  # includes index invar\n   out_binders_known, _ = partition_list(unks_out, eqn.outvars)\n-  params_known = dict(branches=branches_known)\n+  params_known = dict(branches=branches_known, **eqn_rest_params)\n   effects_known = _join_cond_effects(branches_known)\n   eqn_known = pe.new_jaxpr_eqn(\n       ins_known, [*out_binders_known, *res_binders], cond_p, params_known,\n@@ -672,7 +691,7 @@ def _cond_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n \n   # Build the staged eqn.\n   _, out_binders_staged = partition_list(inst_out, eqn.outvars)\n-  params_staged = dict(branches=branches_staged)\n+  params_staged = dict(branches=branches_staged, **eqn_rest_params)\n   effects_staged = _join_cond_effects(branches_staged)\n   eqn_staged = pe.new_jaxpr_eqn(\n       [eqn.invars[0], *res_binders, *eqn.invars[1:]], out_binders_staged,\n@@ -818,7 +837,7 @@ def transposed(*args):\n                                          debug_info=jaxpr.jaxpr.debug_info),\n                             res_avals + jaxpr.out_avals)\n \n-def _cond_transpose(cts, *args, branches):\n+def _cond_transpose(cts, *args, branches, **params):\n   index, *ops = args\n   assert type(index) is not ad.UndefinedPrimal\n   linear = [type(x) is ad.UndefinedPrimal for x in ops]\n@@ -838,7 +857,8 @@ def _cond_transpose(cts, *args, branches):\n   res = ops[:num_res]\n   cts = map(ad.instantiate_zeros, cts)\n \n-  out = cond_p.bind(index, *res, *cts, branches=branches_trans)\n+  out = cond_p.bind(index, *res, *cts, branches=branches_trans,\n+                    **params)\n   assert all(map(core.typecheck, lin_in_avals, out))\n \n   out_iter = iter(out)\n@@ -846,7 +866,8 @@ def _cond_transpose(cts, *args, branches):\n   assert next(out_iter, None) is None\n   return [None] + out\n \n-def _cond_typecheck(bind_time, *in_atoms, branches):\n+def _cond_typecheck(bind_time, *in_atoms, branches, **params):\n+  del params\n   if not bind_time:\n     _, *in_atoms = in_atoms\n   avals = [x.aval for x in in_atoms]\n@@ -900,6 +921,16 @@ def _cond_typecheck(bind_time, *in_atoms, branches):\n       f'called with operands of type {_avals_short(op_avals)}')\n   return jaxpr0.out_avals, joined_effects\n \n+\n+BranchesPlatforms = tuple[tuple[str, ...] | None, ...]\n+# cond_p takes an optional branches_platforms param of type `BranchesPlatforms`\n+# when it is a `platform_dependent` conditional.\n+# In that case, `branches_platforms` is a tuple as long\n+# as `branches` and for each branch it specifies the lowering platforms it\n+# corresponds to. The last element, corresponding to the last branch,\n+# can be `None` to represent a default match-all-lowering-platforms.\n+# The index argument of a `platform_dependent` cond is always a\n+# `platform_index` primitive.\n cond_p = core.Primitive('cond')\n cond_p.multiple_results = True\n cond_p.skip_canonicalization = True\n@@ -915,7 +946,39 @@ def _cond_typecheck(bind_time, *in_atoms, branches):\n pe.dce_rules[cond_p] = _cond_dce_rule\n batching.ragged_prop_rules[cond_p] = batching.ragged_mask_assert_no_op_rule\n \n-def _cond_lowering(ctx, index, *args, branches):\n+def _cond_lowering(ctx, index, *args, branches,\n+                   **params):\n+  if (branches_platforms := params.get(\"branches_platforms\", None)) is not None:\n+    branches_kept: list[core.ClosedJaxpr] = []\n+    index_to_kept_index: dict[int, int] = {}\n+    for p in mlir._platforms_for_eqn(ctx):\n+      # Each `p` must appear in exactly one branches_platforms, or in the\n+      # last default branch. Otherwise, platform_index lowering would have\n+      # failed already.\n+      for b_idx, b_platforms in enumerate(branches_platforms):\n+        if b_platforms is None or p in b_platforms:\n+          if b_idx not in index_to_kept_index:\n+            index_to_kept_index[b_idx] = len(branches_kept)\n+            branches_kept.append(branches[b_idx])\n+          break\n+      else:\n+        assert False, p\n+\n+    # Compute the new index into branches_keep\n+    i32_type = ir.RankedTensorType.get([], mlir.dtype_to_ir_type(dtypes.dtype(np.int32)))\n+    kept_index_case_op = hlo.CaseOp([i32_type],\n+                                    index=index,\n+                                    num_branches=len(branches))\n+    for i in range(len(branches)):\n+      branch = kept_index_case_op.regions[i].blocks.append()\n+      with ir.InsertionPoint(branch):\n+        kept_i = np.int32(index_to_kept_index.get(i, 0))\n+        hlo.return_([mlir.ir_constant(kept_i)])\n+\n+    index = kept_index_case_op\n+    branches = branches_kept\n+    assert branches, \"platform_index lowering should have failed first\"\n+\n   joined_effects = core.join_effects(*(branch.effects for branch in branches))\n   ordered_effects = list(effects.ordered_effects.filter_in(joined_effects))\n   num_tokens = len(ordered_effects)\n@@ -952,7 +1015,8 @@ def _cond_lowering(ctx, index, *args, branches):\n mlir.register_lowering(cond_p, _cond_lowering)\n \n @register_partial_discharge_rule(cond_p)\n-def _cond_state_discharge_rule(should_discharge, in_avals, out_avals, index, *args, branches):\n+def _cond_state_discharge_rule(should_discharge, in_avals, out_avals, index, *args,\n+                               branches, **params):\n   assert not should_discharge[0], \"Can't discharge the index.\"\n   discharged_branches = tuple(\n       discharge_state(branch.jaxpr, (), should_discharge=should_discharge[1:])[0]\n@@ -981,7 +1045,8 @@ def _cond_state_discharge_rule(should_discharge, in_avals, out_avals, index, *ar\n                                   if fwd is None]), ())\n       for branch in discharged_branches\n   )\n-  out_vals_no_fwd = cond_p.bind(index, *args, branches=new_branches)\n+  out_vals_no_fwd = cond_p.bind(index, *args, branches=new_branches,\n+                                **params)\n   out_vals, out_ref_vals_no_fwd = util.split_list(out_vals_no_fwd, [len(out_avals)])\n   # Insert forwarded values into reference outputs\n   ref_val_no_fwd_iter = iter(out_ref_vals_no_fwd)\n@@ -1046,50 +1111,41 @@ def other_platforms_code(*args): ...\n     The value ``per_platform[execution_platform](*args)``.\n   \"\"\"\n   # Join identical branches\n-  platform_branches: list[tuple[list[str], Callable]] = []\n+  branches_platforms_list: list[tuple[list[str], Callable]] = []\n   for pname, pbranch in per_platform.items():\n+    if not callable(pbranch):\n+      raise TypeError(f\"lax.platform_dependent: the '{pname}' branch must \"\n+                      \"be a callable.\")\n     if pname == \"gpu\":\n       raise ValueError(\"Use 'cuda' or 'rocm' for lax.platform_dependent.\")\n-    for ps, b in platform_branches:\n+    for ps, b in branches_platforms_list:\n       if b == pbranch:\n         ps.append(pname)\n         break\n     else:\n-      platform_branches.append(([pname], pbranch))\n-\n-  platforms_lists, branches = util.unzip2(platform_branches)\n-  platform_index = platform_index_p.bind(\n-    platforms=tuple(tuple(ps) for ps in platforms_lists),\n-    has_default=(default is not None))\n+      branches_platforms_list.append(([pname], pbranch))\n \n+  platforms_lists, branches = util.unzip2(branches_platforms_list)\n+  branches_platforms: BranchesPlatforms = tuple(tuple(ps) for ps in platforms_lists)\n   if default is not None:\n+    if not callable(default):\n+      raise TypeError(\"lax.platform_dependent: the 'default' branch must \"\n+                      \"be a callable.\")\n     branches = branches + (default,)\n-  # Use a switch, to get the proper transformation rules for free. Since\n-  # platform index has no dependence on the input data, it won't be vectorized\n-  # under vmap.\n-  # If the switch and the platform_index_p above are in the same compilation\n-  # unit then constant-folding will remove the unnecessary branches. However,\n-  # if we run in eager mode the switch below cannot be constant-folded and\n-  # the compilation may fail if some of the branches contain custom calls not\n-  # recognized on the compilation platform. Detect eager mode and keep only the\n-  # needed branch.\n-  try:\n-    # Note/TODO(mvoz): This actually rarely seems to concretize - we could look into\n-    # core.ensure_compile_time_eval to get better single-branch selection.\n-    platform_index_concrete = core.concrete_or_error(operator.index, platform_index)\n-  except core.ConcretizationTypeError:\n-    return switch(platform_index, branches, *args)\n-  else:\n-    assert 0 <= platform_index_concrete < len(branches)\n-    return branches[platform_index_concrete](*args)\n+    branches_platforms = branches_platforms + (None,)  # type: ignore\n+  platform_index = platform_index_p.bind(platforms=branches_platforms)\n+\n+  if core.is_concrete(platform_index):\n+    return branches[int(platform_index)](*args)\n+  return _switch_internal(platform_index, branches, args,\n+                          branches_platforms=branches_platforms)\n+\n \n # A primitive to compute the index of a platform into a list of platforms.\n # Args:\n-#   platforms: Sequence[Sequence[str]]: a sequence of sequences of platform\n-#     names. If the current lowering platform is in one of the inner sequences\n-#     returns the index of that inner sequence in the outer sequence.\n-#   has_default: if True, and if the lowering platform is not found in\n-#     `platforms` then return `len(platforms)`. Otherwise, raise an error.\n+#   platforms: BranchesPlatforms. If the current lowering\n+#     platform is in one of the inner tuples returns the index of that inner\n+#     tuple in the outer tuple.\n platform_index_p = core.Primitive(\"platform_index\")\n platform_index_p.multiple_results = False\n platform_index_p.def_impl(functools.partial(dispatch.apply_primitive,\n@@ -1101,25 +1157,25 @@ def _platform_index_aval(*_, **__):\n \n def _platform_index_lowering(ctx: mlir.LoweringRuleContext,\n                              *,\n-                             platforms: Sequence[Sequence[str]],\n-                             has_default: bool):\n-  def lower_constant(\n-      ctx: mlir.LoweringRuleContext, *, i: int\n-  ) -> Sequence[ir.Value]:\n+                             platforms: BranchesPlatforms):\n+  def lower_constant(ctx: mlir.LoweringRuleContext, *,\n+                     i: int) -> Sequence[ir.Value]:\n     v = mlir.ir_constant(np.int32(i))\n-    assert isinstance(v, ir.Value), v\n     return [v]\n+\n   platform_rules: dict[str, mlir.LoweringRule] = {}\n+  default_rule = None\n   for i, ps in enumerate(platforms):\n     rule = partial(lower_constant, i=i)\n-    for p in ps:\n-      platform_rules[p] = rule\n+    if ps is None:\n+      default_rule = rule\n+    else:\n+      for p in ps:\n+        platform_rules[p] = rule\n \n-  default_rule = (\n-    partial(lower_constant, i=len(platforms)) if has_default else None)\n   return mlir.lower_per_platform(\n     ctx,\n-    f\"platform_index(platforms={platforms}, has_default={has_default})\",\n+    f\"platform_index(platforms={platforms})\",\n     platform_rules, default_rule, effects.no_effects)\n \n mlir.register_lowering(platform_index_p, _platform_index_lowering)\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 776ac1cb8143..9af3cf1e3c0a 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -47,7 +47,7 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.lax import control_flow\n from jax._src.lax import lax as lax_internal\n-from jax._src.lax.control_flow import for_loop\n+from jax._src.lax.control_flow import for_loop, BranchesPlatforms\n from jax._src.lib import version as jaxlib_version\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import arith\n@@ -3128,7 +3128,7 @@ def _while_lowering_rule(\n \n \n @register_lowering_rule(lax.cond_p)\n-def _cond_lowering_rule(ctx: LoweringRuleContext, *args, branches):\n+def _cond_lowering_rule(ctx: LoweringRuleContext, *args, branches, **params):\n   index, *args = args\n   constant_index = _fold_and_get_constant_value(index)\n \n@@ -3898,17 +3898,13 @@ def _pad(val):\n def _platform_index_lowering(\n     ctx: mlir.LoweringRuleContext,\n     *,\n-    platforms: Sequence[Sequence[str]],\n-    has_default: bool,\n+    platforms: BranchesPlatforms,\n ):\n   for i, ps in enumerate(platforms):\n     # note - slightly odd structure here, as platforms is a seq[seq[str]]\n-    if \"mosaic\" in ps:\n+    if \"mosaic\" in ps or ps is None:\n       return ir_constant(i)\n \n-  if has_default:\n-    return ir_constant(len(platforms))\n-\n   raise NotImplementedError(\n       \"No mosaic or default platform indexing rule found.\"\n   )\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex b501693bf627..9ead4f16c1a6 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -2598,7 +2598,10 @@ def _while_lowering_rule(\n @register_lowering_rule(lax.cond_p,\n   mgpu.LoweringSemantics.Lane, gpu_core.PrimitiveSemantics.Warp)\n @register_lowering_rule(lax.cond_p, mgpu.LoweringSemantics.Warpgroup)\n-def _cond_lowering_rule(ctx: LoweringRuleContext, index, *args, branches):\n+def _cond_lowering_rule(ctx: LoweringRuleContext, index, *args, branches,\n+                        **params):\n+  if params:\n+    raise NotImplementedError(\"platform_dependent cond\")\n   index_aval, *_arg_avals = ctx.avals_in\n \n   def _yielded_values(outs, avals):\ndiff --git a/jax/experimental/jax2tf/jax2tf.py b/jax/experimental/jax2tf/jax2tf.py\nindex 4c2f35a95c57..786e021e2ff0 100644\n--- a/jax/experimental/jax2tf/jax2tf.py\n+++ b/jax/experimental/jax2tf/jax2tf.py\n@@ -3062,8 +3062,11 @@ def update_computation(arg1: TfVal, arg2: TfVal) -> TfVal:\n \n \n def _cond(\n-    index: TfVal, *operands: TfVal, branches: Sequence[core.ClosedJaxpr]\n+    index: TfVal, *operands: TfVal, branches: Sequence[core.ClosedJaxpr],\n+    **params\n ) -> Sequence[TfVal]:\n+  if params:\n+    raise NotImplementedError(\"jax2tf conversion for platform_dependent\")\n   # tf.cond needs lambdas with no arguments.\n   branches_tf = [\n       partial(_interpret_jaxpr, jaxpr, *operands,\ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex 422ef769e392..d32d761ee1fa 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -37,8 +37,10 @@\n from jax.ad_checkpoint import checkpoint as new_checkpoint, checkpoint_policies\n import jax.numpy as jnp  # scan tests use numpy\n import jax.scipy as jsp\n+from jax._src import dispatch\n from jax._src.lax import control_flow as lax_control_flow\n from jax._src.lax.control_flow import for_loop\n+from jax._src.interpreters import batching\n from jax._src.interpreters import mlir\n \n jax.config.parse_flags_with_absl()\n@@ -137,6 +139,36 @@ def scan_reference(f, init, xs):\n     lambda ctx, x: mlir.hlo.CustomCallOp(\n         [x.type], [x],\n         call_target_name=mlir.ir.StringAttr.get(\"__testing_non_existent_custom_call\")).results)\n+batching.primitive_batchers[prim_non_existent_custom_call] = (\n+    lambda batched_args, batch_dims: (prim_non_existent_custom_call.bind(batched_args[0]),\n+                                      batch_dims[0]))\n+\n+# A JAX primitive that triggers error when lowering on unintended platforms\n+prim_with_lowering_error = core.Primitive(\"__testing_prim_with_lowering_error\")\n+prim_with_lowering_error.def_abstract_eval(lambda x_aval, **_: x_aval)\n+def prim_with_lowering_error_lowering(platform: str,\n+                                      ctx: mlir.LoweringRuleContext, x, *,\n+                                      only_on: str):\n+  if platform != only_on:\n+    raise ValueError(f\"prim_with_lowering_error with only_on={only_on} lowered for {platform}\")\n+  return mlir.hlo.SineOp(x).results\n+def prim_with_lowering_error_batch_rule(batched_args, batch_dims, **params):\n+  xs, = batched_args\n+  xs_bdim, = batch_dims\n+  return prim_with_lowering_error.bind(xs, **params), xs_bdim\n+\n+batching.primitive_batchers[prim_with_lowering_error] = prim_with_lowering_error_batch_rule\n+\n+mlir.register_lowering(\n+    prim_with_lowering_error,\n+    partial(prim_with_lowering_error_lowering, \"cpu\"),\n+    platform=\"cpu\")\n+mlir.register_lowering(\n+    prim_with_lowering_error,\n+    partial(prim_with_lowering_error_lowering, \"tpu\"),\n+    platform=\"tpu\")\n+prim_with_lowering_error.def_impl(partial(dispatch.apply_primitive,\n+                                          prim_with_lowering_error))\n \n \n class LaxControlFlowTest(jtu.JaxTestCase):\n@@ -1378,7 +1410,7 @@ def f(x):\n   @parameterized.named_parameters(\n       {\"testcase_name\": f\"_{name}\", \"cond\": cond}\n       for cond, name in COND_IMPLS)\n-  def testCondGrad2(self, cond):\n+  def testCondGrad2(self, cond=cond_with_new_checkpoint):\n     def f_ref(x):\n       z = jnp.array([1., 2.], x.dtype) * x if x[0] < 2 else jnp.sin(x)\n       return z.sum()\n@@ -2905,18 +2937,13 @@ def f(x):\n     x = np.arange(3, dtype=np.float32)\n     lowered = jax.jit(f).lower(x)\n     stablehlo = lowered.as_text()\n-    self.assertIn(\"stablehlo.case\", stablehlo)\n-    self.assertIn(\"stablehlo.sine\", stablehlo)\n-    self.assertIn(\"stablehlo.cosine\", stablehlo)\n-\n-    # The HLO has been canonicalized and contains only the branch we need\n-    hlo = lowered.as_text(\"hlo\")\n+    # The StableHLO contains only the branch we need\n     if jtu.device_under_test() == \"cpu\":\n-      self.assertIn(\" sine\", hlo)\n-      self.assertNotIn(\" cosine\", hlo)\n+      self.assertIn(\"stablehlo.sine\", stablehlo)\n+      self.assertNotIn(\"stablehlo.cosine\", stablehlo)\n     else:\n-      self.assertNotIn(\" sine\", hlo)\n-      self.assertIn(\" cosine\", hlo)\n+      self.assertNotIn(\"stablehlo.sine\", stablehlo)\n+      self.assertIn(\"stablehlo.cosine\", stablehlo)\n \n   def test_platform_dependent_with_non_existent_custom_call(self):\n     if not jtu.test_device_matches([\"cpu\"]):\n@@ -2939,8 +2966,7 @@ def f(x):\n \n     x = np.arange(3, dtype=np.float32)\n     hlo = str(jax.jit(f).lower(x).compiler_ir())\n-    occurrences = re.findall(prim_non_existent_custom_call.name, hlo)\n-    self.assertLen(occurrences, 3)\n+    self.assertNotIn(prim_non_existent_custom_call.name, hlo)\n \n     res_eager = f(x)\n     self.assertAllClose(res_eager, 3. * np.sin(x))\n@@ -2956,6 +2982,26 @@ def f(x):\n     res_grad = jax.grad(f)(1.)\n     self.assertAllClose(res_grad, 3. * np.cos(1.))\n \n+  def test_platform_dependent_with_primitive_with_lowering_error(self):\n+    if not jtu.test_device_matches([\"cpu\", \"tpu\"]):\n+      self.skipTest(\"Only for CPU and TPU\")\n+\n+    def f(x):\n+      return lax.platform_dependent(\n+          x,\n+          # Check that we only lower on the intended platform\n+          cpu=lambda x: prim_with_lowering_error.bind(x, only_on=\"cpu\"),\n+          tpu=lambda x: prim_with_lowering_error.bind(x, only_on=\"tpu\"))\n+\n+    self.assertAllClose(np.sin(1.), f(1.))  # Eager\n+    self.assertAllClose(np.sin(1.), jax.jit(f)(1.))\n+    self.assertAllClose(np.sin(1.), lax.cond(True, f, lambda x: x, 1.))\n+    self.assertAllClose(1., lax.cond(False, f, lambda x: x, 1.))\n+    self.assertAllClose((0., np.sin(np.arange(8.))),\n+                        lax.scan(lambda carry, x: (carry, f(x)),\n+                                 0., np.arange(8.)))\n+    self.assertAllClose(np.sin(np.arange(8.)), jax.vmap(f)(np.arange(8.)))\n+\n   def test_platform_dependent_multiple_identical_branches(self):\n     x = np.arange(3, dtype=np.float32)\n     def f(x):\n@@ -2965,13 +3011,14 @@ def f(x):\n         tpu=jnp.sin,\n         default=lambda x: x)\n     res = f(x)\n+    on_cpu_tpu = jtu.device_under_test() in [\"cpu\", \"tpu\"]\n     self.assertAllClose(\n       res,\n-      np.sin(x) if jtu.device_under_test() in [\"cpu\", \"tpu\"] else x)\n-    # We only lower the common branches once\n+      np.sin(x) if on_cpu_tpu else x)\n+\n     stablehlo = jax.jit(f).lower(x).as_text()\n     sines = re.findall(r\"stablehlo.sine\", stablehlo)\n-    self.assertEqual(1, len(sines))\n+    self.assertEqual(1 if on_cpu_tpu else 0, len(sines))\n \n   def test_platform_dependent_no_default(self):\n     ctx = contextlib.ExitStack()\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add `block_until_ready()` to FAQ code snippet.\n\nPiperOrigin-RevId: 757963427",
        "output": [
            "```diff\nCommit: 189ba3a99959245724105217a72902cc50e38b14\nDate: 2025-05-12T23:44:05Z\nURL: https://github.com/jax-ml/jax/commit/189ba3a99959245724105217a72902cc50e38b14\nFiles changed: 1\nAdditions: +3, Deletions: -2\ndiff --git a/docs/faq.rst b/docs/faq.rst\nindex f5d43d25afb6..25d1d9ffab57 100644\n--- a/docs/faq.rst\n+++ b/docs/faq.rst\n@@ -422,7 +422,6 @@ for comparing JAX versus NumPy, making using of IPython's convenient\n `%time and %timeit magics`_::\n \n     import numpy as np\n-    import jax.numpy as jnp\n     import jax\n \n     def f(x):  # function we're benchmarking (works in both NumPy & JAX)\n@@ -431,7 +430,9 @@ for comparing JAX versus NumPy, making using of IPython's convenient\n     x_np = np.ones((1000, 1000), dtype=np.float32)  # same as JAX default dtype\n     %timeit f(x_np)  # measure NumPy runtime\n \n-    %time x_jax = jax.device_put(x_np)  # measure JAX device transfer time\n+    # measure JAX device transfer time\n+    %time x_jax = jax.device_put(x_np).block_until_ready()\n+\n     f_jit = jax.jit(f)\n     %time f_jit(x_jax).block_until_ready()  # measure JAX compilation time\n     %timeit f_jit(x_jax).block_until_ready()  # measure JAX runtime\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Support pltpu.roll on sublanes when not all lanes are used.\n\nPiperOrigin-RevId: 757942183",
        "output": [
            "```diff\nCommit: e43432128b3be8e4e94a82c3c6cb6a24ca44863d\nDate: 2025-05-12T22:46:43Z\nURL: https://github.com/jax-ml/jax/commit/e43432128b3be8e4e94a82c3c6cb6a24ca44863d\nFiles changed: 3\nAdditions: +157, Deletions: -9\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex b8ba61e7c914..d625e8bf4d6f 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -36,6 +36,7 @@ limitations under the License.\n #include \"llvm/ADT/APInt.h\"\n #include \"llvm/ADT/ArrayRef.h\"\n #include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/ADT/SmallVectorExtras.h\"\n #include \"llvm/ADT/StringMap.h\"\n #include \"llvm/ADT/iterator_range.h\"\n@@ -2141,16 +2142,41 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n   if (layout_in != layout) {\n     return op.emitOpError(\"Not implemented: unsupported layout for input\");\n   }\n-  if (layout_out != layout) {\n+  LayoutOffsets expected_offsets_out = layout_in.offsets();\n+  auto shift = getIntConst(amount, /*silent=*/true);\n+  const bool has_static_shift = succeeded(shift);\n+  int rotated_tiled_dim = op.getDimension() - (op.getType().getRank() - 2);\n+  bool has_padding_along_rotation =\n+      (rotated_tiled_dim == 0 || rotated_tiled_dim == 1) &&\n+      op.getType().getShape()[op.getDimension()] %\n+              layout.tiling()[rotated_tiled_dim] !=\n+          0;\n+  if (has_static_shift && has_padding_along_rotation) {\n+    // We checked above that there are no implicit dims.\n+    const int64_t dim_size = op.getType().getShape()[op.getDimension()];\n+    // TODO(b/337384645): Currently we assume {0, 0} offsets in the input\n+    // layout. Relax this assumption.\n+    expected_offsets_out[rotated_tiled_dim] =\n+        (dim_size - (shift.value() % dim_size)) %\n+        layout.tiling()[rotated_tiled_dim];\n+  }\n+  if (layout_out.bitwidth() != layout.bitwidth() ||\n+      layout_out.offsets() != expected_offsets_out ||\n+      layout_out.tiling() != layout.tiling() ||\n+      layout_out.implicit_dim() != layout.implicit_dim()) {\n     return op.emitOpError(\"Not implemented: unsupported layout for output\");\n   }\n   auto vty = op.getResult().getType();\n   if (vty.getRank() < 2) {\n     return op.emitOpError(\"Not implemented: unsupported 1D shape\");\n   }\n-  if (*(vty.getShape().end() - 2) % *(layout.tiling().end() - 2) != 0 ||\n-      *(vty.getShape().end() - 1) % *(layout.tiling().end() - 1) != 0) {\n-    return op.emitOpError(\"Not implemented: unsupported unaliged shape\");\n+  // TODO(b/411170715): Allow sublane rotation once the bug is fixed.\n+  // TODO(b/337384645): Support non-zero stride.\n+  if (has_padding_along_rotation &&\n+      (!has_static_shift ||\n+       (rotated_tiled_dim == 0 ||\n+        (rotated_tiled_dim == 1 && op.getStride().value_or(0) != 0)))) {\n+    return op.emitOpError(\"Not implemented: unsupported unaligned shape\");\n   }\n \n   ImplicitLocOpBuilder builder(op.getLoc(), op.getOperation());\n@@ -2277,6 +2303,88 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n     return concatenate(chunks, axis);\n   };\n \n+  // Applies lazy rotation (see go/pltpu-roll for details).\n+  auto lazyRotate = [&](const xla::Array<Value> &vregs, int64_t shift,\n+                        int axis) {\n+    const int tiling_dim = axis - (vregs.num_dimensions() - 2);\n+    const int64_t tile_size = ctx.target_shape[tiling_dim];\n+    const int64_t input_size = vty.getShape()[axis];\n+    const int64_t normalized_shift = shift % input_size;\n+    const int64_t start_idx = input_size - normalized_shift;\n+    const int64_t start_vreg_idx = start_idx / tile_size;\n+    const int64_t valid_amount = input_size % tile_size;\n+\n+    // We start with the following:\n+    //\n+    // vregs:\n+    // +------+ +------+ +------+\n+    // |░░░ 0 | |  1   | | 2 XXX|\n+    // +------+ +------+ +------+\n+    //\n+    // where XXX is the padding and ░░░ is the prefix of the same size as the\n+    // padding.\n+\n+    // After concatenation:\n+    //\n+    // concat:\n+    // +------+ +------+ +------+ +------+ +------+ +------+\n+    // |░░░ 0 | |  1   | | 2 XXX| |░░░ 0 | |  1   | | 2 XXX|\n+    // +------+ +------+ +------+ +------+ +------+ +------+\n+    auto concat = concatenate({vregs, vregs}, axis);\n+    auto chunks = split(concat, axis);\n+    int64_t original_num_chunks = chunks.size() / 2;\n+\n+    Value rotate_amount = mlirI32Const(valid_amount);\n+    SmallVector<Value, 2> low = {mlirIndexConst(0), mlirIndexConst(0)};\n+    low[tiling_dim] = mlirIndexConst(valid_amount);\n+    auto mask = builder.create<tpu::CreateMaskOp>(\n+        VectorType::get(ctx.target_shape, builder.getI1Type()), low,\n+        /*high=*/\n+        ArrayRef<Value>{mlirIndexConst(ctx.target_shape[0]),\n+                        mlirIndexConst(ctx.target_shape[1])});\n+    // overwrite padding in the last vreg with valid data from the first vreg,\n+    // yielding:\n+    //\n+    // +------+ +------+ +------+ +------+ +------+ +------+\n+    // |░░░ 0 | |  1   | | 2 XXX| |░░░ 0 | |  1   | | 2 ░░░|\n+    // +------+ +------+ +------+ +------+ +------+ +------+\n+    chunks.back().Each([&](absl::Span<const int64_t> idxs, Value *v) {\n+      *v = builder.create<arith::SelectOp>(\n+          mask,\n+          builder.create<tpu::DynamicRotateOp>(\n+              res_vreg_ty, chunks.front()(idxs), rotate_amount, tiling_dim,\n+              nullptr, nullptr),\n+          *v);\n+    });\n+    // rotate the vregs starting from the middle vreg and then blend the vregs\n+    // to overwrite the padding, yielding:\n+    //\n+    // +------+ +------+ +---+ +------+ +------+ +------+\n+    // |░░░ 0 | |  1   | | 2 | |░░░ 0 | |  1   | | 2 ░░░|\n+    // +------+ +------+ +---+ +------+ +------+ +------+\n+    for (int64_t i = original_num_chunks; i < chunks.size(); ++i) {\n+      chunks[i].Each([&](absl::Span<const int64_t> idxs, Value *v) {\n+        *v = builder.create<tpu::DynamicRotateOp>(\n+            res_vreg_ty, *v, rotate_amount, tiling_dim, nullptr, nullptr);\n+      });\n+    }\n+    for (int64_t i = original_num_chunks - 1; i < chunks.size() - 1; ++i) {\n+      chunks[i].Each([&](absl::Span<const int64_t> idxs, Value *v) {\n+        *v = builder.create<arith::SelectOp>(mask, chunks[i + 1](idxs), *v);\n+      });\n+    }\n+    SmallVector<int64_t> result_dimensions =\n+        layout_out.tileArrayImplicitShape(vty.getShape(), ctx.target_shape);\n+    // assemble the result\n+    xla::Array<Value> result(result_dimensions);\n+    SmallVector<int64_t> starts(result.num_dimensions(), 0);\n+    for (int64_t i = 0; i < result_dimensions[axis]; ++i) {\n+      starts[axis] = i;\n+      result.UpdateSlice(chunks[i + start_vreg_idx], starts);\n+    }\n+    return result;\n+  };\n+\n   std::function<xla::Array<Value>(const xla::Array<Value> &, Value, int, int)>\n       rotate;\n   rotate = [&](const xla::Array<Value> &vregs, Value shift, int axis,\n@@ -2290,6 +2398,9 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n     if (auto shift_cst = getIntConst(shift, /*silent=*/true);\n         succeeded(shift_cst)) {\n       int64_t static_shift = shift_cst.value();\n+      if (has_padding_along_rotation) {\n+        return lazyRotate(vregs, static_shift, axis);\n+      }\n       if (tiling_dim >= 0) {\n         shift = mlirI32Const(static_shift % ctx.target_shape[tiling_dim]);\n         static_shift /= ctx.target_shape[tiling_dim];\n@@ -2379,7 +2490,9 @@ LogicalResult rotate_rule_impl(RewriteContext &ctx, OpTy op, Value amount,\n     return result;\n   };\n \n-  xla::Array<Value> out_tiles(in_tiles.dimensions());\n+  SmallVector<int64_t> out_dimensions =\n+      layout_out.tileArrayImplicitShape(vty.getShape(), ctx.target_shape);\n+  xla::Array<Value> out_tiles(out_dimensions);\n   const auto dim = op.getDimension();\n   amount = modI(amount, vty.getDimSize(dim));\n \ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex 2e4c1c9c48a9..f42cfb139a37 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -757,9 +757,28 @@ class VectorLayoutInferer {\n     if (op.getType().getRank() < 2) {\n       NYI(\"Unsupported 1D shape\");\n     }\n+    // TODO(b/337384645): Currently we assume {0, 0} offsets in the input\n+    // layout. Relax this assumption.\n     auto layout = VectorLayout(bitwidth, {0, 0}, nativeTiling(bitwidth),\n                                ImplicitDim::kNone);\n-    setLayout(op, {layout, kNoLayout}, layout);\n+    // Calculate the offsets for the output layout.\n+    LayoutOffsets offsets_out = layout.offsets();\n+    // We assume there are no implicit dims.\n+    int tiling_dim = op.getDimension() - (op.getType().getRank() - 2);\n+    if (auto amount = op.getAmount().getDefiningOp<arith::ConstantOp>();\n+        amount && (tiling_dim == 0 || tiling_dim == 1)) {\n+      if (auto integer_attr = dyn_cast<IntegerAttr>(amount.getValue())) {\n+        const int64_t tile_size = layout.tiling()[tiling_dim];\n+        const int64_t dim_size = op.getType().getShape()[op.getDimension()];\n+        const int64_t shift = integer_attr.getValue().getSExtValue();\n+        if (dim_size % tile_size != 0) {\n+          offsets_out[tiling_dim] = (dim_size - (shift % dim_size)) % tile_size;\n+        }\n+      }\n+    }\n+    auto out_layout = VectorLayout(bitwidth, offsets_out,\n+                                   nativeTiling(bitwidth), ImplicitDim::kNone);\n+    setLayout(op, {layout, kNoLayout}, out_layout);\n     return success();\n   }\n \ndiff --git a/tests/pallas/tpu_pallas_test.py b/tests/pallas/tpu_pallas_test.py\nindex a70aa19bda4d..83f21bca7fc1 100644\n--- a/tests/pallas/tpu_pallas_test.py\n+++ b/tests/pallas/tpu_pallas_test.py\n@@ -2895,9 +2895,9 @@ def kernel(x_ref, out_ref):\n     )(x)\n     np.testing.assert_array_equal(out, state_utils.bitcast(x, jnp.uint32))\n \n-  @only_passes_in_interpret()\n-  def test_roll_partial(self):\n-    \"\"\"b/337384645\"\"\"\n+  def test_roll_partial_with_static_shift(self):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 15):\n+      self.skipTest('Needs a newer libtpu')\n     x = np.arange(8192, dtype=jnp.float32).reshape(128, 64)\n \n     def kernel(x_ref, out_ref):\n@@ -2908,6 +2908,22 @@ def kernel(x_ref, out_ref):\n     )(x)\n     np.testing.assert_array_equal(out, np.roll(x, 3, 1))\n \n+  def test_roll_partial_with_dynamic_shift(self):\n+    if not jtu.if_cloud_tpu_at_least(2025, 5, 15):\n+      self.skipTest('Needs a newer libtpu')\n+    if self.INTERPRET:\n+      self.skipTest('Test only applies to non-interpret mode.')\n+    x = np.arange(8192, dtype=jnp.float32).reshape(128, 64)\n+\n+    def kernel(x_ref, out_ref):\n+      amount = x_ref[0, 0].astype(jnp.int32)\n+      out_ref[...] = pltpu.roll(x_ref[...], amount, 1)\n+\n+    with self.assertRaisesRegex(Exception, 'unsupported unaligned shape'):\n+      _ = self.pallas_call(\n+          kernel, out_shape=jax.ShapeDtypeStruct((128, 64), jnp.float32)\n+      )(x)\n+\n   @only_passes_in_interpret()\n   def test_retiling1(self):\n     \"\"\"b/352626602\"\"\"\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[pallas:mosaic] Allowed registering lowering per `pltpu.KernelType`\n\nPiperOrigin-RevId: 757925207",
        "output": [
            "```diff\nCommit: bc3a3f0b24e3cb3c8c329053be12e3311b9ef2ff\nDate: 2025-05-12T22:00:25Z\nURL: https://github.com/jax-ml/jax/commit/bc3a3f0b24e3cb3c8c329053be12e3311b9ef2ff\nFiles changed: 3\nAdditions: +46, Deletions: -19\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 1ea5a048a17e..776ac1cb8143 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -15,7 +15,7 @@\n \"\"\"Module for lowering JAX to Mosaic-compatible MLIR dialects.\"\"\"\n from __future__ import annotations\n \n-from collections.abc import Callable, Sequence\n+from collections.abc import Callable, Collection, Sequence\n import contextlib\n import dataclasses\n import functools\n@@ -168,6 +168,7 @@ class LoweringContext:\n   block_shapes: list[tuple[int | pallas_core.Squeezed, ...]]\n   name_stack: source_info_util.NameStack\n   mesh_context: MeshContext | None\n+  kernel_type: tpu_core.KernelType\n   traceback_caches: mlir.TracebackCaches\n   for_verification: bool\n   forward_compatible: bool\n@@ -324,7 +325,7 @@ def ir_constant(x, mlir_type=None):\n   raise NotImplementedError(x.dtype)\n \n \n-lowering_rules = {}\n+lowering_rules = {kernel_type: {} for kernel_type in tpu_core.KernelType}\n skip_mlir_conversions = set()\n \n \n@@ -332,10 +333,14 @@ def ir_constant(x, mlir_type=None):\n \n \n def register_lowering_rule(\n-    prim: jax_core.Primitive, *, ensure_mlir_values: bool = True\n+    prim: jax_core.Primitive,\n+    *,\n+    kernel_types: Collection[tpu_core.KernelType] = (tpu_core.KernelType.TC,),\n+    ensure_mlir_values: bool = True,\n ) -> Callable[[T], T]:\n   def decorator(rule: T) -> T:\n-    lowering_rules[prim] = rule\n+    for kernel_type in kernel_types:\n+      lowering_rules[kernel_type][prim] = rule\n     if not ensure_mlir_values:\n       skip_mlir_conversions.add(prim)\n     return rule\n@@ -673,6 +678,7 @@ def lower_jaxpr_to_module(\n     jaxpr: jax_core.Jaxpr,\n     *,\n     dimension_semantics: Sequence[tpu_core.DimensionSemantics] | None,\n+    kernel_type: tpu_core.KernelType,\n     mesh: mesh_lib.Mesh | None = None,\n     for_verification: bool = False,\n     dynamic_shape_replacement_enabled: bool = False,\n@@ -724,6 +730,7 @@ def dynamic_shape_replacement_fn(\n       jaxpr,\n       mosaic_grid_mapping=mosaic_grid_mapping,\n       name=\"main\",\n+      kernel_type=kernel_type,\n       for_verification=for_verification,\n       forward_compatible=lowering_context.is_forward_compat(),\n       dynamic_shape_replacement_fn=dynamic_shape_replacement_fn,\n@@ -759,6 +766,7 @@ def dynamic_shape_replacement_fn(\n           bm.block_aval,\n           name=func_name,\n           mosaic_grid_mapping=mosaic_grid_mapping,\n+          kernel_type=kernel_type,\n           for_verification=for_verification,\n           forward_compatible=lowering_context.is_forward_compat(),\n           dynamic_shape_replacement_fn=dynamic_shape_replacement_fn,\n@@ -906,8 +914,9 @@ def lower_jaxpr_to_transform_func(\n     *,\n     name: str,\n     mosaic_grid_mapping: MosaicGridMapping,\n+    kernel_type: tpu_core.KernelType,\n     for_verification: bool,\n-     forward_compatible: bool,\n+    forward_compatible: bool,\n     dynamic_shape_replacement_fn: (\n         Callable[[tuple[jax.DimSize, ...]], tuple[int, ...]] | None\n     ) = None,\n@@ -942,6 +951,7 @@ def body_func(*args):\n         arg_block_shapes,\n         source_info_util.NameStack(),\n         mesh_context=mesh_context,\n+        kernel_type=kernel_type,\n         traceback_caches=mlir.TracebackCaches(),\n         for_verification=for_verification,\n         forward_compatible=forward_compatible,\n@@ -966,11 +976,19 @@ def body_func(*args):\n   return body.func_op\n \n \n+lower_jaxpr_to_func_fns = {}\n+\n+\n+def register_jaxpr_to_func(kernel_type: tpu_core.KernelType):\n+  lower_jaxpr_to_func_fns[kernel_type] = lower_jaxpr_to_func\n+\n+\n def lower_jaxpr_to_func(\n     jaxpr: jax_core.Jaxpr,\n     *,\n     mosaic_grid_mapping: MosaicGridMapping,\n     name: str,\n+    kernel_type: tpu_core.KernelType,\n     for_verification: bool,\n     forward_compatible: bool,\n     dynamic_shape_replacement_fn: (\n@@ -1012,6 +1030,7 @@ def body_func(*args):\n         arg_block_shapes,\n         source_info_util.NameStack(),\n         mesh_context=mesh_context,\n+        kernel_type=kernel_type,\n         traceback_caches=mlir.TracebackCaches(),\n         for_verification=for_verification,\n         forward_compatible=forward_compatible,\n@@ -1119,7 +1138,7 @@ def write_env(var: jax_core.Var, val):\n     loc = mlir._source_info_to_location(ctx, eqn.primitive, source_info)\n     with (source_info_util.user_context(eqn.source_info.traceback), loc,\n           eqn.ctx.manager):\n-      if eqn.primitive in lowering_rules:\n+      if eqn.primitive in lowering_rules[ctx.kernel_type]:\n         if eqn.primitive not in skip_mlir_conversions:\n           invals = [_ensure_mlir_value(x, v.aval)\n                     for x, v in zip(invals, eqn.invars)]\n@@ -1142,7 +1161,7 @@ def write_env(var: jax_core.Var, val):\n           tpu.trace_start(message=name, level=10)\n \n         try:\n-          ans = lowering_rules[eqn.primitive](\n+          ans = lowering_rules[ctx.kernel_type][eqn.primitive](\n               rule_context, *invals, **eqn.params\n           )\n         except LoweringException:\n@@ -1162,9 +1181,10 @@ def write_env(var: jax_core.Var, val):\n           raise new_error from e\n       else:\n         raise NotImplementedError(\n-            \"Unimplemented primitive in Pallas TPU lowering: \"\n-            f\"{eqn.primitive.name}. \"\n-            \"Please file an issue on https://github.com/jax-ml/jax/issues.\")\n+            \"Unimplemented primitive in Pallas TPU lowering for\"\n+            f\" {ctx.kernel_type}: {eqn.primitive.name}. Please file an issue on\"\n+            \" https://github.com/jax-ml/jax/issues.\"\n+        )\n       if eqn.primitive.multiple_results:\n         foreach(write_env, eqn.outvars, ans)\n       else:\n@@ -1889,7 +1909,9 @@ def _broadcast_to_lowering_rule(\n   )\n \n \n-@register_lowering_rule(lax.broadcast_in_dim_p)\n+@register_lowering_rule(\n+    lax.broadcast_in_dim_p, kernel_types=[*tpu_core.KernelType]\n+)\n def _broadcast_in_dim_lowering_rule(\n     ctx: LoweringRuleContext, val, *, shape, broadcast_dimensions, sharding\n ):\n@@ -2139,7 +2161,9 @@ def _convert_helper(x, *, to_dtype):\n   raise NotImplementedError(f\"Unsupported cast: {from_dtype} -> {to_dtype}\")\n \n \n-@register_lowering_rule(lax.convert_element_type_p)\n+@register_lowering_rule(\n+    lax.convert_element_type_p, kernel_types=[*tpu_core.KernelType]\n+)\n def _convert_element_type_lowering_rule(\n     ctx: LoweringRuleContext, x, *, new_dtype, weak_type, sharding\n ):\n@@ -2397,7 +2421,9 @@ def _bcast(x, y, x_aval, y_aval, out_aval):\n   return x, y\n \n \n-@register_lowering_rule(lax.add_p, ensure_mlir_values=False)\n+@register_lowering_rule(\n+    lax.add_p, kernel_types=[*tpu_core.KernelType], ensure_mlir_values=False\n+)\n @register_lowering_rule(ad_util.add_any_p, ensure_mlir_values=False)\n def _add_lowering_rule(ctx: LoweringRuleContext, x, y):\n   x, y = _bcast(x, y, ctx.avals_in[0], ctx.avals_in[1], ctx.avals_out[0])\n@@ -2806,7 +2832,9 @@ def _cmp_lowering_rule(primitive, ctx: LoweringRuleContext, x, y):\n \n \n for prim in [lax.eq_p, lax.ne_p, lax.lt_p, lax.le_p, lax.gt_p, lax.ge_p]:\n-  register_lowering_rule(prim)(functools.partial(_cmp_lowering_rule, prim))\n+  register_lowering_rule(prim, kernel_types=[*tpu_core.KernelType])(\n+      functools.partial(_cmp_lowering_rule, prim)\n+  )\n \n \n @register_lowering_rule(lax.and_p, ensure_mlir_values=False)\n@@ -3530,7 +3558,7 @@ def _dma_wait_lowering_rule(ctx: LoweringRuleContext, *args, tree,\n   return []\n \n \n-@register_lowering_rule(lax.axis_index_p)\n+@register_lowering_rule(lax.axis_index_p, kernel_types=[*tpu_core.KernelType])\n def _axis_index_rule(ctx: LoweringRuleContext, *, axis_name: Hashable):\n   grid_names = ctx.lowering_context.grid_names\n   if grid_names and axis_name in grid_names:\ndiff --git a/jax/_src/pallas/mosaic/pallas_call_registration.py b/jax/_src/pallas/mosaic/pallas_call_registration.py\nindex 5de917d077ce..74253e809a35 100644\n--- a/jax/_src/pallas/mosaic/pallas_call_registration.py\n+++ b/jax/_src/pallas/mosaic/pallas_call_registration.py\n@@ -150,6 +150,7 @@ def lower_module(for_verification: bool):\n           grid_mapping,\n           jaxpr,\n           dimension_semantics=mosaic_params.dimension_semantics,\n+          kernel_type=mosaic_params.kernel_type,\n           mesh=jax_mesh,\n           for_verification=for_verification,\n           dynamic_shape_replacement_enabled=pallas_core.dynamic_shapes_export_enabled(),\ndiff --git a/jax/_src/pallas/mosaic/verification.py b/jax/_src/pallas/mosaic/verification.py\nindex 08ff58770804..f45f36a473e9 100644\n--- a/jax/_src/pallas/mosaic/verification.py\n+++ b/jax/_src/pallas/mosaic/verification.py\n@@ -596,11 +596,10 @@ def _assume_abstract_eval(x, y):\n   assert jax_core.typematch(x, y)\n   return x\n \n+@lowering.register_lowering_rule(assume_p)\n def _assume_lowering(ctx: lowering.LoweringRuleContext, x, y):\n   return y if ctx.lowering_context.for_verification else x\n \n-lowering.lowering_rules[assume_p] = _assume_lowering  # type: ignore\n-\n def assume(normally, *, when_verifying):\n   return assume_p.bind(normally, when_verifying)\n \n@@ -613,6 +612,7 @@ def _pretend_abstract_eval(*_, **params):\n   del params  # Unused.\n   return ()\n \n+@lowering.register_lowering_rule(pretend_p)\n def _pretend_lowering(ctx: lowering.LoweringRuleContext, *flat_args, tree):\n   if ctx.lowering_context.for_verification:\n     (base_read_refs, transforms) = tree_util.tree_unflatten(tree, flat_args)\n@@ -631,8 +631,6 @@ def _pretend_lowering(ctx: lowering.LoweringRuleContext, *flat_args, tree):\n     ir.Operation.create(\"verification.pretend\", operands=read_refs)\n   return ()\n \n-lowering.lowering_rules[pretend_p] = _pretend_lowering  # type: ignore\n-\n def pretend(read_refs):\n   refs, transforms = unzip2(\n       primitives._get_ref_and_transforms(r) for r in read_refs\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Do all mesh checks at shard_map **call time** instead of at construction time.\n\n**Why do this change?**\n\n* If a `smap`/`shard_map` is constructed NOT under a mesh context but called under a mesh context, we will error at construction time. After this change, we won't.\n\n* If a `smap`/`shard_map` is nested but constructed at the top level, it will be bound with the mesh available at construction time instead of at call time. This is not ideal since while nesting one axis at a time, manualness of a mesh changes for the nested shard_map call. So we need to look at the mesh at call time. For example:\n\n```\n@jtu.with_explicit_mesh((2, 2), ('x', 'y'),\n                        axis_types=(AxisType.Explicit, AxisType.Auto))\ndef test_smap_auto_explicit_nest_mesh_call_time(self, mesh):\n  @partial(smap, in_axes=1, out_axes=1, axis_name='x')\n  def g(b):\n    return jnp.sin(b)\n\n  @partial(smap, in_axes=0, out_axes=0, axis_name='y')\n  def f(a):\n    self.assertEqual(a.aval.vma, {'y'})\n    b = a * 2\n    return g(b)\n\n  arr = jax.device_put(np.arange(16).reshape(8, 2), P('y'))\n  jax.jit(f)(arr)  # doesn't crash\n```\n\nIn the above example, before this change, `g` would be bound with the mesh whose axis_types were `Explict, Auto` but since `g` is being used inside a `smap` i.e. it's nested, it needs to be bound with the mesh at call time which would have axis_types `Explicit, Manual` for the computation to be correct.\n\nOne minor point regarding this change is that since `axis_name` or `in_specs/out_specs` refer to mesh axis names, the shard_map would need to be called with the correct mesh. Before this errored out at construction time but now it'll error out at call time.\n\nPiperOrigin-RevId: 757912835",
        "output": [
            "```diff\nCommit: 6fc9e17a9e7d46dad326bd595ad77218fa5389e5\nDate: 2025-05-12T21:26:47Z\nURL: https://github.com/jax-ml/jax/commit/6fc9e17a9e7d46dad326bd595ad77218fa5389e5\nFiles changed: 2\nAdditions: +76, Deletions: -59\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex e9f2d3b6072e..b772a3de239e 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -164,82 +164,30 @@ def smap(f, /, *, in_axes=Infer, out_axes, axis_name: AxisName):\n   if not all(isinstance(l, int) for l in tree_leaves(out_axes)):\n     raise TypeError(\"smap out_axes must be an int, None, or (nested) container \"\n                     f\"with those types as leaves, but got {out_axes}.\")\n-  mesh = get_abstract_mesh()\n-  if mesh.empty:\n-    raise ValueError(\n-        \"The context mesh cannot be empty. Use\"\n-        \" `jax.sharding.use_mesh(mesh)` to enter into a mesh context.\")\n-  if mesh._name_to_type[axis_name] != AxisType.Explicit and in_axes is Infer:\n-    raise TypeError(\n-        f\"in_axes was not specified when {axis_name=} was of type\"\n-        f\" {mesh._name_to_type[axis_name]}.\")\n \n   in_specs = (None if in_axes is Infer else\n               tree_map(partial(_axes_to_pspec, axis_name), in_axes,\n                        is_leaf=lambda x: x is None))\n   out_specs = tree_map(partial(_axes_to_pspec, axis_name), out_axes,\n                        is_leaf=lambda x: x is None)\n-  return shard_map(f, axis_names={axis_name}, in_specs=in_specs,\n-                   out_specs=out_specs)\n+  return _shard_map(f, mesh=None, in_specs=in_specs, out_specs=out_specs,\n+                    axis_names={axis_name}, check_vma=True, _smap=True)\n \n \n def _shard_map(f: Callable, *, mesh: Mesh | AbstractMesh | None,\n                in_specs: Specs, out_specs: Specs | Callable[[], Specs],\n                axis_names: Set[AxisName], check_vma: bool,\n-               _skip_mesh_check: bool = False) -> Callable:\n+               _skip_mesh_check: bool = False, _smap: bool = False) -> Callable:\n   if not callable(f):\n     raise TypeError(\"shard_map requires a callable for its first argument, \"\n                     f\"but got {f} of type {type(f)}.\")\n \n-  if mesh is None:\n-    mesh = get_abstract_mesh()\n-    if mesh.empty:\n-      raise ValueError(\n-          \"The context mesh cannot be empty. Either use\"\n-          \" `jax.sharding.use_mesh(mesh)` to enter into a mesh context or pass\"\n-          \" a mesh to `shard_map` via the `mesh` keyword argument.\")\n-  else:\n-    ctx_mesh = get_abstract_mesh()\n-    if (not _skip_mesh_check and not ctx_mesh.empty and\n-        mesh.abstract_mesh != ctx_mesh):\n-      raise ValueError(\n-          f\"The context mesh {ctx_mesh} should match the mesh passed to\"\n-          f\" shard_map {mesh}\")\n-\n-  if not isinstance(mesh, (Mesh, AbstractMesh)):\n-    raise TypeError(\"shard_map requires a `jax.sharding.Mesh` or a \"\n-                    \"`jax.sharding.AbstractMesh` instance for its \"\n-                    f\"second argument, but got {mesh} of type {type(mesh)}.\")\n-\n-  if not isinstance(axis_names, (frozenset, set)):\n-    raise TypeError(\n-        \"`axis_names` argument of shard_map should be of type `frozenset` or\"\n-        f\" `set`. Got type: {type(axis_names)}\")\n-  if isinstance(axis_names, set):\n-    axis_names = frozenset(axis_names)\n-  if not axis_names:\n-    axis_names = frozenset(mesh.axis_names)\n-  if not axis_names.issubset(mesh.axis_names):\n-    raise ValueError(\n-        f\"jax.shard_map requires axis_names={axis_names} to be a subset of \"\n-        f\"mesh.axis_names={mesh.axis_names}\")\n-\n-  if (in_specs is None and\n-      not all(mesh._name_to_type[a] == AxisType.Explicit for a in axis_names)):\n-    raise TypeError(\n-        \"shard_map in_specs argument must be a pytree of\"\n-        \" `jax.sharding.PartitionSpec` instances, but it was `None` when\"\n-        f\" {axis_names=} are of type\"\n-        f\" {', '.join(str(mesh._name_to_type[a]) for a in axis_names)}\")\n-\n-  if in_specs is not None:\n-    _check_specs(SpecErrorType.input, in_specs, axis_names)\n-  if not callable(out_specs):\n-    _check_specs(SpecErrorType.out, out_specs, axis_names)\n-\n   @util.wraps(f)\n   @traceback_util.api_boundary\n   def wrapped(*args):\n+    nonlocal mesh, axis_names\n+    mesh, axis_names = _shmap_checks(mesh, axis_names, in_specs, out_specs,\n+                                     _skip_mesh_check, _smap)\n     fun = lu.wrap_init(\n         f, debug_info=api_util.debug_info(\"shard_map\", f, args, {}))\n     args_flat, in_tree = tree_flatten(args)\n@@ -305,6 +253,59 @@ def out_names_thunk():\n   return wrapped\n \n \n+def _shmap_checks(mesh, axis_names, in_specs, out_specs, _skip_mesh_check,\n+                  _smap):\n+  if mesh is None:\n+    mesh = get_abstract_mesh()\n+    if mesh.empty:\n+      raise ValueError(\n+          \"The context mesh cannot be empty. Use\"\n+          \" `jax.sharding.use_mesh(mesh)` to enter into a mesh context\")\n+  else:\n+    ctx_mesh = get_abstract_mesh()\n+    if (not _skip_mesh_check and not ctx_mesh.empty and\n+        mesh.abstract_mesh != ctx_mesh):\n+      raise ValueError(\n+          f\"The context mesh {ctx_mesh} should match the mesh passed to\"\n+          f\" shard_map {mesh}\")\n+\n+  if not isinstance(mesh, (Mesh, AbstractMesh)):\n+    raise TypeError(\"shard_map requires a `jax.sharding.Mesh` or a \"\n+                    \"`jax.sharding.AbstractMesh` instance for its \"\n+                    f\"second argument, but got {mesh} of type {type(mesh)}.\")\n+\n+  if not isinstance(axis_names, (frozenset, set)):\n+    raise TypeError(\n+        \"`axis_names` argument of shard_map should be of type `frozenset` or\"\n+        f\" `set`. Got type: {type(axis_names)}\")\n+  if isinstance(axis_names, set):\n+    axis_names = frozenset(axis_names)\n+  if not axis_names:\n+    axis_names = frozenset(mesh.axis_names)\n+  if not axis_names.issubset(mesh.axis_names):\n+    raise ValueError(\n+        f\"jax.shard_map requires axis_names={axis_names} to be a subset of \"\n+        f\"mesh.axis_names={mesh.axis_names}\")\n+\n+  if (in_specs is None and\n+      not all(mesh._name_to_type[a] == AxisType.Explicit for a in axis_names)):\n+    axis_types = ', '.join(str(mesh._name_to_type[a]) for a in axis_names)\n+    if _smap:\n+      msg = (f\"in_axes was not specified when axis_name={axis_names} was of\"\n+             f\" type {axis_types}\")\n+    else:\n+      msg = (\"shard_map in_specs argument must be a pytree of\"\n+             \" `jax.sharding.PartitionSpec` instances, but it was `None` when\"\n+             f\" {axis_names=} are of type {axis_types}\")\n+    raise TypeError(msg)\n+\n+  if in_specs is not None:\n+    _check_specs(SpecErrorType.input, in_specs, axis_names)\n+  if not callable(out_specs):\n+    _check_specs(SpecErrorType.out, out_specs, axis_names)\n+  return mesh, axis_names\n+\n+\n # Internally use AxisNames = dict[int, tuple[AxisName, ...]], not PartitionSpecs\n AxisNames = dict[int, tuple[AxisName, ...]]  # TODO(mattjj): make it hashable\n def _canonicalize_spec(spec: PartitionSpec) -> AxisNames:\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 1abef7b06323..00d437aadb08 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -3218,7 +3218,7 @@ def g(x, y):\n   @jtu.with_explicit_mesh((2,), ('x',), axis_types=(AxisType.Auto,))\n   def test_smap_auto_error(self, mesh):\n     with self.assertRaisesRegex(TypeError, \"in_axes was not specified\"):\n-      smap(lambda x: x * 2, out_axes=0, axis_name='x')\n+      smap(lambda x: x * 2, out_axes=0, axis_name='x')(np.arange(4))\n \n   @jtu.with_explicit_mesh((2, 2), ('x', 'y'),\n                           axis_types=(AxisType.Explicit, AxisType.Auto))\n@@ -3273,6 +3273,22 @@ def f(a):\n     arr = jax.device_put(np.arange(16).reshape(8, 2), P('y'))\n     jax.jit(smap(f, in_axes=0, out_axes=0, axis_name='y'))(arr)  # doesn't crash\n \n+  @jtu.with_explicit_mesh((2, 2), ('x', 'y'),\n+                          axis_types=(AxisType.Explicit, AxisType.Auto))\n+  def test_smap_auto_explicit_nest_mesh_call_time(self, mesh):\n+    @partial(smap, in_axes=1, out_axes=1, axis_name='x')\n+    def g(b):\n+      return jnp.sin(b)\n+\n+    @partial(smap, in_axes=0, out_axes=0, axis_name='y')\n+    def f(a):\n+      self.assertEqual(a.aval.vma, {'y'})\n+      b = a * 2\n+      return g(b)\n+\n+    arr = jax.device_put(np.arange(16).reshape(8, 2), P('y'))\n+    jax.jit(f)(arr)  # doesn't crash\n+\n \n class FunSpec(NamedTuple):\n   name: str\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Default `in_axes` of `smap` to `Infer`. This matches the behavior of `jax.shard_map` where `in_specs` is optional. If the `axis_name` `smap` is going manual over is not of type `Explicit`, we error out and providing `in_axes` is compulsory.\n\nThis also allows us to not expose `Infer` as a public API!\n\nAdded some more tests and fixed some bugs too.\n\nPiperOrigin-RevId: 757905314",
        "output": [
            "```diff\nCommit: 1a3d3e37b2a71a1ebe9c0bdc3ffb4d95ac4e0bd5\nDate: 2025-05-12T21:09:03Z\nURL: https://github.com/jax-ml/jax/commit/1a3d3e37b2a71a1ebe9c0bdc3ffb4d95ac4e0bd5\nFiles changed: 2\nAdditions: +80, Deletions: -12\ndiff --git a/jax/_src/shard_map.py b/jax/_src/shard_map.py\nindex 939dbeddf3d7..e9f2d3b6072e 100644\n--- a/jax/_src/shard_map.py\n+++ b/jax/_src/shard_map.py\n@@ -146,7 +146,7 @@ def _get_default_infer():\n \n # TODO(yashkatariya): We need a singleton which users can provide to `in_axes`\n # to tell smap to infer in_specs from args when mesh is fully explicit.\n-def smap(f, in_axes, out_axes, axis_name: AxisName):\n+def smap(f, /, *, in_axes=Infer, out_axes, axis_name: AxisName):\n   if isinstance(axis_name, (list, tuple)):\n     raise TypeError(\n         f\"smap axis_name should be a `str` or a `Hashable`, but got {axis_name}\")\n@@ -164,6 +164,15 @@ def smap(f, in_axes, out_axes, axis_name: AxisName):\n   if not all(isinstance(l, int) for l in tree_leaves(out_axes)):\n     raise TypeError(\"smap out_axes must be an int, None, or (nested) container \"\n                     f\"with those types as leaves, but got {out_axes}.\")\n+  mesh = get_abstract_mesh()\n+  if mesh.empty:\n+    raise ValueError(\n+        \"The context mesh cannot be empty. Use\"\n+        \" `jax.sharding.use_mesh(mesh)` to enter into a mesh context.\")\n+  if mesh._name_to_type[axis_name] != AxisType.Explicit and in_axes is Infer:\n+    raise TypeError(\n+        f\"in_axes was not specified when {axis_name=} was of type\"\n+        f\" {mesh._name_to_type[axis_name]}.\")\n \n   in_specs = (None if in_axes is Infer else\n               tree_map(partial(_axes_to_pspec, axis_name), in_axes,\n@@ -215,12 +224,13 @@ def _shard_map(f: Callable, *, mesh: Mesh | AbstractMesh | None,\n         f\"jax.shard_map requires axis_names={axis_names} to be a subset of \"\n         f\"mesh.axis_names={mesh.axis_names}\")\n \n-  # TODO(yashkatariya): Maybe we don't have to be this strict?\n-  if mesh._any_axis_auto_or_manual and in_specs is None:\n+  if (in_specs is None and\n+      not all(mesh._name_to_type[a] == AxisType.Explicit for a in axis_names)):\n     raise TypeError(\n         \"shard_map in_specs argument must be a pytree of\"\n-        \" `jax.sharding.PartitionSpec` instances, but it was None when mesh\"\n-        f\" has `Auto` axes {mesh}\")\n+        \" `jax.sharding.PartitionSpec` instances, but it was `None` when\"\n+        f\" {axis_names=} are of type\"\n+        f\" {', '.join(str(mesh._name_to_type[a]) for a in axis_names)}\")\n \n   if in_specs is not None:\n     _check_specs(SpecErrorType.input, in_specs, axis_names)\n@@ -242,9 +252,8 @@ def wrapped(*args):\n       e, *_ = prefix_errors(in_specs, args)\n       raise e('shard_map in_specs') from None\n \n-    # TODO(yashkatariya): Relax this and convert only `None`s in `in_specs_flat`\n-    # and accept the other specs as is.\n-    if mesh._are_all_axes_explicit and in_specs is None:\n+    if (in_specs is None and\n+        all(mesh._name_to_type[a] == AxisType.Explicit for a in axis_names)):\n       arg_s = [typeof(a).sharding for a in args_flat]\n       assert all(i is None for i in in_specs_flat), in_specs_flat\n       in_specs_flat = [_manual_spec(axis_names, s.spec) for s in arg_s]\n@@ -597,7 +606,8 @@ def _as_manual_mesh(mesh, manual_axes: frozenset):\n     if cur_mesh._name_to_type[a] == AxisType.Auto:\n       auto_axes.add(a)\n     else:\n-      assert cur_mesh._name_to_type[a] == AxisType.Explicit, cur_mesh._name_to_type[a]\n+      assert cur_mesh._name_to_type[a] == AxisType.Explicit, (\n+          a, cur_mesh._name_to_type[a])\n       explicit_axes.add(a)\n \n   new_axis_types = []\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 4d3b265bd869..1abef7b06323 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -36,7 +36,7 @@\n from jax._src import config\n from jax._src import core\n from jax._src import prng\n-from jax._src.shard_map import shard_map, smap, Infer\n+from jax._src.shard_map import shard_map, smap\n from jax._src import test_util as jtu\n from jax._src.lib.mlir.dialects import sdy\n from jax._src.util import safe_zip, safe_map, partition_list, merge_lists\n@@ -971,7 +971,7 @@ def test_in_specs_none_error(self):\n \n     def f(x): return x\n \n-    with self.assertRaisesRegex(TypeError, \"but it was None\"):\n+    with self.assertRaisesRegex(TypeError, \"but it was `None`\"):\n       shard_map(f, mesh=mesh, in_specs=None, out_specs=P())(3.)\n \n     # TODO(mattjj): enable this test once we fix the tree_map(f, None, 3.0) bug\n@@ -3182,7 +3182,7 @@ def h(x):\n \n     @jax.jit\n     def f(x):\n-      return smap(h, in_axes=Infer, out_axes=0, axis_name='x')(x)\n+      return smap(h, out_axes=0, axis_name='x')(x)\n \n     out = f(arr)\n     self.assertArraysEqual(out, np_inp * np_inp)\n@@ -3215,6 +3215,64 @@ def g(x, y):\n     out = g(np.arange(4), np.arange(8))\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('data')))\n \n+  @jtu.with_explicit_mesh((2,), ('x',), axis_types=(AxisType.Auto,))\n+  def test_smap_auto_error(self, mesh):\n+    with self.assertRaisesRegex(TypeError, \"in_axes was not specified\"):\n+      smap(lambda x: x * 2, out_axes=0, axis_name='x')\n+\n+  @jtu.with_explicit_mesh((2, 2), ('x', 'y'),\n+                          axis_types=(AxisType.Explicit, AxisType.Auto))\n+  def test_smap_auto_explicit(self, mesh):\n+    def f(x):\n+      self.assertEqual(x.aval.vma, {'x'})\n+      return x * 2\n+\n+    arr = jax.device_put(np.arange(4), P('x'))\n+    out = jax.jit(smap(f, out_axes=0, axis_name='x'))(arr)\n+    self.assertArraysEqual(out, np.arange(4) * 2)\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P('x')))\n+\n+    def g(x):\n+      self.assertEqual(x.aval.vma, {'y'})\n+      return x * 2\n+\n+    arr = jax.device_put(np.arange(4), P('y'))\n+    out = jax.jit(smap(g, in_axes=0, out_axes=0, axis_name='y'))(arr)\n+    self.assertArraysEqual(out, np.arange(4) * 2)\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P('y')))\n+\n+  @jtu.with_explicit_mesh((2, 2), ('x', 'y'),\n+                          axis_types=(AxisType.Explicit, AxisType.Auto))\n+  def test_smap_auto_explicit_nest(self, mesh):\n+    def g(b):\n+      self.assertEqual(b.aval.vma, {'x', 'y'})\n+      return jnp.sin(b)\n+\n+    def f(a):\n+      self.assertEqual(a.aval.vma, {'y'})\n+      b = a * 2\n+      return smap(g, in_axes=1, out_axes=1, axis_name='x')(b)\n+\n+    arr = jax.device_put(np.arange(16).reshape(8, 2), P('y'))\n+    jax.jit(smap(f, in_axes=0, out_axes=0, axis_name='y'))(arr)  # doesn't crash\n+\n+  @jtu.with_explicit_mesh((2, 2), ('x', 'y'),\n+                          axis_types=(AxisType.Explicit, AxisType.Auto))\n+  def test_smap_auto_explicit_nest_inner_none(self, mesh):\n+    def g(b):\n+      self.assertEqual(b.aval.vma, {'y'})\n+      return jnp.sin(b)\n+\n+    def f(a):\n+      self.assertEqual(a.aval.vma, {'y'})\n+      b = a * 2\n+      # Going manual over explicit axis `x` but in_axes is Infer and since\n+      # input has no sharding, it will default to None.\n+      return smap(g, out_axes=1, axis_name='x')(b)\n+\n+    arr = jax.device_put(np.arange(16).reshape(8, 2), P('y'))\n+    jax.jit(smap(f, in_axes=0, out_axes=0, axis_name='y'))(arr)  # doesn't crash\n+\n \n class FunSpec(NamedTuple):\n   name: str\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Move the existing mask handling code to the relayout fn, invoke it from the existing tpu relayout rule.\n\nPiperOrigin-RevId: 757902288",
        "output": [
            "```diff\nCommit: 167d6bc765d05f2f49c9aedf1e1c94500d6eefde\nDate: 2025-05-12T21:01:18Z\nURL: https://github.com/jax-ml/jax/commit/167d6bc765d05f2f49c9aedf1e1c94500d6eefde\nFiles changed: 5\nAdditions: +219, Deletions: -128\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu.td b/jaxlib/mosaic/dialect/tpu/tpu.td\nindex fb72b6948d9d..226fc6285192 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu.td\n+++ b/jaxlib/mosaic/dialect/tpu/tpu.td\n@@ -441,6 +441,7 @@ def TPU_RelayoutOp : TPU_Op<\"relayout\", [SameOperandsAndResultType]> {\n   let arguments = (ins AnyType:$input);\n   let results = (outs AnyType:$output);\n   let assemblyFormat = [{ $input attr-dict `:` type($input) `->` type($output) }];\n+  let hasVerifier = 1;\n }\n \n def TPU_PackMaskOp : TPU_Op<\"pack_vmsk\", [Pure, SameTypeOperands]> {\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\nindex 934088e91506..b5e68bf08370 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n+++ b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n@@ -343,6 +343,54 @@ LogicalResult MemRefSqueezeOp::canonicalize(MemRefSqueezeOp op,\n   return success();\n }\n \n+LogicalResult RelayoutOp::verify() {\n+  auto in_layout_array_attr =\n+      getOperation()->getAttrOfType<ArrayAttr>(\"in_layout\");\n+  if (!in_layout_array_attr || in_layout_array_attr.empty()) {\n+    return emitOpError(\"missing or empty 'in_layout' attribute\");\n+  }\n+  if (in_layout_array_attr.size() != 1) {\n+    return emitOpError(\n+        \"'in_layout' attribute must be an array containing a single \"\n+        \"VectorLayoutAttr\");\n+  }\n+  auto src_vla = dyn_cast<tpu::VectorLayoutAttr>(in_layout_array_attr[0]);\n+  if (!src_vla) {\n+    return emitOpError(\"'in_layout' attribute is not a VectorLayoutAttr\");\n+  }\n+\n+  auto out_layout_array_attr =\n+      getOperation()->getAttrOfType<ArrayAttr>(\"out_layout\");\n+  if (!out_layout_array_attr || out_layout_array_attr.empty()) {\n+    return emitOpError(\"missing or empty 'out_layout' attribute\");\n+  }\n+  if (out_layout_array_attr.size() != 1) {\n+    return emitOpError(\n+        \"'out_layout' attribute must be an array containing a single \"\n+        \"VectorLayoutAttr\");\n+  }\n+  auto dst_vla = dyn_cast<tpu::VectorLayoutAttr>(out_layout_array_attr[0]);\n+  if (!dst_vla) {\n+    return emitOpError(\"'out_layout' attribute is not a VectorLayoutAttr\");\n+  }\n+\n+  VectorType input_type = cast<VectorType>(getInput().getType());\n+  VectorType output_type = cast<VectorType>(getOutput().getType());\n+\n+  if (input_type.getShape() != output_type.getShape()) {\n+    return emitOpError(\"input and output shapes must match\");\n+  }\n+  if (input_type.getElementType() != output_type.getElementType()) {\n+    // Allow i1 to i1 even if bitwidth in layout changes.\n+    if (!(input_type.getElementType().isInteger(1) &&\n+          output_type.getElementType().isInteger(1))) {\n+      return emitOpError(\n+          \"input and output element types must match for non-mask relayouts\");\n+    }\n+  }\n+  return success();\n+}\n+\n LogicalResult MemRefReshapeOp::verify() {\n   auto src_ty = getMemRefType(getInput());\n   auto tgt_ty = getType();\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex f8e18070e5e7..b8ba61e7c914 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -2101,74 +2101,6 @@ LogicalResult tpu_assume_layout_rule(RewriteContext &ctx, Operation &op,\n   return success();\n }\n \n-LogicalResult tpu_relayout_rule(RewriteContext &ctx, Operation &op,\n-                                const ArrayRef<Layout> layouts_in,\n-                                const ArrayRef<Layout> layouts_out) {\n-  TPU_ASSERT_EQ_OP(op.getNumOperands(), 1);\n-  TPU_ASSERT_EQ_OP(op.getNumResults(), 1);\n-  TPU_ASSERT_EQ_OP(layouts_in.size(), 1);\n-  TPU_ASSERT_EQ_OP(layouts_out.size(), 1);\n-  TPU_ASSERT_OP(layouts_in[0].has_value());\n-  TPU_ASSERT_OP(layouts_out[0].has_value());\n-  const auto& in_layout = *layouts_in[0];\n-  const auto& out_layout = *layouts_out[0];\n-  auto realyout_op = cast<tpu::RelayoutOp>(op);\n-  auto in_bitwidth = in_layout.bitwidth();\n-  auto out_bitwidth = out_layout.bitwidth();\n-  auto vty = cast<VectorType>(realyout_op.getType());\n-  ImplicitLocOpBuilder builder(op.getLoc(), &op);\n-  if (in_layout == out_layout) {\n-    realyout_op.replaceAllUsesWith(realyout_op.getInput());\n-    realyout_op.erase();\n-    return success();\n-  }\n-  FAILUREOR_ASSIGN_OR_RETURN(\n-      xla::Array<Value> vals,\n-      disassemble(builder, in_layout,\n-                  cast<TypedValue<VectorType>>(realyout_op.getInput()),\n-                  ctx.target_shape,\n-                  /*use_implicit_shape=*/true));\n-  // Packing vector masks from 32-bit to 16-bit.\n-  if (vty.getElementType() == builder.getI1Type() && in_bitwidth == 32 &&\n-      out_bitwidth == 16 &&\n-      in_layout.tiling()[0] == in_layout.packing() * ctx.target_shape[0] &&\n-      in_layout.tiling()[1] == ctx.target_shape[1] &&\n-      in_layout.tiling() == out_layout.tiling() &&\n-      in_layout.offsets() == out_layout.offsets() &&\n-      in_layout.implicit_dim() == out_layout.implicit_dim()) {\n-    std::vector<int64_t> vmsks_shape(vals.dimensions().begin(),\n-                                     vals.dimensions().end());\n-    *(vmsks_shape.end() - 1) = llvm::divideCeil(vmsks_shape.back(), 2);\n-    xla::Array<Value> out_vmsks(vmsks_shape, nullptr);\n-    SmallVector<int64_t> val_idx;\n-    Value default_val =\n-        getFullLikeVector(builder, cast<TypedValue<VectorType>>(*vals.begin()),\n-                          IntegerAttr::get(builder.getI1Type(), 0));\n-    out_vmsks.Each([&](absl::Span<const int64_t> idx, Value *v) {\n-      val_idx.assign(idx.begin(), idx.end());\n-      // TODO(jevinjiang): can be simplified when offset is replicated.\n-      *(val_idx.end() - 1) *= 2;\n-      Value low_part = *(val_idx.end() - 1) < *(vals.dimensions().end() - 1)\n-                           ? vals(val_idx)\n-                           : default_val;\n-      *(val_idx.end() - 1) += 1;\n-      Value high_part = *(val_idx.end() - 1) < *(vals.dimensions().end() - 1)\n-                            ? vals(val_idx)\n-                            : default_val;\n-      const VectorType mask_ty = getNativeVregOrVmaskType(\n-          builder.getI1Type(), in_bitwidth / 2, ctx.target_shape);\n-      *v = builder.create<PackMaskOp>(mask_ty, low_part, high_part);\n-    });\n-    const RollVectorsOp rolled_op =\n-        assemble(builder, vty, out_layout, out_vmsks, ctx.target_shape,\n-                 /*use_implicit_shape=*/true);\n-    op.replaceAllUsesWith(rolled_op);\n-    op.erase();\n-    return success();\n-  }\n-  return op.emitOpError(\"Not implemented: unsupported layout change\");\n-}\n-\n Value createSubelementMask(OpBuilder &builder, const Location loc,\n                            const int bitwidth, const int64_t from,\n                            const int64_t to,\n@@ -4827,60 +4759,6 @@ LogicalResult tpu_prng_random_bits_rule(RewriteContext &ctx, Operation &op,\n   return success();\n }\n \n-const llvm::StringMap<rule_type> &rules() {\n-  static const llvm::StringMap<rule_type> *rules = [] {\n-    static auto rules = new llvm::StringMap<rule_type>{\n-        {arith::ConstantOp::getOperationName(), arith_constant_rule},\n-        {arith::ExtFOp::getOperationName(), arith_extf_rule},\n-        {arith::ExtSIOp::getOperationName(), arith_extsi_rule},\n-        {arith::ExtUIOp::getOperationName(), arith_extui_rule},\n-        {arith::TruncFOp::getOperationName(), arith_truncf_rule},\n-        {arith::TruncIOp::getOperationName(), arith_trunci_rule},\n-        {func::ReturnOp::getOperationName(), func_return_rule},\n-        {scf::ForOp::getOperationName(), scf_for_rule},\n-        {scf::WhileOp::getOperationName(), scf_while_rule},\n-        {scf::ConditionOp::getOperationName(), scf_condition_rule},\n-        {scf::IfOp::getOperationName(), scf_if_rule},\n-        {scf::YieldOp::getOperationName(), yield_rule},\n-        {tpu::YieldOp::getOperationName(), yield_rule},\n-        {tpu::RotateOp::getOperationName(), tpu_rotate_rule},\n-        {tpu::DynamicRotateOp::getOperationName(), tpu_dynamic_rotate_rule},\n-        {tpu::ConcatenateOp::getOperationName(), tpu_concatenate_rule},\n-        {tpu::IotaOp::getOperationName(), tpu_iota_rule},\n-        {tpu::GatherOp::getOperationName(), tpu_gather_rule},\n-        {tpu::DynamicGatherOp::getOperationName(), tpu_dynamic_gather_rule},\n-        {tpu::LoadOp::getOperationName(), tpu_load_rule},\n-        {tpu::StoreOp::getOperationName(), tpu_store_rule},\n-        {tpu::StridedLoadOp::getOperationName(), tpu_strided_load_rule},\n-        {tpu::StridedStoreOp::getOperationName(), tpu_strided_store_rule},\n-        {tpu::VectorStoreOp::getOperationName(), tpu_vector_store_rule},\n-        {tpu::MatmulOp::getOperationName(), tpu_matmul_rule},\n-        {tpu::RegionOp::getOperationName(), tpu_region_rule},\n-        {tpu::BitcastOp::getOperationName(), tpu_bitcast_rule},\n-        {tpu::TraceOp::getOperationName(), tpu_trace_rule},\n-        {tpu::AssumeLayoutOp::getOperationName(), tpu_assume_layout_rule},\n-        {tpu::PRNGRandomBitsOp::getOperationName(), tpu_prng_random_bits_rule},\n-        {tpu::RelayoutOp::getOperationName(), tpu_relayout_rule},\n-        {tpu::FPToSIOp::getOperationName(), tpu_fptosi_rule},\n-        {vector::BroadcastOp::getOperationName(), vector_broadcast_rule},\n-        {vector::ExtractOp::getOperationName(), vector_extract_rule},\n-        {vector::LoadOp::getOperationName(), vector_load_rule},\n-        {vector::MultiDimReductionOp::getOperationName(),\n-         vector_multi_reduction_rule},\n-        {vector::ExtractStridedSliceOp::getOperationName(),\n-         vector_extract_strided_slice_rule},\n-        {vector::ShapeCastOp::getOperationName(), vector_shape_cast_rule},\n-        {vector::StoreOp::getOperationName(), vector_store_rule},\n-        {tpu::TransposeOp::getOperationName(), vector_transpose_rule}};\n-\n-    for (const auto &[name, rule] : mlir::tpu::extensions::rules()) {\n-      rules->insert({name, rule});\n-    }\n-    return rules;\n-  }();\n-  return *rules;\n-}\n-\n // Determines whether we should handle bank conflict for the given stride and\n // max_sublane_offset.\n //\n@@ -6773,12 +6651,20 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n                                            VectorLayout src,\n                                            VectorLayout dst) {\n   const auto target_shape = ctx.target_shape;\n+  VectorType vty = v.getType();\n   const int8_t bitwidth = src.bitwidth();\n-  if (bitwidth != dst.bitwidth()) {\n+  const bool is_mask = vty.getElementTypeBitWidth() == 1;\n+  const bool is_mask_pack =\n+      is_mask && bitwidth == 32 && dst.bitwidth() == 16 &&\n+      src.tiling()[0] == src.packing() * target_shape[0] &&\n+      src.tiling()[1] == target_shape[1] && src.tiling() == dst.tiling() &&\n+      src.offsets() == dst.offsets() &&\n+      src.implicit_dim() == dst.implicit_dim();\n+\n+  if (bitwidth != dst.bitwidth() && !is_mask_pack) {\n     return emitError(v.getLoc(), \"Can't change bitwidth during a relayout\");\n   }\n-  VectorType vty = v.getType();\n-  const bool is_mask = vty.getElementTypeBitWidth() == 1;\n+\n   {\n     // Replication imposes a replication constraint on the *logical* value of\n     // the vector: When moving along a replicated axis, all elements must be\n@@ -6812,6 +6698,38 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n   FAILUREOR_ASSIGN_OR_RETURN(\n       xla::Array<Value> src_tiles,\n       disassemble(builder, src, v, target_shape, /*use_implicit_shape=*/true));\n+  if (is_mask_pack) {\n+    std::vector<int64_t> vmsks_shape(src_tiles.dimensions().begin(),\n+                                     src_tiles.dimensions().end());\n+    *(vmsks_shape.end() - 1) = llvm::divideCeil(vmsks_shape.back(), 2);\n+    xla::Array<Value> out_vmsks(vmsks_shape, nullptr);\n+    SmallVector<int64_t> val_idx;\n+    Value default_val = getFullVector(\n+        builder, v.getLoc(),\n+        cast<TypedValue<VectorType>>(*src_tiles.begin()).getType(),\n+        IntegerAttr::get(builder.getI1Type(), 0));\n+    out_vmsks.Each([&](absl::Span<const int64_t> idx, Value *v_slot_in_array) {\n+      val_idx.assign(idx.begin(), idx.end());\n+      *(val_idx.end() - 1) *= 2;\n+      Value low_part =\n+          *(val_idx.end() - 1) < *(src_tiles.dimensions().end() - 1)\n+              ? src_tiles(val_idx)\n+              : default_val;\n+      *(val_idx.end() - 1) += 1;\n+      Value high_part =\n+          *(val_idx.end() - 1) < *(src_tiles.dimensions().end() - 1)\n+              ? src_tiles(val_idx)\n+              : default_val;\n+      const VectorType mask_ty = getNativeVregOrVmaskType(\n+          builder.getI1Type(), bitwidth / 2, target_shape);\n+      *v_slot_in_array =\n+          builder.create<PackMaskOp>(v.getLoc(), mask_ty, low_part, high_part);\n+    });\n+    return assemble(builder, vty, dst, out_vmsks, target_shape,\n+                    /*use_implicit_shape=*/true)\n+        .getResult();\n+  }\n+\n   if (is_mask) {\n     auto new_tile_ty = getNativeVregOrVmaskType(\n         builder.getIntegerType(bitwidth), bitwidth, target_shape);\n@@ -6823,6 +6741,7 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n   }\n   auto assemble_with_mask_check = [&](xla::Array<Value> &tiles,\n                                       bool use_implicit_shape = false) {\n+\n     if (is_mask) {\n       auto zeros_tile = builder.create<arith::ConstantOp>(\n           tiles.begin()->getLoc(),\n@@ -6941,9 +6860,110 @@ FailureOr<TypedValue<VectorType>> relayout(RewriteContext &ctx,\n       changeOffsets(ctx, builder, v.getLoc(), vty, src, std::move(src_tiles),\n                     dst.offsets()));\n \n-  CHECK_EQ(src, dst);  // At this point we've should be done.\n-  return assemble_with_mask_check(src_tiles,\n-                                  /*use_implicit_shape=*/true);\n+  CHECK_EQ(src, dst);\n+  return assemble_with_mask_check(src_tiles, /*use_implicit_shape=*/true);\n+}\n+\n+LogicalResult tpu_relayout_rule(RewriteContext &ctx, Operation &op,\n+                                const ArrayRef<Layout> layouts_in,\n+                                const ArrayRef<Layout> layouts_out) {\n+  auto tpu_relayout_op = cast<tpu::RelayoutOp>(op);\n+  auto input_val = dyn_cast<TypedValue<VectorType>>(tpu_relayout_op.getInput());\n+\n+  auto in_layout_array_attr =\n+      tpu_relayout_op->getAttrOfType<ArrayAttr>(\"in_layout\");\n+  if (!in_layout_array_attr || in_layout_array_attr.empty()) {\n+    return tpu_relayout_op.emitOpError(\n+        \"missing or empty 'in_layout' attribute\");\n+  }\n+  auto src_vla = dyn_cast<tpu::VectorLayoutAttr>(in_layout_array_attr[0]);\n+  if (!src_vla) {\n+    return tpu_relayout_op.emitOpError(\n+        \"'in_layout' attribute is not a VectorLayoutAttr\");\n+  }\n+  VectorLayout src_layout = src_vla.getLayout().value();\n+\n+  auto out_layout_array_attr =\n+      tpu_relayout_op->getAttrOfType<ArrayAttr>(\"out_layout\");\n+  if (!out_layout_array_attr || out_layout_array_attr.empty()) {\n+    return tpu_relayout_op.emitOpError(\n+        \"missing or empty 'out_layout' attribute\");\n+  }\n+  auto dst_vla = dyn_cast<tpu::VectorLayoutAttr>(out_layout_array_attr[0]);\n+  if (!dst_vla) {\n+    return tpu_relayout_op.emitOpError(\n+        \"'out_layout' attribute is not a VectorLayoutAttr\");\n+  }\n+  VectorLayout dst_layout = dst_vla.getLayout().value();\n+\n+  if (src_layout == dst_layout) {\n+    tpu_relayout_op.replaceAllUsesWith(tpu_relayout_op.getInput());\n+    tpu_relayout_op.erase();\n+    return success();\n+  }\n+\n+  OpBuilder builder(&op);\n+  FAILUREOR_ASSIGN_OR_RETURN(\n+      TypedValue<VectorType> new_v,\n+      relayout(ctx, builder, input_val, src_layout, dst_layout));\n+\n+  tpu_relayout_op.replaceAllUsesWith(new_v);\n+  tpu_relayout_op.erase();\n+  return success();\n+}\n+\n+const llvm::StringMap<rule_type> &rules() {\n+  static const llvm::StringMap<rule_type> *rules = [] {\n+    static auto rules = new llvm::StringMap<rule_type>{\n+        {arith::ConstantOp::getOperationName(), arith_constant_rule},\n+        {arith::ExtFOp::getOperationName(), arith_extf_rule},\n+        {arith::ExtSIOp::getOperationName(), arith_extsi_rule},\n+        {arith::ExtUIOp::getOperationName(), arith_extui_rule},\n+        {arith::TruncFOp::getOperationName(), arith_truncf_rule},\n+        {arith::TruncIOp::getOperationName(), arith_trunci_rule},\n+        {func::ReturnOp::getOperationName(), func_return_rule},\n+        {scf::ForOp::getOperationName(), scf_for_rule},\n+        {scf::WhileOp::getOperationName(), scf_while_rule},\n+        {scf::ConditionOp::getOperationName(), scf_condition_rule},\n+        {scf::IfOp::getOperationName(), scf_if_rule},\n+        {scf::YieldOp::getOperationName(), yield_rule},\n+        {tpu::YieldOp::getOperationName(), yield_rule},\n+        {tpu::RotateOp::getOperationName(), tpu_rotate_rule},\n+        {tpu::DynamicRotateOp::getOperationName(), tpu_dynamic_rotate_rule},\n+        {tpu::ConcatenateOp::getOperationName(), tpu_concatenate_rule},\n+        {tpu::IotaOp::getOperationName(), tpu_iota_rule},\n+        {tpu::GatherOp::getOperationName(), tpu_gather_rule},\n+        {tpu::DynamicGatherOp::getOperationName(), tpu_dynamic_gather_rule},\n+        {tpu::LoadOp::getOperationName(), tpu_load_rule},\n+        {tpu::StoreOp::getOperationName(), tpu_store_rule},\n+        {tpu::StridedLoadOp::getOperationName(), tpu_strided_load_rule},\n+        {tpu::StridedStoreOp::getOperationName(), tpu_strided_store_rule},\n+        {tpu::VectorStoreOp::getOperationName(), tpu_vector_store_rule},\n+        {tpu::MatmulOp::getOperationName(), tpu_matmul_rule},\n+        {tpu::RegionOp::getOperationName(), tpu_region_rule},\n+        {tpu::BitcastOp::getOperationName(), tpu_bitcast_rule},\n+        {tpu::TraceOp::getOperationName(), tpu_trace_rule},\n+        {tpu::AssumeLayoutOp::getOperationName(), tpu_assume_layout_rule},\n+        {tpu::PRNGRandomBitsOp::getOperationName(), tpu_prng_random_bits_rule},\n+        {tpu::RelayoutOp::getOperationName(), tpu_relayout_rule},\n+        {tpu::FPToSIOp::getOperationName(), tpu_fptosi_rule},\n+        {vector::BroadcastOp::getOperationName(), vector_broadcast_rule},\n+        {vector::ExtractOp::getOperationName(), vector_extract_rule},\n+        {vector::LoadOp::getOperationName(), vector_load_rule},\n+        {vector::MultiDimReductionOp::getOperationName(),\n+         vector_multi_reduction_rule},\n+        {vector::ExtractStridedSliceOp::getOperationName(),\n+         vector_extract_strided_slice_rule},\n+        {vector::ShapeCastOp::getOperationName(), vector_shape_cast_rule},\n+        {vector::StoreOp::getOperationName(), vector_store_rule},\n+        {tpu::TransposeOp::getOperationName(), vector_transpose_rule}};\n+\n+    for (const auto &[name, rule] : mlir::tpu::extensions::rules()) {\n+      rules->insert({name, rule});\n+    }\n+    return rules;\n+  }();\n+  return *rules;\n }\n \n // TODO(apaszke): Implement a debug mode that inserts additional assertions.\ndiff --git a/jaxlib/mosaic/dialect/tpu/vreg_util.cc b/jaxlib/mosaic/dialect/tpu/vreg_util.cc\nindex 72e0bf7f0caf..237bbe5cc722 100644\n--- a/jaxlib/mosaic/dialect/tpu/vreg_util.cc\n+++ b/jaxlib/mosaic/dialect/tpu/vreg_util.cc\n@@ -79,6 +79,19 @@ TypedValue<VectorType> getFullLikeVector(ImplicitLocOpBuilder &builder,\n   return getFullVector(builder, vec.getType(), value);\n }\n \n+TypedValue<VectorType> getFullVector(OpBuilder &builder, Location loc,\n+                                     VectorType vty, Attribute value) {\n+  return cast<TypedValue<VectorType>>(\n+      builder.create<arith::ConstantOp>(loc, DenseElementsAttr::get(vty, value))\n+          .getResult());\n+}\n+\n+TypedValue<VectorType> getFullLikeVector(OpBuilder &builder, Location loc,\n+                                         TypedValue<VectorType> vec,\n+                                         Attribute value) {\n+  return getFullVector(builder, loc, vec.getType(), value);\n+}\n+\n TypedValue<VectorType> getZerosVector(ImplicitLocOpBuilder &builder,\n                                       VectorType vty) {\n   return getFullVector(builder, vty, builder.getZeroAttr(vty.getElementType()));\ndiff --git a/jaxlib/mosaic/dialect/tpu/vreg_util.h b/jaxlib/mosaic/dialect/tpu/vreg_util.h\nindex 8c2967e776c7..8833390ef87b 100644\n--- a/jaxlib/mosaic/dialect/tpu/vreg_util.h\n+++ b/jaxlib/mosaic/dialect/tpu/vreg_util.h\n@@ -50,6 +50,15 @@ TypedValue<VectorType> getFullLikeVector(ImplicitLocOpBuilder &builder,\n                                          TypedValue<VectorType> vec,\n                                          Attribute value);\n \n+// Same as above, but takes a `loc` as input, in case of an OpBuilder.\n+TypedValue<VectorType> getFullVector(OpBuilder &builder, Location loc,\n+                                     VectorType vty, Attribute value);\n+\n+// Same as above, but takes a `vec` as input.\n+TypedValue<VectorType> getFullLikeVector(OpBuilder &builder, Location loc,\n+                                         TypedValue<VectorType> vec,\n+                                         Attribute value);\n+\n // Creates a vmask with false flags to bottom (dim = 0)\n // or right (dim = 1) where the flag count corresponds to the (dim_size -\n // padding).\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Support squeezing tiled memrefs to 1d shapes.\n\nPreviously, squeezing to a 1D memref failed w/ verification errors, as we would always use the old layout.\n\nIf we are squeezing from a source to a 1D shape, we need to modify the tile dimensions when we emit the result layout, as the removed dimensions should not be included in the new tiling.\n\nPiperOrigin-RevId: 757887087",
        "output": [
            "```diff\nCommit: 4dfcbc2c934456bbda866b8882d57864210f333d\nDate: 2025-05-12T20:22:39Z\nURL: https://github.com/jax-ml/jax/commit/4dfcbc2c934456bbda866b8882d57864210f333d\nFiles changed: 1\nAdditions: +147, Deletions: -65\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\nindex 134db412042d..934088e91506 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n+++ b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n@@ -13,6 +13,7 @@ See the License for the specific language governing permissions and\n limitations under the License.\n ==============================================================================*/\n \n+#include <algorithm>\n #include <cstddef>\n #include <cstdint>\n #include <optional>\n@@ -22,6 +23,7 @@ limitations under the License.\n #include \"absl/log/check.h\"\n #include \"absl/strings/str_format.h\"\n #include \"llvm/ADT/STLExtras.h\"\n+#include \"llvm/ADT/SmallVector.h\"\n #include \"llvm/Support/FormatVariadic.h\"\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n@@ -164,53 +166,115 @@ LogicalResult MemRefSliceOp::canonicalize(MemRefSliceOp op,\n   return success();\n }\n \n+// Computes the dimensions that were squeezed from the source shape to match the\n+// target shape. Returns the dimensions in increasing order.\n+FailureOr<SmallVector<int>> computeSqueezedDimsChecked(\n+    Operation *op, ArrayRef<int64_t> source_shape,\n+    ArrayRef<int64_t> target_shape) {\n+  SmallVector<int> squeezed;\n+  int source_index = source_shape.size() - 1;\n+  int target_index = target_shape.size() - 1;\n+\n+  while (source_index >= 0 || target_index >= 0) {\n+    int64_t target_dim = (target_index >= 0) ? target_shape[target_index] : -1;\n+    if (source_index < 0) {\n+      op->emitError() << llvm::formatv(\n+          \"Target shape is not valid. Source: {0}, Target: {1}.\",\n+          shapeToString(source_shape), shapeToString(target_shape));\n+      return failure();\n+    }\n+    int64_t source_dim = source_shape[source_index];\n+    if (source_dim == target_dim) {\n+      source_index--;\n+      target_index--;\n+    } else {\n+      if (source_dim != 1) {\n+        op->emitError() << llvm::formatv(\n+            \"Target shape is not valid. Source: {0}, Target: {1}.\",\n+            shapeToString(source_shape), shapeToString(target_shape));\n+        return failure();\n+      }\n+      squeezed.push_back(source_index);\n+      source_index--;\n+    }\n+  }\n+\n+  if (source_index != -1 || target_index != -1) {\n+    op->emitError() << \"Shape mismatch after traversal. Source shape: \"\n+                    << shapeToString(source_shape)\n+                    << \", target shape: \" << shapeToString(target_shape);\n+    return failure();\n+  }\n+  std::reverse(squeezed.begin(), squeezed.end());\n+  return squeezed;\n+}\n+\n LogicalResult MemRefSqueezeOp::verify() {\n   auto source_type = getMemRefType(getInput());\n   auto target_type = getType();\n-  // Source and target attributes may be different before propagation is done by\n-  // the canonicalizer, so we allow this when attributes are \"unset\" in the\n-  // target type.\n+\n   if (target_type.getMemorySpace() != nullptr &&\n       target_type.getMemorySpace() != source_type.getMemorySpace()) {\n-    emitOpError(\"Memory spaces do not match.\");\n-    return failure();\n+    return emitOpError(\"Memory spaces do not match.\");\n   }\n+\n   if (target_type.getElementType() != source_type.getElementType()) {\n-    this->emitOpError(\"Element types don't match.\");\n-    return failure();\n-  }\n-  if (!HasMemorySpace(source_type, tpu::MemorySpace::kSemaphoreMem) &&\n-      source_type.getRank() > 1 && target_type.getRank() == 1) {\n-    return emitError(\"Not implemented: squeeze memref to 1d.\");\n+    return emitOpError(\"Element types don't match.\");\n   }\n+\n   auto source_shape = source_type.getShape();\n   auto target_shape = target_type.getShape();\n-  int source_index = source_shape.size() - 1;\n-  int target_index = target_shape.size() - 1;\n-  auto error_msg = llvm::formatv(\n-      \"Target shape is not valid. \"\n-      \"Source type: {0}. Target type: {1}.\",\n-      source_type, target_type);\n-  while (source_index >= 0 || target_index >= 0) {\n-    int target_dim = target_index < 0 ? -1 : target_shape[target_index];\n-    if (source_index < 0) {\n-       // We have run out of source shape but target shape still remains.\n-       emitOpError(error_msg);\n-       return failure();\n+  auto squeezed_or =\n+      computeSqueezedDimsChecked(*this, source_shape, target_shape);\n+  if (failed(squeezed_or)) {\n+    return failure();\n+  }\n+\n+  auto erase_layout_op = getInput().getDefiningOp<tpu::EraseLayoutOp>();\n+  if (!erase_layout_op) {\n+    return success();\n+  }\n+\n+  auto layout_ref = erase_layout_op.getOperand();\n+  MemRefType layout_ty = getMemRefType(layout_ref);\n+  auto layout_attr = dyn_cast<tpu::TiledLayoutAttr>(layout_ty.getLayout());\n+  if (!layout_attr) {\n+    return emitOpError(\n+        \"Input from EraseLayoutOp is expected to have a TiledLayoutAttr.\");\n+  }\n+  auto &squeezed = squeezed_or.value();\n+  if (squeezed.empty() && source_shape != target_shape) {\n+    return failure();\n+  }\n+\n+  auto tiles = layout_attr.getTiles();\n+  if (tiles.size() == 1) {\n+    auto tile = layout_attr.getTiles().front();\n+    auto tile_dims = tile.dimensions();\n+    int first_tiled = source_shape.size() - tile_dims.size();\n+    for (int dim : squeezed) {\n+      if (dim >= first_tiled) {\n+        int tile_idx = dim - first_tiled;\n+        if (tile_idx < 0 || tile_idx >= static_cast<int>(tile_dims.size())) {\n+          return emitOpError() << \"Internal error: tile index out of bounds.\";\n+        }\n+        if (tile_dims[tile_idx] != 1) {\n+          return emitOpError()\n+                 << \"All tiled squeezed dimensions must be of size 1.\";\n+        }\n+      }\n     }\n-    int source_dim = source_shape[source_index];\n-    if (source_dim == target_dim) {\n-       source_index--;\n-       target_index--;\n-    } else {\n-       // Only the source dim can be 1 here.\n-       if (source_dim != 1) {\n-         this->emitOpError(error_msg);\n-         return failure();\n-       }\n-       source_index--;\n+  } else {\n+    auto first_tile = tiles.front();\n+    for (int dim : squeezed) {\n+      int first_tiled = source_shape.size() - first_tile.dimensions().size();\n+      if (dim >= first_tiled) {\n+        return emitOpError() << \"When multiple tiles are present, no tiled \"\n+                                \"dimensions can be squeezed.\";\n+      }\n     }\n   }\n+\n   return success();\n }\n \n@@ -222,42 +286,60 @@ LogicalResult MemRefSqueezeOp::canonicalize(MemRefSqueezeOp op,\n   if (!erase_layout) {\n     return failure();\n   }\n-  // Push layout erasure through squeezing. It is important we see the layout\n-  // for lowering and don't make it hard for other ops to query it.\n+\n   auto layout_ref = erase_layout.getOperand();\n-  MemRefType layout_ty = layout_ref.getType();\n+  MemRefType layout_ty = getMemRefType(layout_ref);\n+  auto layout_attr = dyn_cast<tpu::TiledLayoutAttr>(layout_ty.getLayout());\n+  if (!layout_attr) {\n+    return failure();\n+  }\n+\n   auto source_shape = source_type.getShape();\n   auto target_shape = target_type.getShape();\n-  int source_index = source_shape.size() - 1;\n-  int target_index = target_shape.size() - 1;\n-  auto old_layout = dyn_cast<tpu::TiledLayoutAttr>(layout_ty.getLayout());\n-  auto target_strides = old_layout.getTileStrides();\n-  SmallVector<int64_t> tile_strides(target_strides.begin(),\n-                                    target_strides.end());\n-  // We want to remove all strides that correspond to squeezed dimensions and\n-  // update the corresponding output layout.\n-  while (source_index >= 0 || target_index >= 0) {\n-    int target_dim = target_index < 0 ? -1 : target_shape[target_index];\n-    int source_dim = source_shape[source_index];\n-    if (source_dim == target_dim) {\n-       source_index--;\n-       target_index--;\n-    } else {\n-       // Source index must be 1 here (otherwise verification will have failed).\n-       // We are safe to mutate the strides vector here because we are looping\n-       // backwards.\n-       tile_strides.erase(tile_strides.begin() + source_index);\n-       source_index--;\n+  auto squeezed_or = computeSqueezedDimsChecked(op, source_shape, target_shape);\n+  if (failed(squeezed_or)) {\n+    return failure();\n+  }\n+  auto &squeezed = squeezed_or.value();\n+  if (squeezed.empty() && source_shape != target_shape) {\n+    return failure();\n+  }\n+\n+  SmallVector<int64_t> tile_strides =\n+      llvm::to_vector(layout_attr.getTileStrides());\n+  for (int i = squeezed.size() - 1; i >= 0; --i) {\n+    tile_strides.erase(tile_strides.begin() + squeezed[i]);\n+  }\n+\n+  tpu::TiledLayoutAttr new_layout;\n+  bool target_is_1d = target_shape.size() == 1;\n+  auto tiles = layout_attr.getTiles();\n+  if (target_is_1d && tiles.size() == 1) {\n+    auto tile_dims = llvm::to_vector(tiles.front().dimensions());\n+    int first_tiled = source_shape.size() - tile_dims.size();\n+    for (int i = squeezed.size() - 1; i >= 0; --i) {\n+      int dim = squeezed[i];\n+      if (dim >= first_tiled) {\n+        int tile_idx = dim - first_tiled;\n+        if (tile_idx < 0 || tile_idx >= static_cast<int>(tile_dims.size())) {\n+          return op.emitError() << \"Internal error: tile index out of bounds.\";\n+        }\n+        tile_dims.erase(tile_dims.begin() + tile_idx);\n+      }\n     }\n+    new_layout = tpu::TiledLayoutAttr::get(\n+        op.getContext(), {xla::Tile(tile_dims)}, tile_strides);\n+  } else {\n+    new_layout = tpu::TiledLayoutAttr::get(\n+        op.getContext(), layout_attr.getTiles(), tile_strides);\n   }\n-  auto new_layout = tpu::TiledLayoutAttr::get(\n-      source_type.getContext(), old_layout.getTiles(), tile_strides);\n-  auto new_result_type = MemRefType::get(op.getResult().getType().getShape(),\n-                                         layout_ty.getElementType(), new_layout,\n-                                         layout_ty.getMemorySpace());\n-  auto squeeze = rewriter.create<MemRefSqueezeOp>(op.getLoc(), new_result_type,\n-                                                  layout_ref);\n-  rewriter.replaceOpWithNewOp<EraseLayoutOp>(op, op.getType(), squeeze);\n+\n+  auto new_ty = MemRefType::get(target_shape, layout_ty.getElementType(),\n+                                new_layout, layout_ty.getMemorySpace());\n+\n+  auto new_squeeze =\n+      rewriter.create<MemRefSqueezeOp>(op.getLoc(), new_ty, layout_ref);\n+  rewriter.replaceOpWithNewOp<tpu::EraseLayoutOp>(op, target_type, new_squeeze);\n   return success();\n }\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "remove :custom_call and :runtime from mosaic_gpu since they are in :mosaic_gpu_support now.\n\nPiperOrigin-RevId: 757881025",
        "output": [
            "```diff\nCommit: f6b9f7d7e32272ff10d447fb7c986edbe3fd3ef8\nDate: 2025-05-12T20:06:45Z\nURL: https://github.com/jax-ml/jax/commit/f6b9f7d7e32272ff10d447fb7c986edbe3fd3ef8\nFiles changed: 1\nAdditions: +1, Deletions: -4\ndiff --git a/jaxlib/mosaic/gpu/BUILD b/jaxlib/mosaic/gpu/BUILD\nindex b694258fed1e..115d0c47cc52 100644\n--- a/jaxlib/mosaic/gpu/BUILD\n+++ b/jaxlib/mosaic/gpu/BUILD\n@@ -23,10 +23,7 @@ package(\n py_library(\n     name = \"mosaic_gpu\",\n     data = [\":libmosaic_gpu_runtime.so\"],\n-    deps = [\n-        \":_mosaic_gpu_ext\",\n-        \":mosaic_gpu_support\",\n-    ],\n+    deps = [\":_mosaic_gpu_ext\"],\n )\n \n cc_library(\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28695 from jakevdp:array-api-tests\n\nPiperOrigin-RevId: 757862940",
        "output": [
            "```diff\nCommit: 76e5bc6f5d5c89c9d56d3211dba1953ae8d20cde\nDate: 2025-05-12T19:15:19Z\nURL: https://github.com/jax-ml/jax/commit/76e5bc6f5d5c89c9d56d3211dba1953ae8d20cde\nFiles changed: 2\nAdditions: +4, Deletions: -1\ndiff --git a/.github/workflows/jax-array-api.yml b/.github/workflows/jax-array-api.yml\nindex 825d3ada9a0b..6419cb730b71 100644\n--- a/.github/workflows/jax-array-api.yml\n+++ b/.github/workflows/jax-array-api.yml\n@@ -32,7 +32,7 @@ jobs:\n       with:\n         repository: data-apis/array-api-tests\n         # TODO(jakevdp) update this to a stable release/tag when available.\n-        ref: 'c48410f96fc58e02eea844e6b7f6cc01680f77ce'  # Latest commit as of 2025-04-02\n+        ref: 'c847143beb8d769bde5dbcc063fe19ed7acc2f9b'  # Latest commit as of 2025-05-12\n         submodules: 'true'\n         path: 'array-api-tests'\n     - name: Install dependencies\ndiff --git a/tests/array_api_skips.txt b/tests/array_api_skips.txt\nindex 7534cf6f8acd..7781b93e7820 100644\n--- a/tests/array_api_skips.txt\n+++ b/tests/array_api_skips.txt\n@@ -2,6 +2,7 @@\n \n # finfo return type misalignment (https://github.com/data-apis/array-api/issues/405)\n array_api_tests/test_data_type_functions.py::test_finfo[float32]\n+array_api_tests/test_data_type_functions.py::test_finfo[complex64]\n \n # Test suite attempts in-place mutation:\n array_api_tests/test_array_object.py::test_setitem\n@@ -28,6 +29,8 @@ array_api_tests/test_special_cases.py::test_iop[__ifloordiv__(x1_i is -0 and x2_\n array_api_tests/test_special_cases.py::test_iop[__ifloordiv__(x1_i is -0 and x2_i < 0) -> +0]\n array_api_tests/test_special_cases.py::test_iop[__ifloordiv__(isfinite(x1_i) and x1_i < 0 and x2_i is -infinity) -> +0]\n \n+# Array API expects default value for axis argument.\n+array_api_tests/test_indexing_functions.py::test_take_along_axis\n \n # Returns int32 when int64 is expected\n array_api_tests/test_searching_functions.py::test_searchsorted\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28662 from dsuedholt:faster-stft\n\nPiperOrigin-RevId: 757839805",
        "output": [
            "```diff\nCommit: b845ecdc2c79c439ac10099946f94ab217828d60\nDate: 2025-05-12T18:17:49Z\nURL: https://github.com/jax-ml/jax/commit/b845ecdc2c79c439ac10099946f94ab217828d60\nFiles changed: 1\nAdditions: +3, Deletions: -7\ndiff --git a/jax/_src/scipy/signal.py b/jax/_src/scipy/signal.py\nindex 565909e8a6d1..f8c2563027f5 100644\n--- a/jax/_src/scipy/signal.py\n+++ b/jax/_src/scipy/signal.py\n@@ -566,13 +566,9 @@ def _fft_helper(x: Array, win: Array, detrend_func: Callable[[Array], Array],\n     result = x[..., np.newaxis]\n   else:\n     step = nperseg - noverlap\n-    batch_shape = list(batch_shape)\n-    x = x.reshape((math.prod(batch_shape), signal_length, 1))\n-    result = jax.lax.conv_general_dilated_patches(\n-        x, (nperseg,), (step,),\n-        'VALID',\n-        dimension_numbers=('NTC', 'OIT', 'NTC'))\n-    result = result.reshape(*batch_shape, *result.shape[-2:])\n+    starts = jnp.arange(signal_length - nperseg + 1, step=step)\n+    slice_func = partial(jax.lax.dynamic_slice_in_dim, operand=x, slice_size=nperseg, axis=-1)\n+    result = jax.vmap(slice_func, out_axes=-2)(start_index=starts)\n \n   # Detrend each data segment individually\n   result = detrend_func(result)\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[array API] update test suite to most recent version",
        "output": [
            "```diff\nCommit: 3b9865af0c418634811284953f76e70c347319e9\nDate: 2025-05-12T17:35:04Z\nURL: https://github.com/jax-ml/jax/commit/3b9865af0c418634811284953f76e70c347319e9\nFiles changed: 2\nAdditions: +4, Deletions: -1\ndiff --git a/.github/workflows/jax-array-api.yml b/.github/workflows/jax-array-api.yml\nindex 825d3ada9a0b..6419cb730b71 100644\n--- a/.github/workflows/jax-array-api.yml\n+++ b/.github/workflows/jax-array-api.yml\n@@ -32,7 +32,7 @@ jobs:\n       with:\n         repository: data-apis/array-api-tests\n         # TODO(jakevdp) update this to a stable release/tag when available.\n-        ref: 'c48410f96fc58e02eea844e6b7f6cc01680f77ce'  # Latest commit as of 2025-04-02\n+        ref: 'c847143beb8d769bde5dbcc063fe19ed7acc2f9b'  # Latest commit as of 2025-05-12\n         submodules: 'true'\n         path: 'array-api-tests'\n     - name: Install dependencies\ndiff --git a/tests/array_api_skips.txt b/tests/array_api_skips.txt\nindex 7534cf6f8acd..7781b93e7820 100644\n--- a/tests/array_api_skips.txt\n+++ b/tests/array_api_skips.txt\n@@ -2,6 +2,7 @@\n \n # finfo return type misalignment (https://github.com/data-apis/array-api/issues/405)\n array_api_tests/test_data_type_functions.py::test_finfo[float32]\n+array_api_tests/test_data_type_functions.py::test_finfo[complex64]\n \n # Test suite attempts in-place mutation:\n array_api_tests/test_array_object.py::test_setitem\n@@ -28,6 +29,8 @@ array_api_tests/test_special_cases.py::test_iop[__ifloordiv__(x1_i is -0 and x2_\n array_api_tests/test_special_cases.py::test_iop[__ifloordiv__(x1_i is -0 and x2_i < 0) -> +0]\n array_api_tests/test_special_cases.py::test_iop[__ifloordiv__(isfinite(x1_i) and x1_i < 0 and x2_i is -infinity) -> +0]\n \n+# Array API expects default value for axis argument.\n+array_api_tests/test_indexing_functions.py::test_take_along_axis\n \n # Returns int32 when int64 is expected\n array_api_tests/test_searching_functions.py::test_searchsorted\n\n```"
        ],
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "[platform_dependent] Ensure that platform_dependent only lowers for intended platforms\n\nFixes: #28594\n\nCurrently `lax.platform_dependent` allows specifying code that behaves\ndifferently when lowered on different platforms. However, this function\noperates in a confusing way, in that it will create a branch on the\nplatform, but will lower all branches for the **current** lowering platforms.\n\nFor example, in the following code:\n```\n   lax.platform_dependent(x,\n                          cpu=for_cpu, tpu=for_tpu)\n```\n\nIf we lower for CPU, we lower both `for_cpu` and `for_tpu`\nfor CPU (!), but only the branch corresponding to `for_cpu`\nwill actually run.\n\nThis is a problem if, e.g., `for_tpu` does not have a lowering\nfor CPU. We will get an error during lowering. Instead there should\nbe no error during lowering, because that branch is not actually needed.\n\nWe add a new test `test_platform_dependent_with_primitive_with_lowering_error`\nto demonstrate this.\n\nThe solution implememented here is the Solution A from #28594: we\nadd a `branches_platform` param to the `cond` primitive, which is\npropagated by all transformations. This param is used only for the\nconditionals arising from `lax.platform_dependendet`.\nDuring lowering we drop the branches corresponding to the platforms\nthat are not interesting.",
        "output": [
            "```diff\nCommit: f2121a72fc97c555eda6f519dba28dfe883e62cb\nDate: 2025-05-12T15:58:03Z\nURL: https://github.com/jax-ml/jax/commit/f2121a72fc97c555eda6f519dba28dfe883e62cb\nFiles changed: 8\nAdditions: +201, Deletions: -90\ndiff --git a/jax/_src/checkify.py b/jax/_src/checkify.py\nindex 5a6456762db7..144cbaf5cd21 100644\n--- a/jax/_src/checkify.py\n+++ b/jax/_src/checkify.py\n@@ -759,7 +759,8 @@ def jaxpr_to_checkify_jaxpr(\n   out_tree, error_effects = metadata()\n   return checked_jaxpr, out_tree, error_effects\n \n-def cond_error_check(error: Error, enabled_errors, index, *ops, branches):\n+def cond_error_check(error: Error, enabled_errors, index, *ops,\n+                     branches, **params):\n   # Get the error-effects out of all branches so the cond can be called with\n   # a merged error with all these effects.\n   err_vals, err_tree = jtu.tree_flatten(error)\n@@ -780,7 +781,7 @@ def get_error_effects_from_jaxpr(jxpr):\n \n   err_and_outs = lax.cond_p.bind(\n       index, *err_vals, *ops,\n-      branches=tuple(new_branches))\n+      branches=tuple(new_branches), **params)\n \n   # we need to merge metadata across out_trees (a tuple)\n   err0, out = tree_unflatten(out_trees[0], err_and_outs)\ndiff --git a/jax/_src/interpreters/mlir.py b/jax/_src/interpreters/mlir.py\nindex e9deb8d3fff9..f6ef5787ccbf 100644\n--- a/jax/_src/interpreters/mlir.py\n+++ b/jax/_src/interpreters/mlir.py\n@@ -2080,6 +2080,11 @@ def _platforms_for_eqn_ctx(eqn_ctx: core.JaxprEqnContext | None\n     return ('tpu',)\n   return ()\n \n+def _platforms_for_eqn(ctx: LoweringRuleContext) -> tuple[str, ...]:\n+  \"\"\"The lowering platforms for the current eqn\"\"\"\n+  return tuple((_platforms_for_eqn_ctx(ctx.jaxpr_eqn_ctx) or\n+               ctx.platforms or ctx.module_context.platforms))\n+\n \n def lower_per_platform(ctx: LoweringRuleContext,\n                        description: str,\n@@ -2122,8 +2127,7 @@ def lower_per_platform(ctx: LoweringRuleContext,\n    rule_args: the args of the lowering rules.\n    rule_kwargs: the kwargs of the lowering rules.\n   \"\"\"\n-  platforms: Sequence[str] = (_platforms_for_eqn_ctx(ctx.jaxpr_eqn_ctx) or\n-                              ctx.platforms or ctx.module_context.platforms)\n+  platforms: Sequence[str] = _platforms_for_eqn(ctx)\n   # Special case the common case (single-platform lowering)\n   if len(platforms) == 1:\n     rule = platform_rules.get(platforms[0], default_rule)\ndiff --git a/jax/_src/lax/control_flow/__init__.py b/jax/_src/lax/control_flow/__init__.py\nindex f89e4d53a476..44ee94e14ca2 100644\n--- a/jax/_src/lax/control_flow/__init__.py\n+++ b/jax/_src/lax/control_flow/__init__.py\n@@ -34,6 +34,7 @@\n     while_p as while_p,\n )\n from jax._src.lax.control_flow.conditionals import (\n+    BranchesPlatforms as BranchesPlatforms,\n     cond as cond,\n     cond_p as cond_p,\n     switch as switch,\ndiff --git a/jax/_src/lax/control_flow/conditionals.py b/jax/_src/lax/control_flow/conditionals.py\nindex 99fa72421ea1..d875989921d0 100644\n--- a/jax/_src/lax/control_flow/conditionals.py\n+++ b/jax/_src/lax/control_flow/conditionals.py\n@@ -46,6 +46,7 @@\n from jax._src.interpreters import xla\n from jax._src.lax import lax\n from jax._src.traceback_util import api_boundary\n+from jax._src.typing import ArrayLike\n from jax._src.util import safe_map, split_list, partition_list, unzip2\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import hlo\n@@ -127,9 +128,17 @@ def switch(index, branches, *operands):\n   lo = np.array(0, np.int32)\n   hi = np.array(len(branches) - 1, np.int32)\n   index = lax.clamp(lo, index, hi)\n+  return _switch_internal(index, branches, operands,\n+                          branches_platforms=None)\n \n+\n+def _switch_internal(\n+    index: ArrayLike,\n+    branches: Sequence[Callable],\n+    operands: Sequence[ArrayLike], *,\n+    branches_platforms: BranchesPlatforms | None):\n   if (config.disable_jit.value and core.is_concrete(index)):\n-    return branches[int(index)](*operands)\n+    return branches[int(index)](*operands)  # type: ignore\n \n   dbgs = [api_util.debug_info(\"switch\", branch, operands, {})\n           for branch in branches]\n@@ -159,7 +168,10 @@ def switch(index, branches, *operands):\n     raise NotImplementedError(\n         f'Effects not supported in `switch`: {disallowed_effects}')\n   jaxprs = [replace_jaxpr_effects(jaxpr, joined_effects) for jaxpr in jaxprs]\n-  out = cond_p.bind(index, *consts, *ops, branches=tuple(jaxprs))\n+  params = dict(branches=tuple(jaxprs))\n+  if branches_platforms is not None:\n+    params[\"branches_platforms\"] = branches_platforms\n+  out = cond_p.bind(index, *consts, *ops, **params)\n   out_ = iter(out)\n \n   all_inputs = [*consts, *ops]\n@@ -464,7 +476,7 @@ def _bcast_select_n(pred, *cases):\n     pred = lax.broadcast_in_dim(pred, np.shape(cases[0]), idx)\n   return lax.select_n(pred, *cases)\n \n-def _cond_batching_rule(axis_data, args, dims, branches):\n+def _cond_batching_rule(axis_data, args, dims, *, branches, **params):\n   index, *ops = args\n   index_dim, *op_dims = dims\n   # TODO(sharadmv): clean this up by adding a specific blocklist\n@@ -480,6 +492,9 @@ def _cond_batching_rule(axis_data, args, dims, branches):\n \n \n   if index_dim is not batching.not_mapped:\n+    assert \"branches_platforms\" not in params, (\n+        \"The index of a cond with branches_platforms should be a \"\n+        \"platform_index and should never be mapped\")\n     # Convert to a lax.select. While we could get away with not broadcasting\n     # some operands yet, because all outputs must be broadcast together anyway\n     # for the select we broadcast the input operands for simplicity and leave\n@@ -518,10 +533,11 @@ def _cond_batching_rule(axis_data, args, dims, branches):\n         for jaxpr in branches)\n \n     out_dims = [0 if b else batching.not_mapped for b in out_bat]\n-    out = cond_p.bind(index, *ops, branches=branches_batched)\n+    out = cond_p.bind(index, *ops, branches=branches_batched,\n+                      **params)\n     return out, out_dims\n \n-def _cond_jvp(primals, tangents, branches):\n+def _cond_jvp(primals, tangents, *, branches, **params):\n   nonzeros = [type(t) is not ad_util.Zero for t in tangents]\n \n   index_nz, *ops_nz = nonzeros\n@@ -538,14 +554,15 @@ def _cond_jvp(primals, tangents, branches):\n   _, *ops_dot = tangents\n   ops_dot = _prune_zeros(ops_dot)\n \n-  out = cond_p.bind(index, *ops, *ops_dot, branches=branches_jvp)\n+  out = cond_p.bind(index, *ops, *ops_dot, branches=branches_jvp,\n+                    **params)\n   out_primals, out_tangents = split_list(out, [len(out_nz)])\n   out_tangents_iter = iter(out_tangents)\n   out_tangents = [next(out_tangents_iter) if nz else ad_util.Zero.from_primal_value(p)\n                   for p, nz in zip(out_primals, out_nz)]\n   return out_primals, out_tangents\n \n-def _cond_partial_eval(trace, *tracers, branches):\n+def _cond_partial_eval(trace, *tracers, branches, **params):\n   in_unknowns = [t.pval[0] is not None for t in tracers]\n   index_uk, *ops_uk = in_unknowns\n   if any(isinstance(eff, RefEffect) for branch in branches for eff in\n@@ -556,7 +573,7 @@ def _cond_partial_eval(trace, *tracers, branches):\n   if index_uk:\n     # When the branch index is unknown, we stage out the whole cond.\n     # TODO(mattjj): remove this path when old remat is removed\n-    params = dict(branches=branches)\n+    params = dict(branches=branches, **params)\n     return trace.default_process_primitive(cond_p, tracers, params)\n \n   branches_out_uks = []\n@@ -586,7 +603,8 @@ def _cond_partial_eval(trace, *tracers, branches):\n              for j in branches_known[1:])\n \n   in_consts = [t.pval.get_known() for t in tracers if t.pval.is_known()]\n-  out_consts_res = cond_p.bind(*in_consts, branches=branches_known)\n+  out_consts_res = cond_p.bind(*in_consts, branches=branches_known,\n+                               **params)\n   out_consts, res = split_list(out_consts_res, [len(out_consts_res) - num_res])\n \n   index_tracer = trace.instantiate_const(tracers[0])\n@@ -595,7 +613,7 @@ def _cond_partial_eval(trace, *tracers, branches):\n   res_tracers = map(trace.new_instantiated_const, res)\n   out_tracers = [pe.JaxprTracer(trace, pe.PartialVal.unknown(aval), None)\n                  for aval in branches_unknown[0].out_avals]\n-  params = dict(branches=branches_unknown)\n+  params = dict(branches=branches_unknown, **params)\n   name_stack = source_info_util.current_name_stack()[len(trace.name_stack):]\n   source = source_info_util.current().replace(name_stack=name_stack)\n   eqn = pe.new_eqn_recipe(\n@@ -608,6 +626,7 @@ def _cond_partial_eval(trace, *tracers, branches):\n def _cond_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n   index_uk, *ops_uk = unks_in\n   branches = eqn.params['branches']\n+  eqn_rest_params = dict(k_v for k_v in eqn.params.items() if k_v[0] != 'branches')\n \n   # Instantiate all inputs (b/c jaxpr_staged will take all inputs).\n   new_inst = [x for x, inst in zip(eqn.invars, inst_in)\n@@ -664,7 +683,7 @@ def _cond_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n   # Build the known eqn.\n   ins_known, _ = partition_list(unks_in, eqn.invars)  # includes index invar\n   out_binders_known, _ = partition_list(unks_out, eqn.outvars)\n-  params_known = dict(branches=branches_known)\n+  params_known = dict(branches=branches_known, **eqn_rest_params)\n   effects_known = _join_cond_effects(branches_known)\n   eqn_known = pe.new_jaxpr_eqn(\n       ins_known, [*out_binders_known, *res_binders], cond_p, params_known,\n@@ -672,7 +691,7 @@ def _cond_partial_eval_custom(saveable, unks_in, inst_in, eqn):\n \n   # Build the staged eqn.\n   _, out_binders_staged = partition_list(inst_out, eqn.outvars)\n-  params_staged = dict(branches=branches_staged)\n+  params_staged = dict(branches=branches_staged, **eqn_rest_params)\n   effects_staged = _join_cond_effects(branches_staged)\n   eqn_staged = pe.new_jaxpr_eqn(\n       [eqn.invars[0], *res_binders, *eqn.invars[1:]], out_binders_staged,\n@@ -818,7 +837,7 @@ def transposed(*args):\n                                          debug_info=jaxpr.jaxpr.debug_info),\n                             res_avals + jaxpr.out_avals)\n \n-def _cond_transpose(cts, *args, branches):\n+def _cond_transpose(cts, *args, branches, **params):\n   index, *ops = args\n   assert type(index) is not ad.UndefinedPrimal\n   linear = [type(x) is ad.UndefinedPrimal for x in ops]\n@@ -838,7 +857,8 @@ def _cond_transpose(cts, *args, branches):\n   res = ops[:num_res]\n   cts = map(ad.instantiate_zeros, cts)\n \n-  out = cond_p.bind(index, *res, *cts, branches=branches_trans)\n+  out = cond_p.bind(index, *res, *cts, branches=branches_trans,\n+                    **params)\n   assert all(map(core.typecheck, lin_in_avals, out))\n \n   out_iter = iter(out)\n@@ -846,7 +866,8 @@ def _cond_transpose(cts, *args, branches):\n   assert next(out_iter, None) is None\n   return [None] + out\n \n-def _cond_typecheck(bind_time, *in_atoms, branches):\n+def _cond_typecheck(bind_time, *in_atoms, branches, **params):\n+  del params\n   if not bind_time:\n     _, *in_atoms = in_atoms\n   avals = [x.aval for x in in_atoms]\n@@ -900,6 +921,16 @@ def _cond_typecheck(bind_time, *in_atoms, branches):\n       f'called with operands of type {_avals_short(op_avals)}')\n   return jaxpr0.out_avals, joined_effects\n \n+\n+BranchesPlatforms = tuple[tuple[str, ...] | None, ...]\n+# cond_p takes an optional branches_platforms param of type `BranchesPlatforms`\n+# when it is a `platform_dependent` conditional.\n+# In that case, `branches_platforms` is a tuple as long\n+# as `branches` and for each branch it specifies the lowering platforms it\n+# corresponds to. The last element, corresponding to the last branch,\n+# can be `None` to represent a default match-all-lowering-platforms.\n+# The index argument of a `platform_dependent` cond is always a\n+# `platform_index` primitive.\n cond_p = core.Primitive('cond')\n cond_p.multiple_results = True\n cond_p.skip_canonicalization = True\n@@ -915,7 +946,39 @@ def _cond_typecheck(bind_time, *in_atoms, branches):\n pe.dce_rules[cond_p] = _cond_dce_rule\n batching.ragged_prop_rules[cond_p] = batching.ragged_mask_assert_no_op_rule\n \n-def _cond_lowering(ctx, index, *args, branches):\n+def _cond_lowering(ctx, index, *args, branches,\n+                   **params):\n+  if (branches_platforms := params.get(\"branches_platforms\", None)) is not None:\n+    branches_kept: list[core.ClosedJaxpr] = []\n+    index_to_kept_index: dict[int, int] = {}\n+    for p in mlir._platforms_for_eqn(ctx):\n+      # Each `p` must appear in exactly one branches_platforms, or in the\n+      # last default branch. Otherwise, platform_index lowering would have\n+      # failed already.\n+      for b_idx, b_platforms in enumerate(branches_platforms):\n+        if b_platforms is None or p in b_platforms:\n+          if b_idx not in index_to_kept_index:\n+            index_to_kept_index[b_idx] = len(branches_kept)\n+            branches_kept.append(branches[b_idx])\n+          break\n+      else:\n+        assert False, p\n+\n+    # Compute the new index into branches_keep\n+    i32_type = ir.RankedTensorType.get([], mlir.dtype_to_ir_type(dtypes.dtype(np.int32)))\n+    kept_index_case_op = hlo.CaseOp([i32_type],\n+                                    index=index,\n+                                    num_branches=len(branches))\n+    for i in range(len(branches)):\n+      branch = kept_index_case_op.regions[i].blocks.append()\n+      with ir.InsertionPoint(branch):\n+        kept_i = np.int32(index_to_kept_index.get(i, 0))\n+        hlo.return_([mlir.ir_constant(kept_i)])\n+\n+    index = kept_index_case_op\n+    branches = branches_kept\n+    assert branches, \"platform_index lowering should have failed first\"\n+\n   joined_effects = core.join_effects(*(branch.effects for branch in branches))\n   ordered_effects = list(effects.ordered_effects.filter_in(joined_effects))\n   num_tokens = len(ordered_effects)\n@@ -952,7 +1015,8 @@ def _cond_lowering(ctx, index, *args, branches):\n mlir.register_lowering(cond_p, _cond_lowering)\n \n @register_partial_discharge_rule(cond_p)\n-def _cond_state_discharge_rule(should_discharge, in_avals, out_avals, index, *args, branches):\n+def _cond_state_discharge_rule(should_discharge, in_avals, out_avals, index, *args,\n+                               branches, **params):\n   assert not should_discharge[0], \"Can't discharge the index.\"\n   discharged_branches = tuple(\n       discharge_state(branch.jaxpr, (), should_discharge=should_discharge[1:])[0]\n@@ -981,7 +1045,8 @@ def _cond_state_discharge_rule(should_discharge, in_avals, out_avals, index, *ar\n                                   if fwd is None]), ())\n       for branch in discharged_branches\n   )\n-  out_vals_no_fwd = cond_p.bind(index, *args, branches=new_branches)\n+  out_vals_no_fwd = cond_p.bind(index, *args, branches=new_branches,\n+                                **params)\n   out_vals, out_ref_vals_no_fwd = util.split_list(out_vals_no_fwd, [len(out_avals)])\n   # Insert forwarded values into reference outputs\n   ref_val_no_fwd_iter = iter(out_ref_vals_no_fwd)\n@@ -1046,50 +1111,41 @@ def other_platforms_code(*args): ...\n     The value ``per_platform[execution_platform](*args)``.\n   \"\"\"\n   # Join identical branches\n-  platform_branches: list[tuple[list[str], Callable]] = []\n+  branches_platforms_list: list[tuple[list[str], Callable]] = []\n   for pname, pbranch in per_platform.items():\n+    if not callable(pbranch):\n+      raise TypeError(f\"lax.platform_dependent: the '{pname}' branch must \"\n+                      \"be a callable.\")\n     if pname == \"gpu\":\n       raise ValueError(\"Use 'cuda' or 'rocm' for lax.platform_dependent.\")\n-    for ps, b in platform_branches:\n+    for ps, b in branches_platforms_list:\n       if b == pbranch:\n         ps.append(pname)\n         break\n     else:\n-      platform_branches.append(([pname], pbranch))\n-\n-  platforms_lists, branches = util.unzip2(platform_branches)\n-  platform_index = platform_index_p.bind(\n-    platforms=tuple(tuple(ps) for ps in platforms_lists),\n-    has_default=(default is not None))\n+      branches_platforms_list.append(([pname], pbranch))\n \n+  platforms_lists, branches = util.unzip2(branches_platforms_list)\n+  branches_platforms: BranchesPlatforms = tuple(tuple(ps) for ps in platforms_lists)\n   if default is not None:\n+    if not callable(default):\n+      raise TypeError(\"lax.platform_dependent: the 'default' branch must \"\n+                      \"be a callable.\")\n     branches = branches + (default,)\n-  # Use a switch, to get the proper transformation rules for free. Since\n-  # platform index has no dependence on the input data, it won't be vectorized\n-  # under vmap.\n-  # If the switch and the platform_index_p above are in the same compilation\n-  # unit then constant-folding will remove the unnecessary branches. However,\n-  # if we run in eager mode the switch below cannot be constant-folded and\n-  # the compilation may fail if some of the branches contain custom calls not\n-  # recognized on the compilation platform. Detect eager mode and keep only the\n-  # needed branch.\n-  try:\n-    # Note/TODO(mvoz): This actually rarely seems to concretize - we could look into\n-    # core.ensure_compile_time_eval to get better single-branch selection.\n-    platform_index_concrete = core.concrete_or_error(operator.index, platform_index)\n-  except core.ConcretizationTypeError:\n-    return switch(platform_index, branches, *args)\n-  else:\n-    assert 0 <= platform_index_concrete < len(branches)\n-    return branches[platform_index_concrete](*args)\n+    branches_platforms = branches_platforms + (None,)  # type: ignore\n+  platform_index = platform_index_p.bind(platforms=branches_platforms)\n+\n+  if core.is_concrete(platform_index):\n+    return branches[int(platform_index)](*args)\n+  return _switch_internal(platform_index, branches, args,\n+                          branches_platforms=branches_platforms)\n+\n \n # A primitive to compute the index of a platform into a list of platforms.\n # Args:\n-#   platforms: Sequence[Sequence[str]]: a sequence of sequences of platform\n-#     names. If the current lowering platform is in one of the inner sequences\n-#     returns the index of that inner sequence in the outer sequence.\n-#   has_default: if True, and if the lowering platform is not found in\n-#     `platforms` then return `len(platforms)`. Otherwise, raise an error.\n+#   platforms: BranchesPlatforms. If the current lowering\n+#     platform is in one of the inner tuples returns the index of that inner\n+#     tuple in the outer tuple.\n platform_index_p = core.Primitive(\"platform_index\")\n platform_index_p.multiple_results = False\n platform_index_p.def_impl(functools.partial(dispatch.apply_primitive,\n@@ -1101,25 +1157,25 @@ def _platform_index_aval(*_, **__):\n \n def _platform_index_lowering(ctx: mlir.LoweringRuleContext,\n                              *,\n-                             platforms: Sequence[Sequence[str]],\n-                             has_default: bool):\n-  def lower_constant(\n-      ctx: mlir.LoweringRuleContext, *, i: int\n-  ) -> Sequence[ir.Value]:\n+                             platforms: BranchesPlatforms):\n+  def lower_constant(ctx: mlir.LoweringRuleContext, *,\n+                     i: int) -> Sequence[ir.Value]:\n     v = mlir.ir_constant(np.int32(i))\n-    assert isinstance(v, ir.Value), v\n     return [v]\n+\n   platform_rules: dict[str, mlir.LoweringRule] = {}\n+  default_rule = None\n   for i, ps in enumerate(platforms):\n     rule = partial(lower_constant, i=i)\n-    for p in ps:\n-      platform_rules[p] = rule\n+    if ps is None:\n+      default_rule = rule\n+    else:\n+      for p in ps:\n+        platform_rules[p] = rule\n \n-  default_rule = (\n-    partial(lower_constant, i=len(platforms)) if has_default else None)\n   return mlir.lower_per_platform(\n     ctx,\n-    f\"platform_index(platforms={platforms}, has_default={has_default})\",\n+    f\"platform_index(platforms={platforms})\",\n     platform_rules, default_rule, effects.no_effects)\n \n mlir.register_lowering(platform_index_p, _platform_index_lowering)\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 1ea5a048a17e..bba49c75f9df 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -47,7 +47,7 @@\n from jax._src.interpreters import partial_eval as pe\n from jax._src.lax import control_flow\n from jax._src.lax import lax as lax_internal\n-from jax._src.lax.control_flow import for_loop\n+from jax._src.lax.control_flow import for_loop, BranchesPlatforms\n from jax._src.lib import version as jaxlib_version\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import arith\n@@ -3100,7 +3100,7 @@ def _while_lowering_rule(\n \n \n @register_lowering_rule(lax.cond_p)\n-def _cond_lowering_rule(ctx: LoweringRuleContext, *args, branches):\n+def _cond_lowering_rule(ctx: LoweringRuleContext, *args, branches, **params):\n   index, *args = args\n   constant_index = _fold_and_get_constant_value(index)\n \n@@ -3870,17 +3870,13 @@ def _pad(val):\n def _platform_index_lowering(\n     ctx: mlir.LoweringRuleContext,\n     *,\n-    platforms: Sequence[Sequence[str]],\n-    has_default: bool,\n+    platforms: BranchesPlatforms,\n ):\n   for i, ps in enumerate(platforms):\n     # note - slightly odd structure here, as platforms is a seq[seq[str]]\n-    if \"mosaic\" in ps:\n+    if \"mosaic\" in ps or ps is None:\n       return ir_constant(i)\n \n-  if has_default:\n-    return ir_constant(len(platforms))\n-\n   raise NotImplementedError(\n       \"No mosaic or default platform indexing rule found.\"\n   )\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex b501693bf627..9ead4f16c1a6 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -2598,7 +2598,10 @@ def _while_lowering_rule(\n @register_lowering_rule(lax.cond_p,\n   mgpu.LoweringSemantics.Lane, gpu_core.PrimitiveSemantics.Warp)\n @register_lowering_rule(lax.cond_p, mgpu.LoweringSemantics.Warpgroup)\n-def _cond_lowering_rule(ctx: LoweringRuleContext, index, *args, branches):\n+def _cond_lowering_rule(ctx: LoweringRuleContext, index, *args, branches,\n+                        **params):\n+  if params:\n+    raise NotImplementedError(\"platform_dependent cond\")\n   index_aval, *_arg_avals = ctx.avals_in\n \n   def _yielded_values(outs, avals):\ndiff --git a/jax/experimental/jax2tf/jax2tf.py b/jax/experimental/jax2tf/jax2tf.py\nindex 4c2f35a95c57..786e021e2ff0 100644\n--- a/jax/experimental/jax2tf/jax2tf.py\n+++ b/jax/experimental/jax2tf/jax2tf.py\n@@ -3062,8 +3062,11 @@ def update_computation(arg1: TfVal, arg2: TfVal) -> TfVal:\n \n \n def _cond(\n-    index: TfVal, *operands: TfVal, branches: Sequence[core.ClosedJaxpr]\n+    index: TfVal, *operands: TfVal, branches: Sequence[core.ClosedJaxpr],\n+    **params\n ) -> Sequence[TfVal]:\n+  if params:\n+    raise NotImplementedError(\"jax2tf conversion for platform_dependent\")\n   # tf.cond needs lambdas with no arguments.\n   branches_tf = [\n       partial(_interpret_jaxpr, jaxpr, *operands,\ndiff --git a/tests/lax_control_flow_test.py b/tests/lax_control_flow_test.py\nindex 422ef769e392..d32d761ee1fa 100644\n--- a/tests/lax_control_flow_test.py\n+++ b/tests/lax_control_flow_test.py\n@@ -37,8 +37,10 @@\n from jax.ad_checkpoint import checkpoint as new_checkpoint, checkpoint_policies\n import jax.numpy as jnp  # scan tests use numpy\n import jax.scipy as jsp\n+from jax._src import dispatch\n from jax._src.lax import control_flow as lax_control_flow\n from jax._src.lax.control_flow import for_loop\n+from jax._src.interpreters import batching\n from jax._src.interpreters import mlir\n \n jax.config.parse_flags_with_absl()\n@@ -137,6 +139,36 @@ def scan_reference(f, init, xs):\n     lambda ctx, x: mlir.hlo.CustomCallOp(\n         [x.type], [x],\n         call_target_name=mlir.ir.StringAttr.get(\"__testing_non_existent_custom_call\")).results)\n+batching.primitive_batchers[prim_non_existent_custom_call] = (\n+    lambda batched_args, batch_dims: (prim_non_existent_custom_call.bind(batched_args[0]),\n+                                      batch_dims[0]))\n+\n+# A JAX primitive that triggers error when lowering on unintended platforms\n+prim_with_lowering_error = core.Primitive(\"__testing_prim_with_lowering_error\")\n+prim_with_lowering_error.def_abstract_eval(lambda x_aval, **_: x_aval)\n+def prim_with_lowering_error_lowering(platform: str,\n+                                      ctx: mlir.LoweringRuleContext, x, *,\n+                                      only_on: str):\n+  if platform != only_on:\n+    raise ValueError(f\"prim_with_lowering_error with only_on={only_on} lowered for {platform}\")\n+  return mlir.hlo.SineOp(x).results\n+def prim_with_lowering_error_batch_rule(batched_args, batch_dims, **params):\n+  xs, = batched_args\n+  xs_bdim, = batch_dims\n+  return prim_with_lowering_error.bind(xs, **params), xs_bdim\n+\n+batching.primitive_batchers[prim_with_lowering_error] = prim_with_lowering_error_batch_rule\n+\n+mlir.register_lowering(\n+    prim_with_lowering_error,\n+    partial(prim_with_lowering_error_lowering, \"cpu\"),\n+    platform=\"cpu\")\n+mlir.register_lowering(\n+    prim_with_lowering_error,\n+    partial(prim_with_lowering_error_lowering, \"tpu\"),\n+    platform=\"tpu\")\n+prim_with_lowering_error.def_impl(partial(dispatch.apply_primitive,\n+                                          prim_with_lowering_error))\n \n \n class LaxControlFlowTest(jtu.JaxTestCase):\n@@ -1378,7 +1410,7 @@ def f(x):\n   @parameterized.named_parameters(\n       {\"testcase_name\": f\"_{name}\", \"cond\": cond}\n       for cond, name in COND_IMPLS)\n-  def testCondGrad2(self, cond):\n+  def testCondGrad2(self, cond=cond_with_new_checkpoint):\n     def f_ref(x):\n       z = jnp.array([1., 2.], x.dtype) * x if x[0] < 2 else jnp.sin(x)\n       return z.sum()\n@@ -2905,18 +2937,13 @@ def f(x):\n     x = np.arange(3, dtype=np.float32)\n     lowered = jax.jit(f).lower(x)\n     stablehlo = lowered.as_text()\n-    self.assertIn(\"stablehlo.case\", stablehlo)\n-    self.assertIn(\"stablehlo.sine\", stablehlo)\n-    self.assertIn(\"stablehlo.cosine\", stablehlo)\n-\n-    # The HLO has been canonicalized and contains only the branch we need\n-    hlo = lowered.as_text(\"hlo\")\n+    # The StableHLO contains only the branch we need\n     if jtu.device_under_test() == \"cpu\":\n-      self.assertIn(\" sine\", hlo)\n-      self.assertNotIn(\" cosine\", hlo)\n+      self.assertIn(\"stablehlo.sine\", stablehlo)\n+      self.assertNotIn(\"stablehlo.cosine\", stablehlo)\n     else:\n-      self.assertNotIn(\" sine\", hlo)\n-      self.assertIn(\" cosine\", hlo)\n+      self.assertNotIn(\"stablehlo.sine\", stablehlo)\n+      self.assertIn(\"stablehlo.cosine\", stablehlo)\n \n   def test_platform_dependent_with_non_existent_custom_call(self):\n     if not jtu.test_device_matches([\"cpu\"]):\n@@ -2939,8 +2966,7 @@ def f(x):\n \n     x = np.arange(3, dtype=np.float32)\n     hlo = str(jax.jit(f).lower(x).compiler_ir())\n-    occurrences = re.findall(prim_non_existent_custom_call.name, hlo)\n-    self.assertLen(occurrences, 3)\n+    self.assertNotIn(prim_non_existent_custom_call.name, hlo)\n \n     res_eager = f(x)\n     self.assertAllClose(res_eager, 3. * np.sin(x))\n@@ -2956,6 +2982,26 @@ def f(x):\n     res_grad = jax.grad(f)(1.)\n     self.assertAllClose(res_grad, 3. * np.cos(1.))\n \n+  def test_platform_dependent_with_primitive_with_lowering_error(self):\n+    if not jtu.test_device_matches([\"cpu\", \"tpu\"]):\n+      self.skipTest(\"Only for CPU and TPU\")\n+\n+    def f(x):\n+      return lax.platform_dependent(\n+          x,\n+          # Check that we only lower on the intended platform\n+          cpu=lambda x: prim_with_lowering_error.bind(x, only_on=\"cpu\"),\n+          tpu=lambda x: prim_with_lowering_error.bind(x, only_on=\"tpu\"))\n+\n+    self.assertAllClose(np.sin(1.), f(1.))  # Eager\n+    self.assertAllClose(np.sin(1.), jax.jit(f)(1.))\n+    self.assertAllClose(np.sin(1.), lax.cond(True, f, lambda x: x, 1.))\n+    self.assertAllClose(1., lax.cond(False, f, lambda x: x, 1.))\n+    self.assertAllClose((0., np.sin(np.arange(8.))),\n+                        lax.scan(lambda carry, x: (carry, f(x)),\n+                                 0., np.arange(8.)))\n+    self.assertAllClose(np.sin(np.arange(8.)), jax.vmap(f)(np.arange(8.)))\n+\n   def test_platform_dependent_multiple_identical_branches(self):\n     x = np.arange(3, dtype=np.float32)\n     def f(x):\n@@ -2965,13 +3011,14 @@ def f(x):\n         tpu=jnp.sin,\n         default=lambda x: x)\n     res = f(x)\n+    on_cpu_tpu = jtu.device_under_test() in [\"cpu\", \"tpu\"]\n     self.assertAllClose(\n       res,\n-      np.sin(x) if jtu.device_under_test() in [\"cpu\", \"tpu\"] else x)\n-    # We only lower the common branches once\n+      np.sin(x) if on_cpu_tpu else x)\n+\n     stablehlo = jax.jit(f).lower(x).as_text()\n     sines = re.findall(r\"stablehlo.sine\", stablehlo)\n-    self.assertEqual(1, len(sines))\n+    self.assertEqual(1 if on_cpu_tpu else 0, len(sines))\n \n   def test_platform_dependent_no_default(self):\n     ctx = contextlib.ExitStack()\n\n```"
        ],
        "from_id": [
            "gnecula"
        ]
    },
    {
        "text_input": "Speed up `scipy.signal.stft` by using `lax.dynamic_slice_in_dim` for windowing",
        "output": [
            "```diff\nCommit: e65b317a4cef945eec0ed2378442df743fa1ee31\nDate: 2025-05-12T15:40:46Z\nURL: https://github.com/jax-ml/jax/commit/e65b317a4cef945eec0ed2378442df743fa1ee31\nFiles changed: 1\nAdditions: +3, Deletions: -7\ndiff --git a/jax/_src/scipy/signal.py b/jax/_src/scipy/signal.py\nindex 565909e8a6d1..f8c2563027f5 100644\n--- a/jax/_src/scipy/signal.py\n+++ b/jax/_src/scipy/signal.py\n@@ -566,13 +566,9 @@ def _fft_helper(x: Array, win: Array, detrend_func: Callable[[Array], Array],\n     result = x[..., np.newaxis]\n   else:\n     step = nperseg - noverlap\n-    batch_shape = list(batch_shape)\n-    x = x.reshape((math.prod(batch_shape), signal_length, 1))\n-    result = jax.lax.conv_general_dilated_patches(\n-        x, (nperseg,), (step,),\n-        'VALID',\n-        dimension_numbers=('NTC', 'OIT', 'NTC'))\n-    result = result.reshape(*batch_shape, *result.shape[-2:])\n+    starts = jnp.arange(signal_length - nperseg + 1, step=step)\n+    slice_func = partial(jax.lax.dynamic_slice_in_dim, operand=x, slice_size=nperseg, axis=-1)\n+    result = jax.vmap(slice_func, out_axes=-2)(start_index=starts)\n \n   # Detrend each data segment individually\n   result = detrend_func(result)\n\n```"
        ],
        "from_id": [
            "dsuedholt"
        ]
    },
    {
        "text_input": "[pallas:mosaic_gpu] Slightly generalized `MosaicGridMapping`\n\nPiperOrigin-RevId: 757755328",
        "output": [
            "```diff\nCommit: fc12df095e01e630fde9c9520a740b92977779a1\nDate: 2025-05-12T14:26:31Z\nURL: https://github.com/jax-ml/jax/commit/fc12df095e01e630fde9c9520a740b92977779a1\nFiles changed: 1\nAdditions: +6, Deletions: -4\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex b2a1f356ad0b..1ea5a048a17e 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -425,6 +425,7 @@ def __init__(\n       dynamic_shape_replacement_fn: Callable[\n           [tuple[jax.DimSize, ...]], tuple[int, ...]\n       ],\n+      arg_type_fn: Callable[..., ir.Type],\n   ):\n     self.grid = grid_mapping.grid\n     self.grid_names = grid_mapping.grid_names\n@@ -464,17 +465,17 @@ def __init__(\n     operand_avals = in_avals[grid_mapping.slice_block_ops]\n     scratch_avals = in_avals[grid_mapping.slice_scratch_ops]\n     self.scalar_prefetch_types, _ = unzip2([\n-        _get_arg_type(dynamic_shape_replacement_fn, aval, None)\n+        arg_type_fn(dynamic_shape_replacement_fn, aval, None)\n         for aval in scalar_prefetch_avals\n     ])\n     self.scalar_prefetch_block_shapes = tuple(\n         aval.shape for aval in scalar_prefetch_avals)\n     self.operand_types, self.operand_block_shapes = unzip2([\n-        _get_arg_type(dynamic_shape_replacement_fn, aval, block_mapping)\n+        arg_type_fn(dynamic_shape_replacement_fn, aval, block_mapping)\n         for aval, block_mapping in zip(operand_avals, self.block_mappings)\n     ])\n     self.scratch_types, _ = unzip2([\n-        _get_arg_type(dynamic_shape_replacement_fn, aval, None)\n+        arg_type_fn(dynamic_shape_replacement_fn, aval, None)\n         for aval in scratch_avals\n     ])\n     self.scratch_block_shapes = tuple(\n@@ -482,7 +483,7 @@ def __init__(\n         for aval in scratch_avals\n     )\n     self.grid_types, _ = unzip2([\n-        _get_arg_type(\n+        arg_type_fn(\n             dynamic_shape_replacement_fn,\n             pallas_core.index_map_grid_aval,\n             None,\n@@ -710,6 +711,7 @@ def dynamic_shape_replacement_fn(\n       dimension_semantics,\n       mesh,\n       dynamic_shape_replacement_fn,\n+      arg_type_fn=_get_arg_type,\n   )\n   mosaic_grid_mapping.maybe_compress_grid()\n   m = ir.Module.create()\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add support for TMEM loads/stores with the 32x32b shape\n\nThis should be useful for kernels such as FlashAttention since row-wise\nreductions can be performed entirely without any communication with other threads.\n\nPiperOrigin-RevId: 757746207",
        "output": [
            "```diff\nCommit: d99778cfe61b256479d3102ffa4a667e6f97a815\nDate: 2025-05-12T13:57:08Z\nURL: https://github.com/jax-ml/jax/commit/d99778cfe61b256479d3102ffa4a667e6f97a815\nFiles changed: 3\nAdditions: +234, Deletions: -68\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex c46e24b9ada2..89a58e4788f5 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -43,6 +43,20 @@\n     lane_dims=(-4, -3),\n     vector_dim=-1,\n )\n+# A layout resembling the logical organization of TMEM. The 128 rows in a tile\n+# are assigned to 128 lanes in the warpgroup. Useful when the result needs to be\n+# processed in registers and then stored back into TMEM. Should not be used if\n+# the result is to be written back to SMEM, as there is no good way to store it\n+# without bank conflicts.\n+#\n+# We use a vector_dim of 2, to be able to make sure that the vectors are always\n+# a multiple of 32-bits, even when the data is 16-bits.\n+TMEM_NATIVE_LAYOUT = fa.TiledLayout(\n+    fa.Tiling(((128, 2), (32, 2))),\n+    warp_dim=-4,\n+    lane_dims=(-2,),\n+    vector_dim=-1,\n+)\n \n \n def create_instr_descriptor(\n@@ -428,6 +442,8 @@ def _tmem_access_helper(shape, num):\n   if num.bit_count() != 1 or num > 128:\n     raise ValueError(f\"num must be a power of 2 and <= 128, got: {num}\")\n   match shape:\n+    case \"32x32b\":\n+      num_regs = 1\n     case \"16x128b\":\n       num_regs = 2\n     case \"16x256b\":\n@@ -657,43 +673,60 @@ def load(self, layout: fa.TiledLayout = LAYOUT):\n       raise NotImplementedError\n     if utils.bitwidth(self.dtype) not in {16, 32}:\n       raise NotImplementedError(f\"Unsupported dtype: {self.dtype}\")\n-    if layout != LAYOUT:\n+    if layout == LAYOUT:\n+      regs_shape = layout.registers_shape(self.shape)\n+      match self.layout:\n+        case TMEMLayout(elements_in_tile=(r, 8), packing=packing) if (\n+            r == TMEM_ROWS\n+        ):\n+          # load_32xcols returns a 4xN array, but the FA tiling we use here tiles\n+          # columns before rows, and so it is Nx4 (after ignoring all 1 dims).\n+          registers = _load_32xcols(\n+              self.address, self.shape[1], self.dtype, packing\n+          ).T.reshape(regs_shape)\n+        case TMEMLayout(elements_in_tile=(r, 128), column_tile_stride=2) if r == TMEM_ROWS:\n+          if self.shape[1] % 128 != 0:\n+            raise ValueError(\n+                f\"TMEM layout {self.layout} is not compatible with shape {self.shape}\"\n+            )\n+          num_column_tiles = self.shape[1] // 128\n+          column_tile_stride = self.layout.column_tile_stride\n+          num_strided_col_groups = utils.ceil_div(num_column_tiles, column_tile_stride)\n+          tiles = []\n+          for col_tile_base in range(num_strided_col_groups):\n+            for col_tile in range(col_tile_base, num_column_tiles, column_tile_stride):\n+              tiles.append(\n+                  _load_32xcols(\n+                      arith.addi(self.address, arith.constant(i32, col_tile * 128)),\n+                      cols=128,\n+                      dtype=self.dtype,\n+                      tmem_packing=1,\n+                  )\n+              )\n+          registers = np.concatenate(tiles, axis=1).T.reshape(regs_shape)\n+        case _:\n+          raise NotImplementedError(\n+              f\"Loads only implemented for refs with standard layout, got: {self.layout}\"\n+          )\n+    elif layout == TMEM_NATIVE_LAYOUT:\n+      regs_shape = layout.registers_shape(self.shape)\n+      match self.layout:\n+        case TMEMLayout(elements_in_tile=(r, c), packing=packing) if (\n+            r == TMEM_ROWS and c % 2 == 0\n+        ):\n+          registers = _load_32xcols_native(\n+              self.address, self.shape[1], self.dtype, packing\n+          ).reshape(regs_shape)\n+        case _:\n+          raise NotImplementedError(\n+              \"Loads only implemented for refs with standard layout, got:\"\n+              f\" {self.layout}\"\n+          )\n+    else:\n       raise ValueError(\n-          \"TMEM loads can only produce results in the tcgen05 layout\"\n-          f\" ({LAYOUT}), but got: {layout}\"\n+          \"TMEM loads can only produce results in the tcgen05 layouts\"\n+          f\" ({LAYOUT} and {TMEM_NATIVE_LAYOUT}), but got: {layout}\"\n       )\n-    regs_shape = layout.registers_shape(self.shape)\n-    match self.layout:\n-      case TMEMLayout(elements_in_tile=(r, 8), packing=packing) if r == TMEM_ROWS:\n-        # load_32xcols returns a 4xN array, but the FA tiling we use here tiles\n-        # columns before rows, and so it is Nx4 (after ignoring all 1 dims).\n-        registers = _load_32xcols(\n-            self.address, self.shape[1], self.dtype, packing\n-        ).T.reshape(regs_shape)\n-      case TMEMLayout(elements_in_tile=(r, 128), column_tile_stride=2) if r == TMEM_ROWS:\n-        if self.shape[1] % 128 != 0:\n-          raise ValueError(\n-              f\"TMEM layout {self.layout} is not compatible with shape {self.shape}\"\n-          )\n-        num_column_tiles = self.shape[1] // 128\n-        column_tile_stride = self.layout.column_tile_stride\n-        num_strided_col_groups = utils.ceil_div(num_column_tiles, column_tile_stride)\n-        tiles = []\n-        for col_tile_base in range(num_strided_col_groups):\n-          for col_tile in range(col_tile_base, num_column_tiles, column_tile_stride):\n-            tiles.append(\n-                _load_32xcols(\n-                    arith.addi(self.address, arith.constant(i32, col_tile * 128)),\n-                    cols=128,\n-                    dtype=self.dtype,\n-                    tmem_packing=1,\n-                )\n-            )\n-        registers = np.concatenate(tiles, axis=1).T.reshape(regs_shape)\n-      case _:\n-        raise NotImplementedError(\n-            f\"Loads only implemented for refs with standard layout, got: {self.layout}\"\n-        )\n     return fa.FragmentedArray(_registers=registers, _layout=layout, _is_signed=None)\n \n   def store(self, value):\n@@ -713,23 +746,39 @@ def store(self, value):\n           f\"Stored array has dtype {value.mlir_dtype}, but TMEM has dtype\"\n           f\" {self.dtype}\"\n       )\n-    if value.layout != LAYOUT:\n+    if value.layout == LAYOUT:\n+      # TODO(apaszke): Collective MMA layout\n+      match self.layout:\n+        case TMEMLayout(elements_in_tile=(r, 8), packing=packing) if (\n+            r == TMEM_ROWS\n+        ):\n+          # store_32xcols needs a 4xN array, but the FA tiling we use here tiles\n+          # columns before rows, and so it is Nx4 (after ignoring all 1 dims).\n+          _store_32xcols(\n+              self.address, value.registers.T.reshape((4, -1)), packing\n+          )\n+        case _:\n+          raise NotImplementedError(\n+              f\"Stores only implemented for refs with standard layout, got: {self.layout}\"\n+          )\n+    elif value.layout == TMEM_NATIVE_LAYOUT:\n+      # TODO(apaszke): Collective MMA layout\n+      match self.layout:\n+        case TMEMLayout(elements_in_tile=(r, c), packing=packing) if (\n+            r == TMEM_ROWS and c % 2 == 0\n+        ):\n+          _store_32xcols_native(\n+              self.address, value.registers.reshape(-1), packing\n+          )\n+        case _:\n+          raise NotImplementedError(\n+              f\"Stores only implemented for refs with standard layout, got: {self.layout}\"\n+          )\n+    else:\n       raise ValueError(\n-          f\"Stored array has layout {value.layout}, but only tcgen05.LAYOUT is\"\n-          \" supported\"\n+          f\"Stored array has layout {value.layout}, but only tcgen05.LAYOUT and\"\n+          \" tcgen05.TMEM_NATIVE_LAYOUT are supported\"\n       )\n-    # TODO(apaszke): Collective MMA layout\n-    match self.layout:\n-      case TMEMLayout(elements_in_tile=(r, 8), packing=packing) if r == TMEM_ROWS:\n-        # store_32xcols needs a 4xN array, but the FA tiling we use here tiles\n-        # columns before rows, and so it is Nx4 (after ignoring all 1 dims).\n-        _store_32xcols(\n-            self.address, value.registers.T.reshape((4, -1)), packing\n-        )\n-      case _:\n-        raise NotImplementedError(\n-            f\"Stores only implemented for refs with standard layout, got: {self.layout}\"\n-        )\n \n   def _debug_print(self):\n     i32 = ir.IntegerType.get_signless(32)\n@@ -756,28 +805,43 @@ def _debug_print(self):\n       utils.debug_print(f\"[{{}}, {c}]: {{}}\", lane, val, uniform=False)\n \n \n-def _transfer_32xcols(base_addr: ir.Value, cols: int, packing: int):\n+def _transfer_32xcols(\n+    base_addr: ir.Value,\n+    cols: int,\n+    atom_shape: tuple[int, int],\n+    tmem_packing: int,\n+    reg_packing: int,\n+):\n+  \"\"\"Generates a sequence of parameters for a given TMEM read or write.\n+\n+  Arguments:\n+    base_addr: The base address of the TMEM region.\n+    cols: The number of logical columns to transfer.\n+    atom_shape: The logical shape of the tile written by the warp in a single\n+      TMEM transfer.\n+    tmem_packing: Packing degree in TMEM. When packing is 1, but the data is\n+      16-bit, we expect that each transfer actually involves double the number\n+      of physical columns.\n+    reg_packing: The number of elements that fit in a single 32-bit register.\n+  \"\"\"\n   i32 = ir.IntegerType.get_signless(32)\n-  cols_per_num = 8  # Here we generate a plan compatible with tcgen05.LAYOUT.\n-  assert cols % cols_per_num == 0\n-  total_num = cols // cols_per_num\n+  atom_rows, atom_cols = atom_shape\n+  assert cols % atom_cols == 0\n+  total_num = cols // atom_cols\n   assert total_num.bit_count() == 1\n+  regs_per_instr = atom_shape[0] * atom_shape[1] // (utils.WARP_SIZE * reg_packing)\n   # We artificially lower the instr_num compared to its limits, because higher\n   # values can lead to register spills..\n-  if total_num <= 16:\n-    instr_num = total_num\n-  elif 32 <= total_num <= 64:\n-    instr_num = 16\n-  else:\n-    raise NotImplementedError(total_num)\n-  # We transfer 16 lanes at a time, but have 32 to deal with.\n-  for lane_step in range(2):\n-    addr_row = arith.addi(base_addr, utils.c((lane_step * 16) << 16, i32))\n-    cols_per_instr = instr_num * cols_per_num\n+  instr_num = min(total_num, 64 // regs_per_instr)\n+  assert 32 % atom_rows == 0\n+  num_row_steps = 32 // atom_rows\n+  for lane_step in range(num_row_steps):\n+    addr_row = arith.addi(base_addr, utils.c((lane_step * atom_rows) << 16, i32))\n+    cols_per_instr = instr_num * atom_cols\n     for num_step in range(total_num // instr_num):\n       num_slice = slice(num_step * instr_num, (num_step + 1) * instr_num)\n       addr_row_col = arith.addi(\n-          addr_row, utils.c(num_step * cols_per_instr // packing, i32)\n+          addr_row, utils.c(num_step * cols_per_instr // tmem_packing, i32)\n       )\n       yield addr_row_col, instr_num, lane_step, num_slice\n \n@@ -813,12 +877,44 @@ def _store_32xcols(base_addr, vector_regs, tmem_packing):\n   else:\n     raise NotImplementedError(reg_packing)\n \n-  it = _transfer_32xcols(base_addr, cols, tmem_packing)\n+  it = _transfer_32xcols(base_addr, cols, (16, 8), tmem_packing, reg_packing)\n   for addr_row_col, instr_num, lane_step, num_slice in it:\n     regs_slice = regs[lane_step, num_slice].flat\n     tmem_store(addr_row_col, store_shape, instr_num, regs_slice, unpack)\n \n \n+def _store_32xcols_native(base_addr, vector_regs, tmem_packing):\n+  i32 = ir.IntegerType.get_signless(32)\n+  assert vector_regs.ndim == 1\n+  cols = len(vector_regs) * TMEM_NATIVE_LAYOUT.vector_length\n+\n+  reg_packing = 64 // utils.bitwidth(vector_regs.flat[0].type)\n+  store_shape = \"32x32b\"\n+  if reg_packing == 1:\n+    store_atom_shape = (32, 1)\n+    regs = [None] * (len(vector_regs) * 2)\n+    c0 = arith.constant(i32, 0)\n+    c1 = arith.constant(i32, 1)\n+    for idx, vreg in enumerate(vector_regs):\n+      regs[2 * idx] = llvm.extractelement(vreg, c0)\n+      regs[2 * idx + 1] = llvm.extractelement(vreg, c1)\n+    assert tmem_packing == 1\n+    unpack = False\n+  elif reg_packing == 2:\n+    store_atom_shape = (32, 2)\n+    regs = vector_regs\n+    assert 1 <= tmem_packing <= 2\n+    unpack = tmem_packing == 1\n+  else:\n+    raise NotImplementedError(reg_packing)\n+\n+  it = _transfer_32xcols(base_addr, cols, store_atom_shape, tmem_packing, reg_packing)\n+  for addr_row_col, instr_num, lane_step, num_slice in it:\n+    assert lane_step == 0\n+    regs_slice = regs[num_slice]\n+    tmem_store(addr_row_col, store_shape, instr_num, regs_slice, unpack)\n+\n+\n def _load_32xcols(base_addr, cols, dtype, tmem_packing):\n   i32 = ir.IntegerType.get_signless(32)\n   vec_ty = ir.VectorType.get((2,), dtype)\n@@ -836,7 +932,7 @@ def _load_32xcols(base_addr, cols, dtype, tmem_packing):\n \n   vector_regs = np.ndarray((4, cols // 8), dtype=object)\n \n-  it = _transfer_32xcols(base_addr, cols, tmem_packing)\n+  it = _transfer_32xcols(base_addr, cols, (16, 8), tmem_packing, reg_packing)\n   c0 = arith.constant(i32, 0)\n   c1 = arith.constant(i32, 1)\n   for addr_row_col, instr_num, lane_step, num_slice in it:\n@@ -868,6 +964,50 @@ def _load_32xcols(base_addr, cols, dtype, tmem_packing):\n   return vector_regs\n \n \n+def _load_32xcols_native(base_addr, cols, dtype, tmem_packing):\n+  i32 = ir.IntegerType.get_signless(32)\n+  vec_ty = ir.VectorType.get((2,), dtype)\n+  reg_packing = 32 // utils.bitwidth(dtype)\n+  load_shape = \"32x32b\"\n+  if reg_packing == 1:\n+    load_atom_shape = (32, 1)\n+    assert tmem_packing == 1\n+    pack = False\n+  elif reg_packing == 2:\n+    load_atom_shape = (32, 2)\n+    assert 1 <= tmem_packing <= 2\n+    pack = tmem_packing == 1\n+  else:\n+    raise NotImplementedError(reg_packing)\n+\n+  it = _transfer_32xcols(base_addr, cols, load_atom_shape, tmem_packing, reg_packing)\n+  c0 = arith.constant(i32, 0)\n+  c1 = arith.constant(i32, 1)\n+  regs = [None] * (cols // reg_packing)\n+  for addr_row_col, instr_num, lane_step, num_slice in it:\n+    assert lane_step == 0, lane_step\n+    instr_regs = tmem_load(addr_row_col, load_shape, instr_num, pack)\n+    if reg_packing == 1:\n+      regs[num_slice] = [llvm.bitcast(dtype, r) for r in instr_regs]\n+    else:\n+      assert reg_packing == 2\n+      regs[num_slice] = [llvm.bitcast(vec_ty, r) for r in instr_regs]\n+\n+  if reg_packing == 1:\n+    vector_regs = np.ndarray((cols // 2,), dtype=object)\n+    undef = llvm.mlir_undef(vec_ty)\n+    for idx in range(vector_regs.size):\n+      high_undef = llvm.insertelement(undef, regs[2 * idx], c0)\n+      vreg = llvm.insertelement(high_undef, regs[2 * idx + 1], c1)\n+      vector_regs[idx] = vreg\n+  else:\n+    assert reg_packing == 2\n+    vector_regs = np.asarray(regs, dtype=object)\n+\n+  assert vector_regs.shape == (cols // TMEM_NATIVE_LAYOUT.vector_length,)\n+  return vector_regs\n+\n+\n def _m128_layout(shape: tuple[int, ...]):\n   if len(shape) != 2:\n     raise ValueError(f\"Shape {shape} is not 2D\")\ndiff --git a/jax/experimental/mosaic/gpu/utils.py b/jax/experimental/mosaic/gpu/utils.py\nindex bd11c3a07544..bf0b06ccb9c9 100644\n--- a/jax/experimental/mosaic/gpu/utils.py\n+++ b/jax/experimental/mosaic/gpu/utils.py\n@@ -40,6 +40,7 @@\n \n # mypy: ignore-errors\n \n+WARP_SIZE: int = 32\n WARPGROUP_SIZE: int = 128\n DYNAMIC = -9223372036854775808\n DYNAMIC32 = -2147483648\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex e02acf8cce13..03ded0ac446c 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -906,7 +906,7 @@ def setUp(self):\n       self.skipTest(\"Only works on GPU with capability sm_100a or sm_101a\")\n \n   @parameterized.parameters([(jnp.float32, 1), (jnp.float16, 1), (jnp.float16, 2)])\n-  def test_load_store_tmem(self, jax_dtype, packing):\n+  def test_load_store_tmem_swizzle(self, jax_dtype, packing):\n     swizzle = 128\n     in_mlir_dtype = utils.dtype_to_ir_type(jax_dtype)\n     swizzle_elems = swizzle // bytewidth(in_mlir_dtype)\n@@ -942,6 +942,31 @@ def kernel(ctx, input, output, scratch):\n     )(x)\n     np.testing.assert_array_equal(x, y)\n \n+  @parameterized.parameters([(jnp.float32, 1), (jnp.float16, 1), (jnp.float16, 2)])\n+  def test_load_store_tmem_native(self, jax_dtype, packing):\n+\n+    def kernel(ctx, input, output, scratch):\n+      smem, barrier, tmem = scratch\n+      ctx.async_copy(src_ref=input, dst_ref=smem, barrier=barrier)\n+      barrier.wait()\n+      tmem.store(fa.FragmentedArray.load_untiled(smem, layout=tcgen05.TMEM_NATIVE_LAYOUT, optimized=False))\n+      tcgen05.commit_tmem()\n+      tmem.load(tcgen05.TMEM_NATIVE_LAYOUT).store_untiled(smem, optimized=False)\n+      mgpu.commit_shared()\n+      ctx.async_copy(src_ref=smem, dst_ref=output)\n+      ctx.await_async_copy(0)\n+\n+    x = self.prng.uniform(-1, 1, (128, 128)).astype(jax_dtype)\n+    scratch_shape = [\n+        jax.ShapeDtypeStruct(x.shape, jax_dtype),\n+        mgpu.TMABarrier(),\n+        mgpu.TMEM(x.shape, jax_dtype, packing=packing),\n+    ]\n+    y = mgpu.as_gpu_kernel(\n+        kernel, (1, 1, 1), (128, 1, 1), x, x, scratch_shape\n+    )(x)\n+    np.testing.assert_array_equal(x, y)\n+\n   @parameterized.parameters([\n       (jnp.float32, 1, \"130.0000\"),\n       (jnp.float16, 1, \"130.0000\"),\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Use f8e4m3fn in place of f8e4m3\n\nPTX docs are a bit confusing because the type is called e4m3, but\n[its description](https://docs.nvidia.com/cuda/parallel-thread-execution/#alternate-floating-point-data-formats)\nindicates that it is actually e4m3fn (no infs, limited NaNs).\n\nPiperOrigin-RevId: 757741649",
        "output": [
            "```diff\nCommit: f5c63053eb2ee59edfd669feb95cf81626b4cd48\nDate: 2025-05-12T13:41:06Z\nURL: https://github.com/jax-ml/jax/commit/f5c63053eb2ee59edfd669feb95cf81626b4cd48\nFiles changed: 3\nAdditions: +5, Deletions: -5\ndiff --git a/jax/experimental/mosaic/gpu/launch_context.py b/jax/experimental/mosaic/gpu/launch_context.py\nindex 02ed3859d8c2..eccb363f7537 100644\n--- a/jax/experimental/mosaic/gpu/launch_context.py\n+++ b/jax/experimental/mosaic/gpu/launch_context.py\n@@ -467,7 +467,7 @@ def init_tma_desc(host_ptr):\n           # We treat 8 bit floats as 8 bit integers\n           elif ir.Float8E5M2Type.isinstance(ref_ty.element_type):\n             tma_dtype = 1\n-          elif ir.Float8E4M3Type.isinstance(ref_ty.element_type):\n+          elif ir.Float8E4M3FNType.isinstance(ref_ty.element_type):\n             tma_dtype = 1\n           else:\n             raise ValueError(f\"unsupported TMA dtype {ref_ty.element_type}\")\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex 730761cb7eff..c46e24b9ada2 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -74,7 +74,7 @@ def create_instr_descriptor(\n     desc = 0\n     desc |= (acc_dtype == f32) << 4  # D dtype, bits 4-5\n     # Bit 6 is reserved\n-    if input_dtype == ir.Float8E4M3Type.get():\n+    if input_dtype == ir.Float8E4M3FNType.get():\n       input_dtype_enum = 0\n     elif input_dtype == ir.Float8E5M2Type.get():\n       input_dtype_enum = 1\n@@ -173,7 +173,7 @@ def mma(\n       )\n   elif any(\n       t.isinstance(element_type)\n-      for t in {ir.F16Type, ir.Float8E5M2Type, ir.Float8E4M3Type}\n+      for t in {ir.F16Type, ir.Float8E5M2Type, ir.Float8E4M3FNType}\n   ):\n     if d.dtype != f16 and d.dtype != f32:\n       raise ValueError(\n@@ -299,7 +299,7 @@ def _do_mma(\n     kind = \"f16\"\n   elif ir.Float8E5M2Type.isinstance(element_type):\n     kind = \"f8f6f4\"\n-  elif ir.Float8E4M3Type.isinstance(element_type):\n+  elif ir.Float8E4M3FNType.isinstance(element_type):\n     kind = \"f8f6f4\"\n   else:\n     raise NotImplementedError(f\"Unsupported input element type: {element_type}\")\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex b9350b0c995b..e02acf8cce13 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -983,7 +983,7 @@ def kernel(ctx, input, output, scratch):\n   @parameterized.product(\n       lhs_transpose=(False, True),\n       rhs_transpose=(False, True),\n-      in_jax_dtype=(jnp.float16, jnp.bfloat16, jnp.float8_e5m2, jnp.float8_e4m3),  # TODO(apaszke): f32\n+      in_jax_dtype=(jnp.float16, jnp.bfloat16, jnp.float8_e5m2, jnp.float8_e4m3fn),  # TODO(apaszke): f32\n       out_jax_dtype=(jnp.float16, jnp.float32,),\n       m=(128,),  # TODO(apaszke): 64, 192, 256\n       n=(64, 128, 256, 512),  # TODO(apaszke): 192, other non-power-of-2\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add collective_axes to run_scoped\n\nOur current allocation scheme on GPU is unsafe in presence of multiple threads\nthat might take diverging control paths. We work around this problem using our\nfavorite trick and simply forbid this!\n\nWith this change, `run_scoped(..., collective_axes=\"wg\")` means that the same\nallocation will be returned in all programs that only differ in the `wg` axis.\nWhat's more, this call is a user promise that the allocation is a collective that\nwill be executed by all threads along that axis. Only executing it on a subset is\nundefined behavior and in our current Mosaic GPU implementation might lead to deadlocks\ndue to barriers.\n\nNote that nothing changes for single-threaded kernels, where run_scoped is always\nallowed.\n\nPiperOrigin-RevId: 757734362",
        "output": [
            "```diff\nCommit: bd8765d3832ced4cc5f0482ef65cb19a16d22dad\nDate: 2025-05-12T13:16:04Z\nURL: https://github.com/jax-ml/jax/commit/bd8765d3832ced4cc5f0482ef65cb19a16d22dad\nFiles changed: 8\nAdditions: +97, Deletions: -15\ndiff --git a/jax/_src/pallas/hlo_interpreter.py b/jax/_src/pallas/hlo_interpreter.py\nindex f3d2c46ad9a9..755df2cd8ceb 100644\n--- a/jax/_src/pallas/hlo_interpreter.py\n+++ b/jax/_src/pallas/hlo_interpreter.py\n@@ -312,9 +312,15 @@ def rule(interpreter, *args, **params):\n     lax.while_p, 'body_jaxpr', 'cond_jaxpr')\n _eval_jaxpr_hop_rules[lax.cond_p] = make_hop_rule(lax.cond_p, 'branches')\n def _run_scoped_physicalize_rule(\n-    interpreter, *consts, jaxpr: jax_core.Jaxpr):\n+    interpreter, *consts, jaxpr: jax_core.Jaxpr, collective_axes):\n+  if collective_axes:\n+    raise NotImplementedError(\n+        \"run_scoped interpret rule does not support collective axes\"\n+    )\n   physical_jaxpr, physical_consts = interpreter(jaxpr, consts)\n-  return primitives.run_scoped_p.bind(*physical_consts, jaxpr=physical_jaxpr)\n+  return primitives.run_scoped_p.bind(\n+      *physical_consts, jaxpr=physical_jaxpr, collective_axes=collective_axes\n+  )\n _eval_jaxpr_hop_rules[primitives.run_scoped_p] = _run_scoped_physicalize_rule\n \n \ndiff --git a/jax/_src/pallas/mosaic/interpret.py b/jax/_src/pallas/mosaic/interpret.py\nindex f7160f5af386..c0e52f54e6f3 100644\n--- a/jax/_src/pallas/mosaic/interpret.py\n+++ b/jax/_src/pallas/mosaic/interpret.py\n@@ -1169,6 +1169,10 @@ def f(*args, jaxpr):\n         out = pjit.pjit_p.bind(*invals, **(eqn.params | {'jaxpr': new_jaxpr}))\n \n       elif prim is primitives.run_scoped_p:\n+        if eqn.params['collective_axes']:\n+          raise NotImplementedError(\n+              'run_scoped_p with collective axes is not supported'\n+          )\n         # Allocate a buffer or semaphore for each element of\n         # eqn.params['jaxpr'].invars .\n         allocs = []\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex cdc64bbf96f4..b2a1f356ad0b 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -3347,7 +3347,9 @@ def _alloc_value(\n \n \n @register_lowering_rule(primitives.run_scoped_p)\n-def _run_scoped_lowering_rule(ctx: LoweringRuleContext, *consts, jaxpr):\n+def _run_scoped_lowering_rule(ctx: LoweringRuleContext, *consts, jaxpr, collective_axes):\n+  if collective_axes:\n+    raise NotImplementedError(\"run_scoped lowering does not support collective axes\")\n   out_type = [\n       aval_to_ir_type(ctx.lowering_context.dynamic_shape_replacement_fn, aval)\n       for aval in ctx.avals_out\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 6be8b3c4a8a5..21c78720e812 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -182,13 +182,16 @@ def kernel(\n   def wrapper(*operands):\n     def stateful(operand_and_out_refs):\n       operand_refs, out_refs = operand_and_out_refs\n+      mesh = GPUMesh(**mesh_kwargs)\n+      thread_name = mesh.thread_name if mesh.thread_name is not None else ()\n       def cmap_body():\n         pallas_primitives.run_scoped(\n             lambda *scratch_refs: body(*operand_refs, *out_refs, *scratch_refs),\n             *scratch_shapes,\n+            collective_axes=thread_name,\n         )\n       pallas_core.core_map(\n-          GPUMesh(**mesh_kwargs), compiler_params=compiler_params\n+          mesh, compiler_params=compiler_params\n       )(cmap_body)\n     _, outs = state_discharge.run_state(stateful)(\n         (operands, jax.tree.map(jnp.zeros_like, out_shape))\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 2ef504e518f6..b501693bf627 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -215,8 +215,11 @@ def _while_resource_estimator(\n \n @_register_resource_estimator(primitives.run_scoped_p)\n def _run_scoped_resource_estimator(\n-    ctx: ResourceEstimatorContext, *consts, jaxpr: jax_core.Jaxpr\n+    ctx: ResourceEstimatorContext, *consts, jaxpr: jax_core.Jaxpr, collective_axes\n ) -> int:\n+  # NOTE: This rule assumes that the allocation happens collectively, although\n+  # it can't be checked here due to limited context. We check this in the actual\n+  # lowering rule.\n   del consts  # Unused.\n   rs = Resources()\n   for v in jaxpr.invars:\n@@ -298,7 +301,7 @@ def __iter__(self) -> Iterable[Hashable]:\n @dataclasses.dataclass\n class ModuleContext:\n   name: str\n-  axis_names: _AxisNames | None\n+  axis_names: _AxisNames\n   program_ids: Sequence[ir.Value] | None\n   approx_math: bool\n   single_wg_lane_predicate: ir.Value | None\n@@ -602,9 +605,13 @@ def lower_pipelined_jaxpr_to_module(\n     assert isinstance(gpu_mesh, gpu_core.GPUMesh)\n     block = (128 * (gpu_mesh.num_threads or 1), 1, 1)\n     grid = gpu_mesh.grid\n+    thread_axis = (\n+        gpu_mesh.thread_name if gpu_mesh.thread_name is not None else ()\n+    )\n   else:\n     block = (128, 1, 1)\n     grid = grid_mapping.grid\n+    thread_axis = ()\n \n   if params.dimension_semantics is None:\n     which_parallel = [True] * len(grid)\n@@ -659,6 +666,7 @@ def pipeline_fn(*refs):\n             ref_for_aval(aval) if aval is not sem_placeholder else aval\n             for aval in scratch_avals\n         ],\n+        collective_axes=thread_axis,  # scratch_refs are shared across threads\n     )\n     return ()  # ``wrap_init`` does not support functions returning None.\n \n@@ -1937,6 +1945,14 @@ def _reduce_sum_lowering_rule(ctx: LoweringRuleContext, x, *, axes):\n     case mgpu.WGStridedFragLayout():\n       if set(axes) != set(range(x_aval.ndim)):\n         raise NotImplementedError(\"No support for axes yet\")\n+      # To relax the restriction below, you need to ensure sufficient\n+      # synchronization with other places that use `scratch_view` (which at the\n+      # time of writing is only `run_scoped`).\n+      if ctx.module_ctx.axis_names.wg is not None:\n+        raise NotImplementedError(\n+            \"No support for reduce_sum over all axes and multiple Pallas\"\n+            \" threads\"\n+        )\n       scratch_ty = jax.ShapeDtypeStruct(shape=(4,), dtype=x_aval.dtype)\n       with ctx.module_ctx.scratch_view([scratch_ty]) as [scratch]:\n         return x.reduce(\"add\", axes, scratch)\n@@ -2178,14 +2194,28 @@ def _debug_print_lowering_rule_wg(\n @register_lowering_rule(primitives.run_scoped_p, mgpu.LoweringSemantics.Lane)\n @register_lowering_rule(primitives.run_scoped_p, mgpu.LoweringSemantics.Warpgroup)\n def _run_scoped_lowering_rule(\n-    ctx: LoweringRuleContext, *consts, jaxpr: jax_core.Jaxpr\n+    ctx: LoweringRuleContext, *consts, jaxpr: jax_core.Jaxpr, collective_axes\n ):\n   input_refs = []\n   should_discharge = []\n+  wg_axis = ctx.module_ctx.axis_names.wg\n+  is_multithreaded = wg_axis is not None\n+  is_thread_collective = is_multithreaded and collective_axes == (wg_axis,)\n+  # Make sure everyone has exited previous scoped allocations. Note that we\n+  # don't synchronize when we exit the allocation, but only when we might want\n+  # to reuse its memory again.\n+  if is_multithreaded and is_thread_collective:\n+    gpu_dialect.barrier()\n   with contextlib.ExitStack() as alloc_stack:\n     for v in jaxpr.invars:\n       aval = v.aval\n       if isinstance(aval, gpu_core.WGMMAAbstractAccumulatorRef):\n+        if collective_axes:\n+          raise ValueError(\n+              \"WGMMA accumulators can only be allocated non-collectively. Hint:\"\n+              \" remove collective_axes from run_scoped. If other allocations\"\n+              \" are performed as well, split the run_scoped into two.\"\n+          )\n         dtype = mlir.dtype_to_ir_type(aval.dtype)\n         if ctx.module_ctx.lowering_semantics == mgpu.LoweringSemantics.Lane:\n           input_refs.append(mgpu.WGMMAAccumulator.zero(*aval.shape, dtype))\n@@ -2196,7 +2226,17 @@ def _run_scoped_lowering_rule(\n           nvvm_dialect.wgmma_fence_aligned()\n           input_refs.append(acc)\n         should_discharge.append(True)\n-      elif isinstance(aval.dtype, gpu_core.BarrierType):\n+        continue\n+      # All other allocations must be made collectively across all threads.\n+      if is_multithreaded and not is_thread_collective:\n+        raise NotImplementedError(\n+            \"Only thread-collective allocations are supported in multithreaded\"\n+            \" kernels. Hint: add\"\n+            f\" collective_axes={ctx.module_ctx.axis_names.wg} to your\"\n+            \" run_scoped if you intend all threads to share the same\"\n+            f\" allocation (currently collective_axes={collective_axes}).\"\n+        )\n+      if isinstance(aval.dtype, gpu_core.BarrierType):\n         multiplier = (1 if aval.dtype.for_tensor_core else\n                       ctx.estimator_ctx.arrival_multiplier)\n         barrier_ref = alloc_stack.enter_context(\ndiff --git a/jax/_src/pallas/mosaic_gpu/pipeline.py b/jax/_src/pallas/mosaic_gpu/pipeline.py\nindex 426f314bc3a1..db5fb4fb316a 100644\n--- a/jax/_src/pallas/mosaic_gpu/pipeline.py\n+++ b/jax/_src/pallas/mosaic_gpu/pipeline.py\n@@ -558,6 +558,7 @@ def pipeline(*gmem_refs: pallas_core.AbstractMemoryRef):\n         out_smem_refs=out_smem_refs,\n         in_smem_barrier_refs=in_smem_barriers,\n         consumed_barrier_refs=consumed_barriers,\n+        collective_axes=wg_axis,\n     )\n \n   def scoped_pipeline(\ndiff --git a/jax/_src/pallas/primitives.py b/jax/_src/pallas/primitives.py\nindex 986a62571010..5038ac6e5171 100644\n--- a/jax/_src/pallas/primitives.py\n+++ b/jax/_src/pallas/primitives.py\n@@ -19,6 +19,7 @@\n import enum\n import functools\n import string\n+from collections.abc import Hashable\n from typing import Any, Callable\n \n import jax\n@@ -878,13 +879,25 @@ def wrap_with_transforms(f, transforms, *args):\n run_scoped_p.multiple_results = True\n \n \n-def run_scoped(f: Callable[..., Any], *types: Any, **kw_types: Any) -> Any:\n+def run_scoped(\n+    f: Callable[..., Any],\n+    *types: Any,\n+    collective_axes: Hashable | tuple[Hashable, ...] = (),\n+    **kw_types: Any,\n+) -> Any:\n   \"\"\"Calls the function with allocated references and returns the result.\n \n   The positional and keyword arguments describe which reference types\n   to allocate for each argument. Each backend has its own set of reference\n   types in addition to :class:`jax.experimental.pallas.MemoryRef`.\n+\n+  When `collective_axes` is specified, the same allocation will be returned for\n+  all programs that only differ in their program ids along the collective axes.\n+  It is an error not to call the same `run_scoped` in all programs along that\n+  axis.\n   \"\"\"\n+  if not isinstance(collective_axes, tuple):\n+    collective_axes = (collective_axes,)\n   flat_types, in_tree = tree_util.tree_flatten((types, kw_types))\n   flat_fun, out_tree_thunk = api_util.flatten_fun(\n       lu.wrap_init(f,\n@@ -908,13 +921,13 @@ def run_scoped(f: Callable[..., Any], *types: Any, **kw_types: Any) -> Any:\n   # are not in the invars of an operation so we just put them all\n   # there.\n   jaxpr, _, consts, () = pe.trace_to_jaxpr_dynamic(flat_fun, avals)\n-  out = run_scoped_p.bind(*consts, jaxpr=jaxpr)\n+  out = run_scoped_p.bind(*consts, jaxpr=jaxpr, collective_axes=collective_axes)\n   return tree_util.tree_unflatten(out_tree_thunk(), out)\n \n \n @run_scoped_p.def_effectful_abstract_eval\n-def _run_scoped_abstract_eval(*args, jaxpr):\n-  del args\n+def _run_scoped_abstract_eval(*args, jaxpr, collective_axes):\n+  del args, collective_axes\n   # jaxpr will have effects for its inputs (Refs that are allocated) and for\n   # constvars (closed over Refs). The effects for the allocated Refs are local\n   # to the jaxpr and shouldn't propagate out.\n@@ -935,8 +948,12 @@ def _run_scoped_discharge_rule(\n     out_avals,\n     *args_flat,\n     jaxpr,\n-    **_):\n+    collective_axes):\n   del out_avals\n+  if collective_axes:\n+    raise NotImplementedError(\n+        \"run_scoped discharge does not support collective_axes yet.\"\n+    )\n   num_consts = len(args_flat)\n   # discharge_state only discharges invars, not consts, so in order to\n   # discharge the requested refs we need to move them to the invar set.\n@@ -956,7 +973,9 @@ def _run_scoped_discharge_rule(\n \n   # Run_scoped discharged the external variables but the scoped ones\n   # are not discharged.\n-  out = run_scoped_p.bind(*args_flat, jaxpr=discharged_body)\n+  out = run_scoped_p.bind(\n+      *args_flat, jaxpr=discharged_body, collective_axes=collective_axes\n+  )\n   # Order of outputs:\n   # (1) return values, (2) closed refs, (3) scoped refs.\n   return_values = out[:num_return_values]\n@@ -975,7 +994,12 @@ def _run_scoped_discharge_rule(\n \n \n @functools.partial(mlir.register_lowering, run_scoped_p)\n-def _run_scoped_lowering_rule(ctx, *args, jaxpr):\n+def _run_scoped_lowering_rule(ctx, *args, jaxpr, collective_axes):\n+  if collective_axes:\n+    raise ValueError(\n+        \"run_scoped lowering outside of Pallas does not support\"\n+        \" collective_axes.\"\n+    )\n   jaxpr_noconst = pe.convert_constvars_jaxpr(jaxpr)\n   num_return_values = len(jaxpr_noconst.outvars)\n   discharged_body, new_consts = state_discharge.discharge_state(\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex e7a9898bf9f3..3256953cd332 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -245,6 +245,7 @@ def entry(q_ref, k_ref, v_ref, out_ref, lse_ref):\n         ),\n         (plgpu.Barrier(num_arrivals=compute_wgs, num_barriers=max_concurrent_steps),) * 2,\n         plgpu.Barrier(num_arrivals=compute_wgs),\n+        collective_axes=\"wg\",\n     )\n \n   num_q_tiles, rem = divmod(q_seq_len, block_q * 2)\n@@ -740,6 +741,7 @@ def _kernel_entry():\n           scratch,\n           plgpu.Barrier(1, num_barriers=compute_wgs),\n           plgpu.Barrier(num_arrivals=compute_wgs),\n+          collective_axes=\"wg\",\n       )\n   @jax.jit\n   def run_function(q, k, v, o, lse):\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add an additional WG barrier before copy_gmem_to_smem\n\nThis is necessary to ensure that all SMEM reads issued from a current WG\nhave completed before we schedule the copy (that acts as an SMEM write)!\n\nPiperOrigin-RevId: 757647993",
        "output": [
            "```diff\nCommit: eb2fe9715d849d6b3d63d5345b96b6865c68b3e2\nDate: 2025-05-12T08:38:31Z\nURL: https://github.com/jax-ml/jax/commit/eb2fe9715d849d6b3d63d5345b96b6865c68b3e2\nFiles changed: 1\nAdditions: +9, Deletions: -3\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 40eccca7c711..f199b7b245c6 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -449,9 +449,6 @@ def _copy_gmem_to_smem_pp_eqn(\n \n @lowering.register_lowering_rule(\n     copy_gmem_to_smem_p, mgpu.LoweringSemantics.Lane)\n-@lowering.register_lowering_rule(\n-    copy_gmem_to_smem_p, mgpu.LoweringSemantics.Lane,\n-    primitive_semantics=gpu_core.PrimitiveSemantics.Warp)\n @lowering.register_lowering_rule(\n     copy_gmem_to_smem_p, mgpu.LoweringSemantics.Warpgroup\n )\n@@ -465,6 +462,7 @@ def _copy_gmem_to_smem_lowering(\n     dst_transforms_treedef,\n     barrier_transforms_treedef,\n     collective_axes,\n+    warpgroup_sync: bool = True,\n ):\n   flat_src_transforms, flat_dst_transforms, flat_barrier_transforms = (\n       util.split_list(\n@@ -509,6 +507,8 @@ def _copy_gmem_to_smem_lowering(\n     # arrive with the whole transfer size, while everyone else arrives with 0.\n     # But we should continue using this scheme as it's likely to be faster.\n     bytes //= WARPGROUP_SIZE\n+    if warpgroup_sync:\n+      mgpu.warpgroup_barrier()  # Make sure all reads have completed.\n     barrier.arrive_expect_tx(bytes)\n     ctx.launch_ctx.async_copy(\n         src_ref=src,\n@@ -541,6 +541,12 @@ def _copy_gmem_to_smem_lowering(\n   )\n   return ()\n \n+lowering.register_lowering_rule(\n+    copy_gmem_to_smem_p,\n+    mgpu.LoweringSemantics.Lane,\n+    primitive_semantics=gpu_core.PrimitiveSemantics.Warp,\n+)(functools.partial(_copy_gmem_to_smem_lowering, warpgroup_sync=False))\n+\n \n def copy_gmem_to_smem(\n     src: _Ref,\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Add support for 8-bit MMA on Blackwell\n\nPiperOrigin-RevId: 757608204",
        "output": [
            "```diff\nCommit: caf10dfc4503b98ea815b5c44c08d75f32fa83c7\nDate: 2025-05-12T06:17:25Z\nURL: https://github.com/jax-ml/jax/commit/caf10dfc4503b98ea815b5c44c08d75f32fa83c7\nFiles changed: 3\nAdditions: +71, Deletions: -21\ndiff --git a/jax/experimental/mosaic/gpu/launch_context.py b/jax/experimental/mosaic/gpu/launch_context.py\nindex d169c448a80e..02ed3859d8c2 100644\n--- a/jax/experimental/mosaic/gpu/launch_context.py\n+++ b/jax/experimental/mosaic/gpu/launch_context.py\n@@ -456,12 +456,19 @@ def init_tma_desc(host_ptr):\n               tma_dtype = 3\n             elif bitwidth == 64:\n               tma_dtype = 4\n+            else:\n+              raise ValueError(f\"Unsupported integer bitwidth: {bitwidth}\")\n           elif ir.F16Type.isinstance(ref_ty.element_type):\n             tma_dtype = 5\n           elif ir.F32Type.isinstance(ref_ty.element_type):\n             tma_dtype = 6\n           elif ir.BF16Type.isinstance(ref_ty.element_type):\n             tma_dtype = 7\n+          # We treat 8 bit floats as 8 bit integers\n+          elif ir.Float8E5M2Type.isinstance(ref_ty.element_type):\n+            tma_dtype = 1\n+          elif ir.Float8E4M3Type.isinstance(ref_ty.element_type):\n+            tma_dtype = 1\n           else:\n             raise ValueError(f\"unsupported TMA dtype {ref_ty.element_type}\")\n           dtype_or_bitwidth = c(tma_dtype, i64)\n@@ -584,12 +591,18 @@ def async_copy(\n           \" multiple of 16 bytes\"\n       )\n \n-    if reduction_op is not None and jaxlib.version < (0, 5, 4):\n-      raise ValueError(\"TMA with reduction is only supported with jaxlib >= 0.5.4\")\n-    if reduction_op is not None and not isinstance(gmem_ref_ty.element_type, ir.FloatType):\n-      raise ValueError(\"TMA with reduction is only supported with float dtype\")\n-    if reduction_op is not None and reduction_op != \"add\":\n-      raise ValueError(\"TMA with reduction is only supported with add operation\")\n+    if reduction_op is not None:\n+      if not any(\n+          t.isinstance(gmem_ref_ty.element_type)\n+          for t in (ir.F32Type, ir.BF16Type, ir.F16Type)\n+      ):\n+        raise ValueError(\n+            \"TMA with reduction is only supported with f32, f16 and bf16\"\n+        )\n+      if reduction_op != \"add\":\n+        raise ValueError(\n+            \"TMA with reduction is only supported with add operation\"\n+        )\n \n     # NOTE: TMA supports OOB indices, so we skip the check.\n     base_indices, slice_shape, is_squeezed = utils.parse_indices(\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex f4ea8e289f01..730761cb7eff 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -56,17 +56,42 @@ def create_instr_descriptor(\n   f32 = ir.F32Type.get()\n   bf16 = ir.BF16Type.get()\n   f16 = ir.F16Type.get()\n-  if input_dtype not in {f16, bf16}:\n-    raise NotImplementedError(\"Only float16 and bfloat16 inputs supported\")\n   if acc_dtype not in {f32, f16}:\n     raise NotImplementedError(\"Only float32 and float16 accumulators supported\")\n+  if utils.bitwidth(input_dtype) == 16:\n+    if input_dtype not in {f16, bf16}:\n+      raise NotImplementedError(\n+          \"The only supported 16-bit input types are float16 and bfloat16, got\"\n+          f\" {input_dtype}\"\n+      )\n+    desc = 0\n+    desc |= (acc_dtype == f32) << 4  # D dtype, bits 4-5\n+    # Bit 6 is reserved\n+    desc |= (input_dtype == bf16) << 7  # A dtype, bits 7-9\n+    desc |= (input_dtype == bf16) << 10  # B dtype, bits 10-12\n+    return _finish_instr_descriptor(desc, m, n, transpose_a, transpose_b)\n+  elif utils.bitwidth(input_dtype) == 8:\n+    desc = 0\n+    desc |= (acc_dtype == f32) << 4  # D dtype, bits 4-5\n+    # Bit 6 is reserved\n+    if input_dtype == ir.Float8E4M3Type.get():\n+      input_dtype_enum = 0\n+    elif input_dtype == ir.Float8E5M2Type.get():\n+      input_dtype_enum = 1\n+    else:\n+      raise NotImplementedError(f\"Unsupported input dtype: {input_dtype}\")\n+    desc |= input_dtype_enum << 7  # A dtype, bits 7-9\n+    desc |= input_dtype_enum << 10  # B dtype, bits 10-12\n+    return _finish_instr_descriptor(desc, m, n, transpose_a, transpose_b)\n+  else:\n+    raise NotImplementedError(f\"Unsupported input dtype: {input_dtype}\")\n \n-  desc = 0\n+\n+def _finish_instr_descriptor(\n+    desc: int, m: int, n: int, transpose_a: bool, transpose_b: bool,\n+):\n   # We ignore sparsity in bits 0-3\n-  desc |= (acc_dtype == f32) << 4  # D dtype, bits 4-5\n-  # Bit 6 is reserved\n-  desc |= (input_dtype == bf16) << 7  # A dtype, bits 7-9\n-  desc |= (input_dtype == bf16) << 10  # B dtype, bits 10-12\n+  # A, B and D types are set by the caller\n   # We ignore negate bits 13-14\n   desc |= transpose_a << 15  # Transpose A\n   desc |= transpose_b << 16  # Transpose B\n@@ -139,20 +164,24 @@ def mma(\n         f\"Accumulator layout mismatch: expected {expected_layout}, got {d.layout}\"\n     )\n   f32 = ir.F32Type.get()\n+  f16 = ir.F16Type.get()\n   if element_type == f32 or element_type == ir.BF16Type.get():\n     if d.dtype != f32:\n       raise ValueError(\n           f\"MMA with element type {element_type} only supports accumulators\"\n           f\" of type f32, but got: {d.dtype}\"\n       )\n-  elif element_type == ir.F16Type.get():\n-    if d.dtype != element_type and d.dtype != f32:\n+  elif any(\n+      t.isinstance(element_type)\n+      for t in {ir.F16Type, ir.Float8E5M2Type, ir.Float8E4M3Type}\n+  ):\n+    if d.dtype != f16 and d.dtype != f32:\n       raise ValueError(\n-          \"MMA with element type f16 only supports accumulators of type f32\"\n-          f\" or f16, but got: {d.dtype}\"\n+          f\"MMA with element type {element_type} only supports accumulators of\"\n+          f\" type f32 or f16, but got: {d.dtype}\"\n       )\n   else:\n-    raise NotImplementedError(f\"Unsupported element type: {element_type}\")\n+    raise NotImplementedError(f\"Unsupported element type: {element_type}\", type(element_type))\n \n   # Step 2. Decide on the instruction shapes we'll use. Note that with swizzles,\n   # instructions must be issued in groups of the same width as the swizzle.\n@@ -268,6 +297,10 @@ def _do_mma(\n \n   if ir.F16Type.isinstance(element_type) or ir.BF16Type.isinstance(element_type):\n     kind = \"f16\"\n+  elif ir.Float8E5M2Type.isinstance(element_type):\n+    kind = \"f8f6f4\"\n+  elif ir.Float8E4M3Type.isinstance(element_type):\n+    kind = \"f8f6f4\"\n   else:\n     raise NotImplementedError(f\"Unsupported input element type: {element_type}\")\n \ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 8c26f64bd203..b9350b0c995b 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -983,13 +983,15 @@ def kernel(ctx, input, output, scratch):\n   @parameterized.product(\n       lhs_transpose=(False, True),\n       rhs_transpose=(False, True),\n-      in_jax_dtype=(jnp.float16, jnp.bfloat16),  # TODO(apaszke): f32\n+      in_jax_dtype=(jnp.float16, jnp.bfloat16, jnp.float8_e5m2, jnp.float8_e4m3),  # TODO(apaszke): f32\n       out_jax_dtype=(jnp.float16, jnp.float32,),\n       m=(128,),  # TODO(apaszke): 64, 192, 256\n       n=(64, 128, 256, 512),  # TODO(apaszke): 192, other non-power-of-2\n       swizzle=(32, 64, 128,),\n   )\n   def test_mma_basic(self, **kwargs):\n+    if kwargs[\"n\"] * jnp.dtype(kwargs[\"in_jax_dtype\"]).itemsize < kwargs[\"swizzle\"]:\n+      self.skipTest(\"swizzle too large for input\")\n     self._basic_mma_test(\n         **kwargs,\n         k_steps=2,  # Reducing to 1 can be helpful while debugging.\n@@ -1029,8 +1031,10 @@ def _basic_mma_test(\n       rhs_transpose_tiles,\n       lhs_transpose_tiles,\n   ):\n-    if out_jax_dtype == jnp.float16 and in_jax_dtype != jnp.float16:\n-      self.skipTest(\"Only f16 input is supported for f16 output.\")\n+    if out_jax_dtype != jnp.float32 and (\n+        in_jax_dtype == jnp.float32 or in_jax_dtype == jnp.bfloat16\n+    ):\n+      self.skipTest(\"Only f32 output is supported for f32 and bf16 input.\")\n \n     in_mlir_dtype = utils.dtype_to_ir_type(in_jax_dtype)\n     swizzle_elems = swizzle // bytewidth(in_mlir_dtype)\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28664 from mattjj:rahul-fix\n\nPiperOrigin-RevId: 757200726",
        "output": [
            "```diff\nCommit: ddfcf84a375677e4dccfa92ddc2b4a7eb7234560\nDate: 2025-05-10T19:35:18Z\nURL: https://github.com/jax-ml/jax/commit/ddfcf84a375677e4dccfa92ddc2b4a7eb7234560\nFiles changed: 2\nAdditions: +12, Deletions: -5\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex a03933573e5d..154ff5132a39 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -2199,7 +2199,8 @@ def saved_input_vjp(f: Callable, which: Sequence[bool], *primals,\n   fun = lu.wrap_init(f, debug_info=dbg)\n   primals_flat, in_tree = tree_flatten(primals)\n   fun, out_tree = flatten_fun_nokwargs(fun, in_tree)\n-  out_primals_flat, _, jaxpr, residuals = ad.linearize(fun, *primals_flat)\n+  out_primals_flat, out_pvals, jaxpr, residuals = ad.linearize(fun, *primals_flat)\n+  out_known = [pval.is_known() for pval in out_pvals]\n   primals_filt, filt_tree = tree_flatten(tuple(p for w, p in zip(which, primals) if w))\n   id_map = {id(x): i for i, x in enumerate(primals_filt)}\n   opaque_residuals = []\n@@ -2207,7 +2208,7 @@ def saved_input_vjp(f: Callable, which: Sequence[bool], *primals,\n               RSpec(opaque_residuals.append(r) or (len(opaque_residuals) - 1), False)  # type: ignore\n               for r in residuals]\n   f_vjp = Partial(partial(_saved_input_vjpfun, res_spec, filt_tree, in_tree,\n-                          out_tree(), jaxpr), opaque_residuals)\n+                          out_tree(), out_known, jaxpr), opaque_residuals)\n \n   if not allow_unused and not set(id_map).issubset(res_ids := {id(r) for r in residuals}):\n     unused = [(i, core.get_aval(x)) for i, (x, w) in enumerate(zip(primals, which))\n@@ -2232,8 +2233,8 @@ def saved_input_vjp(f: Callable, which: Sequence[bool], *primals,\n   out_primals = tree_unflatten(out_tree(), out_primals_flat)\n   return out_primals, f_vjp\n \n-def _saved_input_vjpfun(res_spec, filtered_tree, in_tree, out_tree, jaxpr,\n-                        opaque_residuals, ct, *saved_primals):\n+def _saved_input_vjpfun(res_spec, filtered_tree, in_tree, out_tree, out_known,\n+                        jaxpr, opaque_residuals, ct, *saved_primals):\n   primals_filtered, filtered_tree_ = tree_flatten(saved_primals)\n   if filtered_tree != filtered_tree_:\n     raise ValueError(\n@@ -2253,8 +2254,9 @@ def _saved_input_vjpfun(res_spec, filtered_tree, in_tree, out_tree, jaxpr,\n   dummy_args = [ad.UndefinedPrimal(v.aval) for v in jaxpr.invars]\n   cts_flat, out_tree_ = tree_flatten(ct)\n   assert out_tree_ == out_tree\n+  cts_flat = [ct for ct, k in zip(cts_flat, out_known) if not k]\n   arg_cts = ad.backward_pass(jaxpr, True, residuals, dummy_args, cts_flat)\n-  return tree_unflatten(in_tree, arg_cts)\n+  return tree_unflatten(in_tree, map(ad.instantiate_zeros, arg_cts))\n \n @dataclasses.dataclass(frozen=True)\n class RSpec:\ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 6e55e732151d..15966c678d87 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -7323,6 +7323,11 @@ def f2(x, w):\n     self.assertAllClose(x_grad, 2. * y_grad @ w.T)\n     self.assertAllClose(w_grad, 2. * x.T @ y_grad)\n \n+  def test_doesnt_leak_symbolic_zeros(self):\n+    _, vjp = api.si_vjp(lambda x: 1., [False], 3.14)\n+    ans, = vjp(1.0)\n+    self.assertIsInstance(ans, jax.Array)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[shard-map] start adding systematic smap tests",
        "output": [
            "```diff\nCommit: 254d64e3f8e884b0fa3a21a547f1438a66414967\nDate: 2025-05-10T05:30:06Z\nURL: https://github.com/jax-ml/jax/commit/254d64e3f8e884b0fa3a21a547f1438a66414967\nFiles changed: 1\nAdditions: +88, Deletions: -0\ndiff --git a/tests/shard_map_test.py b/tests/shard_map_test.py\nindex 4d3b265bd869..5b417bb4b87a 100644\n--- a/tests/shard_map_test.py\n+++ b/tests/shard_map_test.py\n@@ -3649,6 +3649,94 @@ def f(x):\n     self.assertAllClose(f(jnp.arange(8.)), jnp.array([1.,  5.,  9., 13.]))\n \n \n+def smap_ref(f, in_axes, out_axes, axis_name, axis_size):\n+  del axis_name  # no collectives\n+  def smapped(*args):\n+    split_args = zip(*[split_arg(x, d, axis_size) for x, d in zip(args, in_axes)])\n+    split_result = [f(*xs) for xs in split_args]\n+    return concat_result(split_result, out_axes)\n+  return smapped\n+\n+def split_arg(x, d, axis_size):\n+  if d is None:\n+    x = np.tile(x, [axis_size] + [1] * (x.ndim - 1))\n+  return np.split(x, axis_size, d or 0)\n+\n+def concat_result(results, out_axes):\n+  if not isinstance(results[0], (list, tuple)):\n+    return results[0] if out_axes is None else np.concatenate(results, out_axes)\n+  return [res[0] if d is None else np.concatenate(res, d)\n+          for res, d in zip(zip(*results), out_axes)]\n+\n+def sample_smap() -> Chooser:\n+  spec = yield fun_specs\n+  mesh_shape = yield mesh_shapes\n+  axis_names = ('i', 'j', 'k', 'l')[:len(mesh_shape)]\n+  mesh = SimpleNamespace(shape=dict(zip(axis_names, mesh_shape)),\n+                         axis_names=axis_names)\n+  axis_name = yield axis_names\n+  body_in_types = yield (tys for tys in it.product(input_shapes, repeat=spec.num_inputs)\n+                         if not spec.valid_types or spec.valid_types(*tys))\n+  in_axes = yield from sample_in_axes(body_in_types)\n+  out_rep = spec.out_rep(*[ax is None for ax in in_axes])\n+  body_out_type = jax.eval_shape(spec.fun, *body_in_types)\n+  out_axes = yield from sample_out_axes(out_rep, body_out_type)\n+  in_str = '(' + ','.join(jax.core.ShapedArray(t.shape, t.dtype).str_short()\n+                          for t in body_in_types) + ')'\n+  name = f'{spec.name}_{mesh.shape}_{in_axes}_{out_axes}_{axis_name}_{in_str}'\n+  in_types = [ty.update(shape=dilate_axis(ty.shape, d, mesh.shape[axis_name]))\n+              for ty, d in zip(body_in_types, in_axes)]\n+  args = [np.arange(ty.size, dtype=ty.dtype).reshape(ty.shape) / ty.size\n+          for ty in in_types]\n+  return name, spec, mesh.shape, in_axes, out_axes, axis_name, args\n+\n+def sample_in_axes(body_in_types) -> Chooser:\n+  in_axes = []\n+  for ty in body_in_types:\n+    in_axes.append((yield [None, *range(ty.ndim)]))\n+  return tuple(in_axes)\n+\n+def sample_out_axes(out_rep, body_out_type) -> Chooser:\n+  if not isinstance(body_out_type, (list, tuple)):\n+    out_axes = yield [None] * out_rep + list(range(body_out_type.ndim))\n+  else:\n+    out_axes_ = []\n+    for ty, r in zip(body_out_type, out_rep):\n+      out_axes_.append((yield [None] * r + list(range(ty.ndim))))\n+    out_axes = tuple(out_axes_)\n+  return out_axes\n+\n+def dilate_axis(shape: tuple[int, ...], i: int | None, size: int) -> tuple[int, ...]:\n+  if i is None:\n+    return shape\n+  shp = list(shape)\n+  shp[i] *= size\n+  return tuple(shp)\n+\n+class SmapSystematicTest(jtu.JaxTestCase):\n+\n+  @staticmethod\n+  def make_mesh(mesh_shape):\n+    return jtu.create_mesh(tuple(mesh_shape.values()), tuple(mesh_shape))\n+\n+  @parameterized.parameters(\n+      sample(jtu.NUM_GENERATED_CASES.value, sample_smap))\n+  def test_against_ref(self, fun_spec, mesh_shape, in_axes, out_axes, axis_name, args):\n+    fun = fun_spec.fun\n+    mesh = self.make_mesh(mesh_shape)\n+    args = map(jnp.array, args)\n+\n+    with jax.sharding.use_mesh(mesh):\n+      fun_ = smap(fun, in_axes=in_axes, out_axes=out_axes, axis_name=axis_name)\n+      out = jax.jit(fun_)(*args)\n+\n+    fun_ref = smap_ref(fun, in_axes=in_axes, out_axes=out_axes, axis_name=axis_name,\n+                       axis_size=mesh_shape[axis_name])\n+    expected = fun_ref(*args)\n+\n+    self.assertAllClose(out, expected, check_dtypes=False)\n+\n+\n @jtu.with_config(jax_use_shardy_partitioner=True)\n # TODO(phawkins): enable this test unconditionally once shardy is the default.\n @unittest.skipIf(sdy is None, \"shardy is not enabled\")\n\n```"
        ],
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "[si_vjp] fix bugs around symbolic zeros\n\n* fix leaking of internal symbolic zeros in returned cotangents\n* fix a bug around symbolic zero output tangents",
        "output": [
            "```diff\nCommit: 55a9de3c8a689bc48a2772a2d9dc359b0ecca51e\nDate: 2025-05-10T04:01:35Z\nURL: https://github.com/jax-ml/jax/commit/55a9de3c8a689bc48a2772a2d9dc359b0ecca51e\nFiles changed: 2\nAdditions: +12, Deletions: -5\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex a03933573e5d..154ff5132a39 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -2199,7 +2199,8 @@ def saved_input_vjp(f: Callable, which: Sequence[bool], *primals,\n   fun = lu.wrap_init(f, debug_info=dbg)\n   primals_flat, in_tree = tree_flatten(primals)\n   fun, out_tree = flatten_fun_nokwargs(fun, in_tree)\n-  out_primals_flat, _, jaxpr, residuals = ad.linearize(fun, *primals_flat)\n+  out_primals_flat, out_pvals, jaxpr, residuals = ad.linearize(fun, *primals_flat)\n+  out_known = [pval.is_known() for pval in out_pvals]\n   primals_filt, filt_tree = tree_flatten(tuple(p for w, p in zip(which, primals) if w))\n   id_map = {id(x): i for i, x in enumerate(primals_filt)}\n   opaque_residuals = []\n@@ -2207,7 +2208,7 @@ def saved_input_vjp(f: Callable, which: Sequence[bool], *primals,\n               RSpec(opaque_residuals.append(r) or (len(opaque_residuals) - 1), False)  # type: ignore\n               for r in residuals]\n   f_vjp = Partial(partial(_saved_input_vjpfun, res_spec, filt_tree, in_tree,\n-                          out_tree(), jaxpr), opaque_residuals)\n+                          out_tree(), out_known, jaxpr), opaque_residuals)\n \n   if not allow_unused and not set(id_map).issubset(res_ids := {id(r) for r in residuals}):\n     unused = [(i, core.get_aval(x)) for i, (x, w) in enumerate(zip(primals, which))\n@@ -2232,8 +2233,8 @@ def saved_input_vjp(f: Callable, which: Sequence[bool], *primals,\n   out_primals = tree_unflatten(out_tree(), out_primals_flat)\n   return out_primals, f_vjp\n \n-def _saved_input_vjpfun(res_spec, filtered_tree, in_tree, out_tree, jaxpr,\n-                        opaque_residuals, ct, *saved_primals):\n+def _saved_input_vjpfun(res_spec, filtered_tree, in_tree, out_tree, out_known,\n+                        jaxpr, opaque_residuals, ct, *saved_primals):\n   primals_filtered, filtered_tree_ = tree_flatten(saved_primals)\n   if filtered_tree != filtered_tree_:\n     raise ValueError(\n@@ -2253,8 +2254,9 @@ def _saved_input_vjpfun(res_spec, filtered_tree, in_tree, out_tree, jaxpr,\n   dummy_args = [ad.UndefinedPrimal(v.aval) for v in jaxpr.invars]\n   cts_flat, out_tree_ = tree_flatten(ct)\n   assert out_tree_ == out_tree\n+  cts_flat = [ct for ct, k in zip(cts_flat, out_known) if not k]\n   arg_cts = ad.backward_pass(jaxpr, True, residuals, dummy_args, cts_flat)\n-  return tree_unflatten(in_tree, arg_cts)\n+  return tree_unflatten(in_tree, map(ad.instantiate_zeros, arg_cts))\n \n @dataclasses.dataclass(frozen=True)\n class RSpec:\ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 6e55e732151d..15966c678d87 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -7323,6 +7323,11 @@ def f2(x, w):\n     self.assertAllClose(x_grad, 2. * y_grad @ w.T)\n     self.assertAllClose(w_grad, 2. * x.T @ y_grad)\n \n+  def test_doesnt_leak_symbolic_zeros(self):\n+    _, vjp = api.si_vjp(lambda x: 1., [False], 3.14)\n+    ans, = vjp(1.0)\n+    self.assertIsInstance(ans, jax.Array)\n+\n \n if __name__ == '__main__':\n   absltest.main(testLoader=jtu.JaxTestLoader())\n\n```"
        ],
        "from_id": [
            "mattjj"
        ]
    },
    {
        "text_input": "[Pallas] Allow more int casting tests.\n\nPiperOrigin-RevId: 756989842",
        "output": [
            "```diff\nCommit: 35e2657be8308917c7fa407be5a0b53192134890\nDate: 2025-05-10T02:17:07Z\nURL: https://github.com/jax-ml/jax/commit/35e2657be8308917c7fa407be5a0b53192134890\nFiles changed: 1\nAdditions: +15, Deletions: -13\ndiff --git a/tests/pallas/ops_test.py b/tests/pallas/ops_test.py\nindex cf6536df2344..61ebc19e018f 100644\n--- a/tests/pallas/ops_test.py\n+++ b/tests/pallas/ops_test.py\n@@ -606,10 +606,17 @@ def test_cast_from_32bit(self, from_dtype, to_dtype, data):\n     if from_dtype == to_dtype:\n       self.skipTest(\"Unnecessary test\")\n     if jtu.is_device_tpu(version=4):\n-      if to_dtype in {\"int8\", \"uint8\", \"int4\", \"uint4\", \"int2\", \"uint2\"}:\n+      if to_dtype in {\"int2\", \"uint2\"}:\n         self.skipTest(\"Not supported on this TPU generation\")\n       if to_dtype in {\"int16\", \"uint16\"} and not jtu.if_cloud_tpu_at_least(2025, 1, 18):\n         self.skipTest(\"Test requires libtpu from 2025/1/18 or later\")\n+      if to_dtype in {\n+          \"int4\",\n+          \"uint4\",\n+          \"int8\",\n+          \"uint8\",\n+      } and not jtu.if_cloud_tpu_at_least(2025, 5, 15):\n+        self.skipTest(\"Test requires libtpu from 2025/5/15 or later\")\n     if jtu.test_device_matches([\"tpu\"]) and jtu.get_tpu_version() < 4:\n       # Currently only casts between 32-bit types and to bf16 are supported.\n       if to_dtype not in {\"int32\", \"uint32\", \"float32\", \"bfloat16\"}:\n@@ -673,18 +680,7 @@ def test_cast_from_sub_32bit(self, from_dtype, to_dtype, randomize):\n     if jtu.is_device_tpu(version=4):\n       allowed_v4_cats = {(\"int16\", \"int32\"): (2025, 1, 18)}\n       if (\n-          from_dtype\n-          in {\n-              \"int16\",\n-              \"int8\",\n-              \"uint16\",\n-              \"uint8\",\n-              \"int4\",\n-              \"uint4\",\n-              \"int2\",\n-              \"uint2\",\n-          }\n-          or to_dtype in {\"int8\", \"uint8\", \"int4\", \"uint4\", \"int2\", \"uint2\"}\n+          from_dtype in {\"int2\", \"uint2\"} or to_dtype in {\"int2\", \"uint2\"}\n       ) and (from_dtype, to_dtype) not in allowed_v4_cats:\n         self.skipTest(\"Not supported on this TPU generation\")\n       if minimum_libtpu_date := allowed_v4_cats.get((from_dtype, to_dtype), None):\n@@ -692,6 +688,12 @@ def test_cast_from_sub_32bit(self, from_dtype, to_dtype, randomize):\n           self.skipTest(\"Test requires a newer libtpu\")\n       if to_dtype in {\"int16\", \"uint16\"} and not jtu.if_cloud_tpu_at_least(2025, 1, 18):\n         self.skipTest(\"Test requires libtpu from 2025/1/18 or later\")\n+      if (\n+          to_dtype in {\"int4\", \"uint4\", \"int8\", \"uint8\"}\n+          and from_dtype in {\"int4\", \"uint4\", \"int8\", \"uint8\"}\n+          and not jtu.if_cloud_tpu_at_least(2025, 5, 15)\n+      ):\n+        self.skipTest(\"Test requires libtpu from 2025/5/15 or later\")\n     if jtu.test_device_matches([\"tpu\"]) and jtu.get_tpu_version() < 4:\n       self.skipTest(\"Not supported on this TPU generation\")\n     if jtu.test_device_matches([\"gpu\"]) and (\n\n```"
        ],
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "JEP 28661: the __jax_array__ protocol",
        "output": [
            "```diff\nCommit: b95ba89b24a16ca2c61c77581c7264c2dffffcbb\nDate: 2025-05-09T21:41:26Z\nURL: https://github.com/jax-ml/jax/commit/b95ba89b24a16ca2c61c77581c7264c2dffffcbb\nFiles changed: 2\nAdditions: +215, Deletions: -0\ndiff --git a/docs/jep/28661-jax-array-protocol.md b/docs/jep/28661-jax-array-protocol.md\nnew file mode 100644\nindex 000000000000..e05d69d2822d\n--- /dev/null\n+++ b/docs/jep/28661-jax-array-protocol.md\n@@ -0,0 +1,214 @@\n+# JEP 28661: Supporting the `__jax_array__` protocol\n+\n+[@jakevdp](http://github.com/jakevdp), *May 2025*\n+\n+An occasional user request is for the ability to define custom array-like objects that\n+work with jax APIs. JAX currently has a partial implementation of a mechanism that does\n+this via a `__jax_array__` method defined on the custom object. This was never intended\n+to be a load-bearing public API (see the discussion at {jax-issue}`#4725`), but has\n+become essential to packages like Keras and flax, which explicitly document the ability\n+to use their custom array objects with jax functions. This JEP proposes a design for\n+full, documented support of the `__jax_array__` protocol.\n+\n+## Levels of array extensibility\n+Requests for extensibility of JAX arrays come in a few flavors:\n+\n+### Level 1 Extensibility: polymorphic inputs\n+What I’ll call \"Level 1\" extensibility is the desire that JAX APIs accept polymorphic inputs.\n+That is, a user desires behavior like this:\n+\n+```python\n+class CustomArray:\n+  data: numpy.ndarray\n+  ...\n+\n+x = CustomArray(np.arange(5))\n+result = jnp.sin(x)  # Converts `x` to JAX array and returns a JAX array\n+```\n+\n+Under this extensibility model, JAX functions would accept CustomArray objects as inputs,\n+implicitly converting them to `jax.Array` objects for the sake of computation.\n+This is similar to the functionality offered by NumPy via the `__array__` method, and in\n+JAX (in many but not all cases) via the `__jax_array__` method.\n+\n+This is the mode of extensibility that has been requested by the maintainers of `flax.nnx`\n+and others. The current implementation is also used by JAX internally for the case of\n+symbolic dimensions.\n+\n+### Level 2 extensibility: polymorphic outputs\n+What I’ll call \"Level 2\" extensibility is the desire that JAX APIs should not only accept\n+polymorphic inputs, but also wrap outputs to match the class of the input.\n+That is, a user desires behavior like this:\n+\n+```python\n+class CustomArray:\n+  data: numpy.ndarray\n+  ...\n+\n+x = CustomArray(np.arange(5))\n+result = jnp.sin(x)  # returns a new CustomArray\n+```\n+\n+Under this extensibility model, JAX functions would not only accept custom objects\n+as inputs, but have some protocol to determine how to correctly re-wrap outputs with\n+the same class. In NumPy, this sort of functionality is offered in varying degrees by\n+the special `__array_ufunc__`, `__array_wrap__`, and `__array_function__` protocols,\n+which allow user-defined objects to customize how NumPy API functions operate on\n+arbitrary inputs and map input types to outputs.\n+JAX does not currently have any equivalent to these interfaces in NumPy.\n+\n+This is the mode of extensibility that has been requested by the maintainers of `keras`,\n+among others.\n+\n+### Level 3 extensibility: subclassing `Array`\n+\n+What I’ll call \"Level 3\" extensibility is the desire that the JAX array object itself\n+could be subclassable. NumPy provides some APIs that allow this\n+(see [Subclassing ndarray](https://numpy.org/devdocs/user/basics.subclassing.html)) but\n+this sort of approach would take some extra thought in JAX due to the need for\n+representing array objects abstractly via tracing.\n+\n+This mode of extensibility has occasionally been requested by users who want to add\n+special metadata to JAX arrays, such as units of measurement.\n+\n+## Synopsis\n+\n+For the sake of this proposal, we will stick with the simplest, level 1 extensibility\n+model. The proposed interface is the one currently non-uniformly supported by a number\n+of JAX APIs, the `__jax_array__` method. Its usage looks something like this:\n+\n+```python\n+import jax\n+import jax.numpy as jnp\n+import numpy as np\n+\n+class CustomArray:\n+  data: np.ndarray\n+\n+  def __init__(self, data: np.ndarray):\n+    self.data = data\n+\n+  def __jax_array__(self) -> jax.Array:\n+    return jnp.asarray(self.data)\n+\n+arr = CustomArray(np.arange(5))\n+result = jnp.multiply(arr, 2)\n+print(repr(result))\n+# Array([0, 2, 4, 6, 8], dtype=int32)\n+```\n+\n+We may revisit other extensibility levels in the future.\n+\n+## Design challenges\n+\n+JAX presents some interesting design challenges related to this kind of extensibility,\n+which have not been fully explored previously. We’ll discuss them in turn here:\n+\n+### Priority of `__jax_array__` vs. PyTree flattening\n+JAX already has a supported mechanism for registering custom objects, namely pytree\n+registration (see [Extending pytrees](https://docs.jax.dev/en/latest/pytrees.html#extending-pytrees)).\n+If we also support __jax_array__, which one should take precedence?\n+\n+To put this more concretely, what should be the result of this code?\n+\n+```python\n+@jax.jit\n+def f(x):\n+  print(\"is JAX array:\", isinstance(x, jax.Array))\n+\n+f(CustomArray(...))\n+```\n+\n+If we choose to prioritize `__jax_array__` at the JIT boundary, then the output of this\n+function would be:\n+```\n+is JAX array: True\n+```\n+That is, at the JIT boundary, the `CustomArray` object would be converted into a\n+`__jax_array__`, and its shape and dtype would be used to construct a standard JAX\n+tracer for the function.\n+\n+If we choose to prioritize pytree flattening at the JIT boundary, then the output of\n+this function would be:\n+```\n+type(x)=CustomArray\n+```\n+That is, at the JIT boundary, the `CustomArray` object is flattened, and then unflattened\n+before being passed to the JIT-compiled function for tracing. If `CustomArray` has been\n+registered as a pytree, it will generally contain traced arrays as its attributes, and\n+when x is passed to any JAX API that supports `__jax_array__`, these traced attributes\n+will be converted to a single traced array according to the logic specified in the method.\n+\n+There are deeper consequences here for how other transformations like vmap and grad work\n+when encountering custom objects: for example, if we prioritize pytree flattening, vmap\n+would operate over the dimensions of the flattened contents of the custom object, while\n+if we prioritize `__jax_array__`, vmap would operate over the converted array dimensions.\n+\n+This also has consequences when it comes to JIT invariance: consider a function like this:\n+```python\n+def f(x):\n+  if isinstance(x, CustomArray):\n+    return x.custom_method()\n+  else:\n+    # do something else\n+    ...\n+\n+result1 = f(x)\n+result2 = jax.jit(f)(x)\n+```\n+If `jit` consumes `x` via pytree flattening, the results should agree for a well-specified\n+flattening rule. If `jit` consumes `x` via `__jax_array__`, the results will differ because\n+`x` is no longer a CustomArray within the JIT-compiled version of the function.\n+\n+#### Synopsis\n+As of JAX v0.6.0, transformations prioritize `__jax_array__` when it is available. This status\n+quo can lead to confusion around lack of JIT invariance, and the current implementation in practice\n+leads to subtle bugs in the case of automatic differentiation, where the forward and backward pass\n+do not treat inputs consistently.\n+\n+Because the pytree extensibility mechanism already exists for the case of customizing\n+transformations, it seems most straightforward if transformations act only via this\n+mechanism: that is, **we propose to remove `__jax_array__` parsing during abstractification.**\n+This approach will preserve object identity through transformations, and give the user the\n+most possible flexibility. If the user wants to opt-in to array conversion semantics, that\n+is always possible by explicitly casting their input via jnp.asarray, which will trigger the \n+`__jax_array__` protocol.\n+\n+### Which APIs should support `__jax_array__`?\n+JAX has a number of different levels of API, from the level of explicit primitive binding\n+(e.g. `jax.lax.add_p.bind(x, y)`) to the `jax.lax` APIs (e.g. `jax.lax.add(x, y)`) to the\n+`jax.numpy` APIs (e.g. `jax.numpy.add(x, y)`). Which of these API categories should handle\n+implicit conversion via `__jax_array__`?\n+\n+In order to limit the scope of the change and the required testing, I propose that `__jax_array__`\n+only be explicitly supported in `jax.numpy` APIs: after all, it is inspired by the` __array__`\n+protocol which is supported by the NumPy package. We could always expand this in the future to\n+`jax.lax` APIs if needed.\n+\n+This is in line with the current state of the package, where `__jax_array__` handling is mainly\n+within the input validation utilities used by `jax.numpy` APIs.\n+\n+## Implementation\n+With these design choices in mind, we plan to implement this as follows:\n+\n+- **Adding runtime support to `jax.numpy`**: This is likely the easiest part, as most\n+  `jax.numpy` functions use a common internal utility (`ensure_arraylike`) to validate\n+  inputs and convert them to array. This utility already supports `__jax_array__`, and\n+  so most jax.numpy APIs are already compliant.\n+- **Adding test coverage**:  To ensure compliance across the APIs, we should add a new\n+  test scaffold that calls every `jax.numpy` API with custom inputs and validates correct\n+  behavior.\n+- **Deprecating `__jax_array__` during abstractification**: Currently JAX's abstractification\n+  pass, used in `jit` and other transformations, does parse the `__jax_array__` protocol,\n+  and this is not the behavior we want long-term. We need to deprecate this behavior, and\n+  ensure that downstream packages that rely on it can move toward pytree registration or\n+  explicit array conversion where necessary.\n+- **Adding type annotations**: the type interface for jax.numpy functions is in\n+  `jax/numpy/__init__.pyi`, and we’ll need to change each input type from `ArrayLike` to\n+  `ArrayLike | SupportsJAXArray`, where the latter is a protocol with a `__jax_array__`\n+  method. We cannot add this directly to the `ArrayLike` definition, because `ArrayLike`\n+  is used in contexts where `__jax_array__` should not be supported.\n+- **Documentation**: once the above support is added, we should add a documentation section\n+  on array extensibility that outlines exactly what to expect regarding the `__jax_array__`\n+  protocol, with examples of how it can be used in conjunction with pytree registration\n+  in order to effectively work with user-defined types.\ndiff --git a/docs/jep/index.rst b/docs/jep/index.rst\nindex 1c4ecbb3411f..2ba85a5f4a8d 100644\n--- a/docs/jep/index.rst\n+++ b/docs/jep/index.rst\n@@ -52,6 +52,7 @@ Then create a pull request that adds a file named\n   17111: Efficient transposition of `shard_map` (and other maps) <17111-shmap-transpose>\n   18137: Scope of JAX NumPy & SciPy Wrappers <18137-numpy-scipy-scope>\n   25516: Effort-based versioning <25516-effver>\n+  28661: Supporting the `__jax_array__` protocol <28661-jax-array-protocol>\n \n \n Several early JEPs were converted in hindsight from other documentation,\n\n```"
        ],
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Disallow unreduced inputs for all primitives except those that implement the unreduced rule. Currently that's only `add`.\n\nPiperOrigin-RevId: 756902404",
        "output": [
            "```diff\nCommit: 4aee38103e2f5795ee3f55d4d6b2b59b37ffc6a1\nDate: 2025-05-09T21:15:20Z\nURL: https://github.com/jax-ml/jax/commit/4aee38103e2f5795ee3f55d4d6b2b59b37ffc6a1\nFiles changed: 3\nAdditions: +21, Deletions: -18\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 9b4ce0fa0f24..a03933573e5d 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -2866,14 +2866,13 @@ def __eq__(self, other):\n               (other.shape, other.dtype, other.sharding, other.layout, other.weak_type))\n \n   def __hash__(self):\n-    # TODO(frostig): avoid the conversion from dict by addressing\n-    # https://github.com/jax-ml/jax/issues/8182\n-    return hash((self.shape, self.dtype, self.sharding, self.layout, self.weak_type))\n+    return hash((self.shape, self.dtype, self.sharding, self.layout,\n+                 self.weak_type))\n \n   def __setattr__(self, name, value):\n     if hasattr(self, name):\n       if getattr(self, name) == value:\n-        # This can to happen if two threads race, for example if two threads\n+        # This can happen if two threads race, for example if two threads\n         # are trying to hash the same SDS instance.\n         return\n       raise RuntimeError(\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex 74297fc57b43..a49c27d06eee 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -5213,9 +5213,6 @@ def _dot_general_sharding_rule(lhs, rhs, *, dimension_numbers, precision,\n     raise core.ShardingTypeError(\n         'Mesh of both lhs and rhs should match. Got lhs:'\n         f' {lhs.sharding.mesh} and rhs: {rhs.sharding.mesh}')\n-  if lhs.sharding.spec.unreduced or rhs.sharding.spec.unreduced:\n-    raise NotImplementedError(\n-        'Please file an issue at https://github.com/jax-ml/jax/issues')\n \n   (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers\n   lhs_contracting_spec = tuple(lhs.sharding.spec[i] for i in lhs_contracting)\ndiff --git a/jax/_src/lax/utils.py b/jax/_src/lax/utils.py\nindex a850b2965338..97a2687bbb67 100644\n--- a/jax/_src/lax/utils.py\n+++ b/jax/_src/lax/utils.py\n@@ -67,7 +67,7 @@ def _get_abstract_mesh_from_avals(in_avals) -> mesh_lib.AbstractMesh:\n   return mesh_lib.empty_abstract_mesh if m is None else m\n \n \n-def call_sharding_rule(prim, rule, num_out, *avals, **kwargs):\n+def call_sharding_rule(prim, sh_rule, unreduced_rule, num_out, *avals, **kwargs):\n   cur_mesh = mesh_lib.get_abstract_mesh()\n   aval_mesh = _get_abstract_mesh_from_avals(avals)\n   if ((cur_mesh.empty or cur_mesh._are_all_axes_auto_or_manual) and\n@@ -75,22 +75,30 @@ def call_sharding_rule(prim, rule, num_out, *avals, **kwargs):\n     aval_mesh = cur_mesh if aval_mesh.empty else aval_mesh\n     s = NamedSharding(aval_mesh, P())\n     return s if num_out is None else [s] * num_out\n-  if rule is None:\n+  if sh_rule is None:\n     raise core.ShardingTypeError(\n-        f'sharding rule for {prim.name} is not implemented. Please file a'\n-        ' bug at https://github.com/jax-ml/jax/issues. You can work around'\n+        f'sharding rule for {prim.name} is not implemented. Please file an'\n+        ' issue at https://github.com/jax-ml/jax/issues. You can work around'\n         ' this error by dropping that operation into full auto sharding'\n         ' mode via: `jax.experimental.shard.auto_axes(fun, out_shardings=...)`')\n-  return rule(*avals, **kwargs)\n+  out_sharding = sh_rule(*avals, **kwargs)\n+  if unreduced_rule is not None:\n+    out_sharding = unreduced_rule(out_sharding, *avals, **kwargs)\n+  else:\n+    if any(a.sharding.spec.unreduced for a in avals):\n+      raise NotImplementedError(\n+          f'unreduced rule for {prim.name} is not implemented. Please file an'\n+          ' issue at https://github.com/jax-ml/jax/issues')\n+  return out_sharding\n \n def call_shape_dtype_sharding_rule(prim, shape_rule, dtype_rule, sharding_rule,\n-                                   multi_out, *avals, **kwargs):\n+                                   unreduced_rule, multi_out, *avals, **kwargs):\n   out_shapes = shape_rule(*avals, **kwargs)\n   out_dtypes = dtype_rule(*avals, **kwargs)\n   num_out = len(out_shapes) if multi_out else None\n   try:\n     out_shardings = call_sharding_rule(\n-        prim, sharding_rule, num_out, *avals, **kwargs)\n+        prim, sharding_rule, unreduced_rule, num_out, *avals, **kwargs)\n   except DuplicateSpecError as e:\n     if multi_out:\n       raise\n@@ -124,11 +132,9 @@ def standard_abstract_eval(prim, shape_rule, dtype_rule, weak_type_rule,\n   if least_specialized is core.ShapedArray:\n     core.check_avals_context_mesh(avals, prim.name)\n     out_shape, out_dtype, out_sharding = call_shape_dtype_sharding_rule(\n-        prim, shape_rule, dtype_rule, sharding_rule, False,\n+        prim, shape_rule, dtype_rule, sharding_rule, unreduced_rule, False,\n         *avals, **kwargs)\n     out_vma = vma_rule(*avals, **kwargs)\n-    if unreduced_rule is not None:\n-      out_sharding = unreduced_rule(out_sharding, *avals, **kwargs)\n     out_aval = core.ShapedArray(\n         out_shape, out_dtype, weak_type=weak_type, sharding=out_sharding,\n         vma=out_vma)\n@@ -154,7 +160,8 @@ def standard_multi_result_abstract_eval(\n   if least_specialized is core.ShapedArray:\n     core.check_avals_context_mesh(avals, prim.name)\n     out_shapes, out_dtypes, out_shardings = call_shape_dtype_sharding_rule(\n-        prim, shape_rule, dtype_rule, sharding_rule, True, *avals, **kwargs)\n+        prim, shape_rule, dtype_rule, sharding_rule, None, True,\n+        *avals, **kwargs)\n     out_vmas = vma_rule(*avals, **kwargs)\n     if isinstance(weak_types, bool):\n       weak_types = (weak_types,) * len(out_shapes)\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28656 from hawkinsp:profiler\n\nPiperOrigin-RevId: 756887370",
        "output": [
            "```diff\nCommit: d4645650fdf5894f5cb7c67c696dd3451534a60f\nDate: 2025-05-09T20:34:52Z\nURL: https://github.com/jax-ml/jax/commit/d4645650fdf5894f5cb7c67c696dd3451534a60f\nFiles changed: 1\nAdditions: +6, Deletions: -0\ndiff --git a/tests/profiler_test.py b/tests/profiler_test.py\nindex 215e363e446d..d577f1c24c49 100644\n--- a/tests/profiler_test.py\n+++ b/tests/profiler_test.py\n@@ -61,6 +61,12 @@ class ProfilerTest(unittest.TestCase):\n   # check functional correctness.\n \n   def setUp(self):\n+    if sys.version_info >= (3, 14) and jtu.TEST_NUM_THREADS.value > 1:\n+      # TODO(phawkins): try reenabling these after\n+      # https://github.com/python/cpython/issues/132817 is fixed. Simply\n+      # installing the profiler hook is unsafe if there are multiple threads.\n+      self.skipTest(\"Profiler tests are not thread-safe under Python 3.14\")\n+\n     super().setUp()\n     self.worker_start = threading.Event()\n     self.profile_done = False\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Disable profiler tests under Python 3.14 if multithreaded.\n\nThese are currently thread-unsafe due to\nhttps://github.com/python/cpython/issues/132817",
        "output": [
            "```diff\nCommit: 0d5771ccaae3e82704c315e8c45852779578172f\nDate: 2025-05-09T19:28:09Z\nURL: https://github.com/jax-ml/jax/commit/0d5771ccaae3e82704c315e8c45852779578172f\nFiles changed: 1\nAdditions: +6, Deletions: -0\ndiff --git a/tests/profiler_test.py b/tests/profiler_test.py\nindex 215e363e446d..d577f1c24c49 100644\n--- a/tests/profiler_test.py\n+++ b/tests/profiler_test.py\n@@ -61,6 +61,12 @@ class ProfilerTest(unittest.TestCase):\n   # check functional correctness.\n \n   def setUp(self):\n+    if sys.version_info >= (3, 14) and jtu.TEST_NUM_THREADS.value > 1:\n+      # TODO(phawkins): try reenabling these after\n+      # https://github.com/python/cpython/issues/132817 is fixed. Simply\n+      # installing the profiler hook is unsafe if there are multiple threads.\n+      self.skipTest(\"Profiler tests are not thread-safe under Python 3.14\")\n+\n     super().setUp()\n     self.worker_start = threading.Event()\n     self.profile_done = False\n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Make `jax.ShapeDtypeStruct` immutable. It was always supposed to be immutable inside `jax.Array` is immutable and `ShapeDtypeStruct` is a duck of `jax.Array` but immutability was never enforced.\n\n**If you are broken by this change, just update your code to use sds.update(...)**\n\nPiperOrigin-RevId: 756852248",
        "output": [
            "```diff\nCommit: 0605bc4a5dbe688489b38ea0c8ea89c24e1a7327\nDate: 2025-05-09T18:54:30Z\nURL: https://github.com/jax-ml/jax/commit/0605bc4a5dbe688489b38ea0c8ea89c24e1a7327\nFiles changed: 2\nAdditions: +14, Deletions: -0\ndiff --git a/CHANGELOG.md b/CHANGELOG.md\nindex a03eb80eb973..518e854b5bb1 100644\n--- a/CHANGELOG.md\n+++ b/CHANGELOG.md\n@@ -24,6 +24,8 @@ When releasing, please add the new-release-boilerplate to docs/pallas/CHANGELOG.\n   * JAX nightly packages are now published to artifact registry. To install\n     these packages, see the [JAX installation guide](https://docs.jax.dev/en/latest/installation.html#jax-nightly-installation).\n   * `jax.sharding.PartitionSpec` no longer inherits from a tuple.\n+  * `jax.ShapeDtypeStruct` is immutable now. Please use `.update` method to\n+    update your `ShapeDtypeStruct` instead of doing in-place updates.\n \n ## JAX 0.6.0 (April 16, 2025)\n \ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 5c8a86c035e6..9b4ce0fa0f24 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -2870,6 +2870,17 @@ def __hash__(self):\n     # https://github.com/jax-ml/jax/issues/8182\n     return hash((self.shape, self.dtype, self.sharding, self.layout, self.weak_type))\n \n+  def __setattr__(self, name, value):\n+    if hasattr(self, name):\n+      if getattr(self, name) == value:\n+        # This can to happen if two threads race, for example if two threads\n+        # are trying to hash the same SDS instance.\n+        return\n+      raise RuntimeError(\n+          f\"Cannot reassign attributes ({name}) of immutable ShapeDtypeStruct\"\n+          \" objects\")\n+    super().__setattr__(name, value)\n+\n   def update(self, **kwargs):\n     if 'sharding' in kwargs:\n       s = kwargs['sharding']\n@@ -2888,6 +2899,7 @@ def update(self, **kwargs):\n         sharding=sharding,\n         weak_type=kwargs.pop('weak_type', self.weak_type))\n \n+\n def _sds_aval_mapping(x):\n   aval = ShapedArray(\n       x.shape, dtypes.canonicalize_dtype(x.dtype, allow_extended_dtype=True),\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Reverts 8137c37e324c9cb5c8f991a16d78310b6e37bd05\n\nPiperOrigin-RevId: 756850393",
        "output": [
            "```diff\nCommit: 701af068593bbf44e540f1641a86697d27afadd2\nDate: 2025-05-09T18:49:42Z\nURL: https://github.com/jax-ml/jax/commit/701af068593bbf44e540f1641a86697d27afadd2\nFiles changed: 2\nAdditions: +5, Deletions: -1\ndiff --git a/jax/_src/array.py b/jax/_src/array.py\nindex f2b070c8221d..422fa5086e62 100644\n--- a/jax/_src/array.py\n+++ b/jax/_src/array.py\n@@ -636,7 +636,8 @@ def _value(self) -> np.ndarray:\n     self._check_if_deleted()\n \n     if self._npy_value is None:\n-      if self.is_fully_replicated:\n+      if (self.is_fully_replicated and\n+          self.sharding._internal_device_list.addressable_device_list):  # type: ignore\n         npy_value, did_copy = self._single_device_array_to_np_array_did_copy()\n         npy_value.flags.writeable = False\n         if did_copy:\ndiff --git a/jaxlib/py_array.cc b/jaxlib/py_array.cc\nindex 022c7a831c92..1222d410bad8 100644\n--- a/jaxlib/py_array.cc\n+++ b/jaxlib/py_array.cc\n@@ -1528,6 +1528,9 @@ int PyArray_bf_getbuffer(PyObject* exporter, Py_buffer* view, int flags) {\n     absl::Span<const std::shared_ptr<PjRtBuffer>> buffers =\n         array->pjrt_buffers();\n \n+    if (buffers.empty()) {\n+      return InvalidArgument(\"Array has no buffers.\");\n+    }\n     PjRtBuffer& buffer = *buffers.front();\n     if (!buffer.IsOnCpu()) {\n       return InvalidArgument(\n\n```"
        ],
        "from_id": [
            "emilyfertig",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28642 from hawkinsp:tsan\n\nPiperOrigin-RevId: 756828256",
        "output": [
            "```diff\nCommit: a4612e41e8b3d8b7d7c9a1281ae6608870d8dccb\nDate: 2025-05-09T17:52:19Z\nURL: https://github.com/jax-ml/jax/commit/a4612e41e8b3d8b7d7c9a1281ae6608870d8dccb\nFiles changed: 1\nAdditions: +2, Deletions: -11\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex 596bc425bfeb..882e140b91ad 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -31,7 +31,7 @@ jobs:\n             requirements_lock_name: \"requirements_lock_3_13_ft\"\n           - name-prefix: \"with 3.14\"\n             python-version: \"3.14\"\n-            github_branch: \"main\"\n+            github_branch: \"3.14\"\n             requirements_lock_name: \"requirements_lock_3_14_ft\"\n     defaults:\n       run:\n@@ -133,9 +133,6 @@ jobs:\n \n           python3 -m pip install uv~=0.5.30\n \n-          # Install Cython same as in numpy CI: https://github.com/numpy/numpy/blob/9ead596ce4f8df0189f9ba3d54937e22e2785a5e/.github/workflows/linux.yml#L75C21-L75C96\n-          python3 -m uv pip install -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple cython\n-\n           python3 -m uv pip install -r requirements/build_requirements.txt\n \n           CC=clang-18 CXX=clang++-18 python3 -m pip wheel --wheel-dir dist -v . --no-build-isolation -Csetup-args=-Db_sanitize=thread -Csetup-args=-Dbuildtype=debugoptimized\n@@ -204,11 +201,8 @@ jobs:\n \n           python3 -m pip install uv~=0.5.30\n \n-          # Install Cython same as in numpy CI: https://github.com/numpy/numpy/blob/9ead596ce4f8df0189f9ba3d54937e22e2785a5e/.github/workflows/linux.yml#L75C21-L75C96\n-          python3 -m uv pip install -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple cython\n-\n           python3 -m uv pip install -U --pre numpy --extra-index-url file://${GITHUB_WORKSPACE}/wheelhouse/\n-          python3 -m uv pip install pythran pybind11 meson-python ninja\n+          python3 -m uv pip install cython pythran pybind11 meson-python ninja\n \n           python3 -m uv pip list | grep -E \"(numpy|pythran|cython|pybind11)\"\n \n@@ -216,8 +210,6 @@ jobs:\n           export CXX=clang++-18\n           python3 -m pip wheel --wheel-dir dist -vvv . --no-build-isolation --no-deps -Csetup-args=-Dbuildtype=debugoptimized\n \n-          python3 -m uv pip list | grep -E \"(numpy|pythran|cython|pybind11)\"\n-\n           # Create simple index and copy the wheel\n           mkdir -p ${GITHUB_WORKSPACE}/wheelhouse/scipy\n \n@@ -266,7 +258,6 @@ jobs:\n           export PYTHON_SHA256=($(sha256sum ${GITHUB_WORKSPACE}/python-tsan.tgz))\n           echo \"Python sha256: ${PYTHON_SHA256}\"\n \n-          python3 -VV\n           python3 build/build.py build --configure_only \\\n             --python_version=${{ matrix.python-version }}-ft \\\n             --bazel_options=--repo_env=HERMETIC_PYTHON_URL=\"file://${GITHUB_WORKSPACE}/python-tsan.tgz\" \\\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Move sitofp lowering to Mosaic.\n\nAlso, the compatibility check in fptosi is likely wrong - we should check if both src and dst bit widths < 32, not just dst. Correct it while I'm here.\n\nPiperOrigin-RevId: 756816853",
        "output": [
            "```diff\nCommit: 9eb8c9eacbda9f395c2561889af1866b36be7a23\nDate: 2025-05-09T17:22:11Z\nURL: https://github.com/jax-ml/jax/commit/9eb8c9eacbda9f395c2561889af1866b36be7a23\nFiles changed: 3\nAdditions: +76, Deletions: -23\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex d7e26ec3b342..cdc64bbf96f4 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -2177,11 +2177,14 @@ def _convert_element_type_lowering_rule(\n     elif jnp.iinfo(old_dtype).bits == jnp.iinfo(new_dtype).bits:\n       # This case triggers when casting signed to unsigned or vice versa.\n       return x\n-  # TODO(apaszke): Remove both_32bit constraints using the Mosaic canonicalizer.\n   elif _from(floating) and _to(signed):\n     return arith.fptosi(out_type, x)\n-  elif _from(signed) and _to(floating) and both_32bit:\n-    return arith.sitofp(out_type, x)\n+  elif _from(signed) and _to(floating):\n+    if (\n+        not (ctx.forward_compatible or is_cloud_tpu_older_than(2025, 5, 12))\n+        or both_32bit\n+    ):\n+      return arith.sitofp(out_type, x)\n   elif old_dtype == jnp.bool_ and _to(integer) and new_dtype.itemsize == 4:\n     return arith.extui(out_type, x)\n   return lower_fun(functools.partial(_convert_helper, to_dtype=new_dtype),\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\nindex 110550127ca5..258fec8caff1 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n@@ -44,13 +44,13 @@ limitations under the License.\n #include \"mlir/IR/ImplicitLocOpBuilder.h\"\n #include \"mlir/IR/OpDefinition.h\"\n #include \"mlir/IR/Operation.h\"\n-#include \"mlir/IR/PatternMatch.h\"\n #include \"mlir/IR/Region.h\"\n #include \"mlir/IR/Value.h\"\n #include \"mlir/Pass/Pass.h\"\n #include \"mlir/Support/LLVM.h\"\n #include \"mlir/Support/LogicalResult.h\"\n #include \"jaxlib/mosaic/dialect/tpu/tpu_dialect.h\"\n+#include \"jaxlib/mosaic/dialect/tpu/util.h\"\n #include \"jaxlib/mosaic/dialect/tpu/vreg_util.h\"\n \n namespace mlir::tpu {\n@@ -601,14 +601,10 @@ LogicalResult canonicalize_fptosi(const CanonicalizeContext &ctx,\n     return op.emitOpError(\"Vector/scalar mismatch between input and output\");\n   }\n   bool is_vector = static_cast<bool>(src_vty);\n-  unsigned src_bitwidth, dst_bitwidth;\n-  if (is_vector) {\n-    src_bitwidth = src_vty.getElementTypeBitWidth();\n-    dst_bitwidth = dst_vty.getElementTypeBitWidth();\n-  } else {\n-    src_bitwidth = op.getIn().getType().getIntOrFloatBitWidth();\n-    dst_bitwidth = op.getType().getIntOrFloatBitWidth();\n-  }\n+  FAILUREOR_ASSIGN_OR_RETURN(const unsigned src_bitwidth,\n+                             getElementTypeBitwidth(op.getIn().getType()));\n+  FAILUREOR_ASSIGN_OR_RETURN(const unsigned dst_bitwidth,\n+                             getElementTypeBitwidth(op.getType()));\n   if (dst_bitwidth > 32) {\n     return op.emitOpError(\"Target bitwidth too large\");\n   }\n@@ -623,6 +619,14 @@ LogicalResult canonicalize_fptosi(const CanonicalizeContext &ctx,\n     op.erase();\n     return success();\n   }\n+\n+  if ((src_bitwidth < 32 || dst_bitwidth < 32) && !ctx.compatibility_mode) {\n+    return op.emitOpError(\n+        \"On this target float-to-integer conversions can only happen on \"\n+        \"32-bit values. Enable compatibility mode or upcast to float32, cast \"\n+        \"to int32 and truncate to desired bitwidth.\");\n+  }\n+\n   Value x = op.getIn();\n   // Upcast the input to f32.\n   if (src_bitwidth < 32) {\n@@ -634,11 +638,6 @@ LogicalResult canonicalize_fptosi(const CanonicalizeContext &ctx,\n     }\n   }\n   if (dst_bitwidth < 32) {\n-    if (!ctx.compatibility_mode) {\n-      return op.emitOpError(\n-          \"On this target only float-to-integer conversions can only happen on \"\n-          \"32-bit values. Enable compatibility mode or upcast to float32.\");\n-    }\n     // Need to clip values to match XLA\n     auto clip = [&](Value x, Value low, Value high) {\n       x = builder.create<arith::MaximumFOp>(x, low);\n@@ -666,12 +665,6 @@ LogicalResult canonicalize_fptosi(const CanonicalizeContext &ctx,\n     x = builder.create<arith::FPToSIOp>(builder.getI32Type(), x);\n   }\n   if (dst_bitwidth < 32) {\n-    if (!ctx.compatibility_mode) {\n-      return op.emitOpError(\n-          \"On this target only float-to-integer conversions can only happen on \"\n-          \"32-bit values. Enable compatibility mode or cast to int32 and \"\n-          \"truncate later.\");\n-    }\n     x = builder.create<arith::TruncIOp>(op.getType(), x);\n   }\n   op.replaceAllUsesWith(x);\n@@ -679,6 +672,52 @@ LogicalResult canonicalize_fptosi(const CanonicalizeContext &ctx,\n   return success();\n }\n \n+LogicalResult canonicalize_sitofp(const CanonicalizeContext &ctx,\n+                                  Operation &raw_op) {\n+  auto op = cast<arith::FPToSIOp>(raw_op);\n+  ImplicitLocOpBuilder builder(op->getLoc(), op.getOperation());\n+  auto src_vty = dyn_cast<VectorType>(op.getIn().getType());\n+  auto dst_vty = dyn_cast<VectorType>(op.getType());\n+  if (static_cast<bool>(src_vty) != static_cast<bool>(dst_vty)) {\n+    return op.emitOpError(\"Vector/scalar mismatch between input and output\");\n+  }\n+  bool is_vector = static_cast<bool>(src_vty);\n+  FAILUREOR_ASSIGN_OR_RETURN(const unsigned src_bitwidth,\n+                             getElementTypeBitwidth(op.getIn().getType()));\n+  FAILUREOR_ASSIGN_OR_RETURN(const unsigned dst_bitwidth,\n+                             getElementTypeBitwidth(op.getType()));\n+\n+  if ((src_bitwidth < 32 || dst_bitwidth < 32) && !ctx.compatibility_mode) {\n+    return op.emitOpError(\n+        \"On this target integer-to-float conversions can only happen on \"\n+        \"32-bit values. Enable compatibility mode or upcast to int32, cast to \"\n+        \"float32 and truncate to desired bitwidth.\");\n+  }\n+\n+  // Canonicalize (intX -> floatY) to (intX -> int32 -> float32 -> floatY).\n+  Value x = op.getIn();\n+  if (src_bitwidth < 32) {\n+    if (is_vector) {\n+      x = builder.create<arith::ExtSIOp>(\n+          VectorType::get(src_vty.getShape(), builder.getI32Type()), x);\n+    } else {\n+      x = builder.create<arith::ExtSIOp>(builder.getI32Type(), x);\n+    }\n+  }\n+  if (is_vector) {\n+    x = builder.create<arith::SIToFPOp>(\n+        VectorType::get(src_vty.getShape(), builder.getF32Type()), x);\n+  } else {\n+    x = builder.create<arith::SIToFPOp>(builder.getF32Type(), x);\n+  }\n+  if (dst_bitwidth < 32) {\n+    x = builder.create<arith::TruncFOp>(op.getType(), x);\n+  }\n+  op.replaceAllUsesWith(x);\n+  op.erase();\n+  return success();\n+}\n+\n LogicalResult canonicalize_repeat(const CanonicalizeContext &ctx,\n                                   Operation &raw_op) {\n   auto op = dyn_cast<tpu::RepeatOp>(raw_op);\n@@ -727,6 +766,7 @@ const llvm::StringMap<canonicalize_rule_type> &rules() {\n       {vector::TransposeOp::getOperationName(), canonicalize_vector_transpose},\n       {arith::SelectOp::getOperationName(), canonicalize_select},\n       {arith::FPToSIOp::getOperationName(), canonicalize_fptosi},\n+      {arith::SIToFPOp::getOperationName(), canonicalize_sitofp},\n       {tpu::RepeatOp::getOperationName(), canonicalize_repeat}};\n   return *rules;\n }\ndiff --git a/jaxlib/mosaic/dialect/tpu/util.h b/jaxlib/mosaic/dialect/tpu/util.h\nindex eed0df14f707..b9aea1b087dc 100644\n--- a/jaxlib/mosaic/dialect/tpu/util.h\n+++ b/jaxlib/mosaic/dialect/tpu/util.h\n@@ -180,6 +180,16 @@ FailureOr<int8_t> getTypeBitwidth(Type ty) {\n          << ty;\n }\n \n+// Returns the bitwidth of the element type. The function works for both\n+// scalar and vector types.\n+template <bool adjust_bool = false>\n+inline FailureOr<int8_t> getElementTypeBitwidth(Type ty) {\n+  if (auto vty = dyn_cast<VectorType>(ty)) {\n+    return getTypeBitwidth<adjust_bool>(vty.getElementType());\n+  }\n+  return getTypeBitwidth<adjust_bool>(ty);\n+}\n+\n template <typename T>\n ArrayRef<std::remove_const_t<T>> toArrayRef(absl::Span<T> span) {\n   return ArrayRef<std::remove_const_t<T>>(span.data(), span.size());\n\n```"
        ],
        "from_id": [
            "WindQAQ",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Use cython from pypi in tsan CI build.\n\nCython 3.1 was released, which means we no longer need a prerelease of cython for free-threaded builds.",
        "output": [
            "```diff\nCommit: 7caebde796b60cf7ff2763b5caa2f4e78b40ca12\nDate: 2025-05-09T17:17:00Z\nURL: https://github.com/jax-ml/jax/commit/7caebde796b60cf7ff2763b5caa2f4e78b40ca12\nFiles changed: 1\nAdditions: +2, Deletions: -11\ndiff --git a/.github/workflows/tsan.yaml b/.github/workflows/tsan.yaml\nindex 596bc425bfeb..882e140b91ad 100644\n--- a/.github/workflows/tsan.yaml\n+++ b/.github/workflows/tsan.yaml\n@@ -31,7 +31,7 @@ jobs:\n             requirements_lock_name: \"requirements_lock_3_13_ft\"\n           - name-prefix: \"with 3.14\"\n             python-version: \"3.14\"\n-            github_branch: \"main\"\n+            github_branch: \"3.14\"\n             requirements_lock_name: \"requirements_lock_3_14_ft\"\n     defaults:\n       run:\n@@ -133,9 +133,6 @@ jobs:\n \n           python3 -m pip install uv~=0.5.30\n \n-          # Install Cython same as in numpy CI: https://github.com/numpy/numpy/blob/9ead596ce4f8df0189f9ba3d54937e22e2785a5e/.github/workflows/linux.yml#L75C21-L75C96\n-          python3 -m uv pip install -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple cython\n-\n           python3 -m uv pip install -r requirements/build_requirements.txt\n \n           CC=clang-18 CXX=clang++-18 python3 -m pip wheel --wheel-dir dist -v . --no-build-isolation -Csetup-args=-Db_sanitize=thread -Csetup-args=-Dbuildtype=debugoptimized\n@@ -204,11 +201,8 @@ jobs:\n \n           python3 -m pip install uv~=0.5.30\n \n-          # Install Cython same as in numpy CI: https://github.com/numpy/numpy/blob/9ead596ce4f8df0189f9ba3d54937e22e2785a5e/.github/workflows/linux.yml#L75C21-L75C96\n-          python3 -m uv pip install -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple cython\n-\n           python3 -m uv pip install -U --pre numpy --extra-index-url file://${GITHUB_WORKSPACE}/wheelhouse/\n-          python3 -m uv pip install pythran pybind11 meson-python ninja\n+          python3 -m uv pip install cython pythran pybind11 meson-python ninja\n \n           python3 -m uv pip list | grep -E \"(numpy|pythran|cython|pybind11)\"\n \n@@ -216,8 +210,6 @@ jobs:\n           export CXX=clang++-18\n           python3 -m pip wheel --wheel-dir dist -vvv . --no-build-isolation --no-deps -Csetup-args=-Dbuildtype=debugoptimized\n \n-          python3 -m uv pip list | grep -E \"(numpy|pythran|cython|pybind11)\"\n-\n           # Create simple index and copy the wheel\n           mkdir -p ${GITHUB_WORKSPACE}/wheelhouse/scipy\n \n@@ -266,7 +258,6 @@ jobs:\n           export PYTHON_SHA256=($(sha256sum ${GITHUB_WORKSPACE}/python-tsan.tgz))\n           echo \"Python sha256: ${PYTHON_SHA256}\"\n \n-          python3 -VV\n           python3 build/build.py build --configure_only \\\n             --python_version=${{ matrix.python-version }}-ft \\\n             --bazel_options=--repo_env=HERMETIC_PYTHON_URL=\"file://${GITHUB_WORKSPACE}/python-tsan.tgz\" \\\n\n```"
        ],
        "from_id": [
            "hawkinsp"
        ]
    },
    {
        "text_input": "Add `.update` to ShapeDtypeStruct\n\nPiperOrigin-RevId: 756804171",
        "output": [
            "```diff\nCommit: e194d532a46903b8ca5d811ab4d63c6bb401c367\nDate: 2025-05-09T16:49:55Z\nURL: https://github.com/jax-ml/jax/commit/e194d532a46903b8ca5d811ab4d63c6bb401c367\nFiles changed: 2\nAdditions: +53, Deletions: -0\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex 379f6d8d7c93..5c8a86c035e6 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -2870,6 +2870,24 @@ def __hash__(self):\n     # https://github.com/jax-ml/jax/issues/8182\n     return hash((self.shape, self.dtype, self.sharding, self.layout, self.weak_type))\n \n+  def update(self, **kwargs):\n+    if 'sharding' in kwargs:\n+      s = kwargs['sharding']\n+      if self._dll is not None and isinstance(s, Sharding):\n+        raise ValueError(\n+            f\"You are updating ShapeDtypeStruct with a {type(s)} when the\"\n+            f\" original ShapeDtypeStruct had a concrete layout {self.layout}.\"\n+            \" This might lead to bugs. If you want to do this, create a new\"\n+            \" ShapeDtypeStruct via the constructor.\")\n+      sharding = s\n+    else:\n+      sharding = self.layout\n+    return ShapeDtypeStruct(\n+        shape=kwargs.pop('shape', self.shape),\n+        dtype=kwargs.pop('dtype', self.dtype),\n+        sharding=sharding,\n+        weak_type=kwargs.pop('weak_type', self.weak_type))\n+\n def _sds_aval_mapping(x):\n   aval = ShapedArray(\n       x.shape, dtypes.canonicalize_dtype(x.dtype, allow_extended_dtype=True),\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex e4b91aa17644..55c38f6c2ef8 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -55,6 +55,7 @@\n     SingleDeviceSharding, parse_flatten_op_sharding)\n from jax._src.pjit import (pjit, mesh_cast, auto_axes, explicit_axes,\n                            use_auto_axes, use_explicit_axes, reshard)\n+from jax._src.layout import Layout, DeviceLocalLayout as DLL\n from jax._src.named_sharding import DuplicateSpecError\n from jax._src import mesh as mesh_lib\n from jax._src.mesh import AxisType\n@@ -4973,6 +4974,40 @@ def make_keys(seeds):\n     self.assertEqual(out.shape, input_shape)\n     jax.random.key_data(out)  # doesn't crash\n \n+  def test_sds_update(self):\n+    mesh = jtu.create_mesh((2, 1), ('x', 'y'))\n+    s1 = jax.ShapeDtypeStruct((2, 2), jnp.int32)\n+    s1_u = s1.update(shape=(4, 2), dtype=np.float32)\n+    self.assertEqual(s1_u.shape, (4, 2))\n+    self.assertEqual(s1_u.dtype, np.float32)\n+    self.assertFalse(s1_u.weak_type)\n+\n+    s2 = jax.ShapeDtypeStruct((2, 2), jnp.int32)\n+    s2_u = s2.update(shape=(4, 2), weak_type=True)\n+    self.assertEqual(s2_u.shape, (4, 2))\n+    self.assertEqual(s2_u.dtype, np.int32)\n+    self.assertTrue(s2_u.weak_type)\n+\n+    s3 = jax.ShapeDtypeStruct((2, 2), jnp.int32,\n+                              sharding=NamedSharding(mesh, P()))\n+    s3_u = s3.update(sharding=NamedSharding(mesh, P('x')))\n+    self.assertEqual(s3_u.sharding, NamedSharding(mesh, P('x')))\n+\n+    s32_u = s3.update(shape=(4, 2))\n+    self.assertEqual(s32_u.shape, (4, 2))\n+    self.assertEqual(s32_u.sharding, NamedSharding(mesh, P()))\n+\n+    sh = NamedSharding(mesh, P())\n+    s4 = jax.ShapeDtypeStruct((2, 2), jnp.int32,\n+                              sharding=Layout(DLL((0, 1)), sh))\n+    new_layout = Layout(DLL((1, 0)), NamedSharding(mesh, P('x')))\n+    s4_u = s4.update(sharding=new_layout)\n+    self.assertEqual(s4_u.sharding, new_layout.sharding)\n+    self.assertEqual(s4_u.layout, new_layout)\n+\n+    with self.assertRaisesRegex(ValueError, \"updating ShapeDtypeStruct\"):\n+      s4.update(sharding=NamedSharding(mesh, P('x')))\n+\n \n def spec_regex(s):\n   return str(s).replace(r\"(\", r\"\\(\").replace(r\")\", r\"\\)\")\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28608 from dfm:pp-custom-jvp\n\nPiperOrigin-RevId: 756760690",
        "output": [
            "```diff\nCommit: 87a8051ed83a8abbfcbe7c884d68a8130a36929d\nDate: 2025-05-09T14:37:52Z\nURL: https://github.com/jax-ml/jax/commit/87a8051ed83a8abbfcbe7c884d68a8130a36929d\nFiles changed: 2\nAdditions: +42, Deletions: -0\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex a8f136477bd9..dc8fc90e3d1f 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -487,6 +487,21 @@ def dce_jvp_jaxpr_thunk(*in_zeros):\n pe.dce_rules[custom_jvp_call_p] = _custom_jvp_call_dce\n \n \n+def _custom_jvp_call_pp_rule(eqn: core.JaxprEqn,\n+                             context: core.JaxprPpContext,\n+                             settings: core.JaxprPpSettings) -> core.pp.Doc:\n+  params = dict(eqn.params)\n+  if not params[\"num_consts\"]:\n+    params.pop(\"num_consts\")\n+  params[\"jvp\"] = params.pop(\"jvp_jaxpr_fun\").debug_info.func_name\n+  names = sorted(params)\n+  params[\"name\"] = params[\"call_jaxpr\"].jaxpr.debug_info.func_name\n+  return core._pp_eqn(eqn.replace(params=params), context, settings,\n+                      params=[\"name\"] + names)\n+\n+\n+core.pp_eqn_rules[custom_jvp_call_p] = _custom_jvp_call_pp_rule\n+\n ### VJPs\n \n @custom_api_util.register_custom_decorator_type\ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex d75986be738e..48ce56ee935c 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -34,6 +34,7 @@\n import re\n import subprocess\n import sys\n+import textwrap\n import traceback\n import types\n from typing import NamedTuple\n@@ -8418,6 +8419,32 @@ def f_jvp(x, t):\n     x = jnp.arange(3.0)\n     jax.jvp(jax.vmap(jax.jit(f)), (x,), (x,))  # doesn't crash\n \n+  def test_pretty_print(self):\n+    @jax.custom_jvp\n+    def f(x):\n+      return x + 1\n+\n+    @f.defjvp\n+    def f_jvp(primals, tangents):\n+      return f(*primals), tangents[0]\n+\n+    x = jnp.array([4.2], dtype=jnp.float32)\n+    jaxpr = jax.make_jaxpr(f)(x)\n+    actual = jaxpr.pretty_print(use_color=False)\n+    expected = textwrap.dedent(\n+        \"\"\"\n+        { lambda ; a:f32[1]. let\n+            b:f32[1] = custom_jvp_call[\n+              name=f\n+              call_jaxpr={ lambda ; c:f32[1]. let d:f32[1] = add c 1.0:f32[] in (d,) }\n+              jvp=f_jvp\n+              symbolic_zeros=False\n+            ] a\n+          in (b,) }\n+        \"\"\").strip()\n+    self.assertEqual(actual, expected)\n+\n+\n \n class CustomVJPTest(jtu.JaxTestCase):\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas/TPU]\n* Ensure we wrap scheduler initialize/finalize with grid_env\n* Add support for basic swaps/gets to pull_block_spec\n\nPiperOrigin-RevId: 756747431",
        "output": [
            "```diff\nCommit: 60892e691fcad26f8e81900030fe1d498fff9f16\nDate: 2025-05-09T13:52:13Z\nURL: https://github.com/jax-ml/jax/commit/60892e691fcad26f8e81900030fe1d498fff9f16\nFiles changed: 3\nAdditions: +267, Deletions: -2\ndiff --git a/jax/_src/pallas/fuser/block_spec.py b/jax/_src/pallas/fuser/block_spec.py\nindex ba2c182014b5..3d4df549949c 100644\n--- a/jax/_src/pallas/fuser/block_spec.py\n+++ b/jax/_src/pallas/fuser/block_spec.py\n@@ -29,6 +29,7 @@\n from jax._src import core\n from jax._src import custom_derivatives\n from jax._src import pjit\n+from jax._src import state\n from jax._src import tree_util\n from jax._src import util\n from jax._src.interpreters import partial_eval as pe\n@@ -451,6 +452,8 @@ def _remove_nones(\n   _no_aval = object()\n \n   def _get_block_aval(bs, aval):\n+    if isinstance(aval, state.AbstractRef):\n+      return aval\n     if bs is pallas_core.no_block_spec or bs is None:\n       return _no_aval\n     return aval.update(shape=_remove_nones(bs.block_shape))  # pytype: disable=attribute-error\n@@ -1065,6 +1068,171 @@ def new_index_map(*args):\n       len(ctx.avals_in) - 1\n   )\n \n+@register_pull_block_spec_rule(state_primitives.swap_p)\n+def _swap_pull_rule(\n+    ctx: PullRuleContext,\n+    block_spec: pallas_core.BlockSpec,\n+    **kwargs,\n+):\n+  del ctx, kwargs\n+  # The output and val block spec are the same.\n+  return [block_spec, block_spec]\n+\n+@register_eval_rule(state_primitives.swap_p)\n+def _swap_eval_rule(\n+    ctx: KernelEvalContext,\n+    ref,\n+    val,\n+    *idx,\n+    tree\n+):\n+  indexers = tree_util.tree_unflatten(tree, idx)\n+  ref_aval, _ = ctx.avals_in[:2]\n+  indexers_avals = tree_util.tree_unflatten(tree, ctx.avals_in[2:])\n+  assert hasattr(ref_aval, 'shape')\n+  if len(indexers) > 1:\n+    raise NotImplementedError('swap not supported yet')\n+  indexer_aval = indexers_avals[0]\n+  for idx_aval, size in zip(indexer_aval.indices, ref_aval.shape, strict=True):\n+    if not isinstance(idx_aval, indexing.Slice):\n+      raise NotImplementedError('swap not supported yet')\n+    if not isinstance(idx_aval.start, int):\n+      raise NotImplementedError('swap not supported yet')\n+    if not isinstance(idx_aval.size, int):\n+      raise NotImplementedError('swap not supported yet')\n+    if idx_aval.stride != 1:\n+      raise NotImplementedError('swap not supported yet')\n+    if idx_aval.start != 0:\n+      raise NotImplementedError('swap not supported yet')\n+    if idx_aval.size != size:\n+      raise NotImplementedError('swap not supported yet')\n+  # We have a pure slice so now we can just re-index the ref according to the\n+  # block indices.\n+  block_spec = ctx.out_block_specs[0]\n+  block_idx = ctx.get_out_block_indices()[0]\n+\n+  def _slice(i, b):\n+    if not isinstance(b, int):\n+      raise NotImplementedError('swap not supported yet')\n+    return i if b is None else indexing.ds(i * b, b)\n+\n+  indexer = tuple(\n+      _slice(i, b) for i, b in zip(block_idx, block_spec.block_shape,\n+                                   strict=True)\n+  )\n+  return ref.swap(val, idx=indexer)\n+\n+@register_pull_block_spec_rule(state_primitives.get_p)\n+def _get_pull_rule(\n+    ctx: PullRuleContext,\n+    block_spec: pallas_core.BlockSpec,\n+    *,\n+    tree\n+):\n+  ref_aval = ctx.avals_in[0]\n+  assert hasattr(ref_aval, 'shape')\n+  indexers_avals = tree_util.tree_unflatten(tree, ctx.avals_in[1:])\n+  if len(indexers_avals) > 1:\n+    raise NotImplementedError('get not supported yet')\n+  indexer_aval = indexers_avals[0]\n+  block_shape_iter = iter(block_spec.block_shape)\n+  block_shape = []\n+  if not all(\n+      isinstance(bd, (int, pallas_core.Blocked, pallas_core.Squeezed, None))\n+      for bd in block_spec.block_shape\n+  ):\n+    raise NotImplementedError('get not supported yet')\n+  for idx_aval, size in zip(indexer_aval.indices, ref_aval.shape, strict=True):\n+    if not isinstance(idx_aval, indexing.Slice):\n+      assert hasattr(idx_aval, 'shape') and not idx_aval.shape\n+      block_shape.append(pallas_core.Squeezed())\n+      continue\n+    if not isinstance(idx_aval.start, int):\n+      raise NotImplementedError('get not supported yet')\n+    if not isinstance(idx_aval.size, int):\n+      raise NotImplementedError('get not supported yet')\n+    if idx_aval.stride != 1:\n+      raise NotImplementedError('get not supported yet')\n+    if idx_aval.start != 0:\n+      raise NotImplementedError('get not supported yet')\n+    if idx_aval.size != size:\n+      raise NotImplementedError('get not supported yet')\n+    bd = next(block_shape_iter)\n+    block_shape.append(_block_size(bd))\n+  assert next(block_shape_iter, None) is None\n+  def new_index_map(*args):\n+    idx = block_spec.index_map(*args)\n+    idx_iter = iter(idx)\n+    indices = tuple(\n+        0\n+        if (bd is None or isinstance(bd, pallas_core.Squeezed))\n+        else next(idx_iter)\n+        for bd in range(len(block_shape))\n+    )\n+    assert next(idx_iter, None) is None\n+    return indices\n+  block_spec = pallas_core.BlockSpec(block_shape, new_index_map)\n+  return [block_spec] + [pallas_core.no_block_spec] * (len(ctx.avals_in) - 1)\n+\n+@register_eval_rule(state_primitives.get_p)\n+def _get_eval_rule(\n+    ctx: KernelEvalContext,\n+    ref,\n+    *idx,\n+    tree\n+):\n+  indexers = tree_util.tree_unflatten(tree, idx)\n+  ref_aval = ctx.avals_in[0]\n+  indexers_avals = tree_util.tree_unflatten(tree, ctx.avals_in[1:])\n+  ref_block_spec = ctx.in_block_specs[0]\n+  assert hasattr(ref_aval, 'shape')\n+  if len(indexers) > 1:\n+    raise NotImplementedError('get not supported yet')\n+  indexer = indexers[0]\n+  indexer_aval = indexers_avals[0]\n+  block_indexer = []\n+\n+  def _slice(i, b):\n+    match b:\n+      case int():\n+        return indexing.ds(i * b, b)\n+      case pallas_core.Blocked(bs):\n+        return indexing.ds(i * bs, bs)\n+      case pallas_core.Squeezed() | None:\n+        return i\n+      case _:\n+        raise NotImplementedError('get not supported yet')\n+\n+  if ref_block_spec is pallas_core.no_block_spec:\n+    # Short-circuit if the ref is not blocked.\n+    return state_primitives.get_p.bind(ref, *idx, tree=tree)\n+  block_idx_iter = iter(ctx.get_out_block_indices()[0])\n+  for idx_aval, size, idx, bd in zip(\n+      indexer_aval.indices,\n+      ref_aval.shape,\n+      indexer.indices,\n+      ref_block_spec.block_shape,\n+      strict=True,\n+  ):\n+    if not isinstance(idx_aval, indexing.Slice):\n+      assert hasattr(idx_aval, 'shape') and not idx_aval.shape, idx_aval\n+      assert bd is None or isinstance(bd, pallas_core.Squeezed)\n+      block_indexer.append(idx)\n+      continue\n+    if not isinstance(idx_aval.start, int):\n+      raise NotImplementedError('get not supported yet')\n+    if not isinstance(idx_aval.size, int):\n+      raise NotImplementedError('get not supported yet')\n+    if idx_aval.stride != 1:\n+      raise NotImplementedError('get not supported yet')\n+    if idx_aval.start != 0:\n+      raise NotImplementedError('get not supported yet')\n+    if idx_aval.size != size:\n+      raise NotImplementedError('get not supported yet')\n+    bidx = next(block_idx_iter)\n+    block_indexer.append(_slice(bidx, bd))\n+  assert next(block_idx_iter, None) is None\n+  return ref.get(idx=tuple(block_indexer))\n \n @register_eval_rule(lax.concatenate_p)\n def _concatenate_eval_rule(ctx: KernelEvalContext, *args, dimension):\ndiff --git a/jax/_src/pallas/mosaic/pipeline.py b/jax/_src/pallas/mosaic/pipeline.py\nindex df7be297c9e8..4ef22179260b 100644\n--- a/jax/_src/pallas/mosaic/pipeline.py\n+++ b/jax/_src/pallas/mosaic/pipeline.py\n@@ -1369,7 +1369,8 @@ def _():\n       initial_indices = (0,) * len(grid)\n       scheduler = make_scheduler(0, initial_indices)\n       brefs = map_brefs(scheduler.alias_local_refs, allocations, refs)\n-      map_brefs(scheduler.initialize, brefs, refs, schedule)\n+      with scheduler.grid_env():\n+        map_brefs(scheduler.initialize, brefs, refs, schedule)\n \n       # pipeline loop\n       next_indices = lax.fori_loop(0, num_steps, loop_body, initial_indices)\n@@ -1378,7 +1379,8 @@ def _():\n       final_indices = _prev_index(next_indices, grid)\n       scheduler = make_scheduler(num_steps - 1, final_indices)\n       brefs = map_brefs(scheduler.alias_local_refs, allocations, refs)\n-      map_brefs(scheduler.finalize, brefs, refs, schedule)\n+      with scheduler.grid_env():\n+        map_brefs(scheduler.finalize, brefs, refs, schedule)\n \n   return pipeline\n \ndiff --git a/tests/pallas/fuser_block_spec_test.py b/tests/pallas/fuser_block_spec_test.py\nindex 665cdfb1dd6b..f7e70ec1d708 100644\n--- a/tests/pallas/fuser_block_spec_test.py\n+++ b/tests/pallas/fuser_block_spec_test.py\n@@ -761,6 +761,101 @@ def f(x):\n     y = kernel_fn((0, 1, 2), scalar_prefetch_values, (), x)\n     np.testing.assert_array_equal(y, x.reshape((256, 1024)))\n \n+  def test_basic_swap(self):\n+    value = jnp.arange((512 * 1024), dtype=jnp.int32).reshape((512, 1024)) * 2\n+    x = jnp.zeros((256, 512), dtype=jnp.int32)\n+    def outer(refs):\n+      ref, y_ref = refs\n+      def f(x):\n+        return ref.swap(x)\n+      in_type = jax.ShapeDtypeStruct((512, 1024), jnp.int32)\n+      f2, new_values, scalar_prefetch_values = block_spec_lib.get_fusion_values(\n+          f, in_type\n+      )\n+      self.assertLen(new_values, 1)  # Captures Ref\n+      self.assertEmpty(scalar_prefetch_values)\n+\n+      block_spec = pl.BlockSpec((256, 512), lambda i, j, k: (i, k))\n+      kernel_fn, (value_block_specs, x_block_spec), _ = (\n+          block_spec_lib.pull_block_spec(\n+              f2,\n+              block_spec,\n+              grid=(2, 3, 4),\n+              scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+          )(new_values, in_type)\n+      )\n+      self.assertLen(value_block_specs, 1)\n+      self.assertEqual(x_block_spec.index_map(0, 1, 2), (0, 2))\n+      self.assertEqual(x_block_spec.index_map(3, 2, 1), (3, 1))\n+\n+      y_ref[...] = kernel_fn((0, 1, 1), scalar_prefetch_values, (ref,), x)\n+    y = jnp.zeros((256, 512), jnp.int32)\n+    _, y = pl.run_state(outer)((value, y))\n+    np.testing.assert_array_equal(y, value[:256, 512:1024])\n+\n+  def test_basic_get(self):\n+    value = jnp.arange((512 * 1024), dtype=jnp.int32).reshape((512, 1024)) * 2\n+    def outer(refs):\n+      ref, y_ref = refs\n+      def f():\n+        return ref.get()\n+\n+      block_spec = pl.BlockSpec((256, 512), lambda i, j, k: (i, k))\n+      kernel_fn, (), _ = (\n+          block_spec_lib.pull_block_spec(\n+              f,\n+              block_spec,\n+              grid=(2, 3, 4),\n+              scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+          )()\n+      )\n+      y_ref[...] = kernel_fn((0, 1, 1), ())\n+    y = jnp.zeros((256, 512), jnp.int32)\n+    _, y = pl.run_state(outer)((value, y))\n+    np.testing.assert_array_equal(y, value[:256, 512:1024])\n+\n+  def test_get_with_squeezed_block_spec(self):\n+    value = jnp.arange((4 * 512 * 1024), dtype=jnp.int32).reshape((4, 512, 1024)) * 2\n+    def outer(refs):\n+      ref, y_ref = refs\n+      def f():\n+        return ref.get()\n+\n+      block_spec = pl.BlockSpec((pl.Squeezed(), 256, 512), lambda i, j, k: (j, i, k))\n+      kernel_fn, (), _ = (\n+          block_spec_lib.pull_block_spec(\n+              f,\n+              block_spec,\n+              grid=(2, 3, 4),\n+              scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+          )()\n+      )\n+      y_ref[...] = kernel_fn((0, 3, 1), ())\n+    y = jnp.zeros((256, 512), jnp.int32)\n+    _, y = pl.run_state(outer)((value, y))\n+    np.testing.assert_array_equal(y, value[3, :256, 512:1024])\n+\n+  def test_get_with_squeezed_indexer(self):\n+    value = jnp.arange((4 * 512 * 1024), dtype=jnp.int32).reshape((4, 512, 1024)) * 2\n+    def outer(refs):\n+      ref, y_ref = refs\n+      def f():\n+        return ref[3]\n+\n+      block_spec = pl.BlockSpec((256, 512), lambda i, j, k: (i, k))\n+      kernel_fn, (), _ = (\n+          block_spec_lib.pull_block_spec(\n+              f,\n+              block_spec,\n+              grid=(2, 3, 4),\n+              scalar_prefetch_handler=block_spec_lib.make_scalar_prefetch_handler(),\n+          )()\n+      )\n+      y_ref[...] = kernel_fn((0, 2, 1), ())\n+    y = jnp.zeros((256, 512), jnp.int32)\n+    _, y = pl.run_state(outer)((value, y))\n+    np.testing.assert_array_equal(y, value[3, :256, 512:1024])\n+\n \n class PullBlockSpecHOPTest(jtu.JaxTestCase):\n \n\n```"
        ],
        "from_id": [
            "sharadmv",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic GPU] Use explicit load/store methods instead of __getitem__/__setitem__\n\nWe pretty never use slicing in those methods and I want to add the ability to load\nin other layouts than the default one (which means we will need extra non-index arguments).\n\nPiperOrigin-RevId: 756707034",
        "output": [
            "```diff\nCommit: 244cb362bcd2ae95c8d0da6d45f2ba6a103012cf\nDate: 2025-05-09T11:24:22Z\nURL: https://github.com/jax-ml/jax/commit/244cb362bcd2ae95c8d0da6d45f2ba6a103012cf\nFiles changed: 4\nAdditions: +31, Deletions: -38\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 55aca65c6a19..d50e39d5c3db 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -1276,7 +1276,7 @@ def _get_lowering_rule(ctx: LoweringRuleContext, x_ref, *leaves, tree):\n     if not gpu_core.is_trivial_index(indexer.indices, x_ref.shape):\n       raise NotImplementedError(\n           \"Only trivial indexing is supported for TMEM refs.\")\n-    return x_ref[:]\n+    return x_ref.load()\n \n   if not isinstance(x_ref, ir.Value) and ir.MemRefType.isinstance(x_ref):\n     raise TypeError(f\"Can only load from references (got {x_ref}).\")\ndiff --git a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\nindex 6af394d00138..03363c1e365f 100644\n--- a/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n+++ b/jax/experimental/mosaic/gpu/examples/matmul_blackwell.py\n@@ -162,7 +162,7 @@ def _mma_body(ki, accumulate):\n     gpu.barrier()\n     mma_done_barrier.wait(for_tensor_core=True)\n \n-    acc[:].astype(ir.F16Type.get()).store_tiled(d_smem, swizzle=128)\n+    acc.load().astype(ir.F16Type.get()).store_tiled(d_smem, swizzle=128)\n     mgpu.commit_shared()\n     ctx.async_copy(\n         src_ref=d_smem,\ndiff --git a/jax/experimental/mosaic/gpu/tcgen05.py b/jax/experimental/mosaic/gpu/tcgen05.py\nindex 4726805f5b76..f4ea8e289f01 100644\n--- a/jax/experimental/mosaic/gpu/tcgen05.py\n+++ b/jax/experimental/mosaic/gpu/tcgen05.py\n@@ -35,6 +35,15 @@\n \n TMEM_ROWS = 128\n TCGEN05_SMEM_DESCRIPTOR_BIT = 1 << 46\n+# Like WGMMA_LAYOUT, only each warp holds a 32xN strip instead of 16xN.\n+# The name is so short, because it's meant to be used qualified (tcgen05.LAYOUT)\n+LAYOUT = fa.TiledLayout(\n+    fa.Tiling(((128, 8), (32, 8), (8, 8), (1, 2))),\n+    warp_dim=-8,\n+    lane_dims=(-4, -3),\n+    vector_dim=-1,\n+)\n+\n \n def create_instr_descriptor(\n     m: int,\n@@ -582,7 +591,9 @@ def slice(self, *idxs):\n     if any(is_squeezed):\n       raise ValueError(\"TMEM can only be sliced, not indexed\")\n     match self.layout:\n-      case TMEMLayout(elements_in_tile=(r, 8), packing=packing) if r == TMEM_ROWS:\n+      case TMEMLayout(elements_in_tile=(r, 8), packing=packing) if (\n+          r == TMEM_ROWS\n+      ):\n         pass\n       case _:\n         raise NotImplementedError(\n@@ -607,18 +618,17 @@ def slice(self, *idxs):\n         dtype=self.dtype,\n     )\n \n-  def __getitem__(self, *idxs):\n+  def load(self, layout: fa.TiledLayout = LAYOUT):\n     i32 = ir.IntegerType.get_signless(32)\n-    base_idxs, slice_shape, is_squeezed = utils.parse_indices(idxs, self.shape)\n-    if any(is_squeezed):\n-      raise ValueError(\"TMEM loads only support slicing\")\n-    if any(idx != 0 for idx in base_idxs) or tuple(slice_shape) != self.shape:\n-      raise NotImplementedError(\"Slicing of TMEM not impelmented yet\")\n     if self.shape[1] % 8:\n       raise NotImplementedError\n     if utils.bitwidth(self.dtype) not in {16, 32}:\n       raise NotImplementedError(f\"Unsupported dtype: {self.dtype}\")\n-    layout = _m128_layout(self.shape)\n+    if layout != LAYOUT:\n+      raise ValueError(\n+          \"TMEM loads can only produce results in the tcgen05 layout\"\n+          f\" ({LAYOUT}), but got: {layout}\"\n+      )\n     regs_shape = layout.registers_shape(self.shape)\n     match self.layout:\n       case TMEMLayout(elements_in_tile=(r, 8), packing=packing) if r == TMEM_ROWS:\n@@ -653,16 +663,7 @@ def __getitem__(self, *idxs):\n         )\n     return fa.FragmentedArray(_registers=registers, _layout=layout, _is_signed=None)\n \n-  def __setitem__(self, idxs, value):\n-    if not isinstance(idxs, tuple):\n-      idxs = (idxs,)\n-    base_idxs, slice_shape, is_squeezed = utils.parse_indices(idxs, self.shape)\n-    if any(is_squeezed):\n-      raise ValueError(\n-          \"TMEM stores don't support integer indexing (only slices allowed)\"\n-      )\n-    if any(idx != 0 for idx in base_idxs) or tuple(slice_shape) != self.shape:\n-      raise NotImplementedError(\"Slicing parts of TMEM not implemented yet\")\n+  def store(self, value):\n     if self.shape[1] % 8:\n       raise NotImplementedError\n     if utils.bitwidth(self.dtype) not in {16, 32}:\n@@ -842,16 +843,6 @@ def _m128_layout(shape: tuple[int, ...]):\n   return LAYOUT\n \n \n-# Like WGMMA_LAYOUT, only each warp holds a 32xN strip instead of 16xN.\n-# The name is so short, because it's meant to be used qualified (tcgen05.LAYOUT)\n-LAYOUT = fa.TiledLayout(\n-    fa.Tiling(((128, 8), (32, 8), (8, 8), (1, 2))),\n-    warp_dim=-8,\n-    lane_dims=(-4, -3),\n-    vector_dim=-1,\n-)\n-\n-\n def commit_tmem():\n   void = ir.Type.parse(\"!llvm.void\")\n   llvm.inline_asm(\ndiff --git a/tests/mosaic/gpu_test.py b/tests/mosaic/gpu_test.py\nindex 50d60cae0080..8c26f64bd203 100644\n--- a/tests/mosaic/gpu_test.py\n+++ b/tests/mosaic/gpu_test.py\n@@ -922,9 +922,9 @@ def kernel(ctx, input, output, scratch):\n           barrier=barrier,\n       )\n       barrier.wait()\n-      tmem[:] = fa.FragmentedArray.load_tiled(smem, swizzle, layout=tcgen05.LAYOUT)\n+      tmem.store(fa.FragmentedArray.load_tiled(smem, swizzle, layout=tcgen05.LAYOUT))\n       tcgen05.commit_tmem()\n-      tmem[:].store_tiled(smem, swizzle)\n+      tmem.load().store_tiled(smem, swizzle)\n       mgpu.commit_shared()\n       ctx.async_copy(\n           src_ref=smem, dst_ref=output, swizzle=swizzle, gmem_transform=mgpu.TileTransform(tiling),\n@@ -964,7 +964,7 @@ def kernel(ctx, input, output, scratch):\n           barrier=barrier,\n       )\n       barrier.wait()\n-      tmem[:] = fa.FragmentedArray.load_tiled(smem, swizzle, layout=tcgen05.LAYOUT)\n+      tmem.store(fa.FragmentedArray.load_tiled(smem, swizzle, layout=tcgen05.LAYOUT))\n       tcgen05.commit_tmem()\n       tmem.slice(slice(None), slice(0, 8))._debug_print()\n \n@@ -1075,7 +1075,7 @@ def kernel(ctx, lhs, rhs, out, scratch):\n         )\n         tcgen05.commit_arrive(barriers[2])\n       barriers[2].wait(for_tensor_core=True)\n-      acc[:].store_untiled(out, optimized=False)\n+      acc.load().store_untiled(out, optimized=False)\n \n     x_shape = (k, m) if lhs_transpose else (m, k)\n     x = self.prng.uniform(-1, 1, x_shape).astype(in_jax_dtype)\n@@ -1144,8 +1144,10 @@ def kernel(ctx, lhs, rhs, out, scratch):\n       )\n       barriers[0].wait()\n       barriers[1].wait()\n-      lhs_tmem[:] = fa.FragmentedArray.load_tiled(\n-          lhs_smem, swizzle, layout=tcgen05.LAYOUT\n+      lhs_tmem.store(\n+          fa.FragmentedArray.load_tiled(\n+              lhs_smem, swizzle, layout=tcgen05.LAYOUT\n+          )\n       )\n       tcgen05.commit_tmem()\n       with mgpu.single_thread():\n@@ -1154,7 +1156,7 @@ def kernel(ctx, lhs, rhs, out, scratch):\n         )\n         tcgen05.commit_arrive(barriers[2])\n       barriers[2].wait(for_tensor_core=True)\n-      acc[:].store_untiled(out, optimized=False)\n+      acc.load().store_untiled(out, optimized=False)\n \n     x_shape = (m, k)\n     x = self.prng.uniform(-1, 1, x_shape).astype(in_jax_dtype)\n@@ -1246,7 +1248,7 @@ def kernel(ctx, lhs, rhs, out, scratch):\n         tcgen05.commit_arrive(barriers[2], collective=True, ctx=ctx)\n       barriers[2].wait(for_tensor_core=True)\n       m_slice = ds(arith.muli(block_id, c(m_block_tile, index)), m_block_tile)\n-      acc[:].store_untiled(memref_slice(out, m_slice), optimized=False)\n+      acc.load().store_untiled(memref_slice(out, m_slice), optimized=False)\n \n     in_finfo = jnp.finfo(in_jax_dtype)\n     exponent_bits, mantissa_bits = in_finfo.nexp, in_finfo.nmant\n\n```"
        ],
        "from_id": [
            "apaszke",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add supports_pinned_allocator to allow debugging pinning issues.\n\nPiperOrigin-RevId: 756548068",
        "output": [
            "```diff\nCommit: f8a3f06129d783a75692979b95d6229633b39227\nDate: 2025-05-09T02:01:20Z\nURL: https://github.com/jax-ml/jax/commit/f8a3f06129d783a75692979b95d6229633b39227\nFiles changed: 1\nAdditions: +17, Deletions: -6\ndiff --git a/jaxlib/py_socket_transfer.cc b/jaxlib/py_socket_transfer.cc\nindex 89900b02bd93..491e90d778cf 100644\n--- a/jaxlib/py_socket_transfer.cc\n+++ b/jaxlib/py_socket_transfer.cc\n@@ -197,7 +197,8 @@ class PyTransferServer {\n   PyTransferServer() = default;\n   absl::Status Start(xla::ifrt::Client* client, size_t max_num_parallel_copies,\n                      size_t xfer_size, const SocketAddress& addr,\n-                     const std::vector<SocketAddress>& transport_addresses) {\n+                     const std::vector<SocketAddress>& transport_addresses,\n+                     bool supports_pinned_allocator) {\n     std::shared_ptr<BulkTransportFactory> factory;\n     if (transport_addresses.empty()) {\n       factory = BulkTransportFactory::CreateLocal();\n@@ -207,8 +208,16 @@ class PyTransferServer {\n       SlabAllocator uallocator(xla::ValueOrThrow(MapPjrtMemory(\n                                    client, tmp->data(), tmp->size(), tmp)),\n                                xfer_size);\n+      std::optional<SlabAllocator> pinned_allocator;\n+      if (supports_pinned_allocator) {\n+        auto tmp = xla::ValueOrThrow(\n+            AllocateNetworkPinnedMemory(xfer_size * max_num_parallel_copies));\n+        pinned_allocator.emplace(xla::ValueOrThrow(MapPjrtMemory(\n+                                     client, tmp->data(), tmp->size(), tmp)),\n+                                 xfer_size);\n+      }\n       factory = xla::ValueOrThrow(CreateSocketBulkTransportFactory(\n-          transport_addresses, std::nullopt, uallocator));\n+          transport_addresses, pinned_allocator, uallocator));\n     }\n \n     server_ = std::make_shared<SocketServer>();\n@@ -387,8 +396,8 @@ void RegisterTransferServerTypes(nanobind::module_& m) {\n       \"start_transfer_server\",\n       [](xla::nb_class_ptr<xla::PyClient> py_client, std::string address,\n          std::vector<std::string> transport_addresses_str,\n-         size_t max_num_parallel_copies,\n-         size_t transfer_size) -> PyTransferServer {\n+         size_t max_num_parallel_copies, size_t transfer_size,\n+         bool supports_pinned_allocator) -> PyTransferServer {\n         PyTransferServer result;\n         std::vector<SocketAddress> transport_addresses;\n         transport_addresses.reserve(transport_addresses_str.size());\n@@ -399,13 +408,15 @@ void RegisterTransferServerTypes(nanobind::module_& m) {\n         xla::ThrowIfError(result.Start(\n             py_client->ifrt_client(), max_num_parallel_copies, transfer_size,\n             xla::ValueOrThrow(SocketAddress::Parse(address)),\n-            transport_addresses));\n+            transport_addresses, supports_pinned_allocator));\n         return result;\n       },\n       nb::arg(\"client\"), nb::arg(\"address\") = SocketAddress().ToString(),\n       nb::arg(\"transport_addresses\") = std::vector<std::string>(),\n       nb::arg(\"max_num_parallel_copies\") = 8,\n-      nb::arg(\"transfer_size\") = 256 * 1024 * 1024);\n+      nb::arg(\"transfer_size\") = 256 * 1024 * 1024,\n+      // Dual pinning not confirmed to be supported.\n+      nb::arg(\"supports_pinned_allocator\") = false);\n }\n \n }  // namespace aux\n\n```"
        ],
        "from_id": [
            "pschuh",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix empty string handling for cloud_tpu_cluster\n\nPiperOrigin-RevId: 756525786",
        "output": [
            "```diff\nCommit: da8f62b4742901a9388b6f91ab8c4740868bb32a\nDate: 2025-05-09T00:36:26Z\nURL: https://github.com/jax-ml/jax/commit/da8f62b4742901a9388b6f91ab8c4740868bb32a\nFiles changed: 1\nAdditions: +32, Deletions: -29\ndiff --git a/jax/_src/clusters/cloud_tpu_cluster.py b/jax/_src/clusters/cloud_tpu_cluster.py\nindex c8aa765c181c..4807a7194c5b 100644\n--- a/jax/_src/clusters/cloud_tpu_cluster.py\n+++ b/jax/_src/clusters/cloud_tpu_cluster.py\n@@ -14,6 +14,7 @@\n \n from __future__ import annotations\n \n+from typing import Optional\n import logging\n import os\n import re\n@@ -54,24 +55,26 @@ def get_metadata(key):\n     raise RuntimeError(f\"Getting metadata['{key}'] failed for 6 tries\")\n   return api_resp.text, api_resp.status_code\n \n-def get_tpu_env_value(key):\n-  def get_tpu_env_value_from_metadata(key):\n-    tpu_env_data = get_metadata('tpu-env')[0]\n-    key_value_pairs = tpu_env_data.split('\\n')\n-    for key_value_pair in key_value_pairs:\n-      # Typical line is MEGASCALE_NUM_SLICES: '2'\n-      if ':' in key_value_pair:\n-        row_key, value = re.split(':', key_value_pair, 1)\n-        row_key = row_key.strip()\n-        if row_key == key:\n-          return value.strip().strip(\"'\")\n-    return None\n-\n+def get_tpu_env_value_from_metadata(key) -> Optional[str]:\n+  metadata_value = None\n+  tpu_env_data = get_metadata('tpu-env')[0]\n+  key_value_pairs = tpu_env_data.split('\\n')\n+  for key_value_pair in key_value_pairs:\n+    # Typical line is MEGASCALE_NUM_SLICES: '2'\n+    if ':' in key_value_pair:\n+      row_key, value = re.split(':', key_value_pair, 1)\n+      row_key = row_key.strip()\n+      if row_key == key:\n+        metadata_value = value.strip().strip(\"'\")\n+  return metadata_value\n+\n+def get_tpu_env_value(key) -> Optional[str]:\n+  # First try to get the value from the environment.\n   value = os.environ.get(key, None)\n-  return value if value is not None else get_tpu_env_value_from_metadata(key)\n-\n-def has_megascale_address():\n-  return get_tpu_env_value('MEGASCALE_COORDINATOR_ADDRESS') is not None\n+  if value is None:\n+    # If not found, try to get it from the metadata.\n+    value = get_tpu_env_value_from_metadata(key)\n+  return value\n \n class BaseTpuCluster(clusters.ClusterEnv):\n \n@@ -94,12 +97,11 @@ def is_env_present(cls) -> bool:\n \n   @classmethod\n   def get_coordinator_address(cls, timeout_secs: int | None) -> str:\n-    if has_megascale_address():\n-      # For both GCE via QueuedResources and GKE via JobSet, the\n-      # Megascale coordinator address is set as the host with process id = 0,\n-      # so can be used as the jax distributed system coordinator.\n-      coordinator_address = get_tpu_env_value('MEGASCALE_COORDINATOR_ADDRESS')\n-    else:\n+    # For both GCE via QueuedResources and GKE via JobSet, the\n+    # Megascale coordinator address is set as the host with process id = 0,\n+    # so can be used as the jax distributed system coordinator.\n+    coordinator_address = get_tpu_env_value('MEGASCALE_COORDINATOR_ADDRESS')\n+    if not coordinator_address:\n       # For both GCE (QueuedResources and TPUVM create) and GKE via Job API,\n       # the workers lists are sorted by process ID so the first one can\n       # be used as the jax distributed system coordinator.\n@@ -149,17 +151,18 @@ def get_process_id(cls) -> int:\n \n   @staticmethod\n   def _get_num_slices() -> int:\n-    if has_megascale_address():\n-      return int(get_tpu_env_value('MEGASCALE_NUM_SLICES'))\n-    else:\n+    num_slices = get_tpu_env_value('MEGASCALE_NUM_SLICES')\n+    if not num_slices:\n       return 1\n+    return int(num_slices)  # type: ignore\n+\n \n   @staticmethod\n   def _get_slice_id() -> int:\n-    if has_megascale_address():\n-      return int(get_tpu_env_value('MEGASCALE_SLICE_ID'))\n-    else:\n+    slice_id = get_tpu_env_value('MEGASCALE_SLICE_ID')\n+    if not slice_id:\n       return 0\n+    return int(slice_id)  # type: ignore\n \n   @staticmethod\n   def _get_process_id_in_slice() -> int:\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Simplify add's unreduced rule. Only propagate unreduced if both lhs and rhs are unreduced.\n\nPiperOrigin-RevId: 756515463",
        "output": [
            "```diff\nCommit: a9c49ac085bc8c9635a5c2785c32d26bfa624ab9\nDate: 2025-05-09T00:01:53Z\nURL: https://github.com/jax-ml/jax/commit/a9c49ac085bc8c9635a5c2785c32d26bfa624ab9\nFiles changed: 2\nAdditions: +34, Deletions: -14\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex b41e78899ba9..74297fc57b43 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -4574,16 +4574,24 @@ def _add_transpose(t, x, y):\n   else:\n     return [_unbroadcast(x_aval, t), _unbroadcast(y_aval, t)]\n \n-def _add_unreduced(out_sharding, *avals):\n-  unreduced = [a.sharding.spec.unreduced for a in avals if a.shape]\n-  # TODO(yashkatariya): Relax this restriction to allow\n-  # `f32[8]{R:x} + f32[8]{U:x} -> f32[8]{U:x}` for example and maybe more cases.\n-  if unreduced:\n-    if not all(unreduced[0] == u for u in unreduced[1:]):\n+def _add_unreduced(out_sharding, x, y):\n+  x_ur, y_ur = x.sharding.spec.unreduced, y.sharding.spec.unreduced\n+  if x_ur and y_ur:\n+    if x_ur != y_ur:\n       raise core.ShardingTypeError(\n-          'All arrays must be unreduced along the same mesh axes. Got'\n-          f' {\", \".join(map(str, map(tuple, unreduced)))}')\n-    res_unreduced = unreduced[0]\n+          'lhs and rhs to `add` must be unreduced along the same mesh axes. '\n+          f'Got lhs={x_ur}, rhs={y_ur}')\n+    res_unreduced = x_ur\n+  elif x_ur or y_ur:\n+    if x_ur and not y_ur:\n+      lhs_str, rhs_str = 'lhs', 'rhs'\n+    else:\n+      assert not x_ur and y_ur\n+      lhs_str, rhs_str = 'rhs', 'lhs'\n+    raise core.ShardingTypeError(\n+        f'{lhs_str} is unreduced while {rhs_str} is not. `add` operation does'\n+        ' not allow this because there will be implicit communication. Please'\n+        f' reduce {lhs_str} via `reshard` before calling `add`.')\n   else:\n     res_unreduced = None\n   return out_sharding.with_spec(out_sharding.spec.with_unreduced(res_unreduced))\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 0c43df76cc5f..e4b91aa17644 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7763,22 +7763,34 @@ def h(x, y):\n         \"unreduced axes should be equal to the contracting specs\"):\n       h.trace(x, y)\n \n-  @jtu.with_explicit_mesh((2, 2), ('x', 'y'))\n+  @jtu.with_explicit_mesh((2, 2, 1), ('x', 'y', 'z'))\n   def test_add_unreduced_error(self, mesh):\n     np_inp = np.arange(16).reshape(8, 2)\n     x = jax.device_put(np_inp, P('x', 'y'))\n     y = jax.device_put(np_inp.T, P('y', None))\n+    a = jax.device_put(np_inp, P('x', 'z'))\n+    b = jax.device_put(np_inp.T, P('z', None))\n \n     @jax.jit\n-    def f(x, y):\n+    def f(x, y, a, b):\n       m1 = jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='y'))\n-      m2 = jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x'))\n+      m2 = jnp.einsum('xy,yz->xz', a, b, out_sharding=P('x', unreduced='z'))\n       return m1 + m2\n \n     with self.assertRaisesRegex(\n         core.ShardingTypeError,\n-        \"arrays must be unreduced along the same mesh axes\"):\n-      f.trace(x, y)\n+        \"lhs and rhs to `add` must be unreduced along the same mesh axes\"):\n+      f.trace(x, y, a, b)\n+\n+    @jax.jit\n+    def g(x, y):\n+      m1 = jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='y'))\n+      m2 = jnp.einsum('xy,yz->xz', a, b, out_sharding=P('x'))\n+      return m1 + m2\n+\n+    with self.assertRaisesRegex(\n+        core.ShardingTypeError, \"lhs is unreduced while rhs is not\"):\n+      g.trace(x, y)\n \n \n @jtu.pytest_mark_if_available('multiaccelerator')\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Integrate LLVM at llvm/llvm-project@2d287f51eff2\n\nUpdates LLVM usage to match\n[2d287f51eff2](https://github.com/llvm/llvm-project/commit/2d287f51eff2)\n\nPiperOrigin-RevId: 756508479",
        "output": [
            "```diff\nCommit: 8683b76dad586124c32ee33504d93a3c4751ded2\nDate: 2025-05-08T23:39:55Z\nURL: https://github.com/jax-ml/jax/commit/8683b76dad586124c32ee33504d93a3c4751ded2\nFiles changed: 2\nAdditions: +12, Deletions: -7\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/communication.cc b/jaxlib/mosaic/dialect/tpu/transforms/communication.cc\nindex 7e99dd15611b..dfe42111916c 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/communication.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/communication.cc\n@@ -111,8 +111,12 @@ struct LogicalToPhysicalDeviceIdPass\n           {total_devices}, IntegerType::get(func.getContext(), 32),\n           TiledLayoutAttr::get(func.getContext(), {xla::Tile({128})}, {1}),\n           MemorySpaceAttr::get(func.getContext(), MemorySpace::smem));\n-      func.insertArgument(func.getNumArguments(), device_assignment_type,\n-                          nullptr, UnknownLoc::get(func.getContext()));\n+\n+      if (failed(func.insertArgument(func.getNumArguments(),\n+                                     device_assignment_type, nullptr,\n+                                     UnknownLoc::get(func.getContext())))) {\n+        return signalPassFailure();\n+      }\n       auto device_assignment_arg = func.getArgument(func.getNumArguments() - 1);\n       func.walk([device_assignment_arg](Operation *some_op) {\n         if (auto op = dyn_cast<tpu::EnqueueDMAOp>(some_op)) {\ndiff --git a/jaxlib/mosaic/gpu/launch_lowering.cc b/jaxlib/mosaic/gpu/launch_lowering.cc\nindex 53d4f47e58cc..44362e825345 100644\n--- a/jaxlib/mosaic/gpu/launch_lowering.cc\n+++ b/jaxlib/mosaic/gpu/launch_lowering.cc\n@@ -238,7 +238,7 @@ mlir::LogicalResult launchPreloadedKernel(mlir::func::FuncOp func,\n     cluster = as_32bit(launch.getClusterSizeOperandValues());\n   } else {\n     cluster.x = cluster.y = cluster.z = builder.create<mlir::LLVM::ConstantOp>(\n-      launch.getLoc(), builder.getI32Type(), builder.getI32IntegerAttr(0));\n+        launch.getLoc(), builder.getI32Type(), builder.getI32IntegerAttr(0));\n   }\n   mlir::Value stream = launch.getAsyncObject();\n   builder.create<mlir::func::CallOp>(\n@@ -337,15 +337,16 @@ class GpuLaunchLoweringPass : public ::mlir::OperationPass<mlir::ModuleOp> {\n                             launch.getDynamicSharedMemorySize(), cluster_shape);\n \n           // Add a new function argument for the kernel handle.\n-          func.insertArgument(0, ptr_ty,\n-                              mlir::DictionaryAttr::get(func.getContext()),\n-                              mlir::UnknownLoc::get(func.getContext()));\n+          if (failed(func.insertArgument(\n+                  0, ptr_ty, mlir::DictionaryAttr::get(func.getContext()),\n+                  mlir::UnknownLoc::get(func.getContext())))) {\n+            return mlir::WalkResult::interrupt();\n+          }\n           mlir::Value kernel_handle = func.getArgument(0);\n           if (launchPreloadedKernel(func, launch, kernel_handle).failed()) {\n             return mlir::WalkResult::interrupt();\n           }\n           launch.erase();\n-\n           // TODO(apaszke): Generate a destructor function.\n           // builder.CreateCall(getModuleUnloadFn(), {moduleObject});\n \n\n```"
        ],
        "from_id": [
            "alinas",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Remove type annotation of get_gpu_client\n\nPiperOrigin-RevId: 756454023",
        "output": [
            "```diff\nCommit: 4786d122433ff9aa5525d901d0797070c975fb3d\nDate: 2025-05-08T21:11:16Z\nURL: https://github.com/jax-ml/jax/commit/4786d122433ff9aa5525d901d0797070c975fb3d\nFiles changed: 1\nAdditions: +0, Deletions: -11\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 8eb9b5f8173d..6f4f952be9c3 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -572,17 +572,6 @@ def get_tfrt_cpu_client(\n     collectives: CpuCollectives | None = ...,\n     num_devices: int | None = ...,\n ) -> Client: ...\n-def get_gpu_client(\n-    asynchronous: bool = ...,\n-    allocator_config: GpuAllocatorConfig = ...,\n-    distributed_client: DistributedRuntimeClient | None = ...,\n-    node_id: int = ...,\n-    num_nodes: int = ...,\n-    allowed_devices: Any | None = ...,\n-    platform_name: str | None = ...,\n-    mock: bool | None = ...,\n-    mock_gpu_topology: str | None = ...,\n-) -> Client: ...\n def get_mock_gpu_client(\n     asynchronous: bool = ...,\n     allocator_config: GpuAllocatorConfig = ...,\n\n```"
        ],
        "from_id": [
            "hhb",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix the PyTest TPU jobs on the Continuous Wheel Tests workflow.\n\nPiperOrigin-RevId: 756443164",
        "output": [
            "```diff\nCommit: 3712656b636614527a5cbaa5bf25fec619d8c681\nDate: 2025-05-08T20:45:21Z\nURL: https://github.com/jax-ml/jax/commit/3712656b636614527a5cbaa5bf25fec619d8c681\nFiles changed: 1\nAdditions: +9, Deletions: -4\ndiff --git a/tests/xla_bridge_test.py b/tests/xla_bridge_test.py\nindex 5a6bf80a469d..3306cb64aced 100644\n--- a/tests/xla_bridge_test.py\n+++ b/tests/xla_bridge_test.py\n@@ -210,18 +210,23 @@ def test_register_plugin_with_config(self):\n   def test_register_plugin_with_lazy_config(self):\n     options = {\"bar\": \"baz\"}\n \n-    def f():\n+    def getopts():\n       return options\n \n+    def make_c_api_client(plugin_name, new_options, *args, **kwargs):\n+      self.assertContainsSubset(new_options, options)\n+\n     with mock.patch.object(xc, \"load_pjrt_plugin_dynamically\", autospec=True):\n       with mock.patch.object(\n           _profiler, \"register_plugin_profiler\", autospec=True\n       ):\n-        xb.register_plugin(\"foo\", options=f, library_path=\"/dev/null\")\n-    with mock.patch.object(xc, \"make_c_api_client\", autospec=True) as mock_make:\n+        xb.register_plugin(\"foo\", options=getopts, library_path=\"/dev/null\")\n+    with mock.patch.object(\n+        xc, \"make_c_api_client\", autospec=True, wraps=make_c_api_client\n+    ) as mock_make:\n       with mock.patch.object(xc, \"pjrt_plugin_initialized\", autospec=True):\n         xb._backend_factories[\"foo\"].factory()\n-    mock_make.assert_called_once_with(\"foo\", options, None)\n+    mock_make.assert_called_once()\n \n \n class GetBackendTest(jtu.JaxTestCase):\n\n```"
        ],
        "from_id": [
            "matthiaskramm",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Allow unreduced propagation only for `add` right now.\n\nAll nary ops cannot forward unreduced as is. `mul` is an example since it's not linear when both inputs are unreduced. `mul` can forward unreduced when one of the inputs is replicated or a constant and the other is unreduced.\n\nPiperOrigin-RevId: 756423798",
        "output": [
            "```diff\nCommit: d2284bf89314d447d7270156ae2cda40395b521c\nDate: 2025-05-08T19:54:09Z\nURL: https://github.com/jax-ml/jax/commit/d2284bf89314d447d7270156ae2cda40395b521c\nFiles changed: 4\nAdditions: +37, Deletions: -23\ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex f6b4c1be102f..b41e78899ba9 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -3974,23 +3974,10 @@ def broadcasting_sharding_rule(name, *avals):\n             raise core.ShardingTypeError(\n                 f'{name} got incompatible shardings for broadcasting: '\n                 f'{\", \".join(map(str, map(tuple, specs)))}.')\n-\n-  unreduced = [a.sharding.spec.unreduced for a in avals if a.shape]\n-  # TODO(yashkatariya): Relax this restriction to allow\n-  # `f32[8]{R:x} * f32[8]{U:x} -> f32[8]{U:x}` for example and maybe more cases.\n-  if unreduced:\n-    if not all(unreduced[0] == u for u in unreduced[1:]):\n-      raise core.ShardingTypeError(\n-          'All arrays must be unreduced along the same mesh axes. Got'\n-          f' {\", \".join(map(str, map(tuple, unreduced)))}')\n-    result_unreduced = unreduced[0]\n-  else:\n-    result_unreduced = None\n-\n-  return NamedSharding(mesh, P(*result_specs, unreduced=result_unreduced))\n+  return NamedSharding(mesh, P(*result_specs))\n \n def naryop(result_dtype, accepted_dtypes, name, allow_extended_dtype=False,\n-           require_same_dtypes=True):\n+           require_same_dtypes=True, unreduced_rule=None):\n   dtype_rule = partial(naryop_dtype_rule, result_dtype, accepted_dtypes, name,\n                        allow_extended_dtype=allow_extended_dtype,\n                        require_same=require_same_dtypes)\n@@ -3998,7 +3985,8 @@ def naryop(result_dtype, accepted_dtypes, name, allow_extended_dtype=False,\n   sharding_rule = partial(broadcasting_sharding_rule, name)\n   prim = standard_primitive(\n       shape_rule, dtype_rule, name, sharding_rule=sharding_rule,\n-      vma_rule=partial(core.standard_vma_rule, name))\n+      vma_rule=partial(core.standard_vma_rule, name),\n+      unreduced_rule=unreduced_rule)\n   batching.defbroadcasting(prim)\n   pe.def_trivial_padding(prim)\n   return prim\n@@ -4586,8 +4574,22 @@ def _add_transpose(t, x, y):\n   else:\n     return [_unbroadcast(x_aval, t), _unbroadcast(y_aval, t)]\n \n-# TODO(slebedev): Why does mypy fail to infer the type here?\n-add_p: Primitive = standard_naryop([_num, _num], 'add')\n+def _add_unreduced(out_sharding, *avals):\n+  unreduced = [a.sharding.spec.unreduced for a in avals if a.shape]\n+  # TODO(yashkatariya): Relax this restriction to allow\n+  # `f32[8]{R:x} + f32[8]{U:x} -> f32[8]{U:x}` for example and maybe more cases.\n+  if unreduced:\n+    if not all(unreduced[0] == u for u in unreduced[1:]):\n+      raise core.ShardingTypeError(\n+          'All arrays must be unreduced along the same mesh axes. Got'\n+          f' {\", \".join(map(str, map(tuple, unreduced)))}')\n+    res_unreduced = unreduced[0]\n+  else:\n+    res_unreduced = None\n+  return out_sharding.with_spec(out_sharding.spec.with_unreduced(res_unreduced))\n+\n+add_p: Primitive = naryop(_input_dtype, [_num, _num], 'add',\n+                          unreduced_rule=_add_unreduced)\n ad.primitive_jvps[add_p] = _add_jvp\n ad.primitive_transposes[add_p] = _add_transpose\n mlir.register_lowering(add_p, partial(_nary_lower_hlo, hlo.add))\n@@ -4897,7 +4899,8 @@ def _convert_element_type_bind_with_trace(trace, args, params):\n             _convert_element_type_shape_rule, _convert_element_type_dtype_rule,\n             _convert_element_type_weak_type_rule,\n             _convert_element_type_sharding_rule,\n-            partial(core.standard_vma_rule, convert_element_type_p.name)))\n+            partial(core.standard_vma_rule, convert_element_type_p.name),\n+            None))\n ad.defjvp2(convert_element_type_p, _convert_element_type_jvp_rule)\n ad.primitive_transposes[convert_element_type_p] = _convert_element_type_transpose_rule\n \n@@ -5202,6 +5205,9 @@ def _dot_general_sharding_rule(lhs, rhs, *, dimension_numbers, precision,\n     raise core.ShardingTypeError(\n         'Mesh of both lhs and rhs should match. Got lhs:'\n         f' {lhs.sharding.mesh} and rhs: {rhs.sharding.mesh}')\n+  if lhs.sharding.spec.unreduced or rhs.sharding.spec.unreduced:\n+    raise NotImplementedError(\n+        'Please file an issue at https://github.com/jax-ml/jax/issues')\n \n   (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers\n   lhs_contracting_spec = tuple(lhs.sharding.spec[i] for i in lhs_contracting)\ndiff --git a/jax/_src/lax/linalg.py b/jax/_src/lax/linalg.py\nindex 848107faf204..dd86c22432d8 100644\n--- a/jax/_src/lax/linalg.py\n+++ b/jax/_src/lax/linalg.py\n@@ -780,7 +780,8 @@ def linalg_primitive(result_dtype, accepted_dtypes, ranks, result_shape, name,\n     prim.def_abstract_eval(\n       partial(lax_utils.standard_abstract_eval, prim, shape_rule, dtype_rule,\n               lax_utils._standard_weak_type_rule, sharding_rule,\n-              partial(core.standard_vma_rule, name)))\n+              partial(core.standard_vma_rule, name),\n+              None))\n   if supports_batching:\n     batching.primitive_batchers[prim] = partial(\n         batching.expand_dims_batcher, prim)\ndiff --git a/jax/_src/lax/utils.py b/jax/_src/lax/utils.py\nindex 9e033cadd933..a850b2965338 100644\n--- a/jax/_src/lax/utils.py\n+++ b/jax/_src/lax/utils.py\n@@ -38,13 +38,14 @@ def _argnum_weak_type(*argnums):\n   return lambda *args, **_: all(args[i].weak_type for i in argnums)\n \n def standard_primitive(shape_rule, dtype_rule, name,\n-                       weak_type_rule=None, sharding_rule=None, vma_rule=None):\n+                       weak_type_rule=None, sharding_rule=None, vma_rule=None,\n+                       unreduced_rule=None):\n   weak_type_rule = weak_type_rule or _standard_weak_type_rule\n   prim = core.Primitive(name)\n   prim.def_impl(partial(dispatch.apply_primitive, prim))\n   prim.def_abstract_eval(\n       partial(standard_abstract_eval, prim, shape_rule, dtype_rule,\n-              weak_type_rule, sharding_rule, vma_rule))\n+              weak_type_rule, sharding_rule, vma_rule, unreduced_rule))\n   return prim\n \n def _get_array_abstraction_level(a): return a.array_abstraction_level\n@@ -103,7 +104,8 @@ def call_shape_dtype_sharding_rule(prim, shape_rule, dtype_rule, sharding_rule,\n   return out_shapes, out_dtypes, out_shardings\n \n def standard_abstract_eval(prim, shape_rule, dtype_rule, weak_type_rule,\n-                           sharding_rule, vma_rule, *avals, **kwargs):\n+                           sharding_rule, vma_rule, unreduced_rule,\n+                           *avals, **kwargs):\n   for a in avals:\n     if isinstance(a, state.AbstractRef):\n       raise ValueError(\n@@ -125,6 +127,8 @@ def standard_abstract_eval(prim, shape_rule, dtype_rule, weak_type_rule,\n         prim, shape_rule, dtype_rule, sharding_rule, False,\n         *avals, **kwargs)\n     out_vma = vma_rule(*avals, **kwargs)\n+    if unreduced_rule is not None:\n+      out_sharding = unreduced_rule(out_sharding, *avals, **kwargs)\n     out_aval = core.ShapedArray(\n         out_shape, out_dtype, weak_type=weak_type, sharding=out_sharding,\n         vma=out_vma)\ndiff --git a/jax/_src/partition_spec.py b/jax/_src/partition_spec.py\nindex 3542a232ba35..040db35ccb2b 100644\n--- a/jax/_src/partition_spec.py\n+++ b/jax/_src/partition_spec.py\n@@ -160,6 +160,9 @@ def count(self, value):\n   def with_partitions(self, new_partitions):\n     return PartitionSpec(*new_partitions, unreduced=self._unreduced)\n \n+  def with_unreduced(self, new_unreduced):\n+    return PartitionSpec(*self._partitions, unreduced=new_unreduced)\n+\n   def _normalized_spec_for_aval(self, ndim: int) -> PartitionSpec:\n     out = [None if p is _UNCONSTRAINED_PARTITION else p\n            for p in self._partitions]\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas][Mosaic GPU] Add transpose support to tcgen05_mma\n\nPiperOrigin-RevId: 756407270",
        "output": [
            "```diff\nCommit: 44a30b2090291fa9f8531c98d088082c4f934b82\nDate: 2025-05-08T19:08:49Z\nURL: https://github.com/jax-ml/jax/commit/44a30b2090291fa9f8531c98d088082c4f934b82\nFiles changed: 2\nAdditions: +33, Deletions: -17\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex c5cf257070c4..40eccca7c711 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -1227,7 +1227,7 @@ def _tcgen05_mma_lowering(\n     collective_axis,\n ):\n   _, a_aval, b_aval, *_ = ctx.avals_in\n-  lhs_swizzle: int = 128\n+  lhs_swizzle: int | None = None\n   lhs_transpose: bool = False\n   if a_transforms_tree is not None:\n     a_transforms_leaves, b_transforms_leaves = util.split_list(\n@@ -1277,14 +1277,20 @@ def _tcgen05_mma_lowering(\n       )\n \n   swizzle_elems = rhs_swizzle // b_aval.dtype.itemsize\n-  if rhs_swizzle != lhs_swizzle:\n+  if lhs_swizzle is None:\n+    lhs_swizzle = rhs_swizzle\n+  elif rhs_swizzle != lhs_swizzle:\n     raise ValueError(\"MMA rhs swizzle must match lhs swizzle.\"\n                       f\" {lhs_swizzle=} {rhs_swizzle=}\")\n   if rhs_tiling != (8, swizzle_elems):\n     raise ValueError(\"MMA rhs tiling does not fit swizzle\"\n                       f\" {rhs_tiling=} expected={(8, swizzle_elems)}\")\n-  if lhs_transpose or rhs_transpose:\n-    raise NotImplementedError(\"Lowering does not yet support transpose\")\n+  if lhs_transpose:\n+    if isinstance(a_ref, tcgen05.TMEMRef):\n+      raise ValueError(\"TMEM transpose not allowed.\")\n+    a_ref = mgpu.memref_transpose(a_ref, (1, 0, 3, 2))\n+  if rhs_transpose:\n+    b_ref = mgpu.memref_transpose(b_ref, (1, 0, 3, 2))\n   if isinstance(accumulate, bool):\n     accumulate = mgpu.c(accumulate, ir.IntegerType.get_signless(1))\n \n@@ -1314,8 +1320,8 @@ def _tcgen05_mma_lowering(\n               acc,\n               a_ref,\n               b_ref,\n-              a_swizzle=rhs_swizzle,\n-              b_swizzle=lhs_swizzle,\n+              a_swizzle=lhs_swizzle,\n+              b_swizzle=rhs_swizzle,\n               accumulate=accumulate,\n               collective=collective,\n           )\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 71b4b491f7e4..a5719b01f4ad 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -2131,18 +2131,20 @@ def kernel(x_ref, y_ref, tmem_ref, tmem_ref2, smem_ref, barrier_ref):\n     x_result = jax.block_until_ready(kernel(x))\n     np.testing.assert_array_equal(x_result, x + 1)\n \n-  @parameterized.parameters(\n-      ((128, 128), 128, jnp.float16, False),\n-      # Test LHS in TMEM.\n-      ((128, 128), 128, jnp.float16, True),\n-      # Test bfloat16\n-      ((128, 128), 128, jnp.bfloat16, False),\n-      # Test additional swizzles.\n-      ((128, 128), 64, jnp.float16, False),\n-      ((128, 128), 32, jnp.float16, False),\n-  )\n-  def test_simple_matmul(self, shape, swizzle, dtype, lhs_tmem=False):\n+  @parameterized.product(shape=[(128, 128)],\n+                         swizzle=[128, 64, 32],\n+                         dtype=[jnp.float16, jnp.bfloat16],\n+                         lhs_tmem=[False, True],\n+                         transpose_rhs=[False, True],\n+                         transpose_lhs=[False, True])\n+  def test_simple_matmul(self, shape, swizzle,\n+                         dtype=jnp.float16,\n+                         lhs_tmem=False,\n+                         transpose_lhs=False,\n+                         transpose_rhs=False):\n     self.skip_if_wg_semantics()\n+    if transpose_lhs and lhs_tmem:\n+      self.skipTest(\"TMEM transpose not supported.\")\n     # Test a matmul with a single block.\n     swizzle_elems = swizzle // jnp.dtype(dtype).itemsize\n     transforms = (\n@@ -2152,6 +2154,10 @@ def test_simple_matmul(self, shape, swizzle, dtype, lhs_tmem=False):\n \n     def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref,\n                a_tmem_ref):\n+      if transpose_lhs:\n+        a_smem = plgpu.transpose_ref(a_smem, (1, 0))\n+      if transpose_rhs:\n+        b_smem = plgpu.transpose_ref(b_smem, (1, 0))\n       if lhs_tmem:\n         lhs_ref = a_tmem_ref\n         lhs_ref[...] = plgpu.load(a_smem, (), layout=plgpu.Layout.TCGEN05)\n@@ -2194,6 +2200,10 @@ def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref,\n     x = jax.random.uniform(jax.random.key(0), shape=shape, dtype=dtype)\n     y = jax.random.uniform(jax.random.key(1), shape=shape, dtype=dtype)\n     result = f(x, y)\n+    if transpose_lhs:\n+      x = jnp.transpose(x, (1, 0))\n+    if transpose_rhs:\n+      y = jnp.transpose(y, (1, 0))\n     expected = x @ y\n     np.testing.assert_allclose(result, expected, rtol=1e-3)\n \n\n```"
        ],
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Shut down `PreemptionSyncManager` when `jax.distributed.shutdown()` is called.\n\nPiperOrigin-RevId: 756398914",
        "output": [
            "```diff\nCommit: 515f81bfa3e454ccc2f2b757de18e31d4bcb2ae4\nDate: 2025-05-08T18:46:24Z\nURL: https://github.com/jax-ml/jax/commit/515f81bfa3e454ccc2f2b757de18e31d4bcb2ae4\nFiles changed: 3\nAdditions: +11, Deletions: -3\ndiff --git a/jax/_src/distributed.py b/jax/_src/distributed.py\nindex ef8c48a61293..dad445b8e539 100644\n--- a/jax/_src/distributed.py\n+++ b/jax/_src/distributed.py\n@@ -156,14 +156,17 @@ def initialize(self,\n     self.slice_index = slice_index\n \n   def shutdown(self):\n+    if self.preemption_sync_manager:\n+      # It's important to shut down the preemption sync manager before the\n+      # client because the preemption sync manager depends on the client.\n+      self.preemption_sync_manager.shutdown()\n+      self.preemption_sync_manager = None\n     if self.client:\n       self.client.shutdown()\n       self.client = None\n     if self.service:\n       self.service.shutdown()\n       self.service = None\n-    if self.preemption_sync_manager:\n-      self.preemption_sync_manager = None\n \n   def initialize_preemption_sync_manager(self):\n     if self.preemption_sync_manager is not None:\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 1582930dd44d..8eb9b5f8173d 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -847,6 +847,7 @@ def get_distributed_runtime_client(\n class PreemptionSyncManager:\n   def initialize(self, client: DistributedRuntimeClient) -> _Status: ...\n   def reached_sync_point(self, step_counter: int) -> bool: ...\n+  def shutdown(self) -> None: ...\n \n def create_preemption_sync_manager() -> PreemptionSyncManager: ...\n def collect_garbage() -> None: ...\ndiff --git a/jaxlib/xla.cc b/jaxlib/xla.cc\nindex 0d3d8f6e1b29..adf6f3c98297 100644\n--- a/jaxlib/xla.cc\n+++ b/jaxlib/xla.cc\n@@ -627,7 +627,11 @@ NB_MODULE(_jax, m) {\n       .def(\"reached_sync_point\",\n            [](tsl::PreemptionSyncManager& manager, int step_counter) {\n              return manager.ReachedSyncPoint(step_counter);\n-           });\n+           })\n+      .def(\"shutdown\", [](tsl::PreemptionSyncManager& manager) {\n+        nb::gil_scoped_release gil_release;\n+        manager.Shutdown();\n+      });\n   m.def(\"create_preemption_sync_manager\",\n         []() { return tsl::CreatePreemptionSyncManager(); });\n \n\n```"
        ],
        "from_id": [
            "mwhittaker",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic][SC] Expose control over the number of active SC cores\n\nOn SparseCore, the `core_parallel` dimension semantic allows the user to set the number of active cores to `all` or `1` based on the dimension size.\n\nPiperOrigin-RevId: 756388100",
        "output": [
            "```diff\nCommit: 6c66e938ee362a5ee562d7917bc607779d0a6f64\nDate: 2025-05-08T18:18:44Z\nURL: https://github.com/jax-ml/jax/commit/6c66e938ee362a5ee562d7917bc607779d0a6f64\nFiles changed: 1\nAdditions: +84, Deletions: -2\ndiff --git a/jax/_src/tpu_custom_call.py b/jax/_src/tpu_custom_call.py\nindex 6039979df37b..0f099ed45cac 100644\n--- a/jax/_src/tpu_custom_call.py\n+++ b/jax/_src/tpu_custom_call.py\n@@ -125,6 +125,7 @@ class CustomCallBackendConfig:\n   internal_scratch_in_bytes: int | None\n   output_memory_spaces: tuple[MemorySpace | None, ...] | None\n   disable_bounds_checks: bool\n+  active_core_count: int | None\n \n   # We omit the body while printing, because primitive params get embedded\n   # in HLO metadata, and the body blows up its size.\n@@ -212,6 +213,8 @@ def to_json(self) -> bytes:\n         if i + 1 != len(self.flags):\n           config.write(b\",\")\n       config.write(b\"]\")\n+    if self.device_type == \"sparsecore\" and self.active_core_count == 1:\n+      config.write(b', \"megachip_parallelism_config\": {\"cores\": [\"0\"]}')\n     config.write(b\"}\")\n     return config.getvalue()\n \n@@ -355,14 +358,89 @@ def assign_device_type_based_on_core_type(op: ir.Operation) -> ir.WalkResult:\n   )\n   if tensorcore_func_found and sparsecore_func_found:\n     raise ValueError(\n-        \"A single Mosaic kernel cannot contain both \"\n-        \"TensorCore and SparseCore functions.\"\n+        \"A single Mosaic kernel cannot contain both TensorCore and SparseCore\"\n+        \" functions.\"\n     )\n   if sparsecore_func_found:\n     return \"sparsecore\"\n   return None\n \n \n+def _get_active_core_count(module: ir.Module) -> int | None:\n+\n+  def get_core_parallel_dim_size(\n+      dim_semantics: ir.ArrayAttr,\n+      iter_bounds: ir.DenseI64ArrayAttr,\n+      other_subkernel_core_dim_size: int | None = None) -> int | None:\n+\n+    if len(iter_bounds) != len(dim_semantics):\n+      raise ValueError(\n+          \"The iteration bounds and dimension semantics attributes must have\"\n+          \" the same number of elements.\"\n+      )\n+\n+    subkernel_core_dim_size = None\n+\n+    for dim_idx, (dim_size, dim_sem) in enumerate(\n+        zip(iter_bounds, dim_semantics)\n+    ):\n+      if str(dim_sem) != \"#tpu.dimension_semantics<core_parallel>\":\n+        continue\n+\n+      if ir.ShapedType.is_dynamic_size(dim_size):\n+        raise ValueError(\n+            \"The iteration bound corresponding to the core-parallel dimension \"\n+            f\"{dim_idx} must be statically known.\"\n+        )\n+      if subkernel_core_dim_size is not None:\n+        raise ValueError(\n+            \"A single Mosaic subkernel cannot contain multiple core sharding \"\n+            \"dimensions.\"\n+        )\n+      if (\n+          other_subkernel_core_dim_size is not None\n+          and other_subkernel_core_dim_size != dim_size\n+      ):\n+        raise ValueError(\n+            \"The iteration bound corresponding to the core-parallel dimension \"\n+            \"be the same across all subkernels.\"\n+        )\n+      subkernel_core_dim_size = dim_size\n+\n+    return subkernel_core_dim_size\n+\n+  core_parallel_dim_size = None\n+\n+  for op in module.body.operations:\n+    if op.operation.name != \"func.func\":\n+      continue\n+\n+    if (\n+        \"iteration_bounds\" not in op.attributes\n+        or \"dimension_semantics\" not in op.attributes\n+    ):\n+      continue\n+\n+    try:\n+      iter_bounds = ir.DenseI64ArrayAttr(op.attributes[\"iteration_bounds\"])\n+    except ValueError as e:\n+      e.add_note(\"The iteration bounds attribute must be an array.\")\n+      raise\n+    try:\n+      dim_semantics = ir.ArrayAttr(op.attributes[\"dimension_semantics\"])\n+    except ValueError as e:\n+      e.add_note(\"The dimension semantics attribute must be an array.\")\n+      raise\n+\n+    core_parallel_dim_size = get_core_parallel_dim_size(\n+        dim_semantics=dim_semantics,\n+        iter_bounds=iter_bounds,\n+        other_subkernel_core_dim_size=core_parallel_dim_size,\n+    )\n+\n+  return core_parallel_dim_size\n+\n+\n def _lower_to_custom_call_config(\n     module: ir.Module,\n     *,\n@@ -392,6 +470,7 @@ def _lower_to_custom_call_config(\n       kernel_name=kernel_name,\n       ir_version=ir_version,\n   )\n+  active_core_count = _get_active_core_count(module)\n   return _lowered_to_custom_call_config(\n       lowered_module_asm,\n       vmem_limit_bytes=vmem_limit_bytes,\n@@ -408,6 +487,7 @@ def _lower_to_custom_call_config(\n       needs_layout_passes=needs_layout_passes,\n       output_memory_spaces=output_memory_spaces,\n       disable_bounds_checks=disable_bounds_checks,\n+      active_core_count=active_core_count,\n   )\n \n \n@@ -428,6 +508,7 @@ def _lowered_to_custom_call_config(\n     device_type: str | None,\n     output_memory_spaces: tuple[MemorySpace | None, ...] | None = None,\n     disable_bounds_checks: bool = False,\n+    active_core_count: int | None = None,\n ):\n   if has_custom_barrier:\n     if collective_id is None:\n@@ -459,6 +540,7 @@ def _lowered_to_custom_call_config(\n       internal_scratch_in_bytes,\n       output_memory_spaces,\n       disable_bounds_checks,\n+      active_core_count=active_core_count,\n   )\n   return config\n \n\n```"
        ],
        "from_id": [
            "naummo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[jaxlib] Add compile_and_load, compile_and_load_ifrt_program to xla_client stub.\n\nPiperOrigin-RevId: 756385283",
        "output": [
            "```diff\nCommit: 08385f51d0975d0a46661e95a6a22969c48145fc\nDate: 2025-05-08T18:12:59Z\nURL: https://github.com/jax-ml/jax/commit/08385f51d0975d0a46661e95a6a22969c48145fc\nFiles changed: 2\nAdditions: +13, Deletions: -1\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex 8c02bb4ba722..1582930dd44d 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -511,11 +511,23 @@ class Client:\n       compile_options: CompileOptions = ...,\n       host_callbacks: Sequence[Any] = ...,\n   ) -> LoadedExecutable: ...\n+  def compile_and_load(\n+      self,\n+      computation: str | bytes,\n+      executable_devices: DeviceList | Sequence[Device],\n+      compile_options: CompileOptions = ...,\n+      host_callbacks: Sequence[Any] = ...,\n+  ) -> LoadedExecutable: ...\n   def compile_ifrt_program(\n       self,\n       program: ifrt_programs.Program,\n       program_options: ifrt_programs.CompileOptions,\n   ) -> LoadedExecutable: ...\n+  def compile_and_load_ifrt_program(\n+      self,\n+      program: ifrt_programs.Program,\n+      program_options: ifrt_programs.CompileOptions,\n+  ) -> LoadedExecutable: ...\n   def serialize_executable(self, executable: LoadedExecutable) -> bytes: ...\n   def deserialize_executable(\n       self,\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex 725d05a2dace..449dfa653286 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 338\n+_version = 339\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\n\n```"
        ],
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Make the type checker match the runtime behavior of PartitionSpec not inherting from a tuple.\n\nPiperOrigin-RevId: 756385141",
        "output": [
            "```diff\nCommit: 098624dcf81ad65c7d41dc35efcbc3feda010c7f\nDate: 2025-05-08T18:11:02Z\nURL: https://github.com/jax-ml/jax/commit/098624dcf81ad65c7d41dc35efcbc3feda010c7f\nFiles changed: 3\nAdditions: +4, Deletions: -11\ndiff --git a/jax/_src/partition_spec.py b/jax/_src/partition_spec.py\nindex fcea21934bfb..3542a232ba35 100644\n--- a/jax/_src/partition_spec.py\n+++ b/jax/_src/partition_spec.py\n@@ -13,7 +13,7 @@\n # limitations under the License.\n \n from __future__ import annotations\n-from typing import TYPE_CHECKING, Any\n+from typing import Any\n \n class UnconstrainedSingleton:\n \n@@ -63,7 +63,7 @@ def unpicke_pspec(partitions, unreduced):\n \n AxisName = Any\n \n-class PartitionSpecImpl:\n+class PartitionSpec:\n   \"\"\"Tuple describing how to partition an array across a mesh of devices.\n \n   Each element is either ``None``, a string, or a tuple of strings.\n@@ -166,10 +166,3 @@ def _normalized_spec_for_aval(self, ndim: int) -> PartitionSpec:\n     if len(out) < ndim:\n       out.extend([None] * (ndim - len(out)))\n     return self.with_partitions(out)\n-\n-\n-if TYPE_CHECKING:\n-  class PartitionSpec(PartitionSpecImpl, tuple):  # type: ignore\n-    ...\n-else:\n-  PartitionSpec = PartitionSpecImpl\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex f87207e0a796..ca77b659a08d 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -2141,7 +2141,7 @@ def _insert_axis_partitions(spec, dim, val):\n   too_short = dim - len(spec)\n   if too_short > 0:\n     spec += (None,) * too_short\n-  new_partitions = tuple_insert(spec, dim, val)\n+  new_partitions = tuple_insert(spec, dim, val)  # type: ignore\n   return PartitionSpec(*new_partitions)\n \n def _pjit_batcher_for_sharding(\ndiff --git a/jax/_src/sharding_impls.py b/jax/_src/sharding_impls.py\nindex 6e86911e63b0..2394e9e18f38 100644\n--- a/jax/_src/sharding_impls.py\n+++ b/jax/_src/sharding_impls.py\n@@ -1244,7 +1244,7 @@ def logical_sharding(logical_shape, dtype, phys_sharding) -> jsharding.Sharding:\n       phys_spec = (*phys_sharding.spec,\n                    *[None] * (len(phys_shape) - len(phys_sharding.spec)))\n     else:\n-      phys_spec = phys_sharding.spec\n+      phys_spec = phys_sharding.spec  # type: ignore\n     return phys_sharding.with_spec(phys_spec[:-elt_aval.ndim])\n   else:\n     return get_logical_gspmd_sharding(logical_shape, dtype, phys_sharding)\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28612 from jakevdp:fix-matrix-power\n\nPiperOrigin-RevId: 756378398",
        "output": [
            "```diff\nCommit: 5d0cd68ea261658a04aebee5dc05ec1e4b41506a\nDate: 2025-05-08T17:57:40Z\nURL: https://github.com/jax-ml/jax/commit/5d0cd68ea261658a04aebee5dc05ec1e4b41506a\nFiles changed: 2\nAdditions: +8, Deletions: -2\ndiff --git a/jax/_src/numpy/linalg.py b/jax/_src/numpy/linalg.py\nindex 146bbbda0213..0e20e5b2a416 100644\n--- a/jax/_src/numpy/linalg.py\n+++ b/jax/_src/numpy/linalg.py\n@@ -367,8 +367,7 @@ def matrix_power(a: ArrayLike, n: int) -> Array:\n     Array([[ 5.5 , -2.5 ],\n            [-3.75,  1.75]], dtype=float32)\n   \"\"\"\n-  a = ensure_arraylike(\"jnp.linalg.matrix_power\", a)\n-  arr, = promote_dtypes_inexact(a)\n+  arr = ensure_arraylike(\"jnp.linalg.matrix_power\", a)\n \n   if arr.ndim < 2:\n     raise TypeError(\"{}-dimensional array given. Array must be at least \"\ndiff --git a/tests/linalg_test.py b/tests/linalg_test.py\nindex 74259c300cf7..033ca989c8e7 100644\n--- a/tests/linalg_test.py\n+++ b/tests/linalg_test.py\n@@ -1230,6 +1230,13 @@ def testMatrixPower(self, shape, dtype, n):\n     self._CompileAndCheck(partial(jnp.linalg.matrix_power, n=n), args_maker,\n                           rtol=1e-3)\n \n+  def testMatrixPowerBool(self):\n+    # Regression test for https://github.com/jax-ml/jax/issues/28603\n+    mat = np.array([[True,True], [False,True]])\n+    np_result = np.linalg.matrix_power(mat, 2)\n+    jnp_result = jnp.linalg.matrix_power(mat, 2)\n+    self.assertArraysEqual(np_result, jnp_result)\n+\n   @jtu.sample_product(\n     shape=[(3, ), (1, 2), (8, 5), (4, 4), (5, 5), (50, 50), (3, 4, 5),\n            (2, 3, 4, 5)],\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas][Mosaic GPU] Refactor carry_coroutine in warp specialized pipeline to be a callback instead of a coroutine.\n\nPiperOrigin-RevId: 756378224",
        "output": [
            "```diff\nCommit: f15ad5a3521ed9a966e37a3abcc3e9002bd80ba3\nDate: 2025-05-08T17:55:30Z\nURL: https://github.com/jax-ml/jax/commit/f15ad5a3521ed9a966e37a3abcc3e9002bd80ba3\nFiles changed: 3\nAdditions: +78, Deletions: -43\ndiff --git a/jax/_src/pallas/mosaic_gpu/pipeline.py b/jax/_src/pallas/mosaic_gpu/pipeline.py\nindex 9b743bb18b37..426f314bc3a1 100644\n--- a/jax/_src/pallas/mosaic_gpu/pipeline.py\n+++ b/jax/_src/pallas/mosaic_gpu/pipeline.py\n@@ -16,6 +16,7 @@\n \n from __future__ import annotations\n \n+from typing import Protocol, TypeVar\n from collections.abc import Callable, Sequence\n import dataclasses\n import functools\n@@ -39,6 +40,7 @@\n \n map = util.safe_map\n zip = util.safe_zip\n+T = TypeVar('T')\n \n def _get_block_size(\n     bd: pl.Blocked | pl.Element | pl.Squeezed | pl.BoundedSlice | int | None,\n@@ -378,6 +380,34 @@ def do_fetch():\n   return pipeline\n \n \n+class ComputeContext(Protocol):\n+  \"\"\"Protocol for a compute context for the warp specialized pipeline.\n+\n+  The ComputeContext is run exclusively in the compute thread and allows\n+  the user to set up a prologue to initialize a pipeline carry and an epilogue\n+  to consume the final carry.\n+\n+  All values allocated in the ComputeContext will only be allocated in the\n+  compute thread and not the memory thread. This can potentially reduce\n+  register pressure if certain values are only consumed by the compute threads.\n+\n+  Usage will usually follow this structure:\n+\n+  ```\n+  def compute_context(pipeline):\n+    # Perform prologue work and compute the initial carry.\n+    initial_carry = ...\n+    # Run the pipeline.\n+    final_carry = pipeline(*initial_carry)\n+    # Perform epilogue work using the final carry.\n+    do_work(final_carry)\n+  ```\n+\n+  \"\"\"\n+  def __call__(self, pipeline: Callable[[T], T]) -> None:\n+    ...\n+\n+\n def emit_pipeline_warp_specialized(\n     body: Callable[..., None],\n     *,\n@@ -389,7 +419,7 @@ def emit_pipeline_warp_specialized(\n     wg_axis: str,\n     num_compute_wgs: int,\n     manual_consumed_barriers: bool = False,\n-    carry_coroutine: Any | None = None,\n+    compute_context: ComputeContext | None = None,\n     memory_thread_idx: int | None = None,\n ):\n   \"\"\"Creates a function to emit a warp-specialized pipeline.\n@@ -402,7 +432,7 @@ def emit_pipeline_warp_specialized(\n   def body(indices, *input_refs, *output_refs, [consumed_barriers]) -> None:\n   ```\n \n-  or with a carries enabled (enabled via the ``carry_coroutine`` argument),\n+  or with a carries enabled (enabled via the ``compute_context`` argument),\n   where the body returns the next carry:\n \n   ```\n@@ -425,11 +455,15 @@ def body(\n     manual_consumed_barriers: If True, consumed barriers will be\n       passed into the body function after the output refs. There will be one\n       barrier per input and will be passed in the same order.\n-    carry_coroutine: If specified, enables carries in the pipeline.\n-      The signature of the body function will be modified such that the last\n-      argument will be the current carry and it must return the next carry.\n-      The coroutine itself should yield the initial carry, and the\n-      yield statement will return the final value of the carry.\n+    compute_context: If specified, enables carries in the pipeline and allows\n+      a user-specified prologue/epilogue that is only executed in the compute\n+      thread. The signature of the pipeline body function will be modified\n+      such that the last argument will be the current carry and it must\n+      return the next carry.\n+      The compute_context itself should follow the signature of `ComputeContext`\n+      and take a pipeline function as its sole argument. Calling the\n+      pipeline with the initial carry will run the pipeline and return the\n+      final carry.\n     memory_thread_idx: The index of the memory thread. If not specified,\n       defaults to the last thread.\n   \"\"\"\n@@ -443,7 +477,7 @@ def body(\n     # thread is the last thread.\n     raise NotImplementedError(\"Memory thread must be the last thread.\")\n \n-  has_carry = carry_coroutine is not None\n+  has_carry = compute_context is not None\n \n   # Trace the index maps to determine if they depend on the grid.\n   # Grid-independent values will not be multiple-buffered.\n@@ -622,25 +656,29 @@ def compute_loop_body(step, carry):\n       ]\n \n       if has_carry:\n-        _carry = carry_coroutine()\n-        try:\n-          carry_init = next(_carry)\n-        except StopIteration:\n-          raise ValueError(\"carry_coroutine must yield the initial carry.\")  # pylint: disable=raise-missing-from\n+        last_indices = None\n+        def pipeline_callback(user_init_carry):\n+          nonlocal last_indices\n+          if last_indices is not None:\n+            raise ValueError(\n+              \"Cannot call pipeline more than once in `compute_context`\")\n+          print(\"[DEBUG] user_init_carry: \", user_init_carry)\n+          init_loop_carry = (init_indices, last_store_slices, user_init_carry)\n+          last_indices, _, final_body_carry = lax.fori_loop(0,\n+                        num_steps,\n+                        compute_loop_body,\n+                        init_loop_carry)\n+          print(\"[DEBUG] final_body_carry: \", final_body_carry)\n+          return final_body_carry\n+        compute_context(pipeline_callback)\n+        if last_indices is None:\n+          raise ValueError(\"Pipeline was not called in `compute_context`\")\n       else:\n-        _carry = None\n-        carry_init = None\n-      init_loop_carry = (init_indices, last_store_slices, carry_init)\n-      last_indices, _, final_body_carry = lax.fori_loop(0,\n-                    num_steps,\n-                    compute_loop_body,\n-                    init_loop_carry)\n-      if has_carry:\n-        try:\n-          _carry.send(final_body_carry)  # pytype: disable=attribute-error\n-          raise ValueError(\"carry_coroutine must only yield once.\")\n-        except StopIteration:\n-          pass\n+        assert compute_context is None\n+        last_indices, _, _ = lax.fori_loop(\n+            0, num_steps, compute_loop_body,\n+            (init_indices, last_store_slices, None)\n+        )\n \n       # Handle index_invariant outputs after the loop. They are not\n       # written in the main pipeline loop.\ndiff --git a/jax/experimental/pallas/ops/gpu/attention_mgpu.py b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\nindex 4d43d6045fee..e7a9898bf9f3 100644\n--- a/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n+++ b/jax/experimental/pallas/ops/gpu/attention_mgpu.py\n@@ -336,7 +336,7 @@ def kernel_dq(q_ref, k_ref, v_ref, do_ref, lse_ref, delta_ref, dq_ref,\n     kv_head = lax.div(q_head, jnp.array(q_heads_per_kv_head, q_head.dtype))\n     q_smem2, do_smem2, lse_smem2, delta_smem2 = smem_buffers\n     q_barriers, do_barriers, lse_barriers, delta_barriers = buffer_barriers\n-    def _compute_thread():\n+    def _compute_thread(pipeline_callback):\n       q_smem, do_smem, lse_smem, delta_smem = q_smem2.at[wg_idx], do_smem2.at[wg_idx], lse_smem2.at[wg_idx], delta_smem2.at[wg_idx]\n       q_seq_base = lax.axis_index(\"q_seq\") * (compute_wgs * block_q) + wg_idx * block_q\n       q_slice = (batch, pl.ds(q_seq_base, block_q), q_head)\n@@ -360,7 +360,7 @@ def _compute_thread():\n       dq_acc = plgpu.layout_cast(\n           jnp.full((block_q, head_dim), 0, dtype=jnp.float32), plgpu.Layout.WGMMA,\n       )\n-      dq, _, _ = (yield (dq_acc, lse, delta))\n+      dq, _, _ = pipeline_callback((dq_acc, lse, delta))\n       q_smem[...] = dq.astype(dtype)\n       plgpu.commit_smem()\n       plgpu.copy_smem_to_gmem(q_smem, dq_ref.at[q_slice])\n@@ -406,7 +406,7 @@ def compute_dq(acc_ref):\n         memory_registers=40,\n         wg_axis=\"wg\",\n         manual_consumed_barriers=True,\n-        carry_coroutine=_compute_thread,\n+        compute_context=_compute_thread,\n         in_specs=[\n             plgpu.GPUBlockSpec(  # k\n                 block_shape=(block_kv, head_dim),\n@@ -429,7 +429,7 @@ def kernel_dkv(q_ref, k_ref, v_ref, do_ref, lse_ref, delta_ref,\n     (k_smem2, v_smem2) = smem_buffers\n     (k_barriers, v_barriers) = buffer_barriers\n \n-    def _compute_thread():\n+    def _compute_thread(pipeline_callback):\n       k_smem, v_smem = k_smem2.at[wg_idx], v_smem2.at[wg_idx]\n       kv_seq_base = lax.axis_index(\"kv_seq\") * (compute_wgs * block_kv) + wg_idx * block_kv\n       kv_head = lax.div(q_head, jnp.array(q_heads_per_kv_head, q_head.dtype))\n@@ -449,7 +449,7 @@ def _compute_thread():\n       dv_acc = plgpu.layout_cast(\n           jnp.full((block_kv, head_dim), 0, dtype=jnp.float32), plgpu.Layout.WGMMA,\n       )\n-      (dk, dv) = (yield (dv_acc, dk_acc))\n+      (dk, dv) = pipeline_callback((dv_acc, dk_acc))\n       k_smem[...] = dk.astype(dtype)\n       v_smem[...] = dv.astype(dtype)\n \n@@ -513,7 +513,7 @@ def compute_dk(acc_ref):\n       memory_registers=40,\n       wg_axis=\"wg\",\n       manual_consumed_barriers=True,\n-      carry_coroutine=_compute_thread,\n+      compute_context=_compute_thread,\n       in_specs=[\n           plgpu.GPUBlockSpec(  # q\n               block_shape=(block_q, head_dim),\n@@ -627,7 +627,7 @@ def perform_schedule_barrier():\n         plgpu.barrier_arrive(schedule_barrier)\n         plgpu.barrier_wait(schedule_barrier)\n \n-    def _compute_thread():\n+    def _compute_thread(pipeline_callback):\n       qo_smem = qo_smem2.at[wg_idx]\n       lse_smem = lse_smem2.at[wg_idx] if lse_smem2 is not None else None\n       m_i = jnp.full((block_q,), -jnp.inf, dtype=jnp.float32)\n@@ -641,7 +641,7 @@ def _compute_thread():\n       )\n       plgpu.barrier_wait(q_barriers.at[wg_idx])\n       pl.when(wg_idx == 1)(perform_schedule_barrier)\n-      final_carry = (yield (acc, m_i, l_i))\n+      final_carry = pipeline_callback((acc, m_i, l_i))\n       pl.when(wg_idx == 0)(perform_schedule_barrier)\n       acc, m_i, l_i = final_carry\n       acc /= lax.broadcast_in_dim(l_i, (block_q, head_dim), [0])\n@@ -699,7 +699,7 @@ def compute_pv(acc_ref):\n         memory_registers=40,\n         wg_axis=\"wg\",\n         manual_consumed_barriers=True,\n-        carry_coroutine=_compute_thread,\n+        compute_context=_compute_thread,\n         in_specs=[\n             plgpu.GPUBlockSpec(  # k\n                 block_shape=(block_kv, head_dim),\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex 6746674250d0..71b4b491f7e4 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -2749,16 +2749,14 @@ def test_carry_accumulate(self, m=256, n=256, num_compute_wgs=2):\n         thread_name=\"wg\",\n     )\n     def kernel(x_gmem, acc_gmem, acc_smem):\n-      def _compute_thread():\n+      def _compute_thread(pipeline_fn):\n         # Cast the init value to the same layout as x_smem, so the pipeline loop\n         # carry has a constant signature.\n         o_acc = plgpu.layout_cast(\n           jnp.full((blk_m, blk_n,), 0, dtype=jnp.float32),\n           plgpu.Layout.WG_STRIDED((blk_m, blk_n), vec_size=2))\n-        carry_init = (o_acc,)\n         # Pass control to the pipeline emitter and return the final carry.\n-        final_carry = (yield carry_init)\n-        o_final, = final_carry\n+        o_final = pipeline_fn(o_acc)\n         # Note that both compute WGs are doing identical work so the potential\n         # race condition on the store here won't affect the result.\n         acc_smem[...] = o_final\n@@ -2767,9 +2765,8 @@ def _compute_thread():\n         plgpu.wait_smem_to_gmem(0)\n \n       def tiled_acc_kernel(_, x_smem, carry):\n-        o_carry, = carry\n-        new_carry = x_smem[...] + o_carry\n-        return (new_carry,)\n+        new_carry = x_smem[...] + carry\n+        return new_carry\n \n       pipeline = mgpu_pipeline.emit_pipeline_warp_specialized(\n           tiled_acc_kernel,\n@@ -2778,7 +2775,7 @@ def tiled_acc_kernel(_, x_smem, carry):\n           num_compute_wgs=num_compute_wgs,\n           memory_registers=40,\n           wg_axis=\"wg\",\n-          carry_coroutine=_compute_thread,\n+          compute_context=_compute_thread,\n           in_specs=[\n               pl.BlockSpec(\n                   block_shape=(blk_m, blk_n), index_map=lambda i, j: (i, j)\n\n```"
        ],
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28610 from jakevdp:fix-put\n\nPiperOrigin-RevId: 756376329",
        "output": [
            "```diff\nCommit: 4fbd7f1df2c2255f533b01c438bcb064015e3f8d\nDate: 2025-05-08T17:50:13Z\nURL: https://github.com/jax-ml/jax/commit/4fbd7f1df2c2255f533b01c438bcb064015e3f8d\nFiles changed: 1\nAdditions: +4, Deletions: -4\ndiff --git a/jax/_src/numpy/indexing.py b/jax/_src/numpy/indexing.py\nindex 17fbccd7ac9d..6aa5d6b87ef4 100644\n--- a/jax/_src/numpy/indexing.py\n+++ b/jax/_src/numpy/indexing.py\n@@ -1271,16 +1271,16 @@ def put(a: ArrayLike, ind: ArrayLike, v: ArrayLike,\n            [ 0,  0, 20,  0,  0],\n            [ 0,  0,  0,  0, 30]], dtype=int32)\n   \"\"\"\n+  if inplace:\n+    raise ValueError(\n+      \"jax.numpy.put cannot modify arrays in-place, because JAX arrays are immutable. \"\n+      \"Pass inplace=False to instead return an updated array.\")\n   arr, ind_arr, _ = util.ensure_arraylike(\"put\", a, ind, v)\n   ind_arr = ind_arr.ravel()\n   v_arr = lax_numpy.ravel(v)\n   if not arr.size or not ind_arr.size or not v_arr.size:\n     return arr\n   v_arr = lax_numpy._tile_to_size(v_arr, len(ind_arr))\n-  if inplace:\n-    raise ValueError(\n-      \"jax.numpy.put cannot modify arrays in-place, because JAX arrays are immutable. \"\n-      \"Pass inplace=False to instead return an updated array.\")\n   if mode is None:\n     scatter_mode = \"drop\"\n   elif mode == \"clip\":\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas][Mosaic GPU] Add collective support to Blackwell/tcgen05 MMA.\n\nThis exposes the collective argument to tcgen05_mma which allows paired CTAs to collaborate on matmuls across blocks.\n\nPiperOrigin-RevId: 756372254",
        "output": [
            "```diff\nCommit: bf830823246b4d604438285baf89f0c6ec6e8738\nDate: 2025-05-08T17:41:12Z\nURL: https://github.com/jax-ml/jax/commit/bf830823246b4d604438285baf89f0c6ec6e8738\nFiles changed: 4\nAdditions: +135, Deletions: -16\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex f193ce7d2743..6be8b3c4a8a5 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -126,11 +126,12 @@ def __call__(\n       dtype: jnp.dtype,\n       *,\n       transforms: Sequence[MemoryRefTransform] = (),\n-      packed: bool | None = None\n+      packed: bool | None = None,\n+      collective: bool | None = None\n   ) -> pallas_core.MemoryRef:\n     # A convenience function for constructing MemoryRef types.\n     return GPUMemoryRef(shape, dtype, memory_space=self, transforms=transforms,\n-                        packed=packed)\n+                        packed=packed, collective=collective)\n \n \n class SemaphoreType(enum.Enum):\n@@ -224,10 +225,14 @@ class GPUMemoryRef(pallas_core.MemoryRef):\n \n   # Whether to allow TMEM packing for sub 4-byte dtypes.\n   packed: bool | None = dataclasses.field(default=None, kw_only=True)\n+  collective: bool | None = dataclasses.field(default=None, kw_only=True)\n \n   def __post_init__(self):\n-    if self.packed is not None and self.memory_space != GPUMemorySpace.TMEM:\n-      raise ValueError(\"Packed option is only supported for TMEM.\")\n+    if self.memory_space != GPUMemorySpace.TMEM:\n+      if self.packed is not None:\n+        raise ValueError(\"Packed option is only supported for TMEM.\")\n+      if self.collective is not None:\n+        raise ValueError(\"Collective option is only supported for TMEM.\")\n \n   def get_ref_aval(self) -> _Ref:\n     aval = jax_core.ShapedArray(self.shape, self.dtype)\n@@ -237,7 +242,8 @@ def get_ref_aval(self) -> _Ref:\n       ref = pallas_core.TransformedRef(\n           AbstractTMEMRef(aval,\n                           memory_space=self.memory_space,\n-                          packed=self.packed), ()\n+                          packed=self.packed,\n+                          collective=self.collective), ()\n       )\n     else:\n       ref = pallas_core.TransformedRef(\n@@ -936,11 +942,12 @@ def _as_accum(ref) -> WGMMAAbstractAccumulatorRef:\n   )\n \n class AbstractTMEMRef(AbstractMemoryRef):\n-  __slots__ = [\"inner_aval\", \"memory_space\", \"packed\"]\n+  __slots__ = [\"inner_aval\", \"memory_space\", \"packed\", \"collective\"]\n \n-  def __init__(self, inner_aval, memory_space, packed):\n+  def __init__(self, inner_aval, memory_space, packed, collective):\n     super().__init__(inner_aval, memory_space)\n     self.packed = packed\n+    self.collective = collective\n \n   def __repr__(self) -> str:\n     return f'TMEM({self.inner_aval.str_short()},packed={self.packed})'\ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex 29ec0d16d2fb..55aca65c6a19 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -251,7 +251,7 @@ def _run_scoped_resource_estimator(\n       else:\n         packing = 1\n       layout = tcgen05._infer_tmem_layout(\n-          aval.shape, collective=False, packing=packing)\n+          aval.shape, collective=aval.collective, packing=packing)\n       cols_used = layout.cols_in_shape(aval.shape)\n       cols_used = tcgen05._alloc_ncols(cols_used, exact=False)\n       rs += Resources(tmem_scratch_cols=cols_used)\n@@ -2235,6 +2235,7 @@ def _run_scoped_lowering_rule(\n                 jax.ShapeDtypeStruct(shape=aval.shape, dtype=aval.dtype),\n                 packed=aval.packed,\n                 exact_cols=False,\n+                collective=aval.collective,\n             )\n         )\n         input_refs.append(input_ref)\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 1081f052f4c1..c5cf257070c4 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -1124,9 +1124,20 @@ def tcgen05_mma(acc: _Ref,\n                 a: _Ref,\n                 b: _Ref,\n                 barrier: _Ref,\n-                accumulate: bool | jax.Array = True):\n+                accumulate: bool | jax.Array = True,\n+                collective_axis: str | None = None):\n   \"\"\"Asynchronous matrix-multiply accumulate for TensorCore gen 5 (Blackwell).\n \n+  If run in collective mode, `acc`, `a` (LHS), and `b` (RHS) should correspond\n+  to half of the total inputs to the MMA, where `acc` and `a` (LHS) are split\n+  in half along the rows and `b` (RHS) is split along the columns like so:\n+\n+   -----------    -----------   -----------\n+   |  ACC1   |    |  LHS1   |   |    |    |\n+   ----------- += ----------- @ |RHS1|RHS2|\n+   |  ACC2   |    |  LHS2   |   |    |    |\n+   -----------    -----------   -----------\n+\n   Args:\n     acc: The accumulator. Must be a TMEM Ref.\n     a: The left-hand side. Must be a TMEM/SMEM Ref.\n@@ -1134,10 +1145,15 @@ def tcgen05_mma(acc: _Ref,\n     barrier: Barrier Ref for synchronizing with the tensor core. Should have\n       for_tensor_core set to True.\n     accumulate: Whether to accumulate into acc or overwrite it.\n+    collective_axis: The name of the cluster axis along which to perform\n+      a collective MMA. The cluster axis should have a size of exactly 2,\n+      and must be on the minormost cluster axis.\n   \"\"\"\n   acc_m, acc_n = acc.shape\n   lhs_m, lhs_k = a.shape\n   rhs_k, rhs_n = b.shape\n+  if collective_axis is not None:\n+    acc_n /= 2\n   if acc_m != lhs_m:\n     raise ValueError(\n         f\"Accumulator and LHS have incompatible shapes. Accumulator: {acc.shape}. LHS: {a.shape}.\")\n@@ -1164,16 +1180,14 @@ def tcgen05_mma(acc: _Ref,\n                       *a_transforms_leaves, *b_transforms_leaves,\n                       a_transforms_tree=a_transforms_tree,\n                       b_transforms_tree=b_transforms_tree,\n-                      collective=False)\n+                      collective_axis=collective_axis)\n \n @tcgen05_mma_p.def_abstract_eval\n def _tcgen05_mma_abstract_eval(acc, a, b, barrier, accumulate,\n                                *transforms_leaves,\n                                a_transforms_tree, b_transforms_tree,\n-                               collective):\n+                               collective_axis):\n   del (accumulate, transforms_leaves, a_transforms_tree, b_transforms_tree)\n-  if collective:\n-    raise NotImplementedError(\"Collective MMA not yet implemented.\")\n \n   if acc.memory_space != gpu_core.GPUMemorySpace.TMEM:\n     raise ValueError(\"Accumulator must be a TMEM Ref.\")\n@@ -1183,6 +1197,14 @@ def _tcgen05_mma_abstract_eval(acc, a, b, barrier, accumulate,\n   if b.memory_space != gpu_core.GPUMemorySpace.SMEM:\n     raise ValueError(\"RHS must be an SMEM Ref.\")\n \n+  if collective_axis is not None:\n+    if not acc.collective:\n+      raise ValueError(\n+          \"Accumulator Ref must be collective if collective_axis is set.\")\n+    if a.memory_space == gpu_core.GPUMemorySpace.TMEM and not a.collective:\n+      raise ValueError(\n+          \"LHS TMEM Ref must be collective if collective_axis is set.\")\n+\n   for_tensor_core = getattr(\n       barrier.inner_aval.dtype, \"for_tensor_core\", False)\n   if not for_tensor_core:\n@@ -1202,7 +1224,7 @@ def _tcgen05_mma_lowering(\n     *transforms_leaves,\n     a_transforms_tree,\n     b_transforms_tree,\n-    collective: bool,\n+    collective_axis,\n ):\n   _, a_aval, b_aval, *_ = ctx.avals_in\n   lhs_swizzle: int = 128\n@@ -1267,12 +1289,26 @@ def _tcgen05_mma_lowering(\n     accumulate = mgpu.c(accumulate, ir.IntegerType.get_signless(1))\n \n   predicate = ctx.module_ctx.single_lane_predicate\n-  if collective:\n+  collective = False\n+  if collective_axis is not None:\n+    cluster_axis = lowering._resolve_cluster_axis(\n+        ctx.module_ctx.axis_names, collective_axis)\n+    if cluster_axis != gpu_dialect.Dimension(0):\n+      # Note: resolve_cluster_axis checks if axis_names exists.\n+      assert ctx.module_ctx.axis_names is not None\n+      if len(ctx.module_ctx.axis_names.cluster) <= 1:\n+        raise ValueError(\"No cluster axes found.\")\n+      minormost_cluster_axis = ctx.module_ctx.axis_names.cluster[0]\n+      raise ValueError(\n+          \"Can only perform collective MMA along minormost cluster axis. \"\n+          f\"Got {collective_axis}, expected {minormost_cluster_axis}.\")\n     index = ir.IndexType.get()\n     is_leader_block = arith_dialect.cmpi(\n         arith_dialect.CmpIPredicate.eq,\n-        ctx.launch_ctx.cluster_idx(gpu_dialect.Dimension.x), mgpu.c(0, index))\n+        ctx.launch_ctx.cluster_idx(cluster_axis), mgpu.c(0, index))\n     predicate = arith_dialect.andi(predicate, is_leader_block)\n+    collective = True\n+\n   with mgpu.when(predicate):\n     tcgen05.mma(\n               acc,\ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex ed45c3b9f1e6..6746674250d0 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -2197,6 +2197,81 @@ def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref,\n     expected = x @ y\n     np.testing.assert_allclose(result, expected, rtol=1e-3)\n \n+  @parameterized.parameters(\n+      ((256, 256), (256, 256), 128, jnp.float16),\n+      # Test additional shape combinations.\n+      ((256, 128), (128, 128), 128, jnp.float16),\n+      ((256, 64), (64, 256), 128, jnp.float16),\n+      # Test bfloat16.\n+      ((256, 256), (256, 256), 128, jnp.bfloat16),\n+      # Test additional swizzles.\n+      ((256, 256), (256, 256), 64, jnp.float16),\n+      ((256, 256), (256, 256), 32, jnp.float16),\n+  )\n+  def test_simple_collective_matmul(self, lhs_shape, rhs_shape, swizzle, dtype):\n+    self.skip_if_wg_semantics()\n+    # Test a collective (paired CTA) matmul on a single block.\n+    swizzle_elems = swizzle // jnp.dtype(dtype).itemsize\n+    transforms = (\n+        plgpu.TilingTransform((8, swizzle_elems)),\n+        plgpu.SwizzleTransform(swizzle),\n+    )\n+\n+    acc_shape = (lhs_shape[0], rhs_shape[1])\n+    _acc_shape = (lhs_shape[0] // 2, rhs_shape[1])\n+    _lhs_shape = (lhs_shape[0] // 2, lhs_shape[1])\n+    _rhs_shape = (rhs_shape[0], rhs_shape[1] // 2)\n+\n+    def kernel(a_gmem, b_gmem, out_gmem):\n+      cluster_idx = lax.axis_index(\"x\")\n+      slice_lhs = pl.ds(cluster_idx * _lhs_shape[0], _lhs_shape[0])\n+      slice_rhs = pl.ds(cluster_idx * _rhs_shape[1], _rhs_shape[1])\n+\n+      @functools.partial(pl.run_scoped,\n+        a_smem=plgpu.SMEM(_lhs_shape, dtype, transforms=transforms),\n+        b_smem=plgpu.SMEM(_rhs_shape, dtype, transforms=transforms),\n+        acc_tmem=plgpu.TMEM(_acc_shape, jnp.float32, collective=True),\n+        scratch_smem=plgpu.SMEM(_acc_shape, dtype, transforms=transforms),\n+        tma_barrier=plgpu.Barrier(num_arrivals=1),\n+        mma_barrier=plgpu.Barrier(num_arrivals=1, for_tensor_core=True),\n+        cluster_barrier=plgpu.ClusterBarrier(collective_axes=(\"x\",)),\n+      )\n+      def _scoped(a_smem, b_smem,\n+                  acc_tmem, scratch_smem, tma_barrier, mma_barrier, cluster_barrier):\n+        plgpu.copy_gmem_to_smem(a_gmem.at[slice_lhs, :], a_smem, tma_barrier)\n+        plgpu.barrier_wait(tma_barrier)\n+        plgpu.copy_gmem_to_smem(b_gmem.at[:, slice_rhs], b_smem, tma_barrier)\n+        plgpu.barrier_wait(tma_barrier)\n+\n+        plgpu.barrier_arrive(cluster_barrier)\n+        plgpu.barrier_wait(cluster_barrier)\n+\n+        plgpu.tcgen05_mma(acc_tmem,\n+                          a_smem,\n+                          b_smem,\n+                          mma_barrier,\n+                          accumulate=False,\n+                          collective_axis=\"x\")\n+        plgpu.barrier_wait(mma_barrier)\n+        scratch_smem[...] = acc_tmem[...].astype(dtype)\n+        plgpu.commit_smem()\n+        plgpu.copy_smem_to_gmem(scratch_smem, out_gmem.at[slice_lhs, :])\n+        plgpu.wait_smem_to_gmem(0)\n+\n+    f = self.kernel(\n+      kernel,\n+      out_shape=jax.ShapeDtypeStruct(acc_shape, dtype),\n+      grid=(1,),\n+      grid_names=(\"_\",),\n+      cluster=(2,),\n+      cluster_names=(\"x\",),\n+    )\n+    x = jax.random.uniform(jax.random.key(0), shape=lhs_shape, dtype=dtype)\n+    y = jax.random.uniform(jax.random.key(1), shape=rhs_shape, dtype=dtype)\n+    result = f(x, y)\n+    expected = x @ y\n+    np.testing.assert_allclose(result, expected, rtol=1e-3)\n+\n \n class PallasCallSm100AWGTest(\n     PallasCallSm100ATest, lowering_semantics=plgpu.LoweringSemantics.Warpgroup\n\n```"
        ],
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add `out_sharding` to `jnp.repeat`. Drop into auto mode if out_sharding is provided.\n\nIn cases where axis is None or the input is sharded on the `axis` we are going to repeat on.\n\nIf the input is not sharded on the repeat axis, forward the input sharding to the output.\n\nFixes https://github.com/jax-ml/jax/issues/28538\n\nPiperOrigin-RevId: 756372112",
        "output": [
            "```diff\nCommit: 93d02349aab18dbff83569b62281801f4b509908\nDate: 2025-05-08T17:39:40Z\nURL: https://github.com/jax-ml/jax/commit/93d02349aab18dbff83569b62281801f4b509908\nFiles changed: 5\nAdditions: +52, Deletions: -6\ndiff --git a/jax/_src/basearray.pyi b/jax/_src/basearray.pyi\nindex cf64afdacfe3..a98cc012031e 100644\n--- a/jax/_src/basearray.pyi\n+++ b/jax/_src/basearray.pyi\n@@ -189,7 +189,8 @@ class Array(metaclass=abc.ABCMeta):\n   @property\n   def real(self) -> Array: ...\n   def repeat(self, repeats: ArrayLike, axis: int | None = None, *,\n-             total_repeat_length: int | None = None) -> Array: ...\n+             total_repeat_length: int | None = None,\n+             out_sharding: NamedSharding | P | None = None) -> Array: ...\n   def reshape(self, *args: Any, order: str = \"C\",\n               out_sharding: NamedSharding | P | None = ...) -> Array: ...\n   def round(self, decimals: int = 0, out: None = None) -> Array: ...\ndiff --git a/jax/_src/numpy/array_methods.py b/jax/_src/numpy/array_methods.py\nindex a3dbc0f9f6c6..73f916537af0 100644\n--- a/jax/_src/numpy/array_methods.py\n+++ b/jax/_src/numpy/array_methods.py\n@@ -293,12 +293,15 @@ def _real_property(self: Array) -> Array:\n   return ufuncs.real(self)\n \n def _repeat(self: Array, repeats: ArrayLike, axis: int | None = None, *,\n-            total_repeat_length: int | None = None) -> Array:\n+            total_repeat_length: int | None = None,\n+            out_sharding: NamedSharding | PartitionSpec | None = None) -> Array:\n   \"\"\"Construct an array from repeated elements.\n \n   Refer to :func:`jax.numpy.repeat` for the full documentation.\n   \"\"\"\n-  return lax_numpy.repeat(self, repeats=repeats, axis=axis, total_repeat_length=total_repeat_length)\n+  return lax_numpy.repeat(self, repeats=repeats, axis=axis,\n+                          total_repeat_length=total_repeat_length,\n+                          out_sharding=out_sharding)\n \n def _reshape(self: Array, *args: Any, order: str = \"C\", out_sharding=None\n              ) -> Array:\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex b662f1d6f7ed..2a1ec7227439 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -65,7 +65,9 @@\n     NumpyComplexWarning, canonicalize_axis as _canonicalize_axis,\n     ceil_of_ratio, safe_zip, set_module, unzip2)\n from jax.sharding import Sharding\n-from jax._src.sharding_impls import (NamedSharding, PartitionSpec as P)\n+from jax._src.sharding_impls import NamedSharding, PartitionSpec as P\n+from jax._src.mesh import get_abstract_mesh\n+from jax._src.pjit import auto_axes\n from jax.tree_util import tree_flatten, tree_map\n import numpy as np\n \n@@ -6630,7 +6632,8 @@ def indices(dimensions: Sequence[int], dtype: DTypeLike | None = None,\n \n @export\n def repeat(a: ArrayLike, repeats: ArrayLike, axis: int | None = None, *,\n-           total_repeat_length: int | None = None) -> Array:\n+           total_repeat_length: int | None = None,\n+           out_sharding: NamedSharding | P | None = None) -> Array:\n   \"\"\"Construct an array from repeated elements.\n \n   JAX implementation of :func:`numpy.repeat`.\n@@ -6694,6 +6697,31 @@ def repeat(a: ArrayLike, repeats: ArrayLike, axis: int | None = None, *,\n     Array([[1, 1, 2, 2, 2, 2, 2],\n            [3, 3, 4, 4, 4, 4, 4]], dtype=int32)\n   \"\"\"\n+  if out_sharding is not None:\n+    return auto_axes(\n+        partial(_repeat, axis=axis, total_repeat_length=total_repeat_length),\n+        out_sharding=out_sharding)(a, repeats)\n+  ctx_mesh = get_abstract_mesh()\n+  if ctx_mesh._are_all_axes_explicit:\n+    aval = core.typeof(a)\n+    if axis is None or aval.sharding.spec[axis] is not None:\n+      raise ValueError(\n+          \"Please pass sharding to `jnp.repeat` via `out_sharding` parameter.\")\n+    assert axis is not None and aval.sharding.spec[axis] is None\n+    out_sharding = (NamedSharding(ctx_mesh, P())\n+                    if aval.sharding.mesh.empty else aval.sharding)\n+    return auto_axes(\n+        partial(_repeat, axis=axis, total_repeat_length=total_repeat_length),\n+        out_sharding=out_sharding)(a, repeats)\n+  try:\n+    return _repeat(a, repeats, axis=axis,\n+                   total_repeat_length=total_repeat_length)\n+  except core.ShardingTypeError as e:\n+    raise ValueError(\n+        \"Please pass sharding to `jnp.repeat` via `out_sharding` parameter.\")\n+\n+def _repeat(a: ArrayLike, repeats: ArrayLike, *, axis: int | None = None,\n+            total_repeat_length: int | None = None) -> Array:\n   if core.is_dim(repeats):\n     util.check_arraylike(\"repeat\", a)\n   else:\ndiff --git a/jax/numpy/__init__.pyi b/jax/numpy/__init__.pyi\nindex a8de717a0d07..c52ce2628cda 100644\n--- a/jax/numpy/__init__.pyi\n+++ b/jax/numpy/__init__.pyi\n@@ -808,7 +808,8 @@ def reciprocal(x: ArrayLike, /) -> Array: ...\n register_jax_array_methods: Any\n def remainder(x: ArrayLike, y: ArrayLike, /) -> Array: ...\n def repeat(a: ArrayLike, repeats: ArrayLike, axis: int | None = ..., *,\n-           total_repeat_length: int | None = ...) -> Array: ...\n+           total_repeat_length: int | None = ...,\n+           out_sharding: NamedSharding | P | None = None) -> Array: ...\n def reshape(\n     a: ArrayLike, shape: DimSize | Shape, order: str = ..., *, copy: bool | None = ...,\n     out_sharding: NamedSharding | P | None = ...,\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 373e3ceff7a8..0c43df76cc5f 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7569,6 +7569,19 @@ def f(x):\n     out = f(arr)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('x', 'y')))\n \n+  @jtu.with_explicit_mesh((2,), ('x',))\n+  def test_jnp_repeat(self, mesh):\n+    out = jnp.repeat(np.eye(3), np.array((2,2,2,)) - 1, axis=0)\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P(None, None)))\n+\n+    a = jnp.eye(3)\n+    out = jnp.repeat(a, np.array((2,2,2,)) - 1, axis=0)\n+    self.assertEqual(out.sharding, a.sharding)\n+\n+    a = jax.device_put(jnp.eye(4), P('x'))\n+    out = jnp.repeat(a, np.array((2,2,2,2)) - 1, axis=0, out_sharding=P('x'))\n+    self.assertEqual(out.sharding, NamedSharding(mesh, P('x', None)))\n+\n   @jtu.with_explicit_mesh((2,), ('x',))\n   def test_scatter_gather(self, mesh):\n     x = np.random.uniform(size=(mesh.size * 2, 3))\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28279 from gspschmid:gschmid/jax_fwd_and_bwd_argnums\n\nPiperOrigin-RevId: 756356462",
        "output": [
            "```diff\nCommit: 87d16157d250a3831616dc30975e7a48fa8a87f1\nDate: 2025-05-08T17:03:15Z\nURL: https://github.com/jax-ml/jax/commit/87d16157d250a3831616dc30975e7a48fa8a87f1\nFiles changed: 2\nAdditions: +21, Deletions: -9\ndiff --git a/jax/_src/api.py b/jax/_src/api.py\nindex babfde1b6a7d..379f6d8d7c93 100644\n--- a/jax/_src/api.py\n+++ b/jax/_src/api.py\n@@ -576,8 +576,9 @@ def _check_output_dtype_revderiv(name, holomorphic, x):\n _check_output_dtype_grad = partial(_check_output_dtype_revderiv, \"grad\")\n \n def fwd_and_bwd(\n-    fun: Callable, has_aux: bool = False, jitted: bool = True\n-  ) -> tuple[Callable, Callable]:\n+    fun: Callable, argnums: int | Sequence[int], has_aux: bool = False,\n+    jitted: bool = True,\n+) -> tuple[Callable, Callable]:\n   \"\"\"Creates functions ``fwd`` and ``bwd`` corresponding to the forward and\n   backward pass of a given function ``fun``. The forward function ``fwd(*args)``\n   functionally behaves much like ``y, fun_vjp = jax.vjp(fun, *args)``, but allows\n@@ -598,7 +599,7 @@ def fwd_and_bwd(\n   ...     cot_x, cot_W = f_vjp(cot_out)           # not jitted\n   ...     cot_x, cot_W = jax.jit(f_vjp)(cot_out)  # recompiles on every iteration\n   ...\n-  >>> fwd, bwd = jax.fwd_and_bwd(f)\n+  >>> fwd, bwd = jax.fwd_and_bwd(f, argnums=(0,1))\n   >>> for i in range(3):\n   ...     y, residuals = fwd(x, W)\n   ...     cot_x, cot_W = bwd(residuals, cot_out)  # jitted, compiles once\n@@ -606,6 +607,8 @@ def fwd_and_bwd(\n \n   Args:\n     fun: Function to produce a forward and backward of.\n+    argnums: Integer or sequence of integers. Specifies which positional argument(s)\n+      to differentiate with respect to.\n     has_aux: Optional, bool. Indicates whether ``fun`` returns a pair where the\n      first element is considered the output of the mathematical function to be\n      differentiated and the second element is auxiliary data. Default False.\n@@ -624,13 +627,22 @@ def fwd_and_bwd(\n \n     ``bwd`` is a function from ``residuals`` and a cotangent vector with the same\n     shape as ``primals_out`` to a tuple of cotangent vectors with the same number\n-    and shapes as ``primals``, representing the vector-Jacobian product of ``fun``\n-    evaluated at ``primals``.\n+    and shapes as the ``primals`` designated by ``argnums``, representing the\n+    vector-Jacobian product of ``fun`` evaluated at ``primals``.\n   \"\"\"\n-  def fwd(*args):\n-    return vjp(fun, *args, has_aux=has_aux)  # type: ignore\n+  check_callable(fun)\n+  argnums = _ensure_index(argnums)\n+\n+  def fwd(*args, **kwargs):\n+    dbg = debug_info('fwd_and_bwd', fun, args, kwargs)\n+    f = lu.wrap_init(fun, params=kwargs, debug_info=dbg)\n+    f_partial, dyn_args = argnums_partial(\n+        f, argnums, args, require_static_args_hashable=False)\n+    return _vjp(f_partial, *dyn_args, has_aux=has_aux)  # type: ignore\n   def bwd(f_vjp, outgrad):\n-    return f_vjp(outgrad)\n+    g = f_vjp(outgrad)\n+    g = g[0] if isinstance(argnums, int) else g\n+    return g\n   if jitted:\n     fwd = jit(fwd)\n     bwd = jit(bwd)\ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 610719518a03..d75986be738e 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -1657,7 +1657,7 @@ def f(x, W):\n     expected_y, f_vjp = api.vjp(f, x, W)\n     expected_cot_x, expected_cot_W = f_vjp(cot_out)\n \n-    fwd, bwd = api.fwd_and_bwd(f)\n+    fwd, bwd = api.fwd_and_bwd(f, argnums=(0,1))\n     y, residuals = fwd(x, W)\n     cot_x, cot_W = bwd(residuals, cot_out)\n \n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Pallas][Mosaic GPU] Expand TMEM support.\n\nAllow:\n- Multiple TMEM allocs\n- Expand support to 16-bit dtypes.\n- Allow storing to TMEM from SMEM.\nPiperOrigin-RevId: 756332369",
        "output": [
            "```diff\nCommit: f7e11660e9a66e2764781997dc1086863d4af672\nDate: 2025-05-08T15:58:37Z\nURL: https://github.com/jax-ml/jax/commit/f7e11660e9a66e2764781997dc1086863d4af672\nFiles changed: 6\nAdditions: +168, Deletions: -44\ndiff --git a/jax/_src/pallas/mosaic_gpu/core.py b/jax/_src/pallas/mosaic_gpu/core.py\nindex 808759edf35c..f193ce7d2743 100644\n--- a/jax/_src/pallas/mosaic_gpu/core.py\n+++ b/jax/_src/pallas/mosaic_gpu/core.py\n@@ -124,10 +124,13 @@ def __call__(\n       self,\n       shape: tuple[int, ...],\n       dtype: jnp.dtype,\n+      *,\n       transforms: Sequence[MemoryRefTransform] = (),\n+      packed: bool | None = None\n   ) -> pallas_core.MemoryRef:\n     # A convenience function for constructing MemoryRef types.\n-    return GPUMemoryRef(shape, dtype, memory_space=self, transforms=transforms)\n+    return GPUMemoryRef(shape, dtype, memory_space=self, transforms=transforms,\n+                        packed=packed)\n \n \n class SemaphoreType(enum.Enum):\n@@ -219,13 +222,27 @@ def _is_known_divisible(value, divisor, fuel=10) -> bool:\n class GPUMemoryRef(pallas_core.MemoryRef):\n   transforms: Sequence[MemoryRefTransform] = ()\n \n+  # Whether to allow TMEM packing for sub 4-byte dtypes.\n+  packed: bool | None = dataclasses.field(default=None, kw_only=True)\n+\n+  def __post_init__(self):\n+    if self.packed is not None and self.memory_space != GPUMemorySpace.TMEM:\n+      raise ValueError(\"Packed option is only supported for TMEM.\")\n+\n   def get_ref_aval(self) -> _Ref:\n     aval = jax_core.ShapedArray(self.shape, self.dtype)\n     for t in self.transforms:\n       aval = t(aval)\n-    ref = pallas_core.TransformedRef(\n-        AbstractMemoryRef(aval, memory_space=self.memory_space), ()\n-    )\n+    if self.memory_space == GPUMemorySpace.TMEM:\n+      ref = pallas_core.TransformedRef(\n+          AbstractTMEMRef(aval,\n+                          memory_space=self.memory_space,\n+                          packed=self.packed), ()\n+      )\n+    else:\n+      ref = pallas_core.TransformedRef(\n+          AbstractMemoryRef(aval, memory_space=self.memory_space), ()\n+      )\n     for t in reversed(self.transforms):\n       ref = t.undo(ref)\n     if not ref.transforms:\n@@ -918,6 +935,16 @@ def _as_accum(ref) -> WGMMAAbstractAccumulatorRef:\n       memory_space=ref.memory_space,  # pytype: disable=attribute-error\n   )\n \n+class AbstractTMEMRef(AbstractMemoryRef):\n+  __slots__ = [\"inner_aval\", \"memory_space\", \"packed\"]\n+\n+  def __init__(self, inner_aval, memory_space, packed):\n+    super().__init__(inner_aval, memory_space)\n+    self.packed = packed\n+\n+  def __repr__(self) -> str:\n+    return f'TMEM({self.inner_aval.str_short()},packed={self.packed})'\n+\n \n _WARPGROUP_AXIS_NAME = object()\n \ndiff --git a/jax/_src/pallas/mosaic_gpu/lowering.py b/jax/_src/pallas/mosaic_gpu/lowering.py\nindex a6bdf76206d3..29ec0d16d2fb 100644\n--- a/jax/_src/pallas/mosaic_gpu/lowering.py\n+++ b/jax/_src/pallas/mosaic_gpu/lowering.py\n@@ -241,15 +241,20 @@ def _run_scoped_resource_estimator(\n           )\n       )\n     elif aval.memory_space == gpu_core.TMEM:\n-      if aval.dtype.itemsize != 4:\n-        raise ValueError(\"TMEM only supports 32-bit types.\")\n       if len(aval.shape) != 2:\n-        raise ValueError(\"TMEM allocations must be 2D.\")\n+        raise ValueError(f\"TMEM allocations must be 2D. Got {aval.shape}\")\n       if aval.shape[0] % tcgen05.TMEM_ROWS != 0:\n-        raise ValueError(\"TMEM shape[0] must be a multiple of 128.\")\n-      if aval.shape[1] % 8 != 0:\n-        raise ValueError(\"TMEM shape[1] must be a multiple of 8.\")\n-      rs += Resources(tmem_scratch_cols=aval.shape[1])\n+        raise ValueError(\n+            f\"TMEM shape[0] must be a multiple of 128. Got {aval.shape[0]}.\")\n+      if aval.packed:\n+        packing = 4 // aval.dtype.itemsize\n+      else:\n+        packing = 1\n+      layout = tcgen05._infer_tmem_layout(\n+          aval.shape, collective=False, packing=packing)\n+      cols_used = layout.cols_in_shape(aval.shape)\n+      cols_used = tcgen05._alloc_ncols(cols_used, exact=False)\n+      rs += Resources(tmem_scratch_cols=cols_used)\n     elif aval.memory_space == gpu_core.SMEM:\n       rs += Resources(\n           smem_scratch_bytes=math.prod(aval.shape) * aval.dtype.itemsize\n@@ -346,20 +351,30 @@ def reserve_barrier(\n   def alloc_tmem(\n       self,\n       struct: jax.ShapeDtypeStruct,\n-      layout: tcgen05.TMEMLayout | None = None\n+      *,\n+      layout: tcgen05.TMEMLayout | None = None,\n+      collective: bool = False,\n+      packed: bool = False,\n+      exact_cols: bool = False\n   ) -> ir.Value:\n-    if self.tmem_used_cols > 0:\n-      raise NotImplementedError(\n-          \"Multiple TMEM allocations are not implemented.\")\n+    if packed:\n+      packing = 4 // struct.dtype.itemsize\n+    else:\n+      packing = 1\n     if layout is None:\n-      layout = tcgen05._infer_tmem_layout(struct.shape, collective=False)\n-    cols_used = np.prod(struct.shape) // tcgen05.TMEM_ROWS\n+      layout = tcgen05._infer_tmem_layout(\n+          struct.shape, collective, packing=packing)\n+    unpadded_cols_used = layout.cols_in_shape(struct.shape)\n+    cols_used = tcgen05._alloc_ncols(unpadded_cols_used, exact_cols)\n+\n+    off = arith_dialect.addi(self.tmem_base_ptr,\n+                             _i32_constant(self.tmem_used_cols))\n+    tmem_ref = tcgen05.TMEMRef(\n+        address=off,\n+        shape=struct.shape,\n+        dtype=mgpu_utils.dtype_to_ir_type(struct.dtype),\n+        layout=layout)\n     self.tmem_used_cols += cols_used\n-    off = self.tmem_base_ptr\n-    tmem_ref = tcgen05.TMEMRef(address=off,\n-                               shape=struct.shape,\n-                               dtype=mgpu_utils.dtype_to_ir_type(struct.dtype),\n-                               layout=layout)\n     yield tmem_ref\n     self.tmem_used_cols -= cols_used\n \n@@ -610,6 +625,8 @@ def lower_pipelined_jaxpr_to_module(\n   def ref_for_aval(aval: jax_core.AbstractValue):\n     if isinstance(aval, gpu_core.WGMMAAbstractAccumulatorRef):\n       return gpu_core.WGMMAAccumulatorRef(aval.shape, aval.dtype)\n+    elif isinstance(aval, gpu_core.AbstractTMEMRef):\n+      return gpu_core.TMEM(aval.shape, aval.dtype, packed=aval.packed)\n     elif isinstance(aval, pallas_core.AbstractMemoryRef):\n       return pallas_core.MemoryRef(aval.shape, aval.dtype, aval.memory_space)\n     else:\n@@ -1324,17 +1341,32 @@ def _get_lowering_rule_wg(ctx: LoweringRuleContext, x_smem, *leaves, tree):\n \n @register_lowering_rule(sp.swap_p, mgpu.LoweringSemantics.Lane)\n def _swap_lowering_rule(\n-    ctx: LoweringRuleContext, x_smem, value, *leaves, tree\n+    ctx: LoweringRuleContext, x_ref, value, *leaves, tree\n ):\n   if not isinstance(value, mgpu.FragmentedArray):\n     raise TypeError(f\"Can only store arrays (got {value}).\")\n-  if not isinstance(x_smem, ir.Value) and ir.MemRefType.isinstance(x_smem):\n-    raise TypeError(f\"Can only store to references (got {x_smem}).\")\n+\n+  if isinstance(x_ref, tcgen05.TMEMRef):\n+    transforms = jax.tree.unflatten(tree, leaves)\n+    match transforms:\n+      case (indexer,) if isinstance(indexer, indexing.NDIndexer):\n+        if not gpu_core.is_trivial_index(indexer.indices, x_ref.shape):\n+          raise NotImplementedError(\n+              \"Only trivial indexing is supported for TMEM refs.\")\n+      case _:\n+        raise NotImplementedError(\n+            \"Only a single indexing transform is supported for TMEM refs.\")\n+    old_value = x_ref[:]\n+    x_ref[:] = value\n+    return old_value\n+\n+  if not isinstance(x_ref, ir.Value) and ir.MemRefType.isinstance(x_ref):\n+    raise TypeError(f\"Can only store to references (got {x_ref}).\")\n   v_aval = ctx.avals_in[1]\n   transforms = jax.tree.unflatten(tree, leaves)\n   transposed_value = value.layout == mgpu.WGMMA_TRANSPOSED_LAYOUT\n   x_smem, transforms = _handle_transforms(\n-      ctx, x_smem, transforms, handle_transposes=not transposed_value, allow_peer_refs=True\n+      ctx, x_ref, transforms, handle_transposes=not transposed_value, allow_peer_refs=True\n   )\n   mgpu.warpgroup_barrier()  # Make sure reads have completed before we write.\n   match transforms:\n@@ -2201,6 +2233,8 @@ def _run_scoped_lowering_rule(\n         input_ref = alloc_stack.enter_context(\n             ctx.module_ctx.alloc_tmem(\n                 jax.ShapeDtypeStruct(shape=aval.shape, dtype=aval.dtype),\n+                packed=aval.packed,\n+                exact_cols=False,\n             )\n         )\n         input_refs.append(input_ref)\ndiff --git a/jax/_src/pallas/mosaic_gpu/primitives.py b/jax/_src/pallas/mosaic_gpu/primitives.py\nindex 8d0e0c82671d..1081f052f4c1 100644\n--- a/jax/_src/pallas/mosaic_gpu/primitives.py\n+++ b/jax/_src/pallas/mosaic_gpu/primitives.py\n@@ -1177,8 +1177,9 @@ def _tcgen05_mma_abstract_eval(acc, a, b, barrier, accumulate,\n \n   if acc.memory_space != gpu_core.GPUMemorySpace.TMEM:\n     raise ValueError(\"Accumulator must be a TMEM Ref.\")\n-  if a.memory_space != gpu_core.GPUMemorySpace.SMEM:\n-    raise ValueError(\"LHS must be an SMEM Ref. TMEM not yet supported.\")\n+  if a.memory_space not in (gpu_core.GPUMemorySpace.SMEM,\n+                            gpu_core.GPUMemorySpace.TMEM):\n+    raise ValueError(\"LHS must be a TMEM/SMEM Ref.\")\n   if b.memory_space != gpu_core.GPUMemorySpace.SMEM:\n     raise ValueError(\"RHS must be an SMEM Ref.\")\n \n@@ -1287,6 +1288,27 @@ def _tcgen05_mma_lowering(\n                           ctx=ctx.launch_ctx)\n   return []\n \n+\n+commit_tmem_p = jax_core.Primitive(\"commit_tmem\")\n+commit_tmem_p.multiple_results = True\n+\n+\n+@commit_tmem_p.def_effectful_abstract_eval\n+def _commit_tmem_abstract_eval():\n+  return (), {gpu_core._memory_effect}\n+\n+\n+@lowering.register_lowering_rule(commit_tmem_p, mgpu.LoweringSemantics.Lane)\n+def _commit_tmem_lowering(_):\n+  tcgen05.commit_tmem()\n+  return ()\n+\n+\n+def commit_tmem():\n+  \"\"\"Commits all writes to TMEM, making them visible to loads and MMA.\"\"\"\n+  commit_tmem_p.bind()\n+\n+\n class Layout(enum.Enum):\n   #: [m, n] matrix, where m % 64 == 0 == n % 8.\n   WGMMA = enum.auto()\n@@ -1299,6 +1321,8 @@ class Layout(enum.Enum):\n   WG_SPLAT = enum.auto()\n   WG_STRIDED = enum.auto()\n \n+  TCGEN05 = enum.auto()\n+\n   def __call__(self, *args, **kwargs) -> ParameterizedLayout:\n     return ParameterizedLayout(self, args, kwargs)\n \n@@ -1324,6 +1348,9 @@ def check_no_args():\n         return mgpu.WGSplatFragLayout(*args, **kwargs)  # pytype: disable=missing-parameter\n       case Layout.WG_STRIDED:\n         return mgpu.WGStridedFragLayout(*args, **kwargs)  # pytype: disable=missing-parameter\n+      case Layout.TCGEN05:\n+        check_no_args()\n+        return mgpu.TCGEN05_LAYOUT\n \n @dataclasses.dataclass(frozen=True)\n class ParameterizedLayout:\ndiff --git a/jax/experimental/mosaic/gpu/__init__.py b/jax/experimental/mosaic/gpu/__init__.py\nindex 074890d1816d..82155f86d9ea 100644\n--- a/jax/experimental/mosaic/gpu/__init__.py\n+++ b/jax/experimental/mosaic/gpu/__init__.py\n@@ -101,3 +101,7 @@\n     WGMMAAccumulator as WGMMAAccumulator,\n     wgmma as wgmma,\n )\n+\n+from .tcgen05 import (\n+  LAYOUT as TCGEN05_LAYOUT,  # noqa: F401\n+)\ndiff --git a/jax/experimental/pallas/mosaic_gpu.py b/jax/experimental/pallas/mosaic_gpu.py\nindex dd1bd3aba4bd..7b300b8cfbfa 100644\n--- a/jax/experimental/pallas/mosaic_gpu.py\n+++ b/jax/experimental/pallas/mosaic_gpu.py\n@@ -58,6 +58,7 @@\n from jax._src.pallas.mosaic_gpu.primitives import wgmma as wgmma\n from jax._src.pallas.mosaic_gpu.primitives import wgmma_wait as wgmma_wait\n from jax._src.pallas.mosaic_gpu.primitives import tcgen05_mma as tcgen05_mma\n+from jax._src.pallas.mosaic_gpu.primitives import commit_tmem as commit_tmem\n from jax.experimental.mosaic.gpu.core import LoweringSemantics as LoweringSemantics\n \n \ndiff --git a/tests/pallas/mosaic_gpu_test.py b/tests/pallas/mosaic_gpu_test.py\nindex dcafa9b7277e..ed45c3b9f1e6 100644\n--- a/tests/pallas/mosaic_gpu_test.py\n+++ b/tests/pallas/mosaic_gpu_test.py\n@@ -1707,6 +1707,7 @@ def test_missing_primitive_lowerings_are_tracked(self):\n         mgpu_primitives.broadcasted_iota_p,\n         mgpu_primitives.load_p,\n         mgpu_primitives.tcgen05_mma_p,\n+        mgpu_primitives.commit_tmem_p,\n         lax.slice_p,\n         pallas_core.core_map_p,\n         pallas_primitives.semaphore_signal_p,\n@@ -2092,38 +2093,56 @@ class PallasCallSm90AWGTest(\n \n class PallasCallSm100ATest(PallasSm100ATest):\n \n-  def test_tmem_alloc(self):\n+  def test_tmem(self):\n     self.skip_if_wg_semantics()  # TMEM read not wired up in the WG get rule.\n-\n+    swizzle_elems = 128 // jnp.dtype(jnp.float32).itemsize\n+    transforms = (\n+        plgpu.TilingTransform((8, swizzle_elems)),\n+        plgpu.SwizzleTransform(128),\n+    )\n     @functools.partial(\n         self.kernel,\n         out_shape=jnp.zeros((128, 128), jnp.float32),\n         scratch_shapes=[\n             plgpu.TMEM((128, 128), jnp.float32),\n-            plgpu.SMEM((128, 128), jnp.float32),\n+            plgpu.TMEM((128, 128), jnp.float32),\n+            plgpu.SMEM((128, 128), jnp.float32, transforms=transforms),\n+            plgpu.Barrier(num_arrivals=1),\n         ],\n         num_threads=1,\n         thread_name=\"x\",\n     )\n-    def kernel(y_ref, tmem_ref, smem_ref):\n-      # Issue a write so the TMEM load is not DCE'd.\n-      smem_ref[...] = tmem_ref[...]\n+    def kernel(x_ref, y_ref, tmem_ref, tmem_ref2, smem_ref, barrier_ref):\n+      plgpu.copy_gmem_to_smem(x_ref, smem_ref, barrier_ref)\n+      plgpu.barrier_wait(barrier_ref)\n+      # Exercise TMEM by roundtripping SMEM -> TMEM -> TMEM -> SMEM.\n+      x_val = plgpu.load(smem_ref, (), layout=plgpu.Layout.TCGEN05)\n+      tmem_ref[...] = x_val + 1\n+      plgpu.commit_tmem()\n+      tmem_ref2[...] = tmem_ref[...]\n+      plgpu.commit_tmem()\n+      smem_ref[...] = tmem_ref2[...]\n       plgpu.commit_smem()\n       plgpu.copy_smem_to_gmem(smem_ref, y_ref)\n       plgpu.wait_smem_to_gmem(0)\n \n-    # Test that this runs without errors.\n-    jax.block_until_ready(kernel())\n+    x = jax.random.uniform(\n+        jax.random.key(0), shape=(128, 128), dtype=jnp.float32)\n+    x_result = jax.block_until_ready(kernel(x))\n+    np.testing.assert_array_equal(x_result, x + 1)\n \n   @parameterized.parameters(\n-      ((128, 128), 128, jnp.float16),\n+      ((128, 128), 128, jnp.float16, False),\n+      # Test LHS in TMEM.\n+      ((128, 128), 128, jnp.float16, True),\n       # Test bfloat16\n-      ((128, 128), 128, jnp.bfloat16),\n+      ((128, 128), 128, jnp.bfloat16, False),\n       # Test additional swizzles.\n-      ((128, 128), 64, jnp.float16),\n-      ((128, 128), 32, jnp.float16),\n+      ((128, 128), 64, jnp.float16, False),\n+      ((128, 128), 32, jnp.float16, False),\n   )\n-  def test_simple_matmul(self, shape, swizzle, dtype):\n+  def test_simple_matmul(self, shape, swizzle, dtype, lhs_tmem=False):\n+    self.skip_if_wg_semantics()\n     # Test a matmul with a single block.\n     swizzle_elems = swizzle // jnp.dtype(dtype).itemsize\n     transforms = (\n@@ -2131,9 +2150,16 @@ def test_simple_matmul(self, shape, swizzle, dtype):\n         plgpu.SwizzleTransform(swizzle),\n     )\n \n-    def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref):\n+    def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref,\n+               a_tmem_ref):\n+      if lhs_tmem:\n+        lhs_ref = a_tmem_ref\n+        lhs_ref[...] = plgpu.load(a_smem, (), layout=plgpu.Layout.TCGEN05)\n+        plgpu.commit_tmem()\n+      else:\n+        lhs_ref = a_smem\n       plgpu.tcgen05_mma(acc_tmem,\n-                        a_smem,\n+                        lhs_ref,\n                         b_smem,\n                         barrier_ref,\n                         accumulate=False)\n@@ -2144,10 +2170,15 @@ def kernel(a_smem, b_smem, out_ref, acc_tmem, scratch_smem, barrier_ref):\n       plgpu.wait_smem_to_gmem(0)\n \n     scratch_shapes = [\n-        plgpu.TMEM(shape, jnp.float32),\n+        plgpu.TMEM(shape, jnp.float32, packed=False),\n         plgpu.SMEM(shape, dtype, transforms=transforms),\n         plgpu.Barrier(num_arrivals=1, for_tensor_core=True),\n     ]\n+    if lhs_tmem:\n+      scratch_shapes.append(plgpu.TMEM(shape, dtype, packed=True))\n+    else:\n+      scratch_shapes.append(None)\n+\n     f = self.pallas_call(\n         kernel,\n         in_specs=(\n\n```"
        ],
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "jnp.linalg.matrix_power: support non-float inputs",
        "output": [
            "```diff\nCommit: 1e1f1e0d7aaff0c48e1dce0eff56837848848fa9\nDate: 2025-05-08T15:17:03Z\nURL: https://github.com/jax-ml/jax/commit/1e1f1e0d7aaff0c48e1dce0eff56837848848fa9\nFiles changed: 2\nAdditions: +8, Deletions: -2\ndiff --git a/jax/_src/numpy/linalg.py b/jax/_src/numpy/linalg.py\nindex 146bbbda0213..0e20e5b2a416 100644\n--- a/jax/_src/numpy/linalg.py\n+++ b/jax/_src/numpy/linalg.py\n@@ -367,8 +367,7 @@ def matrix_power(a: ArrayLike, n: int) -> Array:\n     Array([[ 5.5 , -2.5 ],\n            [-3.75,  1.75]], dtype=float32)\n   \"\"\"\n-  a = ensure_arraylike(\"jnp.linalg.matrix_power\", a)\n-  arr, = promote_dtypes_inexact(a)\n+  arr = ensure_arraylike(\"jnp.linalg.matrix_power\", a)\n \n   if arr.ndim < 2:\n     raise TypeError(\"{}-dimensional array given. Array must be at least \"\ndiff --git a/tests/linalg_test.py b/tests/linalg_test.py\nindex 74259c300cf7..033ca989c8e7 100644\n--- a/tests/linalg_test.py\n+++ b/tests/linalg_test.py\n@@ -1230,6 +1230,13 @@ def testMatrixPower(self, shape, dtype, n):\n     self._CompileAndCheck(partial(jnp.linalg.matrix_power, n=n), args_maker,\n                           rtol=1e-3)\n \n+  def testMatrixPowerBool(self):\n+    # Regression test for https://github.com/jax-ml/jax/issues/28603\n+    mat = np.array([[True,True], [False,True]])\n+    np_result = np.linalg.matrix_power(mat, 2)\n+    jnp_result = jnp.linalg.matrix_power(mat, 2)\n+    self.assertArraysEqual(np_result, jnp_result)\n+\n   @jtu.sample_product(\n     shape=[(3, ), (1, 2), (8, 5), (4, 4), (5, 5), (50, 50), (3, 4, 5),\n            (2, 3, 4, 5)],\n\n```"
        ],
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "jnp.put: check inplace before other conditions",
        "output": [
            "```diff\nCommit: 4013965d8165fff028ccb144271116216003f289\nDate: 2025-05-08T15:08:52Z\nURL: https://github.com/jax-ml/jax/commit/4013965d8165fff028ccb144271116216003f289\nFiles changed: 1\nAdditions: +4, Deletions: -4\ndiff --git a/jax/_src/numpy/indexing.py b/jax/_src/numpy/indexing.py\nindex 17fbccd7ac9d..6aa5d6b87ef4 100644\n--- a/jax/_src/numpy/indexing.py\n+++ b/jax/_src/numpy/indexing.py\n@@ -1271,16 +1271,16 @@ def put(a: ArrayLike, ind: ArrayLike, v: ArrayLike,\n            [ 0,  0, 20,  0,  0],\n            [ 0,  0,  0,  0, 30]], dtype=int32)\n   \"\"\"\n+  if inplace:\n+    raise ValueError(\n+      \"jax.numpy.put cannot modify arrays in-place, because JAX arrays are immutable. \"\n+      \"Pass inplace=False to instead return an updated array.\")\n   arr, ind_arr, _ = util.ensure_arraylike(\"put\", a, ind, v)\n   ind_arr = ind_arr.ravel()\n   v_arr = lax_numpy.ravel(v)\n   if not arr.size or not ind_arr.size or not v_arr.size:\n     return arr\n   v_arr = lax_numpy._tile_to_size(v_arr, len(ind_arr))\n-  if inplace:\n-    raise ValueError(\n-      \"jax.numpy.put cannot modify arrays in-place, because JAX arrays are immutable. \"\n-      \"Pass inplace=False to instead return an updated array.\")\n   if mode is None:\n     scatter_mode = \"drop\"\n   elif mode == \"clip\":\n\n```"
        ],
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "Record event start time in `dispatch.LogElapsedTimeContextManager`.\n\nWe want to collect timings for various stages of the jax compilation process, but `log_compiles` logs everything. It'd be nice to filter for top-level functions (or some top-n level).\n\nWe pass fun_name to the event callbacks and that's necessary for labeling, but a) not necessarily unique and b) need to know ahead of time to filter.\n\nWith just one event for start/end time, we can solve by recording metrics the entire run and do some post-processing at the end. Having a start event just makes this a little easier (in a non-free-threading world, anyway) since we can process\nas we go / throw out events nested beyond the level(s) we care about.\n\nPiperOrigin-RevId: 756297365",
        "output": [
            "```diff\nCommit: 5c77f249b575481d234ae4c9a8afa733d2457ce3\nDate: 2025-05-08T14:10:36Z\nURL: https://github.com/jax-ml/jax/commit/5c77f249b575481d234ae4c9a8afa733d2457ce3\nFiles changed: 2\nAdditions: +7, Deletions: -3\ndiff --git a/jax/_src/dispatch.py b/jax/_src/dispatch.py\nindex ebab2120c4d0..409b22c849e3 100644\n--- a/jax/_src/dispatch.py\n+++ b/jax/_src/dispatch.py\n@@ -46,7 +46,7 @@\n from jax._src.layout import DeviceLocalLayout, Layout\n from jax._src.lib import xla_client as xc\n from jax._src.mesh import AbstractMesh, Mesh\n-from jax._src.monitoring import record_event_duration_secs, record_event_time_span\n+from jax._src.monitoring import record_scalar, record_event_duration_secs, record_event_time_span\n from jax._src.partition_spec import PartitionSpec\n from jax._src.sharding import Sharding\n from jax._src.sharding_impls import (\n@@ -179,6 +179,10 @@ def __init__(self, fmt: str, fun_name: str, event: str | None = None):\n \n   def __enter__(self):\n     self.start_time = time.time()\n+    if self.event is not None:\n+      record_scalar(\n+          self.event, self.start_time, fun_name=self.fun_name\n+      )\n \n   def __exit__(self, exc_type, exc_value, traceback):\n     if _on_exit:\ndiff --git a/tests/monitoring_test.py b/tests/monitoring_test.py\nindex 5ef5c5d928ba..a50ddf6f4cc6 100644\n--- a/tests/monitoring_test.py\n+++ b/tests/monitoring_test.py\n@@ -68,10 +68,10 @@ def test_record_scalar(self):\n     observed_values = []\n \n     monitoring.register_scalar_listener(\n-        lambda key, _: observed_keys.append(key),\n+        lambda key, _, **kwargs: observed_keys.append(key),\n     )\n     monitoring.register_scalar_listener(\n-        lambda _, value: observed_values.append(value),\n+        lambda _, value, **kwargs: observed_values.append(value),\n     )\n \n     monitoring.record_scalar(\"test_unique_event\", 1)\n\n```"
        ],
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[xla::PyClient] Update PyClient to use xla::ifrt::CompileAndLoad.\n\nPiperOrigin-RevId: 756294938",
        "output": [
            "```diff\nCommit: 9f64ddd380381d511889d91e620701f82a15d24c\nDate: 2025-05-08T14:02:15Z\nURL: https://github.com/jax-ml/jax/commit/9f64ddd380381d511889d91e620701f82a15d24c\nFiles changed: 3\nAdditions: +127, Deletions: -29\ndiff --git a/jaxlib/py_client.cc b/jaxlib/py_client.cc\nindex ecd412ddbb99..0a99d94f81cc 100644\n--- a/jaxlib/py_client.cc\n+++ b/jaxlib/py_client.cc\n@@ -411,7 +411,7 @@ MakeIfrtDeserializeExecutableOptions(std::optional<CompileOptions> options,\n }  // namespace\n \n /* static */ absl::StatusOr<nb_class_ptr<PyLoadedExecutable>>\n-PyClient::CompileIfrtProgram(\n+PyClient::CompileAndLoadIfrtProgram(\n     nb_class_ptr<PyClient> client, std::unique_ptr<ifrt::Program> ifrt_program,\n     std::unique_ptr<ifrt::CompileOptions> ifrt_options) {\n   auto* pjrt_compatible_client =\n@@ -448,9 +448,10 @@ PyClient::CompileIfrtProgram(\n   std::optional<std::string> fingerprint;\n   {\n     nb::gil_scoped_release gil_release;\n-    TF_ASSIGN_OR_RETURN(ifrt_loaded_executable,\n-                        client->ifrt_client_->GetDefaultCompiler()->Compile(\n-                            std::move(ifrt_program), std::move(ifrt_options)));\n+    TF_ASSIGN_OR_RETURN(\n+        ifrt_loaded_executable,\n+        client->ifrt_client_->GetDefaultCompiler()->CompileAndLoad(\n+            std::move(ifrt_program), std::move(ifrt_options)));\n     TF_RETURN_IF_ERROR(ifrt_loaded_executable->GetReadyFuture().Await());\n     TF_ASSIGN_OR_RETURN(fingerprint, ifrt_loaded_executable->Fingerprint());\n   }\n@@ -460,10 +461,11 @@ PyClient::CompileIfrtProgram(\n       std::move(traceback), std::move(fingerprint));\n }\n \n-/* static */ absl::StatusOr<nb_class_ptr<PyLoadedExecutable>> PyClient::Compile(\n-    nb_class_ptr<PyClient> client, std::string mlir_module,\n-    ifrt::DeviceListRef executable_devices, CompileOptions options,\n-    std::vector<nb::capsule> host_callbacks) {\n+/* static */ absl::StatusOr<nb_class_ptr<PyLoadedExecutable>>\n+PyClient::CompileAndLoad(nb_class_ptr<PyClient> client, std::string mlir_module,\n+                         ifrt::DeviceListRef executable_devices,\n+                         CompileOptions options,\n+                         std::vector<nb::capsule> host_callbacks) {\n   mlir::MLIRContext context;\n   TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n                       ParseMlirModuleString(mlir_module, context));\n@@ -472,16 +474,17 @@ PyClient::CompileIfrtProgram(\n     // export it before going to HLO while preserving Shardy ops and attrs.\n     TF_RETURN_IF_ERROR(ExportShardyForHloRoundTrip(*module));\n   }\n-  return CompileIfrtProgram(\n+  return CompileAndLoadIfrtProgram(\n       client, std::make_unique<xla::ifrt::HloProgram>(module.get()),\n       MakeIfrtCompileOptions(std::move(options), std::move(executable_devices),\n                              std::move(host_callbacks)));\n }\n \n-/* static */ absl::StatusOr<nb_class_ptr<PyLoadedExecutable>> PyClient::Compile(\n-    nb_class_ptr<PyClient> client, std::string mlir_module,\n-    ifrt::DeviceListRef executable_devices, CompileOptions options,\n-    std::vector<nb::callable> host_callbacks) {\n+/* static */ absl::StatusOr<nb_class_ptr<PyLoadedExecutable>>\n+PyClient::CompileAndLoad(nb_class_ptr<PyClient> client, std::string mlir_module,\n+                         ifrt::DeviceListRef executable_devices,\n+                         CompileOptions options,\n+                         std::vector<nb::callable> host_callbacks) {\n   mlir::MLIRContext context;\n   TF_ASSIGN_OR_RETURN(mlir::OwningOpRef<mlir::ModuleOp> module,\n                       ParseMlirModuleString(mlir_module, context));\n@@ -509,7 +512,7 @@ PyClient::CompileIfrtProgram(\n   auto compile_options = std::make_unique<ifrt::XlaCompileOptions>(\n       std::move(options), std::move(ifrt_loaded_host_callbacks));\n #endif\n-  return CompileIfrtProgram(\n+  return CompileAndLoadIfrtProgram(\n       client, std::make_unique<xla::ifrt::HloProgram>(module.get()),\n       std::move(compile_options));\n }\n@@ -761,7 +764,7 @@ PyType_Slot PyClient::slots_[] = {\n              std::vector<nb::capsule> host_callbacks) {\n             ifrt::DeviceListRef executable_devices =\n                 ValueOrThrow(py_executable_devices.ifrt_device_list());\n-            return ValueOrThrow(PyClient::Compile(\n+            return ValueOrThrow(PyClient::CompileAndLoad(\n                 std::move(client),\n                 std::string(mlir_module.c_str(), mlir_module.size()),\n                 std::move(executable_devices), std::move(options),\n@@ -777,7 +780,7 @@ PyType_Slot PyClient::slots_[] = {\n              std::vector<nb::callable> host_callbacks) {\n             ifrt::DeviceListRef executable_devices =\n                 ValueOrThrow(py_executable_devices.ifrt_device_list());\n-            return ValueOrThrow(PyClient::Compile(\n+            return ValueOrThrow(PyClient::CompileAndLoad(\n                 std::move(client),\n                 std::string(mlir_module.c_str(), mlir_module.size()),\n                 std::move(executable_devices), std::move(options),\n@@ -793,7 +796,7 @@ PyType_Slot PyClient::slots_[] = {\n              std::vector<nb::capsule> host_callbacks) {\n             ifrt::DeviceListRef executable_devices =\n                 ValueOrThrow(py_executable_devices.ifrt_device_list());\n-            return ValueOrThrow(PyClient::Compile(\n+            return ValueOrThrow(PyClient::CompileAndLoad(\n                 std::move(client), std::move(mlir_module),\n                 std::move(executable_devices), std::move(options),\n                 std::move(host_callbacks)));\n@@ -808,7 +811,7 @@ PyType_Slot PyClient::slots_[] = {\n              std::vector<nb::callable> host_callbacks) {\n             ifrt::DeviceListRef executable_devices =\n                 ValueOrThrow(py_executable_devices.ifrt_device_list());\n-            return ValueOrThrow(PyClient::Compile(\n+            return ValueOrThrow(PyClient::CompileAndLoad(\n                 std::move(client), std::move(mlir_module),\n                 std::move(executable_devices), std::move(options),\n                 std::move(host_callbacks)));\n@@ -825,7 +828,7 @@ PyType_Slot PyClient::slots_[] = {\n             ifrt::DeviceListRef executable_devices =\n                 ValueOrThrow(jax::PyDeviceList(nb::tuple(py_executable_devices))\n                                  .ifrt_device_list());\n-            return ValueOrThrow(PyClient::Compile(\n+            return ValueOrThrow(PyClient::CompileAndLoad(\n                 std::move(client),\n                 std::string(mlir_module.c_str(), mlir_module.size()),\n                 std::move(executable_devices), std::move(options),\n@@ -840,7 +843,100 @@ PyType_Slot PyClient::slots_[] = {\n             ifrt::DeviceListRef executable_devices =\n                 ValueOrThrow(jax::PyDeviceList(nb::tuple(py_executable_devices))\n                                  .ifrt_device_list());\n-            return ValueOrThrow(PyClient::Compile(\n+            return ValueOrThrow(PyClient::CompileAndLoad(\n+                std::move(client), std::move(mlir_module),\n+                std::move(executable_devices), std::move(options),\n+                std::vector<nb::capsule>()));\n+          },\n+          nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n+          nb::arg(\"compile_options\") = CompileOptions())\n+      .def(\n+          \"compile_and_load\",\n+          [](nb_class_ptr<PyClient> client, nb::bytes mlir_module,\n+             jax::PyDeviceList& py_executable_devices, CompileOptions options,\n+             std::vector<nb::capsule> host_callbacks) {\n+            ifrt::DeviceListRef executable_devices =\n+                ValueOrThrow(py_executable_devices.ifrt_device_list());\n+            return ValueOrThrow(PyClient::CompileAndLoad(\n+                std::move(client),\n+                std::string(mlir_module.c_str(), mlir_module.size()),\n+                std::move(executable_devices), std::move(options),\n+                std::move(host_callbacks)));\n+          },\n+          nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n+          nb::arg(\"compile_options\") = CompileOptions(),\n+          nb::arg(\"host_callbacks\") = std::vector<nb::capsule>())\n+      .def(\n+          \"compile_and_load\",\n+          [](nb_class_ptr<PyClient> client, nb::bytes mlir_module,\n+             jax::PyDeviceList& py_executable_devices, CompileOptions options,\n+             std::vector<nb::callable> host_callbacks) {\n+            ifrt::DeviceListRef executable_devices =\n+                ValueOrThrow(py_executable_devices.ifrt_device_list());\n+            return ValueOrThrow(PyClient::CompileAndLoad(\n+                std::move(client),\n+                std::string(mlir_module.c_str(), mlir_module.size()),\n+                std::move(executable_devices), std::move(options),\n+                std::move(host_callbacks)));\n+          },\n+          nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n+          nb::arg(\"compile_options\") = CompileOptions(),\n+          nb::arg(\"host_callbacks\") = std::vector<nb::callable>())\n+      .def(\n+          \"compile_and_load\",\n+          [](nb_class_ptr<PyClient> client, std::string mlir_module,\n+             jax::PyDeviceList& py_executable_devices, CompileOptions options,\n+             std::vector<nb::capsule> host_callbacks) {\n+            ifrt::DeviceListRef executable_devices =\n+                ValueOrThrow(py_executable_devices.ifrt_device_list());\n+            return ValueOrThrow(PyClient::CompileAndLoad(\n+                std::move(client), std::move(mlir_module),\n+                std::move(executable_devices), std::move(options),\n+                std::move(host_callbacks)));\n+          },\n+          nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n+          nb::arg(\"compile_options\") = CompileOptions(),\n+          nb::arg(\"host_callbacks\") = std::vector<nb::capsule>())\n+      .def(\n+          \"compile_and_load\",\n+          [](nb_class_ptr<PyClient> client, std::string mlir_module,\n+             jax::PyDeviceList& py_executable_devices, CompileOptions options,\n+             std::vector<nb::callable> host_callbacks) {\n+            ifrt::DeviceListRef executable_devices =\n+                ValueOrThrow(py_executable_devices.ifrt_device_list());\n+            return ValueOrThrow(PyClient::CompileAndLoad(\n+                std::move(client), std::move(mlir_module),\n+                std::move(executable_devices), std::move(options),\n+                std::move(host_callbacks)));\n+          },\n+          nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n+          nb::arg(\"compile_options\") = CompileOptions(),\n+          nb::arg(\"host_callbacks\") = std::vector<nb::callable>())\n+      // The following two overloads are for users of deprecated APIs who call\n+      // `backend.compile` but do not have visibility to `DeviceList`.\n+      .def(\n+          \"compile_and_load\",\n+          [](nb_class_ptr<PyClient> client, nb::bytes mlir_module,\n+             nb::sequence& py_executable_devices, CompileOptions options) {\n+            ifrt::DeviceListRef executable_devices =\n+                ValueOrThrow(jax::PyDeviceList(nb::tuple(py_executable_devices))\n+                                 .ifrt_device_list());\n+            return ValueOrThrow(PyClient::CompileAndLoad(\n+                std::move(client),\n+                std::string(mlir_module.c_str(), mlir_module.size()),\n+                std::move(executable_devices), std::move(options),\n+                std::vector<nb::capsule>()));\n+          },\n+          nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n+          nb::arg(\"compile_options\") = CompileOptions())\n+      .def(\n+          \"compile_and_load\",\n+          [](nb_class_ptr<PyClient> client, std::string mlir_module,\n+             nb::sequence& py_executable_devices, CompileOptions options) {\n+            ifrt::DeviceListRef executable_devices =\n+                ValueOrThrow(jax::PyDeviceList(nb::tuple(py_executable_devices))\n+                                 .ifrt_device_list());\n+            return ValueOrThrow(PyClient::CompileAndLoad(\n                 std::move(client), std::move(mlir_module),\n                 std::move(executable_devices), std::move(options),\n                 std::vector<nb::capsule>()));\n@@ -848,7 +944,9 @@ PyType_Slot PyClient::slots_[] = {\n           nb::arg(\"computation\"), nb::arg(\"executable_devices\"),\n           nb::arg(\"compile_options\") = CompileOptions())\n       .def(\"compile_ifrt_program\",\n-           xla::ValueOrThrowWrapper(PyClient::CompileIfrtProgram))\n+           xla::ValueOrThrowWrapper(PyClient::CompileAndLoadIfrtProgram))\n+      .def(\"compile_and_load_ifrt_program\",\n+           xla::ValueOrThrowWrapper(PyClient::CompileAndLoadIfrtProgram))\n       .def(\"serialize_executable\",\n            xla::ValueOrThrowWrapper(&PyClient::SerializeExecutable))\n       .def(\ndiff --git a/jaxlib/py_client.h b/jaxlib/py_client.h\nindex 50529fac5c7e..7f70fa4f111b 100644\n--- a/jaxlib/py_client.h\n+++ b/jaxlib/py_client.h\n@@ -162,17 +162,17 @@ class PyClient {\n       ifrt::Device* device, bool force_copy,\n       ifrt::Client::HostBufferSemantics host_buffer_semantics);\n \n-  static absl::StatusOr<nb_class_ptr<PyLoadedExecutable>> CompileIfrtProgram(\n-      nb_class_ptr<PyClient> client,\n-      std::unique_ptr<ifrt::Program> ifrt_program,\n-      std::unique_ptr<ifrt::CompileOptions> ifrt_options);\n+  static absl::StatusOr<nb_class_ptr<PyLoadedExecutable>>\n+  CompileAndLoadIfrtProgram(nb_class_ptr<PyClient> client,\n+                            std::unique_ptr<ifrt::Program> ifrt_program,\n+                            std::unique_ptr<ifrt::CompileOptions> ifrt_options);\n \n-  static absl::StatusOr<nb_class_ptr<PyLoadedExecutable>> Compile(\n+  static absl::StatusOr<nb_class_ptr<PyLoadedExecutable>> CompileAndLoad(\n       nb_class_ptr<PyClient> client, std::string mlir_module,\n       ifrt::DeviceListRef executable_devices, CompileOptions options,\n       std::vector<nanobind::capsule> host_callbacks);\n \n-  static absl::StatusOr<nb_class_ptr<PyLoadedExecutable>> Compile(\n+  static absl::StatusOr<nb_class_ptr<PyLoadedExecutable>> CompileAndLoad(\n       nb_class_ptr<PyClient> client, std::string mlir_module,\n       ifrt::DeviceListRef executable_devices, CompileOptions options,\n       std::vector<nanobind::callable> host_callbacks);\n@@ -193,7 +193,7 @@ class PyClient {\n   // program through `send_channel_ids` and the results correspond to Recv ops\n   // through `recv_channel_ids`. It returns the host callback as an opaque\n   // object whose reference will keep the Python callback alive. The host\n-  // callback can be passed to `PyClient::Compile` or\n+  // callback can be passed to `PyClient::CompileAndLoad` or\n   // `PyClient::DeserializeExecutable`. The corresponding Send/Recv ops in the\n   // XLA computation can trigger the execution of this host callback.\n   // `serializer` is a function that takes `callable` as an argument and returns\ndiff --git a/jaxlib/xla_client.py b/jaxlib/xla_client.py\nindex a77a8226d944..725d05a2dace 100644\n--- a/jaxlib/xla_client.py\n+++ b/jaxlib/xla_client.py\n@@ -43,7 +43,7 @@\n \n # Just an internal arbitrary increasing number to help with backward-compatible\n # changes. In JAX, reference this via jax._src.lib.jaxlib_extension_version.\n-_version = 337\n+_version = 338\n \n # An internal increasing version number for protecting jaxlib code against\n # ifrt changes.\n\n```"
        ],
        "from_id": [
            "danielsuo",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add a pretty printing rule for custom_jvp.",
        "output": [
            "```diff\nCommit: 7e8c74985a78ff1dec528aa0f1936129dd7a31cf\nDate: 2025-05-08T13:32:54Z\nURL: https://github.com/jax-ml/jax/commit/7e8c74985a78ff1dec528aa0f1936129dd7a31cf\nFiles changed: 2\nAdditions: +42, Deletions: -0\ndiff --git a/jax/_src/custom_derivatives.py b/jax/_src/custom_derivatives.py\nindex a8f136477bd9..dc8fc90e3d1f 100644\n--- a/jax/_src/custom_derivatives.py\n+++ b/jax/_src/custom_derivatives.py\n@@ -487,6 +487,21 @@ def dce_jvp_jaxpr_thunk(*in_zeros):\n pe.dce_rules[custom_jvp_call_p] = _custom_jvp_call_dce\n \n \n+def _custom_jvp_call_pp_rule(eqn: core.JaxprEqn,\n+                             context: core.JaxprPpContext,\n+                             settings: core.JaxprPpSettings) -> core.pp.Doc:\n+  params = dict(eqn.params)\n+  if not params[\"num_consts\"]:\n+    params.pop(\"num_consts\")\n+  params[\"jvp\"] = params.pop(\"jvp_jaxpr_fun\").debug_info.func_name\n+  names = sorted(params)\n+  params[\"name\"] = params[\"call_jaxpr\"].jaxpr.debug_info.func_name\n+  return core._pp_eqn(eqn.replace(params=params), context, settings,\n+                      params=[\"name\"] + names)\n+\n+\n+core.pp_eqn_rules[custom_jvp_call_p] = _custom_jvp_call_pp_rule\n+\n ### VJPs\n \n @custom_api_util.register_custom_decorator_type\ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 610719518a03..86c0c8ceeee3 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -34,6 +34,7 @@\n import re\n import subprocess\n import sys\n+import textwrap\n import traceback\n import types\n from typing import NamedTuple\n@@ -8418,6 +8419,32 @@ def f_jvp(x, t):\n     x = jnp.arange(3.0)\n     jax.jvp(jax.vmap(jax.jit(f)), (x,), (x,))  # doesn't crash\n \n+  def test_pretty_print(self):\n+    @jax.custom_jvp\n+    def f(x):\n+      return x + 1\n+\n+    @f.defjvp\n+    def f_jvp(primals, tangents):\n+      return f(*primals), tangents[0]\n+\n+    x = jnp.array([4.2], dtype=jnp.float32)\n+    jaxpr = jax.make_jaxpr(f)(x)\n+    actual = jaxpr.pretty_print(use_color=False)\n+    expected = textwrap.dedent(\n+        \"\"\"\n+        { lambda ; a:f32[1]. let\n+            b:f32[1] = custom_jvp_call[\n+              name=f\n+              call_jaxpr={ lambda ; c:f32[1]. let d:f32[1] = add c 1.0:f32[] in (d,) }\n+              jvp=f_jvp\n+              symbolic_zeros=False\n+            ] a\n+          in (b,) }\n+        \"\"\").strip()\n+    self.assertEqual(actual, expected)\n+\n+\n \n class CustomVJPTest(jtu.JaxTestCase):\n \n\n```"
        ],
        "from_id": [
            "dfm"
        ]
    },
    {
        "text_input": "Reverts 6d1b5271a115007162e9f98561d6b118aa66382c\n\nPiperOrigin-RevId: 756139245",
        "output": [
            "```diff\nCommit: 8137c37e324c9cb5c8f991a16d78310b6e37bd05\nDate: 2025-05-08T04:42:46Z\nURL: https://github.com/jax-ml/jax/commit/8137c37e324c9cb5c8f991a16d78310b6e37bd05\nFiles changed: 2\nAdditions: +1, Deletions: -5\ndiff --git a/jax/_src/array.py b/jax/_src/array.py\nindex 422fa5086e62..f2b070c8221d 100644\n--- a/jax/_src/array.py\n+++ b/jax/_src/array.py\n@@ -636,8 +636,7 @@ def _value(self) -> np.ndarray:\n     self._check_if_deleted()\n \n     if self._npy_value is None:\n-      if (self.is_fully_replicated and\n-          self.sharding._internal_device_list.addressable_device_list):  # type: ignore\n+      if self.is_fully_replicated:\n         npy_value, did_copy = self._single_device_array_to_np_array_did_copy()\n         npy_value.flags.writeable = False\n         if did_copy:\ndiff --git a/jaxlib/py_array.cc b/jaxlib/py_array.cc\nindex 1222d410bad8..022c7a831c92 100644\n--- a/jaxlib/py_array.cc\n+++ b/jaxlib/py_array.cc\n@@ -1528,9 +1528,6 @@ int PyArray_bf_getbuffer(PyObject* exporter, Py_buffer* view, int flags) {\n     absl::Span<const std::shared_ptr<PjRtBuffer>> buffers =\n         array->pjrt_buffers();\n \n-    if (buffers.empty()) {\n-      return InvalidArgument(\"Array has no buffers.\");\n-    }\n     PjRtBuffer& buffer = *buffers.front();\n     if (!buffer.IsOnCpu()) {\n       return InvalidArgument(\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Merge pull request #28584 from jakevdp:fix-istft\n\nPiperOrigin-RevId: 756093297",
        "output": [
            "```diff\nCommit: 7a1ba3affe05d8bb616c45605bf05de21d714cb2\nDate: 2025-05-08T02:00:58Z\nURL: https://github.com/jax-ml/jax/commit/7a1ba3affe05d8bb616c45605bf05de21d714cb2\nFiles changed: 3\nAdditions: +15, Deletions: -4\ndiff --git a/jax/_src/dtypes.py b/jax/_src/dtypes.py\nindex d1e5b7bf430b..ae3516ea671c 100644\n--- a/jax/_src/dtypes.py\n+++ b/jax/_src/dtypes.py\n@@ -279,6 +279,12 @@ def to_inexact_dtype(dtype: DTypeLike) -> DType:\n   return _dtype_to_inexact.get(dtype_, dtype_)\n \n \n+def to_floating_dtype(dtype: DTypeLike) -> DType:\n+  \"\"\"Promotes a dtype to a non-complex floating dtype.\"\"\"\n+  dtype_ = np.dtype(dtype)\n+  return finfo(_dtype_to_inexact.get(dtype_, dtype_)).dtype\n+\n+\n def to_complex_dtype(dtype: DTypeLike) -> DType:\n   ftype = to_inexact_dtype(dtype)\n   if ftype in [np.dtype('float64'), np.dtype('complex128')]:\ndiff --git a/jax/_src/scipy/signal.py b/jax/_src/scipy/signal.py\nindex d950cd2ea395..565909e8a6d1 100644\n--- a/jax/_src/scipy/signal.py\n+++ b/jax/_src/scipy/signal.py\n@@ -1071,7 +1071,7 @@ def istft(Zxx: Array, fs: ArrayLike = 1.0, window: str = 'hann',\n     noverlap: Number of points to overlap between segments (default: ``nperseg // 2``).\n     nfft: Number of FFT points used in the STFT. If ``None`` (default), the\n       value is determined from the size of ``Zxx``.\n-    input_onesided: If Tru` (default), interpret the input as a one-sided STFT\n+    input_onesided: If True (default), interpret the input as a one-sided STFT\n       (positive frequencies only). If False, interpret the input as a two-sided STFT.\n     boundary: If True (default), it is assumed that the input signal was extended at\n       its boundaries by ``stft``. If `False`, the input signal is assumed to have been truncated at the boundaries by `stft`.\n@@ -1108,7 +1108,7 @@ def istft(Zxx: Array, fs: ArrayLike = 1.0, window: str = 'hann',\n     raise ValueError('Must specify differing time and frequency axes!')\n \n   Zxx = jnp.asarray(Zxx, dtype=jax.dtypes.canonicalize_dtype(\n-      np.result_type(Zxx, np.complex64)))\n+    dtypes.to_complex_dtype(Zxx.dtype)))\n \n   n_default = (2 * (Zxx.shape[freq_axis] - 1) if input_onesided\n                else Zxx.shape[freq_axis])\n@@ -1147,7 +1147,7 @@ def istft(Zxx: Array, fs: ArrayLike = 1.0, window: str = 'hann',\n   xsubs = ifunc(Zxx, axis=-2, n=nfft)[..., :nperseg_int, :]\n \n   # Get window as array\n-  if window == 'hann':\n+  if isinstance(window, str) and window == 'hann':\n     # Implement the default case without scipy\n     win = jnp.array([1.0]) if nperseg_int == 1 else jnp.sin(jnp.linspace(0, jnp.pi, nperseg_int, endpoint=False)) ** 2\n     win = win.astype(xsubs.dtype)\ndiff --git a/tests/scipy_signal_test.py b/tests/scipy_signal_test.py\nindex 11923257a9dd..7ff3c87435c7 100644\n--- a/tests/scipy_signal_test.py\n+++ b/tests/scipy_signal_test.py\n@@ -388,7 +388,7 @@ def osp_fun(x):\n     ],\n     dtype=default_dtypes,\n     fs=[1.0, 16000.0],\n-    window=['boxcar', 'triang', 'blackman', 'hamming', 'hann'],\n+    window=['boxcar', 'triang', 'blackman', 'hamming', 'hann', 'USE_ARRAY'],\n     onesided=[False, True],\n     boundary=[False, True],\n   )\n@@ -399,6 +399,11 @@ def testIstftAgainstNumpy(self, *, shape, dtype, fs, window, nperseg,\n       new_freq_len = (shape[freqaxis] - 1) * 2\n       shape = shape[:freqaxis] + (new_freq_len ,) + shape[freqaxis + 1:]\n \n+    if window == 'USE_ARRAY':\n+      # ensure dtype matches the expected dtype of `xsubs` within the implementation.\n+      window = np.ones(nperseg, dtype=(\n+        dtypes.to_floating_dtype(dtype) if onesided else dtypes.to_complex_dtype(dtype)))\n+\n     kwds = dict(fs=fs, window=window, nperseg=nperseg, noverlap=noverlap,\n                 nfft=nfft, input_onesided=onesided, boundary=boundary,\n                 time_axis=timeaxis, freq_axis=freqaxis)\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fix `with_sharding_constraint` with a scalar input\n\nPiperOrigin-RevId: 756092441",
        "output": [
            "```diff\nCommit: 4ab810b4835908d9e06a4fe55b6d37b986ed9cf1\nDate: 2025-05-08T01:55:06Z\nURL: https://github.com/jax-ml/jax/commit/4ab810b4835908d9e06a4fe55b6d37b986ed9cf1\nFiles changed: 2\nAdditions: +13, Deletions: -8\ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex f141d6c6237b..f87207e0a796 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -1578,9 +1578,8 @@ def check_aval_layout_compatibility(\n     if l is None or isinstance(l, AutoLayout):\n       continue\n     name_str = f' with pytree key path {name}' if name else ''\n-    shape = aval.shape\n     try:\n-      l.check_compatible_aval(shape)\n+      l.check_compatible_aval(aval.shape)\n     except ValueError as e:\n       raise ValueError(\n           f'One of {what_aval}{name_str} is incompatible with its layout '\n@@ -2717,7 +2716,7 @@ def with_sharding_constraint(x, shardings):\n   .. _Distributed arrays and automatic parallelization: https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html\n   \"\"\"\n   x_flat, tree = tree_flatten(x)\n-\n+  x_avals_flat = [core.shaped_abstractify(x) for x in x_flat]\n   layouts, shardings = _split_layout_and_sharding(shardings)\n \n   user_shardings = prepare_axis_resources(\n@@ -2753,13 +2752,11 @@ def with_sharding_constraint(x, shardings):\n                         for s in shardings_flat]\n \n   pjit_check_aval_sharding(\n-      shardings_flat, x_flat, (\"\",) * len(shardings_flat),\n+      shardings_flat, x_avals_flat, (\"\",) * len(shardings_flat),\n       \"with_sharding_constraint arguments\",\n       allow_uneven_sharding=True)\n-\n   check_shardings_are_auto(shardings_flat)\n-\n-  check_aval_layout_compatibility(user_layouts_flat, x_flat,\n+  check_aval_layout_compatibility(user_layouts_flat, x_avals_flat,\n                                   (\"\",) * len(user_layouts_flat),\n                                   \"with_sharding_constraint arguments\")\n \n@@ -3125,6 +3122,7 @@ def use_explicit_axes(*axes):\n \n def with_layout_constraint(x, layouts):\n   x_flat, tree = tree_flatten(x)\n+  x_avals_flat = [core.shaped_abstractify(x) for x in x_flat]\n   layouts_flat = tuple(flatten_axes(\"with_layout_constraint layouts\", tree,\n                                     layouts))\n   if any(not isinstance(l, DeviceLocalLayout) for l in layouts_flat):\n@@ -3132,7 +3130,7 @@ def with_layout_constraint(x, layouts):\n         'layouts passed to `with_layout_constraint` must be of type'\n         f' `DeviceLocalLayout`. Got {[type(l) for l in layouts_flat]}')\n   check_aval_layout_compatibility(\n-      layouts_flat, x_flat, (\"\",) * len(layouts_flat),\n+      layouts_flat, x_avals_flat, (\"\",) * len(layouts_flat),\n       \"with_layout_constraint arguments\")\n   outs = [layout_constraint_p.bind(xf, layout=l)\n           for xf, l in zip(x_flat, layouts_flat)]\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex effdc4a4ddbc..373e3ceff7a8 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -4272,6 +4272,13 @@ def make_keys(seeds):\n     else:\n       self.assertIn('unspecified_dims=[0,1,2]', lowered_text)\n \n+  def test_wsc_with_scalar(self):\n+    mesh = jtu.create_mesh((2,), 'x')\n+    s = NamedSharding(mesh, P())\n+    out = jax.lax.with_sharding_constraint(1., s)\n+    self.assertArraysEqual(out, 1.)\n+    self.assertEqual(out.sharding, s)\n+\n   def test_jit_partially_specified_shardings(self):\n \n     mesh = jtu.create_mesh((2, 2), ('x', 'y'))\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Rename `out_shardings -> out_sharding` in the `auto_axes` API and `in_shardings -> in_sharding` in `explicit_axes` API.\n\nPiperOrigin-RevId: 756069295",
        "output": [
            "```diff\nCommit: 5634ca126e776edd1bac2880b8162bd9b2cf791d\nDate: 2025-05-08T00:26:15Z\nURL: https://github.com/jax-ml/jax/commit/5634ca126e776edd1bac2880b8162bd9b2cf791d\nFiles changed: 7\nAdditions: +46, Deletions: -46\ndiff --git a/docs/notebooks/explicit-sharding.ipynb b/docs/notebooks/explicit-sharding.ipynb\nindex f37d0dcdf887..e1bee4b99fb5 100644\n--- a/docs/notebooks/explicit-sharding.ipynb\n+++ b/docs/notebooks/explicit-sharding.ipynb\n@@ -397,7 +397,7 @@\n     \"     which the split/merged axes are sharded as None then we shard the\\n\",\n     \"     resulting split/merged axes as None and the other axes according to their\\n\",\n     \"     corresponding input axis shardings. In all other cases we throw an error\\n\",\n-    \"     and require the user to provide an `out_shardings` argument.\"\n+    \"     and require the user to provide an `out_sharding` argument.\"\n    ]\n   },\n   {\n@@ -494,7 +494,7 @@\n     \"  print(f\\\"We're in auto-sharding mode here. This is the current mesh: {get_abstract_mesh()}\\\")\\n\",\n     \"  return x + y\\n\",\n     \"\\n\",\n-    \"result = add_with_out_sharding_kwarg(some_x, some_y, out_shardings=P(\\\"X\\\", None))\\n\",\n+    \"result = add_with_out_sharding_kwarg(some_x, some_y, out_sharding=P(\\\"X\\\", None))\\n\",\n     \"print(f\\\"Result type: {jax.typeof(result)}\\\")\"\n    ]\n   },\n@@ -637,7 +637,7 @@\n     \"  x = jnp.sin(arr1)\\n\",\n     \"  print(f'x.sharding: {jax.typeof(x)}', end='\\\\n\\\\n')\\n\",\n     \"\\n\",\n-    \"  z = g(x, out_shardings=P(\\\"X\\\", \\\"Y\\\"))\\n\",\n+    \"  z = g(x, out_sharding=P(\\\"X\\\", \\\"Y\\\"))\\n\",\n     \"\\n\",\n     \"  print(f'z.sharding: {jax.typeof(z)}', end=\\\"\\\\n\\\\n\\\")\\n\",\n     \"  return z + 1\\n\",\n@@ -681,7 +681,7 @@\n     \"  print(f'mesh inside f: {get_abstract_mesh()}', end='\\\\n\\\\n')\\n\",\n     \"  x = jnp.sin(arr1)\\n\",\n     \"\\n\",\n-    \"  z = explicit_g(x, in_shardings=P(\\\"X\\\", \\\"Y\\\"))\\n\",\n+    \"  z = explicit_g(x, in_sharding=P(\\\"X\\\", \\\"Y\\\"))\\n\",\n     \"\\n\",\n     \"  return z + 1\\n\",\n     \"\\n\",\n@@ -778,7 +778,7 @@\n     \"  compare_shardings(x)\\n\",\n     \"  return x\\n\",\n     \"\\n\",\n-    \"check_in_auto_context(my_array, out_shardings=P(\\\"X\\\"))\"\n+    \"check_in_auto_context(my_array, out_sharding=P(\\\"X\\\"))\"\n    ]\n   },\n   {\ndiff --git a/docs/notebooks/explicit-sharding.md b/docs/notebooks/explicit-sharding.md\nindex b374b7d7a668..1402bca2415f 100644\n--- a/docs/notebooks/explicit-sharding.md\n+++ b/docs/notebooks/explicit-sharding.md\n@@ -239,7 +239,7 @@ Here are some example sharding rules:\n      which the split/merged axes are sharded as None then we shard the\n      resulting split/merged axes as None and the other axes according to their\n      corresponding input axis shardings. In all other cases we throw an error\n-     and require the user to provide an `out_shardings` argument.\n+     and require the user to provide an `out_sharding` argument.\n \n +++ {\"id\": \"jZMp6w48Xmd7\"}\n \n@@ -308,7 +308,7 @@ def add_with_out_sharding_kwarg(x, y):\n   print(f\"We're in auto-sharding mode here. This is the current mesh: {get_abstract_mesh()}\")\n   return x + y\n \n-result = add_with_out_sharding_kwarg(some_x, some_y, out_shardings=P(\"X\", None))\n+result = add_with_out_sharding_kwarg(some_x, some_y, out_sharding=P(\"X\", None))\n print(f\"Result type: {jax.typeof(result)}\")\n ```\n \n@@ -390,7 +390,7 @@ def f(arr1):\n   x = jnp.sin(arr1)\n   print(f'x.sharding: {jax.typeof(x)}', end='\\n\\n')\n \n-  z = g(x, out_shardings=P(\"X\", \"Y\"))\n+  z = g(x, out_sharding=P(\"X\", \"Y\"))\n \n   print(f'z.sharding: {jax.typeof(z)}', end=\"\\n\\n\")\n   return z + 1\n@@ -423,7 +423,7 @@ def f(arr1):\n   print(f'mesh inside f: {get_abstract_mesh()}', end='\\n\\n')\n   x = jnp.sin(arr1)\n \n-  z = explicit_g(x, in_shardings=P(\"X\", \"Y\"))\n+  z = explicit_g(x, in_sharding=P(\"X\", \"Y\"))\n \n   return z + 1\n \n@@ -469,7 +469,7 @@ def check_in_auto_context(x):\n   compare_shardings(x)\n   return x\n \n-check_in_auto_context(my_array, out_shardings=P(\"X\"))\n+check_in_auto_context(my_array, out_sharding=P(\"X\"))\n ```\n \n +++ {\"id\": \"MRFccsi5X8so\"}\ndiff --git a/jax/_src/numpy/indexing.py b/jax/_src/numpy/indexing.py\nindex 044b5175a46a..17fbccd7ac9d 100644\n--- a/jax/_src/numpy/indexing.py\n+++ b/jax/_src/numpy/indexing.py\n@@ -608,7 +608,7 @@ def _attempt_rewriting_take_via_slice(arr: Array, idx: Any, mode: str | None,\n     internal_ds = partial(lax.dynamic_slice, slice_sizes=slice_sizes,\n                           allow_negative_indices=allow_negative_indices)\n     if out_sharding is not None:\n-      arr = auto_axes(internal_ds, out_shardings=out_sharding)(arr, start_indices)\n+      arr = auto_axes(internal_ds, out_sharding=out_sharding)(arr, start_indices)\n     else:\n       arr = internal_ds(arr, start_indices)\n   if int_indices:\n@@ -646,7 +646,7 @@ def rewriting_take(arr, idx, indices_are_sorted=False, unique_indices=False,\n       indices_are_sorted=indices_are_sorted, unique_indices=unique_indices,\n       mode=mode, fill_value=fill_value)\n   if out_sharding is not None:\n-    return auto_axes(internal_gather, out_shardings=out_sharding\n+    return auto_axes(internal_gather, out_sharding=out_sharding\n                      )(arr, dynamic_idx)\n   return internal_gather(arr, dynamic_idx)\n \ndiff --git a/jax/_src/ops/scatter.py b/jax/_src/ops/scatter.py\nindex fcb3759c5cae..4db79557c3cc 100644\n--- a/jax/_src/ops/scatter.py\n+++ b/jax/_src/ops/scatter.py\n@@ -87,7 +87,7 @@ def _scatter_update(x: ArrayLike, idx: Index, y: ArrayLike, scatter_op: Callable\n       unique_indices=unique_indices, mode=mode,\n       normalize_indices=normalize_indices)\n   if out_sharding is not None:\n-    return auto_axes(internal_scatter, out_shardings=out_sharding\n+    return auto_axes(internal_scatter, out_sharding=out_sharding\n                      )(x, y, dynamic_idx)\n   return internal_scatter(x, y, dynamic_idx)\n \ndiff --git a/jax/_src/pjit.py b/jax/_src/pjit.py\nindex 268f533b971b..f141d6c6237b 100644\n--- a/jax/_src/pjit.py\n+++ b/jax/_src/pjit.py\n@@ -3067,24 +3067,24 @@ def _get_new_mesh(axes: str | tuple[str, ...] | None,\n   return mesh_to_use.update_axis_types({a: axis_type for a in axes})\n \n def auto_axes(fun, *, axes: str | tuple[str, ...] | None = None,\n-              out_shardings=None):\n+              out_sharding=None):\n   def decorator(*args, **kwargs):\n-    if out_shardings is None:\n-      if \"out_shardings\" in kwargs:\n-        _out_shardings = kwargs.pop(\"out_shardings\")\n+    if out_sharding is None:\n+      if \"out_sharding\" in kwargs:\n+        _out_sharding = kwargs.pop(\"out_sharding\")\n       else:\n-        raise TypeError(\"Missing required keyword argument: 'out_shardings'\")\n+        raise TypeError(\"Missing required keyword argument: 'out_sharding'\")\n     else:\n-      _out_shardings = out_shardings\n+      _out_sharding = out_sharding\n     new_mesh = _get_new_mesh(\n-        axes, mesh_lib.AxisType.Auto, 'auto_axes', shardings=_out_shardings,\n+        axes, mesh_lib.AxisType.Auto, 'auto_axes', shardings=_out_sharding,\n         error_on_manual_to_auto_explicit=True)\n     with mesh_lib.use_abstract_mesh(new_mesh):\n       in_specs = tree_map(lambda a: core.modify_spec_for_auto_manual(\n           core.get_aval(a).sharding.spec, new_mesh), args)\n       args = mesh_cast(args, in_specs)\n       out = fun(*args, **kwargs)\n-    return mesh_cast(out, _out_shardings)\n+    return mesh_cast(out, _out_sharding)\n   return decorator\n \n @contextlib.contextmanager\n@@ -3095,19 +3095,19 @@ def use_auto_axes(*axes):\n \n \n def explicit_axes(fun, *, axes: str | tuple[str, ...] | None = None,\n-                  in_shardings=None):\n+                  in_sharding=None):\n   def decorator(*args, **kwargs):\n-    if in_shardings is None:\n-      if \"in_shardings\" in kwargs:\n-        _in_shardings = kwargs.pop(\"in_shardings\")\n+    if in_sharding is None:\n+      if \"in_sharding\" in kwargs:\n+        _in_sharding = kwargs.pop(\"in_sharding\")\n       else:\n-        raise TypeError(\"Missing required keyword argument: 'in_shardings'\")\n+        raise TypeError(\"Missing required keyword argument: 'in_sharding'\")\n     else:\n-      _in_shardings = in_shardings\n+      _in_sharding = in_sharding\n     new_mesh = _get_new_mesh(axes, mesh_lib.AxisType.Explicit, 'explicit_axes',\n                              error_on_manual_to_auto_explicit=True)\n     with mesh_lib.use_abstract_mesh(new_mesh):\n-      args = mesh_cast(args, _in_shardings)\n+      args = mesh_cast(args, _in_sharding)\n       out = fun(*args, **kwargs)\n     out_specs = tree_map(lambda o: core.modify_spec_for_auto_manual(\n         core.get_aval(o).sharding.spec, mesh_lib.get_abstract_mesh()), out)\ndiff --git a/jax/_src/random.py b/jax/_src/random.py\nindex ef02719de146..6f139dd9665c 100644\n--- a/jax/_src/random.py\n+++ b/jax/_src/random.py\n@@ -349,12 +349,12 @@ def _check_shape(name: str, shape: Shape, *param_shapes) -> None:\n       raise ValueError(msg.format(name, shape_, shape))\n \n \n-def maybe_auto_axes(f, out_shardings, **hoist_kwargs):\n+def maybe_auto_axes(f, out_sharding, **hoist_kwargs):\n   f_ = partial(f, **hoist_kwargs)\n-  if out_shardings is None:\n+  if out_sharding is None:\n     return f_\n   else:\n-    return auto_axes(f_, out_shardings=out_shardings)\n+    return auto_axes(f_, out_sharding=out_sharding)\n \n \n def bits(key: ArrayLike,\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex 3215881a2c14..effdc4a4ddbc 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -5137,7 +5137,7 @@ def f(x, y):\n         ValueError,\n         'PartitionSpec passed to einsum cannot contain axis names that are of'\n         ' type Auto or Manual'):\n-      auto_axes(f, out_shardings=P())(arr1, arr2)\n+      auto_axes(f, out_sharding=P())(arr1, arr2)\n \n     out = jax.grad(f, argnums=(0, 1))(arr1, arr2)\n     self.assertEqual(out[0].sharding, arr1.sharding)\n@@ -6371,7 +6371,7 @@ def test_auto_mode_mix(self, mesh):\n     s = NamedSharding(mesh, P('x', 'y'))\n     arr = jax.device_put(np_inp, s)\n \n-    @partial(auto_axes, axes='x', out_shardings=P('x', None))\n+    @partial(auto_axes, axes='x', out_sharding=P('x', None))\n     def h(y):\n       self.assertEqual(y.aval.sharding.spec, P(None, 'y'))\n       z = jnp.sin(y)\n@@ -6433,7 +6433,7 @@ def test_full_user_mode(self, mesh):\n     arr = jax.device_put(np_inp, s)\n \n     # No axes specified means full visible mode.\n-    @partial(explicit_axes, in_shardings=P('x', 'y'))\n+    @partial(explicit_axes, in_sharding=P('x', 'y'))\n     def h(y):\n       self.assertEqual(y.aval.sharding.spec, P('x', 'y'))\n       z = jnp.sin(y)\n@@ -6563,7 +6563,7 @@ def test_mix_to_full_user_mode(self, mesh):\n     s = NamedSharding(mesh, P('x', 'y'))\n     arr = jax.device_put(np_inp, s)\n \n-    @partial(explicit_axes, axes='y', in_shardings=P('x', 'y'))\n+    @partial(explicit_axes, axes='y', in_sharding=P('x', 'y'))\n     def h(y):\n       self.assertEqual(y.aval.sharding.spec, P('x', 'y'))\n       z = jnp.sin(y)\n@@ -6589,7 +6589,7 @@ def test_full_auto_to_partial_user(self, mesh):\n     s = NamedSharding(mesh, P('x', 'y'))\n     arr = jax.device_put(np_inp, s)\n \n-    @partial(explicit_axes, axes='y', in_shardings=P(None, 'y'))\n+    @partial(explicit_axes, axes='y', in_sharding=P(None, 'y'))\n     def h(y):\n       self.assertEqual(y.aval.sharding.spec, P(None, 'y'))\n       z = jnp.sin(y)\n@@ -6703,7 +6703,7 @@ def test_auto_axes_top_level(self):\n     arr1 = jax.device_put(np_inp, NamedSharding(mesh, P('x', 'y')))\n     arr2 = jax.device_put(np_inp.T, NamedSharding(mesh, P('y', 'x')))\n \n-    @partial(auto_axes, out_shardings=P('x', None))\n+    @partial(auto_axes, out_sharding=P('x', None))\n     def auto_matmul(arr1, arr2):\n       return arr1 @ arr2\n \n@@ -6725,7 +6725,7 @@ def test_explicit_axes_top_level(self):\n     arr1 = jax.device_put(np_inp, NamedSharding(mesh, P('x', 'y')))\n     arr2 = jax.device_put(np_inp.T, NamedSharding(mesh, P('y', 'x')))\n \n-    @partial(explicit_axes, in_shardings=(P('x', None), P('x', None)))\n+    @partial(explicit_axes, in_sharding=(P('x', None), P('x', None)))\n     def jax_matmul(arr1, arr2):\n       out = arr1 @ arr2\n       self.assertEqual(out.aval.sharding.spec, P('x', None))\n@@ -6774,7 +6774,7 @@ def f(x):\n       self.assertEqual(a.aval.sharding.spec, P(None, None))\n       return a\n \n-    hf = auto_axes(f, axes=('x', 'y'), out_shardings=P('x', 'y'))\n+    hf = auto_axes(f, axes=('x', 'y'), out_sharding=P('x', 'y'))\n     out = hf(arr)\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('x', 'y')))\n \n@@ -6793,7 +6793,7 @@ def f(x):\n       self.assertEqual(z.aval.sharding.spec, P('x', 'y'))\n       return z\n \n-    hf = explicit_axes(f, axes=('x', 'y'), in_shardings=P('x', 'y'))\n+    hf = explicit_axes(f, axes=('x', 'y'), in_sharding=P('x', 'y'))\n     out = hf(arr)  # doesn't crash\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('x', 'y')))\n \n@@ -7171,7 +7171,7 @@ def test_wsc_pspec_use_mesh(self, sharded_inp):\n   def test_axes_api_error_manual_to_auto_explicit(self, mesh):\n     def g(x):\n       return auto_axes(lambda a: a * 2, axes=('x', 'y'),\n-                       out_shardings=P('x', 'y'))(x)\n+                       out_sharding=P('x', 'y'))(x)\n \n     with self.assertRaisesRegex(\n         NotImplementedError, \"Going from `Manual`.*to.*`Auto`.*`Explicit`\"):\n@@ -7185,7 +7185,7 @@ def f(x):\n       self.assertTrue(x.aval.sharding.mesh._are_all_axes_auto)\n       return x * 2\n \n-    out = auto_axes(f, out_shardings=P('x'))(np.arange(8))\n+    out = auto_axes(f, out_sharding=P('x'))(np.arange(8))\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('x')))\n     self.assertArraysEqual(out, np.arange(8) * 2)\n \n@@ -7268,7 +7268,7 @@ def test_auto_axes_computation_follows_data(self):\n     def f(x):\n       return x * 2\n \n-    out = auto_axes(f, out_shardings=s)(arr)\n+    out = auto_axes(f, out_sharding=s)(arr)\n     self.assertEqual(out.sharding, s)\n     self.assertArraysEqual(out, arr * 2)\n \n@@ -7322,7 +7322,7 @@ def test_auto_axes_late_bind(self, mesh):\n     def f(x):\n       return x * 2\n \n-    out = f(np.arange(8), out_shardings=P('x'))\n+    out = f(np.arange(8), out_sharding=P('x'))\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('x')))\n     self.assertArraysEqual(out, np.arange(8) * 2)\n \n@@ -7332,7 +7332,7 @@ def test_explicit_axes_late_bind(self, mesh):\n     def f(x):\n       return x * 2\n \n-    out = f(np.arange(8), in_shardings=P('x'))\n+    out = f(np.arange(8), in_sharding=P('x'))\n     self.assertEqual(out.sharding, NamedSharding(mesh, P('x')))\n     self.assertArraysEqual(out, np.arange(8) * 2)\n \n@@ -7471,7 +7471,7 @@ def test_auto_axes_no_context_mesh(self):\n     arr = jax.device_put(np_inp, s)\n \n     @partial(auto_axes, axes='x',\n-             out_shardings=NamedSharding(mesh, P('x', 'y')))\n+             out_sharding=NamedSharding(mesh, P('x', 'y')))\n     def h(y):\n       self.assertEqual(y.aval.sharding.spec, P(None, 'y'))\n       z = jnp.sin(y)\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Add basic support of `unreduced` to sharding-in-types! We cannot lower it right now, but it atleast shows up in types.\n\nThe API to specify unreduced is via `PartitionSpec`. For example: `PartitionSpec('x', 'y', None, unreduced='z')` or `PartitionSpec('x', unreduced=('y', 'z'))`.\n\nIn types/jaxpr, unreduced will show up as: `f32[8@x,2]{U:y}`\n\nBut we support unreduced only in dot_general and nary ops (add, mul, etc) as of this change: (the support will be expanded in following changes)\n\n* **dot general** only allows unreduced when contracting dims are sharded. And the unreduced axes specified by the user needs to match the sharding of the contracting dims. In all other cases, an error is raised. An example of how unreduced can be specified: `jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='y'))`\n\n* **nary ops** can propagate unreduced (add, mul, etc). If all ops aren't unreduced across the same mesh axes, an error is raised.\n\nPiperOrigin-RevId: 756063074",
        "output": [
            "```diff\nCommit: 48140bcbe7358f745e53bc3d1f87ad0465fbb9dd\nDate: 2025-05-08T00:06:44Z\nURL: https://github.com/jax-ml/jax/commit/48140bcbe7358f745e53bc3d1f87ad0465fbb9dd\nFiles changed: 8\nAdditions: +308, Deletions: -38\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex 780d2c487173..da9efdb0eacc 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -1825,11 +1825,12 @@ def get_cur_mesh_sharding(spec=None):\n   return NamedSharding(mesh_lib.get_abstract_mesh(), spec)\n \n def _make_lengths_same(sharding, ndim):\n-  if ndim > len(sharding.spec):\n-    return sharding.with_spec(sharding.spec._normalized_spec_for_aval(ndim))\n-  if ndim < len(sharding.spec):\n-    assert all(s is None for s in sharding.spec[ndim:]), (ndim, sharding.spec)\n-    return sharding.with_spec(sharding.spec[:ndim])\n+  pspec = sharding.spec\n+  if ndim > len(pspec):\n+    return sharding.with_spec(pspec._normalized_spec_for_aval(ndim))\n+  if ndim < len(pspec):\n+    assert all(s is None for s in pspec[ndim:]), (ndim, pspec)\n+    return sharding.with_spec(P(*pspec[:ndim], unreduced=pspec.unreduced))\n   assert False, \"unreachable\"\n \n # TODO(yashkatariya): Only works with User/Auto. Generalize it to work with\n@@ -1841,11 +1842,11 @@ def modify_spec_for_auto_manual(spec, mesh) -> P:\n       new_spec.append(s)\n     else:\n       temp_s = s[0] if isinstance(s, tuple) else s\n-      new_spec.append(\n-          None\n-          if mesh._name_to_type[temp_s] in (AxisType.Auto, AxisType.Manual)\n-          else s)\n-  return P(*new_spec)\n+      new_spec.append(s if mesh._name_to_type[temp_s] == AxisType.Explicit\n+                      else None)\n+  new_unreduced = tuple(u for u in spec.unreduced\n+                        if mesh._name_to_type[u] == AxisType.Explicit)\n+  return P(*new_spec, unreduced=new_unreduced)\n \n def _maybe_modify_sharding(sharding, ndim):\n   if len(sharding.spec) == 0 or all(s is None for s in sharding.spec):\n@@ -1905,8 +1906,8 @@ def str_short_aval(shape, dtype, mesh, spec, vma,\n   dt_str = dt_str.replace('void', 'float0')\n   shapestr = _get_shape_sharding_str(shape, spec)\n   mesh_axes = f'({mesh._axis_types_dict})' if mesh_axis_types else ''\n-  vma = f\"{{{','.join(i for i in vma)}}}\" if vma else ''\n-  return f'{dt_str}[{shapestr}]{vma}{mesh_axes}'\n+  vma_ur = _vma_ur_str(vma, spec.unreduced)\n+  return f'{dt_str}[{shapestr}]{vma_ur}{mesh_axes}'\n \n def get_vma(vma, mesh):\n   if mesh.empty:\n@@ -2000,6 +2001,18 @@ def _get_shape_sharding_str(shape, spec):\n       out.append(f\"{s1}@{s2}\")\n   return ','.join(out)\n \n+def _create_str(x, prefix):\n+  x_str = f\"{','.join(i for i in x)}\"\n+  x_str = x_str if len(x) == 1 else f\"({x_str})\"\n+  return f\"{prefix}:{x_str}\"\n+\n+def _vma_ur_str(vma, unreduced):\n+  if not vma and not unreduced:\n+    return ''\n+  vma_str = _create_str(vma, 'V') if vma else ''\n+  ur_str = _create_str(unreduced, 'U') if unreduced else ''\n+  sep = ', ' if vma and unreduced else ''\n+  return f\"{{{vma_str}{sep}{ur_str}}}\"\n \n def primal_dtype_to_tangent_dtype(primal_dtype):\n   if isinstance(primal_dtype, dtypes.ExtendedDType):\ndiff --git a/jax/_src/interpreters/batching.py b/jax/_src/interpreters/batching.py\nindex 1c6e00861448..0fbe54a30672 100644\n--- a/jax/_src/interpreters/batching.py\n+++ b/jax/_src/interpreters/batching.py\n@@ -458,6 +458,8 @@ class AxisData:\n \n def get_sharding_for_vmap(axis_data, orig_sharding, axis):\n   val = axis_data.explicit_mesh_axis\n+  # TODO(yashkatariya): Preserve unreduced here using\n+  # `orig_sharding.spec.with_partitions`\n   new_spec = P(*tuple_insert(orig_sharding.spec, axis, val))\n   return NamedSharding(orig_sharding.mesh, new_spec)\n \ndiff --git a/jax/_src/lax/lax.py b/jax/_src/lax/lax.py\nindex 6c248565174c..f6b4c1be102f 100644\n--- a/jax/_src/lax/lax.py\n+++ b/jax/_src/lax/lax.py\n@@ -3954,7 +3954,7 @@ def broadcasting_sharding_rule(name, *avals):\n \n   result_specs = [None] * len(shapes[0])\n   for i, (ss, ds) in enumerate(zip(zip(*specs), zip(*shapes))):\n-    if all(s == ss[0] for s in ss[1:]):\n+    if all(ss[0] == s for s in ss[1:]):\n       # if all dimension shardings are same, the resulting dimension sharding is\n       # the same.\n       result_specs[i] = ss[0]\n@@ -3974,7 +3974,20 @@ def broadcasting_sharding_rule(name, *avals):\n             raise core.ShardingTypeError(\n                 f'{name} got incompatible shardings for broadcasting: '\n                 f'{\", \".join(map(str, map(tuple, specs)))}.')\n-  return NamedSharding(mesh, P(*result_specs))\n+\n+  unreduced = [a.sharding.spec.unreduced for a in avals if a.shape]\n+  # TODO(yashkatariya): Relax this restriction to allow\n+  # `f32[8]{R:x} * f32[8]{U:x} -> f32[8]{U:x}` for example and maybe more cases.\n+  if unreduced:\n+    if not all(unreduced[0] == u for u in unreduced[1:]):\n+      raise core.ShardingTypeError(\n+          'All arrays must be unreduced along the same mesh axes. Got'\n+          f' {\", \".join(map(str, map(tuple, unreduced)))}')\n+    result_unreduced = unreduced[0]\n+  else:\n+    result_unreduced = None\n+\n+  return NamedSharding(mesh, P(*result_specs, unreduced=result_unreduced))\n \n def naryop(result_dtype, accepted_dtypes, name, allow_extended_dtype=False,\n            require_same_dtypes=True):\n@@ -5190,11 +5203,26 @@ def _dot_general_sharding_rule(lhs, rhs, *, dimension_numbers, precision,\n         'Mesh of both lhs and rhs should match. Got lhs:'\n         f' {lhs.sharding.mesh} and rhs: {rhs.sharding.mesh}')\n \n+  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers\n+  lhs_contracting_spec = tuple(lhs.sharding.spec[i] for i in lhs_contracting)\n+  rhs_contracting_spec = tuple(rhs.sharding.spec[i] for i in rhs_contracting)\n+\n   if out_sharding is not None:\n     assert isinstance(out_sharding, NamedSharding)\n+    if out_sharding.spec.unreduced:\n+      if lhs_contracting_spec != rhs_contracting_spec:\n+        raise core.ShardingTypeError(\n+            'lhs and rhs contracting dims should be sharded identically when'\n+            ' out_sharding provided to dot_general mentions unreduced_axes.'\n+            f' Got {out_sharding=}, {lhs_contracting_spec=},'\n+            f' {rhs_contracting_spec=}')\n+      if out_sharding.spec.unreduced != lhs_contracting_spec:\n+        raise core.ShardingTypeError(\n+            \"out_sharding's unreduced axes should be equal to the contracting\"\n+            f' specs. Got unreduced axes={out_sharding.spec.unreduced} and'\n+            f' contracting spec={lhs_contracting_spec}')\n     return out_sharding\n \n-  (lhs_contracting, rhs_contracting), (lhs_batch, rhs_batch) = dimension_numbers\n   lhs_batch_spec = tuple(lhs.sharding.spec[i] for i in lhs_batch)\n   rhs_batch_spec = tuple(rhs.sharding.spec[i] for i in rhs_batch)\n   msg = (\"dot_general requires lhs batch dimensions and rhs batch dimensions \"\n@@ -5202,8 +5230,6 @@ def _dot_general_sharding_rule(lhs, rhs, *, dimension_numbers, precision,\n         f\"{rhs_batch_spec}.\")\n   _check_specs_match(lhs_batch_spec, rhs_batch_spec, msg)\n \n-  lhs_contracting_spec = tuple(lhs.sharding.spec[i] for i in lhs_contracting)\n-  rhs_contracting_spec = tuple(rhs.sharding.spec[i] for i in rhs_contracting)\n   msg = (\"dot_general requires contracting dimensions to have consistent \"\n         f\"sharding, got {lhs_contracting_spec} and {rhs_contracting_spec}.\")\n   _check_specs_match(lhs_contracting_spec, rhs_contracting_spec, msg)\ndiff --git a/jax/_src/named_sharding.py b/jax/_src/named_sharding.py\nindex d0a49b6b081c..3dfcbd29fc96 100644\n--- a/jax/_src/named_sharding.py\n+++ b/jax/_src/named_sharding.py\n@@ -25,6 +25,7 @@\n from jax._src.lib import xla_client as xc\n from jax._src.lib.mlir.dialects import sdy\n from jax._src import mesh as mesh_lib\n+from jax._src.mesh import AxisType\n from jax._src.partition_spec import PartitionSpec\n from jax._src import sharding as JSharding\n import numpy as np\n@@ -410,6 +411,7 @@ def array_mapping_to_axis_resources(array_mapping: ArrayMapping):\n def check_pspec(mesh, spec, _manual_axes=frozenset()):\n   _check_unique_resources(spec, \"NamedSharding spec\", mesh)\n   _check_mesh_resource_axis(mesh, spec)\n+  _check_mesh_unreduced(mesh, spec)\n \n class DuplicateSpecError(Exception):\n   def __init__(self, message, mesh, pspec):\n@@ -443,14 +445,13 @@ def _check_unique_resources(pspec: PartitionSpec, arg_name: str, mesh=None\n             f' for {mesh_lib.show_axes(multiple_uses)}'),\n         mesh=mesh, pspec=pspec)\n \n-\n def _check_mesh_resource_axis(mesh, pspec):\n   for p in pspec:\n     if p is PartitionSpec.UNCONSTRAINED or p is None:\n       continue\n     p = p if isinstance(p, tuple) else (p,)\n     for r in p:\n-      if r not in mesh.shape:\n+      if r not in mesh.axis_names:\n         raise ValueError(\n             f\"Resource axis: {r} of {pspec} \"\n             f\"is not found in mesh: {tuple(mesh.shape.keys())}.\")\n@@ -459,9 +460,34 @@ def _check_mesh_resource_axis(mesh, pspec):\n           'AxisTypes should be the same in a tuple subset of PartitionSpec:'\n           f' {pspec}. Got subset {p} with axis'\n           f' types: ({\", \".join(str(mesh._name_to_type[r]) for r in p)})')\n-  if (mesh_lib.AxisType.Auto not in mesh._axis_types_dict and\n+  if (AxisType.Auto not in mesh._axis_types_dict and\n       PartitionSpec.UNCONSTRAINED in pspec):\n     raise ValueError(\n         f'{pspec} cannot contain'\n         ' `P.UNCONSTRAINED` when no mesh axis_types are `Auto`. Got mesh'\n         f' axis_types: {mesh._axis_types_dict}')\n+\n+def _check_mesh_unreduced(mesh, pspec):\n+  counts = {}\n+  duplicate = False\n+  for u in pspec.unreduced:\n+    if u not in mesh.axis_names:\n+      raise ValueError(\n+          f'Unreduced axes {u} is not found in {mesh.axis_names=}. '\n+          f'Got {pspec=}')\n+    count = counts.get(u, 0)\n+    if count > 0:\n+      duplicate = True\n+    counts[u] = count + 1\n+  if duplicate:\n+    multiple_uses = [r for r, c in counts.items() if c > 1]\n+    raise ValueError(\n+        f'Unreduced axes in {pspec} has duplicate entries which is not allowed.'\n+        f' Got {mesh_lib.show_axes(multiple_uses)}')\n+\n+  for u in pspec.unreduced:\n+    if mesh._name_to_type[u] in (AxisType.Auto, AxisType.Manual):\n+      raise ValueError(\n+          'Unreduced axes can only refer to mesh axes that is of type'\n+          f' `Explicit`. Got unreduced axes: {pspec.unreduced} and'\n+          f' mesh: {mesh}')\ndiff --git a/jax/_src/numpy/einsum.py b/jax/_src/numpy/einsum.py\nindex 21333a9e7a0d..3f657082e1d4 100644\n--- a/jax/_src/numpy/einsum.py\n+++ b/jax/_src/numpy/einsum.py\n@@ -25,7 +25,7 @@\n from jax._src.lax import lax\n from jax._src.lax.lax import PrecisionLike\n from jax._src.numpy import util\n-from jax._src.sharding_impls import canonicalize_sharding, NamedSharding, PartitionSpec as P\n+from jax._src.sharding_impls import canonicalize_sharding, NamedSharding\n from jax._src.typing import Array, ArrayLike, DTypeLike\n from jax._src.util import partition_list, set_module, unzip2\n \n@@ -422,7 +422,8 @@ def _einsum(\n         \" instances. Please file a bug if this is not enough for your use case.\")\n   dtypes.check_user_dtype_supported(preferred_element_type, \"einsum\")\n   if preferred_element_type is None:\n-    preferred_element_type, output_weak_type = dtypes.result_type(*operands, return_weak_type_flag=True)\n+    preferred_element_type, output_weak_type = dtypes.result_type(\n+        *operands, return_weak_type_flag=True)\n   else:\n     output_weak_type = False\n \n@@ -557,7 +558,7 @@ def filter_singleton_dims(operand, names, other_shape, other_names):\n           spec = out_sharding.spec\n           inverse_spec = tuple(spec[result_names.index(name)] for name in names)\n           dot_general_out_sharding = NamedSharding(\n-              out_sharding.mesh, P(*inverse_spec))\n+              out_sharding.mesh, spec.with_partitions(inverse_spec))\n         else:\n           dot_general_out_sharding = out_sharding  # type: ignore\n         dimension_numbers = ((lhs_cont, rhs_cont), (lhs_batch, rhs_batch))\ndiff --git a/jax/_src/partition_spec.py b/jax/_src/partition_spec.py\nindex 158e361482b7..fcea21934bfb 100644\n--- a/jax/_src/partition_spec.py\n+++ b/jax/_src/partition_spec.py\n@@ -13,7 +13,7 @@\n # limitations under the License.\n \n from __future__ import annotations\n-from typing import TYPE_CHECKING\n+from typing import TYPE_CHECKING, Any\n \n class UnconstrainedSingleton:\n \n@@ -43,6 +43,25 @@ def _canonicalize_partition(partition):\n     return tuple(partition)\n   return partition\n \n+def _check(partitions, unreduced):\n+  us = set(unreduced)\n+  for p in partitions:\n+    p = p if isinstance(p, tuple) else (p,)\n+    for r in p:\n+      if r in us:\n+        raise ValueError(\n+            \"partitions cannot overlap with unreduced axes passed to\"\n+            f\" PartitionSpec. Got partitions: {partitions} and unreduced axes:\"\n+            f\" {unreduced}\")\n+  if None in unreduced:\n+    raise ValueError(\n+        \"unreduced cannot contain None. All elements in unreduced should refer\"\n+        \" to the mesh axes.\")\n+\n+def unpicke_pspec(partitions, unreduced):\n+  return PartitionSpec(*partitions, unreduced=unreduced)\n+\n+AxisName = Any\n \n class PartitionSpecImpl:\n   \"\"\"Tuple describing how to partition an array across a mesh of devices.\n@@ -53,20 +72,34 @@ class PartitionSpecImpl:\n   This class exists so JAX's pytree utilities can distinguish a partition\n   specifications from tuples that should be treated as pytrees.\n   \"\"\"\n-  __slots__ = (\"_partitions\",)\n+  __slots__ = (\"_partitions\", \"_unreduced\")\n   __match_args__ = (\"_partitions\",)\n \n   # A sentinel value representing a dim is unconstrained.\n   UNCONSTRAINED = _UNCONSTRAINED_PARTITION\n \n-  def __init__(self, *partitions):\n+  def __init__(self, *partitions,\n+               unreduced: tuple[AxisName, ...] | AxisName | None = None):\n     self._partitions = tuple(_canonicalize_partition(p) for p in partitions)\n+    self._unreduced = (\n+        () if unreduced is None else tuple(unreduced)\n+        if isinstance(unreduced, (list, tuple)) else (unreduced,))\n+    _check(self._partitions, self._unreduced)\n+\n+  @property\n+  def unreduced(self):\n+    return self._unreduced\n \n   def __repr__(self):\n-    return f\"PartitionSpec{self._partitions!r}\"\n+    pr = repr(self._partitions)[1:-1]\n+    if not self._unreduced:\n+      return f\"PartitionSpec({pr})\"\n+    ur_str = f\"unreduced={self._unreduced!r}\"\n+    pr = '' if not pr else f\"{pr} \" if pr.endswith(',') else f\"{pr}, \"\n+    return (f\"PartitionSpec({pr}{ur_str})\")\n \n   def __reduce__(self):\n-    return (PartitionSpec, self._partitions)\n+    return (unpicke_pspec, (self._partitions, self._unreduced))\n \n   def __getitem__(self, i):\n     return self._partitions[i]\n@@ -80,20 +113,42 @@ def __len__(self):\n   def __eq__(self, other):\n     if not isinstance(other, (PartitionSpec, tuple)):\n       return False\n-    other = tuple(_canonicalize_partition(o) for o in other)\n-    return self._partitions == other\n+    other_p = tuple(_canonicalize_partition(o) for o in other)\n+    if isinstance(other, PartitionSpec):\n+      return (self._partitions == other_p and\n+              self._unreduced == other._unreduced)\n+    else:\n+      if self._unreduced:\n+        raise TypeError(\n+            f\"other {other} cannot be of instance `tuple` when self {self} has\"\n+            \" unreduced in `__eq__` of PartitionSpec.\")\n+      return self._partitions == other_p\n \n   def __hash__(self):\n-    return hash(self._partitions)\n+    return hash((self._partitions, self._unreduced))\n \n   def __add__(self, other):\n     if not isinstance(other, (tuple, PartitionSpec)):\n-      return NotImplemented\n-    return PartitionSpec(*self, *other)\n+      raise NotImplementedError\n+    if isinstance(other, PartitionSpec):\n+      return PartitionSpec(\n+          *self, *other,\n+          unreduced=(*self._unreduced, *other._unreduced))\n+    else:\n+      if self._unreduced:\n+        raise TypeError(\n+            f\"other {other} cannot be of instance `tuple` when self {self} has\"\n+            \" unreduced in `__add__` of PartitionSpec.\")\n+      return PartitionSpec(*self, *other)\n \n   def __radd__(self, other):\n-    if not isinstance(other, (tuple, PartitionSpec)):\n-      return NotImplemented\n+    if not isinstance(other, tuple):\n+      raise NotImplementedError\n+    # other will always be a tuple.\n+    if self._unreduced:\n+      raise TypeError(\n+          f\"other {other} cannot be of instance `tuple` when self {self} has\"\n+          \" unreduced in `__radd__` of PartitionSpec.\")\n     return PartitionSpec(*other, *self)\n \n   def index(self, value):\n@@ -102,12 +157,16 @@ def index(self, value):\n   def count(self, value):\n     return self._partitions.count(_canonicalize_partition(value))\n \n+  def with_partitions(self, new_partitions):\n+    return PartitionSpec(*new_partitions, unreduced=self._unreduced)\n+\n   def _normalized_spec_for_aval(self, ndim: int) -> PartitionSpec:\n     out = [None if p is _UNCONSTRAINED_PARTITION else p\n            for p in self._partitions]\n     if len(out) < ndim:\n       out.extend([None] * (ndim - len(out)))\n-    return PartitionSpec(*out)\n+    return self.with_partitions(out)\n+\n \n if TYPE_CHECKING:\n   class PartitionSpec(PartitionSpecImpl, tuple):  # type: ignore\ndiff --git a/tests/array_test.py b/tests/array_test.py\nindex 97d66566cffa..b951f7f6b4cd 100644\n--- a/tests/array_test.py\n+++ b/tests/array_test.py\n@@ -1403,6 +1403,69 @@ def test_memory_kind_with_abstract_mesh(self):\n         ValueError, 'Got invalid memory kind'):\n       NamedSharding(abstract_mesh, P(), memory_kind='weird_device')\n \n+  def test_pspec_unreduced(self):\n+    pspec1 = P('a', 'b', None, unreduced=('c',))\n+    self.assertEqual(repr(pspec1),\n+                     \"PartitionSpec('a', 'b', None, unreduced=('c',))\")\n+\n+    pspec2 = P('a', 'b', None, unreduced=('c',))\n+    self.assertEqual(pspec1, pspec2)\n+\n+    pspec3 = P('a', 'b', None, unreduced=('d',))\n+    self.assertNotEqual(pspec1, pspec3)\n+\n+    out = P('x', unreduced=('z',)) + P('a', unreduced='b')\n+    self.assertEqual(out, P('x', 'a', unreduced=('z', 'b')))\n+\n+    pspec4 = P('x', unreduced='y')\n+    self.assertEqual(repr(pspec4),\n+                     \"PartitionSpec('x', unreduced=('y',))\")\n+\n+    pspec5 = P(None, None, unreduced='x')\n+    self.assertEqual(repr(pspec5),\n+                     \"PartitionSpec(None, None, unreduced=('x',))\")\n+\n+    pspec6 = P(None, unreduced='x')\n+    self.assertEqual(repr(pspec6), \"PartitionSpec(None, unreduced=('x',))\")\n+\n+    pspec7 = P(unreduced='x')\n+    self.assertEqual(repr(pspec7), \"PartitionSpec(unreduced=('x',))\")\n+\n+    with self.assertRaisesRegex(\n+        TypeError, 'unreduced in `__add__` of PartitionSpec'):\n+      P('x', unreduced=('z',)) + (None,) * 2\n+\n+    with self.assertRaisesRegex(\n+        TypeError, \"unreduced in `__radd__` of PartitionSpec\"):\n+      (None,) * 2 + P('x', unreduced='y')\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"partitions cannot overlap with unreduced\"):\n+      P('x', 'y', unreduced='x')\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"partitions cannot overlap with unreduced\"):\n+      P('x', None, 'y', unreduced=('z', 'y'))\n+\n+  def test_named_sharding_unreduced_error(self):\n+    mesh = jtu.create_mesh((1, 1, 1), ('x', 'y', 'z'))\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"Unreduced axes.*not found in mesh.*\"):\n+      NamedSharding(mesh, P('x', unreduced='a'))\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"Unreduced.*has duplicate entries\"):\n+      NamedSharding(mesh, P('x', unreduced=('y', 'y')))\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"Unreduced axes can only refer to mesh axes.*Explicit\"):\n+      NamedSharding(mesh, P('x', unreduced=('y', 'z')))\n+\n+    with self.assertRaisesRegex(\n+        ValueError, \"unreduced cannot contain None.*\"):\n+      NamedSharding(mesh, P('x', unreduced=('y', None)))\n+\n \n @jtu.with_config(jax_use_shardy_partitioner=True)\n class ShardyShardingTest(jtu.JaxTestCase):\ndiff --git a/tests/pjit_test.py b/tests/pjit_test.py\nindex e8d8d46455ba..3215881a2c14 100644\n--- a/tests/pjit_test.py\n+++ b/tests/pjit_test.py\n@@ -7677,8 +7677,88 @@ def f(x):\n       self.assertEqual(jax.typeof(out).sharding, jax.typeof(x).sharding)\n       return out\n \n-    f(arr)\n-    jax.jit(f)(arr)\n+    f(arr)  # doesn't crash\n+    jax.jit(f)(arr)  # doesn't crash\n+\n+  @jtu.with_explicit_mesh((2, 2), ('x', 'y'))\n+  def test_unreduced_basic(self, mesh):\n+    np_inp = np.arange(16).reshape(8, 2)\n+    x = jax.device_put(np_inp, P('x', 'y'))\n+    y = jax.device_put(np_inp.T, P('y', None))\n+    a = jax.device_put(np_inp, P('x', 'y'))\n+    b = jax.device_put(np_inp.T, P('y', None))\n+\n+    @jax.jit\n+    def f(x, y, a, b):\n+      m1 = jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='y'))\n+      self.assertEqual(m1.aval.sharding.spec, P('x', None, unreduced='y'))\n+\n+      m2 = jnp.einsum('xy,yz->xz', a, b, out_sharding=P('x', unreduced='y'))\n+      self.assertEqual(m2.aval.sharding.spec, P('x', None, unreduced='y'))\n+\n+      s = m1 + m2  # unreduced\n+      self.assertEqual(s.aval.sharding.spec, P('x', None, unreduced='y'))\n+\n+      out = reshard(s, P('x'))  # reduce\n+      self.assertEqual(out.aval.sharding.spec, P('x', None))\n+      return out\n+\n+    f.trace(x, y, a, b)  # doesn't crash\n+\n+  @jtu.with_explicit_mesh((2, 2, 1), ('x', 'y', 'z'))\n+  def test_dot_general_unreduced_error(self, mesh):\n+    np_inp = np.arange(16).reshape(8, 2)\n+    # Case 1\n+    x = jax.device_put(np_inp, P('x', 'y'))\n+    y = jax.device_put(np_inp.T, P('y', None))\n+\n+    @jax.jit\n+    def f(x, y):\n+      return jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='z'))\n+    with self.assertRaisesRegex(\n+        core.ShardingTypeError,\n+        \"unreduced axes should be equal to the contracting specs\"):\n+      f.trace(x, y)\n+\n+    # Case 2\n+    x = jax.device_put(np_inp, P('x', 'y'))\n+    y = jax.device_put(np_inp.T, P(None, None))\n+    @jax.jit\n+    def g(x, y):\n+      return jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='y'))\n+    with self.assertRaisesRegex(\n+        core.ShardingTypeError,\n+        \"lhs and rhs contracting dims should be sharded identically\"):\n+      g.trace(x, y)\n+\n+    # Case 3\n+    x = jax.device_put(np_inp, P('x', None))\n+    y = jax.device_put(np_inp.T, P(None, None))\n+\n+    @jax.jit\n+    def h(x, y):\n+      return jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='y'))\n+    with self.assertRaisesRegex(\n+        core.ShardingTypeError,\n+        \"unreduced axes should be equal to the contracting specs\"):\n+      h.trace(x, y)\n+\n+  @jtu.with_explicit_mesh((2, 2), ('x', 'y'))\n+  def test_add_unreduced_error(self, mesh):\n+    np_inp = np.arange(16).reshape(8, 2)\n+    x = jax.device_put(np_inp, P('x', 'y'))\n+    y = jax.device_put(np_inp.T, P('y', None))\n+\n+    @jax.jit\n+    def f(x, y):\n+      m1 = jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x', unreduced='y'))\n+      m2 = jnp.einsum('xy,yz->xz', x, y, out_sharding=P('x'))\n+      return m1 + m2\n+\n+    with self.assertRaisesRegex(\n+        core.ShardingTypeError,\n+        \"arrays must be unreduced along the same mesh axes\"):\n+      f.trace(x, y)\n \n \n @jtu.pytest_mark_if_available('multiaccelerator')\n\n```"
        ],
        "from_id": [
            "yashk2810",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Disable specific Triton fused_attention backwards test on A100.\n\nPiperOrigin-RevId: 756060415",
        "output": [
            "```diff\nCommit: 0a68794ebb566d898eb1de28959e91f12faa3167\nDate: 2025-05-07T23:58:01Z\nURL: https://github.com/jax-ml/jax/commit/0a68794ebb566d898eb1de28959e91f12faa3167\nFiles changed: 1\nAdditions: +11, Deletions: -0\ndiff --git a/tests/pallas/gpu_ops_test.py b/tests/pallas/gpu_ops_test.py\nindex a33760cbfa86..3c352afe3382 100644\n--- a/tests/pallas/gpu_ops_test.py\n+++ b/tests/pallas/gpu_ops_test.py\n@@ -267,6 +267,17 @@ def test_fused_attention_bwd(\n       causal,\n       use_segment_ids,\n   ):\n+    if jtu.is_cuda_compute_capability_equal(\"8.0\") and all([\n+        block_sizes[\"block_q\"] == 128,\n+        batch_size == 2,\n+        num_heads == 2,\n+        head_dim == 128,\n+        causal,\n+        not use_segment_ids\n+    ]):\n+      # TODO(b/416306534)\n+      self.skipTest(\"Precision issues after CUDA 12.8.1 upgrade\")\n+\n     k1, k2, k3 = random.split(random.key(0), 3)\n     q = random.normal(\n         k1, (batch_size, seq_len, num_heads, head_dim), dtype=jnp.float16\n\n```"
        ],
        "from_id": [
            "justinjfu",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Get rid of `HloModuleProto` from `CompiledMemoryStats`\n\n`HloModuleProto` can be obtained from `LoadedExecutable::GetHloModuleProtos()`, so it's wasteful to duplicate this information.\n\nPiperOrigin-RevId: 756053432",
        "output": [
            "```diff\nCommit: 292b468af2bc454df2906663a1972ce285201579\nDate: 2025-05-07T23:36:12Z\nURL: https://github.com/jax-ml/jax/commit/292b468af2bc454df2906663a1972ce285201579\nFiles changed: 2\nAdditions: +20, Deletions: -4\ndiff --git a/jaxlib/_jax/__init__.pyi b/jaxlib/_jax/__init__.pyi\nindex c6c8a67e4653..8c02bb4ba722 100644\n--- a/jaxlib/_jax/__init__.pyi\n+++ b/jaxlib/_jax/__init__.pyi\n@@ -328,7 +328,7 @@ class CompiledMemoryStats:\n   host_output_size_in_bytes: int\n   host_alias_size_in_bytes: int\n   host_temp_size_in_bytes: int\n-  serialized_hlo_proto: bytes\n+  serialized_buffer_assignment_proto: bytes\n   def __str__(self) -> str: ...\n \n class ExecutableBuildOptions:\ndiff --git a/jaxlib/xla.cc b/jaxlib/xla.cc\nindex fa83afc2dc1f..0d3d8f6e1b29 100644\n--- a/jaxlib/xla.cc\n+++ b/jaxlib/xla.cc\n@@ -487,10 +487,26 @@ NB_MODULE(_jax, m) {\n               &CompiledMemoryStats::host_alias_size_in_bytes)\n       .def_rw(\"host_temp_size_in_bytes\",\n               &CompiledMemoryStats::host_temp_size_in_bytes)\n-      .def_prop_ro(\"serialized_hlo_proto\",\n+      .def_prop_ro(\"serialized_buffer_assignment_proto\",\n                    [](const CompiledMemoryStats& cms) -> nb::bytes {\n-                     return nb::bytes(cms.serialized_hlo_proto.data(),\n-                                      cms.serialized_hlo_proto.size());\n+#if JAX_IFRT_VERSION_NUMBER >= 7\n+                     if (cms.buffer_assignment.has_value()) {\n+                       std::string s =\n+                           cms.buffer_assignment->SerializeAsString();\n+                       return nb::bytes(s.data(), s.size());\n+                     } else {\n+                       return nb::bytes();\n+                     }\n+#else\n+                     xla::HloProto hlo;\n+                     if (!cms.serialized_hlo_proto.empty() &&\n+                         hlo.ParseFromString(cms.serialized_hlo_proto)) {\n+                       std::string s =\n+                           hlo.buffer_assignment().SerializeAsString();\n+                       return nb::bytes(s.data(), s.size());\n+                     }\n+                     return nb::bytes();\n+#endif\n                    })\n       .def(\"__str__\", &CompiledMemoryStats::DebugString);\n \n\n```"
        ],
        "from_id": [
            "junwhanahn",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "[Mosaic] Move pallas + mosaic over to tpu transpose from vector. Keep vector around for compat reasons, as legacy.\n\nPiperOrigin-RevId: 756051773",
        "output": [
            "```diff\nCommit: cfab66d540a7ebbeb37b815b5f250691b0ca98d7\nDate: 2025-05-07T23:31:21Z\nURL: https://github.com/jax-ml/jax/commit/cfab66d540a7ebbeb37b815b5f250691b0ca98d7\nFiles changed: 6\nAdditions: +82, Deletions: -9\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex 6aae3a07c764..d7e26ec3b342 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -2357,7 +2357,10 @@ def _transpose_lowering_rule(ctx: LoweringRuleContext, x, *, permutation):\n   out_type = aval_to_ir_type(\n       ctx.lowering_context.dynamic_shape_replacement_fn, ctx.avals_out[0]\n   )\n-  return vector.transpose(out_type, x, permutation)\n+  if ctx.forward_compatible or is_cloud_tpu_older_than(2025, 5, 8):\n+    return vector.transpose(out_type, x, permutation)\n+  else:\n+    return tpu.transpose(out_type, x, permutation)\n \n \n def _bcast(x, y, x_aval, y_aval, out_aval):\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu.td b/jaxlib/mosaic/dialect/tpu/tpu.td\nindex c2d35f6f694a..7f295b4ec09b 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu.td\n+++ b/jaxlib/mosaic/dialect/tpu/tpu.td\n@@ -909,6 +909,29 @@ def TPU_SublaneShuffleOp : TPU_Op<\"sublane_shuffle\", [SameOperandsAndResultType]\n   let hasVerifier = 1;\n }\n \n+def TPU_TransposeOp : TPU_Op<\"transpose\", [Pure]> {\n+  let summary = \"tpu transpose operation\";\n+  let arguments = (ins AnyVectorOfAnyRank:$vector,\n+                       DenseI64ArrayAttr:$permutation);\n+  let results = (outs AnyVectorOfAnyRank:$result);\n+\n+  let builders = [\n+    OpBuilder<(ins \"Value\":$vector, \"ArrayRef<int64_t>\":$permutation)>\n+  ];\n+  let assemblyFormat = [{\n+    $vector `,` $permutation attr-dict `:` type($vector) `->` type($result)\n+  }];\n+  let extraClassDeclaration = [{\n+    VectorType getSourceVectorType() {\n+      return ::llvm::cast<VectorType>(getVector().getType());\n+    }\n+    VectorType getResultVectorType() {\n+      return ::llvm::cast<VectorType>(getResult().getType());\n+    }\n+  }];\n+  let hasVerifier = 1;\n+}\n+\n def TPU_LogOp : TPU_Op<\"log\"> {\n   let arguments = (ins\n     Variadic<AnyType>:$inputs,\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\nindex 6c2e6b700bba..134db412042d 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n+++ b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n@@ -324,6 +324,41 @@ LogicalResult MemRefReshapeOp::verify() {\n   return success();\n }\n \n+LogicalResult TransposeOp::verify() {\n+  auto source_type = getSourceVectorType();\n+  auto permutation = getPermutation();\n+  auto output_type = getResultVectorType();\n+  auto input_shape = source_type.getShape();\n+  auto output_shape = output_type.getShape();\n+  if (source_type.getElementType() != output_type.getElementType()) {\n+    return emitOpError(\"Expected input and output element types to match\");\n+  }\n+  if (permutation.size() != source_type.getRank()) {\n+    return emitOpError(\"Expected permutation rank to match input rank\");\n+  }\n+  if (permutation.size() != output_type.getRank()) {\n+    return emitOpError(\"Expected permutation rank to match output rank\");\n+  }\n+  std::vector<bool> seen_dims(source_type.getRank(), false);\n+  for (int64_t dim : permutation) {\n+    if (dim < 0 || dim >= source_type.getRank()) {\n+      return emitOpError(\"Permutation element out of bounds: \") << dim;\n+    }\n+    if (seen_dims[dim]) {\n+      return emitOpError(\"Permutation element repeated: \") << dim;\n+    }\n+    seen_dims[dim] = true;\n+  }\n+  for (int i = 0; i < source_type.getRank(); ++i) {\n+    if (input_shape[permutation[i]] != output_shape[i]) {\n+      return emitOpError(\n+          \"Expected input shape permuted by the given permutation to match the \"\n+          \"output shape\");\n+    }\n+  }\n+  return success();\n+}\n+\n LogicalResult MemRefReshapeOp::canonicalize(MemRefReshapeOp op,\n                                             PatternRewriter &rewriter) {\n   auto src_ty = op.getInput().getType();\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\nindex 04902c2af6f3..f8e18070e5e7 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/apply_vector_layout.cc\n@@ -4652,7 +4652,7 @@ LogicalResult vector_transpose_rule(RewriteContext &ctx, Operation &op,\n     return op.emitOpError(\"Not implemented: Unsupported 2D layouts\");\n   }\n   ImplicitLocOpBuilder builder(op.getLoc(), &op);\n-  auto transpose_op = cast<vector::TransposeOp>(op);\n+  auto transpose_op = cast<tpu::TransposeOp>(op);\n   VectorType src_ty = transpose_op.getSourceVectorType();\n   VectorType dst_ty = transpose_op.getResultVectorType();\n   const int64_t rank = src_ty.getRank();\n@@ -4735,7 +4735,7 @@ LogicalResult vector_transpose_rule(RewriteContext &ctx, Operation &op,\n     const Value src_tile = assemble(builder, tile_ty_in, layout_in,\n                                     src_tile_vregs, ctx.target_shape);\n     auto new_transpose_op =\n-        builder.create<vector::TransposeOp>(tile_ty_out, src_tile, minor_perm);\n+        builder.create<tpu::TransposeOp>(tile_ty_out, src_tile, minor_perm);\n     new_transpose_op->setAttr(\"out_layout\",\n                               builder.getAttr<VectorLayoutAttr>(layout_out));\n     auto unroll_vectors_op = builder.create<tpu::UnrollVectorsOp>(\n@@ -4871,7 +4871,7 @@ const llvm::StringMap<rule_type> &rules() {\n          vector_extract_strided_slice_rule},\n         {vector::ShapeCastOp::getOperationName(), vector_shape_cast_rule},\n         {vector::StoreOp::getOperationName(), vector_store_rule},\n-        {vector::TransposeOp::getOperationName(), vector_transpose_rule}};\n+        {tpu::TransposeOp::getOperationName(), vector_transpose_rule}};\n \n     for (const auto &[name, rule] : mlir::tpu::extensions::rules()) {\n       rules->insert({name, rule});\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\nindex 7a5e7ba0c10b..110550127ca5 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n@@ -209,7 +209,7 @@ LogicalResult tpu_matmul_rule(const CanonicalizeContext &ctx,\n   // dimension numbers changed which will later be lowered into a more efficient\n   // operation that fuses the transpose into the matmul.\n   auto transpose_op =\n-      dyn_cast_if_present<vector::TransposeOp>(rhs.getDefiningOp());\n+      dyn_cast_if_present<tpu::TransposeOp>(rhs.getDefiningOp());\n   auto dimension_numbers = op.getDimensionNumbers();\n   if (transpose_op && transpose_op->hasOneUse() &&\n       dimension_numbers->getRhsContractingDims().size() == 1 &&\n@@ -259,7 +259,7 @@ LogicalResult tpu_matmul_rule(const CanonicalizeContext &ctx,\n \n       const SmallVector<int64_t> perm_vec =\n           SmallVector<int64_t>(perm.begin(), perm.end());\n-      lhs = builder.create<vector::TransposeOp>(\n+      lhs = builder.create<tpu::TransposeOp>(\n           lhs_ty_transposed, lhs,\n           DenseI64ArrayAttr::get(builder.getContext(), perm_vec));\n     }\n@@ -703,6 +703,17 @@ LogicalResult canonicalize_repeat(const CanonicalizeContext &ctx,\n   return success();\n }\n \n+LogicalResult canonicalize_vector_transpose(const CanonicalizeContext &ctx,\n+                                            Operation &raw_op) {\n+  auto op = cast<vector::TransposeOp>(raw_op);\n+  ImplicitLocOpBuilder builder(op->getLoc(), op.getOperation());\n+  auto new_op = builder.create<tpu::TransposeOp>(op.getType(), op.getVector(),\n+                                                 op.getPermutation());\n+  op.replaceAllUsesWith(new_op.getResult());\n+  op.erase();\n+  return success();\n+}\n+\n using canonicalize_rule_type =\n     std::function<LogicalResult(const CanonicalizeContext &ctx, Operation &op)>;\n \n@@ -713,6 +724,7 @@ const llvm::StringMap<canonicalize_rule_type> &rules() {\n       {vector::ExtractOp::getOperationName(), canonicalize_extract},\n       {vector::MultiDimReductionOp::getOperationName(),\n        canonicalize_multi_dim_reduction},\n+      {vector::TransposeOp::getOperationName(), canonicalize_vector_transpose},\n       {arith::SelectOp::getOperationName(), canonicalize_select},\n       {arith::FPToSIOp::getOperationName(), canonicalize_fptosi},\n       {tpu::RepeatOp::getOperationName(), canonicalize_repeat}};\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\nindex c81701d9a398..2e4c1c9c48a9 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/infer_vector_layout.cc\n@@ -320,7 +320,7 @@ class VectorLayoutInferer {\n         if (inferStore<vector::StoreOp>(op).failed()) {\n           return failure();\n         }\n-      } else if (auto op = dyn_cast<vector::TransposeOp>(any_op)) {\n+      } else if (auto op = dyn_cast<tpu::TransposeOp>(any_op)) {\n         if (infer(op).failed()) {\n           return failure();\n         }\n@@ -1622,7 +1622,7 @@ class VectorLayoutInferer {\n     return success();\n   }\n \n-  LogicalResult infer(vector::TransposeOp op) {\n+  LogicalResult infer(tpu::TransposeOp op) {\n     auto permutation = op.getPermutation();\n     TPU_CHECK_OP(permutation.size() > 1,\n                  \"Vector and scalar transpose should be a no-op and removed\");\n@@ -1910,7 +1910,7 @@ class VectorLayoutInferer {\n           continue;\n         }\n       }\n-      if (auto transpose = dyn_cast<vector::TransposeOp>(operand.getOwner())) {\n+      if (auto transpose = dyn_cast<tpu::TransposeOp>(operand.getOwner())) {\n         auto perm = transpose.getPermutation();\n         auto rank = perm.size();\n         // Only permutations that actually swap the last two dims need it.\n\n```"
        ],
        "from_id": [
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Deprecate parsing of __jax_array__ during abstractification.\n\nGoing forward, objects defining __jax_array__ should define pytree lowering if they\nwant to be compatible with JAX transformations.",
        "output": [
            "```diff\nCommit: dd375cbd336ea49339c349ad350adef66cbd6176\nDate: 2025-05-07T22:50:40Z\nURL: https://github.com/jax-ml/jax/commit/dd375cbd336ea49339c349ad350adef66cbd6176\nFiles changed: 5\nAdditions: +41, Deletions: -2\ndiff --git a/jax/_src/core.py b/jax/_src/core.py\nindex 780d2c487173..2ee3136e6c65 100644\n--- a/jax/_src/core.py\n+++ b/jax/_src/core.py\n@@ -34,6 +34,7 @@\n \n import numpy as np\n \n+from jax._src import deprecations\n from jax._src import dtypes\n from jax._src import config\n from jax._src import effects\n@@ -1554,6 +1555,12 @@ def shaped_abstractify(x):\n   if isinstance(x, AbstractValue):\n     return x\n   if hasattr(x, '__jax_array__'):\n+    deprecations.warn(\n+      'jax-abstract-dunder-array',\n+      ('Triggering of __jax_array__() during abstractification is deprecated.'\n+       ' To avoid this error, either explicitly convert your object using'\n+       ' jax.numpy.array(), or register your object as a pytree.'),\n+      stacklevel=6)\n     return shaped_abstractify(x.__jax_array__())\n   if hasattr(x, 'dtype'):\n     aval = ShapedArray(np.shape(x), x.dtype,\n@@ -1578,6 +1585,12 @@ def get_aval(x):\n     if (aval_fn := pytype_aval_mappings.get(t)):\n       return aval_fn(x)\n   if hasattr(x, '__jax_array__'):\n+    deprecations.warn(\n+      'jax-abstract-dunder-array',\n+      ('Triggering of __jax_array__() during abstractification is deprecated.'\n+       ' To avoid this error, either explicitly convert your object using'\n+       ' jax.numpy.array(), or register your object as a pytree.'),\n+      stacklevel=6)\n     return get_aval(x.__jax_array__())\n   raise TypeError(f\"Argument '{x}' of type '{typ}' is not a valid JAX type\")\n \ndiff --git a/jax/_src/deprecations.py b/jax/_src/deprecations.py\nindex 329491b1e8a8..4e5e22745658 100644\n--- a/jax/_src/deprecations.py\n+++ b/jax/_src/deprecations.py\n@@ -135,3 +135,4 @@ def warn(deprecation_id: str, message: str, stacklevel: int) -> None:\n register('jax-numpy-trimzeros-not-1d-array')\n register('jax-scipy-special-sph-harm')\n register('jax-jit-positional-args')\n+register('jax-abstract-dunder-array')\ndiff --git a/jax/_src/numpy/lax_numpy.py b/jax/_src/numpy/lax_numpy.py\nindex c40716daee4b..be3ac14647a8 100644\n--- a/jax/_src/numpy/lax_numpy.py\n+++ b/jax/_src/numpy/lax_numpy.py\n@@ -1235,7 +1235,7 @@ def permute_dims(a: ArrayLike, /, axes: tuple[int, ...]) -> Array:\n            [2, 5],\n            [3, 6]], dtype=int32)\n   \"\"\"\n-  util.check_arraylike(\"permute_dims\", a)\n+  a = util.ensure_arraylike(\"permute_dims\", a)\n   return lax.transpose(a, axes)\n \n \ndiff --git a/tests/api_test.py b/tests/api_test.py\nindex 610719518a03..53afed80cb70 100644\n--- a/tests/api_test.py\n+++ b/tests/api_test.py\n@@ -3959,6 +3959,9 @@ def test_default_device(self):\n   def test_dunder_jax_array(self):\n     # https://github.com/jax-ml/jax/pull/4725\n \n+    @partial(jax.tree_util.register_dataclass,\n+             data_fields=['jax_val'],\n+             meta_fields=[])\n     class AlexArray:\n       def __init__(self, jax_val):\n         self.jax_val = jax_val\n@@ -3968,10 +3971,16 @@ def __jax_array__(self):\n       shape = property(lambda self: self.jax_val.shape)\n \n     x = AlexArray(jnp.array([1., 2., 3.]))\n+\n+    y = jax.jit(lambda x: x)(x)\n+    self.assertIsInstance(x, AlexArray)\n+    self.assertArraysEqual(jnp.asarray(x), jnp.asarray(y))\n+\n     y = jnp.sin(x)\n     self.assertAllClose(y, jnp.sin(jnp.array([1., 2., 3.])))\n     y = api.grad(api.jit(lambda x: jnp.sin(x).sum()))(x)\n-    self.assertAllClose(y, jnp.cos(jnp.array([1., 2., 3.])))\n+    self.assertIsInstance(y, AlexArray)\n+    self.assertAllClose(jnp.asarray(y), jnp.cos(jnp.array([1., 2., 3.])))\n \n     x = AlexArray(jnp.array([[1., 2., 3.]]))\n     y = api.pmap(jnp.sin)(x)\n@@ -3989,6 +3998,19 @@ def __jax_array__(self):\n     a2 = jnp.array(((x, x), [x, x]))\n     self.assertAllClose(np.array(((1, 1), (1, 1))), a2)\n \n+  def test_dunder_jax_array_warnings(self):\n+    class AlexArray:\n+      def __init__(self, jax_val):\n+        self.jax_val = jax_val\n+      def __jax_array__(self):\n+        return self.jax_val\n+\n+    f = jax.jit(lambda x: x)\n+    a = AlexArray(jnp.arange(4))\n+    msg = r\"Triggering of __jax_array__\\(\\) during abstractification is deprecated.\"\n+    with self.assertDeprecationWarnsOrRaises('jax-abstract-dunder-array', msg):\n+      f(a)\n+\n   @jtu.thread_unsafe_test()  # count_jit_tracing_cache_miss() isn't thread-safe\n   def test_eval_shape_weak_type(self):\n     # https://github.com/jax-ml/jax/issues/23302\ndiff --git a/tests/array_extensibility_test.py b/tests/array_extensibility_test.py\nindex 91e2a1d9cf6d..36726659c2f9 100644\n--- a/tests/array_extensibility_test.py\n+++ b/tests/array_extensibility_test.py\n@@ -29,6 +29,9 @@\n config.parse_flags_with_absl()\n \n \n+@functools.partial(jax.tree_util.register_dataclass,\n+                   data_fields=['x'],\n+                   meta_fields=[])\n class JaxArrayWrapper:\n   \"\"\"Class that provides a __jax_array__ method.\"\"\"\n   x: ArrayLike\n\n```"
        ],
        "from_id": [
            "jakevdp"
        ]
    },
    {
        "text_input": "[pallas:mosaic] Use `cf.assert` directly in the lowering rule for `checkify.check_p`\n\nWe now bundle the `cf` dialect with jaxlib and register it in the `ir.Context`,\nso `cf.assert` can be used directly.\n\nPiperOrigin-RevId: 755972714",
        "output": [
            "```diff\nCommit: 3ea7a5dac2f1188e79b6c3306fb07d74485d310f\nDate: 2025-05-07T20:01:57Z\nURL: https://github.com/jax-ml/jax/commit/3ea7a5dac2f1188e79b6c3306fb07d74485d310f\nFiles changed: 2\nAdditions: +9, Deletions: -17\ndiff --git a/jax/_src/pallas/mosaic/lowering.py b/jax/_src/pallas/mosaic/lowering.py\nindex f372f8f9b472..6aae3a07c764 100644\n--- a/jax/_src/pallas/mosaic/lowering.py\n+++ b/jax/_src/pallas/mosaic/lowering.py\n@@ -51,6 +51,7 @@\n from jax._src.lib import version as jaxlib_version\n from jax._src.lib.mlir import ir\n from jax._src.lib.mlir.dialects import arith\n+from jax._src.lib.mlir.dialects import cf\n from jax._src.lib.mlir.dialects import func\n from jax._src.lib.mlir.dialects import math\n from jax._src.lib.mlir.dialects import memref\n@@ -160,7 +161,6 @@ def to_placeholder(self, dim_expr: Any) -> ir.Value:\n \n @dataclasses.dataclass\n class LoweringContext:\n-  ir_context: ir.Context\n   grid_sizes: tuple[int, ...]  # Includes both user and vmap axes.\n   grid_names: tuple[Hashable, ...] | None\n   mapped_dims: tuple[int, ...]  # Indices of vmapped grid dimensions.\n@@ -668,7 +668,6 @@ def err_details():\n \n def lower_jaxpr_to_module(\n     lowering_context: mlir.LoweringRuleContext,\n-    ctx: ir.Context,\n     grid_mapping: pallas_core.GridMapping,\n     jaxpr: jax_core.Jaxpr,\n     *,\n@@ -720,7 +719,6 @@ def dynamic_shape_replacement_fn(\n   sym_tab = ir.SymbolTable(m.operation)\n \n   func_op = lower_jaxpr_to_func(\n-      ctx,\n       jaxpr,\n       mosaic_grid_mapping=mosaic_grid_mapping,\n       name=\"main\",\n@@ -755,7 +753,6 @@ def dynamic_shape_replacement_fn(\n         continue\n \n       mlir_func = lower_jaxpr_to_transform_func(\n-          ctx,\n           bm.index_map_jaxpr.jaxpr,\n           bm.block_aval,\n           name=func_name,\n@@ -902,7 +899,6 @@ def dynamic_shape_replacement_fn(\n \n \n def lower_jaxpr_to_transform_func(\n-    ctx: ir.Context,\n     jaxpr: jax_core.Jaxpr,\n     aval: jax_core.AbstractValue,\n     *,\n@@ -937,7 +933,6 @@ def body_func(*args):\n     else:\n       mesh_context = None\n     lowering_context = LoweringContext(\n-        ctx,\n         mosaic_grid_mapping.grid,\n         mosaic_grid_mapping.grid_names,\n         mosaic_grid_mapping.mapped_dims,\n@@ -970,7 +965,6 @@ def body_func(*args):\n \n \n def lower_jaxpr_to_func(\n-    ctx: ir.Context,\n     jaxpr: jax_core.Jaxpr,\n     *,\n     mosaic_grid_mapping: MosaicGridMapping,\n@@ -1009,7 +1003,6 @@ def body_func(*args):\n     else:\n       mesh_context = None\n     lowering_context = LoweringContext(\n-        ctx,\n         mosaic_grid_mapping.grid,\n         mosaic_grid_mapping.grid_names,\n         mosaic_grid_mapping.mapped_dims,\n@@ -3742,9 +3735,12 @@ def _checkify_lowering_rule(\n                               \"--jax_pallas_enable_runtime_assert \"\n                               \"or functionalize with checkify.check.\")\n \n-  assert ctx.lowering_context.ir_context.allow_unregistered_dialects, (\n-    \"allow_unregistered_dialects must be set to True for \"\n-    \"runtime assert check.\")\n+  if cf is None:\n+    # TODO(slebedev): Remove once the minimal jaxlib version is 0.6.1.\n+    raise ValueError(\n+        \"cf dialect is not available. Make sure you have jaxlib 0.6.1 or later.\"\n+    )\n+\n   error = jax.tree.unflatten(err_tree, err_args)\n   assert len(error._pred) == 1\n   assert len(error._metadata) == 1\n@@ -3761,10 +3757,7 @@ def _checkify_lowering_rule(\n   out_scalar_type = _dtype_to_ir_type(jnp.dtype('bool'))\n   minus_one = ir_constant(-1, out_scalar_type)\n   not_pred = arith.xori(pred, minus_one)\n-  attrs = {\"msg\": ir.StringAttr.get(exception.fmt_string)}\n-  ir.Operation.create(\"cf.assert\",\n-                      operands=(not_pred,),\n-                      attributes=attrs)\n+  cf.assert_(not_pred, exception.fmt_string)\n   return []\n \n \ndiff --git a/jax/_src/pallas/mosaic/pallas_call_registration.py b/jax/_src/pallas/mosaic/pallas_call_registration.py\nindex c1d1a8029c5f..5de917d077ce 100644\n--- a/jax/_src/pallas/mosaic/pallas_call_registration.py\n+++ b/jax/_src/pallas/mosaic/pallas_call_registration.py\n@@ -142,12 +142,11 @@ def pallas_call_tpu_lowering_rule(\n   tpu.register_dialect(mlir_ctx)\n \n   def lower_module(for_verification: bool):\n-    if for_verification or tpu_core.runtime_assert_enabled():\n+    if for_verification:\n       mlir_ctx.allow_unregistered_dialects = True\n     with mlir_ctx, ir.Location.unknown(mlir_ctx):\n       return lowering.lower_jaxpr_to_module(\n           ctx,\n-          mlir_ctx,\n           grid_mapping,\n           jaxpr,\n           dimension_semantics=mosaic_params.dimension_semantics,\n\n```"
        ],
        "from_id": [
            "superbobry",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Fold transposes feeding the RHS of a matmul into a change in the dot dimension numbers.\n\nPiperOrigin-RevId: 755935066",
        "output": [
            "```diff\nCommit: 6259cfe31742e8985f9522beb1d64d7903002950\nDate: 2025-05-07T18:27:26Z\nURL: https://github.com/jax-ml/jax/commit/6259cfe31742e8985f9522beb1d64d7903002950\nFiles changed: 1\nAdditions: +31, Deletions: -0\ndiff --git a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\nindex 7f241deb550c..7a5e7ba0c10b 100644\n--- a/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n+++ b/jaxlib/mosaic/dialect/tpu/transforms/canonicalize_mosaic.cc\n@@ -205,6 +205,37 @@ LogicalResult tpu_matmul_rule(const CanonicalizeContext &ctx,\n     }\n   }\n \n+  // Attempt to canonicalize matmul(x, transpose(y)) to a matmul with the\n+  // dimension numbers changed which will later be lowered into a more efficient\n+  // operation that fuses the transpose into the matmul.\n+  auto transpose_op =\n+      dyn_cast_if_present<vector::TransposeOp>(rhs.getDefiningOp());\n+  auto dimension_numbers = op.getDimensionNumbers();\n+  if (transpose_op && transpose_op->hasOneUse() &&\n+      dimension_numbers->getRhsContractingDims().size() == 1 &&\n+      dimension_numbers->getRhsNonContractingDims().size() == 1) {\n+    auto rhs_non_contracting_dim =\n+        dimension_numbers->getRhsNonContractingDims()[0];\n+    auto rhs_contracting_dim = dimension_numbers->getRhsContractingDims()[0];\n+    auto permutation = transpose_op.getPermutation();\n+    if (permutation[rhs_contracting_dim] == rhs_non_contracting_dim &&\n+        permutation[rhs_non_contracting_dim] == rhs_contracting_dim &&\n+        std::all_of(dimension_numbers->getRhsBatchDims().begin(),\n+                    dimension_numbers->getRhsBatchDims().end(),\n+                    [&](long batch_dim) {\n+                      return permutation[batch_dim] == batch_dim;\n+                    })) {\n+      if (auto transpose_op_vector_operand =\n+              dyn_cast<TypedValue<VectorType>>(transpose_op.getOperand())) {\n+        // The transpose is DCE'ed away at a later point.\n+        rhs = transpose_op_vector_operand;\n+        transpose_rhs = !transpose_rhs;\n+      } else {\n+        return op->emitOpError(\"Unexpected operand type for transpose op.\");\n+      }\n+    }\n+  }\n+\n   auto dot_dim_matmul = [&](auto lhs, auto rhs, auto acc) {\n     auto precision_attr = op.getPrecisionAttr();\n \n\n```"
        ],
        "from_id": [
            "jaswanth-",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Replace `std::shared_ptr<xla::ifrt::LoadedExecutable>` with `xla::ifrt::LoadedExecutableRef`\n\nPiperOrigin-RevId: 755932233",
        "output": [
            "```diff\nCommit: a4adf32f9a9bf0ad80e4ae6ad6cbb6c00cc1dae2\nDate: 2025-05-07T18:20:23Z\nURL: https://github.com/jax-ml/jax/commit/a4adf32f9a9bf0ad80e4ae6ad6cbb6c00cc1dae2\nFiles changed: 3\nAdditions: +9, Deletions: -10\ndiff --git a/jaxlib/py_compile_only_client.cc b/jaxlib/py_compile_only_client.cc\nindex 4d53fc6ee832..0fa2f4b48fd7 100644\n--- a/jaxlib/py_compile_only_client.cc\n+++ b/jaxlib/py_compile_only_client.cc\n@@ -70,7 +70,7 @@ class CompileOnlyPyClient : public PyClient {\n     return client;\n   }\n \n-  absl::StatusOr<std::shared_ptr<ifrt::Executable>> CompileUnloaded(\n+  absl::StatusOr<ifrt::ExecutableRef> CompileUnloaded(\n       absl::string_view mlir_module, ifrt::DeviceListRef executable_devices,\n       CompileOptions options, std::vector<nb::capsule> host_callbacks) {\n     if (!host_callbacks.empty()) {\n@@ -102,7 +102,7 @@ class CompileOnlyPyClient : public PyClient {\n                                     *ifrt_client->topology().description()));\n     TF_ASSIGN_OR_RETURN(auto ifrt_executable,\n                         ifrt::PjRtExecutable::Create(std::move(executable)));\n-    return std::shared_ptr<ifrt::Executable>(std::move(ifrt_executable));\n+    return ifrt::ExecutableRef(std::move(ifrt_executable));\n   }\n \n  private:\ndiff --git a/jaxlib/py_executable.cc b/jaxlib/py_executable.cc\nindex d79b236b9241..a9a7ebea2d0b 100644\n--- a/jaxlib/py_executable.cc\n+++ b/jaxlib/py_executable.cc\n@@ -84,7 +84,7 @@ absl::Status PyShardedToken::Await() {\n \n PyLoadedExecutable::PyLoadedExecutable(\n     nb_class_ptr<PyClient> client,\n-    std::shared_ptr<ifrt::LoadedExecutable> ifrt_loaded_executable,\n+    ifrt::LoadedExecutableRef ifrt_loaded_executable,\n     std::optional<nb_traceback> traceback,\n     std::optional<std::string> fingerprint)\n     : client_(std::move(client)),\ndiff --git a/jaxlib/py_executable.h b/jaxlib/py_executable.h\nindex 7e329410c763..0e7762730cfb 100644\n--- a/jaxlib/py_executable.h\n+++ b/jaxlib/py_executable.h\n@@ -131,11 +131,10 @@ using ExecuteShardedArg = std::variant<PyArray, std::vector<PyArray>>;\n // b) to add Python-specific functionality.\n class PyLoadedExecutable {\n  public:\n-  PyLoadedExecutable(\n-      nb_class_ptr<PyClient> client,\n-      std::shared_ptr<ifrt::LoadedExecutable> ifrt_loaded_executable,\n-      std::optional<nb_traceback> traceback,\n-      std::optional<std::string> fingerprint);\n+  PyLoadedExecutable(nb_class_ptr<PyClient> client,\n+                     ifrt::LoadedExecutableRef ifrt_loaded_executable,\n+                     std::optional<nb_traceback> traceback,\n+                     std::optional<std::string> fingerprint);\n   ~PyLoadedExecutable();\n \n   nb_class_ptr<PyClient> client() const { return client_; }\n@@ -143,7 +142,7 @@ class PyLoadedExecutable {\n     return ifrt_loaded_executable_.get();\n   }\n \n-  std::shared_ptr<ifrt::LoadedExecutable> shared_ifrt_loaded_executable() {\n+  ifrt::LoadedExecutableRef shared_ifrt_loaded_executable() {\n     return ifrt_loaded_executable_;\n   }\n \n@@ -221,7 +220,7 @@ class PyLoadedExecutable {\n   friend class PyClient;\n \n   nb_class_ptr<PyClient> client_;\n-  std::shared_ptr<ifrt::LoadedExecutable> ifrt_loaded_executable_;\n+  ifrt::LoadedExecutableRef ifrt_loaded_executable_;\n   std::optional<nb_traceback> traceback_;\n \n   // Identical executables (i.e. representing the same program) will have the\n\n```"
        ],
        "from_id": [
            "junwhanahn",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Use `ArrayRef` instead of `tsl::RCReference<Array>`\n\nPiperOrigin-RevId: 755919798",
        "output": [
            "```diff\nCommit: 4cc4bd3ca1efd787aaaadc23cd8f5ecdfe8a1c4a\nDate: 2025-05-07T17:51:16Z\nURL: https://github.com/jax-ml/jax/commit/4cc4bd3ca1efd787aaaadc23cd8f5ecdfe8a1c4a\nFiles changed: 9\nAdditions: +81, Deletions: -93\ndiff --git a/jaxlib/pjit.cc b/jaxlib/pjit.cc\nindex 8c8800e80706..804352161597 100644\n--- a/jaxlib/pjit.cc\n+++ b/jaxlib/pjit.cc\n@@ -419,11 +419,10 @@ PjitFunction::~PjitFunction() {\n   executables_ = nullptr;\n }\n \n-void CallShardArgFallback(\n-    nb::handle arg, nb::handle sharding, nb::handle layout,\n-    const nb::callable& fallback,\n-    std::vector<tsl::RCReference<xla::ifrt::Array>>& num_args_arrays,\n-    std::vector<nb::object>& keep_alive_objects) {\n+void CallShardArgFallback(nb::handle arg, nb::handle sharding,\n+                          nb::handle layout, const nb::callable& fallback,\n+                          std::vector<xla::ifrt::ArrayRef>& num_args_arrays,\n+                          std::vector<nb::object>& keep_alive_objects) {\n   tsl::profiler::TraceMe traceme(\"cpp_pjit_shard_arg_fallback\");\n   auto py_array_or_bufs = fallback(arg, sharding, layout);\n   auto py_array = nb::cast<xla::PyArray>(py_array_or_bufs);\n@@ -433,8 +432,7 @@ void CallShardArgFallback(\n \n // Prepares the input PjRtBuffers from the python arguments. This is equivalent\n // to shard_args() in pxla.py but for only a few supported cases.\n-absl::StatusOr<std::vector<tsl::RCReference<xla::ifrt::Array>>>\n-PrepareIfrtInputs(\n+absl::StatusOr<std::vector<xla::ifrt::ArrayRef>> PrepareIfrtInputs(\n     const xla::PyLoadedExecutable& executable,\n     absl::Span<nb::object const> flat_dynamic_args,\n     absl::Span<xla::PyArgSignature const> flat_dynamic_arg_signatures,\n@@ -449,12 +447,12 @@ PrepareIfrtInputs(\n       executable.ifrt_loaded_executable()->num_devices();\n   int num_args = flat_dynamic_args.size();\n \n-  std::vector<tsl::RCReference<xla::ifrt::Array>> num_args_arrays;\n+  std::vector<xla::ifrt::ArrayRef> num_args_arrays;\n   num_args_arrays.reserve(num_args);\n \n   struct CopyGroup {\n     std::vector<int> indices;\n-    std::vector<tsl::RCReference<xla::ifrt::Array>> arrays;\n+    std::vector<xla::ifrt::ArrayRef> arrays;\n   };\n   absl::flat_hash_map<std::pair<xla::ifrt::Device*, xla::ifrt::MemoryKind>,\n                       CopyGroup>\n@@ -760,7 +758,7 @@ absl::StatusOr<nb::object> PjitFunction::Call(nb::handle callable,\n       tsl::Env::Default()->GetCurrentThreadId();\n \n   // A vector of [num_outputs].\n-  std::vector<tsl::RCReference<xla::ifrt::Array>> output_arrays;\n+  std::vector<xla::ifrt::ArrayRef> output_arrays;\n   {\n     nb::gil_scoped_release gil_release;\n     TF_ASSIGN_OR_RETURN(auto result,\ndiff --git a/jaxlib/pmap_lib.cc b/jaxlib/pmap_lib.cc\nindex c29c2d1eb2b5..e18dd8b4637a 100644\n--- a/jaxlib/pmap_lib.cc\n+++ b/jaxlib/pmap_lib.cc\n@@ -110,7 +110,7 @@ struct ResultSpec {\n struct ShardArgResult {\n   // Points to the on-device array.\n   // ifrt_array->sharding().num_shards() == `num_devices`.\n-  tsl::RCReference<xla::ifrt::Array> ifrt_array;\n+  xla::ifrt::ArrayRef ifrt_array;\n   // The Python argument will be always be copied to `owning_sda`.\n   nb::object owning_sda;\n };\n@@ -615,7 +615,7 @@ absl::StatusOr<nb::object> PmapFunction::Call(nb::handle callable,\n   const int num_args = flat_dynamic_args.size();\n \n   // We need [num_args] for the `Execute` call below.\n-  std::vector<tsl::RCReference<xla::ifrt::Array>> num_args_arrays(num_args);\n+  std::vector<xla::ifrt::ArrayRef> num_args_arrays(num_args);\n   for (int i = 0; i < num_args; ++i) {\n     TF_ASSIGN_OR_RETURN(\n         ShardArgResult sharded_arg,\n@@ -634,7 +634,7 @@ absl::StatusOr<nb::object> PmapFunction::Call(nb::handle callable,\n       tsl::Env::Default()->GetCurrentThreadId();\n \n   // A vector of [num_outputs].\n-  std::vector<tsl::RCReference<xla::ifrt::Array>> output_arrays;\n+  std::vector<xla::ifrt::ArrayRef> output_arrays;\n   {\n     nb::gil_scoped_release gil_release;\n     auto ifrt_executable = cache_entry.executable->ifrt_executable();\ndiff --git a/jaxlib/py_array.cc b/jaxlib/py_array.cc\nindex a977d5f1a554..1222d410bad8 100644\n--- a/jaxlib/py_array.cc\n+++ b/jaxlib/py_array.cc\n@@ -146,12 +146,12 @@ absl::StatusOr<const Shape*> XlaDynamicShape(ifrt::Array* ifrt_array,\n   return &scratch.value();\n }\n \n-tsl::RCReference<ifrt::Array> CreateIfRtArrayFromSingleDeviceShardedPyArrays(\n+ifrt::ArrayRef CreateIfRtArrayFromSingleDeviceShardedPyArrays(\n     nb_dtype dtype, absl::Span<const int64_t> shape,\n     absl::Span<const PyArray> py_arrays, const nb::object& sharding) {\n   const ifrt::MemoryKind dst_memory_kind = xla::GetMemoryKind(sharding);\n \n-  std::vector<tsl::RCReference<ifrt::Array>> ifrt_arrays;\n+  std::vector<ifrt::ArrayRef> ifrt_arrays;\n   ifrt_arrays.reserve(py_arrays.size());\n   absl::InlinedVector<ifrt::Device*, 1> devices;\n   devices.reserve(py_arrays.size());\n@@ -225,7 +225,7 @@ tsl::RCReference<ifrt::Array> CreateIfRtArrayFromSingleDeviceShardedPyArrays(\n   // TODO(emilyaf): Always use `ifrt_dtype` once tokens are handled correctly.\n   ifrt::DType array_dtype =\n       ifrt_arrays.empty() ? ifrt_dtype.value() : ifrt_arrays[0]->dtype();\n-  absl::StatusOr<tsl::RCReference<ifrt::Array>> ifrt_array =\n+  absl::StatusOr<ifrt::ArrayRef> ifrt_array =\n       device->client()->AssembleArrayFromSingleDeviceArrays(\n           array_dtype, ifrt::Shape(shape), *std::move(ifrt_sharding),\n           absl::MakeSpan(ifrt_arrays), ifrt::ArrayCopySemantics::kReuseInput,\n@@ -458,7 +458,7 @@ PyArray_Storage::PyArray_Storage(\n     nb::object aval, bool weak_type, xla::nb_dtype dtype,\n     std::vector<int64_t> shape, nb::object sharding, bool committed,\n     nb_class_ptr<PyClient> py_client, std::optional<nb_traceback> traceback,\n-    tsl::RCReference<ifrt::Array> ifrt_array, xla::PjRtFuture<> result_status)\n+    ifrt::ArrayRef ifrt_array, xla::PjRtFuture<> result_status)\n     : aval(std::move(aval)),\n       weak_type(weak_type),\n       dtype(std::move(dtype)),\n@@ -515,7 +515,7 @@ void PyArray::PyInit(PyArray self, nb::object aval, nb::object sharding,\n \n PyArray PyArray::MakeFromSingleDeviceArray(\n     nb_class_ptr<PyClient> py_client, std::optional<nb_traceback> traceback,\n-    tsl::RCReference<ifrt::Array> ifrt_array, bool weak_type, bool committed,\n+    ifrt::ArrayRef ifrt_array, bool weak_type, bool committed,\n     xla::PjRtFuture<> result_status) {\n   if (!llvm::isa<ifrt::SingleDeviceSharding>(ifrt_array->sharding())) {\n     throw XlaRuntimeError(\n@@ -547,8 +547,8 @@ PyArray PyArray::MakeFromSingleDeviceArray(\n \n PyArray PyArray::MakeFromIfrtArrayAndSharding(\n     nb_class_ptr<PyClient> py_client, std::optional<nb_traceback> traceback,\n-    tsl::RCReference<ifrt::Array> ifrt_array, nb::object sharding,\n-    bool weak_type, bool committed, bool skip_checks) {\n+    ifrt::ArrayRef ifrt_array, nb::object sharding, bool weak_type,\n+    bool committed, bool skip_checks) {\n   auto shape_span = ifrt_array->shape().dims();\n   ShapedArrayCacheKey key;\n   key.dtype = ifrt_array->dtype();\n@@ -590,7 +590,7 @@ PyArray PyArrayResultHandler::Call(absl::Span<const PyArray> py_arrays) const {\n }\n \n PyArray PyArrayResultHandler::Call(nb_class_ptr<PyClient> py_client,\n-                                   tsl::RCReference<ifrt::Array> ifrt_array,\n+                                   ifrt::ArrayRef ifrt_array,\n                                    xla::PjRtFuture<> result_status) const {\n   return PyArray(aval_, weak_type_, dtype_, shape_, sharding_,\n                  std::move(py_client), Traceback::Get(), std::move(ifrt_array),\n@@ -606,8 +606,8 @@ PyArray::PyArray(nb::object aval, bool weak_type, nb_dtype dtype,\n                  std::vector<int64_t> shape, nb::object sharding,\n                  nb_class_ptr<PyClient> py_client,\n                  std::optional<nb_traceback> traceback,\n-                 tsl::RCReference<ifrt::Array> ifrt_array, bool committed,\n-                 bool skip_checks, xla::PjRtFuture<> result_status) {\n+                 ifrt::ArrayRef ifrt_array, bool committed, bool skip_checks,\n+                 xla::PjRtFuture<> result_status) {\n   auto* self =\n       PyArray_tp_new(reinterpret_cast<PyTypeObject*>(type_), nullptr, nullptr);\n   m_ptr = self;\n@@ -636,7 +636,7 @@ nb::object PyArray::CheckAndRearrange(const absl::Span<const PyArray> py_arrays,\n   return this->attr(\"_check_and_rearrange\")(py_arrays, sharding, aval);\n }\n \n-void PyArray::SetIfrtArray(tsl::RCReference<ifrt::Array> ifrt_array) {\n+void PyArray::SetIfrtArray(ifrt::ArrayRef ifrt_array) {\n   GetStorage().ifrt_array = std::move(ifrt_array);\n }\n \n@@ -683,7 +683,7 @@ nb::object PyArray::arrays() {\n \n absl::Status PyArray::set_arrays(nb::object obj) {\n   if (obj.is_none()) {\n-    SetIfrtArray(tsl::RCReference<ifrt::Array>());\n+    SetIfrtArray(ifrt::ArrayRef());\n     py_arrays().clear();\n     return absl::OkStatus();\n   }\n@@ -697,9 +697,9 @@ absl::Status PyArray::set_arrays(nb::object obj) {\n \n   if (list.size() == 0) return absl::OkStatus();\n \n-  SetIfrtArray(tsl::RCReference<ifrt::Array>());\n+  SetIfrtArray(ifrt::ArrayRef());\n   py_arrays().clear();\n-  std::vector<tsl::RCReference<ifrt::Array>> ifrt_arrays;\n+  std::vector<ifrt::ArrayRef> ifrt_arrays;\n   ifrt_arrays.reserve(list.size());\n   absl::InlinedVector<ifrt::Device*, 1> devices;\n   devices.reserve(list.size());\n@@ -1074,7 +1074,7 @@ absl::Status PyArray::Delete() {\n     // buffer has been deleted or a request must be processed via RPC,\n     // especially as this deletion is done per array.\n     ifrt_array()->Delete();\n-    SetIfrtArray(tsl::RCReference<ifrt::Array>());\n+    SetIfrtArray(ifrt::ArrayRef());\n   }\n   return absl::OkStatus();\n }\n@@ -1090,7 +1090,7 @@ bool PyArray::IsDeleted() const {\n PyArray PyArray::Clone() const {\n   auto array = tsl::FormRef(ifrt_array());\n   auto* ifrt_client = py_client()->ifrt_client();\n-  tsl::RCReference<ifrt::Array> out =\n+  ifrt::ArrayRef out =\n       ifrt_client\n           ->CopyArrays(absl::MakeSpan(&array, 1), /*devices=*/std::nullopt,\n                        /*memory_kind=*/std::nullopt,\n@@ -1149,7 +1149,7 @@ absl::StatusOr<std::vector<PyArray>> PyArray::BatchedCopyToDeviceWithSharding(\n   // kinds. The grouping is enforced by `ifrt::Client::CopyArrays()`.\n   struct Batch {\n     std::vector<int> indexes;\n-    std::vector<tsl::RCReference<ifrt::Array>> ifrt_arrays;\n+    std::vector<ifrt::ArrayRef> ifrt_arrays;\n   };\n   absl::flat_hash_map<BatchedCopyToDeviceWithShardingKey, Batch> batches;\n \n@@ -1206,7 +1206,7 @@ absl::StatusOr<std::vector<PyArray>> PyArray::BatchedCopyToDeviceWithSharding(\n     batch.ifrt_arrays.push_back(tsl::FormRef(ifrt_array_ptr));\n   }\n \n-  std::vector<std::pair<int, tsl::RCReference<ifrt::Array>>> ifrt_arrays;\n+  std::vector<std::pair<int, ifrt::ArrayRef>> ifrt_arrays;\n   {\n     GlobalPyRefManager()->CollectGarbage();\n     nb::gil_scoped_release gil_release;\n@@ -1271,7 +1271,7 @@ absl::StatusOr<PyArray> PyArray::BatchedDevicePut(\n       (!force_copy && (host_buffer_semantics ==\n                        ifrt::Client::HostBufferSemantics::kImmutableZeroCopy));\n \n-  std::vector<tsl::RCReference<ifrt::Array>> ifrt_arrays;\n+  std::vector<ifrt::ArrayRef> ifrt_arrays;\n \n   absl::InlinedVector<ifrt::Device*, 1> devices;\n   devices.reserve(n_devices);\n@@ -1334,7 +1334,7 @@ absl::StatusOr<PyArray> PyArray::ReorderShards(\n       GetIfrtConcreteEvenSharding(dst_sharding, ifrt_array_ptr->dtype(),\n                                   ifrt_array_ptr->shape()));\n \n-  tsl::RCReference<xla::ifrt::Array> new_ifrt_array;\n+  xla::ifrt::ArrayRef new_ifrt_array;\n   {\n     nb::gil_scoped_release gil_release;\n \n@@ -1399,7 +1399,7 @@ absl::StatusOr<PyArray> PyArray::ReorderShards(\n         /*mappings=*/std::move(mappings),\n     };\n     DCHECK_OK(plan.Validate());\n-    std::vector<tsl::RCReference<xla::ifrt::Array>> input;\n+    std::vector<xla::ifrt::ArrayRef> input;\n     input.push_back(tsl::FormRef(ifrt_array_ptr));\n     TF_ASSIGN_OR_RETURN(\n         auto remapped,\n@@ -1717,7 +1717,7 @@ absl::StatusOr<std::pair<nb::object, bool>> PyHostValue::AsNumPyArray(\n                           PrimitiveTypeToNbDtype(shape->element_type()));\n       // Objects that must be kept alive while the array is alive.\n       struct Hold {\n-        tsl::RCReference<ifrt::Array> buffer;\n+        ifrt::ArrayRef buffer;\n         std::unique_ptr<PjRtBuffer::ExternalReference> external_reference_hold;\n       };\n       auto hold = std::make_unique<Hold>();\ndiff --git a/jaxlib/py_array.h b/jaxlib/py_array.h\nindex ddb09bc41771..bf1208c11da5 100644\n--- a/jaxlib/py_array.h\n+++ b/jaxlib/py_array.h\n@@ -94,8 +94,7 @@ struct PyArray_Storage {\n                   std::vector<int64_t> shape, nanobind::object sharding,\n                   bool committed, nb_class_ptr<PyClient> py_client,\n                   std::optional<nb_traceback> traceback,\n-                  tsl::RCReference<ifrt::Array> ifrt_array,\n-                  xla::PjRtFuture<> result_status);\n+                  ifrt::ArrayRef ifrt_array, xla::PjRtFuture<> result_status);\n \n   ~PyArray_Storage();\n   nanobind::handle AsHandle();\n@@ -111,7 +110,7 @@ struct PyArray_Storage {\n \n   nb_class_ptr<PyClient> py_client;\n   std::optional<nb_traceback> traceback;\n-  tsl::RCReference<ifrt::Array> ifrt_array;\n+  ifrt::ArrayRef ifrt_array;\n   nanobind::object fully_replicated_array = nanobind::none();\n \n   // optional field, used only in python\n@@ -153,20 +152,19 @@ class PyArray : public nanobind::object {\n   PyArray(nanobind::object aval, bool weak_type, nb_dtype dtype,\n           std::vector<int64_t> shape, nanobind::object sharding,\n           nb_class_ptr<PyClient> py_client,\n-          std::optional<nb_traceback> traceback,\n-          tsl::RCReference<ifrt::Array> ifrt_array, bool committed,\n-          bool skip_checks,\n+          std::optional<nb_traceback> traceback, ifrt::ArrayRef ifrt_array,\n+          bool committed, bool skip_checks,\n           xla::PjRtFuture<> result_status = xla::PjRtFuture<>());\n \n   static PyArray MakeFromSingleDeviceArray(\n       nb_class_ptr<PyClient> py_client, std::optional<nb_traceback> traceback,\n-      tsl::RCReference<ifrt::Array> ifrt_array, bool weak_type, bool committed,\n+      ifrt::ArrayRef ifrt_array, bool weak_type, bool committed,\n       xla::PjRtFuture<> result_status = xla::PjRtFuture<>());\n \n   static PyArray MakeFromIfrtArrayAndSharding(\n       nb_class_ptr<PyClient> py_client, std::optional<nb_traceback> traceback,\n-      tsl::RCReference<ifrt::Array> ifrt_array, nanobind::object sharding,\n-      bool weak_type, bool committed, bool skip_checks);\n+      ifrt::ArrayRef ifrt_array, nanobind::object sharding, bool weak_type,\n+      bool committed, bool skip_checks);\n \n   static absl::Status RegisterTypes(nanobind::module_& m);\n \n@@ -325,7 +323,7 @@ class PyArray : public nanobind::object {\n                                      nanobind::object sharding,\n                                      nanobind::object aval);\n \n-  void SetIfrtArray(tsl::RCReference<ifrt::Array> ifrt_array);\n+  void SetIfrtArray(ifrt::ArrayRef ifrt_array);\n \n   Storage& GetStorage();\n   const Storage& GetStorage() const;\n@@ -341,8 +339,7 @@ class PyArrayResultHandler {\n   PyArray Call(absl::Span<const PyArray> py_arrays) const;\n   PyArray Call(PyArray py_array) const;\n \n-  PyArray Call(nb_class_ptr<PyClient> py_client,\n-               tsl::RCReference<ifrt::Array> ifrt_array,\n+  PyArray Call(nb_class_ptr<PyClient> py_client, ifrt::ArrayRef ifrt_array,\n                xla::PjRtFuture<> result_status = xla::PjRtFuture<>()) const;\n \n  private:\ndiff --git a/jaxlib/py_executable.cc b/jaxlib/py_executable.cc\nindex 2902a2bd7be0..d79b236b9241 100644\n--- a/jaxlib/py_executable.cc\n+++ b/jaxlib/py_executable.cc\n@@ -140,8 +140,7 @@ static int GetNumDevices(const ExecuteShardedArg& arg) {\n     return std::get<std::vector<PyArray>>(arg).size();\n   }\n }\n-static tsl::RCReference<ifrt::Array> GetIfRtArray(\n-    const ExecuteShardedArg& arg) {\n+static ifrt::ArrayRef GetIfRtArray(const ExecuteShardedArg& arg) {\n   if (std::holds_alternative<PyArray>(arg)) {\n     return tsl::FormRef(std::get<PyArray>(arg).ifrt_array());\n   }\n@@ -151,7 +150,7 @@ static tsl::RCReference<ifrt::Array> GetIfRtArray(\n   // insufficient information about the shape (a dummy shape is used). This\n   // should be removed if possible and only be used in the context where the\n   // shape information is unused.\n-  std::vector<tsl::RCReference<ifrt::Array>> ifrt_arrays;\n+  std::vector<ifrt::ArrayRef> ifrt_arrays;\n   ifrt_arrays.reserve(arg_vector.size());\n   absl::InlinedVector<ifrt::Device*, 1> devices;\n   devices.reserve(arg_vector.size());\n@@ -177,11 +176,11 @@ static tsl::RCReference<ifrt::Array> GetIfRtArray(\n   return *ifrt_array;\n }\n \n-void PopulateExecuteShardedResults(\n-    const nb_class_ptr<PyClient>& client,\n-    std::vector<tsl::RCReference<ifrt::Array>> ifrt_arrays,\n-    const PjRtFuture<>& result_status, int num_computations,\n-    std::vector<std::vector<PyArray>>& outputs) {\n+void PopulateExecuteShardedResults(const nb_class_ptr<PyClient>& client,\n+                                   std::vector<ifrt::ArrayRef> ifrt_arrays,\n+                                   const PjRtFuture<>& result_status,\n+                                   int num_computations,\n+                                   std::vector<std::vector<PyArray>>& outputs) {\n   auto traceback = Traceback::Get();\n   DCHECK_GT(num_computations, 0);\n   int num_output_buffers = ifrt_arrays.size();\n@@ -206,7 +205,7 @@ absl::StatusOr<PyExecuteResults> ExecuteShardedOnLocalDevicesInternal(\n     ifrt::LoadedExecutable* ifrt_loaded_executable,\n     absl::Span<const ExecuteShardedArg> args,\n     std::optional<std::vector<PjRtFuture<>>>& returned_futures) {\n-  std::vector<tsl::RCReference<ifrt::Array>> output_arrays;\n+  std::vector<ifrt::ArrayRef> output_arrays;\n   std::unique_ptr<ifrt::Future<>> returned_future;\n   int num_computations = ifrt_loaded_executable->addressable_devices().size();\n   PjRtFuture<> result_status;\n@@ -224,7 +223,7 @@ absl::StatusOr<PyExecuteResults> ExecuteShardedOnLocalDevicesInternal(\n                           }));\n       }\n     }\n-    std::vector<tsl::RCReference<ifrt::Array>> arg_arrays(args.size());\n+    std::vector<ifrt::ArrayRef> arg_arrays(args.size());\n     absl::c_transform(args, arg_arrays.begin(),\n                       [&](const ExecuteShardedArg& arg) mutable {\n                         return GetIfRtArray(arg);\n@@ -257,10 +256,10 @@ absl::StatusOr<PyExecuteResults> ExecuteShardedOnLocalDevicesInternal(\n \n }  // namespace\n \n-PyExecuteResults::PyExecuteResults(\n-    const nb_class_ptr<PyClient>& client,\n-    std::vector<tsl::RCReference<ifrt::Array>> ifrt_arrays,\n-    int num_computations, PyShardedToken token, PjRtFuture<> result_status)\n+PyExecuteResults::PyExecuteResults(const nb_class_ptr<PyClient>& client,\n+                                   std::vector<ifrt::ArrayRef> ifrt_arrays,\n+                                   int num_computations, PyShardedToken token,\n+                                   PjRtFuture<> result_status)\n     : client_(client),\n       ifrt_arrays_(std::move(ifrt_arrays)),\n       num_computations_(num_computations),\n@@ -273,7 +272,7 @@ void PyExecuteResults::CheckNotDisassembled() const {\n   }\n }\n \n-std::vector<tsl::RCReference<ifrt::Array>> PyExecuteResults::Consume() {\n+std::vector<ifrt::ArrayRef> PyExecuteResults::Consume() {\n   CheckNotDisassembled();\n   is_exploded_ = true;\n   return std::move(ifrt_arrays_);\n@@ -306,7 +305,7 @@ PyExecuteResults::DisassemblePrefixIntoSingleDeviceArrays(size_t n) {\n                      ifrt_arrays_.size())\n             .c_str());\n   }\n-  std::vector<tsl::RCReference<ifrt::Array>> ifrt_arrays;\n+  std::vector<ifrt::ArrayRef> ifrt_arrays;\n   ifrt_arrays.reserve(ifrt_arrays_.size() - n);\n   for (size_t i = n; i < ifrt_arrays_.size(); ++i) {\n     ifrt_arrays.push_back(std::move(ifrt_arrays_[i]));\ndiff --git a/jaxlib/py_executable.h b/jaxlib/py_executable.h\nindex 9f8034ed2675..7e329410c763 100644\n--- a/jaxlib/py_executable.h\n+++ b/jaxlib/py_executable.h\n@@ -89,7 +89,7 @@ class PyShardedToken {\n class PyExecuteResults {\n  public:\n   PyExecuteResults(const nb_class_ptr<PyClient>& client,\n-                   std::vector<tsl::RCReference<ifrt::Array>> ifrt_arrays,\n+                   std::vector<ifrt::ArrayRef> ifrt_arrays,\n                    int num_computations, PyShardedToken token,\n                    PjRtFuture<> result_status = PjRtFuture<>());\n \n@@ -102,7 +102,7 @@ class PyExecuteResults {\n       std::vector<std::variant<const PyArrayResultHandler*, nanobind::object>>\n           out_handlers);\n \n-  std::vector<tsl::RCReference<ifrt::Array>> Consume();\n+  std::vector<ifrt::ArrayRef> Consume();\n \n   PyShardedToken ConsumeToken();\n \n@@ -117,7 +117,7 @@ class PyExecuteResults {\n   bool is_exploded_ = false;\n   bool token_consumed_ = false;\n   nb_class_ptr<PyClient> client_;\n-  std::vector<tsl::RCReference<ifrt::Array>> ifrt_arrays_;\n+  std::vector<ifrt::ArrayRef> ifrt_arrays_;\n   int num_computations_;\n   PyShardedToken token_;\n   // Only set if the computation has tokens.\ndiff --git a/jaxlib/py_socket_transfer.cc b/jaxlib/py_socket_transfer.cc\nindex a0bd943333ee..89900b02bd93 100644\n--- a/jaxlib/py_socket_transfer.cc\n+++ b/jaxlib/py_socket_transfer.cc\n@@ -112,7 +112,7 @@ absl::StatusOr<xla::PjRtMemorySpace*> MemorySpaceFromSharding(\n class IfrtArrayEntry : public PullTable::Entry {\n  public:\n   struct BufferRef {\n-    tsl::RCReference<xla::ifrt::Array> arr;\n+    xla::ifrt::ArrayRef arr;\n     xla::PjRtBuffer* buffer;\n     size_t buf_size;\n   };\n@@ -158,7 +158,7 @@ class IfrtArrayEntry : public PullTable::Entry {\n };\n \n absl::StatusOr<tsl::RCReference<IfrtArrayEntry>> CreatePullEntry(\n-    const std::vector<tsl::RCReference<xla::ifrt::Array>>& arrs,\n+    const std::vector<xla::ifrt::ArrayRef>& arrs,\n     std::shared_ptr<PremappedCopierState> state, size_t xfer_size) {\n   std::vector<IfrtArrayEntry::BufferRef> refs;\n   for (auto& arr : arrs) {\n@@ -228,8 +228,7 @@ class PyTransferServer {\n         server_->Connect(xla::ValueOrThrow(SocketAddress::Parse(saddr))));\n   }\n \n-  void AwaitPull(uint64_t uuid,\n-                 const std::vector<tsl::RCReference<xla::ifrt::Array>>& arrs) {\n+  void AwaitPull(uint64_t uuid, const std::vector<xla::ifrt::ArrayRef>& arrs) {\n     server_->AwaitPull(uuid, xla::ValueOrThrow(CreatePullEntry(\n                                  arrs, premapped_copier_, xfer_size_)));\n   }\n@@ -260,7 +259,7 @@ absl::StatusOr<xla::ifrt::ArraySpec> ArraySpecFromShapeDtypeStruct(\n }\n \n struct BufferSource {\n-  tsl::RCReference<xla::ifrt::Array> arr;\n+  xla::ifrt::ArrayRef arr;\n   xla::PjRtBuffer* buffer;\n };\n \n@@ -373,7 +372,7 @@ void RegisterTransferServerTypes(nanobind::module_& m) {\n       .def(\"_await_pull_flat\",\n            [](PyTransferServer& self, uint64_t uuid,\n               std::vector<xla::PyArray> inputs) {\n-             std::vector<tsl::RCReference<xla::ifrt::Array>> arrs;\n+             std::vector<xla::ifrt::ArrayRef> arrs;\n              arrs.reserve(inputs.size());\n              for (const xla::PyArray& input : inputs) {\n                arrs.push_back(tsl::FormRef(input.ifrt_array()));\ndiff --git a/jaxlib/py_values.cc b/jaxlib/py_values.cc\nindex 5225b67ee93b..81f6523d3e14 100644\n--- a/jaxlib/py_values.cc\n+++ b/jaxlib/py_values.cc\n@@ -81,7 +81,7 @@ namespace {\n // Prepared data for creating a single shard of an array. Holds a single-device\n // IFRT array or a host buffer.\n struct Shard {\n-  explicit Shard(tsl::RCReference<ifrt::Array> ifrt_array, bool weak_type)\n+  explicit Shard(ifrt::ArrayRef ifrt_array, bool weak_type)\n       : ifrt_array_or_host_buffer(std::move(ifrt_array)),\n         weak_type(weak_type),\n         // host_buffer_semantics is not meaningful when\n@@ -101,14 +101,13 @@ struct Shard {\n   Shard& operator=(Shard&&) noexcept = default;\n \n   bool is_ifrt_array() const {\n-    return std::holds_alternative<tsl::RCReference<ifrt::Array>>(\n-        ifrt_array_or_host_buffer);\n+    return std::holds_alternative<ifrt::ArrayRef>(ifrt_array_or_host_buffer);\n   }\n   ifrt::DType ifrt_dtype() const;\n   const ifrt::Shape& ifrt_shape() const;\n \n   // Points to the on-device array or on-host buffer.\n-  std::variant<tsl::RCReference<ifrt::Array>, ifrt::Client::HostBuffer>\n+  std::variant<ifrt::ArrayRef, ifrt::Client::HostBuffer>\n       ifrt_array_or_host_buffer;\n   bool weak_type;\n   ifrt::Client::HostBufferSemantics host_buffer_semantics;\n@@ -155,13 +154,12 @@ using DevicePutHandler = std::function<absl::StatusOr<ShardFn>(\n // buffer, and be not applied when reusing an existing IFRT array.\n //\n // Expected to be called without holding GIL.\n-absl::StatusOr<tsl::RCReference<ifrt::Array>>\n-MakeSingleDeviceIfrtArrayFromShard(\n+absl::StatusOr<ifrt::ArrayRef> MakeSingleDeviceIfrtArrayFromShard(\n     xla::ifrt::Client* ifrt_client, xla::ifrt::Device* ifrt_device,\n     xla::ifrt::MemoryKind ifrt_memory_kind, Shard& shard,\n     tsl::RCReference<ifrt::UserContext> user_context) {\n-  if (auto* ifrt_array = std::get_if<tsl::RCReference<ifrt::Array>>(\n-          &shard.ifrt_array_or_host_buffer)) {\n+  if (auto* ifrt_array =\n+          std::get_if<ifrt::ArrayRef>(&shard.ifrt_array_or_host_buffer)) {\n     return std::move(*ifrt_array);\n   } else {\n     auto host_buffer_shard = std::get<ifrt::Client::HostBuffer>(\n@@ -181,7 +179,7 @@ MakeSingleDeviceIfrtArrayFromShard(\n // path). `shards` will be consumed.\n //\n // Expected to be called without holding GIL.\n-absl::StatusOr<tsl::RCReference<ifrt::Array>> MakeIfrtArrayFromShardsInBatch(\n+absl::StatusOr<ifrt::ArrayRef> MakeIfrtArrayFromShardsInBatch(\n     ifrt::Client* ifrt_client, ifrt::DType ifrt_dtype, ifrt::Shape ifrt_shape,\n     ifrt::ShardingRef ifrt_sharding, absl::Span<Shard> shards,\n     tsl::RCReference<ifrt::UserContext> user_context) {\n@@ -221,8 +219,7 @@ absl::StatusOr<tsl::RCReference<ifrt::Array>> MakeIfrtArrayFromShardsInBatch(\n // `shards` will be consumed.\n //\n // Expected to be called without holding GIL.\n-absl::StatusOr<tsl::RCReference<ifrt::Array>>\n-MakeIfrtArrayFromShardsWithAssembly(\n+absl::StatusOr<ifrt::ArrayRef> MakeIfrtArrayFromShardsWithAssembly(\n     ifrt::Client* ifrt_client, ifrt::DType ifrt_dtype, ifrt::Shape ifrt_shape,\n     ifrt::ShardingRef ifrt_sharding,\n     ifrt::DeviceList* ifrt_addressable_device_list,\n@@ -230,10 +227,10 @@ MakeIfrtArrayFromShardsWithAssembly(\n     tsl::RCReference<ifrt::UserContext> user_context) {\n   absl::Span<ifrt::Device* const> ifrt_addressable_devices =\n       ifrt_addressable_device_list->devices();\n-  std::vector<tsl::RCReference<ifrt::Array>> ifrt_array_shards;\n+  std::vector<ifrt::ArrayRef> ifrt_array_shards;\n   ifrt_array_shards.reserve(shards.size());\n   for (int64_t i = 0; i < shards.size(); ++i) {\n-    TF_ASSIGN_OR_RETURN(tsl::RCReference<ifrt::Array> ifrt_array_shard,\n+    TF_ASSIGN_OR_RETURN(ifrt::ArrayRef ifrt_array_shard,\n                         MakeSingleDeviceIfrtArrayFromShard(\n                             ifrt_client, ifrt_addressable_devices[i],\n                             ifrt_memory_kind, shards[i], user_context));\n@@ -569,8 +566,7 @@ absl::StatusOr<ShardFn> HandlePyArray(nb::handle obj, ifrt::Client* client,\n \n ifrt::DType Shard::ifrt_dtype() const {\n   if (is_ifrt_array()) {\n-    return std::get<tsl::RCReference<ifrt::Array>>(ifrt_array_or_host_buffer)\n-        ->dtype();\n+    return std::get<ifrt::ArrayRef>(ifrt_array_or_host_buffer)->dtype();\n   } else {\n     return std::get<ifrt::Client::HostBuffer>(ifrt_array_or_host_buffer).dtype;\n   }\n@@ -578,8 +574,7 @@ ifrt::DType Shard::ifrt_dtype() const {\n \n const ifrt::Shape& Shard::ifrt_shape() const {\n   if (is_ifrt_array()) {\n-    return std::get<tsl::RCReference<ifrt::Array>>(ifrt_array_or_host_buffer)\n-        ->shape();\n+    return std::get<ifrt::ArrayRef>(ifrt_array_or_host_buffer)->shape();\n   } else {\n     return std::get<ifrt::Client::HostBuffer>(ifrt_array_or_host_buffer).shape;\n   }\n@@ -916,7 +911,7 @@ absl::StatusOr<DevicePutResult> DevicePutWithDevice(\n   nb::gil_scoped_release gil_release;\n \n   TF_ASSIGN_OR_RETURN(Shard shard, std::move(shard_fn)());\n-  TF_ASSIGN_OR_RETURN(tsl::RCReference<ifrt::Array> ifrt_array,\n+  TF_ASSIGN_OR_RETURN(ifrt::ArrayRef ifrt_array,\n                       MakeSingleDeviceIfrtArrayFromShard(\n                           ifrt_client, ifrt_device, ifrt_memory_kind, shard,\n                           std::move(ifrt_user_context)));\n@@ -1024,7 +1019,7 @@ absl::StatusOr<DevicePutResult> DevicePutWithSharding(\n         /*is_fully_replicated=*/false);\n   }\n \n-  tsl::RCReference<ifrt::Array> ifrt_array;\n+  ifrt::ArrayRef ifrt_array;\n   if (should_batch) {\n     TF_ASSIGN_OR_RETURN(ifrt_array,\n                         MakeIfrtArrayFromShardsInBatch(\ndiff --git a/jaxlib/py_values.h b/jaxlib/py_values.h\nindex 40b186fb7fc0..64a83aa66ab9 100644\n--- a/jaxlib/py_values.h\n+++ b/jaxlib/py_values.h\n@@ -38,7 +38,7 @@ limitations under the License.\n namespace xla {\n \n struct DevicePutResult {\n-  DevicePutResult(tsl::RCReference<ifrt::Array> ifrt_array, bool weak_type)\n+  DevicePutResult(ifrt::ArrayRef ifrt_array, bool weak_type)\n       : ifrt_array(std::move(ifrt_array)), weak_type(weak_type) {}\n \n   // Disallow copy. `DevicePutResult` is expected to be consumed by one user.\n@@ -48,7 +48,7 @@ struct DevicePutResult {\n   DevicePutResult& operator=(DevicePutResult&&) noexcept = default;\n \n   // Points to the on-device array.\n-  tsl::RCReference<ifrt::Array> ifrt_array;\n+  ifrt::ArrayRef ifrt_array;\n   bool weak_type;\n };\n \n\n```"
        ],
        "from_id": [
            "junwhanahn",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Declare `tpu.vector_load`, mirroring `tpu.vector_store`.\n\nAt the moment this op does not have any lowerings defined and should not be\nused.\n\nPiperOrigin-RevId: 755914724",
        "output": [
            "```diff\nCommit: 63d2f7d3956cec18aa214b0c2416681a0325f1f2\nDate: 2025-05-07T17:38:54Z\nURL: https://github.com/jax-ml/jax/commit/63d2f7d3956cec18aa214b0c2416681a0325f1f2\nFiles changed: 4\nAdditions: +319, Deletions: -0\ndiff --git a/jaxlib/mosaic/BUILD b/jaxlib/mosaic/BUILD\nindex a4123d0654bf..a212f7afb8bd 100644\n--- a/jaxlib/mosaic/BUILD\n+++ b/jaxlib/mosaic/BUILD\n@@ -232,6 +232,21 @@ cc_test(\n     ],\n )\n \n+cc_test(\n+    name = \"tpu_ops_verification_test\",\n+    srcs = [\"dialect/tpu/tpu_ops_verification_test.cc\"],\n+    deps = [\n+        \":tpu_dialect\",\n+        \"//testing/base/public:gunit_main\",\n+        \"@com_google_absl//absl/status\",\n+        \"@llvm-project//mlir:ArithDialect\",\n+        \"@llvm-project//mlir:IR\",\n+        \"@llvm-project//mlir:MemRefDialect\",\n+        \"@llvm-project//mlir:Support\",\n+        \"@xla//xla/mlir/utils:error_util\",\n+    ],\n+)\n+\n filegroup(\n     name = \"extension_srcs\",\n     srcs = [\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu.td b/jaxlib/mosaic/dialect/tpu/tpu.td\nindex 8b6e005e1719..c2d35f6f694a 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu.td\n+++ b/jaxlib/mosaic/dialect/tpu/tpu.td\n@@ -245,6 +245,33 @@ def TPU_VectorStoreOp :TPU_Op<\"vector_store\", [DefaultMemWrite, AttrSizedOperand\n   let hasVerifier = 1;\n }\n \n+// tpu.vector_load loads a vector from memory into a register.\n+//\n+//  base   : Memref to load from.\n+//  indices: Scalar indices into base. indices must be of the same rank as the\n+//           base memref shape.\n+//  strides: The stride to use for calculating the address of subsequent\n+//           elements. If left unspecified, the stride is implicitly 1 along\n+//           each dimension. Otherwise the stride must match the rank of the\n+//           memref shape.\n+//  mask   : Elementwise vector mask. Must be broadcastable to the shape of the\n+//           result vector. Depending on the core type, this may be a dynamic\n+//           (lane) mask consumed from a register or a static (sublane) mask\n+//           that must be the result of arith.constant.\n+def TPU_VectorLoadOp :TPU_Op<\"vector_load\", [DefaultMemRead, AttrSizedOperandSegments]> {\n+  let arguments = (ins\n+    AnyMemRef:$base,\n+    Variadic<Index>:$indices,\n+    DenseI32ArrayAttr:$strides,\n+    Optional<AnyVectorOfNonZeroRank>:$mask   // Elementwise mask.\n+  );\n+  let results = (outs AnyVectorOfNonZeroRank:$result);\n+  let assemblyFormat = [{\n+    $base `[` $indices `]` (`masked` $mask^)? attr-dict `:` type($base) `,` type($result) `,` type($mask)\n+  }];\n+  let hasVerifier = 1;\n+}\n+\n def TPU_StridedLoadOp : TPU_Op<\"strided_load\", [DefaultMemRead]> {\n   let arguments = (ins\n     AnyMemRef:$base,\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\nindex bbc5be3d125d..6c2e6b700bba 100644\n--- a/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n+++ b/jaxlib/mosaic/dialect/tpu/tpu_ops.cc\n@@ -26,6 +26,7 @@ limitations under the License.\n #include \"mlir/Dialect/Arith/IR/Arith.h\"\n #include \"mlir/Dialect/Func/IR/FuncOps.h\"\n #include \"mlir/Dialect/Math/IR/Math.h\"\n+#include \"mlir/Dialect/Vector/IR/VectorOps.h\"\n #include \"mlir/IR/Builders.h\"\n #include \"mlir/IR/BuiltinAttributes.h\"\n #include \"mlir/IR/BuiltinTypeInterfaces.h\"\n@@ -508,6 +509,36 @@ LogicalResult VectorStoreOp::verify() {\n   return success();\n }\n \n+LogicalResult VectorLoadOp::verify() {\n+  const MemRefType ref_ty = getBase().getType();\n+  if (!getStrides().empty()) {\n+    if (llvm::size(getStrides()) != ref_ty.getRank()) {\n+      return emitOpError(\"Expected \") << ref_ty.getRank() << \" strides.\";\n+    }\n+    return emitError(\"Not implemented: general vector load with strides.\");\n+  }\n+  const VectorType value_ty = getResult().getType();\n+\n+  if (value_ty.getElementType() != ref_ty.getElementType()) {\n+    return emitOpError(\"Expected base and result element type to match.\");\n+  }\n+  if (llvm::size(getIndices()) != ref_ty.getRank()) {\n+    return emitOpError(\"Expected \") << ref_ty.getRank() << \" indices.\";\n+  }\n+  if (getMask()) {\n+    if (value_ty.getElementTypeBitWidth() != 32) {\n+      return emitError(\n+          \"Not implemented: masked load with non-32-bit element type\");\n+    }\n+    if (vector::isBroadcastableTo(getMask().getType(), value_ty) !=\n+        vector::BroadcastableToResult::Success) {\n+      return emitOpError(\n+          \"Expected mask shape to be broadcastable to result shape.\");\n+    }\n+  }\n+  return success();\n+}\n+\n LogicalResult ReinterpretCastOp::verify() {\n   auto source_type = getMemRefType(getInput());\n   auto target_type = getType();\ndiff --git a/jaxlib/mosaic/dialect/tpu/tpu_ops_verification_test.cc b/jaxlib/mosaic/dialect/tpu/tpu_ops_verification_test.cc\nnew file mode 100644\nindex 000000000000..e92403c21ad0\n--- /dev/null\n+++ b/jaxlib/mosaic/dialect/tpu/tpu_ops_verification_test.cc\n@@ -0,0 +1,246 @@\n+/* Copyright 2025 The JAX Authors.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include <vector>\n+\n+#include <gmock/gmock.h>\n+#include <gtest/gtest.h>\n+#include \"absl/status/status.h\"\n+#include \"mlir/Dialect/Arith/IR/Arith.h\"\n+#include \"mlir/Dialect/MemRef/IR/MemRef.h\"\n+#include \"mlir/IR/Builders.h\"\n+#include \"mlir/IR/BuiltinAttributeInterfaces.h\"\n+#include \"mlir/IR/Diagnostics.h\"\n+#include \"mlir/IR/DialectRegistry.h\"\n+#include \"mlir/IR/ImplicitLocOpBuilder.h\"\n+#include \"mlir/IR/Location.h\"\n+#include \"mlir/IR/MLIRContext.h\"\n+#include \"mlir/IR/ValueRange.h\"\n+#include \"mlir/Support/LLVM.h\"\n+#include \"jaxlib/mosaic/dialect/tpu/tpu_dialect.h\"\n+#include \"xla/mlir/utils/error_util.h\"\n+\n+namespace mlir::tpu {\n+namespace {\n+\n+using ::testing::_;\n+using ::testing::HasSubstr;\n+using ::testing::status::StatusIs;\n+\n+class TpuOpsVerificationTest : public ::testing::Test {\n+ protected:\n+  TpuOpsVerificationTest()\n+      : context_([]() {\n+          DialectRegistry registry;\n+          registry\n+              .insert<arith::ArithDialect, memref::MemRefDialect, TPUDialect>();\n+          return registry;\n+        }()),\n+        builder_(UnknownLoc::get(&context_), &context_) {\n+    context_.loadAllAvailableDialects();\n+    context_.printOpOnDiagnostic(true);\n+  }\n+  ~TpuOpsVerificationTest() {\n+    for (int i = ops_.size() - 1; i >= 0; --i) {\n+      ops_[i]->erase();\n+    }\n+  }\n+\n+  template <typename OpTy, typename... Args>\n+  OpTy Create(Args&&... args) {\n+    OpTy op = builder_.create<OpTy>(std::forward<Args>(args)...);\n+    ops_.push_back(op.getOperation());\n+    return op;\n+  }\n+\n+  template <typename OpTy>\n+  absl::Status VerifyOp(OpTy op) {\n+    BaseScopedDiagnosticHandler diag(&context_);\n+    if (op.verify().succeeded()) {\n+      return absl::OkStatus();\n+    }\n+    return diag.ConsumeStatus();\n+  }\n+\n+  ImplicitLocOpBuilder& builder() { return builder_; }\n+\n+ private:\n+  MLIRContext context_;\n+  ImplicitLocOpBuilder builder_;\n+  std::vector<Operation*> ops_;\n+};\n+\n+TEST_F(TpuOpsVerificationTest, VectorLoadVerificationWorks) {\n+  auto c0 = Create<arith::ConstantIndexOp>(0);\n+  auto memref =\n+      Create<memref::AllocaOp>(MemRefType::get({8}, builder().getI32Type()));\n+  auto vl = Create<VectorLoadOp>(\n+      /*result=*/VectorType::get({8}, builder().getI32Type()),\n+      /*base=*/memref.getMemref(),\n+      /*indices=*/ValueRange{c0},\n+      /*strides=*/builder().getDenseI32ArrayAttr({}),\n+      /*mask=*/nullptr);\n+\n+  ASSERT_OK(VerifyOp(vl));\n+}\n+\n+TEST_F(TpuOpsVerificationTest,\n+       VectorLoadRankOfStridesDoesNotMatchBaseMemrefRank) {\n+  auto c0 = Create<arith::ConstantIndexOp>(0);\n+  auto memref =\n+      Create<memref::AllocaOp>(MemRefType::get({8}, builder().getI32Type()));\n+  auto vl = Create<VectorLoadOp>(\n+      /*result=*/VectorType::get({8}, builder().getI32Type()),\n+      /*base=*/memref.getMemref(),\n+      /*indices=*/ValueRange{c0},\n+      /*strides=*/builder().getDenseI32ArrayAttr({1, 1, 1, 1}),\n+      /*mask=*/nullptr);\n+  ASSERT_THAT(VerifyOp(vl), StatusIs(_, HasSubstr(\"Expected 1 strides.\")));\n+}\n+\n+TEST_F(TpuOpsVerificationTest, VectorLoadStridesFeatureNotImplemented) {\n+  auto c0 = Create<arith::ConstantIndexOp>(0);\n+  auto memref =\n+      Create<memref::AllocaOp>(MemRefType::get({8}, builder().getI32Type()));\n+  auto vl = Create<VectorLoadOp>(\n+      /*result=*/VectorType::get({8}, builder().getI32Type()),\n+      /*base=*/memref.getMemref(),\n+      /*indices=*/ValueRange{c0},\n+      /*strides=*/builder().getDenseI32ArrayAttr({1}),\n+      /*mask=*/nullptr);\n+  ASSERT_THAT(\n+      VerifyOp(vl),\n+      StatusIs(\n+          _, HasSubstr(\"Not implemented: general vector load with strides.\")));\n+}\n+\n+TEST_F(TpuOpsVerificationTest, VectorLoadBaseAndResultTypesDoNotMatch) {\n+  auto c0 = Create<arith::ConstantIndexOp>(0);\n+  auto memref =\n+      Create<memref::AllocaOp>(MemRefType::get({8}, builder().getI32Type()));\n+  auto vl = Create<VectorLoadOp>(\n+      /*result=*/VectorType::get({8}, builder().getF32Type()),\n+      /*base=*/memref.getMemref(),\n+      /*indices=*/ValueRange{c0},\n+      /*strides=*/builder().getDenseI32ArrayAttr({}),\n+      /*mask=*/nullptr);\n+\n+  ASSERT_THAT(\n+      VerifyOp(vl),\n+      StatusIs(_,\n+               HasSubstr(\"Expected base and result element type to match.\")));\n+}\n+\n+TEST_F(TpuOpsVerificationTest,\n+       VectorLoadRankOfIndicesDoesNotMatchBaseMemrefRank) {\n+  auto c0 = Create<arith::ConstantIndexOp>(0);\n+  auto memref =\n+      Create<memref::AllocaOp>(MemRefType::get({8}, builder().getI32Type()));\n+  auto vl = Create<VectorLoadOp>(\n+      /*result=*/VectorType::get({8}, builder().getI32Type()),\n+      /*base=*/memref.getMemref(),\n+      /*indices=*/ValueRange{c0, c0, c0},\n+      /*strides=*/builder().getDenseI32ArrayAttr({}),\n+      /*mask=*/nullptr);\n+\n+  ASSERT_THAT(VerifyOp(vl), StatusIs(_, HasSubstr(\"Expected 1 indices.\")));\n+}\n+\n+TEST_F(TpuOpsVerificationTest, VectorLoadValidMaskSucceeds) {\n+  auto c0 = Create<arith::ConstantIndexOp>(0);\n+  auto memref = Create<memref::AllocaOp>(\n+      MemRefType::get({8, 128}, builder().getI32Type()));\n+  auto mask = Create<arith::ConstantOp>(\n+      /*result=*/VectorType::get({8, 1}, builder().getI32Type()),\n+      /*value=*/dyn_cast<TypedAttr>(\n+          builder().getDenseI32ArrayAttr({1, 1, 1, 1, 1, 1, 1, 1})));\n+  auto vl = Create<VectorLoadOp>(\n+      /*result=*/VectorType::get({8, 128}, builder().getI32Type()),\n+      /*base=*/memref.getMemref(),\n+      /*indices=*/ValueRange{c0, c0},\n+      /*strides=*/builder().getDenseI32ArrayAttr({}),\n+      /*mask=*/mask.getResult());\n+\n+  ASSERT_OK(VerifyOp(vl));\n+}\n+\n+TEST_F(TpuOpsVerificationTest, VectorLoadMaskInvalidResultBitWidth) {\n+  auto c0 = Create<arith::ConstantIndexOp>(0);\n+  auto memref = Create<memref::AllocaOp>(\n+      MemRefType::get({8, 128}, builder().getI64Type()));\n+  auto mask = Create<arith::ConstantOp>(\n+      /*result=*/VectorType::get({8, 1}, builder().getI32Type()),\n+      /*value=*/dyn_cast<TypedAttr>(\n+          builder().getDenseI32ArrayAttr({1, 1, 1, 1, 1, 1, 1, 1})));\n+  auto vl = Create<VectorLoadOp>(\n+      /*result=*/VectorType::get({8, 128}, builder().getI64Type()),\n+      /*base=*/memref.getMemref(),\n+      /*indices=*/ValueRange{c0, c0},\n+      /*strides=*/builder().getDenseI32ArrayAttr({}),\n+      /*mask=*/mask.getResult());\n+\n+  ASSERT_THAT(\n+      VerifyOp(vl),\n+      StatusIs(\n+          _, HasSubstr(\n+                 \"Not implemented: masked load with non-32-bit element type\")));\n+}\n+\n+TEST_F(TpuOpsVerificationTest,\n+       VectorLoadMaskNotBroadcastableToResultShapeInvalidMinor) {\n+  auto c0 = Create<arith::ConstantIndexOp>(0);\n+  auto memref = Create<memref::AllocaOp>(\n+      MemRefType::get({8, 128}, builder().getI32Type()));\n+  auto mask = Create<arith::ConstantOp>(\n+      /*result=*/VectorType::get({8, 2}, builder().getI32Type()),\n+      /*value=*/dyn_cast<TypedAttr>(builder().getDenseI32ArrayAttr({1})));\n+  auto vl = Create<VectorLoadOp>(\n+      /*result=*/VectorType::get({8, 128}, builder().getI32Type()),\n+      /*base=*/memref.getMemref(),\n+      /*indices=*/ValueRange{c0, c0},\n+      /*strides=*/builder().getDenseI32ArrayAttr({}),\n+      /*mask=*/mask.getResult());\n+\n+  ASSERT_THAT(\n+      VerifyOp(vl),\n+      StatusIs(\n+          _, HasSubstr(\n+                 \"Expected mask shape to be broadcastable to result shape.\")));\n+}\n+\n+TEST_F(TpuOpsVerificationTest,\n+       VectorLoadMaskNotBroadcastableToResultShapeInvalidMajor) {\n+  auto c0 = Create<arith::ConstantIndexOp>(0);\n+  auto memref = Create<memref::AllocaOp>(\n+      MemRefType::get({8, 128}, builder().getI32Type()));\n+  auto mask = Create<arith::ConstantOp>(\n+      /*result=*/VectorType::get({5, 1}, builder().getI32Type()),\n+      /*value=*/dyn_cast<TypedAttr>(builder().getDenseI32ArrayAttr({1})));\n+  auto vl = Create<VectorLoadOp>(\n+      /*result=*/VectorType::get({8, 128}, builder().getI32Type()),\n+      /*base=*/memref.getMemref(),\n+      /*indices=*/ValueRange{c0, c0},\n+      /*strides=*/builder().getDenseI32ArrayAttr({}),\n+      /*mask=*/mask.getResult());\n+\n+  ASSERT_THAT(\n+      VerifyOp(vl),\n+      StatusIs(\n+          _, HasSubstr(\n+                 \"Expected mask shape to be broadcastable to result shape.\")));\n+}\n+\n+}  // namespace\n+}  // namespace mlir::tpu\n\n```"
        ],
        "from_id": [
            "nvgrw",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "Clean up `LoadedExecutable::Delete` and `LoadedExecutable::IsDeleted`\n\nThis API was added for symmetry with IFRT arrays, but is not used by anyone. Not exposing the \"deleted\" state for executables simplifies IFRT implementations. Users who want to delete executables (such as `PyLoadedExecutable` in JAX) can always just drop the reference to the executable.\n\nWith this, `LoadedExecutable` is completely immutable once constructed. This also helps with some cases where users want to share the same executable across many places and want to ensure that executables are never modified across these shared entities.\n\nIFRT Proxy's server-side logic now returns an `UNIMPLEMENTED` error for `Delete` and false for `IsDeleted`. This will not cause any disruption for existing workloads because no one is actually calling these methods outside tests.\n\nPiperOrigin-RevId: 755912978",
        "output": [
            "```diff\nCommit: 59f07c3bcb374b197f25137e09fa74c25c959efb\nDate: 2025-05-07T17:34:39Z\nURL: https://github.com/jax-ml/jax/commit/59f07c3bcb374b197f25137e09fa74c25c959efb\nFiles changed: 2\nAdditions: +6, Deletions: -15\ndiff --git a/jaxlib/py_client.cc b/jaxlib/py_client.cc\nindex 2cc84f0bf86c..ecd412ddbb99 100644\n--- a/jaxlib/py_client.cc\n+++ b/jaxlib/py_client.cc\n@@ -191,9 +191,7 @@ nb::list PyClient::LiveExecutables() {\n   nb::ft_lock_guard lock(executables_mutex_);\n   nb::list executables;\n   for (PyLoadedExecutable* exec = executables_; exec; exec = exec->next_) {\n-    if (!exec->is_deleted()) {\n-      executables.append(nb::find(exec));\n-    }\n+    executables.append(nb::find(exec));\n   }\n   return executables;\n }\n@@ -621,12 +619,10 @@ absl::StatusOr<nb::bytes> PyClient::HeapProfile() {\n \n   for (PyLoadedExecutable* executable = executables_; executable;\n        executable = executable->next_) {\n-    if (!executable->is_deleted()) {\n-      HeapProfileKey key{\n-          executable->traceback() ? executable->traceback()->get() : nullptr,\n-          executable->SizeOfGeneratedCodeInBytes(), nullptr};\n-      ++entries[key];\n-    }\n+    HeapProfileKey key{\n+        executable->traceback() ? executable->traceback()->get() : nullptr,\n+        executable->SizeOfGeneratedCodeInBytes(), nullptr};\n+    ++entries[key];\n   }\n \n   PprofProfileBuilder builder;\ndiff --git a/jaxlib/py_executable.h b/jaxlib/py_executable.h\nindex 5c7f57301b82..9f8034ed2675 100644\n--- a/jaxlib/py_executable.h\n+++ b/jaxlib/py_executable.h\n@@ -162,12 +162,7 @@ class PyLoadedExecutable {\n     return ifrt_loaded_executable_->GetCostAnalysis();\n   }\n \n-  void Delete() {\n-    // TODO(hyeontaek): Return absl::Status.\n-    TF_CHECK_OK(ifrt_loaded_executable_->Delete().Await());\n-  }\n-\n-  bool is_deleted() { return ifrt_loaded_executable_->IsDeleted(); }\n+  void Delete() {}  // Will be deleted.\n \n   // Takes args indexed by argid then deviceid, transposes them, and passes to\n   // PjRtExecutable::Execute. The result is similarly transposed back into the\n\n```"
        ],
        "from_id": [
            "junwhanahn",
            "Google-ML-Automation"
        ]
    },
    {
        "text_input": "add docs for multi-process run in Kubernete\n\nCo-authored-by: Michael Whittaker <mwhittaker@users.noreply.github.com>",
        "output": [
            "```diff\nCommit: 4a407bbd6e4467ece13094d634f4202a90a5d107\nDate: 2025-05-07T17:13:51Z\nURL: https://github.com/jax-ml/jax/commit/4a407bbd6e4467ece13094d634f4202a90a5d107\nFiles changed: 3\nAdditions: +89, Deletions: -2\ndiff --git a/.github/workflows/k8s.yaml b/.github/workflows/k8s.yaml\nindex 470a899a187e..5756b1afbbd2 100644\n--- a/.github/workflows/k8s.yaml\n+++ b/.github/workflows/k8s.yaml\n@@ -1,4 +1,4 @@\n-name: Distributed run using K8s Jobset\n+name: Multi-process run using K8s\n \n on:\n   push:\ndiff --git a/docs/multi_process.md b/docs/multi_process.md\nindex 8ecc51cc2557..f8c2566ca872 100644\n--- a/docs/multi_process.md\n+++ b/docs/multi_process.md\n@@ -307,6 +307,83 @@ what it prints:\n \n Woohoo, look at all those TPU cores!\n \n+### Kubernetes Example\n+\n+Running multi-controller JAX on a Kubernetes cluster is almost identical in spirit to the GPU and TPU examples above: every pod runs the same Python program, JAX discovers its peers, and the cluster behaves like one giant machine.\n+\n+1. **Container image** - start from a JAX-enabled image, e.g. one of the public JAX AI images on Google Artifact Registry ([TPU][google-artifact-tpu] / [GPU][google-artifact-gpu]) or NVIDIA ([NGC][nvidia-ngc] / [JAX-Toolbox][nvidia-jax-toolbox]).\n+\n+2. **Workload type** - use either a [JobSet][k8s-jobset] or an [indexed Job][k8s-indexed-job]. Each replica corresponds to one JAX process.\n+\n+3. **Service Account** - JAX needs permission to list the pods that belong to the job so that processes discover their peers. A minimal RBAC setup is provided in [examples/k8s/svc-acct.yaml][rbac-svc-acct].\n+\n+Below is a [minimal JobSet][minimal-jobset] that launches two replicas. Replace the placeholders - \n+image, GPU count, and any private registry secrets - with values that match your environment.\n+\n+```yaml\n+apiVersion: jobset.x-k8s.io/v1alpha2\n+kind: JobSet\n+metadata:\n+  name: jaxjob\n+spec:\n+  replicatedJobs:\n+  - name: workers\n+    template:\n+      spec:\n+        parallelism: 2\n+        completions: 2\n+        backoffLimit: 0\n+        template:\n+          spec:\n+            serviceAccountName: jax-job-sa  # kubectl apply -f svc-acct.yaml\n+            restartPolicy: Never\n+            imagePullSecrets:\n+              # https://k8s.io/docs/tasks/configure-pod-container/pull-image-private-registry/\n+            - name: null\n+            containers:\n+            - name: main\n+              image: null  # e.g. ghcr.io/nvidia/jax:jax\n+              imagePullPolicy: Always\n+              resources:\n+                limits:\n+                  cpu: 1\n+                  # https://k8s.io/docs/tasks/manage-gpus/scheduling-gpus/\n+                  nvidia.com/gpu: null\n+              command: \n+                - python\n+              args:\n+                - -c\n+                - |\n+                  import jax\n+                  jax.distributed.initialize()\n+                  print(jax.devices())\n+                  print(jax.local_devices())\n+                  assert jax.process_count() > 1\n+                  assert len(jax.devices()) > len(jax.local_devices())\n+```\n+\n+Apply the manifest and watch the pods complete:\n+\n+```bash\n+$ kubectl apply -f example.yaml\n+$ kubectl get pods -l jobset.sigs.k8s.io/jobset-name=jaxjob\n+NAME                       READY   STATUS      RESTARTS   AGE\n+jaxjob-workers-0-0-xpx8l   0/1     Completed   0          8m32s\n+jaxjob-workers-0-1-ddkq8   0/1     Completed   0          8m32s\n+```\n+\n+When the job finishes, inspect the logs to confirm that every process saw all accelerators:\n+\n+```bash\n+$ kubectl logs -l jobset.sigs.k8s.io/jobset-name=jaxjob\n+[CudaDevice(id=0), CudaDevice(id=1)]\n+[CudaDevice(id=0)]\n+[CudaDevice(id=0), CudaDevice(id=1)]\n+[CudaDevice(id=1)]\n+```\n+\n+Every pod should have the same set of global devices and a different set of local devices. At this point, you can replace the inline script with your real JAX program.\n+\n Once the processes are set up, we can start building global {class}`jax.Array`s\n and running computations. The remaining Python code examples in this tutorial\n are meant to be run on all processes simultaneously, after running\n@@ -580,3 +657,11 @@ assert (np.all(\n [distributed_arrays]: https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html\n [gpu_machines]: https://cloud.google.com/compute/docs/gpus\n [unified_sharding]: https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html\n+[google-artifact-tpu]: https://console.cloud.google.com/artifacts/docker/cloud-tpu-images/us/jax-ai-image/tpu\n+[google-artifact-gpu]: https://console.cloud.google.com/artifacts/docker/deeplearning-images/us-central1/jax-ai-image/gpu\n+[nvidia-ngc]: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/jax\n+[nvidia-jax-toolbox]: https://github.com/NVIDIA/JAX-Toolbox\n+[k8s-jobset]: https://github.com/kubernetes-sigs/jobset\n+[k8s-indexed-job]: https://kubernetes.io/docs/concepts/workloads/controllers/job/#parallel-jobs\n+[rbac-svc-acct]: https://github.com/jax-ml/jax/blob/main/examples/k8s/svc-acct.yaml\n+[minimal-jobset]: https://github.com/jax-ml/jax/blob/main/examples/k8s/example.yaml\ndiff --git a/jax/_src/clusters/k8s_cluster.py b/jax/_src/clusters/k8s_cluster.py\nindex 11f93e36f647..9520b947d9c5 100644\n--- a/jax/_src/clusters/k8s_cluster.py\n+++ b/jax/_src/clusters/k8s_cluster.py\n@@ -69,7 +69,9 @@ def _handle_api_exception(cls):\n           \"this job does not have the permission for pod introspection. Please \"\n           \"either grant the default SA permission to read pod info, or create a \"\n           \"dedicated service account with the permission and associated with \"\n-          \"the job. For more details, see <PLACERHOLDER_LINK>.\",\n+          \"the job. For an example on setting up the service account, see the \"\n+          \"example/k8s directory in the JAX repo. For more details, please refer to \"\n+          \"https://docs.jax.dev/en/latest/multi_process.html#kubernetes-example\",\n           width=80\n         ))\n       raise RuntimeError('\\n'.join(err_msg)) from e\n\n```"
        ],
        "from_id": [
            "yhtang"
        ]
    }
]