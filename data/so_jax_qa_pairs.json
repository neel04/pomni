[
    {
        "text_input": "<p>I am trying to install jaxlib on my windows 10 by the following command which I found on the documentation..</p>\n<blockquote>\n<p>pip install jaxlib</p>\n</blockquote>\n<p>It shows the following error</p>\n<pre><code>Collecting jaxlib\n  Could not find a version that satisfies the requirement jaxlib (from versions: None)\nNo matching distribution found for jaxlib\n</code></pre>\n",
        "output": "<p><em><strong>UPDATE 20240514</strong></em></p>\n<p>Jax is now supported on Windows, because <a href=\"https://pypi.org/project/jaxlib/#history\" rel=\"nofollow noreferrer\">Jaxlib</a> has been so since March 2023.</p>\n<hr>\n<p>Thanks to <a href=\"https://github.com/cloudhan\" rel=\"nofollow noreferrer\">cloudhan</a>'s <a href=\"https://github.com/cloudhan/jax-windows-builder\" rel=\"nofollow noreferrer\">jax-windows-builder</a>, it is now rather easy to install <a href=\"https://jax.readthedocs.io/en/latest/\" rel=\"nofollow noreferrer\">JAX and <code>jaxlib</code></a> on windows. E.g.</p>\n<pre><code>pip install jax==0.3.13 https://whls.blob.core.windows.net/unstable/cuda111/jaxlib-0.3.7+cuda11.cudnn82-cp38-none-win_amd64.whl\n</code></pre>\n<p>That's all. As explained <a href=\"https://github.com/cloudhan/jax-windows-builder#unstable-builds\" rel=\"nofollow noreferrer\">there</a>, I had to <a href=\"https://i.sstatic.net/uPT6u.jpg\" rel=\"nofollow noreferrer\">copy</a> the <em>jaxlib</em>'s link address of the <a href=\"https://realpython.com/python-wheels/\" rel=\"nofollow noreferrer\">whl file</a> I was interested in, i.e. the <code>https://whls.blob.core...</code> above). But I also had to take care that JAX's version and Jaxlib's were compatible, which compatibility is easy to assess at <a href=\"https://github.com/google/jax/releases\" rel=\"nofollow noreferrer\">github.com/google/jax/releases</a>: just pick the last two of each <em>(!)</em><sup><em>the version numbers! nothing to download from there.</em></sup></p>\n<blockquote>\n<p><em><strong>tested</strong></em> <em>with the versions explicited above</em>. <sub><em>I.e. python3.8-64</em> <strong>&amp;</strong> <em>jax==0.3.13</em> <strong>&amp;</strong> <em>jaxlib-0.3.7</em></sub></p>\n</blockquote>\n",
        "from_id": "N/A",
        "to_id": "4194079",
        "answer_votes": 14,
        "question_votes": 15,
        "question_id": 62585395,
        "answer_id": 72499565,
        "question_title": "Not able to install jaxlib",
        "question_link": "https://stackoverflow.com/questions/62585395/not-able-to-install-jaxlib",
        "answer_link": "https://stackoverflow.com/questions/62585395/not-able-to-install-jaxlib/72499565#72499565"
    },
    {
        "text_input": "<p>I am trying to install jaxlib on my windows 10 by the following command which I found on the documentation..</p>\n<blockquote>\n<p>pip install jaxlib</p>\n</blockquote>\n<p>It shows the following error</p>\n<pre><code>Collecting jaxlib\n  Could not find a version that satisfies the requirement jaxlib (from versions: None)\nNo matching distribution found for jaxlib\n</code></pre>\n",
        "output": "<p>Jaxlib is not supported on windows you can see it here..\n<a href=\"https://github.com/google/jax/issues/438\" rel=\"noreferrer\">https://github.com/google/jax/issues/438</a></p>\n",
        "from_id": "N/A",
        "to_id": "N/A",
        "answer_votes": 10,
        "question_votes": 15,
        "question_id": 62585395,
        "answer_id": 62585417,
        "question_title": "Not able to install jaxlib",
        "question_link": "https://stackoverflow.com/questions/62585395/not-able-to-install-jaxlib",
        "answer_link": "https://stackoverflow.com/questions/62585395/not-able-to-install-jaxlib/62585417#62585417"
    },
    {
        "text_input": "<p>I am trying to install jaxlib on my windows 10 by the following command which I found on the documentation..</p>\n<blockquote>\n<p>pip install jaxlib</p>\n</blockquote>\n<p>It shows the following error</p>\n<pre><code>Collecting jaxlib\n  Could not find a version that satisfies the requirement jaxlib (from versions: None)\nNo matching distribution found for jaxlib\n</code></pre>\n",
        "output": "<p>I went through the process the last two days myself so here is what i did:</p>\n<ol>\n<li><p>download and install the latest version of microsoft visual studio (\nto get a c++ compiler and toolchain)</p>\n</li>\n<li><p>download and install python</p>\n</li>\n<li><p>create a virtual python environment with the packages pip, setuptools, six, wheel and numpy (i did it in the GUI of pycharm)</p>\n</li>\n<li><p><a href=\"https://github.com/google/jax\" rel=\"noreferrer\">download jax</a></p>\n</li>\n</ol>\n<ol start=\"5\">\n<li>open up a windows powershell as administrator, change to the jax directory and complete the following steps (commands are in quotes)</li>\n</ol>\n<ul>\n<li>install chocolatey (package manager for easy bazel installation)</li>\n</ul>\n<blockquote>\n<p>Set-ExecutionPolicy Bypass -Scope Process -Force;\n[System.Net.ServicePointManager]::SecurityProtocol =\n[System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex\n((New-Object\nSystem.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))</p>\n</blockquote>\n<ul>\n<li>install bazel (for building jaxlib)</li>\n</ul>\n<blockquote>\n<p>choco install bazel</p>\n</blockquote>\n<ul>\n<li>install msys2 (linux utilities for bazel)</li>\n</ul>\n<blockquote>\n<p>choco install msys2</p>\n</blockquote>\n<ul>\n<li>permamently link the python environment in your powershell</li>\n</ul>\n<blockquote>\n<p>[System.Environment]::SetEnvironmentVariable(&quot;PATH&quot;, $Env:Path +\n&quot;;C:\\path\\to\\venv&quot;, &quot;Machine&quot;)</p>\n</blockquote>\n<ul>\n<li>still beeing in the jax folder in powershell actually build and compile jax</li>\n</ul>\n<blockquote>\n<p>./build/build.py</p>\n</blockquote>\n<ul>\n<li>after installation execute the command you're ask to to install jaxlib, it's something like</li>\n</ul>\n<blockquote>\n<p>pip install c:\\...\\jax\\dist\\jaxlib-0.1.72-cp39-none-win_amd64.whl</p>\n</blockquote>\n<ul>\n<li>and finally install jax with it</li>\n</ul>\n<blockquote>\n<p>pip install -e .</p>\n</blockquote>\n<p>This way it worked for me, since the documentation on the jax homepage utterly confused me.</p>\n",
        "from_id": "N/A",
        "to_id": "16989058",
        "answer_votes": 7,
        "question_votes": 15,
        "question_id": 62585395,
        "answer_id": 69308116,
        "question_title": "Not able to install jaxlib",
        "question_link": "https://stackoverflow.com/questions/62585395/not-able-to-install-jaxlib",
        "answer_link": "https://stackoverflow.com/questions/62585395/not-able-to-install-jaxlib/69308116#69308116"
    },
    {
        "text_input": "<p>I am trying to install jaxlib on my windows 10 by the following command which I found on the documentation..</p>\n<blockquote>\n<p>pip install jaxlib</p>\n</blockquote>\n<p>It shows the following error</p>\n<pre><code>Collecting jaxlib\n  Could not find a version that satisfies the requirement jaxlib (from versions: None)\nNo matching distribution found for jaxlib\n</code></pre>\n",
        "output": "<p>if someone still have same issue, jaxlib is not supported on alpine linux. If you're using a docker image try changing from alpine to debian based images</p>\n",
        "from_id": "N/A",
        "to_id": "12183880",
        "answer_votes": 0,
        "question_votes": 15,
        "question_id": 62585395,
        "answer_id": 79007921,
        "question_title": "Not able to install jaxlib",
        "question_link": "https://stackoverflow.com/questions/62585395/not-able-to-install-jaxlib",
        "answer_link": "https://stackoverflow.com/questions/62585395/not-able-to-install-jaxlib/79007921#79007921"
    },
    {
        "text_input": "<p>What are main differences between <a href=\"https://github.com/google/flax\" rel=\"noreferrer\">flax</a> and <a href=\"https://github.com/deepmind/dm-haiku\" rel=\"noreferrer\">dm-haiku</a>?</p>\n<p>From theirs descriptions:</p>\n<ul>\n<li>Flax, a neural network library for JAX</li>\n<li>Haiku, a neural network library for JAX inspired by Sonnet</li>\n</ul>\n<p><strong>Question</strong>:</p>\n<p>Which one jax-based library should I pick to implement, let's say <a href=\"https://arxiv.org/abs/1512.02595\" rel=\"noreferrer\">DeepSpeech</a> model (consists of CNN layers + LSTM layers + FC) and ctc-loss?</p>\n<hr />\n<p><strong>UPD</strong>.</p>\n<p>Found the <a href=\"https://github.com/deepmind/dm-haiku/issues/105#issuecomment-786570784\" rel=\"noreferrer\">explanation</a> about differences from the developer of dm-haiku:</p>\n<blockquote>\n<p>Flax is a bit more batteries included, and comes with optimizers, mixed precision and some training loops (I am told these are decoupled and you can use as much or as little as you want). Haiku aims to just solve NN modules and state management, it leaves other parts of the problem to other libraries (e.g. optax for optimization).</p>\n</blockquote>\n<blockquote>\n<p>Haiku is designed to be a port of Sonnet (a TF NN library) to JAX. So Haiku is a better choice if (like DeepMind) you have a significant amount of Sonnet+TF code that you might want to use in JAX and you want migrating that code (in either direction) to be as easy as possible.</p>\n</blockquote>\n<blockquote>\n<p>I think otherwise it comes down to personal preference. Within Alphabet there are 100s of researchers using each library so I don't think you can go wrong either way. At DeepMind we have standardised on Haiku because it makes sense for us. I would suggest taking a look at the example code provided by both libraries and seeing which matches your preferences for structuring experiments. I think you'll find that moving code from one library to another is not very complicated if you change your mind in the future.</p>\n</blockquote>\n<hr />\n<p>The original question is still relevant.</p>\n",
        "output": "<p>I recently encountered the same question, and I favored Haiku since I think their implementation (see <a href=\"https://github.com/google/flax/blob/master/flax/linen/linear.py\" rel=\"noreferrer\">Flax Dense()</a> versus <a href=\"https://github.com/deepmind/dm-haiku/blob/master/haiku/_src/basic.py\" rel=\"noreferrer\">Haiku Linear()</a> ) is closer to the original JAX spirit (i.e. chaining <code>init</code> and <code>predict</code> functions and keeping trace of the parameters in Pytrees) which makes it easier for me to modify things.</p>\n<p>But if you do not wish to modify things in depth, the best way to choose is to find a nice blog post on CNNs + LSTMs with Flax/Haiku and to stick with it. My general opinion is that both libraries are really close even if I prefer the more modular way Haiku ( + Optax + Rlax + Chex + ...) is built.</p>\n",
        "from_id": "3960038",
        "to_id": "6084245",
        "answer_votes": 8,
        "question_votes": 14,
        "question_id": 67319350,
        "answer_id": 67600645,
        "question_title": "What is the main difference between flax (google) and dm-haiku (deepmind)?",
        "question_link": "https://stackoverflow.com/questions/67319350/what-is-the-main-difference-between-flax-google-and-dm-haiku-deepmind",
        "answer_link": "https://stackoverflow.com/questions/67319350/what-is-the-main-difference-between-flax-google-and-dm-haiku-deepmind/67600645#67600645"
    },
    {
        "text_input": "<p>For python 3.8.8 and using the new mac air (with the m1 chip), in jupyter notebooks and in python terminal, <code>import jax</code> raises this error</p>\n<pre><code>Python 3.8.8 (default, Apr 13 2021, 12:59:45)\n[Clang 10.0.0 ] :: Anaconda, Inc. on darwin\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import jax\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;/Users/steve/Documents/code/jax/jax/__init__.py&quot;, line 37, in &lt;module&gt;\n    from . import config as _config_module\n  File &quot;/Users/steve/Documents/code/jax/jax/config.py&quot;, line 18, in &lt;module&gt;\n    from jax._src.config import config\n  File &quot;/Users/steve/Documents/code/jax/jax/_src/config.py&quot;, line 26, in &lt;module&gt;\n    from jax import lib\n  File &quot;/Users/steve/Documents/code/jax/jax/lib/__init__.py&quot;, line 63, in &lt;module&gt;\n    cpu_feature_guard.check_cpu_features()\nRuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.\n</code></pre>\n<p>I suspect it occurs because of the m1 chip.</p>\n<p>I tried using jax with <code>pip install jax</code>, then I built it from source as suggested by the comment, by cloning their repository and following the instructions given <a href=\"https://jax.readthedocs.io/en/latest/developer.html#building-from-source\" rel=\"noreferrer\">here</a>, but the same error message shows.</p>\n",
        "output": "<p>I had a similar problem. Since I already had Anaconda installed and didn't want to clutter up my space with Anaconda + miniconda + homebrew versions of python and package management and whatever, I hunted around for a simple solution. What ended up working for me was first uninstalling <code>jax</code> and <code>jaxlib</code> and then installing <code>jax</code> and <code>jaxlib</code> via conda-forge directly:</p>\n<pre><code>pip uninstall jax jaxlib\nconda install -c conda-forge jaxlib\nconda install -c conda-forge jax\n</code></pre>\n",
        "from_id": "13280748",
        "to_id": "2942906",
        "answer_votes": 10,
        "question_votes": 9,
        "question_id": 68327863,
        "answer_id": 71038502,
        "question_title": "importing jax fails on mac with m1 chip",
        "question_link": "https://stackoverflow.com/questions/68327863/importing-jax-fails-on-mac-with-m1-chip",
        "answer_link": "https://stackoverflow.com/questions/68327863/importing-jax-fails-on-mac-with-m1-chip/71038502#71038502"
    },
    {
        "text_input": "<p>For python 3.8.8 and using the new mac air (with the m1 chip), in jupyter notebooks and in python terminal, <code>import jax</code> raises this error</p>\n<pre><code>Python 3.8.8 (default, Apr 13 2021, 12:59:45)\n[Clang 10.0.0 ] :: Anaconda, Inc. on darwin\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import jax\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;/Users/steve/Documents/code/jax/jax/__init__.py&quot;, line 37, in &lt;module&gt;\n    from . import config as _config_module\n  File &quot;/Users/steve/Documents/code/jax/jax/config.py&quot;, line 18, in &lt;module&gt;\n    from jax._src.config import config\n  File &quot;/Users/steve/Documents/code/jax/jax/_src/config.py&quot;, line 26, in &lt;module&gt;\n    from jax import lib\n  File &quot;/Users/steve/Documents/code/jax/jax/lib/__init__.py&quot;, line 63, in &lt;module&gt;\n    cpu_feature_guard.check_cpu_features()\nRuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.\n</code></pre>\n<p>I suspect it occurs because of the m1 chip.</p>\n<p>I tried using jax with <code>pip install jax</code>, then I built it from source as suggested by the comment, by cloning their repository and following the instructions given <a href=\"https://jax.readthedocs.io/en/latest/developer.html#building-from-source\" rel=\"noreferrer\">here</a>, but the same error message shows.</p>\n",
        "output": "<p>Thanks <a href=\"https://stackoverflow.com/users/2937831/jakevdp\">@jakevdp</a> I looked at the issue you linked and found a workaround :</p>\n<p>Thanks to <a href=\"https://github.com/Noahyt?tab=overview&amp;from=2021-07-01&amp;to=2021-07-12\" rel=\"noreferrer\">Noah</a> who mentioned in <a href=\"https://github.com/google/jax/issues/5501\" rel=\"noreferrer\">issue #5501</a> that you could just use a previous version of jax and jaxlib, for my purposes <code>jaxlib==0.1.60</code> and <code>jax==0.2.10</code> work just fine!</p>\n",
        "from_id": "13280748",
        "to_id": "13280748",
        "answer_votes": 5,
        "question_votes": 9,
        "question_id": 68327863,
        "answer_id": 68345659,
        "question_title": "importing jax fails on mac with m1 chip",
        "question_link": "https://stackoverflow.com/questions/68327863/importing-jax-fails-on-mac-with-m1-chip",
        "answer_link": "https://stackoverflow.com/questions/68327863/importing-jax-fails-on-mac-with-m1-chip/68345659#68345659"
    },
    {
        "text_input": "<p>For python 3.8.8 and using the new mac air (with the m1 chip), in jupyter notebooks and in python terminal, <code>import jax</code> raises this error</p>\n<pre><code>Python 3.8.8 (default, Apr 13 2021, 12:59:45)\n[Clang 10.0.0 ] :: Anaconda, Inc. on darwin\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import jax\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;/Users/steve/Documents/code/jax/jax/__init__.py&quot;, line 37, in &lt;module&gt;\n    from . import config as _config_module\n  File &quot;/Users/steve/Documents/code/jax/jax/config.py&quot;, line 18, in &lt;module&gt;\n    from jax._src.config import config\n  File &quot;/Users/steve/Documents/code/jax/jax/_src/config.py&quot;, line 26, in &lt;module&gt;\n    from jax import lib\n  File &quot;/Users/steve/Documents/code/jax/jax/lib/__init__.py&quot;, line 63, in &lt;module&gt;\n    cpu_feature_guard.check_cpu_features()\nRuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.\n</code></pre>\n<p>I suspect it occurs because of the m1 chip.</p>\n<p>I tried using jax with <code>pip install jax</code>, then I built it from source as suggested by the comment, by cloning their repository and following the instructions given <a href=\"https://jax.readthedocs.io/en/latest/developer.html#building-from-source\" rel=\"noreferrer\">here</a>, but the same error message shows.</p>\n",
        "output": "<p>JAX does not yet provide pre-built jaxlib wheels that are compatible with M1 chips. The best source of information I know on building jaxlib on M1 is probably this github issue: <a href=\"https://github.com/google/jax/issues/5501\" rel=\"nofollow noreferrer\">https://github.com/google/jax/issues/5501</a>, which also tracks improving this support.</p>\n<p>Hopefully M1 support will be improved in the near future, but it's taking a while for the scientific computing infrastructure up and down the stack to catch up with the requirements of the new chips.</p>\n",
        "from_id": "13280748",
        "to_id": "2937831",
        "answer_votes": 2,
        "question_votes": 9,
        "question_id": 68327863,
        "answer_id": 68328780,
        "question_title": "importing jax fails on mac with m1 chip",
        "question_link": "https://stackoverflow.com/questions/68327863/importing-jax-fails-on-mac-with-m1-chip",
        "answer_link": "https://stackoverflow.com/questions/68327863/importing-jax-fails-on-mac-with-m1-chip/68328780#68328780"
    },
    {
        "text_input": "<p>For python 3.8.8 and using the new mac air (with the m1 chip), in jupyter notebooks and in python terminal, <code>import jax</code> raises this error</p>\n<pre><code>Python 3.8.8 (default, Apr 13 2021, 12:59:45)\n[Clang 10.0.0 ] :: Anaconda, Inc. on darwin\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\n&gt;&gt;&gt; import jax\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;/Users/steve/Documents/code/jax/jax/__init__.py&quot;, line 37, in &lt;module&gt;\n    from . import config as _config_module\n  File &quot;/Users/steve/Documents/code/jax/jax/config.py&quot;, line 18, in &lt;module&gt;\n    from jax._src.config import config\n  File &quot;/Users/steve/Documents/code/jax/jax/_src/config.py&quot;, line 26, in &lt;module&gt;\n    from jax import lib\n  File &quot;/Users/steve/Documents/code/jax/jax/lib/__init__.py&quot;, line 63, in &lt;module&gt;\n    cpu_feature_guard.check_cpu_features()\nRuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.\n</code></pre>\n<p>I suspect it occurs because of the m1 chip.</p>\n<p>I tried using jax with <code>pip install jax</code>, then I built it from source as suggested by the comment, by cloning their repository and following the instructions given <a href=\"https://jax.readthedocs.io/en/latest/developer.html#building-from-source\" rel=\"noreferrer\">here</a>, but the same error message shows.</p>\n",
        "output": "<p>As of now (January 2022), <code>jax</code> is available for M1 Macs. Make sure to uninstall <code>jax</code> and <code>jaxlib</code> and then install the new packages via pip:</p>\n<p><code>pip install --upgrade jax jaxlib</code></p>\n<p>Afterwards, you can use jax without problems.</p>\n<p>--Edit--\nI am running on a machine with the following specs:</p>\n<pre><code>ProductName:    macOS\nProductVersion: 12.1\nBuildVersion:   21C52\n</code></pre>\n<p>and with <code>Python 3.9.6</code> within a <code>conda</code> environment.</p>\n",
        "from_id": "13280748",
        "to_id": "12859833",
        "answer_votes": 1,
        "question_votes": 9,
        "question_id": 68327863,
        "answer_id": 70678153,
        "question_title": "importing jax fails on mac with m1 chip",
        "question_link": "https://stackoverflow.com/questions/68327863/importing-jax-fails-on-mac-with-m1-chip",
        "answer_link": "https://stackoverflow.com/questions/68327863/importing-jax-fails-on-mac-with-m1-chip/70678153#70678153"
    },
    {
        "text_input": "<p>This is related to <a href=\"https://stackoverflow.com/questions/65612989/jax-cannot-find-the-static-argnums\">this question</a>. After some work, I managed to change it down to the last error. The code looks like this now.</p>\n<pre><code>import jax.numpy as jnp\nfrom jax import grad, jit, value_and_grad\nfrom jax import vmap, pmap\nfrom jax import random\nimport jax\nfrom jax import lax\nfrom jax import custom_jvp\n\n\ndef p_tau(z, tau, alpha=1.5):\n    return jnp.clip((alpha - 1) * z - tau, 0) ** (1 / (alpha - 1))\n\n\ndef get_tau(tau, tau_max, tau_min, z_value):\n    return lax.cond(z_value &lt; 1,\n                    lambda _: (tau, tau_min),\n                    lambda _: (tau_max, tau),\n                    operand=None\n                    )\n\n\ndef body(kwargs, x):\n    tau_min = kwargs['tau_min']\n    tau_max = kwargs['tau_max']\n    z = kwargs['z']\n    alpha = kwargs['alpha']\n\n    tau = (tau_min + tau_max) / 2\n    z_value = p_tau(z, tau, alpha).sum()\n    taus = get_tau(tau, tau_max, tau_min, z_value)\n    tau_max, tau_min = taus[0], taus[1]\n    return {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, None\n\n@jax.partial(jax.jit, static_argnums=(2,))\ndef map_row(z_input, alpha, T):\n    z = (alpha - 1) * z_input\n\n    tau_min, tau_max = jnp.min(z) - 1, jnp.max(z) - z.shape[0] ** (1 - alpha)\n    result, _ = lax.scan(body, {'tau_min': tau_min, 'tau_max': tau_max, 'z': z, 'alpha': alpha}, xs=None,\n                         length=T)\n    tau = (result['tau_max'] + result['tau_min']) / 2\n    result = p_tau(z, tau, alpha)\n    return result / result.sum()\n\n@jax.partial(jax.jit, static_argnums=(1,3,))\ndef _entmax(input, axis=-1, alpha=1.5, T=20):\n    result = vmap(jax.partial(map_row, alpha, T), axis)(input)\n    return result\n\n@jax.partial(custom_jvp, nondiff_argnums=(1, 2, 3,))\ndef entmax(input, axis=-1, alpha=1.5, T=10):\n    return _entmax(input, axis, alpha, T)\n\n@jax.partial(jax.jit, static_argnums=(0,2,))    \ndef _entmax_jvp_impl(axis, alpha, T, primals, tangents):\n    input = primals[0]\n    Y = entmax(input, axis, alpha, T)\n    gppr = Y  ** (2 - alpha)\n    grad_output = tangents[0]\n    dX = grad_output * gppr\n    q = dX.sum(axis=axis) / gppr.sum(axis=axis)\n    q = jnp.expand_dims(q, axis=axis)\n    dX -= q * gppr\n    return Y, dX\n\n\n@entmax.defjvp\ndef entmax_jvp(axis, alpha, T, primals, tangents):\n    return _entmax_jvp_impl(axis, alpha, T, primals, tangents)\n\nimport numpy as np\ninput = jnp.array(np.random.randn(64, 10)).block_until_ready()\nweight = jnp.array(np.random.randn(64, 10)).block_until_ready()\n\ndef toy(input, weight):\n    return (weight*entmax(input, 0, 1.5, 20)).sum()\n\njax.jit(value_and_grad(toy))(input, weight)\n</code></pre>\n<p>This leads to (what I hope) is the final error, that is</p>\n<pre><code>Non-hashable static arguments are not supported, as this can lead to unexpected cache-misses. Static argument (index 2) of type &lt;class 'jax.interpreters.batching.BatchTracer'&gt; for function map_row is non-hashable.\n</code></pre>\n<p>This is very strange, as I think I have marked every everywhere <code>axis</code> appears to be static, yet it still tells me that it is traced.</p>\n",
        "output": "<p>When you write a <code>partial</code> function with positional arguments, those arguments are passed first. So this:</p>\n<pre class=\"lang-python prettyprint-override\"><code>jax.partial(map_row, alpha, T)\n</code></pre>\n<p>is essentially equivalent to this:</p>\n<pre class=\"lang-python prettyprint-override\"><code>lambda z_input: map_row(alpha, T, z_input)\n</code></pre>\n<p>Notice the incorrect order of the arguments – this is what's causing your error: you're passing <code>z_input</code>, a non-hashable tracer, to an argument that is expected to be static.</p>\n<p>You can fix this by replacing the <code>partial</code> statement above with:</p>\n<pre class=\"lang-python prettyprint-override\"><code>lambda z: map_row(z, alpha, T)\n</code></pre>\n<p>and then your code will run correctly.</p>\n",
        "from_id": "14786813",
        "to_id": "2937831",
        "answer_votes": 7,
        "question_votes": 9,
        "question_id": 65685211,
        "answer_id": 65685690,
        "question_title": "Non-hashable static arguments are not supported in Jax when using vmap",
        "question_link": "https://stackoverflow.com/questions/65685211/non-hashable-static-arguments-are-not-supported-in-jax-when-using-vmap",
        "answer_link": "https://stackoverflow.com/questions/65685211/non-hashable-static-arguments-are-not-supported-in-jax-when-using-vmap/65685690#65685690"
    },
    {
        "text_input": "<p>What is the correct method for using multiple CPU cores with <code>jax.pmap</code>?</p>\n<p>The following example creates an environment variable for SPMD on CPU core backends, tests that JAX recognises the devices, and attempts a device lock.</p>\n<pre><code>import os\nos.environ[&quot;XLA_FLAGS&quot;] = '--xla_force_host_platform_device_count=2'\n\nimport jax as jx\nimport jax.numpy as jnp\n\njx.local_device_count()\n# WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n# 2\n\njx.devices(&quot;cpu&quot;)\n# [CpuDevice(id=0), CpuDevice(id=1)]\n\ndef sfunc(x): while True: pass\n\njx.pmap(sfunc)(jnp.arange(2))\n</code></pre>\n<p>Executing from a jupyter kernel and observing <code>htop</code> shows that only one core is locked</p>\n<p><a href=\"https://i.sstatic.net/tB2dF.png\" rel=\"noreferrer\"><img src=\"https://i.sstatic.net/tB2dF.png\" alt=\"execute from jupyter kernel\" /></a></p>\n<p>I receive the same output from <code>htop</code> when omitting the first two lines and running:</p>\n<pre><code>$ env XLA_FLAGS=--xla_force_host_platform_device_count=2 python test.py\n</code></pre>\n<p>Replacing <code>sfunc</code> with</p>\n<pre><code>def sfunc(x): return 2.0*x\n</code></pre>\n<p>and calling</p>\n<pre><code>jx.pmap(sfunc)(jnp.arange(2))\n# ShardedDeviceArray([0., 2.], dtype=float32, weak_type=True)\n</code></pre>\n<p>does return a <code>SharedDeviecArray</code>.</p>\n<p>Clearly I am not correctly configuring JAX/XLA to use two cores. What am I missing and what can I do to diagnose the problem?</p>\n",
        "output": "<p>As far as I can tell, you are configuring the cores correctly (see e.g. <a href=\"https://github.com/google/jax/issues/2714\" rel=\"noreferrer\">Issue #2714</a>). The problem lies in your test function:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def sfunc(x): while True: pass\n</code></pre>\n<p>This function gets stuck in an infinite loop <em>at trace-time</em>, not at run-time. Tracing happens in your host Python process on a single CPU (see <a href=\"https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html\" rel=\"noreferrer\">How to think in JAX</a> for an introduction to the idea of tracing within JAX transformations).</p>\n<p>If you want to observe CPU usage at runtime, you'll have to use a function that finishes tracing and begins running. For that you could use any long-running function that actually produces results. Here is a simple example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def sfunc(x):\n  for i in range(100):\n    x = (x @ x)\n  return x\n\njx.pmap(sfunc)(jnp.zeros((2, 1000, 1000)))\n</code></pre>\n",
        "from_id": "18649992",
        "to_id": "2937831",
        "answer_votes": 5,
        "question_votes": 9,
        "question_id": 72328521,
        "answer_id": 75121769,
        "question_title": "JAX pmap with multi-core CPU",
        "question_link": "https://stackoverflow.com/questions/72328521/jax-pmap-with-multi-core-cpu",
        "answer_link": "https://stackoverflow.com/questions/72328521/jax-pmap-with-multi-core-cpu/75121769#75121769"
    },
    {
        "text_input": "<p>I am rewriting some code from pure Python to JAX. I have gotten to the point where in my old code, I was using Python's multiprocessing module to parallelize the evaluation of a function over all of the CPU cores in a single node as follows:</p>\n<pre class=\"lang-py prettyprint-override\"><code># start pool process \npool = multiprocessing.Pool(processes=10) # if node has 10 CPU cores, start 10 processes\n\n# use pool.map to evaluate function(input) for each input in parallel\n# suppose len(inputs) is very large and 10 inputs are processed in parallel at a time\n# store the results in a list called out\nout = pool.map(function,inputs)\n\n# close pool processes to free memory\npool.close()\npool.join()\n</code></pre>\n<p>I know that JAX has vmap and pmap, but I don't understand if either of those are a drop-in replacement for how I'm using multiprocessing.pool.map above.</p>\n<ol>\n<li>Is <code>vmap(function,in_axes=0)(inputs)</code> distributing to all available CPU cores or what?</li>\n<li>How is <code>pmap(function,in_axes=0)(inputs)</code> different from vmap and multiprocessing.pool.map?</li>\n<li>Is my usage of multiprocessing.pool.map above an example of a &quot;single-program, multiple-data (SPMD)&quot; code that pmap is meant for?</li>\n<li>When I actually do <code>pmap(function,in_axes=0)(inputs)</code> I get an error -- ValueError: compiling computation that requires 10 logical devices, but only 1 XLA devices are available (num_replicas=10, num_partitions=1) -- what does this mean?</li>\n<li>Finally, my use case is very simple: I merely want to use some/all of the CPU cores on a single node (e.g., all 10 CPU cores on my Macbook). But I have heard about nesting pmap(vmap) -- is this used to parallelize over the cores of multiple connected nodes (say on a supercomputer)? This would be more akin to mpi4py rather than multiprocessing (the latter is restricted to a single node).</li>\n</ol>\n",
        "output": "<blockquote>\n<ol>\n<li>Is <code>vmap(function,in_axes=0)(inputs)</code> distributing to all available CPU cores or what?</li>\n</ol>\n</blockquote>\n<p>No, <code>vmap</code> has nothing to do with parallelization. It is a vectorizing transformation, not a parallelizing transformation. In the course of normal operation, JAX may use multiple cores via XLA, so vmapped operations may also do this. But there's no explicit parallelization in <code>vmap</code>.</p>\n<blockquote>\n<ol start=\"2\">\n<li>How is <code>pmap(function,in_axes=0)(inputs)</code> different from <code>vmap</code> and <code>multiprocessing.pool.map</code>?</li>\n</ol>\n</blockquote>\n<p><code>pmap</code> parallelizes over multiple XLA devices. <code>vmap</code> does not parallelize, but rather vectorizes on a single device. <code>multiprocessing</code> parallelizes over multiple Python processes.</p>\n<blockquote>\n<ol start=\"3\">\n<li>Is my usage of multiprocessing.pool.map above an example of a &quot;single-program, multiple-data (SPMD)&quot; code that pmap is meant for?</li>\n</ol>\n</blockquote>\n<p>Yes, it could be described as SPMD across multiple python processes.</p>\n<blockquote>\n<ol start=\"4\">\n<li>When I actually do <code>pmap(function,in_axes=0)(inputs)</code> I get an error -- <code>ValueError: compiling computation that requires 10 logical devices, but only 1 XLA devices are available (num_replicas=10, num_partitions=1)</code> -- what does this mean?</li>\n</ol>\n</blockquote>\n<p><code>pmap</code> parallelizes over multiple XLA devices, and you have configured only a single XLA device, so the requested operation is not possible.</p>\n<blockquote>\n<ol start=\"5\">\n<li>Finally, my use case is very simple: I merely want to use some/all of the CPU cores on a single node (e.g., all 10 CPU cores on my Macbook). But I have heard about nesting pmap(vmap) -- is this used to parallelize over the cores of multiple connected nodes (say on a supercomputer)? This would be more akin to mpi4py rather than multiprocessing (the latter is restricted to a single node).</li>\n</ol>\n</blockquote>\n<p>Yes, I believe that <code>pmap</code> can be used to compute on multiple CPU cores. Whether it's nested with <code>vmap</code> is irrelevant. See <a href=\"https://stackoverflow.com/questions/72328521/jax-pmap-with-multi-core-cpu\">JAX pmap with multi-core CPU</a>.</p>\n<p>Note also that <code>jax.pmap</code> is deprecated in favor of the newer <code>jax.shard_map</code>, which is a much more flexible transformation for multi-device/multi-host computation. There's some info here: <a href=\"https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html\" rel=\"noreferrer\">https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html</a> and <a href=\"https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html\" rel=\"noreferrer\">https://jax.readthedocs.io/en/latest/jep/14273-shard-map.html</a></p>\n",
        "from_id": "18530984",
        "to_id": "2937831",
        "answer_votes": 12,
        "question_votes": 8,
        "question_id": 76536601,
        "answer_id": 76537362,
        "question_title": "JAX vmap vs pmap vs Python multiprocessing",
        "question_link": "https://stackoverflow.com/questions/76536601/jax-vmap-vs-pmap-vs-python-multiprocessing",
        "answer_link": "https://stackoverflow.com/questions/76536601/jax-vmap-vs-pmap-vs-python-multiprocessing/76537362#76537362"
    },
    {
        "text_input": "<p>I have been using TensorRT and TensorFlow-TRT to accelerate the inference of my DL algorithms.</p>\n<p>Then I have heard of:</p>\n<ul>\n<li>JAX <a href=\"https://github.com/google/jax\" rel=\"noreferrer\">https://github.com/google/jax</a></li>\n<li>Trax <a href=\"https://github.com/google/trax\" rel=\"noreferrer\">https://github.com/google/trax</a></li>\n</ul>\n<p>Both seem to accelerate DL. But I am having a hard time to understand them. Can anyone explain them in simple terms?</p>\n",
        "output": "<p><code>Trax</code> is a deep learning framework created by Google and extensively used by the Google Brain team. It comes as an alternative to <code>TensorFlow</code> and <code>PyTorch</code> when it comes to implementing off-the-shelf state of the art deep learning models, for example Transformers, Bert etc. , in principle with respect to the Natural Language Processing field.</p>\n<p><code>Trax</code> is built upon <code>TensorFlow</code> and <code>JAX</code>. <code>JAX</code> is an enhanced and optimised version of Numpy. The important distinction about <code>JAX</code> and <code>NumPy</code> is that the former using a library called XLA (advanced linear algebra) which allows to run your <code>NumPy</code> code on <code>GPU</code> and <code>TPU</code> rather than on <code>CPU</code> like it happens in the plain <code>NumPy</code>, thus speeding up computation.</p>\n",
        "from_id": "6106842",
        "to_id": "6117017",
        "answer_votes": 11,
        "question_votes": 8,
        "question_id": 60766116,
        "answer_id": 64694724,
        "question_title": "What is the difference between JAX, Trax, and TensorRT, in simple terms?",
        "question_link": "https://stackoverflow.com/questions/60766116/what-is-the-difference-between-jax-trax-and-tensorrt-in-simple-terms",
        "answer_link": "https://stackoverflow.com/questions/60766116/what-is-the-difference-between-jax-trax-and-tensorrt-in-simple-terms/64694724#64694724"
    },
    {
        "text_input": "<p>I am coding on a single-device laptop and I am using <code>jax.pmap</code> because my code will run on multiple TPUs. I would like to &quot;fake&quot; having multiple devices to test my code and try different things.</p>\n<p>Is there any way to achieve this? Thanks!</p>\n",
        "output": "<p>You can spoof multiple XLA devices backed by a single device by setting the following environment variable:</p>\n<pre><code>$ set XLA_FLAGS=&quot;--xla_force_host_platform_device_count=8&quot;\n</code></pre>\n<p>In Python, you could do it like this</p>\n<pre class=\"lang-py prettyprint-override\"><code># Note: must set this env variable before jax is imported\nimport os\nos.environ['XLA_FLAGS'] = &quot;--xla_force_host_platform_device_count=8&quot;\n\nimport jax\n\nprint(jax.devices())\n# [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3),\n#  CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]\n\nimport jax.numpy as jnp\nout = jax.pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)\n# [ 0  1  4  9 16 25 36 49]\n</code></pre>\n<p>Note that when a only a single physical device is present, all the &quot;devices&quot; here will be backed by the same threadpool. This will not improve performance of the code, but it can be useful for testing the semantics of parallel implementations on a single-device machine.</p>\n",
        "from_id": "9817677",
        "to_id": "2937831",
        "answer_votes": 9,
        "question_votes": 8,
        "question_id": 74466352,
        "answer_id": 74466666,
        "question_title": "Test jax.pmap before deploying on multi-device hardware",
        "question_link": "https://stackoverflow.com/questions/74466352/test-jax-pmap-before-deploying-on-multi-device-hardware",
        "answer_link": "https://stackoverflow.com/questions/74466352/test-jax-pmap-before-deploying-on-multi-device-hardware/74466666#74466666"
    },
    {
        "text_input": "<p>what are the differences between <code>jax.numpy.vectorize</code>and <code>jax.vmap</code>?\nHere is a small snipset</p>\n<pre><code>import jax\nimport jax.numpy as jnp\n\ndef f(x):\n     return jnp.exp(-x)*jnp.sin(x)\n\ngf = jax.grad(f)\nx = jnp.arange(0,1,0.1)\n\njax.vmap(gf)(x)\njnp.vectorize(gf)(x)\n</code></pre>\n<p>Both computations give the same results:</p>\n<p>DeviceArray([ 1.        ,  0.80998397,  0.63975394,  0.4888039 ,\n0.35637075,  0.24149445,  0.14307144,  0.05990037,\n-0.00927836, -0.06574923], dtype=float32)</p>\n<p>How to decide which one to use, and if there is a  difference in terms of performance?</p>\n",
        "output": "<p><code>jax.vmap</code> and <code>jax.numpy.vectorize</code> have quite different semantics, and only happen to be similar in the case of a single 1D input as in your example.</p>\n<p>The purpose of <code>jax.vmap</code> is to map a function over one or more inputs along <strong>a single explicit axis</strong>, as specified by the <code>in_axes</code> parameter. On the other hand, <code>jax.numpy.vectorize</code> maps a function over one or more inputs along <strong>zero or more implicit axes</strong> according to numpy broadcasting rules.</p>\n<p>To see the difference, let's pass two 2-dimensional inputs and print the shape within the function:</p>\n<pre class=\"lang-python prettyprint-override\"><code>import jax\nimport jax.numpy as jnp\n\ndef print_shape(x, y):\n  print(f&quot;x.shape = {x.shape}&quot;)\n  print(f&quot;y.shape = {y.shape}&quot;)\n  return x + y\n\nx = jnp.zeros((20, 10))\ny = jnp.zeros((20, 10))\n\n_ = jax.vmap(print_shape)(x, y)\n# x.shape = (10,)\n# y.shape = (10,)\n\n_ = jnp.vectorize(print_shape)(x, y)\n# x.shape = ()\n# y.shape = ()\n</code></pre>\n<p>Notice that <code>vmap</code> only maps along the first axis, while <code>vectorize</code> maps along <em>both</em> input axes.</p>\n<p>And notice also that the implicit mapping of <code>vectorize</code> means it can be used much more flexibly; for example:</p>\n<pre class=\"lang-python prettyprint-override\"><code>x2 = jnp.arange(10)\ny2 = jnp.arange(20).reshape(20, 1)\n\ndef add(x, y):\n  # vectorize always maps over all axes, such that the function is applied elementwise\n  assert x.shape == y.shape == ()\n  return x + y\n\njnp.vectorize(add)(x2, y2).shape\n# (20, 10)\n</code></pre>\n<p><code>vectorize</code> will iterate over all axes of the inputs according to numpy broadcasting rules. On the other hand, <code>vmap</code> cannot handle this by default:</p>\n<pre class=\"lang-python prettyprint-override\"><code>jax.vmap(add)(x2, y2)\n# ValueError: vmap got inconsistent sizes for array axes to be mapped:\n# arg 0 has shape (10,) and axis 0 is to be mapped\n# arg 1 has shape (20, 1) and axis 0 is to be mapped\n# so\n# arg 0 has an axis to be mapped of size 10\n# arg 1 has an axis to be mapped of size 20\n</code></pre>\n<p>To accomplish this same operation with <code>vmap</code> requires more thought, because there are two separate mapped axes, and some of the axes are broadcast. But you can accomplish the same thing this way:</p>\n<pre class=\"lang-python prettyprint-override\"><code>jax.vmap(jax.vmap(add, in_axes=(None, 0)), in_axes=(0, None))(x2, y2[:, 0]).shape\n# (20, 10)\n</code></pre>\n<p>This latter nested <code>vmap</code> is essentially what is happening under the hood when you use <code>jax.numpy.vectorize</code>.</p>\n<p>As for which to use in any given situation:</p>\n<ul>\n<li>if you want to map a function across a single, explicitly specified axis of the inputs, use <code>jax.vmap</code></li>\n<li>if you want a function's inputs to be mapped across zero or more axes according to numpy's broadcasting rules as applied to the input, use <code>jax.numpy.vectorize</code>.</li>\n<li>in situations where the transforms are identical (for example when mapping a function of 1D inputs) lean toward using <code>vmap</code>, because it more directly does what you want to do.</li>\n</ul>\n",
        "from_id": "11635654",
        "to_id": "2937831",
        "answer_votes": 11,
        "question_votes": 7,
        "question_id": 69099847,
        "answer_id": 69118863,
        "question_title": "Jax vectorization: vmap and/or numpy.vectorize?",
        "question_link": "https://stackoverflow.com/questions/69099847/jax-vectorization-vmap-and-or-numpy-vectorize",
        "answer_link": "https://stackoverflow.com/questions/69099847/jax-vectorization-vmap-and-or-numpy-vectorize/69118863#69118863"
    },
    {
        "text_input": "<p>When I run the stable diffusion on colab  <a href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb\" rel=\"noreferrer\">https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb</a><br />\nwith no modification, it fails on the line</p>\n<p><code>from diffusers import StableDiffusionPipeline </code></p>\n<p>The error log is</p>\n<p><code>AttributeError: module 'jax.random' has no attribute 'KeyArray'</code></p>\n<p>How can I fix this or any clue ?</p>\n<p>The import should work, the ipynb should run with no error.</p>\n",
        "output": "<p><code>jax.random.KeyArray</code> was <a href=\"https://jax.readthedocs.io/en/latest/changelog.html#jax-0-4-16-sept-18-2023\" rel=\"noreferrer\">deprecated in JAX v0.4.16</a> and <a href=\"https://jax.readthedocs.io/en/latest/changelog.html#jax-0-4-24-feb-6-2024\" rel=\"noreferrer\">removed in JAX v0.4.24</a>. Given this, it sounds like the HuggingFace stable diffusion code only works JAX v0.4.23 or earlier.</p>\n<p>You can install JAX v0.4.23 with GPU support like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pip install &quot;jax[cuda12_pip]==0.4.23&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n</code></pre>\n<p>or, if you prefer targeting a local CUDA installation, like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>pip install &quot;jax[cuda12_local]==0.4.23&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n</code></pre>\n<p>For more information on GPU installation, see <a href=\"https://jax.readthedocs.io/en/latest/installation.html#nvidia-gpu\" rel=\"noreferrer\">JAX Installation: NVIDIA GPU</a>.</p>\n<p>From the colab tutorial, update the second segment into:</p>\n<pre><code>!pip install &quot;jax[cuda12_local]==0.4.23&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n!pip install diffusers==0.11.1\n!pip install transformers scipy ftfy accelerate\n</code></pre>\n",
        "from_id": "14876868",
        "to_id": "2937831",
        "answer_votes": 10,
        "question_votes": 7,
        "question_id": 78302031,
        "answer_id": 78304430,
        "question_title": "Stable diffusion: AttributeError: module &#39;jax.random&#39; has no attribute &#39;KeyArray&#39;",
        "question_link": "https://stackoverflow.com/questions/78302031/stable-diffusion-attributeerror-module-jax-random-has-no-attribute-keyarray",
        "answer_link": "https://stackoverflow.com/questions/78302031/stable-diffusion-attributeerror-module-jax-random-has-no-attribute-keyarray/78304430#78304430"
    },
    {
        "text_input": "<p>When I run the stable diffusion on colab  <a href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb\" rel=\"noreferrer\">https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb</a><br />\nwith no modification, it fails on the line</p>\n<p><code>from diffusers import StableDiffusionPipeline </code></p>\n<p>The error log is</p>\n<p><code>AttributeError: module 'jax.random' has no attribute 'KeyArray'</code></p>\n<p>How can I fix this or any clue ?</p>\n<p>The import should work, the ipynb should run with no error.</p>\n",
        "output": "<p>In the end, we need to downgrade the jax,\nTry each from the lateset to ealier, and luckily it works for</p>\n<pre><code>jax==0.4.23 jaxlib==0.4.23\n</code></pre>\n",
        "from_id": "14876868",
        "to_id": "14876868",
        "answer_votes": 0,
        "question_votes": 7,
        "question_id": 78302031,
        "answer_id": 78302133,
        "question_title": "Stable diffusion: AttributeError: module &#39;jax.random&#39; has no attribute &#39;KeyArray&#39;",
        "question_link": "https://stackoverflow.com/questions/78302031/stable-diffusion-attributeerror-module-jax-random-has-no-attribute-keyarray",
        "answer_link": "https://stackoverflow.com/questions/78302031/stable-diffusion-attributeerror-module-jax-random-has-no-attribute-keyarray/78302133#78302133"
    },
    {
        "text_input": "<p>When I run the stable diffusion on colab  <a href=\"https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb\" rel=\"noreferrer\">https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb</a><br />\nwith no modification, it fails on the line</p>\n<p><code>from diffusers import StableDiffusionPipeline </code></p>\n<p>The error log is</p>\n<p><code>AttributeError: module 'jax.random' has no attribute 'KeyArray'</code></p>\n<p>How can I fix this or any clue ?</p>\n<p>The import should work, the ipynb should run with no error.</p>\n",
        "output": "<pre><code># Change this\n# !pip install diffusers==0.11.1\n\n# To just\n!pip install diffusers \n</code></pre>\n<p>If you've already run <code>pip install</code> in your Colab runtime, you'll need to either disconnect and open a new runtime (my recommendation) or use <code> --upgrade</code>.</p>\n<p>Diffusers v0.11.1 is now over 18 months old, and the notebook works with current v0.29.0 without any other changes. Instead of using an old version of diffusers, requiring an old version of jax, we can use the latest versions.</p>\n",
        "from_id": "14876868",
        "to_id": "1343535",
        "answer_votes": 0,
        "question_votes": 7,
        "question_id": 78302031,
        "answer_id": 78623944,
        "question_title": "Stable diffusion: AttributeError: module &#39;jax.random&#39; has no attribute &#39;KeyArray&#39;",
        "question_link": "https://stackoverflow.com/questions/78302031/stable-diffusion-attributeerror-module-jax-random-has-no-attribute-keyarray",
        "answer_link": "https://stackoverflow.com/questions/78302031/stable-diffusion-attributeerror-module-jax-random-has-no-attribute-keyarray/78623944#78623944"
    },
    {
        "text_input": "<p>I am using JAX, and I want to perform an operation like</p>\n<pre class=\"lang-py prettyprint-override\"><code>@jax.jit\ndef fun(x, index):\n    x[:index] = other_fun(x[:index])\n    return x\n</code></pre>\n<p>This cannot be performed under <code>jit</code>. Is there a way of doing this with <code>jax.ops</code> or <code>jax.lax</code>?\nI thought of using <code>jax.ops.index_update(x, idx, y)</code> but I cannot find a way of computing <code>y</code> without incurring in the same problem again.</p>\n",
        "output": "<p>The <a href=\"https://stackoverflow.com/a/68423274/2937831\">previous answer</a> by @rvinas using <code>dynamic_slice</code> works well if your index is static, but you can also accomplish this with a dynamic index using <code>jnp.where</code>. For example:</p>\n<pre class=\"lang-python prettyprint-override\"><code>import jax\nimport jax.numpy as jnp\n\ndef other_fun(x):\n    return x + 1\n\n@jax.jit\ndef fun(x, index):\n  mask = jnp.arange(x.shape[0]) &lt; index\n  return jnp.where(mask, other_fun(x), x)\n\nx = jnp.arange(5)\nprint(fun(x, 3))\n# [1 2 3 3 4]\n</code></pre>\n",
        "from_id": "8737016",
        "to_id": "2937831",
        "answer_votes": 7,
        "question_votes": 7,
        "question_id": 68419632,
        "answer_id": 68487021,
        "question_title": "Apply function only on slice of array under jit",
        "question_link": "https://stackoverflow.com/questions/68419632/apply-function-only-on-slice-of-array-under-jit",
        "answer_link": "https://stackoverflow.com/questions/68419632/apply-function-only-on-slice-of-array-under-jit/68487021#68487021"
    },
    {
        "text_input": "<p>I am using JAX, and I want to perform an operation like</p>\n<pre class=\"lang-py prettyprint-override\"><code>@jax.jit\ndef fun(x, index):\n    x[:index] = other_fun(x[:index])\n    return x\n</code></pre>\n<p>This cannot be performed under <code>jit</code>. Is there a way of doing this with <code>jax.ops</code> or <code>jax.lax</code>?\nI thought of using <code>jax.ops.index_update(x, idx, y)</code> but I cannot find a way of computing <code>y</code> without incurring in the same problem again.</p>\n",
        "output": "<p>It seems there are two issues in your implementation. First, the slices are producing dynamically shaped arrays (not allowed in jitted code). Second, unlike numpy arrays, JAX arrays are immutable (i.e. the contents of the array cannot be changed).</p>\n<p>You can overcome the two problems by combining <code>static_argnums</code> and <a href=\"https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.dynamic_update_slice.html\" rel=\"nofollow noreferrer\"><code>jax.lax.dynamic_update_slice</code></a>. Here is an example:</p>\n<pre><code>def other_fun(x):\n    return x + 1\n\n@jax.partial(jax.jit, static_argnums=(1,))\ndef fun(x, index):\n    update = other_fun(x[:index])\n    return jax.lax.dynamic_update_slice(x, update, (0,))\n\nx = jnp.arange(5)\nprint(fun(x, 3))  # prints [1 2 3 3 4]\n</code></pre>\n<p>Essentially, the example above uses <code>static_argnums</code> to indicate that the function should be recompiled for different <code>index</code> values and <code>jax.lax.dynamic_update_slice</code> creates a copy of <code>x</code> with updated values at <code>:len(update)</code>.</p>\n",
        "from_id": "8737016",
        "to_id": "5140223",
        "answer_votes": 3,
        "question_votes": 7,
        "question_id": 68419632,
        "answer_id": 68423274,
        "question_title": "Apply function only on slice of array under jit",
        "question_link": "https://stackoverflow.com/questions/68419632/apply-function-only-on-slice-of-array-under-jit",
        "answer_link": "https://stackoverflow.com/questions/68419632/apply-function-only-on-slice-of-array-under-jit/68423274#68423274"
    },
    {
        "text_input": "<p>Nice evening everyone,</p>\n<p>i spent the last 6 hours trying to debug seemingly randomly occuring NaN-values in Jax.\nI have narrowed down that the NaNs initially stem from either the loss-function or its gradient.</p>\n<p>A <strong>minimal-notebook that reproduces the error</strong> is available here <a href=\"https://colab.research.google.com/drive/1uXa-igMm9QBOOl8ZNdK1OkwxRFlLqvZD?usp=sharing\" rel=\"noreferrer\">https://colab.research.google.com/drive/1uXa-igMm9QBOOl8ZNdK1OkwxRFlLqvZD?usp=sharing</a></p>\n<p>This might also be interesting as a use-case for Jax. I use Jax to solve an <strong>orientation estimation task</strong> when only a limited amount of gyro-/acc-measurements is available. Here an efficient implementation of quaternion-operations is nice.</p>\n<p>The training-loop starts off fine but eventually diverges</p>\n<pre class=\"lang-py prettyprint-override\"><code>Step 0| Loss: 4.550444602966309 | Time: 13.910547971725464s\nStep 1| Loss: 4.110116481781006 | Time: 5.478027105331421s\nStep 2| Loss: 3.7159230709075928 | Time: 5.476970911026001s\nStep 3| Loss: 3.491917371749878 | Time: 5.474078416824341s\nStep 4| Loss: 3.232130765914917 | Time: 5.433410406112671s\nStep 5| Loss: 3.095140218734741 | Time: 5.433837413787842s\nStep 6| Loss: 2.9580295085906982 | Time: 5.429029941558838s\nStep 7| Loss: nan | Time: 5.427825689315796s\nStep 8| Loss: nan | Time: 5.463077545166016s\nStep 9| Loss: nan | Time: 5.479652643203735s\n</code></pre>\n<p>This can be traced back by diverging gradients as can be seen from the following snippet</p>\n<pre class=\"lang-py prettyprint-override\"><code>(loss, _), grads = loss_fn(params, X[0], y[0], rnn.reset_carry(bs=2))\n\ngrads[&quot;params&quot;][&quot;Dense_0&quot;][&quot;bias&quot;] # shape=(bs, out_features)\nDeviceArray([[-0.38666773,         nan, -1.0433975 ,         nan],\n             [ 0.623061  , -0.20950513,  0.8459796 , -0.42356613]],            dtype=float32)\n</code></pre>\n<h3>My question is: How to debug this?</h3>\n<h4>Enabling NaN-debugging</h4>\n<p>Enabling nan-debugging did not really help as it only ended up leading to huge stacktraces with many hidden traces ..</p>\n<pre class=\"lang-py prettyprint-override\"><code>from jax.config import config\nconfig.update(&quot;jax_debug_nans&quot;, True)\n</code></pre>\n<p>Any help would be much appreciated!\nThanks :)</p>\n",
        "output": "<p>A few approaches (decently documented in the main docs) may work:</p>\n<ol>\n<li>As a hotfix, switching to <code>float64</code> can do the trick. More info <a href=\"https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#nans\" rel=\"noreferrer\">here</a>: <code>jax.config.update(&quot;jax_enable_x64&quot;, True)</code></li>\n<li>Gradient Clipping is All You Need (<a href=\"https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html#gradient-clipping\" rel=\"noreferrer\">docs</a>)</li>\n<li>You can sometimes <a href=\"https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html#numerical-stability\" rel=\"noreferrer\">implement your own backprop</a>, this can help when e.g. you combine 2 functions that saturate into one that doesn't, or to enforce values at singularities.</li>\n<li>Diagnose your backprop by inspecting the computational graph. Usually look for divisions, signaled with the <code>div</code> token:</li>\n</ol>\n<pre class=\"lang-py prettyprint-override\"><code>from jax import make_jaxpr\n\n# If grad_fn(x) gives you trouble, you can inspect the computation as follows:\ngrad_fn = jit(value_and_grad(my_forward_prop, argnums=0))\nmake_jaxpr(grad_fn)(x)\n</code></pre>\n<p>Note that the community is quite active and some support has been and is being added to diagnose <code>NaNs</code>:</p>\n<ul>\n<li>The <a href=\"https://github.com/google/jax/issues/475\" rel=\"noreferrer\">&quot;jax_debug_nans&quot;</a> config flag</li>\n<li>More <a href=\"https://github.com/google/jax/issues?q=is%3Aissue+nan\" rel=\"noreferrer\">threads</a></li>\n</ul>\n<p>Hope this helps!<br />\nAndres</p>\n",
        "from_id": "14294750",
        "to_id": "4511978",
        "answer_votes": 7,
        "question_votes": 7,
        "question_id": 70057388,
        "answer_id": 71228272,
        "question_title": "Jax - Debugging NaN-values",
        "question_link": "https://stackoverflow.com/questions/70057388/jax-debugging-nan-values",
        "answer_link": "https://stackoverflow.com/questions/70057388/jax-debugging-nan-values/71228272#71228272"
    },
    {
        "text_input": "<p>I'm trying to understand JAX's auto-vectorization capabilities using <code>vmap</code> and implemented a minimal working example based on JAX's documentation.</p>\n<p>I don't understand how <code>in_axes</code> is used correctly. In the example below I can set <code>in_axes=(None, 0)</code> or <code>in_axes=(None, 1)</code> leading to the same results. Why is that the case?</p>\n<p>And why do I have to use <code>in_axes=(None, 0)</code> and not something like <code>in_axes=(0, )</code>?</p>\n<pre><code>import jax.numpy as jnp\nfrom jax import vmap\n\n\ndef predict(params, input_vec):\n    assert input_vec.ndim == 1\n    activations = input_vec\n    for W, b in params:\n        outputs = jnp.dot(W, activations) + b\n        activations = jnp.tanh(outputs)\n    return outputs\n\n\nif __name__ == &quot;__main__&quot;:\n\n    # Parameters\n    dims = [2, 3, 5]\n    input_dims = dims[0]\n    batch_size = 2\n\n    # Weights\n    params = list()\n    for dims_in, dims_out in zip(dims, dims[1:]):\n        params.append((jnp.ones((dims_out, dims_in)), jnp.ones((dims_out,))))\n\n    # Input data\n    input_batch = jnp.ones((batch_size, input_dims))\n\n    # With vmap\n    predictions = vmap(predict, in_axes=(None, 0))(params, input_batch)\n    print(predictions)\n</code></pre>\n",
        "output": "<p><code>in_axes=(None, 0)</code> means that the first argument (here <code>params</code>) will not be mapped, while the second argument (here <code>input_vec</code>) will be mapped along axis 0.</p>\n<blockquote>\n<p>In the example below I can set <code>in_axes=(None, 0)</code> or <code>in_axes=(None, 1)</code> leading to the same results. Why is that the case?</p>\n</blockquote>\n<p>This is because <code>input_vec</code> is a 2x2 matrix of ones, so whether you map along axis 0 or axis 1, the input vectors are length-2 vectors of ones. In more general cases, the two specifications are not equivalent, which you can see by either (1) making <code>batch_size</code> differ from <code>input_dims[0]</code>, or (2) filling your arrays with non-constant values.</p>\n<blockquote>\n<p>why do I have to use <code>in_axes=(None, 0)</code> and not something like <code>in_axes=(0, )</code>?</p>\n</blockquote>\n<p>If you set <code>in_axes=(0, )</code> for a function with two arguments, you get an error because the length of the <code>in_axes</code> tuple must match the number of arguments passed to the function. That said, it is possible to pass a scalar <code>in_axes=0</code> as a shorthand for <code>in_axes=(0, 0)</code>, but for your function this would lead to a shape error because the leading dimension of the arrays in <code>params</code> does not match the leading dimension of <code>input_vec</code>.</p>\n",
        "from_id": "3861775",
        "to_id": "2937831",
        "answer_votes": 4,
        "question_votes": 7,
        "question_id": 70564419,
        "answer_id": 70566823,
        "question_title": "in_axes keyword in JAX&#39;s vmap",
        "question_link": "https://stackoverflow.com/questions/70564419/in-axes-keyword-in-jaxs-vmap",
        "answer_link": "https://stackoverflow.com/questions/70564419/in-axes-keyword-in-jaxs-vmap/70566823#70566823"
    },
    {
        "text_input": "<p>The <a href=\"https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html#to-jit-or-not-to-jit\" rel=\"noreferrer\">documentation for JAX</a> says,</p>\n<blockquote>\n<p>Not all JAX code can be JIT compiled, as it requires array shapes to be static &amp; known at compile time.</p>\n</blockquote>\n<p>Now I am somewhat surprised because tensorflow has operations like <code>tf.boolean_mask</code> that does what JAX seems incapable of doing when compiled.</p>\n<ol>\n<li>Why is there such a regression from Tensorflow? I was under the assumption that the underlying XLA representation was shared between the two frameworks, but I may be mistaken. I don't recall Tensorflow ever having troubles with dynamic shapes, and functions such as <code>tf.boolean_mask</code> have been around forever.</li>\n<li>Can we expect this gap to close in the future? If not, why makes it impossible to do in JAX' jit what Tensorflow (among others) enables?</li>\n</ol>\n<p><strong>EDIT</strong></p>\n<p>The gradient passes through <code>tf.boolean_mask</code> (obviously not on mask values, which are discrete); case in point here using TF1-style graphs where values are unknown, so TF cannot rely on them:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nx1 = tf.placeholder(tf.float32, (3,))\nx2 = tf.placeholder(tf.float32, (3,))\ny = tf.boolean_mask(x1, x2 &gt; 0)\nprint(y.shape)  # prints &quot;(?,)&quot;\ndydx1, dydx2 = tf.gradients(y, [x1, x2])\nassert dydx1 is not None and dydx2 is None\n</code></pre>\n",
        "output": "<p><strong>Currently, you can't</strong> (<em>as discussed here</em>)</p>\n<p>This is not a limitation of JAX jit vs TensorFlow, but a limitation of XLA or rather how the two compile.</p>\n<p>JAX uses simply XLA to compile the function. XLA <em>needs to know</em> the static shape. That's an inherent design choice <em>within XLA</em>.</p>\n<p>TensorFlow uses the <code>function</code>: this creates a graph which can have shapes that are not statically known. This is not as efficient as using XLA, but still fine. However, <code>tf.function</code> offers an option <a href=\"https://www.tensorflow.org/xla/tutorials/jit_compile\" rel=\"nofollow noreferrer\"><code>jit_compile</code></a>, which will compile the graph inside the function with XLA. While this offers often a decent speedup (for free), it comes with restrictions: shapes need to be statically known (surprise, surprise,...)</p>\n<p>This is overall not too surprising behavior: computations in computers are in general faster (given a decent optimizer went over it) <em>the more is previously known</em> as more parameters (memory layout,...) can be optimally scheduled. The less is known, the slower the code (on this end is normal Python).</p>\n",
        "from_id": "9973879",
        "to_id": "4360557",
        "answer_votes": 4,
        "question_votes": 7,
        "question_id": 66711706,
        "answer_id": 70174417,
        "question_title": "Jax, jit and dynamic shapes: a regression from Tensorflow?",
        "question_link": "https://stackoverflow.com/questions/66711706/jax-jit-and-dynamic-shapes-a-regression-from-tensorflow",
        "answer_link": "https://stackoverflow.com/questions/66711706/jax-jit-and-dynamic-shapes-a-regression-from-tensorflow/70174417#70174417"
    },
    {
        "text_input": "<p>This <a href=\"https://github.com/google/jax/issues/2940\" rel=\"noreferrer\">Github issue</a> hints that there are tradeoffs in performance / memory / compilation time when choosing between <code>jax.lax.map</code> and <code>jax.vmap</code>. What are the specific details of these tradeoffs with respect to both GPUs and CPUs?</p>\n",
        "output": "<p>The main difference is that <code>jax.vmap</code> is a vectorizing transformation, while <code>lax.map</code> is an iterative transformation. Let's look at an example.</p>\n<h3>Example function: <code>vector_dot</code></h3>\n<p>Suppose you have implemented a simple function that takes 1D vectors as inputs. For simplicity let's make it a simple dot product, but one that asserts the inputs are one-dimensional:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import jax\nimport jax.numpy as jnp\nimport numpy as np\n\ndef vector_dot(x, y):\n  assert x.ndim == y.ndim == 1, &quot;vector inputs required&quot;\n  return jnp.dot(x, y)\n</code></pre>\n<p>We can create some random 1D vectors to test this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>rng = np.random.default_rng(8675309)\nx = rng.uniform(size=50)\ny = rng.uniform(size=50)\n\nprint(vector_dot(x, y))\n# 14.919376\n</code></pre>\n<p>To see what JAX is doing with this function under the hood, we can print the <a href=\"https://jax.readthedocs.io/en/latest/jaxpr.html\" rel=\"noreferrer\"><code>jaxpr</code></a>, which is JAX's intermediate-level representation of a function:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(jax.make_jaxpr(vector_dot)(x, y))\n# { lambda ; a:f32[50] b:f32[50]. let\n#     c:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] a b\n#   in (c,) }\n</code></pre>\n<p>This shows that JAX lowers this code to a single call to <a href=\"https://www.tensorflow.org/xla/operation_semantics#dotgeneral\" rel=\"noreferrer\"><code>dot_general</code></a>, the primitive for generalized dot products in JAX and XLA.</p>\n<h3>Iterating over <code>vector_dot</code></h3>\n<p>Now, suppose you have a 2D input, and you'd like to apply this function to each row. There are several ways you could imagine doing this: three examples are using a Python <code>for</code> loop, using <code>jax.vmap</code>, or using <code>jax.lax.map</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def batched_dot_for_loop(x_batched, y):\n  return jnp.array([vector_dot(x, y) for x in x_batched])\n\ndef batched_dot_lax_map(x_batched, y):\n  return jax.lax.map(lambda x: vector_dot(x, y), x_batched)\n\nbatched_dot_vmap = jax.vmap(vector_dot, in_axes=(0, None))\n</code></pre>\n<p>Applying these three functions to a batched input yields the same results, to within floating point precision:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x_batched = rng.uniform(size=(4, 50))\n\nprint(batched_dot_for_loop(x_batched, y))\n# [11.964929  12.485695  13.683528  12.9286175]\n\nprint(batched_dot_lax_map(x_batched, y))\n# [11.964929  12.485695  13.683528  12.9286175]\n\nprint(batched_dot_vmap(x_batched, y))\n# [11.964927  12.485697  13.683528  12.9286175]\n</code></pre>\n<p>But if we look at the <code>jaxpr</code> for each, we can see that the three approaches lead to very different computational characteristics.</p>\n<p>The <code>for</code> loop solution looks like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(jax.make_jaxpr(batched_dot_for_loop)(x_batched, y))\n</code></pre>\n<pre><code>{ lambda ; a:f32[4,50] b:f32[50]. let\n    c:f32[1,50] = slice[\n      limit_indices=(1, 50)\n      start_indices=(0, 0)\n      strides=(1, 1)\n    ] a\n    d:f32[50] = squeeze[dimensions=(0,)] c\n    e:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] d b\n    f:f32[1,50] = slice[\n      limit_indices=(2, 50)\n      start_indices=(1, 0)\n      strides=(1, 1)\n    ] a\n    g:f32[50] = squeeze[dimensions=(0,)] f\n    h:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] g b\n    i:f32[1,50] = slice[\n      limit_indices=(3, 50)\n      start_indices=(2, 0)\n      strides=(1, 1)\n    ] a\n    j:f32[50] = squeeze[dimensions=(0,)] i\n    k:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] j b\n    l:f32[1,50] = slice[\n      limit_indices=(4, 50)\n      start_indices=(3, 0)\n      strides=(1, 1)\n    ] a\n    m:f32[50] = squeeze[dimensions=(0,)] l\n    n:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] m b\n    o:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] e\n    p:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] h\n    q:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] k\n    r:f32[1] = broadcast_in_dim[broadcast_dimensions=() shape=(1,)] n\n    s:f32[4] = concatenate[dimension=0] o p q r\n  in (s,) }\n</code></pre>\n<p>The key feature is that the iterations in the <code>for</code> loop are unrolled into a single long program.</p>\n<p>The <code>lax.map</code> version looks like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(jax.make_jaxpr(batched_dot_lax_map)(x_batched, y))\n</code></pre>\n<pre><code>{ lambda ; a:f32[4,50] b:f32[50]. let\n    c:f32[4] = scan[\n      jaxpr={ lambda ; d:f32[50] e:f32[50]. let\n          f:f32[] = dot_general[dimension_numbers=(([0], [0]), ([], []))] e d\n        in (f,) }\n      length=4\n      linear=(False, False)\n      num_carry=0\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] b a\n  in (c,) }\n</code></pre>\n<p>The key feature is that it is loaded into a <code>scan</code> primitive, which is XLA's native static loop operation.</p>\n<p>The <code>vmap</code> version looks like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>print(jax.make_jaxpr(batched_dot_vmap)(x_batched, y))\n</code></pre>\n<pre><code>{ lambda ; a:f32[4,50] b:f32[50]. let\n    c:f32[4] = dot_general[dimension_numbers=(([1], [0]), ([], []))] a b\n  in (c,) }\n</code></pre>\n<p>The key feature here is that the <code>vmap</code> transformation is able to recognize that a batched 1D dot product is equivalent to a 2D dot product, so the result is a single extremely efficient native operation.</p>\n<h3>Performance considerations</h3>\n<p>These three approaches can have very different performance characteristics. The details will depend on the specifics of the original function (here <code>vector_dot</code>) but in broad strokes, we can consider three aspects:</p>\n<h4>Compilation Cost</h4>\n<p>If you JIT-compile your program, you'll find:</p>\n<ul>\n<li>The <code>for</code>-loop based solution will have compilation times that grow super-linearly with the number of iterations. This is due to the unrolling seen in the jaxpr above.</li>\n<li>The <code>lax.map</code> and <code>jax.vmap</code> solutions will have fast compilation time, which under normal circumstances will not grow with the size of the batch dimension.</li>\n</ul>\n<h4>Runtime</h4>\n<p>In terms of runtime:</p>\n<ul>\n<li>The <code>for</code> loop solution can be very fast, because XLA can often fuse operations between the unrolled iterations. This is the flip side of the long compilation times.</li>\n<li>The <code>lax.map</code> solution will generally be slow, because it is always executed sequentially with no possibilty of fusing/parallelization between iterations.</li>\n<li>The <code>jax.vmap</code> solution will generally be the fastest, especially on accelerators like GPU or TPU, because it can make use of native batching parallelism on the device.</li>\n</ul>\n<h4>Memory Cost</h4>\n<ul>\n<li>The <code>for</code> loop and <code>lax.map</code> solutions generally have good memory performance, because they execute sequentially and don't require storage of large intermediate results.</li>\n<li>The main downside of the <code>jax.vmap</code> solution is that it can cause memory to blow up because the entire problem must fit into memory at once. This is not an issue with the simple <code>vector_dot</code> function used here, but can be for more complicated functions.</li>\n</ul>\n<h3>Benchmarks</h3>\n<p>You can see these general principles at play when benchmarking the above functions. The following timings are on a Colab T4 GPU:</p>\n<pre class=\"lang-py prettyprint-override\"><code>y = rng.uniform(size=1000)\nx_batched = rng.uniform(size=(200, 1000))\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>%time jax.jit(batched_dot_for_loop).lower(x_batched, y).compile()\n# CPU times: user 4.96 s, sys: 55 ms, total: 5.01 s\n# Wall time: 7.24 s\n%timeit jax.jit(batched_dot_for_loop)(x_batched, y).block_until_ready()\n# 1.09 ms ± 149 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>%time jax.jit(batched_dot_lax_map).lower(x_batched, y).compile()\n# CPU times: user 117 ms, sys: 2.71 ms, total: 120 ms\n# Wall time: 172 ms\n%timeit jax.jit(batched_dot_lax_map)(x_batched, y).block_until_ready()\n# 2.67 ms ± 56.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n</code></pre>\n<pre class=\"lang-py prettyprint-override\"><code>%time jax.jit(batched_dot_vmap).lower(x_batched, y).compile()\n# CPU times: user 51 ms, sys: 941 µs, total: 52 ms\n# Wall time: 103 ms\n%timeit jax.jit(batched_dot_vmap)(x_batched, y).block_until_ready()\n# 719 µs ± 129 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n</code></pre>\n",
        "from_id": "6035679",
        "to_id": "2937831",
        "answer_votes": 11,
        "question_votes": 6,
        "question_id": 76615802,
        "answer_id": 76638350,
        "question_title": "What are the tradeoffs between jax.lax.map and jax.vmap?",
        "question_link": "https://stackoverflow.com/questions/76615802/what-are-the-tradeoffs-between-jax-lax-map-and-jax-vmap",
        "answer_link": "https://stackoverflow.com/questions/76615802/what-are-the-tradeoffs-between-jax-lax-map-and-jax-vmap/76638350#76638350"
    },
    {
        "text_input": "<p>In JAX's Quickstart tutorial I found that the Hessian matrix can be computed efficiently for a differentiable function <code>fun</code> using the following lines of code:</p>\n<pre><code>from jax import jacfwd, jacrev\n\ndef hessian(fun):\n  return jit(jacfwd(jacrev(fun)))\n</code></pre>\n<p>However, one can compute the Hessian also by computing the following:</p>\n<pre><code>def hessian(fun):\n  return jit(jacrev(jacfwd(fun)))\n\ndef hessian(fun):\n  return jit(jacfwd(jacfwd(fun)))\n\ndef hessian(fun):\n  return jit(jacrev(jacrev(fun)))\n</code></pre>\n<p>Here is a minimal working example:</p>\n<pre><code>import jax.numpy as jnp\nfrom jax import jit\nfrom jax import jacfwd, jacrev\n\ndef comp_hessian():\n\n    x = jnp.arange(1.0, 4.0)\n\n    def sum_logistics(x):\n        return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n\n    def hessian_1(fun):\n        return jit(jacfwd(jacrev(fun)))\n\n    def hessian_2(fun):\n        return jit(jacrev(jacfwd(fun)))\n\n    def hessian_3(fun):\n        return jit(jacrev(jacrev(fun)))\n\n    def hessian_4(fun):\n        return jit(jacfwd(jacfwd(fun)))\n\n    hessian_fn = hessian_1(sum_logistics)\n    print(hessian_fn(x))\n\n    hessian_fn = hessian_2(sum_logistics)\n    print(hessian_fn(x))\n\n    hessian_fn = hessian_3(sum_logistics)\n    print(hessian_fn(x))\n\n    hessian_fn = hessian_4(sum_logistics)\n    print(hessian_fn(x))\n\n\ndef main():\n    comp_hessian()\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n<p>I would like to know which approach is best to use and when? I also would like to know if it is possible to use <code>grad()</code> to compute the Hessian? And how does <code>grad()</code> differ from <code>jacfwd</code> and <code>jacrev</code>?</p>\n",
        "output": "<p>The answer to your question is within the JAX documentation; see for example this section: <a href=\"https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobians-and-hessians-using-jacfwd-and-jacrev\" rel=\"noreferrer\">https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#jacobians-and-hessians-using-jacfwd-and-jacrev</a></p>\n<p>To quote its discussion of <code>jacrev</code> and <code>jacfwd</code>:</p>\n<blockquote>\n<p>These two functions compute the same values (up to machine numerics), but differ in their implementation: <code>jacfwd</code> uses forward-mode automatic differentiation, which is more efficient for “tall” Jacobian matrices, while <code>jacrev</code> uses reverse-mode, which is more efficient for “wide” Jacobian matrices. For matrices that are near-square, <code>jacfwd</code> probably has an edge over <code>jacrev</code>.</p>\n</blockquote>\n<p>and further down,</p>\n<blockquote>\n<p>To implement hessian, we could have used <code>jacfwd(jacrev(f))</code> or <code>jacrev(jacfwd(f))</code> or any other composition of the two. But forward-over-reverse is typically the most efficient. That’s because in the inner Jacobian computation we’re often differentiating a function wide Jacobian (maybe like a loss function <em>𝑓:ℝⁿ→ℝ</em>), while in the outer Jacobian computation we’re differentiating a function with a square Jacobian (since <em>∇𝑓:ℝⁿ→ℝⁿ</em>), which is where forward-mode wins out.</p>\n</blockquote>\n<p>Since your function looks like <em>𝑓:ℝⁿ→ℝ</em>, then <code>jit(jacfwd(jacrev(fun)))</code> is likely the most efficient approach.</p>\n<p>As for why you can't implement a hessian with <code>grad</code>, this is because <code>grad</code> is only designed for derivatives of functions with scalar outputs. A hessian by definition is a composition of vector-valued jacobians, not a composition of scalar gradients.</p>\n",
        "from_id": "3861775",
        "to_id": "2937831",
        "answer_votes": 8,
        "question_votes": 6,
        "question_id": 70572362,
        "answer_id": 70579815,
        "question_title": "Compute efficiently Hessian matrices in JAX",
        "question_link": "https://stackoverflow.com/questions/70572362/compute-efficiently-hessian-matrices-in-jax",
        "answer_link": "https://stackoverflow.com/questions/70572362/compute-efficiently-hessian-matrices-in-jax/70579815#70579815"
    },
    {
        "text_input": "<p>I am interested about training a neural network using JAX. I had a look on <code>tf.data.Dataset</code>, but it provides exclusively tf tensors. I looked for a way to change the dataset into JAX numpy array and I found a lot of implementations that use <code>Dataset.as_numpy_generator()</code> to turn the tf tensors to numpy arrays. However I wonder if it is a good practice, as numpy arrays are stored in CPU memory and it is not what I want for my training (I use the GPU). So the last idea I found is to manually recast the arrays by calling <code>jnp.array</code> but it is not really elegant (I am afraid about the copy in GPU memory). Does anyone have a better idea for that?</p>\n<p>Quick code to illustrate:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport jax.numpy as jnp\nimport tensorflow as tf\n\ndef generator():\n    for _ in range(2):\n        yield tf.random.uniform((1, ))\n\nds = tf.data.Dataset.from_generator(generator, output_types=tf.float32,\n                                    output_shapes=tf.TensorShape([1]))\n\nds1 = ds.take(1).as_numpy_iterator()\nds2 = ds.skip(1)\n\nfor i, batch in enumerate(ds1):\n    print(type(batch))\n\nfor i, batch in enumerate(ds2):\n    print(type(jnp.array(batch)))\n\n# returns:\n\n&lt;class 'numpy.ndarray'&gt; # not good\n&lt;class 'jaxlib.xla_extension.DeviceArray'&gt; # good but not elegant\n</code></pre>\n",
        "output": "<p>Both tensorflow and JAX have the ability to convert arrays to <a href=\"https://github.com/dmlc/dlpack\" rel=\"nofollow noreferrer\">dlpack</a> tensors without copying memory, so one way you can create a JAX array from a tensorflow array without copying the underlying data buffer is to do it via dlpack:</p>\n<pre class=\"lang-none prettyprint-override\"><code>import numpy as np\nimport tensorflow as tf\nimport jax.dlpack\n\ntf_arr = tf.random.uniform((10,))\ndl_arr = tf.experimental.dlpack.to_dlpack(tf_arr)\njax_arr = jax.dlpack.from_dlpack(dl_arr)\n\nnp.testing.assert_array_equal(tf_arr, jax_arr)\n</code></pre>\n<p>By doing the round-trip to JAX, you can compare <code>unsafe_buffer_pointer()</code> to ensure that the arrays point at the same buffer, rather than copying the buffer along the way:</p>\n<pre class=\"lang-none prettyprint-override\"><code>def tf_to_jax(arr):\n  return jax.dlpack.from_dlpack(tf.experimental.dlpack.to_dlpack(arr))\n\ndef jax_to_tf(arr):\n  return tf.experimental.dlpack.from_dlpack(jax.dlpack.to_dlpack(arr))\n\njax_arr = jnp.arange(20.)\ntf_arr = jax_to_tf(jax_arr)\njax_arr2 = tf_to_jax(tf_arr)\n\nprint(jnp.all(jax_arr == jax_arr2))\n# True\nprint(jax_arr.unsafe_buffer_pointer() == jax_arr2.unsafe_buffer_pointer())\n# True\n</code></pre>\n",
        "from_id": "17289463",
        "to_id": "2937831",
        "answer_votes": 7,
        "question_votes": 6,
        "question_id": 69782818,
        "answer_id": 69789522,
        "question_title": "Turn a tf.data.Dataset to a jax.numpy iterator",
        "question_link": "https://stackoverflow.com/questions/69782818/turn-a-tf-data-dataset-to-a-jax-numpy-iterator",
        "answer_link": "https://stackoverflow.com/questions/69782818/turn-a-tf-data-dataset-to-a-jax-numpy-iterator/69789522#69789522"
    },
    {
        "text_input": "<p>I am interested about training a neural network using JAX. I had a look on <code>tf.data.Dataset</code>, but it provides exclusively tf tensors. I looked for a way to change the dataset into JAX numpy array and I found a lot of implementations that use <code>Dataset.as_numpy_generator()</code> to turn the tf tensors to numpy arrays. However I wonder if it is a good practice, as numpy arrays are stored in CPU memory and it is not what I want for my training (I use the GPU). So the last idea I found is to manually recast the arrays by calling <code>jnp.array</code> but it is not really elegant (I am afraid about the copy in GPU memory). Does anyone have a better idea for that?</p>\n<p>Quick code to illustrate:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport jax.numpy as jnp\nimport tensorflow as tf\n\ndef generator():\n    for _ in range(2):\n        yield tf.random.uniform((1, ))\n\nds = tf.data.Dataset.from_generator(generator, output_types=tf.float32,\n                                    output_shapes=tf.TensorShape([1]))\n\nds1 = ds.take(1).as_numpy_iterator()\nds2 = ds.skip(1)\n\nfor i, batch in enumerate(ds1):\n    print(type(batch))\n\nfor i, batch in enumerate(ds2):\n    print(type(jnp.array(batch)))\n\n# returns:\n\n&lt;class 'numpy.ndarray'&gt; # not good\n&lt;class 'jaxlib.xla_extension.DeviceArray'&gt; # good but not elegant\n</code></pre>\n",
        "output": "<p>From Flax example:</p>\n<p><a href=\"https://github.com/google/flax/blob/6ae22681ef6f6c004140c3759e7175533bda55bd/examples/imagenet/train.py#L183\" rel=\"nofollow noreferrer\">https://github.com/google/flax/blob/6ae22681ef6f6c004140c3759e7175533bda55bd/examples/imagenet/train.py#L183</a></p>\n<pre><code>def prepare_tf_data(xs):\n  local_device_count = jax.local_device_count()\n  def _prepare(x):\n    x = x._numpy() \n    return x.reshape((local_device_count, -1) + x.shape[1:])\n  return jax.tree_util.tree_map(_prepare, xs)\n\nit = map(prepare_tf_data, ds)\nit = jax_utils.prefetch_to_device(it, 2)\n</code></pre>\n",
        "from_id": "17289463",
        "to_id": "9815697",
        "answer_votes": 1,
        "question_votes": 6,
        "question_id": 69782818,
        "answer_id": 73150428,
        "question_title": "Turn a tf.data.Dataset to a jax.numpy iterator",
        "question_link": "https://stackoverflow.com/questions/69782818/turn-a-tf-data-dataset-to-a-jax-numpy-iterator",
        "answer_link": "https://stackoverflow.com/questions/69782818/turn-a-tf-data-dataset-to-a-jax-numpy-iterator/73150428#73150428"
    },
    {
        "text_input": "<h1><strong>Description</strong></h1>\n<p>The OUTPUT:</p>\n<p>2023-07-31 01:53:45.016563: E external/xla/xla/stream_executor/cuda/cuda_dnn.cc:427] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0. CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library. If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.</p>\n<p>XlaRuntimeError Traceback (most recent call last)\nCell In[4], line 29</p>\n<pre><code>26 model = trainer.make_model(nmask)\n28 lr_fn, opt = trainer.make_optimizer(steps_per_epoch=len(train_dl))\n---&gt; 29 state = trainer.create_train_state(jax.random.PRNGKey(0), model, opt)\n30 state = checkpoints.restore_checkpoint(ckpt.parent, state)\n</code></pre>\n<p>File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/site-packages/jax/_src/random.py:137, in PRNGKey(seed)</p>\n<pre><code>134 if np.ndim(seed):\n135 raise TypeError(&quot;PRNGKey accepts a scalar seed, but was given an array of&quot;\n136 f&quot;shape {np.shape(seed)} != (). Use jax.vmap for batching&quot;)\n--&gt; 137 key = prng.seed_with_impl(impl, seed)\n138 return _return_prng_keys(True, key)\n</code></pre>\n<p>File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/site-packages/jax/_src/prng.py:320, in seed_with_impl(impl, seed)</p>\n<pre><code>319 def seed_with_impl(impl: PRNGImpl, seed: Union[int, Array]) -&gt; PRNGKeyArrayImpl:\n--&gt; 320 return random_seed(seed, impl=impl)\n</code></pre>\n<p>File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/site-packages/jax/_src/prng.py:734, in random_seed(seeds, impl)</p>\n<pre><code>732 else:\n733 seeds_arr = jnp.asarray(seeds)\n--&gt; 734 return random_seed_p.bind(seeds_arr, impl=impl)\n</code></pre>\n<p>File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/site-packages/jax/_src/core.py:380, in Primitive.bind(self, *args, **params)</p>\n<pre><code>377 def bind(self, *args, **params):\n378 assert (not config.jax_enable_checks or\n379 all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n--&gt; 380 return self.bind_with_trace(find_top_trace(args), args, params)\n</code></pre>\n<p>File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/site-packages/jax/_src/core.py:383, in Primitive.bind_with_trace(self, trace, args, params)</p>\n<pre><code>382 def bind_with_trace(self, trace, args, params):\n--&gt; 383 out = trace.process_primitive(self, map(trace.full_raise, args), params)\n384 return map(full_lower, out) if self.multiple_results else full_lower(out)\n</code></pre>\n<p>File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/site-packages/jax/_src/core.py:790, in EvalTrace.process_primitive(self, primitive, tracers, params)</p>\n<pre><code>789 def process_primitive(self, primitive, tracers, params):\n--&gt; 790 return primitive.impl(*tracers, **params)\n</code></pre>\n<p>File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/site-packages/jax/_src/prng.py:746, in random_seed_impl(seeds, impl)</p>\n<pre><code>744 @random_seed_p.def_impl\n745 def random_seed_impl(seeds, *, impl):\n--&gt; 746 base_arr = random_seed_impl_base(seeds, impl=impl)\n747 return PRNGKeyArrayImpl(impl, base_arr)\n</code></pre>\n<p>File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/site-packages/jax/_src/prng.py:751, in random_seed_impl_base(seeds, impl)</p>\n<pre><code>749 def random_seed_impl_base(seeds, *, impl):\n750 seed = iterated_vmap_unary(seeds.ndim, impl.seed)\n--&gt; 751 return seed(seeds)\n</code></pre>\n<p>File /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/site-packages/jax/_src/prng.py:980, in threefry_seed(seed)</p>\n<pre><code>968 def threefry_seed(seed: typing.Array) -&gt; typing.Array:\n969 &quot;&quot;&quot;Create a single raw threefry PRNG key from an integer seed.\n970\n971 Args:\n(...)\n978 first padding out with zeros).\n979 &quot;&quot;&quot;\n--&gt; 980 return _threefry_seed(seed)\n</code></pre>\n<p>[... skipping hidden 12 frame]\nFile /mnt/data/miniconda/envs/energy_transformer_117/lib/python3.11/site-packages/jax/_src/dispatch.py:463, in backend_compile(backend, module, options, host_callbacks)</p>\n<pre><code>458 return backend.compile(built_c, compile_options=options,\n459 host_callbacks=host_callbacks)\n460 # Some backends don't have host_callbacks option yet\n461 # TODO(sharadmv): remove this fallback when all backends allow compile\n462 # to take in host_callbacks\n--&gt; 463 return backend.compile(built_c, compile_options=options)`\n</code></pre>\n<p>XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.</p>\n<h1><strong>What jax/jaxlib version are you using?</strong></h1>\n<p>jax0.4.10, jaxlib0.4.10+cuda11.cudnn86</p>\n<h1><strong>Which accelerator(s) are you using?</strong></h1>\n<p>GPU</p>\n<h1><strong>Additional system info</strong></h1>\n<p>python3.11.4, Ubuntu22.04, cuda11.7,cudnn86</p>\n<h1><strong>NVIDIA GPU info</strong></h1>\n<p><a href=\"https://i.sstatic.net/cTSJe.png\" rel=\"noreferrer\">enter image description here</a></p>\n<p>I hope to fix this error.</p>\n",
        "output": "<p>The error message tells you what's wrong:</p>\n<pre><code>Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.\nIf using a binary install, upgrade your CuDNN library. If building from\nsources, make sure the library loaded at runtime is compatible with the\nversion specified during compile configuration.\n</code></pre>\n<p>You are using a <code>jaxlib</code> package that was compiled with CuDNN 8.6.0, but at runtime the system is finding CuDNN 8.5.0. In general, the way to fix this is to either install a jaxlib compiled with CuDNN 8.5.0, or to upgrade your system CuDNN to 8.6.0.</p>\n<p>An easy way to achieve this is to reinstall jax in a fresh environment using the <code>cuda11_pip</code> or <code>cuda12_pip</code> options (see <a href=\"https://github.com/google/jax#pip-installation-gpu-cuda-installed-via-pip-easier\" rel=\"noreferrer\">installation instructions</a>), as this will ensure that your package versions match.</p>\n",
        "from_id": "22314357",
        "to_id": "2937831",
        "answer_votes": 6,
        "question_votes": 6,
        "question_id": 76803746,
        "answer_id": 76813468,
        "question_title": "XlaRuntimeError: FAILED_PRECONDITION: DNN library initialization failed",
        "question_link": "https://stackoverflow.com/questions/76803746/xlaruntimeerror-failed-precondition-dnn-library-initialization-failed",
        "answer_link": "https://stackoverflow.com/questions/76803746/xlaruntimeerror-failed-precondition-dnn-library-initialization-failed/76813468#76813468"
    },
    {
        "text_input": "<h2><code>trax</code></h2>\n<p>New to <a href=\"https://github.com/google/trax\" rel=\"noreferrer\"><code>trax</code></a>, I'm trying to run it locally (macOS 12.1, Apple Silicon ARM M1 processor, 8GB RAM, Anaconda), but I'm running into some issues.</p>\n<p>In an environment with python 3.8.5, I installed <code>trax</code> running <code>pip3 install trax==1.3.9</code> inside an (Anaconda) conda environment. Later, I ran into issues when trying to import <code>trax</code> layers in my code with <code>from trax import layers as tl</code></p>\n<pre><code>RuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.\n</code></pre>\n<p>I rushed to start a new conda environment with python 3.10. Then ran into issue just trying to install <code>trax</code>:</p>\n<pre><code>macos ERROR: Could not find a version that satisfies the requirement tensorflow-text (from trax) (from versions: none)\nERROR: No matching distribution found for tensorflow-text\n</code></pre>\n<p>I then created a new environment with python 3.9. Installation went fine, but then ran into the same error importing layers:</p>\n<pre><code>RuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.\n</code></pre>\n<h2>installing <code>jaxlib</code>, <code>jax</code> before <code>trax</code></h2>\n<p>I then tried building jaxlib from source following <a href=\"https://jax.readthedocs.io/en/latest/developer.html#building-jaxlib-from-source\" rel=\"noreferrer\">these instructions</a> and got this error:</p>\n<pre><code>Building XLA and installing it in the jaxlib source tree...\n./bazel-4.2.1-darwin-x86_64 run --verbose_failures=true --config=avx_posix --config=mkl_open_source_only :build_wheel -- --output_path=/my path/jax/dist --cpu=x86_64\nERROR: bazel does not currently work properly from paths containing spaces (/my path/jax).\nb''\nTraceback (most recent call last):\n  File &quot;/my path/jax/build/build.py&quot;, line 524, in &lt;module&gt;\n    main()\n  File &quot;/my path/jax/build/build.py&quot;, line 519, in main\n    shell(command)\n  File &quot;/my path/jax/build/build.py&quot;, line 53, in shell\n    output = subprocess.check_output(cmd)\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 424, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 528, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['./bazel-4.2.1-darwin-x86_64', 'run', '--verbose_failures=true', '--config=avx_posix', '--config=mkl_open_source_only', ':build_wheel', '--', '--output_path=/my path/jax/dist', '--cpu=x86_64']' returned non-zero exit status 36.\n</code></pre>\n<p>Emphasis on the part that I initially missed that says: <em>bazel does not currently work properly from paths containing spaces (/my path/jax)</em>.</p>\n<p>I moved my <code>/my path/</code> directory to a path without spaces <code>/mypath/</code>. Deleted and redownloaded jax directory. Still, the build (for CPU) with <code>python build/build.py</code> failed:</p>\n<pre><code>ERROR: /private/var/tmp/_bazel_a/2caf512c3c5e3f3f654bc58b48b8333a/external/llvm-project/llvm/BUILD.bazel:610:11: Generating code from table: include/llvm/IR/Intrinsics.td @llvm-project//llvm:intrinsic_XCore_gen__gen_intrinsic_enums__intrinsic_prefix_xcore_genrule failed: (Illegal instruction): bash failed: error executing command \n  (cd /private/var/tmp/_bazel_a/2caf512c3c5e3f3f654bc58b48b8333a/execroot/__main__ &amp;&amp; \\\n  exec env - \\\n    PATH=/myuserpath/opt/anaconda3/envs/mytraxenv/bin:/myuserpath/opt/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Library/Apple/usr/bin \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/darwin-opt/bin/external/llvm-project/llvm/llvm-tblgen -I external/llvm-project/llvm/include -I external/llvm-project/clang/include -I $(dirname external/llvm-project/llvm/include/llvm/IR/Intrinsics.td) -gen-intrinsic-enums -intrinsic-prefix=xcore external/llvm-project/llvm/include/llvm/IR/Intrinsics.td  -o bazel-out/darwin-opt/bin/external/llvm-project/llvm/include/llvm/IR/IntrinsicsXCore.h')\nExecution platform: @local_execution_config_platform//:platform\n/bin/bash: line 1: 11140 Illegal instruction: 4  bazel-out/darwin-opt/bin/external/llvm-project/llvm/llvm-tblgen -I external/llvm-project/llvm/include -I external/llvm-project/clang/include -I $(dirname external/llvm-project/llvm/include/llvm/IR/Intrinsics.td) -gen-intrinsic-enums -intrinsic-prefix=xcore external/llvm-project/llvm/include/llvm/IR/Intrinsics.td -o bazel-out/darwin-opt/bin/external/llvm-project/llvm/include/llvm/IR/IntrinsicsXCore.h\nTarget //build:build_wheel failed to build\nINFO: Elapsed time: 620.950s, Critical Path: 45.35s\nINFO: 589 processes: 132 internal, 457 local.\nFAILED: Build did NOT complete successfully\nERROR: Build failed. Not running target\nFAILED: Build did NOT complete successfully\nb''\nTraceback (most recent call last):\n  File &quot;/mypath/jax/build/build.py&quot;, line 524, in &lt;module&gt;\n    main()\n  File &quot;/mypath/jax/build/build.py&quot;, line 519, in main\n    shell(command)\n  File &quot;/mypath/jax/build/build.py&quot;, line 53, in shell\n    output = subprocess.check_output(cmd)\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 424, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 528, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['./bazel-4.2.1-darwin-x86_64', 'run', '--verbose_failures=true', '--config=avx_posix', '--config=mkl_open_source_only', ':build_wheel', '--', '--output_path=/mypath/jax/dist', '--cpu=x86_64']' returned non-zero exit status 1.\n</code></pre>\n<p>Tried again a couple of times (deleted and redownloaded jax directory) and the same line 528 in <code>.../lib/python3.9/subprocess.py</code> seemed to cause the issue but the output, while mostly the above, sometimes was slightly different. Making me suspect an issue with memory, given that I (admittedly) had not restarted my machine in weeks and it was starting to unresponsively slow.</p>\n<p>I updated my XCode command line tools to version 12.2 (pretty sure).</p>\n<p>I restarted my (8GB) machine. Deleted and redownloaded jax directory. I installed bazel version 5.0.0 with homebrew in case that would help. I was a bit concerned that it kept downloading an x86 version for my ARM processor. <code>bazel</code> installation went fine.</p>\n<p>Started again from <a href=\"https://jax.readthedocs.io/en/latest/developer.html#building-jaxlib-from-source\" rel=\"noreferrer\">these instructions</a>. The <code>jaxlib</code> build, though, made it clear that it wanted an earlier (4.2.1) version of bazel and downloaded it, as before:</p>\n<pre><code>b'\\x1b[31mERROR: The project you\\'re trying to build requires Bazel 4.2.1 (specified in /mypath/jax/.bazelversion), but it wasn\\'t found in /opt/homebrew/Cellar/bazel/5.0.0/libexec/bin.\\x1b[0m\\n\\nBazel binaries for all official releases can be downloaded from here:\\n  https://github.com/bazelbuild/bazel/releases\\n\\nYou can download the required version directly using this command:\\n  (cd &quot;/opt/homebrew/Cellar/bazel/5.0.0/libexec/bin&quot; &amp;&amp; curl -fLO https://releases.bazel.build/4.2.1/release/bazel-4.2.1-darwin-x86_64 &amp;&amp; chmod +x bazel-4.2.1-darwin-x86_64)\\n'\n</code></pre>\n<p>Once again, a different error in the same line 528. Showing full run now:</p>\n<pre><code>b'\\x1b[31mERROR: The project you\\'re trying to build requires Bazel 4.2.1 (specified in /mypath/jax/.bazelversion), but it wasn\\'t found in /opt/homebrew/Cellar/bazel/5.0.0/libexec/bin.\\x1b[0m\\n\\nBazel binaries for all official releases can be downloaded from here:\\n  https://github.com/bazelbuild/bazel/releases\\n\\nYou can download the required version directly using this command:\\n  (cd &quot;/opt/homebrew/Cellar/bazel/5.0.0/libexec/bin&quot; &amp;&amp; curl -fLO https://releases.bazel.build/4.2.1/release/bazel-4.2.1-darwin-x86_64 &amp;&amp; chmod +x bazel-4.2.1-darwin-x86_64)\\n'\nDownloading bazel from: https://github.com/bazelbuild/bazel/releases/download/4.2.1/bazel-4.2.1-darwin-x86_64\nbazel-4.2.1-darwin-x86_64 [########################################] 100%\nBazel binary path: ./bazel-4.2.1-darwin-x86_64\nBazel version: 4.2.1\nPython binary path: /myuserpath/opt/anaconda3/envs/mytraxenv/bin/python\nPython version: 3.9\nNumPy version: 1.21.2\nMKL-DNN enabled: yes\nTarget CPU: x86_64\nTarget CPU features: release\nCUDA enabled: no\nTPU enabled: no\nROCm enabled: no\n\nBuilding XLA and installing it in the jaxlib source tree...\n./bazel-4.2.1-darwin-x86_64 run --verbose_failures=true --config=avx_posix --config=mkl_open_source_only :build_wheel -- --output_path=/mypath/jax/dist --cpu=x86_64\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\nINFO: Reading rc options for 'run' from /mypath/jax/.bazelrc:\n  Inherited 'common' options: --experimental_repo_remote_exec\nINFO: Reading rc options for 'run' from /mypath/jax/.bazelrc:\n  Inherited 'build' options: --apple_platform_type=macos --macos_minimum_os=10.9 --announce_rc --define open_source_build=true --spawn_strategy=standalone --enable_platform_specific_config --define=no_aws_support=true --define=no_gcp_support=true --define=no_hdfs_support=true --define=no_kafka_support=true --define=no_ignite_support=true --define=grpc_no_ares=true -c opt --config=short_logs --copt=-DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.\nINFO: Reading rc options for 'run' from /mypath/jax/.jax_configure.bazelrc:\n  Inherited 'build' options: --strategy=Genrule=standalone --repo_env PYTHON_BIN_PATH=/myuserpath/opt/anaconda3/envs/mytraxenv/bin/python --action_env=PYENV_ROOT --python_path=/myuserpath/opt/anaconda3/envs/mytraxenv/bin/python --distinct_host_configuration=false\nINFO: Found applicable config definition build:short_logs in file /mypath/jax/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\nINFO: Found applicable config definition build:avx_posix in file /mypath/jax/.bazelrc: --copt=-mavx --host_copt=-mavx\nINFO: Found applicable config definition build:mkl_open_source_only in file /mypath/jax/.bazelrc: --define=tensorflow_mkldnn_contraction_kernel=1\nINFO: Found applicable config definition build:macos in file /mypath/jax/.bazelrc: --config=posix\nINFO: Found applicable config definition build:posix in file /mypath/jax/.bazelrc: --copt=-fvisibility=hidden --copt=-Wno-sign-compare --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\nLoading: \nLoading: 0 packages loaded\nAnalyzing: target //build:build_wheel (1 packages loaded, 0 targets configured)\nAnalyzing: target //build:build_wheel (8 packages loaded, 286 targets configured)\nAnalyzing: target //build:build_wheel (10 packages loaded, 4271 targets configured)\nAnalyzing: target //build:build_wheel (10 packages loaded, 4271 targets configured)\nAnalyzing: target //build:build_wheel (11 packages loaded, 4494 targets configured)\nINFO: Analyzed target //build:build_wheel (11 packages loaded, 7417 targets configured).\n\nINFO: Found 1 target...\n[0 / 3] [Prepa] BazelWorkspaceStatusAction stable-status.txt\n[21 / 233] Compiling src/google/protobuf/generated_enum_util.cc; 2s local ... (6 actions, 3 running)\n[28 / 233] Compiling src/google/protobuf/extension_set.cc; 4s local ... (8 actions, 7 running)\n[114 / 523] Compiling src/google/protobuf/generated_message_util.cc; 2s local ... (8 actions running)\n[140 / 594] Compiling platform/c++11/src/nsync_semaphore_mutex.cc; 0s local ... (8 actions, 7 running)\n[237 / 597] Compiling src/google/protobuf/util/message_differencer.cc; 0s local ... (8 actions running)\n[252 / 597] Compiling src/google/protobuf/util/message_differencer.cc; 4s local ... (8 actions running)\n[272 / 597] Compiling src/google/protobuf/descriptor.pb.cc; 8s local ... (8 actions running)\n[296 / 597] Compiling src/google/protobuf/descriptor.cc; 12s local ... (8 actions running)\n[315 / 597] Compiling src/google/protobuf/descriptor_database.cc; 2s local ... (5 actions running)\n[487 / 1,963] Compiling src/compiler/python_generator.cc; 2s local ... (8 actions running)\n[579 / 3,117] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv3d.cc; 0s local ... (8 actions, 7 running)\n[619 / 3,173] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv3d.cc; 8s local ... (8 actions running)\n[687 / 3,270] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv3d.cc; 17s local ... (8 actions running)\n[774 / 3,464] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc; 27s local ... (8 actions running)\n[1,204 / 4,860] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc; 39s local ... (8 actions running)\n[1,255 / 4,916] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 52s local ... (8 actions running)\n[1,340 / 5,042] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 68s local ... (8 actions running)\n[1,456 / 5,156] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 86s local ... (8 actions, 7 running)\n[1,661 / 5,704] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 107s local ... (8 actions, 7 running)\n[1,688 / 5,704] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 132s local ... (8 actions, 7 running)\n[1,721 / 5,704] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 160s local ... (8 actions, 7 running)\n[2,106 / 6,584] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 192s local ... (8 actions, 7 running)\n[2,342 / 7,067] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 231s local ... (8 actions, 7 running)\n[2,378 / 7,067] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 274s local ... (8 actions, 7 running)\n[2,422 / 7,067] Compiling src/cpu/rnn/ref_rnn.cpp; 75s local ... (8 actions, 7 running)\n[2,452 / 7,067] Compiling src/cpu/x64/gemm/f32/jit_avx512_core_f32_copy_at_kern_part1_autogen.cpp; 54s local ... (8 actions, 7 running)\n[2,500 / 7,067] Compiling src/cpu/x64/gemm/f32/jit_avx512_core_f32_copy_at_kern_part1_autogen.cpp; 119s local ... (8 actions, 7 running)\n[2,577 / 7,067] Compiling src/common/memory_zero_pad.cpp; 74s local ... (8 actions, 7 running)\nERROR: /private/var/tmp/_bazel_a/f5e9a3325f07a1f02c52d821857db47c/external/org_tensorflow/tensorflow/compiler/xla/BUILD:69:17: ProtoCompile external/org_tensorflow/tensorflow/compiler/xla/xla.pb.h failed: (Illegal instruction): protoc failed: error executing command \n  (cd /private/var/tmp/_bazel_a/f5e9a3325f07a1f02c52d821857db47c/execroot/__main__ &amp;&amp; \\\n  exec env - \\\n    PATH=/myuserpath/opt/anaconda3/envs/mytraxenv/bin:/myuserpath/opt/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Library/Apple/usr/bin \\\n  bazel-out/darwin-opt/bin/external/com_google_protobuf/protoc '--cpp_out=bazel-out/darwin-opt/bin/external/org_tensorflow' -Iexternal/org_tensorflow -Ibazel-out/darwin-opt/bin/external/org_tensorflow -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/any_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/source_context_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/type_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/api_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/descriptor_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/compiler_plugin_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/duration_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/empty_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/field_mask_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/struct_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/timestamp_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/wrappers_proto external/org_tensorflow/tensorflow/compiler/xla/xla.proto)\nExecution platform: @local_execution_config_platform//:platform\nTarget //build:build_wheel failed to build\nINFO: Elapsed time: 631.189s, Critical Path: 283.35s\nINFO: 2563 processes: 935 internal, 1628 local.\nFAILED: Build did NOT complete successfully\nERROR: Build failed. Not running target\nFAILED: Build did NOT complete successfully\nb''\nTraceback (most recent call last):\n  File &quot;/mypath/jax/build/build.py&quot;, line 524, in &lt;module&gt;\n    main()\n  File &quot;/mypath/jax/build/build.py&quot;, line 519, in main\n    shell(command)\n  File &quot;/mypath/jax/build/build.py&quot;, line 53, in shell\n    output = subprocess.check_output(cmd)\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 424, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 528, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['./bazel-4.2.1-darwin-x86_64', 'run', '--verbose_failures=true', '--config=avx_posix', '--config=mkl_open_source_only', ':build_wheel', '--', '--output_path=/mypath/jax/dist', '--cpu=x86_64']' returned non-zero exit status 1.\n</code></pre>\n<p>I switched away from trying to build jaxlib. In <a href=\"https://github.com/google/jax/issues/5501#issuecomment-955590288\" rel=\"noreferrer\">late October 2021</a>, a M1-compatible jaxlib wheel was released, so I tried:</p>\n<pre><code>pip install -U pip\npip install -U https://storage.googleapis.com/jax-releases/mac/jaxlib-0.1.74-cp39-none-macosx_11_0_arm64.whl\n</code></pre>\n<p>but got</p>\n<pre><code>ERROR: jaxlib-0.1.74-cp39-none-macosx_11_0_arm64.whl is not a supported wheel on this platform.\n</code></pre>\n<p>Tried <a href=\"https://github.com/google/jax/issues/5501#issuecomment-1003671111\" rel=\"noreferrer\">upgrading python from 3.9 to 3.10</a>, but got the same message.</p>\n<p>Debugged by, in python, running</p>\n<pre><code>import platform\nprint(platform.machine())\n</code></pre>\n<p>which showed that my python is still running on x86 architechture, since <a href=\"https://github.com/google/jax/issues/5501#issuecomment-968998748\" rel=\"noreferrer\">&quot;Anaconda doesn't yet provide packages for M1/ARM&quot;</a>.</p>\n<p><strong>[see successful (so far) <code>jaxlib</code> installation in own answer below]</strong></p>\n<h2>back to <code>trax</code></h2>\n<p>After successfully installing <code>jaxlib</code> and <code>jax</code>, when trying to install <code>trax</code> with (miniforge’s)<code>conda install trax</code> I get:</p>\n<pre><code>Collecting package metadata (current_repodata.json): done\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\nCollecting package metadata (repodata.json): done\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\n\nPackagesNotFoundError: The following packages are not available from current channels:\n\n  - trax\n\nCurrent channels:\n\n  - https://conda.anaconda.org/conda-forge/osx-arm64\n  - https://conda.anaconda.org/conda-forge/noarch\n\nTo search for alternate channels that may provide the conda package you're\nlooking for, navigate to\n\n    https://anaconda.org\n\nand use the search bar at the top of the page.\n</code></pre>\n<p>I tried installing as <a href=\"https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html\" rel=\"noreferrer\"><code>trax</code> itself suggests</a> but ran into an error building <code>hp5y</code>:</p>\n<pre><code>ERROR: Failed building wheel for h5py\nERROR: Could not build wheels for h5py, which is required to install pyproject.toml-based projects\n</code></pre>\n<p>Looking now into this error and also looking into doing it with <code>miniforge</code> again but using Anaconda's channel to install <code>trax</code>. I'll update when I get to it.</p>\n<p>Any pointers or anyone who managed to install <code>trax</code> on an M1 Mac it by any different means?</p>\n",
        "output": "<h2><code>jaxlib</code> + <code>jax</code></h2>\n<p>After <a href=\"https://github.com/google/jax/issues/5501#issuecomment-969225257\" rel=\"nofollow noreferrer\">someone claimed success using miniforge</a>, I read <a href=\"https://towardsdatascience.com/from-soup-to-nuts-guide-for-setting-up-a-conda-environment-58afc7c4801\" rel=\"nofollow noreferrer\">this</a> and watched <a href=\"https://www.youtube.com/watch?v=w2qlou7n7MA\" rel=\"nofollow noreferrer\">this</a> to clarify using Anaconda and miniforge together.</p>\n<p>I installed miniforge with <a href=\"https://developer.apple.com/metal/tensorflow-plugin/\" rel=\"nofollow noreferrer\">Apple's <em>arm64 : Apple Silicon</em> method</a>. For some reason when I ran miniforge's <code>conda init</code> it set up the initialization code in <code>~/.bash_profile</code> even though I'm using the zsh shell. I tried putting the code manually instead in <code>~/.zprofile</code> but it wouldn't load on interactive shells, so I just ended up putting it where Anaconda had put its initialization code, in <code>~/.zshrc</code>.</p>\n<p>This made miniforge the default manager. Following the very useful video above, I created a <code>~/.start_anaconda.sh</code> script so I can use Anaconda as an alternative.</p>\n<p>With miniforge I</p>\n<ul>\n<li><p>created a new conda environment <code>mytraxenv</code> with <code>conda create -n mytraxenv python=3</code> which has python 3.10.2 at the moment</p>\n</li>\n<li><p>activated the environment: <code>conda activate mytraxenv</code></p>\n</li>\n<li><p>ran <code>conda install numpy</code> and <code>conda install six</code> to ensure <code>numpy</code>. <code>six</code> and <code>wheel</code> (installed by one of the previous two) were installed in my <code>mytraxenv</code> environment</p>\n</li>\n<li><p>tried again, with a slightly updated release (from <a href=\"https://pypi.org/project/jaxlib/#files\" rel=\"nofollow noreferrer\">here</a>):</p>\n<p>pip install -U pip\npip install -U <a href=\"https://storage.googleapis.com/jax-releases/mac/jaxlib-0.1.75-cp310-none-macosx_11_0_arm64.whl\" rel=\"nofollow noreferrer\">https://storage.googleapis.com/jax-releases/mac/jaxlib-0.1.75-cp310-none-macosx_11_0_arm64.whl</a></p>\n</li>\n</ul>\n<p>This worked in installing <code>jaxlib</code>!</p>\n<p>Then, I followed <a href=\"https://github.com/google/jax#installation\" rel=\"nofollow noreferrer\">these instructions</a> to install <code>jax</code>:</p>\n<pre><code>pip install --upgrade pip\npip install --upgrade &quot;jax[cpu]&quot;\n</code></pre>\n<p>That worked as well. Note that when running <code>import jax</code> in python it currently warns:</p>\n<pre><code>/mytraxenv/lib/python3.10/site-packages/jax/_src/lib/__init__.py:32: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n  warnings.warn(&quot;JAX on Mac ARM machines is experimental and minimally tested. &quot;\n</code></pre>\n<h2><code>trax</code></h2>\n<p>No success yet installing trax.</p>\n",
        "from_id": "583834",
        "to_id": "583834",
        "answer_votes": 3,
        "question_votes": 6,
        "question_id": 70815864,
        "answer_id": 70815865,
        "question_title": "How to install trax, jax, jaxlib on M1 Mac on macOS 12?",
        "question_link": "https://stackoverflow.com/questions/70815864/how-to-install-trax-jax-jaxlib-on-m1-mac-on-macos-12",
        "answer_link": "https://stackoverflow.com/questions/70815864/how-to-install-trax-jax-jaxlib-on-m1-mac-on-macos-12/70815865#70815865"
    },
    {
        "text_input": "<h2><code>trax</code></h2>\n<p>New to <a href=\"https://github.com/google/trax\" rel=\"noreferrer\"><code>trax</code></a>, I'm trying to run it locally (macOS 12.1, Apple Silicon ARM M1 processor, 8GB RAM, Anaconda), but I'm running into some issues.</p>\n<p>In an environment with python 3.8.5, I installed <code>trax</code> running <code>pip3 install trax==1.3.9</code> inside an (Anaconda) conda environment. Later, I ran into issues when trying to import <code>trax</code> layers in my code with <code>from trax import layers as tl</code></p>\n<pre><code>RuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.\n</code></pre>\n<p>I rushed to start a new conda environment with python 3.10. Then ran into issue just trying to install <code>trax</code>:</p>\n<pre><code>macos ERROR: Could not find a version that satisfies the requirement tensorflow-text (from trax) (from versions: none)\nERROR: No matching distribution found for tensorflow-text\n</code></pre>\n<p>I then created a new environment with python 3.9. Installation went fine, but then ran into the same error importing layers:</p>\n<pre><code>RuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.\n</code></pre>\n<h2>installing <code>jaxlib</code>, <code>jax</code> before <code>trax</code></h2>\n<p>I then tried building jaxlib from source following <a href=\"https://jax.readthedocs.io/en/latest/developer.html#building-jaxlib-from-source\" rel=\"noreferrer\">these instructions</a> and got this error:</p>\n<pre><code>Building XLA and installing it in the jaxlib source tree...\n./bazel-4.2.1-darwin-x86_64 run --verbose_failures=true --config=avx_posix --config=mkl_open_source_only :build_wheel -- --output_path=/my path/jax/dist --cpu=x86_64\nERROR: bazel does not currently work properly from paths containing spaces (/my path/jax).\nb''\nTraceback (most recent call last):\n  File &quot;/my path/jax/build/build.py&quot;, line 524, in &lt;module&gt;\n    main()\n  File &quot;/my path/jax/build/build.py&quot;, line 519, in main\n    shell(command)\n  File &quot;/my path/jax/build/build.py&quot;, line 53, in shell\n    output = subprocess.check_output(cmd)\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 424, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 528, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['./bazel-4.2.1-darwin-x86_64', 'run', '--verbose_failures=true', '--config=avx_posix', '--config=mkl_open_source_only', ':build_wheel', '--', '--output_path=/my path/jax/dist', '--cpu=x86_64']' returned non-zero exit status 36.\n</code></pre>\n<p>Emphasis on the part that I initially missed that says: <em>bazel does not currently work properly from paths containing spaces (/my path/jax)</em>.</p>\n<p>I moved my <code>/my path/</code> directory to a path without spaces <code>/mypath/</code>. Deleted and redownloaded jax directory. Still, the build (for CPU) with <code>python build/build.py</code> failed:</p>\n<pre><code>ERROR: /private/var/tmp/_bazel_a/2caf512c3c5e3f3f654bc58b48b8333a/external/llvm-project/llvm/BUILD.bazel:610:11: Generating code from table: include/llvm/IR/Intrinsics.td @llvm-project//llvm:intrinsic_XCore_gen__gen_intrinsic_enums__intrinsic_prefix_xcore_genrule failed: (Illegal instruction): bash failed: error executing command \n  (cd /private/var/tmp/_bazel_a/2caf512c3c5e3f3f654bc58b48b8333a/execroot/__main__ &amp;&amp; \\\n  exec env - \\\n    PATH=/myuserpath/opt/anaconda3/envs/mytraxenv/bin:/myuserpath/opt/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Library/Apple/usr/bin \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/darwin-opt/bin/external/llvm-project/llvm/llvm-tblgen -I external/llvm-project/llvm/include -I external/llvm-project/clang/include -I $(dirname external/llvm-project/llvm/include/llvm/IR/Intrinsics.td) -gen-intrinsic-enums -intrinsic-prefix=xcore external/llvm-project/llvm/include/llvm/IR/Intrinsics.td  -o bazel-out/darwin-opt/bin/external/llvm-project/llvm/include/llvm/IR/IntrinsicsXCore.h')\nExecution platform: @local_execution_config_platform//:platform\n/bin/bash: line 1: 11140 Illegal instruction: 4  bazel-out/darwin-opt/bin/external/llvm-project/llvm/llvm-tblgen -I external/llvm-project/llvm/include -I external/llvm-project/clang/include -I $(dirname external/llvm-project/llvm/include/llvm/IR/Intrinsics.td) -gen-intrinsic-enums -intrinsic-prefix=xcore external/llvm-project/llvm/include/llvm/IR/Intrinsics.td -o bazel-out/darwin-opt/bin/external/llvm-project/llvm/include/llvm/IR/IntrinsicsXCore.h\nTarget //build:build_wheel failed to build\nINFO: Elapsed time: 620.950s, Critical Path: 45.35s\nINFO: 589 processes: 132 internal, 457 local.\nFAILED: Build did NOT complete successfully\nERROR: Build failed. Not running target\nFAILED: Build did NOT complete successfully\nb''\nTraceback (most recent call last):\n  File &quot;/mypath/jax/build/build.py&quot;, line 524, in &lt;module&gt;\n    main()\n  File &quot;/mypath/jax/build/build.py&quot;, line 519, in main\n    shell(command)\n  File &quot;/mypath/jax/build/build.py&quot;, line 53, in shell\n    output = subprocess.check_output(cmd)\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 424, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 528, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['./bazel-4.2.1-darwin-x86_64', 'run', '--verbose_failures=true', '--config=avx_posix', '--config=mkl_open_source_only', ':build_wheel', '--', '--output_path=/mypath/jax/dist', '--cpu=x86_64']' returned non-zero exit status 1.\n</code></pre>\n<p>Tried again a couple of times (deleted and redownloaded jax directory) and the same line 528 in <code>.../lib/python3.9/subprocess.py</code> seemed to cause the issue but the output, while mostly the above, sometimes was slightly different. Making me suspect an issue with memory, given that I (admittedly) had not restarted my machine in weeks and it was starting to unresponsively slow.</p>\n<p>I updated my XCode command line tools to version 12.2 (pretty sure).</p>\n<p>I restarted my (8GB) machine. Deleted and redownloaded jax directory. I installed bazel version 5.0.0 with homebrew in case that would help. I was a bit concerned that it kept downloading an x86 version for my ARM processor. <code>bazel</code> installation went fine.</p>\n<p>Started again from <a href=\"https://jax.readthedocs.io/en/latest/developer.html#building-jaxlib-from-source\" rel=\"noreferrer\">these instructions</a>. The <code>jaxlib</code> build, though, made it clear that it wanted an earlier (4.2.1) version of bazel and downloaded it, as before:</p>\n<pre><code>b'\\x1b[31mERROR: The project you\\'re trying to build requires Bazel 4.2.1 (specified in /mypath/jax/.bazelversion), but it wasn\\'t found in /opt/homebrew/Cellar/bazel/5.0.0/libexec/bin.\\x1b[0m\\n\\nBazel binaries for all official releases can be downloaded from here:\\n  https://github.com/bazelbuild/bazel/releases\\n\\nYou can download the required version directly using this command:\\n  (cd &quot;/opt/homebrew/Cellar/bazel/5.0.0/libexec/bin&quot; &amp;&amp; curl -fLO https://releases.bazel.build/4.2.1/release/bazel-4.2.1-darwin-x86_64 &amp;&amp; chmod +x bazel-4.2.1-darwin-x86_64)\\n'\n</code></pre>\n<p>Once again, a different error in the same line 528. Showing full run now:</p>\n<pre><code>b'\\x1b[31mERROR: The project you\\'re trying to build requires Bazel 4.2.1 (specified in /mypath/jax/.bazelversion), but it wasn\\'t found in /opt/homebrew/Cellar/bazel/5.0.0/libexec/bin.\\x1b[0m\\n\\nBazel binaries for all official releases can be downloaded from here:\\n  https://github.com/bazelbuild/bazel/releases\\n\\nYou can download the required version directly using this command:\\n  (cd &quot;/opt/homebrew/Cellar/bazel/5.0.0/libexec/bin&quot; &amp;&amp; curl -fLO https://releases.bazel.build/4.2.1/release/bazel-4.2.1-darwin-x86_64 &amp;&amp; chmod +x bazel-4.2.1-darwin-x86_64)\\n'\nDownloading bazel from: https://github.com/bazelbuild/bazel/releases/download/4.2.1/bazel-4.2.1-darwin-x86_64\nbazel-4.2.1-darwin-x86_64 [########################################] 100%\nBazel binary path: ./bazel-4.2.1-darwin-x86_64\nBazel version: 4.2.1\nPython binary path: /myuserpath/opt/anaconda3/envs/mytraxenv/bin/python\nPython version: 3.9\nNumPy version: 1.21.2\nMKL-DNN enabled: yes\nTarget CPU: x86_64\nTarget CPU features: release\nCUDA enabled: no\nTPU enabled: no\nROCm enabled: no\n\nBuilding XLA and installing it in the jaxlib source tree...\n./bazel-4.2.1-darwin-x86_64 run --verbose_failures=true --config=avx_posix --config=mkl_open_source_only :build_wheel -- --output_path=/mypath/jax/dist --cpu=x86_64\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\nINFO: Reading rc options for 'run' from /mypath/jax/.bazelrc:\n  Inherited 'common' options: --experimental_repo_remote_exec\nINFO: Reading rc options for 'run' from /mypath/jax/.bazelrc:\n  Inherited 'build' options: --apple_platform_type=macos --macos_minimum_os=10.9 --announce_rc --define open_source_build=true --spawn_strategy=standalone --enable_platform_specific_config --define=no_aws_support=true --define=no_gcp_support=true --define=no_hdfs_support=true --define=no_kafka_support=true --define=no_ignite_support=true --define=grpc_no_ares=true -c opt --config=short_logs --copt=-DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.\nINFO: Reading rc options for 'run' from /mypath/jax/.jax_configure.bazelrc:\n  Inherited 'build' options: --strategy=Genrule=standalone --repo_env PYTHON_BIN_PATH=/myuserpath/opt/anaconda3/envs/mytraxenv/bin/python --action_env=PYENV_ROOT --python_path=/myuserpath/opt/anaconda3/envs/mytraxenv/bin/python --distinct_host_configuration=false\nINFO: Found applicable config definition build:short_logs in file /mypath/jax/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\nINFO: Found applicable config definition build:avx_posix in file /mypath/jax/.bazelrc: --copt=-mavx --host_copt=-mavx\nINFO: Found applicable config definition build:mkl_open_source_only in file /mypath/jax/.bazelrc: --define=tensorflow_mkldnn_contraction_kernel=1\nINFO: Found applicable config definition build:macos in file /mypath/jax/.bazelrc: --config=posix\nINFO: Found applicable config definition build:posix in file /mypath/jax/.bazelrc: --copt=-fvisibility=hidden --copt=-Wno-sign-compare --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\nLoading: \nLoading: 0 packages loaded\nAnalyzing: target //build:build_wheel (1 packages loaded, 0 targets configured)\nAnalyzing: target //build:build_wheel (8 packages loaded, 286 targets configured)\nAnalyzing: target //build:build_wheel (10 packages loaded, 4271 targets configured)\nAnalyzing: target //build:build_wheel (10 packages loaded, 4271 targets configured)\nAnalyzing: target //build:build_wheel (11 packages loaded, 4494 targets configured)\nINFO: Analyzed target //build:build_wheel (11 packages loaded, 7417 targets configured).\n\nINFO: Found 1 target...\n[0 / 3] [Prepa] BazelWorkspaceStatusAction stable-status.txt\n[21 / 233] Compiling src/google/protobuf/generated_enum_util.cc; 2s local ... (6 actions, 3 running)\n[28 / 233] Compiling src/google/protobuf/extension_set.cc; 4s local ... (8 actions, 7 running)\n[114 / 523] Compiling src/google/protobuf/generated_message_util.cc; 2s local ... (8 actions running)\n[140 / 594] Compiling platform/c++11/src/nsync_semaphore_mutex.cc; 0s local ... (8 actions, 7 running)\n[237 / 597] Compiling src/google/protobuf/util/message_differencer.cc; 0s local ... (8 actions running)\n[252 / 597] Compiling src/google/protobuf/util/message_differencer.cc; 4s local ... (8 actions running)\n[272 / 597] Compiling src/google/protobuf/descriptor.pb.cc; 8s local ... (8 actions running)\n[296 / 597] Compiling src/google/protobuf/descriptor.cc; 12s local ... (8 actions running)\n[315 / 597] Compiling src/google/protobuf/descriptor_database.cc; 2s local ... (5 actions running)\n[487 / 1,963] Compiling src/compiler/python_generator.cc; 2s local ... (8 actions running)\n[579 / 3,117] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv3d.cc; 0s local ... (8 actions, 7 running)\n[619 / 3,173] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv3d.cc; 8s local ... (8 actions running)\n[687 / 3,270] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv3d.cc; 17s local ... (8 actions running)\n[774 / 3,464] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc; 27s local ... (8 actions running)\n[1,204 / 4,860] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc; 39s local ... (8 actions running)\n[1,255 / 4,916] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 52s local ... (8 actions running)\n[1,340 / 5,042] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 68s local ... (8 actions running)\n[1,456 / 5,156] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 86s local ... (8 actions, 7 running)\n[1,661 / 5,704] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 107s local ... (8 actions, 7 running)\n[1,688 / 5,704] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 132s local ... (8 actions, 7 running)\n[1,721 / 5,704] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 160s local ... (8 actions, 7 running)\n[2,106 / 6,584] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 192s local ... (8 actions, 7 running)\n[2,342 / 7,067] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 231s local ... (8 actions, 7 running)\n[2,378 / 7,067] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 274s local ... (8 actions, 7 running)\n[2,422 / 7,067] Compiling src/cpu/rnn/ref_rnn.cpp; 75s local ... (8 actions, 7 running)\n[2,452 / 7,067] Compiling src/cpu/x64/gemm/f32/jit_avx512_core_f32_copy_at_kern_part1_autogen.cpp; 54s local ... (8 actions, 7 running)\n[2,500 / 7,067] Compiling src/cpu/x64/gemm/f32/jit_avx512_core_f32_copy_at_kern_part1_autogen.cpp; 119s local ... (8 actions, 7 running)\n[2,577 / 7,067] Compiling src/common/memory_zero_pad.cpp; 74s local ... (8 actions, 7 running)\nERROR: /private/var/tmp/_bazel_a/f5e9a3325f07a1f02c52d821857db47c/external/org_tensorflow/tensorflow/compiler/xla/BUILD:69:17: ProtoCompile external/org_tensorflow/tensorflow/compiler/xla/xla.pb.h failed: (Illegal instruction): protoc failed: error executing command \n  (cd /private/var/tmp/_bazel_a/f5e9a3325f07a1f02c52d821857db47c/execroot/__main__ &amp;&amp; \\\n  exec env - \\\n    PATH=/myuserpath/opt/anaconda3/envs/mytraxenv/bin:/myuserpath/opt/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Library/Apple/usr/bin \\\n  bazel-out/darwin-opt/bin/external/com_google_protobuf/protoc '--cpp_out=bazel-out/darwin-opt/bin/external/org_tensorflow' -Iexternal/org_tensorflow -Ibazel-out/darwin-opt/bin/external/org_tensorflow -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/any_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/source_context_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/type_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/api_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/descriptor_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/compiler_plugin_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/duration_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/empty_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/field_mask_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/struct_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/timestamp_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/wrappers_proto external/org_tensorflow/tensorflow/compiler/xla/xla.proto)\nExecution platform: @local_execution_config_platform//:platform\nTarget //build:build_wheel failed to build\nINFO: Elapsed time: 631.189s, Critical Path: 283.35s\nINFO: 2563 processes: 935 internal, 1628 local.\nFAILED: Build did NOT complete successfully\nERROR: Build failed. Not running target\nFAILED: Build did NOT complete successfully\nb''\nTraceback (most recent call last):\n  File &quot;/mypath/jax/build/build.py&quot;, line 524, in &lt;module&gt;\n    main()\n  File &quot;/mypath/jax/build/build.py&quot;, line 519, in main\n    shell(command)\n  File &quot;/mypath/jax/build/build.py&quot;, line 53, in shell\n    output = subprocess.check_output(cmd)\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 424, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 528, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['./bazel-4.2.1-darwin-x86_64', 'run', '--verbose_failures=true', '--config=avx_posix', '--config=mkl_open_source_only', ':build_wheel', '--', '--output_path=/mypath/jax/dist', '--cpu=x86_64']' returned non-zero exit status 1.\n</code></pre>\n<p>I switched away from trying to build jaxlib. In <a href=\"https://github.com/google/jax/issues/5501#issuecomment-955590288\" rel=\"noreferrer\">late October 2021</a>, a M1-compatible jaxlib wheel was released, so I tried:</p>\n<pre><code>pip install -U pip\npip install -U https://storage.googleapis.com/jax-releases/mac/jaxlib-0.1.74-cp39-none-macosx_11_0_arm64.whl\n</code></pre>\n<p>but got</p>\n<pre><code>ERROR: jaxlib-0.1.74-cp39-none-macosx_11_0_arm64.whl is not a supported wheel on this platform.\n</code></pre>\n<p>Tried <a href=\"https://github.com/google/jax/issues/5501#issuecomment-1003671111\" rel=\"noreferrer\">upgrading python from 3.9 to 3.10</a>, but got the same message.</p>\n<p>Debugged by, in python, running</p>\n<pre><code>import platform\nprint(platform.machine())\n</code></pre>\n<p>which showed that my python is still running on x86 architechture, since <a href=\"https://github.com/google/jax/issues/5501#issuecomment-968998748\" rel=\"noreferrer\">&quot;Anaconda doesn't yet provide packages for M1/ARM&quot;</a>.</p>\n<p><strong>[see successful (so far) <code>jaxlib</code> installation in own answer below]</strong></p>\n<h2>back to <code>trax</code></h2>\n<p>After successfully installing <code>jaxlib</code> and <code>jax</code>, when trying to install <code>trax</code> with (miniforge’s)<code>conda install trax</code> I get:</p>\n<pre><code>Collecting package metadata (current_repodata.json): done\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\nCollecting package metadata (repodata.json): done\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\n\nPackagesNotFoundError: The following packages are not available from current channels:\n\n  - trax\n\nCurrent channels:\n\n  - https://conda.anaconda.org/conda-forge/osx-arm64\n  - https://conda.anaconda.org/conda-forge/noarch\n\nTo search for alternate channels that may provide the conda package you're\nlooking for, navigate to\n\n    https://anaconda.org\n\nand use the search bar at the top of the page.\n</code></pre>\n<p>I tried installing as <a href=\"https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html\" rel=\"noreferrer\"><code>trax</code> itself suggests</a> but ran into an error building <code>hp5y</code>:</p>\n<pre><code>ERROR: Failed building wheel for h5py\nERROR: Could not build wheels for h5py, which is required to install pyproject.toml-based projects\n</code></pre>\n<p>Looking now into this error and also looking into doing it with <code>miniforge</code> again but using Anaconda's channel to install <code>trax</code>. I'll update when I get to it.</p>\n<p>Any pointers or anyone who managed to install <code>trax</code> on an M1 Mac it by any different means?</p>\n",
        "output": "<p>[Update] As of <em>Nov 15, 2023</em>, a better solution to installing <code>jax</code> (not <code>trax</code>) is perhaps described <a href=\"https://developer.apple.com/metal/jax/\" rel=\"nofollow noreferrer\">here</a> with:</p>\n<p><code>python -m pip install jax-metal</code></p>\n<p>Followed by <strong>verification</strong> using:</p>\n<p><code>python -c 'import jax; print(jax.numpy.arange(10))'</code></p>\n",
        "from_id": "583834",
        "to_id": "756372",
        "answer_votes": 3,
        "question_votes": 6,
        "question_id": 70815864,
        "answer_id": 77490058,
        "question_title": "How to install trax, jax, jaxlib on M1 Mac on macOS 12?",
        "question_link": "https://stackoverflow.com/questions/70815864/how-to-install-trax-jax-jaxlib-on-m1-mac-on-macos-12",
        "answer_link": "https://stackoverflow.com/questions/70815864/how-to-install-trax-jax-jaxlib-on-m1-mac-on-macos-12/77490058#77490058"
    },
    {
        "text_input": "<h2><code>trax</code></h2>\n<p>New to <a href=\"https://github.com/google/trax\" rel=\"noreferrer\"><code>trax</code></a>, I'm trying to run it locally (macOS 12.1, Apple Silicon ARM M1 processor, 8GB RAM, Anaconda), but I'm running into some issues.</p>\n<p>In an environment with python 3.8.5, I installed <code>trax</code> running <code>pip3 install trax==1.3.9</code> inside an (Anaconda) conda environment. Later, I ran into issues when trying to import <code>trax</code> layers in my code with <code>from trax import layers as tl</code></p>\n<pre><code>RuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.\n</code></pre>\n<p>I rushed to start a new conda environment with python 3.10. Then ran into issue just trying to install <code>trax</code>:</p>\n<pre><code>macos ERROR: Could not find a version that satisfies the requirement tensorflow-text (from trax) (from versions: none)\nERROR: No matching distribution found for tensorflow-text\n</code></pre>\n<p>I then created a new environment with python 3.9. Installation went fine, but then ran into the same error importing layers:</p>\n<pre><code>RuntimeError: This version of jaxlib was built using AVX instructions, which your CPU and/or operating system do not support. You may be able work around this issue by building jaxlib from source.\n</code></pre>\n<h2>installing <code>jaxlib</code>, <code>jax</code> before <code>trax</code></h2>\n<p>I then tried building jaxlib from source following <a href=\"https://jax.readthedocs.io/en/latest/developer.html#building-jaxlib-from-source\" rel=\"noreferrer\">these instructions</a> and got this error:</p>\n<pre><code>Building XLA and installing it in the jaxlib source tree...\n./bazel-4.2.1-darwin-x86_64 run --verbose_failures=true --config=avx_posix --config=mkl_open_source_only :build_wheel -- --output_path=/my path/jax/dist --cpu=x86_64\nERROR: bazel does not currently work properly from paths containing spaces (/my path/jax).\nb''\nTraceback (most recent call last):\n  File &quot;/my path/jax/build/build.py&quot;, line 524, in &lt;module&gt;\n    main()\n  File &quot;/my path/jax/build/build.py&quot;, line 519, in main\n    shell(command)\n  File &quot;/my path/jax/build/build.py&quot;, line 53, in shell\n    output = subprocess.check_output(cmd)\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 424, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 528, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['./bazel-4.2.1-darwin-x86_64', 'run', '--verbose_failures=true', '--config=avx_posix', '--config=mkl_open_source_only', ':build_wheel', '--', '--output_path=/my path/jax/dist', '--cpu=x86_64']' returned non-zero exit status 36.\n</code></pre>\n<p>Emphasis on the part that I initially missed that says: <em>bazel does not currently work properly from paths containing spaces (/my path/jax)</em>.</p>\n<p>I moved my <code>/my path/</code> directory to a path without spaces <code>/mypath/</code>. Deleted and redownloaded jax directory. Still, the build (for CPU) with <code>python build/build.py</code> failed:</p>\n<pre><code>ERROR: /private/var/tmp/_bazel_a/2caf512c3c5e3f3f654bc58b48b8333a/external/llvm-project/llvm/BUILD.bazel:610:11: Generating code from table: include/llvm/IR/Intrinsics.td @llvm-project//llvm:intrinsic_XCore_gen__gen_intrinsic_enums__intrinsic_prefix_xcore_genrule failed: (Illegal instruction): bash failed: error executing command \n  (cd /private/var/tmp/_bazel_a/2caf512c3c5e3f3f654bc58b48b8333a/execroot/__main__ &amp;&amp; \\\n  exec env - \\\n    PATH=/myuserpath/opt/anaconda3/envs/mytraxenv/bin:/myuserpath/opt/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Library/Apple/usr/bin \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/darwin-opt/bin/external/llvm-project/llvm/llvm-tblgen -I external/llvm-project/llvm/include -I external/llvm-project/clang/include -I $(dirname external/llvm-project/llvm/include/llvm/IR/Intrinsics.td) -gen-intrinsic-enums -intrinsic-prefix=xcore external/llvm-project/llvm/include/llvm/IR/Intrinsics.td  -o bazel-out/darwin-opt/bin/external/llvm-project/llvm/include/llvm/IR/IntrinsicsXCore.h')\nExecution platform: @local_execution_config_platform//:platform\n/bin/bash: line 1: 11140 Illegal instruction: 4  bazel-out/darwin-opt/bin/external/llvm-project/llvm/llvm-tblgen -I external/llvm-project/llvm/include -I external/llvm-project/clang/include -I $(dirname external/llvm-project/llvm/include/llvm/IR/Intrinsics.td) -gen-intrinsic-enums -intrinsic-prefix=xcore external/llvm-project/llvm/include/llvm/IR/Intrinsics.td -o bazel-out/darwin-opt/bin/external/llvm-project/llvm/include/llvm/IR/IntrinsicsXCore.h\nTarget //build:build_wheel failed to build\nINFO: Elapsed time: 620.950s, Critical Path: 45.35s\nINFO: 589 processes: 132 internal, 457 local.\nFAILED: Build did NOT complete successfully\nERROR: Build failed. Not running target\nFAILED: Build did NOT complete successfully\nb''\nTraceback (most recent call last):\n  File &quot;/mypath/jax/build/build.py&quot;, line 524, in &lt;module&gt;\n    main()\n  File &quot;/mypath/jax/build/build.py&quot;, line 519, in main\n    shell(command)\n  File &quot;/mypath/jax/build/build.py&quot;, line 53, in shell\n    output = subprocess.check_output(cmd)\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 424, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 528, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['./bazel-4.2.1-darwin-x86_64', 'run', '--verbose_failures=true', '--config=avx_posix', '--config=mkl_open_source_only', ':build_wheel', '--', '--output_path=/mypath/jax/dist', '--cpu=x86_64']' returned non-zero exit status 1.\n</code></pre>\n<p>Tried again a couple of times (deleted and redownloaded jax directory) and the same line 528 in <code>.../lib/python3.9/subprocess.py</code> seemed to cause the issue but the output, while mostly the above, sometimes was slightly different. Making me suspect an issue with memory, given that I (admittedly) had not restarted my machine in weeks and it was starting to unresponsively slow.</p>\n<p>I updated my XCode command line tools to version 12.2 (pretty sure).</p>\n<p>I restarted my (8GB) machine. Deleted and redownloaded jax directory. I installed bazel version 5.0.0 with homebrew in case that would help. I was a bit concerned that it kept downloading an x86 version for my ARM processor. <code>bazel</code> installation went fine.</p>\n<p>Started again from <a href=\"https://jax.readthedocs.io/en/latest/developer.html#building-jaxlib-from-source\" rel=\"noreferrer\">these instructions</a>. The <code>jaxlib</code> build, though, made it clear that it wanted an earlier (4.2.1) version of bazel and downloaded it, as before:</p>\n<pre><code>b'\\x1b[31mERROR: The project you\\'re trying to build requires Bazel 4.2.1 (specified in /mypath/jax/.bazelversion), but it wasn\\'t found in /opt/homebrew/Cellar/bazel/5.0.0/libexec/bin.\\x1b[0m\\n\\nBazel binaries for all official releases can be downloaded from here:\\n  https://github.com/bazelbuild/bazel/releases\\n\\nYou can download the required version directly using this command:\\n  (cd &quot;/opt/homebrew/Cellar/bazel/5.0.0/libexec/bin&quot; &amp;&amp; curl -fLO https://releases.bazel.build/4.2.1/release/bazel-4.2.1-darwin-x86_64 &amp;&amp; chmod +x bazel-4.2.1-darwin-x86_64)\\n'\n</code></pre>\n<p>Once again, a different error in the same line 528. Showing full run now:</p>\n<pre><code>b'\\x1b[31mERROR: The project you\\'re trying to build requires Bazel 4.2.1 (specified in /mypath/jax/.bazelversion), but it wasn\\'t found in /opt/homebrew/Cellar/bazel/5.0.0/libexec/bin.\\x1b[0m\\n\\nBazel binaries for all official releases can be downloaded from here:\\n  https://github.com/bazelbuild/bazel/releases\\n\\nYou can download the required version directly using this command:\\n  (cd &quot;/opt/homebrew/Cellar/bazel/5.0.0/libexec/bin&quot; &amp;&amp; curl -fLO https://releases.bazel.build/4.2.1/release/bazel-4.2.1-darwin-x86_64 &amp;&amp; chmod +x bazel-4.2.1-darwin-x86_64)\\n'\nDownloading bazel from: https://github.com/bazelbuild/bazel/releases/download/4.2.1/bazel-4.2.1-darwin-x86_64\nbazel-4.2.1-darwin-x86_64 [########################################] 100%\nBazel binary path: ./bazel-4.2.1-darwin-x86_64\nBazel version: 4.2.1\nPython binary path: /myuserpath/opt/anaconda3/envs/mytraxenv/bin/python\nPython version: 3.9\nNumPy version: 1.21.2\nMKL-DNN enabled: yes\nTarget CPU: x86_64\nTarget CPU features: release\nCUDA enabled: no\nTPU enabled: no\nROCm enabled: no\n\nBuilding XLA and installing it in the jaxlib source tree...\n./bazel-4.2.1-darwin-x86_64 run --verbose_failures=true --config=avx_posix --config=mkl_open_source_only :build_wheel -- --output_path=/mypath/jax/dist --cpu=x86_64\nINFO: Options provided by the client:\n  Inherited 'common' options: --isatty=0 --terminal_columns=80\nINFO: Reading rc options for 'run' from /mypath/jax/.bazelrc:\n  Inherited 'common' options: --experimental_repo_remote_exec\nINFO: Reading rc options for 'run' from /mypath/jax/.bazelrc:\n  Inherited 'build' options: --apple_platform_type=macos --macos_minimum_os=10.9 --announce_rc --define open_source_build=true --spawn_strategy=standalone --enable_platform_specific_config --define=no_aws_support=true --define=no_gcp_support=true --define=no_hdfs_support=true --define=no_kafka_support=true --define=no_ignite_support=true --define=grpc_no_ares=true -c opt --config=short_logs --copt=-DMLIR_PYTHON_PACKAGE_PREFIX=jaxlib.mlir.\nINFO: Reading rc options for 'run' from /mypath/jax/.jax_configure.bazelrc:\n  Inherited 'build' options: --strategy=Genrule=standalone --repo_env PYTHON_BIN_PATH=/myuserpath/opt/anaconda3/envs/mytraxenv/bin/python --action_env=PYENV_ROOT --python_path=/myuserpath/opt/anaconda3/envs/mytraxenv/bin/python --distinct_host_configuration=false\nINFO: Found applicable config definition build:short_logs in file /mypath/jax/.bazelrc: --output_filter=DONT_MATCH_ANYTHING\nINFO: Found applicable config definition build:avx_posix in file /mypath/jax/.bazelrc: --copt=-mavx --host_copt=-mavx\nINFO: Found applicable config definition build:mkl_open_source_only in file /mypath/jax/.bazelrc: --define=tensorflow_mkldnn_contraction_kernel=1\nINFO: Found applicable config definition build:macos in file /mypath/jax/.bazelrc: --config=posix\nINFO: Found applicable config definition build:posix in file /mypath/jax/.bazelrc: --copt=-fvisibility=hidden --copt=-Wno-sign-compare --cxxopt=-std=c++14 --host_cxxopt=-std=c++14\nLoading: \nLoading: 0 packages loaded\nAnalyzing: target //build:build_wheel (1 packages loaded, 0 targets configured)\nAnalyzing: target //build:build_wheel (8 packages loaded, 286 targets configured)\nAnalyzing: target //build:build_wheel (10 packages loaded, 4271 targets configured)\nAnalyzing: target //build:build_wheel (10 packages loaded, 4271 targets configured)\nAnalyzing: target //build:build_wheel (11 packages loaded, 4494 targets configured)\nINFO: Analyzed target //build:build_wheel (11 packages loaded, 7417 targets configured).\n\nINFO: Found 1 target...\n[0 / 3] [Prepa] BazelWorkspaceStatusAction stable-status.txt\n[21 / 233] Compiling src/google/protobuf/generated_enum_util.cc; 2s local ... (6 actions, 3 running)\n[28 / 233] Compiling src/google/protobuf/extension_set.cc; 4s local ... (8 actions, 7 running)\n[114 / 523] Compiling src/google/protobuf/generated_message_util.cc; 2s local ... (8 actions running)\n[140 / 594] Compiling platform/c++11/src/nsync_semaphore_mutex.cc; 0s local ... (8 actions, 7 running)\n[237 / 597] Compiling src/google/protobuf/util/message_differencer.cc; 0s local ... (8 actions running)\n[252 / 597] Compiling src/google/protobuf/util/message_differencer.cc; 4s local ... (8 actions running)\n[272 / 597] Compiling src/google/protobuf/descriptor.pb.cc; 8s local ... (8 actions running)\n[296 / 597] Compiling src/google/protobuf/descriptor.cc; 12s local ... (8 actions running)\n[315 / 597] Compiling src/google/protobuf/descriptor_database.cc; 2s local ... (5 actions running)\n[487 / 1,963] Compiling src/compiler/python_generator.cc; 2s local ... (8 actions running)\n[579 / 3,117] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv3d.cc; 0s local ... (8 actions, 7 running)\n[619 / 3,173] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv3d.cc; 8s local ... (8 actions running)\n[687 / 3,270] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv3d.cc; 17s local ... (8 actions running)\n[774 / 3,464] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc; 27s local ... (8 actions running)\n[1,204 / 4,860] Compiling tensorflow/compiler/xla/service/cpu/runtime_single_threaded_conv2d.cc; 39s local ... (8 actions running)\n[1,255 / 4,916] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 52s local ... (8 actions running)\n[1,340 / 5,042] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 68s local ... (8 actions running)\n[1,456 / 5,156] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 86s local ... (8 actions, 7 running)\n[1,661 / 5,704] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 107s local ... (8 actions, 7 running)\n[1,688 / 5,704] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 132s local ... (8 actions, 7 running)\n[1,721 / 5,704] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 160s local ... (8 actions, 7 running)\n[2,106 / 6,584] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 192s local ... (8 actions, 7 running)\n[2,342 / 7,067] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 231s local ... (8 actions, 7 running)\n[2,378 / 7,067] Compiling tensorflow/compiler/xla/service/cpu/runtime_matmul.cc; 274s local ... (8 actions, 7 running)\n[2,422 / 7,067] Compiling src/cpu/rnn/ref_rnn.cpp; 75s local ... (8 actions, 7 running)\n[2,452 / 7,067] Compiling src/cpu/x64/gemm/f32/jit_avx512_core_f32_copy_at_kern_part1_autogen.cpp; 54s local ... (8 actions, 7 running)\n[2,500 / 7,067] Compiling src/cpu/x64/gemm/f32/jit_avx512_core_f32_copy_at_kern_part1_autogen.cpp; 119s local ... (8 actions, 7 running)\n[2,577 / 7,067] Compiling src/common/memory_zero_pad.cpp; 74s local ... (8 actions, 7 running)\nERROR: /private/var/tmp/_bazel_a/f5e9a3325f07a1f02c52d821857db47c/external/org_tensorflow/tensorflow/compiler/xla/BUILD:69:17: ProtoCompile external/org_tensorflow/tensorflow/compiler/xla/xla.pb.h failed: (Illegal instruction): protoc failed: error executing command \n  (cd /private/var/tmp/_bazel_a/f5e9a3325f07a1f02c52d821857db47c/execroot/__main__ &amp;&amp; \\\n  exec env - \\\n    PATH=/myuserpath/opt/anaconda3/envs/mytraxenv/bin:/myuserpath/opt/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Library/Apple/usr/bin \\\n  bazel-out/darwin-opt/bin/external/com_google_protobuf/protoc '--cpp_out=bazel-out/darwin-opt/bin/external/org_tensorflow' -Iexternal/org_tensorflow -Ibazel-out/darwin-opt/bin/external/org_tensorflow -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/any_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/source_context_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/type_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/api_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/descriptor_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/compiler_plugin_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/duration_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/empty_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/field_mask_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/struct_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/timestamp_proto -Ibazel-out/darwin-opt/bin/external/com_google_protobuf/_virtual_imports/wrappers_proto external/org_tensorflow/tensorflow/compiler/xla/xla.proto)\nExecution platform: @local_execution_config_platform//:platform\nTarget //build:build_wheel failed to build\nINFO: Elapsed time: 631.189s, Critical Path: 283.35s\nINFO: 2563 processes: 935 internal, 1628 local.\nFAILED: Build did NOT complete successfully\nERROR: Build failed. Not running target\nFAILED: Build did NOT complete successfully\nb''\nTraceback (most recent call last):\n  File &quot;/mypath/jax/build/build.py&quot;, line 524, in &lt;module&gt;\n    main()\n  File &quot;/mypath/jax/build/build.py&quot;, line 519, in main\n    shell(command)\n  File &quot;/mypath/jax/build/build.py&quot;, line 53, in shell\n    output = subprocess.check_output(cmd)\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 424, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File &quot;/myuserpath/opt/anaconda3/envs/mytraxenv/lib/python3.9/subprocess.py&quot;, line 528, in run\n    raise CalledProcessError(retcode, process.args,\nsubprocess.CalledProcessError: Command '['./bazel-4.2.1-darwin-x86_64', 'run', '--verbose_failures=true', '--config=avx_posix', '--config=mkl_open_source_only', ':build_wheel', '--', '--output_path=/mypath/jax/dist', '--cpu=x86_64']' returned non-zero exit status 1.\n</code></pre>\n<p>I switched away from trying to build jaxlib. In <a href=\"https://github.com/google/jax/issues/5501#issuecomment-955590288\" rel=\"noreferrer\">late October 2021</a>, a M1-compatible jaxlib wheel was released, so I tried:</p>\n<pre><code>pip install -U pip\npip install -U https://storage.googleapis.com/jax-releases/mac/jaxlib-0.1.74-cp39-none-macosx_11_0_arm64.whl\n</code></pre>\n<p>but got</p>\n<pre><code>ERROR: jaxlib-0.1.74-cp39-none-macosx_11_0_arm64.whl is not a supported wheel on this platform.\n</code></pre>\n<p>Tried <a href=\"https://github.com/google/jax/issues/5501#issuecomment-1003671111\" rel=\"noreferrer\">upgrading python from 3.9 to 3.10</a>, but got the same message.</p>\n<p>Debugged by, in python, running</p>\n<pre><code>import platform\nprint(platform.machine())\n</code></pre>\n<p>which showed that my python is still running on x86 architechture, since <a href=\"https://github.com/google/jax/issues/5501#issuecomment-968998748\" rel=\"noreferrer\">&quot;Anaconda doesn't yet provide packages for M1/ARM&quot;</a>.</p>\n<p><strong>[see successful (so far) <code>jaxlib</code> installation in own answer below]</strong></p>\n<h2>back to <code>trax</code></h2>\n<p>After successfully installing <code>jaxlib</code> and <code>jax</code>, when trying to install <code>trax</code> with (miniforge’s)<code>conda install trax</code> I get:</p>\n<pre><code>Collecting package metadata (current_repodata.json): done\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\nCollecting package metadata (repodata.json): done\nSolving environment: failed with initial frozen solve. Retrying with flexible solve.\n\nPackagesNotFoundError: The following packages are not available from current channels:\n\n  - trax\n\nCurrent channels:\n\n  - https://conda.anaconda.org/conda-forge/osx-arm64\n  - https://conda.anaconda.org/conda-forge/noarch\n\nTo search for alternate channels that may provide the conda package you're\nlooking for, navigate to\n\n    https://anaconda.org\n\nand use the search bar at the top of the page.\n</code></pre>\n<p>I tried installing as <a href=\"https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html\" rel=\"noreferrer\"><code>trax</code> itself suggests</a> but ran into an error building <code>hp5y</code>:</p>\n<pre><code>ERROR: Failed building wheel for h5py\nERROR: Could not build wheels for h5py, which is required to install pyproject.toml-based projects\n</code></pre>\n<p>Looking now into this error and also looking into doing it with <code>miniforge</code> again but using Anaconda's channel to install <code>trax</code>. I'll update when I get to it.</p>\n<p>Any pointers or anyone who managed to install <code>trax</code> on an M1 Mac it by any different means?</p>\n",
        "output": "<h1>Anaconda Source build</h1>\n<p>As of Mon, 7th of Feb 2022, installing using an Anaconda installation of python fails as Anaconda does not provide arm wheels for python.</p>\n<p>When they do provide them, building from source should work automatically. If it does not, clone the repository, and use the following:</p>\n<pre class=\"lang-sh prettyprint-override\"><code>python build/build.py --target_cpu darwin_arm64  \n</code></pre>\n<p>If bazel does not build and you are using homebrew consider installing it, and removing the downloaded bazel files.</p>\n<pre class=\"lang-sh prettyprint-override\"><code>brew install bazel\nrm -rf bazel-*\n</code></pre>\n<h1>Arm64 Python wheels</h1>\n<p>I recommend downloading arm64 wheels from the official CPython website.\nThe executable will be in <code>/usr/local/bin/python3</code>, and so will <code>pip3</code>.</p>\n<p>This should allow you to install jax using</p>\n<pre><code>/usr/local/bin/pip3 install --upgrade 'jax[cpu]'\n</code></pre>\n<p>To have access to the proper python with the command line/path, add the folder to the path.</p>\n<pre class=\"lang-sh prettyprint-override\"><code>export PATH=/usr/share/bin:$PATH\n</code></pre>\n<h2>Virtual Env</h2>\n<p>I recommend against the above method, and instead to use to use <code>virtual-env</code> from here <a href=\"https://virtualenv.pypa.io\" rel=\"nofollow noreferrer\">https://virtualenv.pypa.io</a>, to create virtual environments as with anaconda, and install jax in a virtual environment as above.</p>\n<h2>Poetry</h2>\n<p>Poetry is an even better option since it uses virtual-env, and allows you to build your projects and list dependencies just as anaconda.</p>\n<p>There is a minor caveat as I am writing this, installing <code>scipy</code> when defining dependencies with poetry results in an odd error, installing <code>numpy</code> and <code>scipy</code> manually using</p>\n<pre><code>poetry shell # to go to virtual environment for the project\npip install --upgrade 'jax[cpu]' jaxlib scipy numpy\n</code></pre>\n<h1>Trax</h1>\n<p>Since anaconda is not option, installing hdf5 is a bit of a hassle, but it can be done with <code>brew</code>/<code>homebrew</code>. Assuming you have activated the correct venv:</p>\n<pre><code>brew install hdf5\nexport HDF5_DIR=/opt/homebrew/Cellar/hdf5/1.12.1 \npip install trax\n</code></pre>\n<p>Unfortunately this results in the following error:</p>\n<pre><code>import trax\n/Users/quinn/Library/Caches/pypoetry/virtualenvs/no-xNvLksOy-py3.9/lib/python3.9/site-packages/jax/_src/lib/__init__.py:32: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n  warnings.warn(&quot;JAX on Mac ARM machines is experimental and minimally tested. &quot;\n\n\n***************************************************************\nFailed to import TensorFlow. Please note that TensorFlow is not installed by default when you install TFDS. This allow you to choose to install either `tf-nightly` or `tensorflow`. Please install the most recent version of TensorFlow, by following instructions at https://tensorflow.org/install.\n***************************************************************\n\n\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;/Users/quinn/Library/Caches/pypoetry/virtualenvs/no-xNvLksOy-py3.9/lib/python3.9/site-packages/trax/__init__.py&quot;, line 18, in &lt;module&gt;\n    from trax import layers\n  File &quot;/Users/quinn/Library/Caches/pypoetry/virtualenvs/no-xNvLksOy-py3.9/lib/python3.9/site-packages/trax/layers/__init__.py&quot;, line 23, in &lt;module&gt;\n    from trax.layers.activation_fns import *\n  File &quot;/Users/quinn/Library/Caches/pypoetry/virtualenvs/no-xNvLksOy-py3.9/lib/python3.9/site-packages/trax/layers/activation_fns.py&quot;, line 26, in &lt;module&gt;\n    from trax import math\n  File &quot;/Users/quinn/Library/Caches/pypoetry/virtualenvs/no-xNvLksOy-py3.9/lib/python3.9/site-packages/trax/math/__init__.py&quot;, line 18, in &lt;module&gt;\n    from trax.math import jax as jax_math\n  File &quot;/Users/quinn/Library/Caches/pypoetry/virtualenvs/no-xNvLksOy-py3.9/lib/python3.9/site-packages/trax/math/jax.py&quot;, line 28, in &lt;module&gt;\n    import tensorflow_datasets as tfds\n  File &quot;/Users/quinn/Library/Caches/pypoetry/virtualenvs/no-xNvLksOy-py3.9/lib/python3.9/site-packages/tensorflow_datasets/__init__.py&quot;, line 43, in &lt;module&gt;\n    from tensorflow_datasets.core import tf_compat\n  File &quot;/Users/quinn/Library/Caches/pypoetry/virtualenvs/no-xNvLksOy-py3.9/lib/python3.9/site-packages/tensorflow_datasets/core/__init__.py&quot;, line 22, in &lt;module&gt;\n    tf_compat.ensure_tf_install()\n  File &quot;/Users/quinn/Library/Caches/pypoetry/virtualenvs/no-xNvLksOy-py3.9/lib/python3.9/site-packages/tensorflow_datasets/core/tf_compat.py&quot;, line 48, in ensure_tf_install\n    import tensorflow as tf  # pylint: disable=import-outside-toplevel\nModuleNotFoundError: No module named 'tensorflow'\n</code></pre>\n",
        "from_id": "583834",
        "to_id": "18144086",
        "answer_votes": 0,
        "question_votes": 6,
        "question_id": 70815864,
        "answer_id": 71022944,
        "question_title": "How to install trax, jax, jaxlib on M1 Mac on macOS 12?",
        "question_link": "https://stackoverflow.com/questions/70815864/how-to-install-trax-jax-jaxlib-on-m1-mac-on-macos-12",
        "answer_link": "https://stackoverflow.com/questions/70815864/how-to-install-trax-jax-jaxlib-on-m1-mac-on-macos-12/71022944#71022944"
    },
    {
        "text_input": "<p>I'm trying to run <a href=\"https://github.com/google-research/google-research/tree/master/regnerf\" rel=\"noreferrer\">RegNeRF</a>, which requires flax. On installing the latest version of flax==0.6.0, I got an error stating flax has no attribute optim. <a href=\"https://stackoverflow.com/a/73501131/3337089\">This answer</a> suggested to downgrade flax to 0.5.1. On doing that, now I'm getting the error <code>AttributeError: module 'flax' has no attribute 'nn'</code></p>\n<p>I could not find any solutions on the web for this error. Any help is appreciated.</p>\n<p>I'm using ubuntu 20.04</p>\n",
        "output": "<p>The <code>flax.optim</code> module has been moved to <code>optax</code> as of flax version 0.6.0; see <a href=\"https://flax.readthedocs.io/en/latest/advanced_topics/optax_update_guide.html#replacing-flax-optim-with-optax\" rel=\"nofollow noreferrer\">Upgrading my Codebase to Optax</a> for information on how to migrate your code. If you're using external code that imports <code>flax.optim</code> and can't update these references, you'll have to install flax version 0.5.3 or older.</p>\n<p>Regarding <code>flax.nn</code>: this module was replaced by <code>flax.linen</code> in flax version 0.4.0. See <a href=\"https://flax.readthedocs.io/en/latest/advanced_topics/linen_upgrade_guide.html?highlight=flax.nn#upgrading-my-codebase-to-linen\" rel=\"nofollow noreferrer\">Upgrading my Codebase to Linen</a> for information on this migration. If you're using external code that imports <code>flax.nn</code> and can't update these references, you'll have to install flax version 0.3.6 or older.</p>\n",
        "from_id": "3337089",
        "to_id": "2937831",
        "answer_votes": 4,
        "question_votes": 6,
        "question_id": 73924768,
        "answer_id": 73926711,
        "question_title": "AttributeError: module &#39;flax&#39; has no attribute &#39;nn&#39;",
        "question_link": "https://stackoverflow.com/questions/73924768/attributeerror-module-flax-has-no-attribute-nn",
        "answer_link": "https://stackoverflow.com/questions/73924768/attributeerror-module-flax-has-no-attribute-nn/73926711#73926711"
    },
    {
        "text_input": "<p>I am using JAX for auto differentiation. In this, I am trying to convert concrete tracer values to regular float values using <code>astype(float)</code> but it still seems to return a concrete tracer value. However when I do <code>astype(int)</code> it seems to correctly convert the concrete tracer into an int.</p>\n<p>Here is an example:</p>\n<pre><code>import jax\nfrom jax import numpy as jnp\n\ndef func(mu):\n\n  print((mu.astype(float)))\n  print(mu)\n  print(mu.astype(int))\n\n  return mu\n\nf = jax.grad(func)\n\nprint(f(jnp.array(5.0)))\n\n</code></pre>\n<p>This returns the following:</p>\n<pre><code>Traced&lt;ConcreteArray(5.0)&gt;with&lt;JVPTrace(level=2/0)&gt;\n  with primal = Traced&lt;ConcreteArray(5.0):JaxprTrace(level=1/0)&gt;\n       tangent = Traced&lt;ShapedArray(float32[]):JaxprTrace(level=1/0)&gt;\nTraced&lt;ConcreteArray(5.0)&gt;with&lt;JVPTrace(level=2/0)&gt;\n  with primal = Traced&lt;ConcreteArray(5.0):JaxprTrace(level=1/0)&gt;\n       tangent = Traced&lt;ShapedArray(float32[]):JaxprTrace(level=1/0)&gt;\n5\n1.0\n</code></pre>\n<p>Clearly <code>print(mu)</code> and <code>print(mu.astype(float))</code> seem to return the exact same thing while <code>print(mu.astype(int))</code> returns the correct int value. Is this expected? This is a simple example but in my original function I would like to convert one of the input parameters into a regular float value.</p>\n<p>Thanks in advance!</p>\n",
        "output": "<p>There is some information on this in JAX's FAQ: <a href=\"https://jax.readthedocs.io/en/latest/faq.html#how-can-i-convert-a-jax-tracer-to-a-numpy-array\" rel=\"nofollow noreferrer\">https://jax.readthedocs.io/en/latest/faq.html#how-can-i-convert-a-jax-tracer-to-a-numpy-array</a></p>\n<p>In general, it is not safe to convert traced values to Python floats, because any operations you do on the float value will no longer be tracked by JAX's autodiff, and so autodiff may return incorrect results. This is why JAX allows you to call <code>int</code> on a JVP tracer (integer values cannot affect autodiff) while returning a traced value on <code>float</code> (float values do affect autodiff!).</p>\n<p>If you would like to call a non-JAX function from within a transformed JAX function, your best bet is probably to use <code>pure_callback</code> along with <code>custom_jvp</code>; there is an example in the JAX docs at <a href=\"https://jax.readthedocs.io/en/latest/notebooks/external_callbacks.html#example-pure-callback-with-custom-jvp\" rel=\"nofollow noreferrer\">External Callbacks in JAX: <code>pure_callback</code> with <code>custom_jvp</code></a>.</p>\n<p>Sorry for not answering the question earlier – these callback tools are relatively new and there was no good answer to your question when it was first asked!</p>\n",
        "from_id": "15342355",
        "to_id": "2937831",
        "answer_votes": 0,
        "question_votes": 6,
        "question_id": 70579352,
        "answer_id": 75087764,
        "question_title": "JAX: Converting Concrete Tracer values to regular float values doesnt work",
        "question_link": "https://stackoverflow.com/questions/70579352/jax-converting-concrete-tracer-values-to-regular-float-values-doesnt-work",
        "answer_link": "https://stackoverflow.com/questions/70579352/jax-converting-concrete-tracer-values-to-regular-float-values-doesnt-work/75087764#75087764"
    },
    {
        "text_input": "<p>I have the following numpy function as seen below that I'm trying to optimize by using JAX but for whatever reason, it's slower.</p>\n<p>Could someone point out what I can do to improve the performance here? I suspect it has to do with the list comprehension taking place for Cg_new but breaking that apart doesn't yield any further performance gains in JAX.</p>\n<pre><code>import numpy as np \n\ndef testFunction_numpy(C, Mi, C_new, Mi_new):\n    Wg_new = np.zeros((len(Mi_new[:,0]), len(Mi[0])))\n    Cg_new = np.zeros((1, len(Mi[0])))\n    invertCsensor_new = np.linalg.inv(C_new)\n\n    Wg_new = np.dot(invertCsensor_new, Mi_new)\n    Cg_new = [np.dot(((-0.5*(Mi_new[:,m].conj().T))), (Wg_new[:,m])) for m in range(0, len(Mi[0]))] \n\n    return C_new, Mi_new, Wg_new, Cg_new\n\nC = np.random.rand(483,483)\nMi = np.random.rand(483,8)\nC_new = np.random.rand(198,198)\nMi_new = np.random.rand(198,8)\n\n%timeit testFunction_numpy(C, Mi, C_new, Mi_new)\n#1000 loops, best of 3: 1.73 ms per loop\n</code></pre>\n<p>Here's the JAX equivalent:</p>\n<pre><code>import jax.numpy as jnp\nimport numpy as np\nimport jax\n\ndef testFunction_JAX(C, Mi, C_new, Mi_new):\n    Wg_new = jnp.zeros((len(Mi_new[:,0]), len(Mi[0])))\n    Cg_new = jnp.zeros((1, len(Mi[0])))\n    invertCsensor_new = jnp.linalg.inv(C_new)\n\n    Wg_new = jnp.dot(invertCsensor_new, Mi_new)\n    Cg_new = [jnp.dot(((-0.5*(Mi_new[:,m].conj().T))), (Wg_new[:,m])) for m in range(0, len(Mi[0]))] \n\n    return C_new, Mi_new, Wg_new, Cg_new\n\nC = np.random.rand(483,483)\nMi = np.random.rand(483,8)\nC_new = np.random.rand(198,198)\nMi_new = np.random.rand(198,8)\n\nC = jnp.asarray(C)\nMi = jnp.asarray(Mi)\nC_new = jnp.asarray(C_new)\nMi_new = jnp.asarray(Mi_new)\n\njitter = jax.jit(testFunction_JAX) \n\n%timeit jitter(C, Mi, C_new, Mi_new)\n#1 loop, best of 3: 4.96 ms per loop\n</code></pre>\n",
        "output": "<p>For general considerations on benchmark comparisons between JAX and NumPy, see <a href=\"https://jax.readthedocs.io/en/latest/faq.html#is-jax-faster-than-numpy\" rel=\"nofollow noreferrer\">https://jax.readthedocs.io/en/latest/faq.html#is-jax-faster-than-numpy</a></p>\n<p>As for your particular code: when JAX jit compilation encounters Python control flow, including list comprehensions, it effectively flattens the loop and stages the full sequence of operations. This can lead to slow jit compile times and suboptimal code. Fortunately, the list comprehension in your function is readily expressed in terms of native numpy broadcasting. Additionally, there are two other improvements you can make:</p>\n<ul>\n<li>there is no need to forward declare <code>Wg_new</code> and <code>Cg_new</code> before computing them</li>\n<li>when computing <code>dot(inv(A), B)</code>, it is much more efficient and precise to use <code>np.linalg.solve</code> rather than explicitly computing the inverse.</li>\n</ul>\n<p>Making these three improvements to both the numpy and JAX versions result in the following:</p>\n<pre class=\"lang-python prettyprint-override\"><code>def testFunction_numpy_v2(C, Mi, C_new, Mi_new):\n    Wg_new = np.linalg.solve(C_new, Mi_new)\n    Cg_new = -0.5 * (Mi_new.conj() * Wg_new).sum(0)\n    return C_new, Mi_new, Wg_new, Cg_new\n\n@jax.jit\ndef testFunction_JAX_v2(C, Mi, C_new, Mi_new):\n    Wg_new = jnp.linalg.solve(C_new, Mi_new)\n    Cg_new = -0.5 * (Mi_new.conj() * Wg_new).sum(0)\n    return C_new, Mi_new, Wg_new, Cg_new\n\n%timeit testFunction_numpy_v2(C, Mi, C_new, Mi_new)\n# 1000 loops, best of 3: 1.11 ms per loop\n%timeit testFunction_JAX_v2(C_jax, Mi_jax, C_new_jax, Mi_new_jax)\n# 1000 loops, best of 3: 1.35 ms per loop\n</code></pre>\n<p>Both functions are a fair bit faster than they were previously due to the improved implementation. You'll notice, however, that JAX is still slower than numpy here; this is somewhat to be expected because for a function of this level of simplicity, JAX and numpy are both generating effectively the same short series of BLAS and LAPACK calls executed on a CPU architecture. There's simply not much room for improvement over numpy's reference implementation, and with such small arrays JAX's overhead is apparent.</p>\n",
        "from_id": "13191305",
        "to_id": "2937831",
        "answer_votes": 12,
        "question_votes": 5,
        "question_id": 64517793,
        "answer_id": 64524180,
        "question_title": "Why is this function slower in JAX vs numpy?",
        "question_link": "https://stackoverflow.com/questions/64517793/why-is-this-function-slower-in-jax-vs-numpy",
        "answer_link": "https://stackoverflow.com/questions/64517793/why-is-this-function-slower-in-jax-vs-numpy/64524180#64524180"
    },
    {
        "text_input": "<p>I have the following numpy function as seen below that I'm trying to optimize by using JAX but for whatever reason, it's slower.</p>\n<p>Could someone point out what I can do to improve the performance here? I suspect it has to do with the list comprehension taking place for Cg_new but breaking that apart doesn't yield any further performance gains in JAX.</p>\n<pre><code>import numpy as np \n\ndef testFunction_numpy(C, Mi, C_new, Mi_new):\n    Wg_new = np.zeros((len(Mi_new[:,0]), len(Mi[0])))\n    Cg_new = np.zeros((1, len(Mi[0])))\n    invertCsensor_new = np.linalg.inv(C_new)\n\n    Wg_new = np.dot(invertCsensor_new, Mi_new)\n    Cg_new = [np.dot(((-0.5*(Mi_new[:,m].conj().T))), (Wg_new[:,m])) for m in range(0, len(Mi[0]))] \n\n    return C_new, Mi_new, Wg_new, Cg_new\n\nC = np.random.rand(483,483)\nMi = np.random.rand(483,8)\nC_new = np.random.rand(198,198)\nMi_new = np.random.rand(198,8)\n\n%timeit testFunction_numpy(C, Mi, C_new, Mi_new)\n#1000 loops, best of 3: 1.73 ms per loop\n</code></pre>\n<p>Here's the JAX equivalent:</p>\n<pre><code>import jax.numpy as jnp\nimport numpy as np\nimport jax\n\ndef testFunction_JAX(C, Mi, C_new, Mi_new):\n    Wg_new = jnp.zeros((len(Mi_new[:,0]), len(Mi[0])))\n    Cg_new = jnp.zeros((1, len(Mi[0])))\n    invertCsensor_new = jnp.linalg.inv(C_new)\n\n    Wg_new = jnp.dot(invertCsensor_new, Mi_new)\n    Cg_new = [jnp.dot(((-0.5*(Mi_new[:,m].conj().T))), (Wg_new[:,m])) for m in range(0, len(Mi[0]))] \n\n    return C_new, Mi_new, Wg_new, Cg_new\n\nC = np.random.rand(483,483)\nMi = np.random.rand(483,8)\nC_new = np.random.rand(198,198)\nMi_new = np.random.rand(198,8)\n\nC = jnp.asarray(C)\nMi = jnp.asarray(Mi)\nC_new = jnp.asarray(C_new)\nMi_new = jnp.asarray(Mi_new)\n\njitter = jax.jit(testFunction_JAX) \n\n%timeit jitter(C, Mi, C_new, Mi_new)\n#1 loop, best of 3: 4.96 ms per loop\n</code></pre>\n",
        "output": "<p>I tested the problem with <a href=\"https://github.com/nschloe/perfplot\" rel=\"nofollow noreferrer\">perfplot</a> across a range of problem sizes. Result: jax is ever so slightly faster. The reason why jax doesn't outperform numpy here is that it's run on a CPU (just like NumPy) and here, NumPy already pretty optimized. (It uses BLAS/LAPACK under the hood.)</p>\n<p><a href=\"https://i.sstatic.net/tG1Eu.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.sstatic.net/tG1Eu.png\" alt=\"enter image description here\" /></a></p>\n<p>Code to reproduce the plot:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import jax.numpy as jnp\nimport jax\nimport numpy as np\nimport perfplot\n\n\ndef setup(n):\n    C_new = np.random.rand(n, n)\n    Mi_new = np.random.rand(n, 8)\n    return C_new, Mi_new\n\n\ndef testFunction_numpy_v2(C_new, Mi_new):\n    Wg_new = np.linalg.solve(C_new, Mi_new)\n    Cg_new = -0.5 * (Mi_new.conj() * Wg_new).sum(0)\n    return Wg_new, Cg_new\n\n\n@jax.jit\ndef testFunction_JAX_v2(C_new, Mi_new):\n    Wg_new = jnp.linalg.solve(C_new, Mi_new)\n    Cg_new = -0.5 * (Mi_new.conj() * Wg_new).sum(0)\n    return Wg_new, Cg_new\n\n\nb = perfplot.bench(\n    setup=setup,\n    kernels=[testFunction_numpy_v2, testFunction_JAX_v2],\n    n_range=[2 ** k for k in range(14)],\n    equality_check=None\n)\nb.save(&quot;out.png&quot;)\nb.show()\n</code></pre>\n",
        "from_id": "13191305",
        "to_id": "353337",
        "answer_votes": 0,
        "question_votes": 5,
        "question_id": 64517793,
        "answer_id": 70931020,
        "question_title": "Why is this function slower in JAX vs numpy?",
        "question_link": "https://stackoverflow.com/questions/64517793/why-is-this-function-slower-in-jax-vs-numpy",
        "answer_link": "https://stackoverflow.com/questions/64517793/why-is-this-function-slower-in-jax-vs-numpy/70931020#70931020"
    },
    {
        "text_input": "<p>Since array is immutable in Jax, so when one updates N indexes, it creates N arrays with</p>\n<pre><code>x = x.at[idx].set(y)\n</code></pre>\n<p>With hundreds of updates per training cycle, it will ultimately create hundreds of arrays if not millions.\nThis seems a little wasteful, is there a way to update multiple index at one go?\nDoes anyone know if there is overhead or if it's significant? Am I overlook at this?</p>\n",
        "output": "<p>You can perform multiple updates in a single operation using the syntax you mention. For example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import jax.numpy as jnp\n\nx = jnp.zeros(10)\nidx = jnp.array([3, 5, 7, 9])\ny = jnp.array([1, 2, 3, 4])\n\nx = x.at[idx].set(y)\nprint(x)\n# [0. 0. 0. 1. 0. 2. 0. 3. 0. 4.]\n</code></pre>\n<p>You're correct that outside JIT, each update operation will create an array copy. But within JIT-compiled functions, the compiler is able to perform such updates in-place when it is possible (for example, when the original array is not referenced again). You can read more at <a href=\"https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#array-updates-x-at-idx-set-y\" rel=\"noreferrer\">JAX Sharp Bits: Array Updates</a>.</p>\n",
        "from_id": "18116886",
        "to_id": "2937831",
        "answer_votes": 5,
        "question_votes": 5,
        "question_id": 74700328,
        "answer_id": 74703173,
        "question_title": "Is there a way to update multiple indexes of Jax array at once?",
        "question_link": "https://stackoverflow.com/questions/74700328/is-there-a-way-to-update-multiple-indexes-of-jax-array-at-once",
        "answer_link": "https://stackoverflow.com/questions/74700328/is-there-a-way-to-update-multiple-indexes-of-jax-array-at-once/74703173#74703173"
    },
    {
        "text_input": "<p>Since array is immutable in Jax, so when one updates N indexes, it creates N arrays with</p>\n<pre><code>x = x.at[idx].set(y)\n</code></pre>\n<p>With hundreds of updates per training cycle, it will ultimately create hundreds of arrays if not millions.\nThis seems a little wasteful, is there a way to update multiple index at one go?\nDoes anyone know if there is overhead or if it's significant? Am I overlook at this?</p>\n",
        "output": "<p>This sounds very like a job for <strong>scatter update</strong>. I'm not really familiar with Jax itself, but major frameworks have it:</p>\n<p><a href=\"https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter.html\" rel=\"nofollow noreferrer\">https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scatter.html</a></p>\n<p>What it does in a nutshell:</p>\n<ol>\n<li>setup your output tensor (<strong>x</strong>)</li>\n<li>accumulate required updates in the other tensor (<strong>y</strong> in your case)</li>\n<li>accumulate in list/tensor indices where to apply you updates (create tensor/list full of <strong>index</strong>)</li>\n<li>feed 1)-3) to scatter_updated</li>\n</ol>\n",
        "from_id": "18116886",
        "to_id": "5823050",
        "answer_votes": 3,
        "question_votes": 5,
        "question_id": 74700328,
        "answer_id": 74700923,
        "question_title": "Is there a way to update multiple indexes of Jax array at once?",
        "question_link": "https://stackoverflow.com/questions/74700328/is-there-a-way-to-update-multiple-indexes-of-jax-array-at-once",
        "answer_link": "https://stackoverflow.com/questions/74700328/is-there-a-way-to-update-multiple-indexes-of-jax-array-at-once/74700923#74700923"
    },
    {
        "text_input": "<p>I've have the following doubt about Jax. I'll use an example from the official <a href=\"https://optax.readthedocs.io/en/latest/optax-101.html\" rel=\"nofollow noreferrer\">optax docs</a> to illustrate it:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def fit(params: optax.Params, optimizer: optax.GradientTransformation) -&gt; optax.Params:\n  opt_state = optimizer.init(params)\n\n  @jax.jit\n  def step(params, opt_state, batch, labels):\n    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n    updates, opt_state = optimizer.update(grads, opt_state, params)\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss_value\n\n  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n    if i % 100 == 0:\n      print(f'step {i}, loss: {loss_value}')\n\n  return params\n\n# Finally, we can fit our parametrized function using the Adam optimizer\n# provided by optax.\noptimizer = optax.adam(learning_rate=1e-2)\nparams = fit(initial_params, optimizer)\n</code></pre>\n<p>In this example, the function <code>step</code> uses the variable <code>optimizer</code> despite it not being passed within the function arguments (since the function is being jitted and <code>optax.GradientTransformation</code> is not a supported type). However, the same function uses other variables that are instead passed as parameters (i.e., <code>params, opt_state, batch, labels</code>). I understand that jax functions needs to be pure in order to be jitted, but what about input (read-only) variables. Is there any difference if I access a variable by passing it through the function arguments or if I access it directly since it's in the <code>step</code> function scope? What if this variable is not constant but modified between separate <code>step</code> calls? Are they treated like static arguments if accessed directly? Or are they simply jitted away and so modifications of such parameters will not be considered?</p>\n<p>To be more specific, let's look at the following example:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def fit(params: optax.Params, optimizer: optax.GradientTransformation) -&gt; optax.Params:\n  opt_state = optimizer.init(params)\n  extra_learning_rate = 0.1\n\n  @jax.jit\n  def step(params, opt_state, batch, labels):\n    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n    updates, opt_state = optimizer.update(grads, opt_state, params)\n    updates *= extra_learning_rate # not really valid code, but you get the idea\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss_value\n\n  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n    extra_learning_rate = 0.1\n    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n    extra_learning_rate = 0.01 # does this affect the next `step` call?\n    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n\n  return params\n</code></pre>\n<p>vs</p>\n<pre class=\"lang-py prettyprint-override\"><code>def fit(params: optax.Params, optimizer: optax.GradientTransformation) -&gt; optax.Params:\n  opt_state = optimizer.init(params)\n  extra_learning_rate = 0.1\n\n  @jax.jit\n  def step(params, opt_state, batch, labels, extra_lr):\n    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n    updates, opt_state = optimizer.update(grads, opt_state, params)\n    updates *= extra_lr # not really valid code, but you get the idea\n    params = optax.apply_updates(params, updates)\n    return params, opt_state, loss_value\n\n  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n    extra_learning_rate = 0.1\n    params, opt_state, loss_value = step(params, opt_state, batch, labels, extra_learning_rate)\n    extra_learning_rate = 0.01 # does this now affect the next `step` call?\n    params, opt_state, loss_value = step(params, opt_state, batch, labels, extra_learning_rate)\n\n  return params\n</code></pre>\n<p>From my limited experiments, they perform differently as the second <code>step</code> call doesn't uses the new learning rates in the global case and also no 're-jitting' happens, however I'd like to know if there's any standard practice/rules I need to be aware of. I'm writing a library where performance is fundamental and I don't want to miss some jit optimizations because I'm doing things wrong.</p>\n",
        "output": "<p>During JIT tracing, JAX treats global values as implicit arguments to the function being traced. You can see this reflected in the <a href=\"https://jax.readthedocs.io/en/latest/jaxpr.html\" rel=\"noreferrer\">jaxpr</a> representing the function.</p>\n<p>Here are two simple functions that return equivalent results, one with implicit arguments and one with explicit:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import jax\nimport jax.numpy as jnp\n\ndef f_explicit(a, b):\n  return a + b\n\ndef f_implicit(b):\n  return a_global + b\n\na_global = jnp.arange(5.0)\nb = jnp.ones(5)\n\nprint(jax.make_jaxpr(f_explicit)(a_global, b))\n# { lambda ; a:f32[5] b:f32[5]. let c:f32[5] = add a b in (c,) }\n\nprint(jax.make_jaxpr(f_implicit)(b))\n# { lambda a:f32[5]; b:f32[5]. let c:f32[5] = add a b in (c,) }\n</code></pre>\n<p>Notice the only difference in the two jaxprs is that in <code>f_implicit</code>, the <code>a</code> variable comes before the semicolon: this is the way that <code>jaxpr</code> representations indicate the argument is passed via closure rather than via an explicit argument. But the computation generated by these two functions will be identical.</p>\n<p>That said, one difference to be aware of is that when an argument passed by closure is a hashable constant, it will be treated as <em>static</em> within the traced function (similar when explicit arguments are marked static via <code>static_argnums</code> or <code>static_argnames</code> within <a href=\"https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html\" rel=\"noreferrer\"><code>jax.jit</code></a>):</p>\n<pre class=\"lang-py prettyprint-override\"><code>a_global = 1.0\nprint(jax.make_jaxpr(f_implicit)(b))\n# { lambda ; a:f32[5]. let b:f32[5] = add 1.0 a in (b,) }\n</code></pre>\n<p>Notice in the jaxpr representation the constant value is inserted directly as an argument to the <code>add</code> operation. The explicit way to to get the same result for a JIT-compiled function would look something like this:</p>\n<pre><code>from functools import partial\n\n@partial(jax.jit, static_argnames=['a'])\ndef f_explicit(a, b):\n  return a + b\n</code></pre>\n",
        "from_id": "3160186",
        "to_id": "2937831",
        "answer_votes": 7,
        "question_votes": 5,
        "question_id": 73621269,
        "answer_id": 73626097,
        "question_title": "JAX - jitting functions: parameters vs &quot;global&quot; variables",
        "question_link": "https://stackoverflow.com/questions/73621269/jax-jitting-functions-parameters-vs-global-variables",
        "answer_link": "https://stackoverflow.com/questions/73621269/jax-jitting-functions-parameters-vs-global-variables/73626097#73626097"
    },
    {
        "text_input": "<p>I need to create a 3D tensor like this (5,3,2) for example</p>\n<pre><code>array([[[0, 0],\n        [0, 1],\n        [0, 0]],\n\n       [[1, 0],\n        [0, 0],\n        [0, 0]],\n\n       [[0, 0],\n        [1, 0],\n        [0, 0]],\n\n       [[0, 0],\n        [0, 0],\n        [1, 0]],\n\n       [[0, 0],\n        [0, 1],\n        [0, 0]]])\n</code></pre>\n<p>There should be exactly one 'one' placed randomly in every slice (if you consider the tensor to be a loaf of bread). This could be done using loops, but I want to vectorize this part.</p>\n",
        "output": "<p>Try generate a random array, then find the <code>max</code>:</p>\n<pre><code>a = np.random.rand(5,3,2)\nout = (a == a.max(axis=(1,2))[:,None,None]).astype(int)\n</code></pre>\n",
        "from_id": "1970053",
        "to_id": "4238408",
        "answer_votes": 4,
        "question_votes": 5,
        "question_id": 66218156,
        "answer_id": 66218229,
        "question_title": "Create a 3D tensor of zeros with exactly one &#39;1&#39; randomly placed on every slice in numpy/jax",
        "question_link": "https://stackoverflow.com/questions/66218156/create-a-3d-tensor-of-zeros-with-exactly-one-1-randomly-placed-on-every-slice",
        "answer_link": "https://stackoverflow.com/questions/66218156/create-a-3d-tensor-of-zeros-with-exactly-one-1-randomly-placed-on-every-slice/66218229#66218229"
    },
    {
        "text_input": "<p>I need to create a 3D tensor like this (5,3,2) for example</p>\n<pre><code>array([[[0, 0],\n        [0, 1],\n        [0, 0]],\n\n       [[1, 0],\n        [0, 0],\n        [0, 0]],\n\n       [[0, 0],\n        [1, 0],\n        [0, 0]],\n\n       [[0, 0],\n        [0, 0],\n        [1, 0]],\n\n       [[0, 0],\n        [0, 1],\n        [0, 0]]])\n</code></pre>\n<p>There should be exactly one 'one' placed randomly in every slice (if you consider the tensor to be a loaf of bread). This could be done using loops, but I want to vectorize this part.</p>\n",
        "output": "<p>The most straightforward way to do this is probably to create an array of zeros, and set a random index to 1. In NumPy, it might look like this:</p>\n<pre class=\"lang-python prettyprint-override\"><code>import numpy as np\n\nK, M, N = 5, 3, 2\ni = np.random.randint(0, M, K)\nj = np.random.randint(0, N, K)\nx = np.zeros((K, M, N))\nx[np.arange(K), i, j] = 1\n</code></pre>\n<p>In JAX, it might look something like this:</p>\n<pre class=\"lang-python prettyprint-override\"><code>import jax.numpy as jnp\nfrom jax import random\n\nK, M, N = 5, 3, 2\nkey1, key2 = random.split(random.PRNGKey(0))\ni = random.randint(key1, (K,), 0, M)\nj = random.randint(key2, (K,), 0, N)\nx = jnp.zeros((K, M, N)).at[jnp.arange(K), i, j].set(1)\n</code></pre>\n<p>A more concise option that also guarantees a single <code>1</code> per slice would be to use broadcasted equality of a random integer with an appropriately constructed range:</p>\n<pre class=\"lang-python prettyprint-override\"><code>r = random.randint(random.PRNGKey(0), (K, 1, 1), 0, M * N)\nx = (r == jnp.arange(M * N).reshape(M, N)).astype(int)\n</code></pre>\n",
        "from_id": "1970053",
        "to_id": "2937831",
        "answer_votes": 3,
        "question_votes": 5,
        "question_id": 66218156,
        "answer_id": 66218953,
        "question_title": "Create a 3D tensor of zeros with exactly one &#39;1&#39; randomly placed on every slice in numpy/jax",
        "question_link": "https://stackoverflow.com/questions/66218156/create-a-3d-tensor-of-zeros-with-exactly-one-1-randomly-placed-on-every-slice",
        "answer_link": "https://stackoverflow.com/questions/66218156/create-a-3d-tensor-of-zeros-with-exactly-one-1-randomly-placed-on-every-slice/66218953#66218953"
    },
    {
        "text_input": "<p>I need to create a 3D tensor like this (5,3,2) for example</p>\n<pre><code>array([[[0, 0],\n        [0, 1],\n        [0, 0]],\n\n       [[1, 0],\n        [0, 0],\n        [0, 0]],\n\n       [[0, 0],\n        [1, 0],\n        [0, 0]],\n\n       [[0, 0],\n        [0, 0],\n        [1, 0]],\n\n       [[0, 0],\n        [0, 1],\n        [0, 0]]])\n</code></pre>\n<p>There should be exactly one 'one' placed randomly in every slice (if you consider the tensor to be a loaf of bread). This could be done using loops, but I want to vectorize this part.</p>\n",
        "output": "<p>You can create a zero array where the first element of each sub-array is 1, and then <code>permute</code> it across the final two axes:</p>\n<pre class=\"lang-py prettyprint-override\"><code>x = np.zeros((5,3,2)); x[:,0,0] = 1\n\nrng = np.random.default_rng()\nx = rng.permuted(rng.permuted(x, axis=-1), axis=-2)\n</code></pre>\n",
        "from_id": "1970053",
        "to_id": "9067615",
        "answer_votes": 0,
        "question_votes": 5,
        "question_id": 66218156,
        "answer_id": 66907322,
        "question_title": "Create a 3D tensor of zeros with exactly one &#39;1&#39; randomly placed on every slice in numpy/jax",
        "question_link": "https://stackoverflow.com/questions/66218156/create-a-3d-tensor-of-zeros-with-exactly-one-1-randomly-placed-on-every-slice",
        "answer_link": "https://stackoverflow.com/questions/66218156/create-a-3d-tensor-of-zeros-with-exactly-one-1-randomly-placed-on-every-slice/66907322#66907322"
    },
    {
        "text_input": "<p>From the <a href=\"https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html#jit-compiling-a-function\" rel=\"noreferrer\">JAX docs</a>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import jax\nimport jax.numpy as jnp\n\ndef selu(x, alpha=1.67, lambda_=1.05):\n  return lambda_ * jnp.where(x &gt; 0, x, alpha * jnp.exp(x) - alpha)\n\nx = jnp.arange(1000000)\n\nselu(x)\n</code></pre>\n<blockquote>\n<p>&quot;The code above is sending one operation at a time to the accelerator. This limits the ability of the XLA compiler to optimize our functions.&quot;</p>\n</blockquote>\n<p>The docs then proceed to wrap <code>selu</code> in <code>jit</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>selu_jit = jax.jit(selu)\n\nselu_jit(x)\n</code></pre>\n<p>And for some reason this improves performance significantly.</p>\n<p><strong>Why is <code>jit</code> even needed here? More specifically, why is the original code &quot;sending one operation at a time to the accelerator&quot;?</strong></p>\n<p>I was under the impression that <code>jax.numpy</code> is meant for this exact purpose, oherwise we might as well be using plain old <code>numpy</code>? What was wrong with the original <code>selu</code>?</p>\n<p>Thanks!</p>\n",
        "output": "<p><em>Edit:</em> after a short discussion below I realized a more concise answer to the original question: JAX uses eager computations by default; if you want lazy evaluation—what's sometimes called <em>graph mode</em> in other packages—you can specify this by wrapping your function in <code>jax.jit</code>.</p>\n<hr />\n<p>Python is an interpreted language, which means that statements are executed one at a time. This is the sense in which the un-jitted code is sending one operation at a time to the compiler: each statement must execute and return a value before the interpreter runs the next.</p>\n<p>Within a jit-compiled function, JAX replaces arrays with abstract <a href=\"https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html#jit-mechanics-tracing-and-static-variables\" rel=\"noreferrer\">tracers</a> in order to determine the full sequence of operations in the function, and to send them all to XLA for compilation, where the operations may be rearranged or transformed by the compiler to make the overall execution more efficient.</p>\n<p>The reason we use <code>jax.numpy</code> rather than normal <code>numpy</code> is because <code>jax.numpy</code> operations work with the JIT tracer machinery, whereas normal <code>numpy</code> operations do not.</p>\n<p>For a high-level intro to how JAX and its transforms work, a good place to start is <a href=\"https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html\" rel=\"noreferrer\">How To Think In JAX</a>.</p>\n",
        "from_id": "4119292",
        "to_id": "2937831",
        "answer_votes": 6,
        "question_votes": 5,
        "question_id": 75138443,
        "answer_id": 75139351,
        "question_title": "Why is JAX jit needed for jax.numpy operations?",
        "question_link": "https://stackoverflow.com/questions/75138443/why-is-jax-jit-needed-for-jax-numpy-operations",
        "answer_link": "https://stackoverflow.com/questions/75138443/why-is-jax-jit-needed-for-jax-numpy-operations/75139351#75139351"
    },
    {
        "text_input": "<p>I have a function <code>compute(x)</code> where <code>x</code> is a <code>jnp.ndarray</code>. Now, I want to use <code>vmap</code> to transform it into a function that takes a batch of arrays <code>x[i]</code>, and then <code>jit</code> to speed it up. <code>compute(x)</code> is something like:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def compute(x):\n    # ... some code\n    y = very_expensive_function(x)\n    return y\n</code></pre>\n<p>However, each array <code>x[i]</code> has a different length. I can easily work around this problem by padding arrays with trailing zeros such that they all have the same length <code>N</code> and <code>vmap(compute)</code> can be applied on batches with shape <code>(batch_size, N)</code>.</p>\n<p>Doing so, however, leads to <code>very_expensive_function()</code> to be called also on the trailing zeros of each array <code>x[i]</code>. Is there a way to modify <code>compute()</code> such that <code>very_expensive_function()</code> is called only on a slice of <code>x</code>, without interfering with <code>vmap</code> and <code>jit</code>?</p>\n",
        "output": "<p>With JAX, when you want to jit a function to speed things up, the given batch parameter <code>x</code> must be a well defined ndarray (i.e. the x[i] must have the same shapes). This is true whether or not you are using <code>vmap</code>.</p>\n<p>Now, the usual way of dealing with that is to pad these arrays. This implies that you add a mask in your parameters such that the padded values don't affect your result. For example, if I want to compute the <code>softmax</code> of padded values <code>x</code> of shape <code>(bath_size, max_length)</code>, I need to &quot;disable&quot; the effect of the padded values. Here is an example:</p>\n<pre><code>import jax.numpy as jnp\nimport jax\n\nPAD = 0\nMINUS_INFINITY = -1e6\n\nx = jnp.array([ \n       [1, 2, 3, 4],\n       [1, 2, PAD, PAD],\n       [1, 2, 3, PAD]\n    ])\n\nmask = jnp.array([\n           [1, 1, 1, 1],\n           [1, 1, 0, 0],\n           [1, 1, 1, 0]\n       ])\n       \nmasked_sofmax = jax.nn.softmax(x + (1-mask)*MINUS_INFINITY)    \n</code></pre>\n<p>It is not as trivial as padding <code>x</code>. You need to actually change your computation at each step to disable the effect of the padding. In the case of softmax, you do this by setting the padded values close to minus infinity.</p>\n<p>Finally, you can't really know in advance if the speed performance will be better with or without padding + masking. In my experience, it often leads to a good improvement on CPU, and to a very big improvement on GPU. In particular, the choice of the size of the batch has a big effect on the performance since a higher <code>batch_size</code> will statistically lead to a higher <code>max_length</code>, hence to a higher number of &quot;useless&quot; computations performed on the padded values.</p>\n",
        "from_id": "8737016",
        "to_id": "6084245",
        "answer_votes": 5,
        "question_votes": 5,
        "question_id": 68303110,
        "answer_id": 68532890,
        "question_title": "JAX batching with different lengths",
        "question_link": "https://stackoverflow.com/questions/68303110/jax-batching-with-different-lengths",
        "answer_link": "https://stackoverflow.com/questions/68303110/jax-batching-with-different-lengths/68532890#68532890"
    },
    {
        "text_input": "<p>I want to use vmap to vectorise this code for performance.</p>\n<pre><code>def matrix(dataA, dataB):\n    return jnp.array([[func(a, b) for b in dataB] for a in dataA])\nmatrix(data, data)\n</code></pre>\n<p>I tried this:</p>\n<pre><code>def f(x, y):\n    return func(x, y)\nmapped = jax.vmap(f)\nmapped(data, data)\n</code></pre>\n<p>But this only gives the diagonal entries.</p>\n<p>Basically I have a vector <code>data = [1,2,3,4,5]</code> (example), I want to get a matrix such that each entry <code>(i, j)</code> of the matrix is <code>f(data[i], data[j])</code>. Thus, the resulting matrix shape will be <code>(len(data), len(data))</code>.</p>\n",
        "output": "<p><code>jax.vmap</code> maps across one set of axes at a time. If you want to map across two independent sets of axes, you can do so by nesting two <code>vmap</code> transformations:</p>\n<pre class=\"lang-python prettyprint-override\"><code>mapped = jax.vmap(jax.vmap(f, in_axes=(None, 0)), in_axes=(0, None))\nresult = mapped(data, data)\n</code></pre>\n",
        "from_id": "13468128",
        "to_id": "2937831",
        "answer_votes": 4,
        "question_votes": 5,
        "question_id": 69429846,
        "answer_id": 69437299,
        "question_title": "How to use jax vmap for nested loops?",
        "question_link": "https://stackoverflow.com/questions/69429846/how-to-use-jax-vmap-for-nested-loops",
        "answer_link": "https://stackoverflow.com/questions/69429846/how-to-use-jax-vmap-for-nested-loops/69437299#69437299"
    },
    {
        "text_input": "<p>I recently implemented a two-layer GRU network in Jax and was disappointed by its performance (it was unusable).</p>\n<h4>So, i tried a little speed comparison with Pytorch.</h4>\n<h5>Minimal working example</h5>\n<p>This is my minimal working example and the output was created on Google Colab with GPU-runtime. <a href=\"https://colab.research.google.com/drive/1aUc2w1N8Xth1qEU7X9fmDyFHBv78R7LY?usp=sharing\" rel=\"noreferrer\">notebook in colab</a></p>\n<pre class=\"lang-py prettyprint-override\"><code>import flax.linen as jnn \nimport jax\nimport torch\nimport torch.nn as tnn\nimport numpy as np \nimport jax.numpy as jnp\n\ndef keyGen(seed):\n    key1 = jax.random.PRNGKey(seed)\n    while True:\n        key1, key2 = jax.random.split(key1)\n        yield key2\nkey = keyGen(1)\n\nhidden_size=200\nseq_length = 1000\nin_features = 6\nout_features = 4\nbatch_size = 8\n\nclass RNN_jax(jnn.Module):\n\n    @jnn.compact\n    def __call__(self, x, carry_gru1, carry_gru2):\n        carry_gru1, x = jnn.GRUCell()(carry_gru1, x)\n        carry_gru2, x = jnn.GRUCell()(carry_gru2, x)\n        x = jnn.Dense(4)(x)\n        x = x/jnp.linalg.norm(x)\n        return x, carry_gru1, carry_gru2\n\nclass RNN_torch(tnn.Module):\n    def __init__(self, batch_size, hidden_size, in_features, out_features):\n        super().__init__()\n\n        self.gru = tnn.GRU(\n            input_size=in_features, \n            hidden_size=hidden_size,\n            num_layers=2\n            )\n        \n        self.dense = tnn.Linear(hidden_size, out_features)\n\n        self.init_carry = torch.zeros((2, batch_size, hidden_size))\n\n    def forward(self, X):\n        X, final_carry = self.gru(X, self.init_carry)\n        X = self.dense(X)\n        return X/X.norm(dim=-1).unsqueeze(-1).repeat((1, 1, 4))\n\nrnn_jax = RNN_jax()\nrnn_torch = RNN_torch(batch_size, hidden_size, in_features, out_features)\n\nXj = jax.random.normal(next(key), (seq_length, batch_size, in_features))\nYj = jax.random.normal(next(key), (seq_length, batch_size, out_features))\nXt = torch.from_numpy(np.array(Xj))\nYt = torch.from_numpy(np.array(Yj))\n\ninitial_carry_gru1 = jnp.zeros((batch_size, hidden_size))\ninitial_carry_gru2 = jnp.zeros((batch_size, hidden_size))\n\nparams = rnn_jax.init(next(key), Xj[0], initial_carry_gru1, initial_carry_gru2)\n\ndef forward(params, X):\n    \n    carry_gru1, carry_gru2 = initial_carry_gru1, initial_carry_gru2\n\n    Yhat = []\n    for x in X: # x.shape = (batch_size, in_features)\n        yhat, carry_gru1, carry_gru2 = rnn_jax.apply(params, x, carry_gru1, carry_gru2)\n        Yhat.append(yhat) # y.shape = (batch_size, out_features)\n\n    #return jnp.concatenate(Y, axis=0)\n\njitted_forward = jax.jit(forward)\n\n</code></pre>\n<h5>Results</h5>\n<pre class=\"lang-py prettyprint-override\"><code># uncompiled jax version\n%time forward(params, Xj)\n</code></pre>\n<p><code>CPU times: user 7min 17s, sys: 8.18 s, total: 7min 25s Wall time: 7min 17s</code></p>\n<pre class=\"lang-py prettyprint-override\"><code># time for compiling\n%time jitted_forward(params, Xj)\n</code></pre>\n<p><code>CPU times: user 8min 9s, sys: 4.46 s, total: 8min 13s Wall time: 8min 12s</code></p>\n<pre class=\"lang-py prettyprint-override\"><code># compiled jax version\n%timeit jitted_forward(params, Xj)\n</code></pre>\n<p><code>The slowest run took 204.20 times longer than the fastest. This could mean that an intermediate result is being cached. 10000 loops, best of 5: 115 µs per loop</code></p>\n<pre class=\"lang-py prettyprint-override\"><code># torch version\n%timeit lambda: rnn_torch(Xt)\n</code></pre>\n<p><code>10000000 loops, best of 5: 65.7 ns per loop</code></p>\n<h5>Questions</h5>\n<p>Why is my Jax-implementation so slow? What am i doing wrong?</p>\n<p>Also, why is compiling taking so long? The sequence is not that long..</p>\n<p>Thank you :)</p>\n",
        "output": "<p>The reason the JAX code compiles slowly is that during JIT compilation JAX unrolls loops. So in terms of XLA compilation, your function is actually very large: you call <code>rnn_jax.apply()</code> 1000 times, and compilation times tend to be roughly quadratic in the number of statements.</p>\n<p>By contrast, your pytorch function uses no Python loops, and so under the hood it is relying on vectorized operations that run much faster.</p>\n<p>Any time you use a <code>for</code> loop over data in Python, a good bet is that your code will be slow: this is true whether you're using JAX, torch, numpy, pandas, etc. I'd suggest finding an approach to the problem in JAX that relies on vectorized operations rather than relying on slow Python looping.</p>\n",
        "from_id": "14294750",
        "to_id": "2937831",
        "answer_votes": 1,
        "question_votes": 5,
        "question_id": 69767707,
        "answer_id": 69769933,
        "question_title": "Jax/Flax (very) slow RNN-forward-pass compared to pyTorch?",
        "question_link": "https://stackoverflow.com/questions/69767707/jax-flax-very-slow-rnn-forward-pass-compared-to-pytorch",
        "answer_link": "https://stackoverflow.com/questions/69767707/jax-flax-very-slow-rnn-forward-pass-compared-to-pytorch/69769933#69769933"
    },
    {
        "text_input": "<p>I'm trying to install jax and jaxlib on my Ubuntu 18 with python 3.8 for snerg (<a href=\"https://github.com/google-research/google-research/tree/master/snerg\" rel=\"nofollow noreferrer\">https://github.com/google-research/google-research/tree/master/snerg</a>). Unfortunately when I try to install jax and jaxlib for Cuda 11.8 with the following command :<br/></p>\n<pre><code>pip install --upgrade jax jaxlib==0.1.69+cuda118 -f https://storage.googleapis.com/jax-releases/jax_releases.html \n</code></pre>\n<p>I get the following error:</p>\n<pre><code>ERROR: Ignored the following versions that require a different python version: 0.4.14 Requires-Python &gt;=3.9\nERROR: Could not find a version that satisfies the requirement jaxlib==0.1.69+cuda118 (from versions: 0.1.32, 0.1.40, 0.1.41, 0.1.42, 0.1.43, 0.1.44, 0.1.46, 0.1.50, 0.1.51, 0.1.52, 0.1.55, 0.1.56, 0.1.57, 0.1.58, 0.1.59, 0.1.60, 0.1.61, 0.1.62, 0.1.63, 0.1.64, 0.1.65, 0.1.66, 0.1.67, 0.1.68, 0.1.69, 0.1.70, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10, 0.3.14, 0.3.15, 0.3.18, 0.3.20, 0.3.22, 0.3.24, 0.3.25, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.6, 0.4.7, 0.4.9, 0.4.10, 0.4.11, 0.4.12, 0.4.13)\nERROR: No matching distribution found for jaxlib==0.1.69+cuda118\n</code></pre>\n<p>Would appreciate any help. Thanks</p>\n",
        "output": "<p>Follow the following instructions which are primarily obtained from the <a href=\"https://jax.readthedocs.io/en/latest/installation.html#installing-jax\" rel=\"noreferrer\">source</a>:</p>\n<p>Uninstall previous versions (if <em>any</em>):</p>\n<pre><code>$ pip uninstall jax jaxlib jaxtyping -y\n</code></pre>\n<p>Upgrade your pip:</p>\n<pre><code>$ pip install --upgrade pip\n</code></pre>\n<p>Find out which CUDA is already installed on your machine:</p>\n<pre><code>$ nvidia-smi\n\nThu Jan  4 11:24:58 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  NVIDIA RTX A1000 6GB Lap...    Off | 00000000:01:00.0 Off |                  N/A |\n| N/A   58C    P0              12W /  35W |      8MiB /  6144MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|    0   N/A  N/A      3219      G   /usr/lib/xorg/Xorg                            4MiB |\n+---------------------------------------------------------------------------------------+\n</code></pre>\n<p>Depending on the CUDA version of your machine( <strong>wheels only available on linux</strong> ), run <em><strong>EITHER</strong></em> of the following:</p>\n<pre><code># CUDA 12.X installation\n$ pip install --upgrade &quot;jax[cuda12_pip]&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n\n#### OR ####\n\n# CUDA 11.X installation\n# Note: wheels only available on linux.\npip install --upgrade &quot;jax[cuda11_pip]&quot; -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n</code></pre>\n<p>To double check if you have have successfully configured the gpu:</p>\n<pre><code>$ python -c &quot;import jax; print(f'Jax backend: {jax.default_backend()}')&quot;\nJax backend: gpu \n</code></pre>\n",
        "from_id": "893411",
        "to_id": "5437090",
        "answer_votes": 11,
        "question_votes": 4,
        "question_id": 76831312,
        "answer_id": 77757198,
        "question_title": "Installing jaxlib for cuda 11.8",
        "question_link": "https://stackoverflow.com/questions/76831312/installing-jaxlib-for-cuda-11-8",
        "answer_link": "https://stackoverflow.com/questions/76831312/installing-jaxlib-for-cuda-11-8/77757198#77757198"
    },
    {
        "text_input": "<p>I'm trying to install jax and jaxlib on my Ubuntu 18 with python 3.8 for snerg (<a href=\"https://github.com/google-research/google-research/tree/master/snerg\" rel=\"nofollow noreferrer\">https://github.com/google-research/google-research/tree/master/snerg</a>). Unfortunately when I try to install jax and jaxlib for Cuda 11.8 with the following command :<br/></p>\n<pre><code>pip install --upgrade jax jaxlib==0.1.69+cuda118 -f https://storage.googleapis.com/jax-releases/jax_releases.html \n</code></pre>\n<p>I get the following error:</p>\n<pre><code>ERROR: Ignored the following versions that require a different python version: 0.4.14 Requires-Python &gt;=3.9\nERROR: Could not find a version that satisfies the requirement jaxlib==0.1.69+cuda118 (from versions: 0.1.32, 0.1.40, 0.1.41, 0.1.42, 0.1.43, 0.1.44, 0.1.46, 0.1.50, 0.1.51, 0.1.52, 0.1.55, 0.1.56, 0.1.57, 0.1.58, 0.1.59, 0.1.60, 0.1.61, 0.1.62, 0.1.63, 0.1.64, 0.1.65, 0.1.66, 0.1.67, 0.1.68, 0.1.69, 0.1.70, 0.1.71, 0.1.72, 0.1.73, 0.1.74, 0.1.75, 0.1.76, 0.3.0, 0.3.2, 0.3.5, 0.3.7, 0.3.8, 0.3.10, 0.3.14, 0.3.15, 0.3.18, 0.3.20, 0.3.22, 0.3.24, 0.3.25, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.6, 0.4.7, 0.4.9, 0.4.10, 0.4.11, 0.4.12, 0.4.13)\nERROR: No matching distribution found for jaxlib==0.1.69+cuda118\n</code></pre>\n<p>Would appreciate any help. Thanks</p>\n",
        "output": "<p>jaxlib version 0.1.69 is quite old (it was released in July 2021) CUDA 11.8 was released over a year later, in September 2022. Thus I would not expect there to be pre-built binaries for jaxlib version 0.1.69 targeting CUDA 11.8.</p>\n<p>If possible, your best bet would be to install a newer version of jaxlib, one which has builds targeting CUDA 11.8. The current jaxlib+CUDA GPU installation instructions can be found <a href=\"https://github.com/google/jax#pip-installation-gpu-cuda-installed-via-pip-easier\" rel=\"nofollow noreferrer\">here</a>.</p>\n<p>If for some reason you absolutely need this very old jaxlib version, you'll probably first have to install an older CUDA version on your system. The CUDA jaxlib installation instructions from jaxlib 0.1.69 can be found <a href=\"https://github.com/google/jax/tree/jaxlib-v0.1.69#pip-installation-gpu-cuda\" rel=\"nofollow noreferrer\">here</a>: it looks like it was built to target CUDA 10.1-10.2, 11.0, or 11.1-11.3.</p>\n",
        "from_id": "893411",
        "to_id": "2937831",
        "answer_votes": 3,
        "question_votes": 4,
        "question_id": 76831312,
        "answer_id": 76831782,
        "question_title": "Installing jaxlib for cuda 11.8",
        "question_link": "https://stackoverflow.com/questions/76831312/installing-jaxlib-for-cuda-11-8",
        "answer_link": "https://stackoverflow.com/questions/76831312/installing-jaxlib-for-cuda-11-8/76831782#76831782"
    },
    {
        "text_input": "<p>I have been playing with JAX (automatic differentiation library in Python) and Zygote (the automatic differentiation library in Julia) to implement Gauss-Newton minimisation method.\nI came upon the <code>@jit</code> macro in Jax that runs my Python code in around 0.6 seconds compared to ~60 seconds for the version that does not use <code>@jit</code>.\nJulia ran the code in around 40 seconds. Is there an equivalent of <code>@jit</code> in Julia or Zygote that results is a better performance?</p>\n<p>Here are the codes I used:</p>\n<p><strong>Python</strong>\n</p>\n<pre><code>from jax import grad, jit, jacfwd\nimport jax.numpy as jnp\nimport numpy as np\nimport time\n\ndef gaussian(x, params):\n    amp = params[0]\n    mu  = params[1]\n    sigma = params[2]\n    amplitude = amp/(jnp.abs(sigma)*jnp.sqrt(2*np.pi))\n    arg = ((x-mu)/sigma)\n    return amplitude*jnp.exp(-0.5*(arg**2))\n\ndef myjacobian(x, params):\n    return jacfwd(gaussian, argnums = 1)(x, params)\n\ndef op(jac):\n    return jnp.matmul(\n        jnp.linalg.inv(jnp.matmul(jnp.transpose(jac),jac)),\n        jnp.transpose(jac))\n                         \ndef res(x, data, params):\n    return data - gaussian(x, params)\n@jit\ndef step(x, data, params):\n    residuals = res(x, data, params)\n    jacobian_operation = op(myjacobian(x, params))\n    temp = jnp.matmul(jacobian_operation, residuals)\n    return params + temp\n\nN = 2000\nx = np.linspace(start = -100, stop = 100, num= N)\ndata = gaussian(x, [5.65, 25.5, 37.23])\n\nini = jnp.array([0.9, 5., 5.0])\nt1 = time.time()\nfor i in range(5000):\n    ini = step(x, data, ini)\nt2 = time.time()\nprint('t2-t1: ', t2-t1)\nini\n</code></pre>\n<p><strong>Julia</strong>\n</p>\n<pre><code>using Zygote\n\nfunction gaussian(x::Union{Vector{Float64}, Float64}, params::Vector{Float64})\n    amp = params[1]\n    mu  = params[2]\n    sigma = params[3]\n    \n    amplitude = amp/(abs(sigma)*sqrt(2*pi))\n    arg = ((x.-mu)./sigma)\n    return amplitude.*exp.(-0.5.*(arg.^2))\n    \nend\n\nfunction myjacobian(x::Vector{Float64}, params::Vector{Float64})\n    output = zeros(length(x), length(params))\n    for (index, ele) in enumerate(x)\n        output[index,:] = collect(gradient((params)-&gt;gaussian(ele, params), params))[1]\n    end\n    return output\nend\n\nfunction op(jac::Matrix{Float64})\n    return inv(jac'*jac)*jac'\nend\n\nfunction res(x::Vector{Float64}, data::Vector{Float64}, params::Vector{Float64})\n    return data - gaussian(x, params)\nend\n\nfunction step(x::Vector{Float64}, data::Vector{Float64}, params::Vector{Float64})\n    residuals = res(x, data, params)\n    jacobian_operation = op(myjacobian(x, params))\n    \n    temp = jacobian_operation*residuals\n    return params + temp\nend\n\nN = 2000\nx = collect(range(start = -100, stop = 100, length= N))\nparams = vec([5.65, 25.5, 37.23])\ndata = gaussian(x, params)\n\nini = vec([0.9, 5., 5.0])\n@time for i in range(start = 1, step = 1, length = 5000)\n    ini = step(x, data, ini)\nend\nini\n</code></pre>\n",
        "output": "<p>Your Julia code doing a number of things that aren't idiomatic and are worsening your performance. This won't be a full overview, but it should give you a good idea to start.</p>\n<p>The first thing is passing <code>params</code> as a <code>Vector</code> is a bad idea. This means it will have to be heap allocated, and the compiler doesn't know how long it is. Instead, use a <code>Tuple</code> which will allow for a lot more optimization. Secondly, don't make <code>gaussian</code> act on a <code>Vector</code> of <code>x</code>s. Instead, write the scalar version and broadcast it. Specifically, with these changes, you will have</p>\n<pre><code>function gaussian(x::Number, params::NTuple{3, Float64})\n    amp, mu, sigma = params\n    \n    # The next 2 lines should probably be done outside this function, but I'll leave them here for now.\n    amplitude = amp/(abs(sigma)*sqrt(2*pi))\n    arg = ((x-mu)/sigma)\n    return amplitude*exp(-0.5*(arg^2))\nend\n</code></pre>\n",
        "from_id": "1381340",
        "to_id": "5141328",
        "answer_votes": 6,
        "question_votes": 4,
        "question_id": 74678931,
        "answer_id": 74679408,
        "question_title": "How to improve Julia&#39;s performance using just in time compilation (JIT)",
        "question_link": "https://stackoverflow.com/questions/74678931/how-to-improve-julias-performance-using-just-in-time-compilation-jit",
        "answer_link": "https://stackoverflow.com/questions/74678931/how-to-improve-julias-performance-using-just-in-time-compilation-jit/74679408#74679408"
    },
    {
        "text_input": "<p>I have been playing with JAX (automatic differentiation library in Python) and Zygote (the automatic differentiation library in Julia) to implement Gauss-Newton minimisation method.\nI came upon the <code>@jit</code> macro in Jax that runs my Python code in around 0.6 seconds compared to ~60 seconds for the version that does not use <code>@jit</code>.\nJulia ran the code in around 40 seconds. Is there an equivalent of <code>@jit</code> in Julia or Zygote that results is a better performance?</p>\n<p>Here are the codes I used:</p>\n<p><strong>Python</strong>\n</p>\n<pre><code>from jax import grad, jit, jacfwd\nimport jax.numpy as jnp\nimport numpy as np\nimport time\n\ndef gaussian(x, params):\n    amp = params[0]\n    mu  = params[1]\n    sigma = params[2]\n    amplitude = amp/(jnp.abs(sigma)*jnp.sqrt(2*np.pi))\n    arg = ((x-mu)/sigma)\n    return amplitude*jnp.exp(-0.5*(arg**2))\n\ndef myjacobian(x, params):\n    return jacfwd(gaussian, argnums = 1)(x, params)\n\ndef op(jac):\n    return jnp.matmul(\n        jnp.linalg.inv(jnp.matmul(jnp.transpose(jac),jac)),\n        jnp.transpose(jac))\n                         \ndef res(x, data, params):\n    return data - gaussian(x, params)\n@jit\ndef step(x, data, params):\n    residuals = res(x, data, params)\n    jacobian_operation = op(myjacobian(x, params))\n    temp = jnp.matmul(jacobian_operation, residuals)\n    return params + temp\n\nN = 2000\nx = np.linspace(start = -100, stop = 100, num= N)\ndata = gaussian(x, [5.65, 25.5, 37.23])\n\nini = jnp.array([0.9, 5., 5.0])\nt1 = time.time()\nfor i in range(5000):\n    ini = step(x, data, ini)\nt2 = time.time()\nprint('t2-t1: ', t2-t1)\nini\n</code></pre>\n<p><strong>Julia</strong>\n</p>\n<pre><code>using Zygote\n\nfunction gaussian(x::Union{Vector{Float64}, Float64}, params::Vector{Float64})\n    amp = params[1]\n    mu  = params[2]\n    sigma = params[3]\n    \n    amplitude = amp/(abs(sigma)*sqrt(2*pi))\n    arg = ((x.-mu)./sigma)\n    return amplitude.*exp.(-0.5.*(arg.^2))\n    \nend\n\nfunction myjacobian(x::Vector{Float64}, params::Vector{Float64})\n    output = zeros(length(x), length(params))\n    for (index, ele) in enumerate(x)\n        output[index,:] = collect(gradient((params)-&gt;gaussian(ele, params), params))[1]\n    end\n    return output\nend\n\nfunction op(jac::Matrix{Float64})\n    return inv(jac'*jac)*jac'\nend\n\nfunction res(x::Vector{Float64}, data::Vector{Float64}, params::Vector{Float64})\n    return data - gaussian(x, params)\nend\n\nfunction step(x::Vector{Float64}, data::Vector{Float64}, params::Vector{Float64})\n    residuals = res(x, data, params)\n    jacobian_operation = op(myjacobian(x, params))\n    \n    temp = jacobian_operation*residuals\n    return params + temp\nend\n\nN = 2000\nx = collect(range(start = -100, stop = 100, length= N))\nparams = vec([5.65, 25.5, 37.23])\ndata = gaussian(x, params)\n\nini = vec([0.9, 5., 5.0])\n@time for i in range(start = 1, step = 1, length = 5000)\n    ini = step(x, data, ini)\nend\nini\n</code></pre>\n",
        "output": "<p>One straightforward way to speed this up is to use ForwardDiff not Zygote, since you are taking a gradient of a vector of length 3, many times. Here this gets me from 16 to 3.5 seconds, with the last factor of 2 involving <code>Chunk(3)</code> to improve type-stability. Perhaps this can be improved further.</p>\n<pre><code>function myjacobian(x::Vector, params)\n    # return rand(eltype(x), length(x), length(params))  # with no gradient, takes 0.5s\n    output = zeros(eltype(x), length(x), length(params))\n    config = ForwardDiff.GradientConfig(nothing, params, ForwardDiff.Chunk(3))\n    for (i, xi) in enumerate(x)\n        # grad = gradient(p-&gt;gaussian(xi, p), params)[1]       # original, takes 16s\n        # grad = ForwardDiff.gradient(p-&gt; gaussian(xi, p))     # ForwardDiff, takes 7s\n        grad = ForwardDiff.gradient(p-&gt; gaussian(xi, p), params, config)  # takes 3.5s\n        copyto!(view(output,i,:), grad)  # this allows params::Tuple, OK for Zygote, no help\n    end\n    return output\nend\n# This needs gaussian.(x, Ref(params)) elsewhere to use on many x, same params\nfunction gaussian(x::Real, params)\n    # amp, mu, sigma = params  # with params::Vector this is slower, 19 sec\n    amp = params[1]\n    mu  = params[2]\n    sigma = params[3]  # like this, 16 sec\n    T = typeof(x)  # avoids having (2*pi)::Float64 promote everything\n    amplitude = amp/(abs(sigma)*sqrt(2*T(pi)))\n    arg = (x-mu)/sigma\n    return amplitude * exp(-(arg^2)/2)\nend\n</code></pre>\n<p>However, this is still computing many small gradient arrays in a loop. It could easily compute one big gradient array instead.</p>\n<p>While in general Julia is happy to compile loops to something fast, loops that make individual arrays tend to be a bad idea. And this is especially true for Zygote, which is fastest on matlab-ish whole-array code.</p>\n<p>Here's how this looks, it gets me under 1s for the whole program:</p>\n<pre><code>function gaussian(x::Real, amp::Real, mu::Real, sigma::Real)\n    T = typeof(x)\n    amplitude = amp/(abs(sigma)*sqrt(2*T(pi)))\n    arg = (x-mu)/sigma\n    return amplitude * exp(-(arg^2)/2)\nend\nfunction myjacobian2(x::Vector, params)  # with this, 0.9s\n    amp = fill(params[1], length(x))\n    mu  = fill(params[2], length(x))\n    sigma = fill(params[3], length(x))  # use same sigma &amp; different x value at each row:\n    grads = gradient((amp, mu, sigma) -&gt; sum(gaussian.(x, amp, mu, sigma)), amp, mu, sigma)\n    hcat(grads...)\nend\n# Check that it agrees:\nmyjacobian2(x, params) ≈ myjacobian(x, params)\n</code></pre>\n<p>While this has little effect on the speed, I think you probably also want <code>op(jac::Matrix) = Hermitian(jac'*jac) \\ jac'</code> rather than <code>inv</code>.</p>\n",
        "from_id": "1381340",
        "to_id": "13706735",
        "answer_votes": 4,
        "question_votes": 4,
        "question_id": 74678931,
        "answer_id": 74680150,
        "question_title": "How to improve Julia&#39;s performance using just in time compilation (JIT)",
        "question_link": "https://stackoverflow.com/questions/74678931/how-to-improve-julias-performance-using-just-in-time-compilation-jit",
        "answer_link": "https://stackoverflow.com/questions/74678931/how-to-improve-julias-performance-using-just-in-time-compilation-jit/74680150#74680150"
    },
    {
        "text_input": "<p>I'm trying to install a particular version of <code>jaxlib</code> to work with my CUDA and cuDNN versions. Following the README, I'm trying</p>\n<p><code>pip install --upgrade jax jaxlib==0.1.52+cuda101 -f https://storage.googleapis.com/jax-releases/jax_releases.html</code></p>\n<p>This returns the following error:</p>\n<p><code>ERROR: Requested jaxlib==0.1.52+cuda101 from https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.52%2Bcuda101-cp37-none-manylinux2010_x86_64.whl has different version in metadata: '0.1.52'</code></p>\n<p>Does anyone know what causes this or how to get around the error?</p>\n",
        "output": "<p>This error appears to be from a new check in pip version 20.3.X and higher, likely related to the new dependency resolver. I can reproduce this error with pip version 20.3.3, but the package installs correctly with pip version 20.2.4.</p>\n<p>The easiest way to proceed would probably be to first downgrade pip; i.e.</p>\n<pre><code>pip install pip==20.2.4\n</code></pre>\n<p>and then proceed with your jaxlib install.</p>\n",
        "from_id": "4570472",
        "to_id": "2937831",
        "answer_votes": 4,
        "question_votes": 4,
        "question_id": 65486358,
        "answer_id": 65494547,
        "question_title": "Unable to Install Specific JAX jaxlib GPU version",
        "question_link": "https://stackoverflow.com/questions/65486358/unable-to-install-specific-jax-jaxlib-gpu-version",
        "answer_link": "https://stackoverflow.com/questions/65486358/unable-to-install-specific-jax-jaxlib-gpu-version/65494547#65494547"
    },
    {
        "text_input": "<p>I'm trying to install a particular version of <code>jaxlib</code> to work with my CUDA and cuDNN versions. Following the README, I'm trying</p>\n<p><code>pip install --upgrade jax jaxlib==0.1.52+cuda101 -f https://storage.googleapis.com/jax-releases/jax_releases.html</code></p>\n<p>This returns the following error:</p>\n<p><code>ERROR: Requested jaxlib==0.1.52+cuda101 from https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.52%2Bcuda101-cp37-none-manylinux2010_x86_64.whl has different version in metadata: '0.1.52'</code></p>\n<p>Does anyone know what causes this or how to get around the error?</p>\n",
        "output": "<p>Note that both the versions of jax and jaxlib has to match. You can use something like:</p>\n<pre><code>$ pip install --upgrade jax==0.3.2 jaxlib==0.3.2+cuda11.cudnn82 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n</code></pre>\n<hr />\n<p>Another workaround would be to first choose a <a href=\"https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\" rel=\"nofollow noreferrer\">specific version of jax and jaxlib from the available wheel files and then install those</a>.</p>\n<pre><code>$ pip install https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.1.76+cuda11.cudnn82-cp39-none-manylinux2010_x86_64.whl\n</code></pre>\n",
        "from_id": "4570472",
        "to_id": "2956066",
        "answer_votes": 3,
        "question_votes": 4,
        "question_id": 65486358,
        "answer_id": 70900004,
        "question_title": "Unable to Install Specific JAX jaxlib GPU version",
        "question_link": "https://stackoverflow.com/questions/65486358/unable-to-install-specific-jax-jaxlib-gpu-version",
        "answer_link": "https://stackoverflow.com/questions/65486358/unable-to-install-specific-jax-jaxlib-gpu-version/70900004#70900004"
    },
    {
        "text_input": "<p>I'm trying to install a particular version of <code>jaxlib</code> to work with my CUDA and cuDNN versions. Following the README, I'm trying</p>\n<p><code>pip install --upgrade jax jaxlib==0.1.52+cuda101 -f https://storage.googleapis.com/jax-releases/jax_releases.html</code></p>\n<p>This returns the following error:</p>\n<p><code>ERROR: Requested jaxlib==0.1.52+cuda101 from https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.52%2Bcuda101-cp37-none-manylinux2010_x86_64.whl has different version in metadata: '0.1.52'</code></p>\n<p>Does anyone know what causes this or how to get around the error?</p>\n",
        "output": "<p>Maybe you need to change the link to:\n<a href=\"https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\" rel=\"nofollow noreferrer\">https://storage.googleapis.com/jax-releases/jax_cuda_releases.html</a>.\nSo, <code>pip install --upgrade jax jaxlib==0.1.52+cuda101 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html</code></p>\n",
        "from_id": "4570472",
        "to_id": "19631650",
        "answer_votes": 2,
        "question_votes": 4,
        "question_id": 65486358,
        "answer_id": 73135635,
        "question_title": "Unable to Install Specific JAX jaxlib GPU version",
        "question_link": "https://stackoverflow.com/questions/65486358/unable-to-install-specific-jax-jaxlib-gpu-version",
        "answer_link": "https://stackoverflow.com/questions/65486358/unable-to-install-specific-jax-jaxlib-gpu-version/73135635#73135635"
    },
    {
        "text_input": "<p>Using <code>type(z1[0])</code> I get <code>jaxlib.xla_extension.ArrayImpl</code>. Printing <code>z1[0]</code> I get <code>Array(0.71530414, dtype=float32)</code>. How can I get the actual number <code>0.71530414</code>?</p>\n<p>I tried <code>z1[0][0]</code> because <code>z1[0]</code> is a kind of array with a single value, but it gives me an error: <code>IndexError: Too many indices for array: 1 non-None/Ellipsis indices for dim 0.</code>.</p>\n<p>I tried also a different approach: I searched on the web if it was possible to convert from jaxnumpy array to a python list, but I didn't find an answer.</p>\n<p>Can someone help me to get the value inside a <code>jaxlib.xla_extension.ArrayImpl</code> object?</p>\n",
        "output": "<p>You can use <code>float(x[0])</code> to convert <code>x[0]</code> to a Python float:</p>\n<pre class=\"lang-py prettyprint-override\"><code>In [1]: import jax.numpy as jnp\n\nIn [2]: x = jnp.array([0.71530414])\n\nIn [3]: x\nOut[3]: Array([0.71530414], dtype=float32)\n\nIn [4]: x[0]\nOut[4]: Array(0.71530414, dtype=float32)\n\nIn [5]: float(x[0])\nOut[5]: 0.7153041362762451\n</code></pre>\n<p>If you're interested in converting the entire JAX array to a list of Python floats, you can use the <code>tolist()</code> method:</p>\n<pre class=\"lang-py prettyprint-override\"><code>In [6]: x.tolist()\nOut[6]: [0.7153041362762451]\n</code></pre>\n",
        "from_id": "11452928",
        "to_id": "2937831",
        "answer_votes": 6,
        "question_votes": 4,
        "question_id": 75867636,
        "answer_id": 75870023,
        "question_title": "How to get value of jaxlib.xla_extension.ArrayImpl",
        "question_link": "https://stackoverflow.com/questions/75867636/how-to-get-value-of-jaxlib-xla-extension-arrayimpl",
        "answer_link": "https://stackoverflow.com/questions/75867636/how-to-get-value-of-jaxlib-xla-extension-arrayimpl/75870023#75870023"
    },
    {
        "text_input": "<p>Using <code>type(z1[0])</code> I get <code>jaxlib.xla_extension.ArrayImpl</code>. Printing <code>z1[0]</code> I get <code>Array(0.71530414, dtype=float32)</code>. How can I get the actual number <code>0.71530414</code>?</p>\n<p>I tried <code>z1[0][0]</code> because <code>z1[0]</code> is a kind of array with a single value, but it gives me an error: <code>IndexError: Too many indices for array: 1 non-None/Ellipsis indices for dim 0.</code>.</p>\n<p>I tried also a different approach: I searched on the web if it was possible to convert from jaxnumpy array to a python list, but I didn't find an answer.</p>\n<p>Can someone help me to get the value inside a <code>jaxlib.xla_extension.ArrayImpl</code> object?</p>\n",
        "output": "<p>You can just use <code>z1[0].item()</code> to get the value. It's the same as, e.g., <code>float(z1[0])</code>, only you don't have to know the type in advance.</p>\n",
        "from_id": "11452928",
        "to_id": "4683578",
        "answer_votes": 1,
        "question_votes": 4,
        "question_id": 75867636,
        "answer_id": 78660541,
        "question_title": "How to get value of jaxlib.xla_extension.ArrayImpl",
        "question_link": "https://stackoverflow.com/questions/75867636/how-to-get-value-of-jaxlib-xla-extension-arrayimpl",
        "answer_link": "https://stackoverflow.com/questions/75867636/how-to-get-value-of-jaxlib-xla-extension-arrayimpl/78660541#78660541"
    },
    {
        "text_input": "<p>So I mean something where you have a categorical feature $X$ (suppose you have turned it into ints already) and say you want to embed that in some dimension using the features $A$ where $A$ is arity x n_embed.</p>\n<p>What is the usual way to do this? Is using a for loop and vmap correct? I do not want something like <code>jax.nn</code>, something more efficient like</p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding</a></p>\n<p>For example consider high arity and low embedding dim.</p>\n<p>Is it <code>jnp.take</code> as in the flax.linen implementation here? <a href=\"https://github.com/google/flax/blob/main/flax/linen/linear.py#L624\" rel=\"nofollow noreferrer\">https://github.com/google/flax/blob/main/flax/linen/linear.py#L624</a></p>\n",
        "output": "<p>Indeed the typical way to do this in pure jax is with <code>jnp.take</code>.  Given array <code>A</code> of embeddings of shape <code>(num_embeddings, num_features)</code> and categorical feature <code>x</code> of integers shaped <code>(n,)</code> then the following gives you the embedding lookup.</p>\n<pre class=\"lang-py prettyprint-override\"><code>jnp.take(A, x, axis=0)  # shape: (n, num_features)\n</code></pre>\n<p>If using Flax then the recommended way would be to use the <a href=\"https://flax.readthedocs.io/en/latest/api_reference/_autosummary/flax.linen.Embed.html\" rel=\"noreferrer\"><code>flax.linen.Embed</code></a> module and would achieve the same effect:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import flax.linen as nn\n\nclass Model(nn.Module): \n  @nn.compact\n  def __call__(self, x):\n    emb = nn.Embed(num_embeddings, num_features)(x)  # shape\n</code></pre>\n",
        "from_id": "287238",
        "to_id": "6263317",
        "answer_votes": 5,
        "question_votes": 4,
        "question_id": 72817730,
        "answer_id": 73293668,
        "question_title": "What is the recommended way to do embeddings in jax?",
        "question_link": "https://stackoverflow.com/questions/72817730/what-is-the-recommended-way-to-do-embeddings-in-jax",
        "answer_link": "https://stackoverflow.com/questions/72817730/what-is-the-recommended-way-to-do-embeddings-in-jax/73293668#73293668"
    },
    {
        "text_input": "<p>So I mean something where you have a categorical feature $X$ (suppose you have turned it into ints already) and say you want to embed that in some dimension using the features $A$ where $A$ is arity x n_embed.</p>\n<p>What is the usual way to do this? Is using a for loop and vmap correct? I do not want something like <code>jax.nn</code>, something more efficient like</p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding</a></p>\n<p>For example consider high arity and low embedding dim.</p>\n<p>Is it <code>jnp.take</code> as in the flax.linen implementation here? <a href=\"https://github.com/google/flax/blob/main/flax/linen/linear.py#L624\" rel=\"nofollow noreferrer\">https://github.com/google/flax/blob/main/flax/linen/linear.py#L624</a></p>\n",
        "output": "<p>Suppose that <code>A</code> is the embedding table and <code>x</code> is any shape of indices.</p>\n<ol>\n<li><code>A[x]</code>, which is like <code>jnp.take(A, x, axis=0)</code> but simpler.</li>\n<li><code>vmap</code>-ed <code>A[x]</code>, which parallelizes along axis 0 of <code>x</code>.</li>\n<li>nested <code>vmap</code>-ed <code>A[x]</code>, which parallelizes along all axes of <code>x</code>.</li>\n</ol>\n<p>Here are the source code for your reference.</p>\n<pre class=\"lang-py prettyprint-override\"><code>import jax\nimport jax.numpy as jnp\n\nembs = jnp.array([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]], dtype=jnp.float32)\n\nx = jnp.array([[3, 1], [2, 0]], dtype=jnp.int32)\n\nprint(&quot;\\ntake\\n&quot;, jnp.take(embs, x, axis=0))\nprint(&quot;\\nuse []\\n&quot;, embs[x])\nprint(\n    &quot;\\nvmap\\n&quot;,\n    jax.vmap(lambda embs, x: embs[x], in_axes=[None, 0], out_axes=0)(embs, x),\n)\n\nprint(\n    &quot;\\nnested vmap\\n&quot;,\n    jax.vmap(\n        jax.vmap(lambda embs, x: embs[x], in_axes=[None, 0], out_axes=0),\n        in_axes=[None, 0],\n        out_axes=0,\n    )(embs, x),\n)\n</code></pre>\n<p>BTW, I learned the nested-vmap trick from <a href=\"https://github.com/iree-org/iree-jax/blob/26006ef5842a604e28ea71e65e9224ad20f028e9/models/gpt2/model.py#L143-L146\" rel=\"nofollow noreferrer\">the IREE GPT2 model code</a> by James Bradbury.</p>\n",
        "from_id": "287238",
        "to_id": "724872",
        "answer_votes": 2,
        "question_votes": 4,
        "question_id": 72817730,
        "answer_id": 75512367,
        "question_title": "What is the recommended way to do embeddings in jax?",
        "question_link": "https://stackoverflow.com/questions/72817730/what-is-the-recommended-way-to-do-embeddings-in-jax",
        "answer_link": "https://stackoverflow.com/questions/72817730/what-is-the-recommended-way-to-do-embeddings-in-jax/75512367#75512367"
    },
    {
        "text_input": "<p>Is it possible to make CPU only reductions with JAX comparable to Numba in terms of computation time?</p>\n<p>The compilers come straight from <code>conda</code>:</p>\n<pre><code>$ conda install -c conda-forge numba jax\n</code></pre>\n<p>Here is a 1-d NumPy array example</p>\n<pre><code>import numpy as np\nimport numba as nb\nimport jax as jx\n\n@nb.njit\ndef reduce_1d_njit_serial(x):\n    s = 0\n    for xi in x:\n        s += xi\n    return s\n\n@jx.jit\ndef reduce_1d_jax_serial(x):\n    s = 0\n    for xi in x:\n        s += xi\n    return s\n\nN = 2**10\na = np.random.randn(N)\n</code></pre>\n<p>Using <code>timeit</code> on the following</p>\n<ol>\n<li><code>np.add.reduce(a)</code> gives <code>1.99 µs ...</code></li>\n<li><code>reduce_1d_njit_serial(a)</code> gives <code>1.43 µs ...</code></li>\n<li><code>reduce_1d_jax_serial(a).item()</code> gives <code>23.5 µs ...</code></li>\n</ol>\n<p>Note that <code>jx.numpy.sum(a)</code> and using <code>jx.lax.fori_loop</code> gives comparable (marginally slower) comp. times to <code>reduce_1d_jax_serial</code>.</p>\n<p>It seems there is a better way to craft the reduction for XLA.</p>\n<p><strong>EDIT</strong>: compile times were not included as a print statement proceeded to check results.</p>\n",
        "output": "<p>When performing these kinds of microbenchmarks with JAX, you have to be careful to ensure you're measuring what you think you're measuring. There are some tips in the <a href=\"https://jax.readthedocs.io/en/latest/faq.html#benchmarking-jax-code\" rel=\"noreferrer\">JAX Benchmarking FAQ</a>. Implementing some of these best practices, I find the following for your benchmarks:</p>\n<pre class=\"lang-python prettyprint-override\"><code>import jax.numpy as jnp\n\n# Native jit-compiled XLA sum\njit_sum = jx.jit(jnp.sum)\n\n# Avoid including device transfer cost in the benchmarks\na_jax = jnp.array(a)\n\n# Prevent measuring compilation time\n_ = reduce_1d_njit_serial(a)\n_ = reduce_1d_jax_serial(a_jax)\n_ = jit_sum(a_jax)\n\n%timeit np.add.reduce(a)\n# 100000 loops, best of 5: 2.33 µs per loop\n\n%timeit reduce_1d_njit_serial(a)\n# 1000000 loops, best of 5: 1.43 µs per loop\n\n%timeit reduce_1d_jax_serial(a_jax).block_until_ready()\n# 100000 loops, best of 5: 6.24 µs per loop\n\n%timeit jit_sum(a_jax).block_until_ready()\n# 100000 loops, best of 5: 4.37 µs per loop\n</code></pre>\n<p>You'll see that for these microbenchmarks, JAX is a few milliseconds slower than both numpy and numba. So does this mean JAX is slow? Yes and no; you'll find a more complete answer to that question in <a href=\"https://jax.readthedocs.io/en/latest/faq.html#is-jax-faster-than-numpy\" rel=\"noreferrer\">JAX FAQ: is JAX faster than numpy?</a>. The short summary is that this computation is so small that the differences are dominated by Python dispatch time rather than time spent operating on the array. The JAX project has not put much effort into optimizing for Python dispatch of microbenchmarks: it's not all that important in practice because the cost is incurred once per program in JAX, as opposed to once per operation in numpy.</p>\n",
        "from_id": "18649992",
        "to_id": "2937831",
        "answer_votes": 7,
        "question_votes": 4,
        "question_id": 71701041,
        "answer_id": 71709788,
        "question_title": "JAX(XLA) vs Numba(LLVM) Reduction",
        "question_link": "https://stackoverflow.com/questions/71701041/jaxxla-vs-numballvm-reduction",
        "answer_link": "https://stackoverflow.com/questions/71701041/jaxxla-vs-numballvm-reduction/71709788#71709788"
    },
    {
        "text_input": "<p>I am trying to modify a code base to create a subarray using an existing array and indices in the form of Jax tracer. When I try to pass these Jax tracers directly for indices. I get the following error:</p>\n<pre><code>IndexError: Array slice indices must have static start/stop/step to be used with NumPy indexing syntax. Found slice(Tracedwith, Tracedwith, None). To index a statically sized array at a dynamic position, try lax.dynamic_slice/dynamic_update_slice (JAX does not support dynamically sized arrays within JIT compiled functions).\n</code></pre>\n<p>What is a possible workaround/ solution for this?</p>\n",
        "output": "<p>There are two main workarounds here that may be applicable depending on your problem: using static indices, or using <code>dynamic_slice</code>.</p>\n<p>Quick background: one constraint of arrays used in JAX transformations like <code>jit</code>, <code>vmap</code>, etc. is that they must be statically shaped (see <a href=\"https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#dynamic-shapes\" rel=\"noreferrer\">JAX Sharp Bits: Dynamic Shapes</a> for some discussion of this).</p>\n<p>With that in mind, a function like <code>f</code> below will always fail, because <code>i</code> and <code>j</code> are non-static variables and so the shape of the returned array cannot be known at compile time:</p>\n<pre class=\"lang-py prettyprint-override\"><code>@jit\ndef f(x, i, j):\n  return x[i:j]\n</code></pre>\n<p>One workaround for this is to make <code>i</code> and <code>j</code> static arguments in <code>jit</code>, so that the shape of the returned array will be static:</p>\n<pre class=\"lang-py prettyprint-override\"><code>@partial(jit, static_argnames=['i', 'j'])\ndef f(x, i, j):\n  return x[i:j]\n</code></pre>\n<p>That's the only possible workaround to use <code>jit</code> in such a situation, because of the static shape constraint.</p>\n<p>Another flavor of slicing problem that can lead to the same error might look like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>@jit\ndef f(x, i):\n  return x[i:i + 5]\n</code></pre>\n<p>This will also result in a non-static index error. It could be fixed as above by marking <code>i</code> as static, but there is more information here: assuming that <code>0 &lt;= i &lt; len(x) - 5</code> holds, we <em>know</em> that the shape of the output array is <code>(5,)</code>. This is a case where <a href=\"https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.dynamic_slice.html\" rel=\"noreferrer\"><code>jax.lax.dynamic_slice</code></a> is applicable (when you have a fixed slice size at a dynamic location):</p>\n<pre class=\"lang-py prettyprint-override\"><code>@jit\ndef f(x, i):\n  return jax.lax.dynamic_slice(x, (i,), (5,))\n</code></pre>\n<p>Note that this will have different semantics than <code>x[i:i + 5]</code> in cases where the slice overruns the bounds of the array, but in most cases of interest it is equivalent.</p>\n<p>There are other examples where neither of these two workarounds are applicable, for example when your program logic is predicated on creating dynamic-length arrays. In these cases, there is no easy work-around, and your best bet is to either (1) re-write your algorithm in terms of static array shapes, perhaps using padded array representations, or (2) not use JAX.</p>\n",
        "from_id": "16669839",
        "to_id": "2937831",
        "answer_votes": 6,
        "question_votes": 4,
        "question_id": 76626143,
        "answer_id": 76628452,
        "question_title": "How to slice jax arrays using jax tracer?",
        "question_link": "https://stackoverflow.com/questions/76626143/how-to-slice-jax-arrays-using-jax-tracer",
        "answer_link": "https://stackoverflow.com/questions/76626143/how-to-slice-jax-arrays-using-jax-tracer/76628452#76628452"
    },
    {
        "text_input": "<p>I'm new to JAX and writing code that JIT compiles is proving to be quite hard for me. I am trying to achieve the following:</p>\n<p>Given an <code>(n,n)</code> array <code>mat</code> in JAX, I would like to add a <code>(1,n)</code> or an <code>(n,1)</code> array to an arbitrary row or column, respectively, of the original array <code>mat</code>.</p>\n<p>If I wanted to add a row array, <code>r</code>, to the third row, the numpy equivalent would be,</p>\n<pre><code># if mat is a numpy array\nmat[2,:] = mat[2,:] + r\n\n</code></pre>\n<p>The only way I know how to update an element of an array in JAX is using <code>array.at[i].set()</code>. I am not sure how one can use this to update a row or a column without explicitly using a for-loop.</p>\n",
        "output": "<p>JAX arrays are immutable, so you cannot do in-place modifications of array entries. But you can accomplish similar results with the <a href=\"https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html\" rel=\"noreferrer\"><code>np.ndarray.at</code></a> syntax. For example, the equivalent of</p>\n<pre class=\"lang-py prettyprint-override\"><code>mat[2,:] = mat[2,:] + r\n</code></pre>\n<p>would be</p>\n<pre class=\"lang-py prettyprint-override\"><code>mat = mat.at[2,:].set(mat[2,:] + r)\n</code></pre>\n<p>But you can use the <code>add</code> method to be more efficient in this case:</p>\n<pre class=\"lang-py prettyprint-override\"><code>mat = mat.at[2:].add(r)\n</code></pre>\n<p>Here is an example of adding a row and column array to a 2D array:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import jax.numpy as jnp\n\nmat = jnp.zeros((5, 5))\n\n# Create 2D row &amp; col arrays, as in question\nrow = jnp.ones(5).reshape(1, 5)\ncol = jnp.ones(5).reshape(5, 1)\n\nmat = mat.at[1:2, :].add(row)\nmat = mat.at[:, 2:3].add(col)\n\nprint(mat)\n# [[0. 0. 1. 0. 0.]\n#  [1. 1. 2. 1. 1.]\n#  [0. 0. 1. 0. 0.]\n#  [0. 0. 1. 0. 0.]\n#  [0. 0. 1. 0. 0.]]\n</code></pre>\n<p>See <a href=\"https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#in-place-updates\" rel=\"noreferrer\">JAX Sharp Bits: In-Place Updates</a> for more discussion of this.</p>\n",
        "from_id": "20954387",
        "to_id": "2937831",
        "answer_votes": 6,
        "question_votes": 4,
        "question_id": 75043981,
        "answer_id": 75044466,
        "question_title": "Updating entire row or column of a 2D array in JAX",
        "question_link": "https://stackoverflow.com/questions/75043981/updating-entire-row-or-column-of-a-2d-array-in-jax",
        "answer_link": "https://stackoverflow.com/questions/75043981/updating-entire-row-or-column-of-a-2d-array-in-jax/75044466#75044466"
    },
    {
        "text_input": "<p>I have a function that will instantiate a huge array and do other things. I am running my code on TPUs so my memory is limited.</p>\n<p>How can I execute my function specifically on the CPU?</p>\n<p>If I do:</p>\n<pre><code>y = jax.device_put(my_function(), device=jax.devices(&quot;cpu&quot;)[0])\n</code></pre>\n<p>I guess that <code>my_function()</code> is first executed on TPU and the result is put on CPU, which gives me memory error.</p>\n<p>and using <code>jax.config.update('jax_platform_name', 'cpu')</code> at the beginning of my code seems to have no effect.</p>\n<p>Also please note that I can't modify <code>my_function()</code></p>\n<p>Thanks!</p>\n",
        "output": "<p>To directly specify the device on which a function should be executed, use the <code>device</code> argument of <code>jax.jit</code>. For example (using a GPU runtime because it's the accelerator I have access to at the moment):</p>\n<pre class=\"lang-py prettyprint-override\"><code>import jax\n\ngpu_device = jax.devices('gpu')[0]\ncpu_device = jax.devices('cpu')[0]\n\ndef my_function(x):\n  return x.sum()\n\nx = jax.numpy.arange(10)\n\nx_gpu = jax.jit(my_function, device=gpu_device)(x)\nprint(x_gpu.device())\n# gpu:0\n\nx_cpu = jax.jit(my_function, device=cpu_device)(x)\nprint(x_cpu.device())\n# TFRT_CPU_0\n</code></pre>\n<p>This can also be controlled with the <code>jax.default_device</code> decorator around the call-site:</p>\n<pre class=\"lang-py prettyprint-override\"><code>with jax.default_device(cpu_device):\n  print(jax.jit(my_function)(x).device())\n  # TFRT_CPU_0\n\nwith jax.default_device(gpu_device):\n  print(jax.jit(my_function)(x).device())\n  # gpu:0\n</code></pre>\n",
        "from_id": "9817677",
        "to_id": "2937831",
        "answer_votes": 5,
        "question_votes": 4,
        "question_id": 74537026,
        "answer_id": 74590238,
        "question_title": "Execute function specifically on CPU in Jax",
        "question_link": "https://stackoverflow.com/questions/74537026/execute-function-specifically-on-cpu-in-jax",
        "answer_link": "https://stackoverflow.com/questions/74537026/execute-function-specifically-on-cpu-in-jax/74590238#74590238"
    },
    {
        "text_input": "<p>I have a function that will instantiate a huge array and do other things. I am running my code on TPUs so my memory is limited.</p>\n<p>How can I execute my function specifically on the CPU?</p>\n<p>If I do:</p>\n<pre><code>y = jax.device_put(my_function(), device=jax.devices(&quot;cpu&quot;)[0])\n</code></pre>\n<p>I guess that <code>my_function()</code> is first executed on TPU and the result is put on CPU, which gives me memory error.</p>\n<p>and using <code>jax.config.update('jax_platform_name', 'cpu')</code> at the beginning of my code seems to have no effect.</p>\n<p>Also please note that I can't modify <code>my_function()</code></p>\n<p>Thanks!</p>\n",
        "output": "<p>I'm going to make a guess here. I can't run it either so you may have to fiddle with it</p>\n<pre><code>with jax.default_device(jax.devices(&quot;cpu&quot;)[0]):\n    y = my_function()\n</code></pre>\n<p>See the docs <a href=\"https://jax.readthedocs.io/en/latest/_autosummary/jax.default_device.html#jax.default_device\" rel=\"nofollow noreferrer\">here</a> and <a href=\"https://jax.readthedocs.io/en/latest/faq.html#faq-data-placement\" rel=\"nofollow noreferrer\">here</a>.</p>\n",
        "from_id": "9817677",
        "to_id": "5986907",
        "answer_votes": 1,
        "question_votes": 4,
        "question_id": 74537026,
        "answer_id": 74540354,
        "question_title": "Execute function specifically on CPU in Jax",
        "question_link": "https://stackoverflow.com/questions/74537026/execute-function-specifically-on-cpu-in-jax",
        "answer_link": "https://stackoverflow.com/questions/74537026/execute-function-specifically-on-cpu-in-jax/74540354#74540354"
    },
    {
        "text_input": "<p>I have the following example code that works with a regular <code>map</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>def f(x_y):\n    x, y = x_y\n    return x.sum() + y.sum()\n\nxs = [jnp.zeros(3) for i in range(4)]\nys = [jnp.zeros(2) for i in range(4)]\n\nlist(map(f, zip(xs, ys)))\n\n# returns:\n[DeviceArray(0., dtype=float32),\n DeviceArray(0., dtype=float32),\n DeviceArray(0., dtype=float32),\n DeviceArray(0., dtype=float32)]\n</code></pre>\n<p>How can I use <code>jax.vmap</code> instead? The naive thing is:</p>\n<pre class=\"lang-py prettyprint-override\"><code>jax.vmap(f)(zip(xs, ys))\n</code></pre>\n<p>but this gives:</p>\n<pre><code>ValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\n</code></pre>\n",
        "output": "<p>For using <a href=\"https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax-vmap\" rel=\"nofollow noreferrer\"><code>jax.vmap</code></a>, you do not need to zip your variables. You can write what you want like below:</p>\n<pre><code>import jax.numpy as jnp\nfrom jax import vmap\n\ndef f(x_y):\n    x, y = x_y\n    return x.sum() + y.sum()\n\nxs = jnp.zeros((4,3))\nys = jnp.zeros((4,2))\nvmap(f)((xs, ys))\n</code></pre>\n<p>Output:</p>\n<pre><code>DeviceArray([0., 0., 0., 0.], dtype=float32)\n</code></pre>\n",
        "from_id": "1078529",
        "to_id": "1740577",
        "answer_votes": 3,
        "question_votes": 4,
        "question_id": 72509839,
        "answer_id": 72510375,
        "question_title": "How to use Jax vmap over zipped arguments?",
        "question_link": "https://stackoverflow.com/questions/72509839/how-to-use-jax-vmap-over-zipped-arguments",
        "answer_link": "https://stackoverflow.com/questions/72509839/how-to-use-jax-vmap-over-zipped-arguments/72510375#72510375"
    },
    {
        "text_input": "<p>I have the following example code that works with a regular <code>map</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>def f(x_y):\n    x, y = x_y\n    return x.sum() + y.sum()\n\nxs = [jnp.zeros(3) for i in range(4)]\nys = [jnp.zeros(2) for i in range(4)]\n\nlist(map(f, zip(xs, ys)))\n\n# returns:\n[DeviceArray(0., dtype=float32),\n DeviceArray(0., dtype=float32),\n DeviceArray(0., dtype=float32),\n DeviceArray(0., dtype=float32)]\n</code></pre>\n<p>How can I use <code>jax.vmap</code> instead? The naive thing is:</p>\n<pre class=\"lang-py prettyprint-override\"><code>jax.vmap(f)(zip(xs, ys))\n</code></pre>\n<p>but this gives:</p>\n<pre><code>ValueError: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())\n</code></pre>\n",
        "output": "<p><code>vmap</code> is designed to map over multiple variables by default, so no <code>zip</code> is needed. Furthermore, it can only map over array axes, not over elements of lists or tuples. So a more canonical way to to write your example would be to convert your lists to arrays and do something like this:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def g(x, y):\n  return x.sum() + y.sum()\n\nxs_arr = jnp.asarray(xs)\nys_arr = jnp.asarray(ys)\n\njax.vmap(g)(xs_arr, ys_arr)\n# DeviceArray([0., 0., 0., 0.], dtype=float32)\n</code></pre>\n",
        "from_id": "1078529",
        "to_id": "2937831",
        "answer_votes": 3,
        "question_votes": 4,
        "question_id": 72509839,
        "answer_id": 72511472,
        "question_title": "How to use Jax vmap over zipped arguments?",
        "question_link": "https://stackoverflow.com/questions/72509839/how-to-use-jax-vmap-over-zipped-arguments",
        "answer_link": "https://stackoverflow.com/questions/72509839/how-to-use-jax-vmap-over-zipped-arguments/72511472#72511472"
    },
    {
        "text_input": "<p>Assume <code>J</code> is the Jacobian of some function <code>f</code> with respect to some parameters. Are there efficient ways (in PyTorch or perhaps Jax) to have a function that takes two inputs (<code>x1</code> and <code>x2</code>) and computes <code>J(x1)*J(x2).transpose()</code> <em>without</em> instantiating the entire <code>J</code> matrices in memory?</p>\n<p>I have come across something like <code>jvp(f, input, v=vjp(f, input))</code> but don't quite understand it and not sure is what I want.</p>\n",
        "output": "<p>In JAX, you can compute a full jacobian matrix using <code>jax.jacfwd</code> or <code>jax.jacrev</code>, or you can compute a jacobian operator and its transpose using <code>jax.jvp</code> and <code>jax.vjp</code>.</p>\n<p>So, for example, say you had a function <code>Rᴺ → Rᴹ</code> that looks something like this:</p>\n<pre><code>import jax.numpy as jnp\nimport numpy as np\n\nnp.random.seed(1701)\nN, M = 10000, 5\nf_mat = np.array(np.random.rand(M, N))\ndef f(x):\n  return jnp.sqrt(f_mat @ x / N)\n</code></pre>\n<p>Given two vectors <code>x1</code> and <code>x2</code>, you can evaluate the Jacobian matrix at each using <code>jax.jacfwd</code></p>\n<pre><code>import jax\nx1 = np.array(np.random.rand(N))\nx2 = np.array(np.random.rand(N))\nJ1 = jax.jacfwd(f)(x1)\nJ2 = jax.jacfwd(f)(x2)\nprint(J1 @ J2.T)\n# [[3.3123782e-05 2.5001222e-05 2.4946943e-05 2.5180108e-05 2.4940484e-05]\n#  [2.5084497e-05 3.3233835e-05 2.4956826e-05 2.5108084e-05 2.5048916e-05]\n#  [2.4969209e-05 2.4896170e-05 3.3232871e-05 2.5006309e-05 2.4947023e-05]\n#  [2.5102483e-05 2.4947576e-05 2.4906987e-05 3.3327218e-05 2.4958186e-05]\n#  [2.4981882e-05 2.5007204e-05 2.4966144e-05 2.5076926e-05 3.3595043e-05]]\n</code></pre>\n<p>But, as you note, along the way to computing this 5x5 result, we instantiate two 5x10,000 matrices. How might we get around this?</p>\n<p>The answer is in <code>jax.jvp</code> and <code>jax.vjp</code>. These have somewhat unintuitive call signatures for the purposes of your question, as they are designed primarily for use in forward-mode and reverse-mode automatic differentiation. But broadly, you can think of them as a way to compute <code>J @ v</code> and <code>J.T @ v</code> for a vector <code>v</code>, without having to actually compute <code>J</code> explicitly.</p>\n<p>For example, you can use <code>jax.jvp</code> to compute the effect of <code>J1</code> operating on a vector, without actually computing <code>J1</code>:</p>\n<pre><code>J1_op = lambda v: jax.jvp(f, (x1,), (v,))[1]\n\nvN = np.random.rand(N)\nnp.allclose(J1 @ vN, J1_op(vN))\n# True\n</code></pre>\n<p>Similarly, you can use <code>jax.vjp</code> to compute the effect of <code>J2.T</code> operating on a vector, without actually computing <code>J2</code>:</p>\n<pre><code>J2T_op = lambda v: jax.vjp(f, x2)[1](v)[0]\n\nvM = np.random.rand(M)\nnp.allclose(J2.T @ vM, J2T_op(vM))\n# True\n</code></pre>\n<p>Putting these together and operating on an identity matrix gives you the full jacobian matrix product that you're after:</p>\n<pre><code>def direct(f, x1, x2):\n  J1 = jax.jacfwd(f)(x1)\n  J2 = jax.jacfwd(f)(x2)\n  return J1 @ J2.T\n\ndef indirect(f, x1, x2, M):\n  J1J2T_op = lambda v: jax.jvp(f, (x1,), jax.vjp(f, x2)[1](v))[1]\n  return jax.vmap(J1J2T_op)(jnp.eye(M)).T\n\nnp.allclose(direct(f, x1, x2), indirect(f, x1, x2, M))\n# True\n</code></pre>\n<p>Along with the memory savings, this indirect method is also a fair bit faster than the direct method, depending on the sizes of the jacobians involved:</p>\n<pre><code>%time direct(f, x1, x2)\n# CPU times: user 1.43 s, sys: 14.9 ms, total: 1.44 s\n# Wall time: 886 ms\n%time indirect(f, x1, x2, M)\n# CPU times: user 311 ms, sys: 0 ns, total: 311 ms\n# Wall time: 158 ms\n</code></pre>\n",
        "from_id": "298209",
        "to_id": "2937831",
        "answer_votes": 6,
        "question_votes": 4,
        "question_id": 63559139,
        "answer_id": 63946469,
        "question_title": "Efficient way to compute Jacobian x Jacobian.T",
        "question_link": "https://stackoverflow.com/questions/63559139/efficient-way-to-compute-jacobian-x-jacobian-t",
        "answer_link": "https://stackoverflow.com/questions/63559139/efficient-way-to-compute-jacobian-x-jacobian-t/63946469#63946469"
    },
    {
        "text_input": "<p>Here's my problem. I have two matrices <code>A</code> and <code>B</code>, with complex entries, of dimensions <code>(n,n,m,m)</code> and <code>(n,n)</code> respectively.</p>\n<p>Below is the operation I perform to get a matrix <code>C</code> -</p>\n<pre><code>C = np.sum(B[:,:,None,None]*A, axis=(0,1))\n</code></pre>\n<p>Computing the above once takes about 6-8 seconds. Since I have to compute many such <code>C</code>s, it takes a lot of time. Is there a faster way to do this? (I'm doing these using JAX NumPy on a multi-core CPU; normal NumPy takes even longer)</p>\n<p><code>n=77</code> and <code>m=512</code>, if you are wondering. I can parallelize as I'm working on a cluster, but the sheer size of the arrays consumes a lot of memory.</p>\n",
        "output": "<p>It looks like you want <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.einsum.html\" rel=\"nofollow noreferrer\"><code>einsum</code></a>:</p>\n<pre class=\"lang-python prettyprint-override\"><code>C = np.einsum('ijkl,ij-&gt;kl', A, B)\n</code></pre>\n<p>With numpy on a Colab CPU I get this:</p>\n<pre class=\"lang-python prettyprint-override\"><code>import numpy as np\nx = np.random.rand(50, 50, 500, 500)\ny = np.random.rand(50, 50)\n\ndef f1(x, y):\n  return np.sum(y[:,:,None,None]*x, axis=(0,1))\n\ndef f2(x, y):\n  return np.einsum('ijkl,ij-&gt;kl', x, y)\n\nnp.testing.assert_allclose(f1(x, y), f2(x, y))\n\n%timeit f1(x, y)\n# 1 loop, best of 5: 1.52 s per loop\n%timeit f2(x, y)\n# 1 loop, best of 5: 620 ms per loop\n</code></pre>\n",
        "from_id": "12342935",
        "to_id": "2937831",
        "answer_votes": 5,
        "question_votes": 4,
        "question_id": 70394566,
        "answer_id": 70394825,
        "question_title": "Fastest way to multiply and sum 4D array with 2D array in python?",
        "question_link": "https://stackoverflow.com/questions/70394566/fastest-way-to-multiply-and-sum-4d-array-with-2d-array-in-python",
        "answer_link": "https://stackoverflow.com/questions/70394566/fastest-way-to-multiply-and-sum-4d-array-with-2d-array-in-python/70394825#70394825"
    },
    {
        "text_input": "<p>This is a basic example.</p>\n<pre><code>@jax.jit\ndef block(arg1, arg2):\n   for x1 in range(cons1):\n       for x2 in range(cons2):\n          for x3 in range(cons3):\n             --do something--\n   return result\n</code></pre>\n<p>When cons are small, the compile-time is around a minute. With larger cons, compile time is much higher—10s of minutes. And I need even higher cons. What can be done?\nFrom what I am reading, the loops are the cause. They are unrolled at compile time.\nAre there any workarounds? There is also jax.fori_loop. But I don't understand how to use it. There is jax.experimental.loops module, but again I'm not able to understand it.</p>\n<p>I am very new to all this. Hence, all help is appreciated.\nIf you can provide some examples of how to use jax loops, that will be much appreciated.</p>\n<p>Also, what is an ok compile time? Is it ok for it to be in minutes?\nIn one of the examples, compile time is 262 seconds and remaining runs are ~0.1-0.2 seconds.</p>\n<p>Any gain in runtime is overshadowed by the compile time.</p>\n",
        "output": "<p>JAX's JIT compiler flattens all Python loops. To see what I mean, take a look at this simple function run through <code>jax.make_jaxpr</code>, which is a way to examine how JAX's tracer interprets python code (see <a href=\"https://jax.readthedocs.io/en/latest/jaxpr.html\" rel=\"noreferrer\">Understanding Jaxprs</a> for more):</p>\n<pre class=\"lang-python prettyprint-override\"><code>import jax\n\ndef f(x):\n  for i in range(5):\n    x += i\n  return x\n\nprint(jax.make_jaxpr(f)(0))\n# { lambda  ; a.\n#   let b = add a 0\n#       c = add b 1\n#       d = add c 2\n#       e = add d 3\n#       f = add e 4\n#   in (f,) }\n</code></pre>\n<p>Notice that the loop is flattened: every step becomes an explicit operation sent to the XLA compiler. The XLA compile time increases as you increase the number of operations in the function, so it makes sense that a triply-nested for-loop would lead to long compile times.</p>\n<p>So, how to address this? Well, unfortunately the answer depends on what your <code>--do something--</code> is doing, so I can't guess that.</p>\n<p>In general, the best option is to use vectorized array operations rather than loops over the values in those vectors; for example, here is a very slow way of adding two vectors:</p>\n<pre><code>import jax.numpy as jnp\n\ndef f_slow(x, y):\n  z = []\n  for xi, yi in zip(xi, yi):\n    z.append(xi + yi)\n  return jnp.array(z)\n</code></pre>\n<p>and here is a much faster way to do the same thing:</p>\n<pre class=\"lang-python prettyprint-override\"><code>def f_fast(x, y):\n  return x + y\n</code></pre>\n<p>If your operations don't lend themselves to vectorization, another option is to use <a href=\"https://jax.readthedocs.io/en/latest/jax.lax.html#control-flow-operators\" rel=\"noreferrer\">lax control flow</a> operators in place of the <code>for</code> loops: this will push the loop down into XLA. This can have quite good performance on CPU, but is slower on accelerators when compared to equivalent vectorized array operations.</p>\n<p>For more discussion on JAX and Python control flow statements (such as <code>for</code>, <code>if</code>, <code>while</code>, etc.), see <a href=\"https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#control-flow\" rel=\"noreferrer\">🔪 JAX - The Sharp Bits 🔪: Control Flow</a>.</p>\n",
        "from_id": "13468128",
        "to_id": "2937831",
        "answer_votes": 5,
        "question_votes": 4,
        "question_id": 69070804,
        "answer_id": 69080028,
        "question_title": "How to reduce JAX compile time when using for loop?",
        "question_link": "https://stackoverflow.com/questions/69070804/how-to-reduce-jax-compile-time-when-using-for-loop",
        "answer_link": "https://stackoverflow.com/questions/69070804/how-to-reduce-jax-compile-time-when-using-for-loop/69080028#69080028"
    },
    {
        "text_input": "<p>This is a basic example.</p>\n<pre><code>@jax.jit\ndef block(arg1, arg2):\n   for x1 in range(cons1):\n       for x2 in range(cons2):\n          for x3 in range(cons3):\n             --do something--\n   return result\n</code></pre>\n<p>When cons are small, the compile-time is around a minute. With larger cons, compile time is much higher—10s of minutes. And I need even higher cons. What can be done?\nFrom what I am reading, the loops are the cause. They are unrolled at compile time.\nAre there any workarounds? There is also jax.fori_loop. But I don't understand how to use it. There is jax.experimental.loops module, but again I'm not able to understand it.</p>\n<p>I am very new to all this. Hence, all help is appreciated.\nIf you can provide some examples of how to use jax loops, that will be much appreciated.</p>\n<p>Also, what is an ok compile time? Is it ok for it to be in minutes?\nIn one of the examples, compile time is 262 seconds and remaining runs are ~0.1-0.2 seconds.</p>\n<p>Any gain in runtime is overshadowed by the compile time.</p>\n",
        "output": "<p>I am not sure if this is will be the same as with <code>numba</code>, but this might be similar case.</p>\n<p>When I use <code>numba.jit</code> compiler and have big data input, first I compile function on some small example data, then use it.</p>\n<p>Pseudo-code:</p>\n<pre><code>func_being_compiled(small_amount_of_data)  # compile-only purpose\nfunc_being_compiled(large_amount_of_data)\n\n</code></pre>\n",
        "from_id": "13468128",
        "to_id": "4601890",
        "answer_votes": 0,
        "question_votes": 4,
        "question_id": 69070804,
        "answer_id": 69072951,
        "question_title": "How to reduce JAX compile time when using for loop?",
        "question_link": "https://stackoverflow.com/questions/69070804/how-to-reduce-jax-compile-time-when-using-for-loop",
        "answer_link": "https://stackoverflow.com/questions/69070804/how-to-reduce-jax-compile-time-when-using-for-loop/69072951#69072951"
    },
    {
        "text_input": "<p>I am trying to use JAX pmap but I am getting the error that XLA devices aren't visible -\nHere's my code -</p>\n<pre><code>import jax.numpy as jnp\nimport os\nfrom jax import pmap\nos.environ[&quot;XLA_FLAGS&quot;] = '--xla_force_host_platform_device_count=8'\n\nout = pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)\n</code></pre>\n<p>Here's the error -</p>\n<pre><code>Traceback (most recent call last):\n  File &quot;new.py&quot;, line 6, in &lt;module&gt;\n    out = pmap(lambda x: x ** 2)(jnp.arange(8))\n  File &quot;/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/traceback_util.py&quot;, line 166, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File &quot;/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/api.py&quot;, line 1779, in cache_miss\n    execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)\n  File &quot;/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py&quot;, line 411, in xla_pmap_impl_lazy\n    compiled_fun, fingerprint = parallel_callable(\n  File &quot;/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/linear_util.py&quot;, line 345, in memoized_fun\n    ans = call(fun, *args)\n  File &quot;/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py&quot;, line 682, in parallel_callable\n    pmap_executable = pmap_computation.compile()\n  File &quot;/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/profiler.py&quot;, line 314, in wrapper\n    return func(*args, **kwargs)\n  File &quot;/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py&quot;, line 923, in compile\n    executable = UnloadedPmapExecutable.from_hlo(\n  File &quot;/home/thoma/anaconda3/envs/tbd/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py&quot;, line 993, in from_hlo\n    raise ValueError(msg.format(shards.num_global_shards,\njax._src.traceback_util.UnfilteredStackTrace: ValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8)\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;new.py&quot;, line 6, in &lt;module&gt;\n    out = pmap(lambda x: x ** 2)(jnp.arange(8))\nValueError: compiling computation that requires 8 logical devices, but only 1 XLA devices are available (num_replicas=8)\n</code></pre>\n<hr />\n<p>Based on <a href=\"https://github.com/google/jax/issues/1408\" rel=\"nofollow noreferrer\">this</a> and <a href=\"https://stackoverflow.com/questions/72328521/jax-pmap-with-multi-core-cpu\">this</a> discussion, I did this <code>os.environ[&quot;XLA_FLAGS&quot;] = '--xla_force_host_platform_device_count=8'</code>, but it doesn't seem to work.</p>\n<hr />\n<p>Edit 1:</p>\n<p>I tried this but it still doesn't work -</p>\n<pre><code>import os\nos.environ[&quot;XLA_FLAGS&quot;] = '--xla_force_host_platform_device_count=8'\n\nimport jax\n\n\nfrom jax import pmap\nimport jax.numpy as jnp\n\nout = pmap(lambda x: x ** 2)(jnp.arange(8))\nprint(out)\n\n</code></pre>\n",
        "output": "<p>XLA flags are read when JAX is imported, so you need to set them before importing JAX if you want the flags to have an effect.</p>\n<p>You should also make sure you're in a clean runtime (i.e. not using a Jupyter kernel where you have previously imported <code>jax</code>).</p>\n<p>Additionally, keep in mind that <code>--xla_force_host_platform_device_count=8</code> only affects the host (CPU) device count, so the code as written above won't work if you're using GPU-enabled JAX with a single GPU device. If this is the case, you can force <code>pmap</code> to run on the non-default CPU devices using the devices argument:</p>\n<pre class=\"lang-py prettyprint-override\"><code>out = pmap(lambda x: x ** 2, devices=jax.devices('cpu')(jnp.arange(8))\n</code></pre>\n",
        "from_id": "11628437",
        "to_id": "2937831",
        "answer_votes": 4,
        "question_votes": 4,
        "question_id": 77889712,
        "answer_id": 77890045,
        "question_title": "How to use JAX pmap with CPU cores",
        "question_link": "https://stackoverflow.com/questions/77889712/how-to-use-jax-pmap-with-cpu-cores",
        "answer_link": "https://stackoverflow.com/questions/77889712/how-to-use-jax-pmap-with-cpu-cores/77890045#77890045"
    },
    {
        "text_input": "<p>How can I generate random numbers between 0 and 1 in jax?\nBasically I am looking to replicate the following function from <code>numpy</code> in <code>jax</code>.</p>\n<pre><code>np.random.random(1000)\n</code></pre>\n",
        "output": "<p>The equivalent in jax would be</p>\n<pre class=\"lang-python prettyprint-override\"><code>from jax import random\nkey = random.PRNGKey(758493)  # Random seed is explicit in JAX\nrandom.uniform(key, shape=(1000,))\n</code></pre>\n<p>For more information, see the documentation of the <a href=\"https://jax.readthedocs.io/en/latest/jax.random.html\" rel=\"nofollow noreferrer\"><code>jax.random</code> module</a>.</p>\n<p>Also be aware that JAX's random number generator does not maintain any sort of global state, so you'll need to think about it a bit differently than you may be accustomed to in NumPy. For more background on this, see <a href=\"https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#random-numbers\" rel=\"nofollow noreferrer\">JAX Sharp Bits: Random Numbers</a>.</p>\n",
        "from_id": "236106",
        "to_id": "2937831",
        "answer_votes": 4,
        "question_votes": 4,
        "question_id": 72320999,
        "answer_id": 72321700,
        "question_title": "How to generate random numbers between 0 and 1 in jax?",
        "question_link": "https://stackoverflow.com/questions/72320999/how-to-generate-random-numbers-between-0-and-1-in-jax",
        "answer_link": "https://stackoverflow.com/questions/72320999/how-to-generate-random-numbers-between-0-and-1-in-jax/72321700#72321700"
    },
    {
        "text_input": "<p>Lets suppose I have some function which returns a sum of inputs.</p>\n<pre><code>@jit\ndef some_func(a,r1,r2):\n    return a + r1 + r2\n\n</code></pre>\n<p>Now I would like to loop over different values of <code>r1</code> and <code>r2</code>, save the result and add it to a counter. This is what I mean:</p>\n<pre><code>a = 0 \nr1 = jnp.arange(0,3)\nr2 = jnp.arange(0,3)\n\n\ns = 0 \nfor i in range(len(r1)): \n    for j in range(len(r2)): \n        s+= some_func(a, r1[i], r2[j])\n    \nprint(s)\nDeviceArray(18, dtype=int32)\n\n</code></pre>\n<p>My question is, how do I do this with <code>jax.vmap</code> to avoid writing the <code>for</code> loops? I have something like this so far:</p>\n<pre><code>vmap(some_func, in_axes=(None, 0,0), out_axes=0)(jnp.arange(0,3), jnp.arange(0,3))\n\n</code></pre>\n<p>but this gives me the following error:</p>\n<pre><code>ValueError: vmap in_axes specification must be a tree prefix of the corresponding value, got specification (None, 0, 0) for value tree PyTreeDef((*, *)).\n</code></pre>\n<p>I have a feeling that the error is in <code>in_axes</code> but I am not sure how to get <code>vmap</code> to pick a value for <code>r1</code> loop over <code>r2</code> and then do the same for all <code>r1</code> whilst saving intermediate results.</p>\n<p>Any help is appreciated.</p>\n",
        "output": "<p><code>vmap</code> will map over a single axis at a time. Because you want to map over two different axes, you'll need two <code>vmap</code> calls:</p>\n<pre class=\"lang-python prettyprint-override\"><code>func_mapped = vmap(vmap(some_func, (None, 0, None)), (None, None, 0))\nfunc_mapped(a, r1, r2).sum()\n# 18\n</code></pre>\n<p>Alternatively, for a simple function like this you can avoid <code>vmap</code> and use numpy-style broadcasting to get the same result:</p>\n<pre class=\"lang-python prettyprint-override\"><code>some_func(a, r1[None, :, None], r2[None, None, :]).sum()\n# 18\n</code></pre>\n",
        "from_id": "18142329",
        "to_id": "2937831",
        "answer_votes": 4,
        "question_votes": 4,
        "question_id": 71875197,
        "answer_id": 71883980,
        "question_title": "vmap in Jax to loop over arguments",
        "question_link": "https://stackoverflow.com/questions/71875197/vmap-in-jax-to-loop-over-arguments",
        "answer_link": "https://stackoverflow.com/questions/71875197/vmap-in-jax-to-loop-over-arguments/71883980#71883980"
    },
    {
        "text_input": "<p>I want to train a simple linear model. these below x and y are my data.</p>\n<pre><code>import numpy as np\nx = np.linspace(0,1,100)\ny = 2 * x + 3 + np.random.randn(100)\n\n</code></pre>\n<p>f is a function that calculates mean square error over all data.</p>\n<pre><code>def f(params, x, y):\n  return np.mean(np.power((params['w'] * x + params['b'])-y , 2))\n\n</code></pre>\n<pre><code>from jax import grad\ndf = grad(f)\nparams = dict()\n#initialize parameters\nparams['w'] = 2.4\nparams['b'] = 10.\ndf(params, x, y) # I will do this in a loop (implementing gradient decent part\n\n</code></pre>\n<p>this gives me an error:</p>\n<pre><code>FilteredStackTrace: jax._src.errors.TracerArrayConversionError: The numpy.ndarray conversion method __array__() was called on the JAX Tracer object Traced&lt;ConcreteArray\n</code></pre>\n<p>when I clear <code>np.power</code> code works. why?</p>\n",
        "output": "<p>JAX cannot compute gradients of <code>numpy</code> functions, but it can compute gradients of <code>jax.numpy</code> functions. If you rewrite your code in terms of <code>jax.numpy</code>, it should work for you:</p>\n<pre class=\"lang-python prettyprint-override\"><code>import numpy as np\nx = np.linspace(0,1,100)\ny = 2 * x + 3 + np.random.randn(100)\n\nimport jax.numpy as jnp\ndef f(params, x, y):\n  return jnp.mean(jnp.power((params['w'] * x + params['b'])-y , 2))\n\nfrom jax import grad\ndf = grad(f)\nparams = dict()\n\nparams['w'] = 2.4\nparams['b'] = 10.\ndf(params, x, y)\n# {'b': DeviceArray(14.661432, dtype=float32),\n#  'w': DeviceArray(7.3792152, dtype=float32)}\n</code></pre>\n<p>You can read more details in the <a href=\"https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerArrayConversionError\" rel=\"nofollow noreferrer\"><code>TracerArrayConversionError</code> documentation page</a>.</p>\n",
        "from_id": "6663733",
        "to_id": "2937831",
        "answer_votes": 4,
        "question_votes": 4,
        "question_id": 67285326,
        "answer_id": 67287238,
        "question_title": "why when i have a np.power in my function jax.grad can&#39;t give me the derivitives?",
        "question_link": "https://stackoverflow.com/questions/67285326/why-when-i-have-a-np-power-in-my-function-jax-grad-cant-give-me-the-derivitives",
        "answer_link": "https://stackoverflow.com/questions/67285326/why-when-i-have-a-np-power-in-my-function-jax-grad-cant-give-me-the-derivitives/67287238#67287238"
    },
    {
        "text_input": "<p>I'm wondering if there is a good way to limit the memory usage for Jax's VMAP function? Equivalently, to vmap in batches at a time if that makes sense?</p>\n<p>In my specific use case, I have a set of images and I'd like to calculate the affinity between each pair of images; so ~order((num_imgs)^2 * (img shape)) bytes of memory used all at once if I'm understanding vmap correctly (which gets huge since in my real example I have 10,000 100x100 images).</p>\n<p>A basic example is:</p>\n<pre><code>def affininty_matrix_ex(n_arrays=10, img_size=5, key=jax.random.PRNGKey(0), gamma=jnp.array([0.5])):\n    arr_of_imgs = jax.random.normal(jax.random.PRNGKey(0), (n_arrays, img_size, img_size))\n    arr_of_indices = jnp.arange(n_arrays)\n    inds_1, inds_2 = zip(*combinations(arr_of_indices, 2))\n    v_cPA = jax.vmap(calcPairAffinity2, (0, 0, None, None), 0)\n    affinities = v_cPA(jnp.array(inds_1), jnp.array(inds_2), arr_of_imgs, gamma)\n    print()\n    print(jax.make_jaxpr(v_cPA)(jnp.array(inds_1), jnp.array(inds_2), arr_of_imgs, gamma))\n    \n    affinities = affinities.reshape(-1)\n    \n    arr = jnp.zeros((n_arrays, n_arrays), dtype=jnp.float16)\n    arr = arr.at[jnp.triu_indices(arr.shape[0], k=1)].set(affinities)\n    arr = arr + arr.T\n    arr = arr + jnp.identity(n_arrays, dtype=jnp.float16)\n    \n    return arr\n\n\ndef calcPairAffinity2(ind1, ind2, imgs, gamma):\n    #Returns a jnp array of 1 float, jnp.sum adds all elements together\n    image1, image2 = imgs[ind1], imgs[ind2]\n    diff = jnp.sum(jnp.abs(image1 - image2))  \n    normed_diff = diff / image1.size\n    val = jnp.exp(-gamma*normed_diff)\n    val = val.astype(jnp.float16)\n    return val\n</code></pre>\n<p>I suppose I could just say something like &quot;only feed into vmap X pairs at a time, and loop through n_chunks = n_arrays/X, appending each groups results to a list&quot; but that doesn't seem to be ideal. My understanding is vmap does not like generators, not sure if that would be an alternative way around the issue.</p>\n",
        "output": "<p><em>Edit, Aug 13 2024</em></p>\n<p>As of JAX version 0.4.31, what you're asking for is possible using the <code>batch_size</code> argument of <a href=\"https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.map.html\" rel=\"nofollow noreferrer\"><code>lax.map</code></a>.\nFor an iterable of size <code>N</code>, this will perform a scan with <code>N // batch_size</code> steps, and within each step will <code>vmap</code> the function over the batch. <code>lax.map</code> has less flexible semantics than <code>jax.vmap</code>, but for the simplest cases they look relatively similar. Here's an example using your <code>calcPairAffinity</code> function:</p>\n<p>For example</p>\n<pre class=\"lang-py prettyprint-override\"><code>import jax\nimport jax.numpy as jnp\n\ndef calcPairAffinity(ind1, ind2, imgs, gamma=0.5):\n    image1, image2 = imgs[ind1], imgs[ind2]\n    diff = jnp.sum(jnp.abs(image1 - image2))  \n    normed_diff = diff / image1.size\n    val = jnp.exp(-gamma*normed_diff)\n    val = val.astype(jnp.float16)\n    return val\n\nimgs = jax.random.normal(jax.random.key(0), (100, 5, 5))\ninds = jnp.arange(imgs.shape[0])\ninds1, inds2 = map(jnp.ravel, jnp.meshgrid(inds, inds))\n\ndef f(inds):\n  return calcPairAffinity(*inds, imgs, 0.5)\n\n\nresult_vmap = jax.vmap(f)((inds1, inds2))\nresult_batched = jax.lax.map(f, (inds1, inds2), batch_size=1000)\nassert jnp.allclose(result_vmap, result_batched)\n</code></pre>\n<hr />\n<p><em>Original answer</em></p>\n<p>This is a frequent request, but unfortunately there's not yet (as of JAX version 0.4.20) any built-in utility to do chunked/batched vmap (<a href=\"https://jax.readthedocs.io/en/latest/_autosummary/jax.experimental.maps.xmap.html\" rel=\"nofollow noreferrer\"><code>xmap</code></a> does have some functionality along these lines, but is experimental/incomplete and I wouldn't recommend relying on it).</p>\n<p>Adding chunking to <code>vmap</code> is tracked in <a href=\"https://github.com/google/jax/issues/11319\" rel=\"nofollow noreferrer\">https://github.com/google/jax/issues/11319</a>, and there's some code there that does a limited version of what you have in mind. Hopefully something like what you describe will be possible with JAX's built-in <code>vmap</code> soon. In the meantime, you might think about applying <code>vmap</code> to chunks manually in the way you describe in your question.</p>\n",
        "from_id": "7067333",
        "to_id": "2937831",
        "answer_votes": 3,
        "question_votes": 4,
        "question_id": 77527847,
        "answer_id": 77530888,
        "question_title": "Jax vmap limit memory",
        "question_link": "https://stackoverflow.com/questions/77527847/jax-vmap-limit-memory",
        "answer_link": "https://stackoverflow.com/questions/77527847/jax-vmap-limit-memory/77530888#77530888"
    }
]